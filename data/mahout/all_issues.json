{
    "MAHOUT-1": {
        "Key": "MAHOUT-1",
        "Summary": "Mahout site doesn't link to mailing list archives.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Documentation",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Yousef Ourabi",
        "Created": "30/Jan/08 18:10",
        "Updated": "31/Jan/20 15:40",
        "Resolved": "30/Jan/08 18:19",
        "Description": "http://lucene.apache.org/mahout/mailinglists.html doesn't link to the mailing lists that are available via the cwiki \u2013 this is confusing to new users, potentially interested developers. Included is a patch to  src/documentation/content/xdocs/mailinglists.xml to include links to those mailingl ists.",
        "Issue Links": []
    },
    "MAHOUT-2": {
        "Key": "MAHOUT-2",
        "Summary": "Initial Imports",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Edward J. Yoon",
        "Created": "03/Feb/08 10:16",
        "Updated": "12/Mar/08 06:46",
        "Resolved": "12/Mar/08 06:42",
        "Description": "We needs initial base code. I'm going to import some implemented code, but\nI think we need to discuss a bit about the directory structure.",
        "Issue Links": []
    },
    "MAHOUT-3": {
        "Key": "MAHOUT-3",
        "Summary": "Build initial canopy clustering prototype",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Jeff Eastman",
        "Created": "06/Feb/08 20:56",
        "Updated": "21/May/11 03:24",
        "Resolved": "20/Feb/08 04:28",
        "Description": "I'd like to reserve some namespace, specifically org.apache.mahout.clustering.canopy to use for an initial prototype of canopy clustering. I'm going to start with a little unit test to get the basic algorithm sorted out, then M/R it.",
        "Issue Links": []
    },
    "MAHOUT-4": {
        "Key": "MAHOUT-4",
        "Summary": "Simple prototype for Expectation Maximization (EM)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ankur Bansal",
        "Created": "13/Feb/08 12:43",
        "Updated": "21/May/11 03:22",
        "Resolved": "18/Nov/09 00:20",
        "Description": "Create a simple prototype implementing Expectation Maximization - EM that demonstrates the algorithm functionality given a set of (user, click-url) data.\nThe prototype should be functionally complete and should serve as a basis for the Map-Reduce version of the EM algorithm.",
        "Issue Links": [
            "/jira/browse/MAHOUT-28",
            "/jira/browse/MAHOUT-29",
            "/jira/browse/MAHOUT-30",
            "/jira/browse/MAHOUT-31"
        ]
    },
    "MAHOUT-5": {
        "Key": "MAHOUT-5",
        "Summary": "Implement a k-means clustering prototype",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Jeff Eastman",
        "Created": "17/Feb/08 19:47",
        "Updated": "21/May/11 03:24",
        "Resolved": "01/Mar/08 03:33",
        "Description": "K-means clustering is closely related to Canopy clustering and often uses canopies to determine the initial clusters. I'd like to implement a k-means prototype and tests in the package org.apache.mahout.clustering.kmeans.",
        "Issue Links": []
    },
    "MAHOUT-6": {
        "Key": "MAHOUT-6",
        "Summary": "Need a matrix implementation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Ted Dunning",
        "Created": "22/Feb/08 03:37",
        "Updated": "21/May/11 03:24",
        "Resolved": "29/Mar/08 21:10",
        "Description": "We need matrices for Mahout.\nAn initial set of basic requirements includes:\na) sparse and dense support are required\nb) row and column labels are important\nc) serialization for hadoop use is required\nd) reasonable floating point performance is required, but awesome FP is not\ne) the API should be simple enough to understand\nf) it should be easy to carve out sub-matrices for sending to different reducers\ng) a reasonable set of matrix operations should be supported, these should eventually include:\n    simple matrix-matrix and matrix-vector and matrix-scalar linear algebra operations, A B, A + B, A v, A + x, v + x, u + v, dot(u, v)\n    row and column sums  \n    generalized level 2 and 3 BLAS primitives, alpha A B + beta C and A u + beta v\nh) easy and efficient iteration constructs, especially for sparse matrices\ni) easy to extend with new implementations",
        "Issue Links": [
            "/jira/browse/MAHOUT-7"
        ]
    },
    "MAHOUT-7": {
        "Key": "MAHOUT-7",
        "Summary": "Lucene indexes should act as matrix factories",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "24/Feb/08 18:57",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "18/Nov/09 00:19",
        "Description": "It would be highly desirable to be able to extract virtual matrices from lucene indexes.\nThe factory methods that I know of would include:\na) the factory would accept the name of a single field and the resulting matrix would use document id's as row labels and terms as column labels.  The values would be the term counts in the document (if available), or 1 if the term is in the document, but the term frequency is not available.  This implies that TermVectors could be viewed as rows of this matrix.  Columns could be extract by boolean retrieval from the index.  Retrieval from that field could be considered a form of matrix-vector multiplication where the vector encodes a query using the values as term boosts and the result wraps a hit structure as a sparse matrix.  Matrix-matrix arithmetic with pairs of this kind of matrix should yield a matrix as in (b).\nb) the factory would accept a linear combination of terms and the resulting matrix would have rows which are linear combinations of the underlying term-vectors (could this be done latently so computation is only on access?  would that help?).  Column access would be a form of retrieval (but what would the semantics be?).  Matrix vector product could again be viewed as retrieval, but it would probably be most useful to view the original coefficients as boosts for the Lucene scoring mechanism rather than computing a linear combination of scores.\nc) the factor would produce a matrix in which the rows are all documents and the columns are all terms from all fields, each labeled with field and term name (probably using lucene query syntax).  Rows would be the concatenation of all term vectors, columns would represent retrieval on a single term.  Matrix vector multiplication would be general Lucene retrieval.  Matrix-matrix operations between lucene indexes should do something interesting (A' A, for instance might compute term coocurrence), but that seems pretty hairy to specify.  Matrix-matrix operations with ordinary matrices on the right might best be considered as multiple retrievals using each column of the right hand matrix as query.\nd) as with (c), but with only a defined list of fields with the rest of the fields not being expressed as columns.\nIssues with this API mostly center around efficiency of how to deal with expressions involving indexes (should operations be eager or lazy) and whether the use of multiplication as retrieval is too controversial.  An alternative might be to add a query operation to the API just for indexes.",
        "Issue Links": [
            "/jira/browse/MAHOUT-6"
        ]
    },
    "MAHOUT-8": {
        "Key": "MAHOUT-8",
        "Summary": "Data definition model",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Karl Wettin",
        "Created": "25/Feb/08 21:32",
        "Updated": "21/May/11 03:27",
        "Resolved": "19/Nov/09 08:24",
        "Description": "How do we define classes, attributes and instance data?\nThis has nothing to do with physical data records, this is about data types, roles, etc.",
        "Issue Links": []
    },
    "MAHOUT-9": {
        "Key": "MAHOUT-9",
        "Summary": "Implement MapReduce BayesianClassifier",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Classification",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "28/Feb/08 18:54",
        "Updated": "21/May/11 03:24",
        "Resolved": "26/Aug/08 12:58",
        "Description": "Implement a Bayesian classifier using M/R.\nI have a simple trainer done (not M/R) and will implement the classifier soon, then will upgrade it to use Hadoop.",
        "Issue Links": []
    },
    "MAHOUT-10": {
        "Key": "MAHOUT-10",
        "Summary": "Replace fall-through exception handlers with propagated unchecked exception.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Dawid Weiss",
        "Reporter": "Dawid Weiss",
        "Created": "06/Mar/08 09:10",
        "Updated": "21/May/11 03:23",
        "Resolved": "09/Mar/08 11:10",
        "Description": "I am doing a belated code review. There certain issues that I would like to change, for example fall-through exception handlers like this one:\n    try \n{\n      Class cl = Class.forName(job.get(DISTANCE_MEASURE_KEY));\n      measure = (DistanceMeasure) cl.newInstance();\n      measure.configure(job);\n    }\n catch (Exception e) \n{\n      e.printStackTrace();\n    }\n\nThis prints the stack trace of an exception to the console, but continues thread's execution after the catch clause. Since distance measure key is required, this makes little sense. A runtime exception should be thrown \u2013 this stops the job and causes a full stack trace to be displayed anyway (with the nested exception's message).",
        "Issue Links": []
    },
    "MAHOUT-11": {
        "Key": "MAHOUT-11",
        "Summary": "Static fields used throughout clustering code (Canopy, K-Means).",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "Drew Farris",
        "Reporter": "Dawid Weiss",
        "Created": "06/Mar/08 09:35",
        "Updated": "21/May/11 03:24",
        "Resolved": "10/Dec/09 09:40",
        "Description": "I file this as a bug, even though I'm not 100% sure it is one. In the currect code the information is exchanged via static fields (for example, distance measure and thresholds for Canopies are static field). Is it always true in Hadoop that one job runs inside one JVM with exclusive access? I haven't seen it anywhere in Hadoop documentation and my impression was that everything uses JobConf to pass configuration to jobs, but jobs are configured on a per-object basis (a job is an object, a mapper is an object and everything else is basically an object).\nIf it's possible for two jobs to run in parallel inside one JVM then this is a limitation and bug in our code that needs to be addressed.",
        "Issue Links": []
    },
    "MAHOUT-12": {
        "Key": "MAHOUT-12",
        "Summary": "Point formatting and parsing improved (StringBuilder, no need for trailing comma).",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Dawid Weiss",
        "Reporter": "Dawid Weiss",
        "Created": "06/Mar/08 09:54",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/Mar/08 11:12",
        "Description": "Added test case to point class, improved parsing (no need to recompile the pattern all over again) and concatenation of points (stringbuilder used internally).",
        "Issue Links": []
    },
    "MAHOUT-13": {
        "Key": "MAHOUT-13",
        "Summary": "Investigate Mahout jar loading",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Dawid Weiss",
        "Reporter": "Grant Ingersoll",
        "Created": "06/Mar/08 13:28",
        "Updated": "21/May/11 03:24",
        "Resolved": "14/Mar/08 14:27",
        "Description": "Currently, the Mahout clustering code requires the JAR to be specified.  Investigate alternatives for class loading in Hadoop.",
        "Issue Links": []
    },
    "MAHOUT-14": {
        "Key": "MAHOUT-14",
        "Summary": "Need an SVM implementation",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Paul Elschot",
        "Created": "07/Mar/08 08:08",
        "Updated": "17/Oct/19 16:11",
        "Resolved": "19/Nov/09 08:20",
        "Description": "As SVM's have been mentioned a few times now, this would be a good place to concentrate discussions on the subject.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/379"
        ]
    },
    "MAHOUT-15": {
        "Key": "MAHOUT-15",
        "Summary": "Investigate Mean Shift Clustering",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "12/Mar/08 01:00",
        "Updated": "21/May/11 03:24",
        "Resolved": "15/Apr/08 01:18",
        "Description": "\"The mean shift algorithm is a nonparametric clustering technique which does not require prior knowledge of the number of clusters, and does not constrain the shape of the clusters.\" http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/TUZEL1/MeanShift.pdf \nInvestigate implementing mean shift clustering using Hadoop",
        "Issue Links": []
    },
    "MAHOUT-16": {
        "Key": "MAHOUT-16",
        "Summary": "Hama contrib package for the mahout",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Edward J. Yoon",
        "Created": "12/Mar/08 06:22",
        "Updated": "21/May/11 03:27",
        "Resolved": "17/Mar/08 08:59",
        "Description": "Introduction\nHama will develop a high-performance and large-scale parallel matrix computational package based on Hadoop Map/Reduce. It will be useful for a massively large-scale Numerical Analysis and Data Mining, which need the intensive computation power of matrix inversion, e.g. linear regression, PCA, SVM and etc. It will be also useful for many scientific applications, e.g. physics computations, linear algebra, computational fluid dynamics, statistics, graphic rendering and many more.\nHama approach proposes the use of 3-dimensional Row and Column (Qualifier), Time space and multi-dimensional Columnfamilies of Hbase (BigTable Clone), which is able to store large sparse and various type of matrices (e.g. Triangular Matrix, 3D Matrix, and etc.). its auto-partitioned sparsity sub-structure will be efficiently managed and serviced by Hbase. Row and Column operations can be done in linear-time, where several algorithms, such as structured Gaussian elimination or iterative methods, run in O(the number of non-zero elements in the matrix / number of mappers) time on Hadoop Map/Reduce.\nSo, it has a strong relationship with the mahout project, and it would be great if the \"hama\" can become a contrib project of the mahout.\nCurrent Status\nIn its current state, the 'hama' is buggy and needs filling out, but generalized matrix interface and basic linear algebra operations was implemented within a large prototype system. In the future, We need new parallel algorithms based on Map/Reduce for performance of heavy decompositions and factorizations. It also needs tools to compose an arbitrary matrix only with certain data filtered from hbase array structure. \nIt would be great if we can collaboration with the mahout members.\nMembers\nThe initial set of committers includes folks from the Hadoop & Hbase communities, and We have a master's (or Ph.D) degrees in the mathematics and computer science.\n\nEdward Yoon (edward AT udanax DOT org)\nChanwit Kaewkasi (chanwit AT gmail DOT com)\nMin Cha (minslovey AT gmail DOT com)\nAntonio Suh (bluesvm AT gmail DOT com)\n\nAt least, I and Min Cha will be involved full-time with this work.",
        "Issue Links": []
    },
    "MAHOUT-17": {
        "Key": "MAHOUT-17",
        "Summary": "Maven support",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "15/Mar/08 22:00",
        "Updated": "21/Aug/08 15:39",
        "Resolved": "23/Apr/08 13:17",
        "Description": "The trunk is a Maven module root. I think it should be refactored to a Maven project root.",
        "Issue Links": []
    },
    "MAHOUT-18": {
        "Key": "MAHOUT-18",
        "Summary": "Embrace interoperability with other softwares",
        "Type": "New JIRA Project",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Shunkai Fu",
        "Created": "19/Mar/08 00:51",
        "Updated": "21/May/11 03:27",
        "Resolved": "18/Nov/09 00:24",
        "Description": "This is an open issue. It is related with all possible components existing or to born in the future. \nML or DM models normally have two phases: training and scoring (or predicting). If we agree \"updating\" is an independent one, we will have 3 phases. \nThere are many softwares about ML/DM outside. We want the users of Mahout be able to import models got built from other software here, update them and/or use them for scoring. To achieve this goal, we need to recognize the commonly used formats. \nBesides, users may choose Mahout because Mahout is speedy in learning. After a model is ready, they may export the model trained, view it with some visualization tool, or import it into other software or application for scoring (or predicting). In this case, exporting into widely recognized format is expected.\nFinally, I want to say that the importing and exporting will not influence the ongoing projects, so developers of other components need not worry about this.",
        "Issue Links": []
    },
    "MAHOUT-19": {
        "Key": "MAHOUT-19",
        "Summary": "Hierarchial clusterer",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "30/Mar/08 06:11",
        "Updated": "21/May/11 03:27",
        "Resolved": "23/Dec/09 19:57",
        "Description": "In a hierarchial clusterer the instances are the leaf nodes in a tree where branch nodes contains the mean features of and the distance between its children.\nFor performance reasons I always trained trees from the top->down. I have been told that it can cause various effects I never encountered. And I believe Huffman solved his problem by training bottom->up? The thing is, I don't think it is possible to train the tree top->down using map reduce. I do however think it is possible to train it bottom->up. I would very much appreciate any thoughts on this.\nOnce this tree is trained one can extract clusters in various ways. The mean distance between all instances is usually a good maximum distance to allow between nodes when navigating the tree in search for a cluster. \nNavigating the tree and gather nodes that are not too far away from each other is usually instant if the tree is available in memory or persisted in a smart way. In my experience there is not much to win from extracting all clusters from start. Also, it usually makes sense to allow for the user to modify the cluster boundary variables in real time using a slider or perhaps present the named summary of neighbouring clusters, blacklist paths in the tree, etc. It is also not to bad to use secondary classification on the instances to create worm holes in the tree. I always thought it would be cool to visualize it using Touchgraph.\nMy focus is on clustering text documents for instant \"more like this\"-feature in search engines and use Tanimoto similarity on the vector spaces to calculate the distance.\nSee LUCENE-1025 for a single threaded all in memory proof of concept of a hierarchial clusterer.",
        "Issue Links": [
            "/jira/browse/HADOOP-3977",
            "/jira/browse/MAHOUT-20"
        ]
    },
    "MAHOUT-20": {
        "Key": "MAHOUT-20",
        "Summary": "Migrate Canopy and KMeans Implementations to Vectors",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "30/Mar/08 19:01",
        "Updated": "21/May/11 03:23",
        "Resolved": "19/Apr/08 00:09",
        "Description": "Canopy and KMeans clustering implementations use Float[] representations instead of the new Vector package. They need to be migrated and the Vector package may need some enhancement to support the notion of payloads. This would be a good project for somebody new to the project who wants to get involved. If somebody wants to implement this, just assign the issue to yourself and I will hold off doing it myself.",
        "Issue Links": [
            "/jira/browse/MAHOUT-42",
            "/jira/browse/MAHOUT-19",
            "/jira/browse/MAHOUT-36",
            "/jira/browse/MAHOUT-34"
        ]
    },
    "MAHOUT-21": {
        "Key": "MAHOUT-21",
        "Summary": "Need reference implementation of Evolutionary Programming",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "31/Mar/08 00:09",
        "Updated": "21/May/11 03:27",
        "Resolved": "18/Nov/09 12:03",
        "Description": "I am about to write a sequential version of EP that could be used as a reference implementation.\nParallelization should be relatively straightforward since every member of the population is independently mutated and evaluated.  \nThe implementation will include the recorded step strategy for dense vector states, but will be easily extended to other state types and other strategies.",
        "Issue Links": []
    },
    "MAHOUT-22": {
        "Key": "MAHOUT-22",
        "Summary": "Several matrix exceptions are checked exceptions, but should be unchecked",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Ted Dunning",
        "Created": "31/Mar/08 00:19",
        "Updated": "21/Aug/08 15:40",
        "Resolved": "04/Apr/08 17:33",
        "Description": "IndexException and CardinalityException should be unchecked exceptions by analogy to ArrayIndexOutOfBoundsException\nHaving them be checked causes lots of try/catches in code that doesn't have a chance of fixing the problem.  It should be understood that these can happen in any matrix manipulation code just like IllegalArgumentException, ArithmeticException or ArrayIndexOutOfBoundsException can happen in any numerical codes.",
        "Issue Links": []
    },
    "MAHOUT-23": {
        "Key": "MAHOUT-23",
        "Summary": "Getting a row or column from a matrix view gives a row or column from the wrapped matrix.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Math",
        "Assignee": "Jeff Eastman",
        "Reporter": "Ted Dunning",
        "Created": "01/Apr/08 22:10",
        "Updated": "21/May/11 03:23",
        "Resolved": "18/Apr/08 19:33",
        "Description": "See summary.",
        "Issue Links": []
    },
    "MAHOUT-24": {
        "Key": "MAHOUT-24",
        "Summary": "Skeletal LWLR implementation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Samee Zahur",
        "Created": "04/Apr/08 06:42",
        "Updated": "21/May/11 03:27",
        "Resolved": "11/Dec/09 12:13",
        "Description": "This is a very skeletal but functional implementation for LWLR. It outputs n lines where n is the number of dimensions. ith line = sum(x[i]*x[ind]) where ind is the index of independant variable. So the actual gradient = 2nd line/1st line for the classical 2D.\nContains a single small test case for demonstration.",
        "Issue Links": []
    },
    "MAHOUT-25": {
        "Key": "MAHOUT-25",
        "Summary": "Minor bugs/issues from code inspection",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Sean R. Owen",
        "Created": "05/Apr/08 03:00",
        "Updated": "21/May/11 03:24",
        "Resolved": "19/Apr/08 00:55",
        "Description": "Hi all, just started checking out the code base to get familiar with it and turned loose IntelliJ on the code. It picked up a few possible issues I thought I'd highlight:\nMatrixView:57\n      for (int col = offset[COL]; col < offset[COL] + cardinality[COL]; row++)\nPretty sure that should be col++ at the end.\nDenseMatrix:122\nInstances are compared uisng == instead of equals(). Doesn't matter if this is exactly the semantics you want, but if DenseMatrix ever defined a notion of equals() this wouldn't use it and might be a bug. Same in many other classes.\nCanopy:146, 168\n      pointStronglyBound = pointStronglyBound | (dist < t2);\nShould this really be a non-shortcircuit, versus\n       pointStronglyBound = pointStronglyBound || (dist < t2);\nor just\n       if (!pointStronglyBound) \n{\n              pointStronglyBound = dist < t2;\n       }\n\n\nCanopyDriver:52,53\nString.valueOf is a smidge faster than \"\" + x.\nActually I've got several more but they're less important, like redundant casts or utility classes without private constructors, etc. I can keep going here... want to help polish a few things but don't want to get overbearing.",
        "Issue Links": []
    },
    "MAHOUT-26": {
        "Key": "MAHOUT-26",
        "Summary": "Matrix implementation bug fix and little addition",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Sergey Chickin",
        "Created": "06/Apr/08 13:19",
        "Updated": "21/Aug/08 15:40",
        "Resolved": "11/Apr/08 14:14",
        "Description": "Some bugfixes to matrix multiplication and implementation of recursive determinant calculation using Laplace theorem",
        "Issue Links": []
    },
    "MAHOUT-27": {
        "Key": "MAHOUT-27",
        "Summary": "Canopy/KMeans unit tests failing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "07/Apr/08 09:23",
        "Updated": "16/Sep/08 12:20",
        "Resolved": "07/Apr/08 10:36",
        "Description": "After checking out the project from svn and running \"ant compile test\" I noticed that three unit tests are failing:\n [junit] Test org.apache.mahout.clustering.canopy.TestCanopyCreation FAILED\n[junit] Testcase: testKMeansMRJob(org.apache.mahout.clustering.kmeans.TestKmeansClustering):\tFAILED\n[junit] Test org.apache.mahout.clustering.kmeans.TestKmeansClustering FAILED\nIsabel",
        "Issue Links": []
    },
    "MAHOUT-28": {
        "Key": "MAHOUT-28",
        "Summary": "EM implementation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "08/Apr/08 14:11",
        "Updated": "21/May/11 03:27",
        "Resolved": "18/Nov/09 11:57",
        "Description": "One quarter of the work that could be done for Mahout-4. EM seems a bit to general a concept to be implemented only for the use case of PLSI, so the task of this issue is to find a way to implement EM general enough to be reused. As EM conceptually is related to k-Means it might be possible to make the implementation general enough to include k-Means as a special case.",
        "Issue Links": [
            "/jira/browse/MAHOUT-4"
        ]
    },
    "MAHOUT-29": {
        "Key": "MAHOUT-29",
        "Summary": "Gibb's sampling implementation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "08/Apr/08 14:15",
        "Updated": "21/May/11 03:27",
        "Resolved": "18/Nov/09 12:03",
        "Description": "Copied over from original issue:\n> EM clustering can also be changed very slightly by assigning points to single clusters chosen at random according to the probability of membership. This\n> turns EM clustering into Gibb's sampling. The important property that is changed is that you now can sample over the distribution of possible samplings\n> which can be very important if some parts of your data are well defined and some parts not so well defined.",
        "Issue Links": [
            "/jira/browse/MAHOUT-4"
        ]
    },
    "MAHOUT-30": {
        "Key": "MAHOUT-30",
        "Summary": "dirichlet process implementation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "08/Apr/08 14:18",
        "Updated": "21/May/11 03:23",
        "Resolved": "16/Mar/09 00:19",
        "Description": "Copied over from original issue:\n> Further extension can also be made by assuming an infinite mixture model. The implementation is only slightly more difficult and the result is a (nearly)\n> non-parametric clustering algorithm.",
        "Issue Links": [
            "/jira/browse/MAHOUT-4"
        ]
    },
    "MAHOUT-31": {
        "Key": "MAHOUT-31",
        "Summary": "Implementation of PLSI that uses EM",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "08/Apr/08 14:22",
        "Updated": "21/May/11 03:27",
        "Resolved": "18/Nov/09 12:14",
        "Description": "This should implement the proposal in the original Google Paper on PLSI in news retrieval.",
        "Issue Links": [
            "/jira/browse/MAHOUT-4"
        ]
    },
    "MAHOUT-32": {
        "Key": "MAHOUT-32",
        "Summary": "Migrate logging statements away from System.out.println",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "08/Apr/08 14:33",
        "Updated": "21/May/11 03:23",
        "Resolved": "12/Nov/08 12:03",
        "Description": "Migrating the clustering code from Float[] over to the new matrix package I came across several System.out.println(...) statements. I think we should think about migrating these to a more general logging framework.",
        "Issue Links": []
    },
    "MAHOUT-33": {
        "Key": "MAHOUT-33",
        "Summary": "Matrix tests share code that can be placed in an abstract class",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "10/Apr/08 00:48",
        "Updated": "21/Aug/08 15:40",
        "Resolved": "11/Apr/08 14:00",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-34": {
        "Key": "MAHOUT-34",
        "Summary": "Iterator interface for Vectors",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Samee Zahur",
        "Created": "11/Apr/08 05:46",
        "Updated": "21/Aug/08 15:39",
        "Resolved": "12/Apr/08 16:44",
        "Description": "Implemented an Iterator interface for the Vector classes. Was necessary for porting from Float[] used in some parts of the code.",
        "Issue Links": [
            "/jira/browse/MAHOUT-20"
        ]
    },
    "MAHOUT-35": {
        "Key": "MAHOUT-35",
        "Summary": "Benchmark performance of Vector.iterator() when reusing Element instances.",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Math",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "12/Apr/08 16:47",
        "Updated": "21/May/11 03:24",
        "Resolved": "19/Nov/09 00:50",
        "Description": "Currently a new instance of Vector.Element is created for each call to iterator.next(). The JVM pools this but we can always help it out.\nSee MAHOUT-34 comments.",
        "Issue Links": []
    },
    "MAHOUT-36": {
        "Key": "MAHOUT-36",
        "Summary": "WeightedDistanceMeasure",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "13/Apr/08 18:54",
        "Updated": "21/Aug/08 15:39",
        "Resolved": "18/Apr/08 20:05",
        "Description": "Optionally allows passing weights to distance measure implementations.",
        "Issue Links": [
            "/jira/browse/MAHOUT-20"
        ]
    },
    "MAHOUT-37": {
        "Key": "MAHOUT-37",
        "Summary": "Tarball for Mahout-ified Taste code",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Sean R. Owen",
        "Created": "13/Apr/08 20:28",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/May/08 21:36",
        "Description": "I will attach to this issue a tarball containing my proposed contribution of the Taste project to Mahout. I think it's been scrubbed, repackaged, reformatted, cleaned up, and generally prepared for easy integration into Mahout. In particular I made many optional dependencies truly optional, so the core distro is quite lean.\nThe MD5 hash of the tarball is db78114f9799905c7d4a5a1907a03e6f",
        "Issue Links": []
    },
    "MAHOUT-38": {
        "Key": "MAHOUT-38",
        "Summary": "Ant job jar task",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "14/Apr/08 18:11",
        "Updated": "21/Aug/08 15:40",
        "Resolved": "16/Apr/08 23:29",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-39": {
        "Key": "MAHOUT-39",
        "Summary": "Vector improvments",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "14/Apr/08 18:16",
        "Updated": "21/Aug/08 15:39",
        "Resolved": "16/Apr/08 23:37",
        "Description": "Vector#fill(double)\nVector#fill(double, int, in)\nimplementations of copy() and like() don't return typed subclasses",
        "Issue Links": []
    },
    "MAHOUT-40": {
        "Key": "MAHOUT-40",
        "Summary": "Matrix factory that loads ARFFish data",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "14/Apr/08 18:22",
        "Updated": "21/May/11 03:27",
        "Resolved": "19/Nov/09 08:20",
        "Description": "% a data set\n@rows 2\n@columns 6\n@data\n0,0,1,2,3,A\n2,3,2,1,2,B",
        "Issue Links": []
    },
    "MAHOUT-41": {
        "Key": "MAHOUT-41",
        "Summary": "VectorWritable",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Math",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "14/Apr/08 18:27",
        "Updated": "21/Aug/08 15:39",
        "Resolved": "17/Apr/08 00:45",
        "Description": "Hadoop serialization",
        "Issue Links": []
    },
    "MAHOUT-42": {
        "Key": "MAHOUT-42",
        "Summary": "Tanimoto coefficient distance measure",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "14/Apr/08 18:33",
        "Updated": "25/Aug/08 12:30",
        "Resolved": "25/Aug/08 12:30",
        "Description": "http://en.wikipedia.org/wiki/Jaccard_index",
        "Issue Links": [
            "/jira/browse/MAHOUT-20"
        ]
    },
    "MAHOUT-43": {
        "Key": "MAHOUT-43",
        "Summary": "Synthetic Data Generation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Gabor Melli",
        "Created": "15/Apr/08 06:53",
        "Updated": "21/May/11 03:27",
        "Resolved": "19/Nov/09 08:19",
        "Description": "Requested by Karl Wettin on Apr 14, 2008 to open a new feature issue in order to transfer my intellectual property rights to ASF for my datgen.c program (which currently supports http://www.datasetgenerator.com).\n1) I am the owner of this code\n2) The code has the ASF2.0 header",
        "Issue Links": []
    },
    "MAHOUT-44": {
        "Key": "MAHOUT-44",
        "Summary": "Override zSum and dot for SparseVector",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Julien Nioche",
        "Created": "17/Apr/08 16:06",
        "Updated": "21/Apr/08 12:33",
        "Resolved": "20/Apr/08 15:16",
        "Description": "revision 649163\nThe patch attached overrides the zSum() and dot() methods in SparseVector. This is more efficient than relying on the methods from AbstractVector which iterate on all the indices of the Vectors.",
        "Issue Links": []
    },
    "MAHOUT-45": {
        "Key": "MAHOUT-45",
        "Summary": "Matrix QR decomposition",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Sergey Chickin",
        "Created": "18/Apr/08 17:18",
        "Updated": "21/May/11 03:27",
        "Resolved": "11/Dec/09 21:31",
        "Description": "Matrix QR decomposition and appropriate determinant calculator",
        "Issue Links": []
    },
    "MAHOUT-46": {
        "Key": "MAHOUT-46",
        "Summary": "Refactor AbstractMatrix.COL and ROW to an enumeration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Karl Wettin",
        "Created": "18/Apr/08 22:21",
        "Updated": "21/May/11 03:27",
        "Resolved": "19/Nov/09 08:19",
        "Description": "Cosmetic reasons only.\nAllows for importing the enumeration in any class and use ROW.intValue() instead of AbstractMatrix.ROW.\nAnother argument is that an enumeration could hold further meta data, but I don't see any use case.",
        "Issue Links": []
    },
    "MAHOUT-47": {
        "Key": "MAHOUT-47",
        "Summary": "Point class is now redundant and should be removed",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Jeff Eastman",
        "Created": "19/Apr/08 00:13",
        "Updated": "21/May/11 03:23",
        "Resolved": "23/Apr/08 21:38",
        "Description": "The Point class that was introduced for clustering when Float[] was used instead of Vectors is no longer necessary and should be removed. The formatting for Vectors can now replace Point and the other methods should be moved so that the class and its test can be eliminated.",
        "Issue Links": []
    },
    "MAHOUT-48": {
        "Key": "MAHOUT-48",
        "Summary": "isConverged() and converge flag OK?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Jeff Eastman",
        "Reporter": "Sean R. Owen",
        "Created": "19/Apr/08 01:03",
        "Updated": "21/May/11 03:24",
        "Resolved": "23/Apr/08 22:08",
        "Description": "Look at KMeansDriver.isConverged() \u2013 we have...\nboolean converged = true;\nwhile (converged && ...) {\n  converged = converged && ...;\n}\nconverged is never false in the loop so the right side's \"converged &&\" could be removed.\nI'm comparing to KMeansDriver.isConverged() which doesn't use \"converged &&\" in the while condition and wondering if it's worth a double-check that the logic is in fact different between the two. This came up during some more code inspection.\nAlso let me tack on this tiny observation:\nWeightedDistanceMeasure.configure(): weightsPathName is never null and doesn't need to be checked",
        "Issue Links": []
    },
    "MAHOUT-49": {
        "Key": "MAHOUT-49",
        "Summary": "ParameterEnumerable",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "19/Apr/08 01:30",
        "Updated": "21/Aug/08 15:41",
        "Resolved": "23/Apr/08 17:47",
        "Description": "A utility package used to \n\nconfigure class\ncreate default configuration files\nparse main method arguments\nproduce human readable help\ngetters and setters for value as object and string, for future generic reflection based GUI.",
        "Issue Links": []
    },
    "MAHOUT-50": {
        "Key": "MAHOUT-50",
        "Summary": "Vector extends Writable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Math",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "22/Apr/08 21:36",
        "Updated": "21/Aug/08 15:37",
        "Resolved": "23/Apr/08 17:47",
        "Description": "I'm removing VectorWritable and subclasses and make Vector extend Writable instead.",
        "Issue Links": []
    },
    "MAHOUT-51": {
        "Key": "MAHOUT-51",
        "Summary": "Upgrade to Hadoop 0.16.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "26/Apr/08 15:17",
        "Updated": "16/Sep/08 12:20",
        "Resolved": "28/Apr/08 19:08",
        "Description": "Summary says it all.",
        "Issue Links": []
    },
    "MAHOUT-52": {
        "Key": "MAHOUT-52",
        "Summary": "Standardize on java.util.logging, Commons Logging, log4j?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sean R. Owen",
        "Created": "10/May/08 19:39",
        "Updated": "21/May/11 03:24",
        "Resolved": "14/May/08 23:04",
        "Description": "I see the log4j and Commons Logging .jars in the lib/ directory. log4j isn't used; Commons Logging is used in one class (Parametered). My code just used java.util.logging directly.\nI figure we should standardize on one approach to logging. I personally think they're all just about the same; the only real best practice is using one system.\nI have always just used java.util.logging since it is built into Java 1.4+. Commons Logging offers an extra layer of abstraction and lets you switch between java.util.logging and log4j underneath. That's cool, but I've not found it compelling enough to want to add another layer and another .jar file.\nBut, I guess log4j is present because hadoop uses it directly? The .jar seems to have a dependency on it.\nIn that case maybe we are better off using Commons Logging to let us integrate with log4j logging that Hadoop uses, and leave open the possibility of other callers using java.util.logging underneath.\nIf that's cool I can switch my code to use Commons Logging.",
        "Issue Links": []
    },
    "MAHOUT-53": {
        "Key": "MAHOUT-53",
        "Summary": "Add documentation for Taste",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sean R. Owen",
        "Created": "10/May/08 19:45",
        "Updated": "21/May/11 03:23",
        "Resolved": "17/May/08 19:50",
        "Description": "I have 'ported' my documentation page at http://taste.sourceforge.net to the site/ subdirectory. Same info, updated package names, removed some obsolete info. I will attach a patch with the change. It's pretty useful info so I would like to preserve it in some form.\nIs it in the right format and place? I'd put it under \"Related Project\" but it's not a separate project. Should there be a subdirectory of Resources for documentation on individual components? this could be the first.\nI'm not sure if there's some other files I need to change or how to regen the site. Clue me in and I can take care of it.",
        "Issue Links": []
    },
    "MAHOUT-54": {
        "Key": "MAHOUT-54",
        "Summary": "parallelize k-means sharing the predominance of canopies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.1",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Min Zhou",
        "Created": "11/May/08 07:24",
        "Updated": "21/May/11 03:27",
        "Resolved": "11/Dec/09 12:14",
        "Description": "The implementation of mahout at present only using canopy algorithm creating initial cluster centroids for k-means.  It will calculate the distance from  each center to every point while iterating. But  the most import improvement of canopies is that needs only calculating the distance from each  center to a much smaller number of points which exists in the same canopy.",
        "Issue Links": []
    },
    "MAHOUT-55": {
        "Key": "MAHOUT-55",
        "Summary": "Update to Hadoop 0.16.4",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sean R. Owen",
        "Created": "11/May/08 22:16",
        "Updated": "21/May/11 03:24",
        "Resolved": "12/May/08 06:50",
        "Description": "I think I'm getting the message that we can make small and probably uncontroversial changes without creating a patch or mailing about it. Here I'll err on the side of making an issue just in case.\nHadoop 0.16.4 is out and I figure we should keep up. I'm using Hadoop a lot right now and so want to make sure I have the very latest.",
        "Issue Links": []
    },
    "MAHOUT-56": {
        "Key": "MAHOUT-56",
        "Summary": "Watchmaker Integration",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "19/May/08 05:41",
        "Updated": "21/May/11 03:24",
        "Resolved": "26/Aug/08 12:57",
        "Description": "The goal of this task is to allow watchmaker definded problems be solved in Mahout.",
        "Issue Links": []
    },
    "MAHOUT-57": {
        "Key": "MAHOUT-57",
        "Summary": "Mahout Project Logo",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Lukas Vlcek",
        "Created": "24/May/08 20:52",
        "Updated": "21/May/11 03:23",
        "Resolved": "27/May/08 13:23",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-58": {
        "Key": "MAHOUT-58",
        "Summary": "Remove deprecated distance(Float[], Float[]), AbstractDistanceMeasure?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sean R. Owen",
        "Created": "26/May/08 05:06",
        "Updated": "21/May/11 03:24",
        "Resolved": "27/May/08 15:23",
        "Description": "HI all, just looking at the deprecation warnings I am seeing when compiling. Looks like we've deprecated DistanceMeasure.distance(Float[], Float[]). Looks like it is easy to just remove this altogether, and with it, AbstractDistanceMeasure. OK to do?",
        "Issue Links": []
    },
    "MAHOUT-59": {
        "Key": "MAHOUT-59",
        "Summary": "Create some examples of clustering well-known datasets",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Jeff Eastman",
        "Created": "27/May/08 00:06",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/Dec/09 12:17",
        "Description": "The existing unit tests for clustering need to be augmented with examples from the literature which illustrate its correct operation on datasets which have known clusters present. See http://archive.ics.uci.edu/ml/ for some candidate datasets.",
        "Issue Links": []
    },
    "MAHOUT-60": {
        "Key": "MAHOUT-9 Implement MapReduce BayesianClassifier",
        "Summary": "Complementary Naive Bayes",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Classification",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Robin Anil",
        "Created": "31/May/08 22:28",
        "Updated": "21/May/11 03:24",
        "Resolved": "26/Aug/08 12:58",
        "Description": "The focus is to implement an improved text classifier based on this paper http://people.csail.mit.edu/jrennie/papers/icml03-nb.pdf.",
        "Issue Links": []
    },
    "MAHOUT-61": {
        "Key": "MAHOUT-61",
        "Summary": "Text problem matrix builder",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "21/Jun/08 00:25",
        "Updated": "21/May/11 03:27",
        "Resolved": "03/Jan/10 15:39",
        "Description": "A set of classes that builds matrices from text.\nCurrently the API consists of TokenMatrixBuilder and TokenInstanceBuilder. Should be thread safe.\nPostReader imports 20news-bydate. This takes several GB heap. It would be nice to bounce the data via JDBM or perhaps using the PersistentHashMap in MAHOUT-19.",
        "Issue Links": [
            "/jira/browse/MAHOUT-126"
        ]
    },
    "MAHOUT-62": {
        "Key": "MAHOUT-62",
        "Summary": "generate html test results",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "21/Jun/08 10:16",
        "Updated": "21/May/11 03:23",
        "Resolved": "26/Jun/08 01:00",
        "Description": "Added a junitreport to the test target, to allow the generation of HTML test results.",
        "Issue Links": []
    },
    "MAHOUT-63": {
        "Key": "MAHOUT-63",
        "Summary": "Self Organizing Map",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Farid Bourennani",
        "Created": "30/Jun/08 03:09",
        "Updated": "21/May/11 03:22",
        "Resolved": "27/May/09 17:51",
        "Description": "Implementation of  the Kohonen's Self organizing map algorithm.\nExecution: run the SOMViewer .\ntakes 300 iteration.\n\nThe algo is too slow because of:\n\tGUI: the current one is a temporary one, but should be replaced by \"prefused library\" as suggested by Ted.\n\tSelf-Organizing Maps: Batch Algorithm is faster than the sequentiel one that I am currently \tusing.\n\n\nDocumentation needs to be completed.",
        "Issue Links": []
    },
    "MAHOUT-64": {
        "Key": "MAHOUT-64",
        "Summary": "Self Organizing Map",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Farid Bourennani",
        "Created": "30/Jun/08 03:10",
        "Updated": "21/May/11 03:27",
        "Resolved": "09/Dec/09 12:11",
        "Description": "Implementation of  the Kohonen's Self organizing map algorithm.",
        "Issue Links": []
    },
    "MAHOUT-65": {
        "Key": "MAHOUT-65",
        "Summary": "Add Element Labels to Vectors and Matrices",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.2",
        "Component/s": "Math",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "06/Jul/08 18:52",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/Dec/09 12:14",
        "Description": "Many applications can benefit by accessing elements in vectors and matrices using String labels in addition to numeric indices. Investigate adding such a capability.",
        "Issue Links": [
            "/jira/browse/MAHOUT-126"
        ]
    },
    "MAHOUT-66": {
        "Key": "MAHOUT-66",
        "Summary": "EuclideanDistanceMeasure and ManhattanDistanceMeasure classes are not optimized for Sparse Vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Pallavi Palleti",
        "Created": "07/Jul/08 09:10",
        "Updated": "21/May/11 03:23",
        "Resolved": "09/Dec/09 12:25",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-67": {
        "Key": "MAHOUT-67",
        "Summary": "plus method and divide method in AbstractVector can be optimized for SparseVectors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Pallavi Palleti",
        "Created": "08/Jul/08 09:51",
        "Updated": "21/May/11 03:24",
        "Resolved": "06/Dec/09 16:15",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-68": {
        "Key": "MAHOUT-68",
        "Summary": "addPoint, computeCentroid can use vector operators to do the respective task",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Pallavi Palleti",
        "Created": "09/Jul/08 09:53",
        "Updated": "21/May/11 03:22",
        "Resolved": "06/Aug/08 20:24",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-73"
        ]
    },
    "MAHOUT-69": {
        "Key": "MAHOUT-69",
        "Summary": "0.1 RELEASE TODO",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "12/Jul/08 02:33",
        "Updated": "21/May/11 03:24",
        "Resolved": "31/Jan/09 14:24",
        "Description": "We need to figure out all the things that need to be done for a 0.1 release.\nThese include, but are not limited to:\n\nLegal issues (NOTICES.txt, license files, etc.)\nDocumentation\nWiki page on making releases (See Lucene or Solr wiki page on said topic)\nRelease Managers should setup signatures\nRemaining issues to be resolved for 0.1.  We should go through the issues and make decisions by setting the fix version to be 0.1\n\nOthers?  I've personally never done an Apache release before, so we will want to take our time and make sure things are right.",
        "Issue Links": []
    },
    "MAHOUT-70": {
        "Key": "MAHOUT-70",
        "Summary": "no way to pass in weight filename to WeightedDistanceMeasure",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.1",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "peter dapkus",
        "Created": "17/Jul/08 19:44",
        "Updated": "21/May/11 03:27",
        "Resolved": "03/Jan/10 15:39",
        "Description": "I might be missing something, but it doesn't seem that there's a way to pass in the weights file for a weighted distance measure without modifying one of the mahout classes (e.g. CanopyDriver, ClusterDriver, CanopyClusteringJob).    Seems like the runJob methods should have an option to include one, or that maybe the distance measure should be passed as something other than just a string.",
        "Issue Links": []
    },
    "MAHOUT-71": {
        "Key": "MAHOUT-71",
        "Summary": "Dataset to Matrix Reader",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "01/Aug/08 09:18",
        "Updated": "21/May/11 03:27",
        "Resolved": "03/Jan/10 15:40",
        "Description": "This component should allow the input datasets to be read as Matrix Rows.\nA Map-Reduce Algorithm should handle any dataset in a matrix format, where the collumns are the attributes (and one of them is the Label) and the rows are the datas.\nWorking with Hadoop, we'll need to pass the dataset in the mapper's input, so it must be a file (or many files). We'll then need a custom InputFormat to feed the mappers with the data, and here comes the lovely-named \"row-wise splitting matrix input format\".\nNow we want to be able to work with any given dataset file format (including the ARFF and my custom format), and thus the InputFormat needs a decoder that converts the dataset lines into matrix rows.",
        "Issue Links": []
    },
    "MAHOUT-72": {
        "Key": "MAHOUT-72",
        "Summary": "Separate out Examples from Core",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Karl Wettin",
        "Reporter": "Grant Ingersoll",
        "Created": "05/Aug/08 12:54",
        "Updated": "21/May/11 03:24",
        "Resolved": "26/Aug/08 13:01",
        "Description": "Let's make the Examples standalone as a separate module w/ a separate build, as they aren't \"core\".",
        "Issue Links": []
    },
    "MAHOUT-73": {
        "Key": "MAHOUT-73",
        "Summary": "addPoint, computeCentroid can be optimized by using vector operators",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Pallavi Palleti",
        "Created": "07/Aug/08 04:23",
        "Updated": "21/May/11 03:27",
        "Resolved": "24/Jun/09 05:10",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-68",
            "/jira/browse/MAHOUT-77"
        ]
    },
    "MAHOUT-74": {
        "Key": "MAHOUT-74",
        "Summary": "Fuzzy K-Means clustering",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Pallavi Palleti",
        "Created": "11/Aug/08 09:34",
        "Updated": "21/May/11 03:24",
        "Resolved": "26/Aug/08 12:57",
        "Description": "Fuzzy KMeans clustering algorithm is an extension to traditional K Means clustering algorithm and performs soft clustering.\nMore details about fuzzy k-means can be found here :http://en.wikipedia.org/wiki/Data_clustering#Fuzzy_c-means_clustering\nI have implemented fuzzy K-Means prototype and tests in org.apache.mahout.clustering.fuzzykmeans",
        "Issue Links": []
    },
    "MAHOUT-75": {
        "Key": "MAHOUT-75",
        "Summary": "asFormatString tests fail",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Math",
        "Assignee": "Karl Wettin",
        "Reporter": "Karl Wettin",
        "Created": "20/Aug/08 21:14",
        "Updated": "21/Aug/08 14:24",
        "Resolved": "21/Aug/08 14:24",
        "Description": "See http://www.nabble.com/asFormatString-tests-fail-tc18878046.html\nThis also affects certain Canopy tests.",
        "Issue Links": []
    },
    "MAHOUT-76": {
        "Key": "MAHOUT-76",
        "Summary": "Singular Value Decomposition for SparseMatrix / DenseMatrix",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Karl Wettin",
        "Reporter": "Allen Day",
        "Created": "21/Aug/08 05:46",
        "Updated": "21/May/11 03:27",
        "Resolved": "09/Dec/09 12:26",
        "Description": "Adding a new class and test harness for a SVD implementation derived from JAMA",
        "Issue Links": [
            "/jira/browse/MAHOUT-180"
        ]
    },
    "MAHOUT-77": {
        "Key": "MAHOUT-77",
        "Summary": "DistanceMeasure calculation slow for SparseVector",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Allen Day",
        "Created": "19/Sep/08 18:33",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "24/Jun/09 05:00",
        "Description": "ManhattanDistanceMeasure and TanimotoDistanceMeasure assume all vector indices up to cardinality() must be compared.  We can speed this up for SparseVectors (and others) because Vector implements Iterable, so we can consider only non-zero indices.",
        "Issue Links": [
            "/jira/browse/MAHOUT-73",
            "/jira/browse/MAHOUT-139",
            "/jira/browse/MAHOUT-121"
        ]
    },
    "MAHOUT-78": {
        "Key": "MAHOUT-78",
        "Summary": "HBase RowResult/BatchUpdate access via Mahout Vector interface",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Allen Day",
        "Created": "19/Sep/08 18:43",
        "Updated": "21/May/11 03:27",
        "Resolved": "03/Jan/10 15:40",
        "Description": "An adapter class is attached that allows read/write operations on HBase rows using the Vector interface.  This allows, e.g. canopy clustering of rows in an HBase table.",
        "Issue Links": []
    },
    "MAHOUT-79": {
        "Key": "MAHOUT-79",
        "Summary": "Improving the speed of Fuzzy K-Means by optimizing data transfer between map and reduce tasks",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Pallavi Palleti",
        "Created": "01/Oct/08 04:34",
        "Updated": "21/May/11 03:23",
        "Resolved": "07/Dec/08 01:09",
        "Description": "Improve the speed of fuzzy k-Means by passing only the cluster-id info as key output of mapper task and reading the cluster information in reducer task where this info is needed.",
        "Issue Links": [
            "/jira/browse/MAHOUT-99"
        ]
    },
    "MAHOUT-80": {
        "Key": "MAHOUT-80",
        "Summary": "Taste build fix",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Otis Gospodnetic",
        "Created": "10/Oct/08 19:58",
        "Updated": "21/May/11 03:23",
        "Resolved": "11/Oct/08 15:35",
        "Description": "I had to make changes to taste-build.xml to be able to build Taste.  Patch attached.",
        "Issue Links": []
    },
    "MAHOUT-81": {
        "Key": "MAHOUT-81",
        "Summary": "Add Taste to Mahout Components in JIRA",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Otis Gospodnetic",
        "Created": "10/Oct/08 20:33",
        "Updated": "21/May/11 03:24",
        "Resolved": "10/Oct/08 21:41",
        "Description": "We've got Classification, Clustering, GA, Matrix.... but not Taste (or CF)",
        "Issue Links": []
    },
    "MAHOUT-82": {
        "Key": "MAHOUT-82",
        "Summary": "Canopy map intermediate file structure should be keyed by canopyId.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Edward J. Yoon",
        "Created": "14/Oct/08 08:06",
        "Updated": "21/May/11 03:24",
        "Resolved": "17/Oct/08 18:42",
        "Description": "When emit the point to the collector, it should be keyed by canopyId w/o computed centroid. (or make a other key datum instead of hadoop.IO.Text)",
        "Issue Links": []
    },
    "MAHOUT-83": {
        "Key": "MAHOUT-83",
        "Summary": "Mahout/Hama Integration",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Edward J. Yoon",
        "Created": "14/Oct/08 08:10",
        "Updated": "21/May/11 03:27",
        "Resolved": "09/Dec/09 12:13",
        "Description": "I'd like to suggest to integrate hama into mahout project. Hama is a matrix computational package based on Hadoop.",
        "Issue Links": []
    },
    "MAHOUT-84": {
        "Key": "MAHOUT-84",
        "Summary": "Add hama site link to mahout related project",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Edward J. Yoon",
        "Created": "14/Oct/08 08:14",
        "Updated": "21/May/11 03:24",
        "Resolved": "15/Oct/08 13:28",
        "Description": "The Hama team which is develop a parallel matrix computational package based on hadoop would like to add our site link to mahout related project.",
        "Issue Links": []
    },
    "MAHOUT-85": {
        "Key": "MAHOUT-85",
        "Summary": "Perceptron/Winnow Trainer",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "16/Oct/08 11:40",
        "Updated": "21/May/11 03:23",
        "Resolved": "10/Jan/10 11:16",
        "Description": "Please find attached a first sketch for perceptron and winnow training. Please look very, very carefully at the patch, as I added the heart of the algorithms in the emergency room at Charite Berlin (after I broke my leg when cycling to the Hadoop Get Together  ). \nThe patch does not yet feature unit tests nor is it parallelised. Currently my plan is to set up an example with the webKb dataset, add unit tests to the code and after that go parallel. I would like to get some feedback early on, in addition I would feel a lot better, if a second and third pair of eyes had a look at the code to make sure all obvious mistakes are out as early as possible.",
        "Issue Links": []
    },
    "MAHOUT-86": {
        "Key": "MAHOUT-86",
        "Summary": "A New Vector Assignment Operator",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Math",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "17/Oct/08 18:38",
        "Updated": "21/May/11 03:24",
        "Resolved": "17/Oct/08 18:45",
        "Description": "The ability to assign a computed value to a Vector using a BinaryFunction and a constant argument would be useful for the Dirichlet Process Clustering algorithm under investigation. Specifically:\ninterface Vector {\n...\npublic Vector assign(BinaryFunction f, double v);\n...\n}\nand a new public class TimesFunction implements BinaryFunction too.",
        "Issue Links": []
    },
    "MAHOUT-87": {
        "Key": "MAHOUT-87",
        "Summary": "Upgrade to Hadoop 0.18.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "17/Oct/08 20:30",
        "Updated": "17/Oct/08 20:33",
        "Resolved": "17/Oct/08 20:32",
        "Description": "As the Summary says...",
        "Issue Links": []
    },
    "MAHOUT-88": {
        "Key": "MAHOUT-88",
        "Summary": "Convert to doubles, other changes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "21/Oct/08 14:35",
        "Updated": "21/May/11 03:24",
        "Resolved": "24/Oct/08 16:02",
        "Description": "This is the large patch I promised/threatened on the mahout-dev mailing list.",
        "Issue Links": []
    },
    "MAHOUT-89": {
        "Key": "MAHOUT-89",
        "Summary": "To test hudson, I just made a 'Patch Available' issue.",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Edward J. Yoon",
        "Created": "22/Oct/08 01:20",
        "Updated": "21/May/11 03:23",
        "Resolved": "23/Oct/08 02:18",
        "Description": "To test hudson, I just made a 'Patch Available' issue.",
        "Issue Links": []
    },
    "MAHOUT-90": {
        "Key": "MAHOUT-90",
        "Summary": "Adding all scripts (for nightly build) to SVN repository.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Edward J. Yoon",
        "Created": "23/Oct/08 01:19",
        "Updated": "21/May/11 03:27",
        "Resolved": "07/Dec/09 08:00",
        "Description": "I made below scripts for the hudson continuous integration service on my hudson account. \nmahout/hudsonBuildMahoutPatch.sh   \nmahout/processMahoutPatchEmail.sh\nmahout/hudsonPatchQueueAdmin.sh\nThey will be modified by only me, so It should be handled via SVN.",
        "Issue Links": []
    },
    "MAHOUT-91": {
        "Key": "MAHOUT-91",
        "Summary": "Wikipedia Example has incorrect input Key",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Classification",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "31/Oct/08 15:27",
        "Updated": "21/May/11 03:24",
        "Resolved": "31/Oct/08 17:31",
        "Description": "Running the WikipediaDataSetCreator \n\n bin/hadoop jar ~/projects/lucene/mahout/mahout-clean/examples/build/ org.apache.mahout.examples.classifiers.cbayes.WikipediaDatasetCreator -i wikipediadump -o wikipediainput -c ~/projects/lucene/mahout/mahout-clean/examples/src/test/resources/country.txt \n\n\nyielded:\n08/10/31 11:15:26 INFO mapred.JobClient: Task Id : attempt_200810301619_0001_m_000000_0, Status : FAILED\njava.lang.ClassCastException: org.apache.hadoop.io.LongWritable cannot be cast to org.apache.hadoop.io.Text\n        at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorMapper.map(WikipediaDatasetCreatorMapper.java:41)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:47)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:227)\n        at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:2207)\nThe fix is:\n\nIndex: src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorMapper.java\n===================================================================\n--- src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorMapper.java (revision 709230)\n+++ src/main/java/org/apache/mahout/classifier/bayes/WikipediaDatasetCreatorMapper.java (working copy)\n@@ -20,6 +20,7 @@\n import org.apache.commons.lang.StringEscapeUtils;\n import org.apache.hadoop.io.DefaultStringifier;\n import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.LongWritable;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapred.MapReduceBase;\n import org.apache.hadoop.mapred.Mapper;\n@@ -39,11 +40,11 @@\n import java.util.Set;\n \n public class WikipediaDatasetCreatorMapper extends MapReduceBase implements\n-    Mapper<Text, Text, Text, Text> {\n+    Mapper<LongWritable, Text, Text, Text> {\n \n   private static Set<String> countries = null;\n   \n-  public void map(Text key, Text value,\n+  public void map(LongWritable key, Text value,\n       OutputCollector<Text, Text> output, Reporter reporter)\n       throws IOException {\n     String document = value.toString();",
        "Issue Links": []
    },
    "MAHOUT-92": {
        "Key": "MAHOUT-92",
        "Summary": "BayesFeatureMapper doesn't properly extract features",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "31/Oct/08 22:11",
        "Updated": "21/May/11 03:23",
        "Resolved": "08/Nov/08 00:01",
        "Description": "The BayesFeatureMapper currently has a bunch of unused variables and doesn't actually do anything.  The problem is it is not using the input value to generate a set of n-grams, from which it can then generate tf-idf information.",
        "Issue Links": []
    },
    "MAHOUT-93": {
        "Key": "MAHOUT-93",
        "Summary": "Refactor Bayes and CBayes to share more common code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Grant Ingersoll",
        "Created": "01/Nov/08 19:56",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "28/Sep/09 18:07",
        "Description": "There's no reason to have two classes w/ the exact same code: BayesClassifier and CBayesClassifier.\nAlso, there appears to be a fair amount of copied code between the two implementations.  Let's refactor so that they share common code.",
        "Issue Links": []
    },
    "MAHOUT-94": {
        "Key": "MAHOUT-94",
        "Summary": "Make the Taste Demo more automated.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "01/Nov/08 20:58",
        "Updated": "21/May/11 03:27",
        "Resolved": "03/Apr/09 13:55",
        "Description": "Would be really cool if the Taste Demo (http://lucene.apache.org/mahout/taste.html#demo) was easier to get up and going.  For instance, we could have an Ant task that automatically gets the data and puts it into the work directory just like we do for Reuters, Wikipedia and Twenty News.  Then, we could also ship Jetty w/ the examples, such that one just needs to do \n\njava -jar start.jar\n\n to fire up the WAR and have it running (Solr does this)",
        "Issue Links": [
            "/jira/browse/MAHOUT-110"
        ]
    },
    "MAHOUT-95": {
        "Key": "MAHOUT-95",
        "Summary": "UserSimilarity-based NearestNNeighborhood",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Otis Gospodnetic",
        "Reporter": "Otis Gospodnetic",
        "Created": "07/Nov/08 18:04",
        "Updated": "21/May/11 03:23",
        "Resolved": "29/Jan/09 19:38",
        "Description": "A variation of NearestNUserNeighborhood.  This version adds the minSimilarity parameter, which is the primary factor for including/excluding other users from the target user's neighbourhood.  Additionally, the 'n' parameter was renamed to maxHoodSize and is used to optionally limit the size of the neighbourhood.\nThe patch is for a brand new class, but we may really want just a single class (either keep this one and axe NearestNUserNeighborhood or add this functionality to NearestNUserNeighborhood), if this sounds good.\nI'll update the unit test and provide a patch for that if others think this can go in.\nThoughts?",
        "Issue Links": []
    },
    "MAHOUT-96": {
        "Key": "MAHOUT-96",
        "Summary": "grouplens-example erroneous path to .dat files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Jens Grivolla",
        "Created": "11/Nov/08 17:31",
        "Updated": "21/May/11 03:23",
        "Resolved": "12/Nov/08 10:54",
        "Description": "GroupLensDataModel.java tries to read from \"src/example/org/apache/mahout/cf/taste/example/grouplens/ratings.dat\" whereas the file is included in taste-build.xml from \"${grouplens-location}/ratings.dat\".\nThe location is hard-coded in GroupLensDataModel.java and would need to be changed to conform to the current code organization.  A better solution would be not to hard-code the path.",
        "Issue Links": []
    },
    "MAHOUT-97": {
        "Key": "MAHOUT-97",
        "Summary": "taste build-server does not include all necessary jars",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jens Grivolla",
        "Created": "11/Nov/08 17:38",
        "Updated": "21/May/11 03:23",
        "Resolved": "12/Nov/08 10:52",
        "Description": "The build-server target in taste-build.xml does not include all needed jar files, e.g.  slf4j-api-1.5.2.jar.\nAdding <lib dir=\"./lib\"/> to the <war> section fixes this apparently, but obviously includes too much.  It would be good to identify the actual dependencies and include them in the build.",
        "Issue Links": []
    },
    "MAHOUT-98": {
        "Key": "MAHOUT-98",
        "Summary": "taste-build references wrong version of hadoop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Jens Grivolla",
        "Created": "11/Nov/08 17:43",
        "Updated": "21/May/11 03:24",
        "Resolved": "12/Nov/08 10:55",
        "Description": "should be <property name=\"hadoop.jar\" location=\"lib/hadoop-0.18.1-core.jar\"/> (currently it points to hadoop-0.18.0)",
        "Issue Links": []
    },
    "MAHOUT-99": {
        "Key": "MAHOUT-99",
        "Summary": "Improving speed of KMeans",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Pallavi Palleti",
        "Created": "28/Nov/08 12:56",
        "Updated": "21/May/11 03:24",
        "Resolved": "19/Mar/09 12:55",
        "Description": "Improved the speed of KMeans by passing only cluster ID from mapper to reducer. Previously, whole Cluster Info as formatted s`tring was being sent.\nAlso removed the implicit assumption of Combiner runs only once approach and the code is modified accordingly so that it won't create a bug when combiner runs zero or more than once.",
        "Issue Links": [
            "/jira/browse/MAHOUT-79"
        ]
    },
    "MAHOUT-100": {
        "Key": "MAHOUT-100",
        "Summary": "MySQLJDBCDiffStorage should use preferenceTable, userIDColumn, etc. from MySQLJDBCDataModel",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jens Grivolla",
        "Created": "03/Dec/08 15:02",
        "Updated": "21/May/11 03:24",
        "Resolved": "03/Dec/08 18:01",
        "Description": "The constructor of MySQLJDBCDiffStorage should use the appropriate values from the corresponding DataModel, instead of the defaults from AbstractJDBCDataModel.\nProposed fix:\nin MySQLJDBCDataModel.java (line numbers may be off):\n80a82,86\n>   private String preferenceTable;\n>   private String userIDColumn;\n>   private String itemIDColumn;\n>   private String preferenceColumn;\n> \n163a170,189\n>     this.preferenceTable = preferenceTable;\n>     this.userIDColumn = userIDColumn;\n>     this.itemIDColumn = itemIDColumn;\n>     this.preferenceColumn = preferenceColumn;\n>   }\n> \n>   public String getPreferenceTable() \n{\n>     return preferenceTable;\n>   }\n> \n>   public String getUserIDColumn() \n{\n>     return userIDColumn;\n>   }\n> \n>   public String getItemIDColumn() \n{\n>     return itemIDColumn;\n>   }\n> \n>   public String getPreferenceColumn() {\n>     return preferenceColumn;\nin MySQLJDBCDiffStorage.java:\n61,64c61,64\n<          AbstractJDBCDataModel.DEFAULT_PREFERENCE_TABLE,\n<          AbstractJDBCDataModel.DEFAULT_USER_ID_COLUMN,\n<          AbstractJDBCDataModel.DEFAULT_ITEM_ID_COLUMN,\n<          AbstractJDBCDataModel.DEFAULT_PREFERENCE_COLUMN,\n\u2014\n>          dataModel.getPreferenceTable(),\n>          dataModel.getUserIDColumn(),\n>          dataModel.getItemIDColumn(),\n>          dataModel.getPreferenceColumn(),",
        "Issue Links": []
    },
    "MAHOUT-101": {
        "Key": "MAHOUT-101",
        "Summary": "MySQLJDBCDiffStorage.java: hard coded column names",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jens Grivolla",
        "Created": "03/Dec/08 15:08",
        "Updated": "21/May/11 03:24",
        "Resolved": "03/Dec/08 18:07",
        "Description": "Some column names in MySQLJDBCDiffStorage.java are hard coded instead of using the values provided in the constructor.\nproposed fix:\nin MySQLJDBCDiffStorage.java:\n116c117\n<           \" WHERE \" + itemIDBColumn + \" = item_id AND \" + userIDColumn + \"=? UNION DISTINCT\" +\n\u2014\n>           \" WHERE \" + itemIDBColumn + \" = \" + itemIDColumn + \" AND \" + userIDColumn + \"=? UNION DISTINCT\" +\n118c119\n<           \" WHERE \" + itemIDAColumn + \" = item_id AND \" + userIDColumn +\n\u2014\n>           \" WHERE \" + itemIDAColumn + \" = \" + itemIDColumn + \" AND \" + userIDColumn +",
        "Issue Links": []
    },
    "MAHOUT-102": {
        "Key": "MAHOUT-102",
        "Summary": "Use Watchmaker 0.5.0 instead of 0.4.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "16/Jan/09 09:00",
        "Updated": "21/May/11 03:24",
        "Resolved": "31/Jan/09 14:22",
        "Description": "Watchmaker 0.5.0 is available.\nThe biggest change is the availability of the SequentialEvolutionEngine, so we no more need our own implementation of a Single Threaded Evolution Engine (STEvolutionEngine).\nThere also other small changes that affect our own code, and should be updated.",
        "Issue Links": []
    },
    "MAHOUT-103": {
        "Key": "MAHOUT-103",
        "Summary": "Co-occurence based nearest neighbourhood",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Ankur Bansal",
        "Reporter": "Ankur Bansal",
        "Created": "21/Jan/09 07:53",
        "Updated": "21/May/11 03:24",
        "Resolved": "23/Dec/09 19:51",
        "Description": "Nearest neighborhood type queries for users/items can be answered efficiently and effectively by analyzing the co-occurrence model of a user/item w.r.t another. This patch aims at providing an implementation for answering such queries based upon simple co-occurrence counts.",
        "Issue Links": []
    },
    "MAHOUT-104": {
        "Key": "MAHOUT-104",
        "Summary": "Move to Maven for Build, drop Ant",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "28/Jan/09 13:22",
        "Updated": "21/May/11 03:24",
        "Resolved": "10/Feb/09 20:08",
        "Description": "Per the mailing list discussion: http://www.lucidimagination.com/search/document/b67e658cbc418c37/inactivity, let's migrate to Maven.",
        "Issue Links": []
    },
    "MAHOUT-105": {
        "Key": "MAHOUT-105",
        "Summary": "Reconceive refresh / cache update mechanism in CF code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.1",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "31/Jan/09 14:29",
        "Updated": "21/May/11 03:27",
        "Resolved": "18/Nov/09 13:52",
        "Description": "From a conversation with Otis \u2013 our conclusion was that the \"Refreshable\" interface and cache update mechanism is effectively backwards, and that updates should be triggered from DataModel upwards via a listener mechanism. It's a large enough change that I'm writing it down to think about and work on \"later\".",
        "Issue Links": []
    },
    "MAHOUT-106": {
        "Key": "MAHOUT-106",
        "Summary": "PLSI/EM in pig based on hofmann's ACM 04 paper.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Prasen Mukherjee",
        "Created": "11/Feb/09 12:27",
        "Updated": "21/May/11 03:27",
        "Resolved": "27/May/10 08:13",
        "Description": "Based on the following paper by hofmann : T. Hofmann Latent Semantic Models for Collaborative Filtering In ACM Transactions on Information Systems, 2004,  vol 22(1), pp. 89-115.",
        "Issue Links": []
    },
    "MAHOUT-107": {
        "Key": "MAHOUT-107",
        "Summary": "make MySQLJDBCDataModel not final",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Jens Grivolla",
        "Created": "18/Feb/09 15:36",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "23/Feb/09 20:37",
        "Description": "Is there a particular reason for MySQLJDBCDataModel being final?\nI am using a derived class to incorporate additional information about the items and users (overriding buildItem etc.) and had to change my version of MySQLJDBCDataModel to be able to extend it.",
        "Issue Links": []
    },
    "MAHOUT-108": {
        "Key": "MAHOUT-108",
        "Summary": "Implementation of Assoication Rules learning by Apriori algorithm",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "chao deng",
        "Created": "05/Mar/09 06:17",
        "Updated": "28/Mar/11 14:45",
        "Resolved": "14/Sep/09 12:57",
        "Description": "Target: Association Rules learning is a popular method for discovering interesting relations between variables in large databases. Here, we would implement the Apriori algorithm using Hadoop&Mapreduce parallel techniques.\nApplications: Typically, association rules  learning is used to discover regularities between products in large scale transaction data in supermarkets. For example, the rule  \"\n{onions, patatoes}\n->beef\" found in the sales data would indicate that if a customer buys onions and potatoes together, he or she is likely to also buy beef. Such information can be used as the basis for decisions about marketing activities. In addition to the market basket analysis, association rules are employed today in many application areas including Web usage mining, intrusion detection and bioinformatics.\nApriori algorithm: Apriori is the best-known algorithm to mine association rules. It uses a breadth-first search strategy to counting the support of itemsets and uses a candidate generation function which exploits the downward closure property of support",
        "Issue Links": []
    },
    "MAHOUT-109": {
        "Key": "MAHOUT-109",
        "Summary": "Implementation of Cosine distance measure, plus unit test.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Richard Tomsett",
        "Created": "08/Mar/09 21:54",
        "Updated": "21/May/11 03:24",
        "Resolved": "13/May/09 20:44",
        "Description": "This is a class implementing a cosine distance measure. In various places I've seen cosine similarity defined as being the cosine of the angle between vectors - cos(a,b) - and cosine distance being (1 - cos(a,b)), so in keeping with the other distance measures, this returns 1-cos(angle) as the distance.\nMade a new test class rather than using the default distance measure check as the vectors in the current default test class all have a cosine distance of zero between them ([1,1,1,1,1,1], [3,3,3,3,3,3] and [6,6,6,6,6,6]). The test checks the cosine distances between [1,0,0,0,0,0], [1,1,1,0,0,0] and [1,1,1,1,1,1].",
        "Issue Links": []
    },
    "MAHOUT-110": {
        "Key": "MAHOUT-110",
        "Summary": "Ant script for building Taste web app",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "17/Mar/09 13:02",
        "Updated": "21/May/11 03:24",
        "Resolved": "18/Mar/09 15:00",
        "Description": "WIll attach patch after creating. This is a follow-up from a thread on mahout-dev.",
        "Issue Links": [
            "/jira/browse/MAHOUT-94"
        ]
    },
    "MAHOUT-111": {
        "Key": "MAHOUT-111",
        "Summary": "Redirect Test output to file",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "18/Mar/09 10:40",
        "Updated": "21/May/11 03:24",
        "Resolved": "18/Mar/09 12:02",
        "Description": "The tests are really verbose to std out.  Have them direct their output to a file and only report pass/fail on std out.  This should be a simple setting on the test plugin.",
        "Issue Links": []
    },
    "MAHOUT-112": {
        "Key": "MAHOUT-112",
        "Summary": "Maven jetty plugin has been relocated",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.1",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jukka Zitting",
        "Created": "30/Mar/09 16:42",
        "Updated": "21/May/11 03:24",
        "Resolved": "07/Apr/09 04:56",
        "Description": "Seen when building taste-web in the 0.1 release candidate and in the current Mahout trunk:\n\nDownloading:\nhttp://repo1.maven.org/maven2/org/mortbay/jetty/maven-jetty-plugin/7.0.0.pre5/maven-jetty-plugin-7.0.0.pre5.jar\n[INFO] ------------------------------------------------------------------------\n[ERROR] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] A required plugin was not found: Plugin could not be found - check that the goal name is correct:\nUnable to download the artifact from any repository\n\n\nThe Jetty plugin has been relocated from maven-jetty-plugin to jetty-maven-plugin.\nThe following change solves the issue:\n\nIndex: taste-web/pom.xml\n===================================================================\n--- taste-web/pom.xml\t(Revision 760032)\n+++ taste-web/pom.xml\t(Arbeitskopie)\n@@ -82,7 +82,7 @@\n \n       <plugin>\n         <groupId>org.mortbay.jetty</groupId>\n-        <artifactId>maven-jetty-plugin</artifactId>\n+        <artifactId>jetty-maven-plugin</artifactId>\n         <configuration>\n           <webApp>${project.build.directory}/${project.artifactId}-${project.version}.war</webApp>\n         </configuration>",
        "Issue Links": []
    },
    "MAHOUT-113": {
        "Key": "MAHOUT-113",
        "Summary": "CDInfosToolTest.testGatherInfos failure in Mahout examples",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Jukka Zitting",
        "Created": "30/Mar/09 16:47",
        "Updated": "18/Nov/09 14:06",
        "Resolved": "11/Oct/09 08:00",
        "Description": "I'm getting the following test failure when running \"mvn clean install\" on a fresh checkout of Mahout trunk:\n\n-------------------------------------------------------------------------------\nTest set: org.apache.mahout.ga.watchmaker.cd.tool.CDInfosToolTest\n-------------------------------------------------------------------------------\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.828 sec <<< FAILURE!\ntestGatherInfos(org.apache.mahout.ga.watchmaker.cd.tool.CDInfosToolTest)  Time elapsed: 1.798 sec  <<< FAILURE!\njunit.framework.AssertionFailedError: expected:<48> but was:<46>\n        at junit.framework.Assert.fail(Assert.java:47)\n        at junit.framework.Assert.failNotEquals(Assert.java:280)\n        at junit.framework.Assert.assertEquals(Assert.java:64)\n        at junit.framework.Assert.assertEquals(Assert.java:198)\n        at junit.framework.Assert.assertEquals(Assert.java:204)\n        at org.apache.mahout.ga.watchmaker.cd.tool.CDInfosToolTest.testGatherInfos(CDInfosToolTest.java:207)\n\n\nI'll attach the test output file.",
        "Issue Links": []
    },
    "MAHOUT-114": {
        "Key": "MAHOUT-114",
        "Summary": "Release Process Needs to sign published dependencies such as Hadoop, etc.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "07/Apr/09 02:11",
        "Updated": "18/Nov/09 14:06",
        "Resolved": "19/Oct/09 23:49",
        "Description": "When releasing, we need the hadoop, uncommons, etc. jars/poms to be signed by the release process.",
        "Issue Links": []
    },
    "MAHOUT-115": {
        "Key": "MAHOUT-115",
        "Summary": "Interpolated Knn and SVD Recommender",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Andre Panisson",
        "Created": "10/Apr/09 21:00",
        "Updated": "16/Feb/10 16:55",
        "Resolved": "12/Apr/09 14:58",
        "Description": "Item based recommender implementation that uses K-nearest neighbors with interpolated weights, based on the paper of Robert M. Bell and Yehuda Koren\nin ICDM '07.\nRecommender implementation that uses Single Value Decomposition to capture the features of a DataSet.",
        "Issue Links": []
    },
    "MAHOUT-116": {
        "Key": "MAHOUT-116",
        "Summary": "Decode matrix methods",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Daniel Nee",
        "Created": "14/Apr/09 19:21",
        "Updated": "21/May/11 03:27",
        "Resolved": "03/Jan/10 15:46",
        "Description": "Currently in the matrix package we have asFormatString() methods to serialize matrices, however there are no corresponding methods to decode the serialized matrices. At the moment I do not think any of the code base uses the matrix asFormatString() methods, however for the Gaussian Mixture Model(GMM) code I am working on I will need to serialize/deserialize covariance matrices.\nThe following matrix classes will require decoding methods:\n1. DenseMatrix\n2. SparseMatrix\n3. SparseColumnMatrix \n4. SparseRowMatrix\n5. MatrixView",
        "Issue Links": []
    },
    "MAHOUT-117": {
        "Key": "MAHOUT-117",
        "Summary": "Add Javadocs to site",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Documentation",
        "Assignee": "Sean R. Owen",
        "Reporter": "Otis Gospodnetic",
        "Created": "14/Apr/09 21:05",
        "Updated": "21/May/11 03:22",
        "Resolved": "11/Dec/09 10:09",
        "Description": "There are no Javadocs on http://lucene.apache.org/mahout .",
        "Issue Links": []
    },
    "MAHOUT-118": {
        "Key": "MAHOUT-118",
        "Summary": "Mahout needs to respect the file system type when getting a FileSystem for an input or output path",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1,                                            0.2",
        "Fix Version/s": "0.1,                                            0.2",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering,                                            Math",
        "Assignee": "Jeff Eastman",
        "Reporter": "Stephen Green",
        "Created": "16/Apr/09 20:22",
        "Updated": "31/Jan/11 07:52",
        "Resolved": "19/Apr/09 23:18",
        "Description": "All of the uses of org.apache.hadoop.fs.FileSystem.get use the single argument version that takes a job configuration.  This will always return the default file system type (which is usually HDFS), rather than using the file system type used in the URIs for the input or output paths.  This is particularly a problem on Amazon's Elastic MapReduce where the input and output data typically reside in a org.apache.hadoop.fs.s3native.NativeS3FileSystem.",
        "Issue Links": []
    },
    "MAHOUT-119": {
        "Key": "MAHOUT-119",
        "Summary": "Create an uber jar for use on Amazon Elastic M/R, etc.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "19/May/09 11:08",
        "Updated": "21/May/11 03:23",
        "Resolved": "06/Dec/09 16:05",
        "Description": "Some cloud resources have problems loading classes across JARs in the Job jar.  See http://www.lucidimagination.com/search/document/3a5680dfe567d812/running_dirichlet_example_on_aemr\nThis can be fixed by adding a new target that creates a single Jar target.",
        "Issue Links": []
    },
    "MAHOUT-120": {
        "Key": "MAHOUT-120",
        "Summary": "Site search powered by Solr",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "19/May/09 14:52",
        "Updated": "21/May/11 03:24",
        "Resolved": "28/May/09 13:15",
        "Description": "For a number of years now, the Lucene community has been criticized for not eating our own \"dog food\" when it comes to search.  My company has built and hosts a site search (http://www.lucidimagination.com/search) that is powered by Apache Solr and Lucene and we'd like to donate it's use to the Lucene community.   Additionally, it allows one to search all of the Mahout content from a single place, including web, wiki, JIRA and mail archives.   See also http://www.lucidimagination.com/search/document/bf22a570bf9385c7/search_on_lucene_apache_org\nA preview of the site is available at http://people.apache.org/~gsingers/mahout/site/publish/\nLucid has a fault tolerant setup with replication and fail over as well as monitoring services in place.  We are committed to maintaining and expanding the search capabilities on the site.\nThe following patch adds a skin to the Forrest site that enables the Mahout site to search Mahout only content using Lucene/Solr.  When a search is submitted, it automatically selects the Mahout facet such that only Mahout content is searched.  From there, users can then narrow/broaden their search criteria.\nI'm submitting this patch to Mahout first, as we'd like to roll out our capabilities to some of the smaller communities first and then broaden to the rest of the Lucene ecosystem.\nI plan on committing in a 3 or 4 days.",
        "Issue Links": []
    },
    "MAHOUT-121": {
        "Key": "MAHOUT-121",
        "Summary": "Speed up distance calculations for sparse vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Shashikant Kore",
        "Created": "22/May/09 11:28",
        "Updated": "04/Aug/10 15:02",
        "Resolved": "12/Aug/09 15:20",
        "Description": "From my mail to the Mahout mailing list.\nI am working on clustering a dataset which has thousands of sparse vectors. The complete dataset has few tens of thousands of feature items but each vector has only couple of hundred feature items. For this, there is an optimization in distance calculation, a link to which I found the archives of Mahout mailing list.\nhttp://lingpipe-blog.com/2009/03/12/speeding-up-k-means-clustering-algebra-sparse-vectors/\nI tried out this optimization.  The test setup had 2000 document  vectors with few hundred items.  I ran canopy generation with Euclidean distance and t1, t2 values as 250 and 200.\nCurrent Canopy Generation: 28 min 15 sec.\nCanopy Generation with distance optimization: 1 min 38 sec.\nI know by experience that using Integer, Double objects instead of primitives is computationally expensive. I changed the sparse vector  implementation to used primitive collections by Trove [\nhttp://trove4j.sourceforge.net/ ].\nDistance optimization with Trove: 59 sec\nCurrent canopy generation with Trove: 21 min 55 sec\nTo sum, these two optimizations reduced cluster generation time by a 97%.\nCurrently, I have made the changes for Euclidean Distance, Canopy and KMeans.  \nLicensing of Trove seems to be an issue which needs to be addressed.",
        "Issue Links": [
            "/jira/browse/MAHOUT-77"
        ]
    },
    "MAHOUT-122": {
        "Key": "MAHOUT-122",
        "Summary": "Random Forests Reference Implementation",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "23/May/09 16:20",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "28/Sep/09 05:42",
        "Description": "This is the first step of my GSOC project. Implement a simple, easy to understand, reference implementation of Random Forests (Building and Classification). The only requirement here is that \"it works\"",
        "Issue Links": [
            "/jira/browse/MAHOUT-140",
            "/jira/browse/MAHOUT-145"
        ]
    },
    "MAHOUT-123": {
        "Key": "MAHOUT-123",
        "Summary": "Implement Latent Dirichlet Allocation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "David Leo Wright Hall",
        "Created": "23/May/09 19:08",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "12/Sep/09 20:13",
        "Description": "(For GSoC)\nAbstract:\nLatent Dirichlet Allocation (Blei et al, 2003) is a powerful learning\nalgorithm for automatically and jointly clustering words into \"topics\"\nand documents into mixtures of topics, and it has been successfully\napplied to model change in scientific fields over time (Griffiths and\nSteyver, 2004; Hall, et al. 2008). In this project, I propose to\nimplement a distributed variant of Latent Dirichlet Allocation using\nMapReduce, and, time permitting, to investigate extensions of LDA and\npossibly more efficient algorithms for distributed inference.\nDetailed Description:\nA topic model is, roughly, a hierarchical Bayesian model that\nassociates with each document a probability distribution over\n\"topics\", which are in turn distributions over words. For instance, a\ntopic in a collection of newswire might include words about \"sports\",\nsuch as \"baseball\", \"home run\", \"player\", and a document about steroid\nuse in baseball might include \"sports\", \"drugs\", and \"politics\". Note\nthat the labels \"sports\", \"drugs\", and \"politics\", are post-hoc labels\nassigned by a human, and that the algorithm itself only assigns\nassociate words with probabilities. The task of parameter estimation\nin these models is to learn both what these topics are, and which\ndocuments employ them in what proportions.\nOne of the promises of unsupervised learning algorithms like Latent\nDirichlet Allocation (LDA; Blei et al, 2003) is the ability to take a\nmassive collections of documents and condense them down into a\ncollection of easily understandable topics. However, all available\nopen source implementations of LDA and related topics models are not\ndistributed, which hampers their utility. This project seeks to\ncorrect this shortcoming.\nIn the literature, there have been several proposals for paralellzing\nLDA. Newman, et al (2007) proposed to create an \"approximate\" LDA in\nwhich each processors gets its own subset of the documents to run\nGibbs sampling over. However, Gibbs sampling is slow and stochastic by\nits very nature, which is not advantageous for repeated runs. Instead,\nI propose to follow Nallapati, et al. (2007) and use a variational\napproximation that is fast and non-random.\nReferences:\nDavid M. Blei, J McAuliffe. Supervised Topic Models. NIPS, 2007.\nDavid M. Blei , Andrew Y. Ng , Michael I. Jordan, Latent dirichlet\nallocation, The Journal of Machine Learning Research, 3, p.993-1022,\n3/1/2003\nT. L. Griffiths and M. Steyvers. Finding scienti\ufb01c topics. Proc Natl\nAcad Sci U S A, 101 Suppl 1: 5228-5235, April 2004.\nDavid LW Hall, Daniel Jurafsky, and Christopher D. Manning. Studying\nthe History of Ideas Using Topic Models. EMNLP, Honolulu, 2008.\nRamesh Nallapati, William Cohen, John Lafferty, Parallelized\nvariational EM for Latent Dirichlet Allocation: An experimental\nevaluation of speed and scalability, ICDM workshop on high performance\ndata mining, 2007.\nNewman, D., Asuncion, A., Smyth, P., & Welling, M. Distributed\nInference for Latent Dirichlet Allocation. NIPS, 2007.\nXuerui Wang , Andrew McCallum, Topics over time: a non-Markov\ncontinuous-time model of topical trends. KDD, 2006\nWolfe, J., Haghighi, A, and Klein, D. Fully distributed EM for very\nlarge datasets. ICML, 2008.",
        "Issue Links": []
    },
    "MAHOUT-124": {
        "Key": "MAHOUT-124",
        "Summary": "Online Classification using HBase",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Robin Anil",
        "Created": "27/May/09 15:18",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "28/Aug/09 09:41",
        "Description": "Batch classification of flat file documents and flat file model:\nStoring the model in HBase and the end of Model Building Map/Reduce stages\nUsing the model stored in HBase create an interface (both command line and web service) to classify a give document\nUsing the model stored in HBase, batch classify documents stored on the HDFS",
        "Issue Links": [
            "/jira/browse/MAHOUT-148"
        ]
    },
    "MAHOUT-125": {
        "Key": "MAHOUT-125",
        "Summary": "Remove Deprecated Ant builds",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "27/May/09 18:05",
        "Updated": "21/May/11 03:23",
        "Resolved": "18/Nov/09 12:15",
        "Description": "Finish transferring functionality from build-deprecated.xml files to Maven.",
        "Issue Links": []
    },
    "MAHOUT-126": {
        "Key": "MAHOUT-126",
        "Summary": "Prepare document vectors from the text",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Shashikant Kore",
        "Created": "29/May/09 07:51",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "29/Jun/09 12:45",
        "Description": "Clustering algorithms presently take the document vectors as input.  Generating these document vectors from the text can be broken in two tasks. \n1. Create lucene index of the input  plain-text documents \n2. From the index, generate the document vectors (sparse) with weights as TF-IDF values of the term. With lucene index, this value can be calculated very easily. \nPresently, I have created two separate utilities, which could possibly be invoked from another class.",
        "Issue Links": [
            "/jira/browse/MAHOUT-65",
            "/jira/browse/MAHOUT-61"
        ]
    },
    "MAHOUT-127": {
        "Key": "MAHOUT-127",
        "Summary": "Remove warnings",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Benson Margulies",
        "Created": "29/May/09 13:50",
        "Updated": "18/Nov/09 14:06",
        "Resolved": "30/May/09 07:13",
        "Description": "The patch I'm about to attach gets rid of all the current yellow triangles for Eclipse 3.4.",
        "Issue Links": []
    },
    "MAHOUT-128": {
        "Key": "MAHOUT-128",
        "Summary": "maven parent not included in build",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Benson Margulies",
        "Created": "29/May/09 15:40",
        "Updated": "21/May/11 03:24",
        "Resolved": "04/Jun/09 18:15",
        "Description": "The maven parent isn't included as a module, so it's pom isn't installed, so building some other project that depends on mahout-core fails.",
        "Issue Links": []
    },
    "MAHOUT-129": {
        "Key": "MAHOUT-129",
        "Summary": "Kmeans sample does not expose numIterations control from KMeansDriver",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Benson Margulies",
        "Created": "29/May/09 17:00",
        "Updated": "21/May/11 03:24",
        "Resolved": "29/May/09 23:08",
        "Description": "The KMeans driver forces the numReduceTasks parameter of KMeans to 1, and there are javadoc/param naming problems in KMeansDriver.",
        "Issue Links": []
    },
    "MAHOUT-130": {
        "Key": "MAHOUT-130",
        "Summary": "Vector should allow for other normalize powers than the L-2 norm",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "08/Jun/09 20:14",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/Jun/09 14:31",
        "Description": "Modify Vector to allow other normalize functions for the Vector\nSee http://www.lucidimagination.com/search/document/bf3a7a7a004d4191/norm_calculations",
        "Issue Links": []
    },
    "MAHOUT-131": {
        "Key": "MAHOUT-131",
        "Summary": "Vector improvements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "11/Jun/09 02:31",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "16/Jun/09 19:21",
        "Description": "Vector and it's implementations could use a few things:\n1. DenseVector should implement equals and hashCode similar to SparseVector\n2. The VectorView asFormatString() is not compatible with actually recreating any type of vector.  \n3. Add tests to VectorTest that assert that decodeFormat/asFormatString is able to do a round trip.\n4. Add static AbstractVector.equivalent(Vector, Vector) that takes in two vectors and compares them for equality, regardless of their implementation.",
        "Issue Links": []
    },
    "MAHOUT-132": {
        "Key": "MAHOUT-132",
        "Summary": "[PATCH] Push magic names into public constants",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Robert Burrell Donkin",
        "Created": "14/Jun/09 10:39",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "15/Jun/09 01:32",
        "Description": "ATM the examples (and any similar code) need to hard code magic strings for directories. This makes the code more fragile and more difficult to understand.",
        "Issue Links": []
    },
    "MAHOUT-133": {
        "Key": "MAHOUT-133",
        "Summary": "[PATCH] Kmeans Clustering Example Tidy Up",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Robert Burrell Donkin",
        "Created": "14/Jun/09 18:35",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "23/Jun/09 19:32",
        "Description": "Changes to the KMeans code mean that the processing performed in org.apache.mahout.clustering.syntheticcontrol.kmeans.OutputDriver and org.apache.mahout.clustering.syntheticcontrol.kmeans.OutputMapper is no longer required. Retaining these classes makes the example code more difficult to understand.",
        "Issue Links": []
    },
    "MAHOUT-134": {
        "Key": "MAHOUT-134",
        "Summary": "[PATCH] Cluster decode error handling",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Robert Burrell Donkin",
        "Created": "15/Jun/09 09:00",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "18/Sep/09 11:37",
        "Description": "ATM the javadocs are unclear as to whether null is an acceptable return value and callers do not null check the return value. However, the implementation may return null in or throw other runtime exceptions when the format is not correct. This makes it hard to diagnose when there's a problem with the format.",
        "Issue Links": []
    },
    "MAHOUT-135": {
        "Key": "MAHOUT-135",
        "Summary": "Allow FileDataModel to transpose users and items",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "18/Jun/09 23:43",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "19/Jun/09 11:09",
        "Description": "Sometimes it would be nice to flip around users and items in the FileDataModel.  This patch adds a transpose boolean that flips userId and itemId in the processLine method.",
        "Issue Links": []
    },
    "MAHOUT-136": {
        "Key": "MAHOUT-136",
        "Summary": "Change Canopy MR Implementation to use Vector Writable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "19/Jun/09 21:44",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "29/Sep/09 10:39",
        "Description": "Internal serialization of Canopy currently uses asFormatString rather than just making the Canopy writable. This is storage inefficient.",
        "Issue Links": [
            "/jira/browse/MAHOUT-137"
        ]
    },
    "MAHOUT-137": {
        "Key": "MAHOUT-137",
        "Summary": "Convert Clustering Algs to use Vector Writable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "20/Jun/09 13:26",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "29/Jun/09 17:03",
        "Description": "All M/R jobs should use Vector writable instead of encoding and decoding strings.  We can have a separate utility that converts serialized GSON, Strings, whatever into the appropriate vectors.  See MAHOUT-136 and http://www.lucidimagination.com/search/document/6a55f260826fd77f/jira_commented_mahout_136_change_canopy_mr_implementation_to_use_vector_writable",
        "Issue Links": [
            "/jira/browse/MAHOUT-136",
            "/jira/browse/MAHOUT-148"
        ]
    },
    "MAHOUT-138": {
        "Key": "MAHOUT-138",
        "Summary": "Convert main() methods to use Commons CLI for argument processing",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "22/Jun/09 12:10",
        "Updated": "18/Nov/09 14:06",
        "Resolved": "15/Oct/09 15:28",
        "Description": "Commons CLI is in the classpath and makes it much easier to handle command line args and they are more self-documenting when done right.  We should convert our main methods to use CLI",
        "Issue Links": []
    },
    "MAHOUT-139": {
        "Key": "MAHOUT-139",
        "Summary": "Make use of Vector Iterator capabilities where appropriate",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "24/Jun/09 04:51",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "25/Jun/09 19:36",
        "Description": "There are a bunch of places where we loop over the size of the vector when we should be taking advantage of the sparseness, or at least be agnostic about it and use an iterator.\nThis patch addresses these issues in the Vector implementations and in the DistanceMeasure implementations\nAlso adds iterateNonZero() and interateAll and drops the Iterable portion of Vector since it wasn't clear what it was iterating",
        "Issue Links": [
            "/jira/browse/MAHOUT-77"
        ]
    },
    "MAHOUT-140": {
        "Key": "MAHOUT-140",
        "Summary": "In-memory mapreduce Random Forests",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "29/Jun/09 11:29",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "29/Sep/09 07:24",
        "Description": "Each mapper is responsible for growing a number of trees with a whole copy of the dataset loaded in memory, it uses the reference implementation's code to build each tree and estimate the oob error.",
        "Issue Links": [
            "/jira/browse/MAHOUT-122"
        ]
    },
    "MAHOUT-141": {
        "Key": "MAHOUT-141",
        "Summary": "Cluster Centroid improperly reported when no points associated w/ the cluster",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "29/Jun/09 17:19",
        "Updated": "21/May/11 03:23",
        "Resolved": "12/Dec/09 11:29",
        "Description": "Per the discussion at http://www.lucidimagination.com/search/document/5d54bb4822354bee/kmeans_help#48571ce2f5014a1f, the Cluster class returns pointTotal as the centroid (center) of a Cluster with zero points, which is not correct.  It should return the Center as the centroid in this case.",
        "Issue Links": []
    },
    "MAHOUT-142": {
        "Key": "MAHOUT-142",
        "Summary": "Upgrade to Hadoop 0.20.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "01/Jul/09 14:42",
        "Updated": "15/Jul/09 20:12",
        "Resolved": "15/Jul/09 19:19",
        "Description": "As the title says",
        "Issue Links": []
    },
    "MAHOUT-143": {
        "Key": "MAHOUT-143",
        "Summary": "Refactor Hadoop deprecations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "01/Jul/09 15:26",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "26/May/10 19:03",
        "Description": "Hadoop 0.20.0 deprecates JobConf in favor of a Configuration.  Need to investigate what this means and convert to using it.  Seems to now require some XML based approach.",
        "Issue Links": []
    },
    "MAHOUT-144": {
        "Key": "MAHOUT-144",
        "Summary": "Some maven refactoring and prep for enforcing code style",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Benson Margulies",
        "Created": "11/Jul/09 20:44",
        "Updated": "21/May/11 03:24",
        "Resolved": "06/Dec/09 16:06",
        "Description": "The attached does a few things:\n1) sorts out the maven parents: now the modules parent to 'maven', and 'maven' parents to the top-level project. \n2) The release management in the top-level POM is in a profile.\n3) the version of 'maven' is consistent with other version numbers.\n4) the source control URLs are corrected.\n5) a new buildtools module to hold pmd and checkstyle config.\n6) dependencyManagement in the parent, initially just for lucene.\n7) backup to current lucene release. -Dlucene.version is there for those who really want to use 2.9-SNAPSHOT.\n8) a profile, sourcecheck, that turns on checkstyle and pmd. This creates a giant pile of complaints. \nThe next step in this process would be to come up with a set of checkstyle and pmd rules consistent with the community's desires.",
        "Issue Links": []
    },
    "MAHOUT-145": {
        "Key": "MAHOUT-145",
        "Summary": "PartialData mapreduce Random Forests",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "12/Jul/09 10:52",
        "Updated": "28/Feb/13 20:38",
        "Resolved": "30/Sep/09 05:44",
        "Description": "This implementation is based on a suggestion by Ted:\n\"modify the original algorithm to build multiple trees for different portions of the data. That loses some of the solidity of the original method, but could actually do better if the splits exposed non-stationary behavior.\"",
        "Issue Links": [
            "/jira/browse/MAHOUT-122"
        ]
    },
    "MAHOUT-146": {
        "Key": "MAHOUT-146",
        "Summary": "Make Wikipedia Example Classifier more generic",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "16/Jul/09 20:22",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "14/Sep/09 16:31",
        "Description": "It would be nice if the Wikipedia classifier example was a bit more generic instead of taking just countries.  For example, one could classify based on other types of categories, such as things like \"subjects\", i.e. History, Math, Science or other things.",
        "Issue Links": []
    },
    "MAHOUT-147": {
        "Key": "MAHOUT-147",
        "Summary": "Wikipedia Example improvements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "17/Jul/09 14:39",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "13/Aug/09 14:50",
        "Description": "The Wikipedia example for classification can be improved by:\n1. streamlining category matching:  Currently, we identify all the categories in the doc and then check to see if there are matches by looping over all the found categories and all the input categories, with the first match winning.\nAm examining a bit closer, so may add more here.",
        "Issue Links": []
    },
    "MAHOUT-148": {
        "Key": "MAHOUT-148",
        "Summary": "Convert Classification Algs to use richer Writable syntax",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1,                                            0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Grant Ingersoll",
        "Created": "19/Jul/09 15:21",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "19/Oct/09 00:09",
        "Description": "Much of the classification capabilities relies on parsing values out from the Text object just to determine what type of \"thing\" is being used.  We should try to avoid having to do string manipulation for this kind of thing and instead encapsulate it in Writable instances.  This should make things perform faster and bring stronger typing to the problem, which should make it easier to understand and debug the code.",
        "Issue Links": [
            "/jira/browse/MAHOUT-124",
            "/jira/browse/MAHOUT-137"
        ]
    },
    "MAHOUT-149": {
        "Key": "MAHOUT-149",
        "Summary": "The Great User/Item Removal Phase 1: de-generify implementations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "22/Jul/09 20:29",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "24/Jul/09 14:37",
        "Description": "As threatened on the mailing list, an effort to remove the User and Item abstractions in the CF code is underway. This issue encapsulates phase 1, in which the GenericUser, GenericItem and BooleanPrefUser classes are de-generified, and then, all \"ID\" objects are converted from Object to Comparable<?>. Patch coming.",
        "Issue Links": []
    },
    "MAHOUT-150": {
        "Key": "MAHOUT-150",
        "Summary": "The Great User/Item Removal Phase 2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "22/Jul/09 20:35",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "24/Jul/09 14:38",
        "Description": "... in which we first remove Item from the code base and its implementations, and replace it with simple references to the item ID.",
        "Issue Links": []
    },
    "MAHOUT-151": {
        "Key": "MAHOUT-151",
        "Summary": "The Great User/Item Removal Phase 3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "24/Jul/09 14:39",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "04/Aug/09 06:48",
        "Description": "... in which I remove User, its implementations and uses. The DataModel will now provide preferences for a user ID.",
        "Issue Links": []
    },
    "MAHOUT-152": {
        "Key": "MAHOUT-152",
        "Summary": "SquaredEuclideanDistanceMeasure distance method has a bug in comparing sizes",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "27/Jul/09 01:26",
        "Updated": "21/May/11 03:24",
        "Resolved": "27/Jul/09 01:28",
        "Description": "Line 80: \n    if (centroid.size() != centroid.size()) {\nShould compare centroid and input vector.",
        "Issue Links": []
    },
    "MAHOUT-153": {
        "Key": "MAHOUT-153",
        "Summary": "Implement kmeans++ for initial cluster selection in kmeans",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.2",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Ted Dunning",
        "Reporter": "Panagiotis Papadimitriou",
        "Created": "27/Jul/09 21:37",
        "Updated": "06/Jun/12 20:50",
        "Resolved": "21/May/11 02:38",
        "Description": "The current implementation of k-means includes the following algorithms for initial cluster selection (seed selection): 1) random selection of k points, 2) use of canopy clusters.\nI plan to implement k-means++. The details of the algorithm are available here: http://www.stanford.edu/~darthur/kMeansPlusPlus.pdf.\nDesign Outline: I will create an abstract class SeedGenerator and a subclass KMeansPlusPlusSeedGenerator. The existing class RandomSeedGenerator will become a subclass of SeedGenerator.",
        "Issue Links": []
    },
    "MAHOUT-154": {
        "Key": "MAHOUT-154",
        "Summary": "Reduce memory usage with smarter data structures",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "31/Jul/09 13:14",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "04/Aug/09 06:49",
        "Description": "Memory usage remains an issue. This issue tracks two changes with API implications that could reduce memory requirements:\n\nuse float, not double, for preference values. It is terribly unlikely that a float (4 bytes) is not enough precision to accurately represent user preferences, which are typically like \"3.0\" or \"4.5\". Using float instead of an 8-byte double saves 4 bytes per preference value, which is significant when loading tens of millions of prefs into memory\n\n\nPreference[] is an inefficient way to store prefs, since it entails a great deal of Preference object overhead (48 bytes per pref is needed, of which 36 is overhead ) Using an abstraction like PreferenceArray which can use parallel arrays internally can cut at least 12 of the 36 bytes of overhead out \u2013 more if crazier data structures are used.\n\nSo far these changes have reduced memory requirements  by about 20% in my particular test case, which is significant.\nI am tracking this as an issue since like MAHOUT-151 it will entail API changes.",
        "Issue Links": []
    },
    "MAHOUT-155": {
        "Key": "MAHOUT-155",
        "Summary": "ARFF VectorIterable",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "01/Aug/09 12:41",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "06/Nov/11 09:45",
        "Description": "Convert ARFF to Vector.  See http://www.cs.waikato.ac.nz/~ml/weka/arff.html\nCreate a VectorIterable implementation for ARFF.",
        "Issue Links": [
            "/jira/browse/MAHOUT-928"
        ]
    },
    "MAHOUT-156": {
        "Key": "MAHOUT-156",
        "Summary": "Documentation and Code cleanup for all Bayesian Classes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "02/Aug/09 10:12",
        "Updated": "21/May/11 03:22",
        "Resolved": "12/Jan/10 13:15",
        "Description": "Cleanup redundancies and unused classes. \nAdd package summary.",
        "Issue Links": []
    },
    "MAHOUT-157": {
        "Key": "MAHOUT-157",
        "Summary": "Frequent Pattern Mining using Parallel FP-Growth",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "02/Aug/09 10:14",
        "Updated": "28/Mar/16 21:04",
        "Resolved": "18/Oct/09 22:22",
        "Description": "Implement: http://infolab.stanford.edu/~echang/recsys08-69.pdf",
        "Issue Links": []
    },
    "MAHOUT-158": {
        "Key": "MAHOUT-158",
        "Summary": "Replace all ID values with long",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "04/Aug/09 09:45",
        "Updated": "02/May/13 02:29",
        "Resolved": "11/Aug/09 12:53",
        "Description": "As mentioned on mailing list, I am tracking this as a possible change for evaluation. The idea is to save more memory / CPU by avoiding the Object overhead of tens of millions of ID objects by using long IDs instead.",
        "Issue Links": [
            "/jira/browse/MAHOUT-162"
        ]
    },
    "MAHOUT-159": {
        "Key": "MAHOUT-159",
        "Summary": "SparseVector and DenseVector hashCode does not conform to the Java standard",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Mark Desnoyer",
        "Created": "04/Aug/09 17:02",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "01/Sep/09 19:22",
        "Description": "The hash codes for SparseVector and DenseVector will not be equal even though equals() may return true. Also, the equals logic is inconsistent because DenseVector takes into account the name parameter but SparseVector does not.",
        "Issue Links": []
    },
    "MAHOUT-160": {
        "Key": "MAHOUT-160",
        "Summary": "ClusterDumper utility to output all the clusters in all sequence files and points",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Shashikant Kore",
        "Created": "06/Aug/09 06:54",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "17/Sep/09 13:08",
        "Description": "The current ClusterDumper utility takes a sequence file and points file as input and prints the cluster vector along with the points that belong to the clusters in the sequence file. This utility doesn't produce correct results in case there are multiple sequence files and points. \nTo avoid this problem, all the point to cluster mappings need to be read first and then iterate on the sequence files.",
        "Issue Links": []
    },
    "MAHOUT-161": {
        "Key": "MAHOUT-161",
        "Summary": "Add Vector.norm to compute k-norms of vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "David Leo Wright Hall",
        "Created": "07/Aug/09 19:59",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "09/Aug/09 11:08",
        "Description": "This patch adds Vector.norm(double power) to Vector (and an implementation to AbstractVector).\nAbstractVector.normalize now calls norm.",
        "Issue Links": []
    },
    "MAHOUT-162": {
        "Key": "MAHOUT-162",
        "Summary": "Added support for mapping String to long IDs in CF code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "11/Aug/09 12:58",
        "Updated": "02/May/13 02:29",
        "Resolved": "17/Aug/09 16:53",
        "Description": "Since the framework now only allows long (64-bit integer) IDs, and no longer Strings, we need to provide some support for translating between the two. The basic proposal is this:\n\nDefine a one-way mapping from Strings to longs that is repeatable and easy to implement in many contexts. In particular I propose using the bottom 64 bits of the MD5 hash of a string.\nDefine support for storing the reverse mapping (longs to Strings) in various ways, in an efficient way, that handles gracefully the very rare possibility of collision",
        "Issue Links": [
            "/jira/browse/MAHOUT-158"
        ]
    },
    "MAHOUT-163": {
        "Key": "MAHOUT-163",
        "Summary": "Get (better) cluster labels using Log Likelihood Ratio",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Shashikant Kore",
        "Created": "13/Aug/09 10:35",
        "Updated": "21/May/11 03:24",
        "Resolved": "12/Jan/10 11:15",
        "Description": "Log Likelihood Ratio (LLR) is a better technique to identify cluster labels instead of the top features of the centroid vector. LLR finds terms/phrases which are common in the cluster but rare outside.",
        "Issue Links": []
    },
    "MAHOUT-164": {
        "Key": "MAHOUT-164",
        "Summary": "\"Potpourri\": a collection of small possible bugs and improvements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "19/Aug/09 11:26",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "20/Aug/09 22:56",
        "Description": "I call this issue \"Potpourri\". It's a number of small items batched together that didn't seem worth an issue on their own.\n1. DenseVector.NonZeroIterator\nThis doesn't seem to work, exactly, as per the Iterator contract. next() doesn't advance the iterator; hasNext() does. A normal series of calls to these might get the right effect, but this seems wrong. See proposed fix.\n2. next() and NoSuchElementException\nIterators need to throw this appropriately \u2013 see changes\n3. DistanceMeasure.distance(double[] p1, double[] p2)\nLooks like subclasses are kinda trying to treat this as a method to be overridden/implemented. I changed to make it so. Am I mistaken and this is on the way out or something? in which case, removable?\n4. Subtraction in compareTo()\nCheck out IntPairWritable. It does the sensible thing and tries to return the difference of two ints to satisfy the compareTo() contract. This works until the subtraction overflows.\n5. Comparators and Serializable\nSuper tiny point. Comparators ought to implement Serializable\n6. Double literals\nAnother super tiny point: suggest we write double literals in canonical form. \"1\" is an int; \"1.0\" is a double (though the former will be widened \u2013 might as well write what's meant. Likewise \"1.0f\" is a float but \"1.0\" is not. Also suggest that \"1.0E-5\" is better than \"1E-5\" for doubles.\n7. compareTo() and equals()\nequals() needs to be overridden when compareTo() is. and hashCode() needs to be there when equals() is. Looking at LDAPrintTopics.StringDoublePair\n8. Redundant braces\nFinally like I mentioned, we should't do...\nfor (...) {\n  {\n     ...\n  }\n}",
        "Issue Links": []
    },
    "MAHOUT-165": {
        "Key": "MAHOUT-165",
        "Summary": "Using better primitives hash for sparse vector for performance gains",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Shashikant Kore",
        "Created": "19/Aug/09 13:59",
        "Updated": "13/Dec/09 23:56",
        "Resolved": "13/Dec/09 23:56",
        "Description": "In SparseVector, we need primitives hash map for index and values. The present implementation of this hash map is not as efficient as some of the other implementations in non-Apache projects. \nIn an experiment, I found that, for get/set operations, the primitive hash of  Colt performance an order of magnitude better than OrderedIntDoubleMapping. For iteration it is 2x slower, though. \nUsing Colt in Sparsevector improved performance of canopy generation. For an experimental dataset, the current implementation takes 50 minutes. Using Colt, reduces this duration to 19-20 minutes. That's 60% reduction in the delay.",
        "Issue Links": [
            "/jira/browse/MAHOUT-204"
        ]
    },
    "MAHOUT-166": {
        "Key": "MAHOUT-166",
        "Summary": "Potpourri 2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "22/Aug/09 18:36",
        "Updated": "17/Sep/12 20:04",
        "Resolved": "24/Aug/09 20:16",
        "Description": "Another large changelist constructed from FindBugs and IntelliJ analysis. It's big enough I figured I'd run it by the list. Key changes:\n\nMaking stuff final, private that can be\nDead code elimination\nSimplifying JUnit assertions \u2013 \"assertTrue(a.equals(b) == true)\" could be \"assertEquals(a, b)\" for instance. Also fixed some expected/actual value issues\nNot compiling a Pattern object millions of times \u2013 String.split() and replace()/replaceAll() do this and can be profitably replaced with a precompiled Pattern.\nSmall bug fixes picked up by analysis",
        "Issue Links": []
    },
    "MAHOUT-167": {
        "Key": "MAHOUT-167",
        "Summary": "Convert code to Hadoop 0.20 API",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jeff Eastman",
        "Created": "28/Aug/09 15:53",
        "Updated": "21/May/11 03:18",
        "Resolved": "07/Mar/11 13:16",
        "Description": "We need to update the various implementations to remove the deprecated Hadoop API calls.",
        "Issue Links": []
    },
    "MAHOUT-168": {
        "Key": "MAHOUT-168",
        "Summary": "Need integer compression routines",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.1",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "31/Aug/09 18:27",
        "Updated": "21/May/11 03:27",
        "Resolved": "11/Dec/09 21:25",
        "Description": "A selection of these algorithms would be very nice to have:\nwww.cs.rmit.edu.au/~jz/fulltext/compjour99.pdf",
        "Issue Links": []
    },
    "MAHOUT-169": {
        "Key": "MAHOUT-169",
        "Summary": "Wrong implementation of the mapper in the canopy clusterer?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Peter Wippermann",
        "Created": "02/Sep/09 03:18",
        "Updated": "21/May/11 03:24",
        "Resolved": "12/Sep/09 00:17",
        "Description": "The class\norg.apache.mahout.clustering.canopy.CanopyMapper\ntakes use of the method \"Canopy.addPointToCanopies(point, canopies)\".\nThe documentation of the Canopy-Class says, that this function is used by the reducer - no mention of the mapper here.\nIt furthermore says, that the mapper would use \"emitPointToNewCanopies\" or \"emitPointToExistingCanopies\", which however he does not.\nI'm just trying to figure out, how the canopy clustering in general works. But this is confusing to me. So if it is a bug, please fix it. If it is not, I'd be very happy, if you could explain me, why \nFurthermore I'm wondering why the syntheticcontrol clustering example will only find ONE SINGLE CLUSTER. Do I mix thinks up here???",
        "Issue Links": []
    },
    "MAHOUT-170": {
        "Key": "MAHOUT-170",
        "Summary": "Enable Java compile optimize flag during build",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "04/Sep/09 11:07",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "18/Oct/09 22:30",
        "Description": "in maven compile plugin enable optimize=true flag",
        "Issue Links": []
    },
    "MAHOUT-171": {
        "Key": "MAHOUT-171",
        "Summary": "Move deployment to repository.apache.org",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "04/Sep/09 11:31",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "19/Oct/09 19:04",
        "Description": "Opening a JIRA task to collect what has to be done for moving over to using apache version 5 parent pom (see also http://markmail.org/thread/ld26m3xxzoztqsk6 ).\n\nLink Apache parent pom into our pom.\nUpdate hudson to build via maven ( ? ).\nFile subtask at INFRA-1896 to include mahout in repository.apache.org",
        "Issue Links": []
    },
    "MAHOUT-172": {
        "Key": "MAHOUT-172",
        "Summary": "When running on a Hadoop cluster LDA fails with Caused by: java.io.IOException: Cannot open filename /user/*/output/state-*/_logs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "04/Sep/09 16:11",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "14/Sep/09 06:40",
        "Description": "I tried running the reuters example of lda on a hadoop cluster today. Seems like the implementation tries to read all files in output/state-* which fails if in that directory \"_logs\" is found.",
        "Issue Links": []
    },
    "MAHOUT-173": {
        "Key": "MAHOUT-173",
        "Summary": "Implement clustering of massive-domain attributes",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Matias Bj\u00f8rling",
        "Created": "06/Sep/09 08:44",
        "Updated": "21/May/11 03:27",
        "Resolved": "12/Jan/10 12:26",
        "Description": "Implement the Clustering algorithm described in \"A Framework for Clustering Massive-Domain Data Streams\" by Chary C. Aggarwal.\nSteps: \n1. Implement baseline solution to compare solutions.\n2. Figure out how to implement the loading of clustering by looking at the k-means implementation.\n3. Implement Count-Min sketch algorithm for each cluster.\n4. Find out how to give the user the power to choose the distance function for the input data ( Maybe already possible? )",
        "Issue Links": []
    },
    "MAHOUT-174": {
        "Key": "MAHOUT-174",
        "Summary": "Unify Pair implementation, Random number generation",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "07/Sep/09 07:25",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "08/Sep/09 13:11",
        "Description": "Per discussion on mahout-dev, starting a series of patches to unify code, starting with Pair. Simple enough, but, it snowballed a little bit:\n\nNow there is just org.apache.mahout.common.Pair\nI moved my LongPair implementation next to it for consistency\nBut it depends on my RandomUtils, and doesn't make sense for a top-level package to depend downward like that, so moved it too \u2013 since we discussed that this should probably get reused a little more too\n\n\nThen I saw that the LFUCache class needs a mutable Pair whereas the new variant is immutable. I judged it was better to actually modify LFUCache to not need mutability \u2013 because in this case it ends up increasing performance as well: Rather than create and re-create Longs as counts, it uses a \"mutable Long\" \u2013 AtomicLong. An array of one long would have done the trick too.\n\n\nNow, the patch also unifies random number generation around MersenneTwisterRNG, and ensures that all RNGs can be set to use a constant seed when in unit test mode.",
        "Issue Links": []
    },
    "MAHOUT-175": {
        "Key": "MAHOUT-175",
        "Summary": "Use IOUtils, FileLineIterable/Iterator across the project",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "09/Sep/09 09:37",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "09/Sep/09 15:43",
        "Description": "The patch I will attach does a couple things:\nUses FileLineIterable/Iterator for iterating over lines of a file. This is slightly tidier, but also addresses a few subtle issues across the code base, where reading of files did not always end by closing the stream, or, relied on platform default character encoding.\nUses IOUtils consistently to close Closeables, swallowing and logging exceptions in cases where they are not to be treated as errors.\nFinally, fixes some issues in ARFFIterator while I'm at it \u2013 hasNext() changes state, when next() should, and next() did not throw NoSuchElementException",
        "Issue Links": []
    },
    "MAHOUT-176": {
        "Key": "MAHOUT-176",
        "Summary": "Remove VectorIterable in favor of just using Iterable<Vector>",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "09/Sep/09 12:54",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "09/Sep/09 13:34",
        "Description": "VectorIterable serves no purpose other than being a marker interface.  Just use Iterable<Vector> instead.",
        "Issue Links": []
    },
    "MAHOUT-177": {
        "Key": "MAHOUT-177",
        "Summary": "Fix for \"java.lang.ClassNotFoundException Exception\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "13/Sep/09 08:20",
        "Updated": "18/Nov/09 14:06",
        "Resolved": "06/Oct/09 05:36",
        "Description": "When trying to run the Synthetic Control Data examples (http://cwiki.apache.org/MAHOUT/syntheticcontroldata.html) in a pseudo-distributed Hadoop cluster I got the java.lang.ClassNotFoundException exception in kmeans and dirichlet. canopy and meanshift worked perfectly.\nThe past solution was to flatten the examples jar: unjar all the libs and pack them in a single jar file. I found that, for the clustering examples, you only need to unpack the mahout-core jar, and because the compiled classes are already available in core/target/classes I changed maven/build.xml as follows:\n\ngrab the core/target/classes directly instead of the packed jar\nexclude apache-mahout-core-*.jar, this should exclude both apache-mahout-core.jar and apache-mahout-core-tests.jar (do the examples really need the core-tests jar ?)",
        "Issue Links": []
    },
    "MAHOUT-178": {
        "Key": "MAHOUT-178",
        "Summary": "Rationalize 'utils' and 'common' stuff",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "15/Sep/09 12:49",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "18/Sep/09 09:21",
        "Description": "Every project needs a common area for code that is not obviously part of any specific piece of the project, typically because it's used in many places. This is good as it promotes reuse. I would like to make an explicit effort to rationalize this project's approach to 'common', starting with some basic reshuffling, which will then pave the way to unify more of the code that is duplicated now (thinking: caches, distance measures, Hadoop integration, etc.)\nRight now we have this common code in three places, when it seems like there should be basically one:\n\nmahout-core: org.apache.mahout.utils\nmahout-core: org.apache.mahout.common\nmahout-utils\n\nI suggest that of the two packages named above, 'common' is slightly preferable; one could easily just merge these packages. I also would like to ask whether it makes sense to have a mahout-utils module? It's like having a mahout-core-core, in my opinion. It appears to serve exactly the same role as the other utils/common package. Would it ever be used as a standalone build product?\nRenaming may sound like a trivial change, but I think the above is merely symptomatic of several developers having independent ideas about where to stash common stuff. I want to force the issue and push everyone's stuff together to begin the hard but necessary work of refactoring the code base into something more unified.\nSo far, I propose pushing all code together into org.apache.mahout.common. This is enough of a big-bang that will break patches that I want to propose it, and if agreed, plan when to commit.\n(Also, shouldn't stuff like the distance measure classes be in a package?)\nAnyway, partial patch will be attached shortly.",
        "Issue Links": []
    },
    "MAHOUT-179": {
        "Key": "MAHOUT-179",
        "Summary": "Taste Demo Help",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tamas Jambor",
        "Created": "22/Sep/09 19:23",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "23/Sep/09 17:22",
        "Description": "hi there,\nI'm trying to run the taste demo, just downloaded the latest version of mahout. I compile the whole thing (mvn install) and I create the package. everything runs smoothly, but in the end it gives me this error:\nHTTP ERROR 404\nProblem accessing /mahout-taste-webapp/RecommenderServlet. Reason:\nNOT_FOUND\ni'm not really familiar with this enviroment but maybe the war file might create a library 'mahout-taste-webapp-0.1' instead of 'mahout-taste-webapp'?\nplease help me out.\nthanks a lot,\nTamas",
        "Issue Links": []
    },
    "MAHOUT-180": {
        "Key": "MAHOUT-180",
        "Summary": "port Hadoop-ified Lanczos SVD implementation from decomposer",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "25/Sep/09 20:01",
        "Updated": "21/May/11 03:24",
        "Resolved": "20/Feb/10 15:47",
        "Description": "I wrote up a hadoop version of the Lanczos algorithm for performing SVD on sparse matrices available at http://decomposer.googlecode.com/, which is Apache-licensed, and I'm willing to donate it.  I'll have to port over the implementation to use Mahout vectors, or else add in these vectors as well.\nCurrent issues with the decomposer implementation include: if your matrix is really big, you need to re-normalize before decomposition: find the largest eigenvalue first, and divide all your rows by that value, then decompose, or else you'll blow over Double.MAX_VALUE once you've run too many iterations (the L^2 norm of intermediate vectors grows roughly as (largest-eigenvalue)^(num-eigenvalues-found-so-far), so losing precision on the lower end is better than blowing over MAX_VALUE).  When this is ported to Mahout, we should add in the capability to do this automatically (run a couple iterations to find the largest eigenvalue, save that, then iterate while scaling vectors by 1/max_eigenvalue).",
        "Issue Links": [
            "/jira/browse/MAHOUT-76"
        ]
    },
    "MAHOUT-181": {
        "Key": "MAHOUT-181",
        "Summary": "DistanceMeasure is broken: iteration is done over nonZeroElements of v1.plus(v2), not v1.minus(v2)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Jake Mannix",
        "Created": "30/Sep/09 10:29",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "14/Oct/09 13:03",
        "Description": "SquaredEuclideanDistanceMeasure iterates over v1.plus(v2), which has the right number of nonzero elements if v1.get != -v2.get for all i indexing nonzero elements, but for example, the simple case of looking at SquaredEuclideanDisanceMeasure.distance(v, v.assign(new NegateFunction())) yeilds zero on current trunk, instead of 4*v.lengthSquared().\nAttached is a patch with a unit test which checks that DistanceMeasure.distance always returns nonnegative results and in particular also does not return , as well as a fix for ManhattanDistanceMeasure, SquaredEuclideanDistanceMeasure, and EuclideanDistanceMeasure.\nUnfortunately, the attached unit test reveals that the TanimotoDistanceMeasure is more broken than I can fix at present.  It doesn't appear to be properly using the referenced formula in wikipedia, and in fact sometimes returns negative results.  This means that with this patch applied, TestTanimotoDistanceMeasure is failing (and rightfully so).",
        "Issue Links": []
    },
    "MAHOUT-182": {
        "Key": "MAHOUT-182",
        "Summary": "New helper methods for Matrix: times(Vector), timesSquared(Vector), numRows() and numCols()",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Jake Mannix",
        "Created": "01/Oct/09 00:32",
        "Updated": "21/May/11 03:24",
        "Resolved": "22/Nov/09 16:13",
        "Description": "numRows() \n{ return size()[ROW]; }\n and numCols() \n{ return size()[COL]; }\n are pretty much no-brainer methods, right?  Who wants to deal with a length-two array of ints all the time when getting the number of rows and columns of a matrix?\nThose are pretty trivial, but the key feature of a Matrix is to map Vector instances to Vector instances, and while you can do that currently by making a a row Matrix and doing Matrix.times(Matrix), it's silly to have to always do that.  Matrix.times(Vector) is pretty needed.\nEven less trivial, for really big sparse Matrices, if you need to get (M'M)v for some matrix M, then this can be computed in one pass through M without ever computing the transpose of M by a simple reordering of the limits of summation.\nAttaching a patch with these implementations, including unit tests (as well as an improvement in the Matrix.times(Matrix) unit test to actually check the math).",
        "Issue Links": []
    },
    "MAHOUT-183": {
        "Key": "MAHOUT-183",
        "Summary": "WikipediaXmlSplitter spits one chunk per line",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Olivier Grisel",
        "Created": "01/Oct/09 11:25",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "05/Oct/09 10:47",
        "Description": "The Wikipedia XML splitter inner loop erronously detects end of the line-iterator which causes it to create chunks with just one line worth of page content instead of respecting the --chunkSize cli option.\nSimple patch to fix this will follow.",
        "Issue Links": []
    },
    "MAHOUT-184": {
        "Key": "MAHOUT-184",
        "Summary": "Code tweaks for .df.* code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "01/Oct/09 23:15",
        "Updated": "18/Nov/09 14:06",
        "Resolved": "05/Oct/09 10:47",
        "Description": "This follows on my last email to the mailing list, and code inspection. It's big enough I made a patch. No surprises I hope given the consensus on code style and practice. Might be some good takeaways in here, or points for further discussion.",
        "Issue Links": []
    },
    "MAHOUT-185": {
        "Key": "MAHOUT-185",
        "Summary": "Add mahout shell script for easy launching of various algorithms",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "06/Oct/09 12:59",
        "Updated": "21/May/11 03:24",
        "Resolved": "12/Feb/10 13:57",
        "Description": "Currently, Each algorithm has a different point of entry. At its too complicated to understand and launch each one.  A mahout shell script needs to be made in the bin directory which does something like the following\nmahout classify -algorithm bayes [OPTIONS]\nmahout cluster -algorithm canopy  [OPTIONS]\nmahout fpm -algorithm pfpgrowth [OPTIONS]\nmahout taste -algorithm slopeone [OPTIONS] \nmahout misc -algorithm createVectorsFromText [OPTIONS]\nmahout examples WikipediaExample",
        "Issue Links": []
    },
    "MAHOUT-186": {
        "Key": "MAHOUT-186",
        "Summary": "Classifier PriorityQueue returns erroneous results",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1,                                            0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "07/Oct/09 12:32",
        "Updated": "19/Oct/09 00:06",
        "Resolved": "19/Oct/09 00:06",
        "Description": "A simple test fails \nimport org.apache.hadoop.util.PriorityQueue;\nPriorityQueue<ClassifierResult> queue = new ClassifierResultPriorityQueue(3);\n    queue.insert(new ClassifierResult(\"label1\", 5));\n    queue.insert(new ClassifierResult(\"label2\", 4));\n    queue.insert(new ClassifierResult(\"label3\", 3));\n    queue.insert(new ClassifierResult(\"label4\", 2));\n    queue.insert(new ClassifierResult(\"label5\", 1));\n    assertEquals(\"Incorrect Size\", 3, queue.size());\n    log.info(queue.pop().toString());\n    log.info(queue.pop().toString());\n    log.info(queue.pop().toString());\n09/10/07 16:58:39 INFO common.ClassifierResultPriorityQueueTest: ClassifierResult\n{category='label3', score=3.0}\n09/10/07 16:58:39 INFO common.ClassifierResultPriorityQueueTest: ClassifierResult\n{category='label4', score=2.0}\n09/10/07 16:58:39 INFO common.ClassifierResultPriorityQueueTest: ClassifierResult\n{category='label5', score=1.0}\n\nExpected label1 and label2 at the top",
        "Issue Links": []
    },
    "MAHOUT-187": {
        "Key": "MAHOUT-187",
        "Summary": "RandomUtils>>isNotPrime throws IllegalArgumentException when argument is less than 2.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Emerson Murphy-HIll",
        "Created": "16/Oct/09 15:43",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "19/Oct/09 14:44",
        "Description": "I was using a FastMap, which happened to be empty, and rehash() was called.  Rehash eventually calls isNotPrime where n=1, which throws an illegal argument exception:\n  public static boolean isNotPrime(int n) {\n    if (n < 2) \n{\n      throw new IllegalArgumentException();\n    }\n\nIt seems to me anything less than two is not prime.  Thus, I suggest:\n  public static boolean isNotPrime(int n) {\n    if (n < 2) \n{\n    \treturn true;\n    }",
        "Issue Links": []
    },
    "MAHOUT-188": {
        "Key": "MAHOUT-188",
        "Summary": "Cleanup of Bayes/CBayes for 0.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "19/Oct/09 19:18",
        "Updated": "19/Oct/09 22:30",
        "Resolved": "19/Oct/09 22:29",
        "Description": "Bring alpha smoothing as a configurable parameter to the cli.\nCleanup javadocs and insert links to the wiki. \nAdd more description in CLI. \nFormatting and other tidying work",
        "Issue Links": []
    },
    "MAHOUT-189": {
        "Key": "MAHOUT-189",
        "Summary": "Standardize use of assert keyword",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "27/Oct/09 12:18",
        "Updated": "21/May/11 03:24",
        "Resolved": "01/Nov/09 19:35",
        "Description": "This is another one of my concept issues, and I'm posting this for discussion. It can be shot down or modified.\nI believe the assert keyword is for declaring conditions that can't be false in a bug-free program. They are checks that are enabled during development \u2013 to find out when the program isn't bug-free! \u2013 but disabled in production, for performance. Since, of course, they'll always be true right?\nSo, in particular, they are never for checking arguments to a non-private method. I see a number of cases where asserts are used for argument checking. This is bad, since these checks will be disabled in production!\nIf this is a reasonable position, then I volunteer to review all uses of assert and change many to some kind of if-statement plus IllegalArgumentException.",
        "Issue Links": []
    },
    "MAHOUT-190": {
        "Key": "MAHOUT-190",
        "Summary": "Make all instance fields private",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "27/Oct/09 12:25",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/Nov/09 15:07",
        "Description": "This one may be more controversial but is useful and interesting enough to discuss.\nI personally believe instance fields should always be private. I think the pro- and con- debate goes like this:\nMaking all fields private increases encapsulation. Fields must be made explicitly accessible via getters and setters, which is good \u2013 default to hiding, rather than exposing. Not-hiding a field amounts to committing it to be a part of the API, which is rarely intended. Using getters/setters allows read/write access to be independently controlled and even allowed \u2013 allows for read-only 'fields'. Getters/setters establish an API independent from the representation which is a Good Thing.\nBut don't getters and setters slow things down?\nTrivially. JIT compilers will easily inline one-liners. Making fields private more readily allows fields to be marked final, and these two factors allow for optimizations by (Proguard or) JIT. It could actually speed things up.\nBut isn't it messy to write all those dang getters/setters?\nNot really, and not at all if you use an IDE, which I think we all should be.\nBut sometimes a class needs to share representation with its subclasses.\nYes, and it remains possible with package-private / protected getters and setters. This is IMHO a rare situation anyway, and, the code is far easier to read when fields from a parent don't magically appear, or one doesn't wonder about where else a field may be accessed in subclasses. I also feel like sometimes making a field more visible is a shortcut enabler to some bad design. It usually is a bad smell.\nThoughts on this narrative. Once again I volunteer to implement the consensus.",
        "Issue Links": []
    },
    "MAHOUT-191": {
        "Key": "MAHOUT-191",
        "Summary": "NPE while creating term vectors with an index on a field that does not exist in all the documents",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sushil Bajracharya",
        "Created": "28/Oct/09 08:14",
        "Updated": "21/May/11 03:24",
        "Resolved": "07/Dec/09 07:34",
        "Description": "(based on the message from here: http://www.nabble.com/Creating-Vectors-from-Text-tt24298643.html#a26090263)\nI checked out mahout from trunk and tried to create term frequency vector from a lucene index and ran into this..\n09/10/27 17:36:10 INFO lucene.Driver: Output File: /Users/shoeseal/DATA/luc2tvec.out\n09/10/27 17:36:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n09/10/27 17:36:11 INFO compress.CodecPool: Got brand-new compressor\nException in thread \"main\" java.lang.NullPointerException\n        at org.apache.mahout.utils.vectors.lucene.LuceneIterable$TDIterator.next(LuceneIterable.java:109)\n        at org.apache.mahout.utils.vectors.lucene.LuceneIterable$TDIterator.next(LuceneIterable.java:1)\n        at org.apache.mahout.utils.vectors.io.SequenceFileVectorWriter.write(SequenceFileVectorWriter.java:40)\n        at org.apache.mahout.utils.vectors.lucene.Driver.main(Driver.java:200)\nI am running this from Eclipse (snow leopard with JDK 6), on an index that has field with stored term vectors..\nmy input parameters for Driver are:\n--dir <path>/smallidx/ --output <path>/luc2tvec.out --idField id_field\n --field field_with_TV --dictOut <path>/luc2tvec.dict --max 50  --weight tf\nLuke shows the following info on the fields I am using:\n id_field is indexed, stored, omit norms\n field_with_TV is indexed, tokenized, stored, term vector",
        "Issue Links": []
    },
    "MAHOUT-192": {
        "Key": "MAHOUT-192",
        "Summary": "RMSRecommenderEvaluator does not catch NoSuchItemException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jens Grivolla",
        "Created": "29/Oct/09 19:04",
        "Updated": "21/May/11 03:24",
        "Resolved": "29/Oct/09 19:22",
        "Description": "RMSRecommenderEvaluator does not catch NoSuchItemException, whereas AverageAbsoluteDifferenceRecommenderEvaluator correctly does.\nCrashes and burns pretty much always (except for SlopeOne, ItemAverageRecommender, etc.).",
        "Issue Links": []
    },
    "MAHOUT-193": {
        "Key": "MAHOUT-193",
        "Summary": "SparseVectors created with the wrong cardinality in SparseMatrix",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Julien Nioche",
        "Created": "29/Oct/09 22:31",
        "Updated": "21/May/11 03:23",
        "Resolved": "12/Dec/09 11:32",
        "Description": "the cardinality of the rows was used for creating SParseVectors e.g. in cases where a vector had no non-zero elements",
        "Issue Links": []
    },
    "MAHOUT-194": {
        "Key": "MAHOUT-194",
        "Summary": "Mersenne Twister RNG writes to System.out",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jens Grivolla",
        "Created": "02/Nov/09 18:38",
        "Updated": "21/May/11 03:23",
        "Resolved": "02/Nov/09 22:59",
        "Description": "Some parts of Mahout Taste use the Uncommons Maths Mersenne Twister RNG that writes to System.out.\nWhile the problem really is in Uncommons Maths (no library should ever print to System.out), it unfortunately affects users of Taste.\nProbably needs to be fixed upstream.  If that is not possible, using a forked version might be considered (given that it's under Apache License this should be possible).\nI don't know if other parts of Mahout are affected also.",
        "Issue Links": []
    },
    "MAHOUT-195": {
        "Key": "MAHOUT-195",
        "Summary": "doubt about SlopeOneRecommender",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jens Grivolla",
        "Created": "05/Nov/09 14:17",
        "Updated": "21/May/11 03:24",
        "Resolved": "05/Nov/09 20:51",
        "Description": "Looking through the SlopeOne code in order to make some changes, I am having some doubts about how MemoryDiffStorage handles things.\nIt looks to me like buildAverageDiffs(), or rather processOneUser() inserts the item pairs in the order they appear in userPreferences, as obtained from dataModel.getPreferencesFromUser(userID).\nSo if user A has items (X,Y,Z) we obtain the pairs (X,Y),(X,Z),(Y,Z) and update their averages,\nif user B has items (Z,X,Y) we obtain (Z,X),(Z,Y),(X,Y).\nWhen using getDiff for (Y,Z) it will not look for the (Z,Y) average that user B contributes to, as the average for (Y,Z) is not null.\nUnless we know that preferences are always ordered, e.g. by itemID, this seems like a bug.  I have not found any mention of it being ordered in the documentation of DataModel or PreferenceArray.  If the items are ordered it would seem to be easier to check the order in getDiff(x,y) instead of trying one, then the other.\nP.s.: I tried to ask on mahout-users, but my message never appeared on the list. There might be some kind of filter rejecting the plus sign in my address or something like that, but it's the one where I receive the list messages.",
        "Issue Links": []
    },
    "MAHOUT-196": {
        "Key": "MAHOUT-196",
        "Summary": "bounded values for RecommenderEvaluator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jens Grivolla",
        "Created": "05/Nov/09 18:00",
        "Updated": "21/May/11 03:24",
        "Resolved": "07/Nov/09 13:29",
        "Description": "When evaluating a recommender using RMSRecommenderEvaluator (or some others) on e.g. Netflix data, a recommender gets heavily penalized for predicting values below 1 or above 5 (that are known to be out of the permitted bounds).\nIt seems to me that it makes no sense to change the recommender to avoid those predictions, since an estimated 6 probably has a greater chance to be highly rated than a predicted 5.1.  I therefore propose to allow truncating predictions to those \"legal\" values directly in the evaluator and leave the recommenders unchanged, since it is more of a post-processing step than part of the recommender itself.\nI added those boundaries to the constructor of RMSRecommenderEvaluator and limit estimatedPreference to the allowed range before calculating \"realPref.getValue() - estimatedPreference\" and seem to get slightly better scores.",
        "Issue Links": []
    },
    "MAHOUT-197": {
        "Key": "MAHOUT-197",
        "Summary": "LDADriver: No job jar file set leads to ClassNotFoundException: org.apache.mahout.clustering.lda.LDAMapper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "David Leo Wright Hall",
        "Reporter": "Drew Farris",
        "Created": "10/Nov/09 04:02",
        "Updated": "21/May/11 03:24",
        "Resolved": "07/Dec/09 06:04",
        "Description": "hadoop jar core/target/mahout-core-0.2-SNAPSHOT.joborg.apache.mahout.clustering.lda.LDADriver -i mahout/foo/foo-vectors -o mahout/foo/lda-cluster -w -k 1000 -v 82342 --maxIter 2\n[...]\n09/11/09 22:02:00 WARN mapred.JobClient: No job jar file set.  User\nclasses may not be found. See JobConf(Class) or\nJobConf#setJar(String).\n[...]\n09/11/09 22:02:00 INFO input.FileInputFormat: Total input paths to process : 1\n09/11/09 22:02:01 INFO mapred.JobClient: Running job: job_200911091316_0005\n09/11/09 22:02:02 INFO mapred.JobClient:  map 0% reduce 0%\n09/11/09 22:02:12 INFO mapred.JobClient: Task Id :\nattempt_200911091316_0005_m_000000_0, Status : FAILED\njava.lang.RuntimeException: java.lang.ClassNotFoundException:\norg.apache.mahout.clustering.lda.LDAMapper\n       at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:808)\n       at org.apache.hadoop.mapreduce.JobContext.getMapperClass(JobContext.java:157)\n       at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:532)\n       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)\n       at org.apache.hadoop.mapred.Child.main(Child.java:170)\nCaused by: java.lang.ClassNotFoundException:\norg.apache.mahout.clustering.lda.LDAMapper\n       at java.net.URLClassLoader$1.run(URLClassLoader.java:200)\nCan be fixed by adding the following line to LDADriver after line 299 in r831743:\njob.setJarByClass(LDADriver.class);\n(will attach trivial patch)",
        "Issue Links": []
    },
    "MAHOUT-198": {
        "Key": "MAHOUT-198",
        "Summary": "Cleanup pom, remove lib dependencies, etc.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "11/Nov/09 23:22",
        "Updated": "18/Nov/09 14:05",
        "Resolved": "12/Nov/09 18:42",
        "Description": "This patch cleans up the poms to not do install.  It removes the core/lib directory.  I have published the necessary artifacts to our Mahout Maven repo already, so they should be publicly available.\nSee http://cwiki.apache.org/confluence/display/MAHOUT/ThirdPartyDependencies and http://www.lucidimagination.com/search/document/6f026182150f0f50/dependencies_outside_maven_central_was_oh_joy",
        "Issue Links": []
    },
    "MAHOUT-199": {
        "Key": "MAHOUT-199",
        "Summary": "Parent POM missing in public maven repository",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthias Friedrich",
        "Created": "13/Nov/09 07:32",
        "Updated": "19/Nov/09 09:26",
        "Resolved": "19/Nov/09 08:24",
        "Description": "I wanted to play with Taste and thus created a Maven project that referenced the mahout-core artifact. But since mahout-parent isn't deployed, I couldn't build my project. I had to download the 0.1 release and install the parent POM in my local repository (cd mahout-0.1/maven && mvn install).\nSteps to reproduce:\n$ mvn archetype:create -DgroupId=de.mafr.demo -DartifactId=MahoutDemo\n$ cd MahoutDemo\n$ vi MahoutDemo\nadd dependencies listed below\n$ mvn package\nThe dependency section I added:\n    <dependency>\n      <groupId>org.apache.mahout</groupId>\n      <artifactId>mahout-core</artifactId>\n      <version>0.1</version>\n    </dependency>\nCould you please deploy the parent POM? It would make it a lot easier to play with Mahout/Taste. Thanks in advance for your help!",
        "Issue Links": []
    },
    "MAHOUT-200": {
        "Key": "MAHOUT-200",
        "Summary": "Update information on Mahout site",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.2",
        "Component/s": "Documentation",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "13/Nov/09 16:40",
        "Updated": "18/Nov/09 14:06",
        "Resolved": "18/Nov/09 12:00",
        "Description": "After several people had trouble finding the docs we provide in the wiki, I have created a \"slightly\" updated version of our website. I added a few links to wiki pages that might be of interest to potential Mahout users.\nI have uploaded the updated version to http://people.apache.org/~isabel/site so all of you can have a look. Will commit on Tuesday next week if noone objects.",
        "Issue Links": []
    },
    "MAHOUT-201": {
        "Key": "MAHOUT-201",
        "Summary": "OrderedIntDoubleMapping / SparseVector is unnecessarily slow",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Jake Mannix",
        "Created": "18/Nov/09 07:37",
        "Updated": "21/May/11 03:22",
        "Resolved": "24/Nov/09 17:16",
        "Description": "In the work on MAHOUT-165, I find that while Colt's sparse vector implementation is great from a hashing standpoint (it's memory efficient and fast for random-access), they don't provide anything like the OrderedIntDoublePair - i.e. a vector implementation which is not fast for random access, or out-of-order modification, but is minimally sized memory-wise and blazingly fast for doing read-only dot-products and vector sums (where the latter is read-only on inputs, and is creating new output) with each other, and with DenseVectors.\nThis line of thinking got me looking back at the current SparseVector implementation we have in Mahout, because it is based on an int[] and a double[].  Unfortunately, it's not at all optimized for the cases where it can outperform all other sparse impls:\n\nit should override dot(Vector) and plus(Vector) to check whether the input is a DenseVector or a SparseVector (or, once we have an OpenIntDoubleMap implementation of SparseVector, that case as well), and do specialized operations here.\neven when those particular methods aren't being used, the AllIterator and NonZeroIterator inner classes are very inefficient:\n\t\nminor things like caching the values.numMappings() and values.getIndices in final instance variables in the Iterators\nthe huge performance drain of Element.get() : \n\n public double get() {  return values.get(ind);  } \n\n, which is implemented as a binary search on index values array (the index of which was already known!) followed by the array lookup\n\n\n\nThis last point is probably the entire reason why we've seen performance problems with the SparseVector, as it's in both the NonZeroIterator and the AllIterator, and so turned any O(numNonZeroElements) operations into O(numNonZeroElements * log(numNonZeroElements)) (with some additional constant factors for too much indirection thrown in for good measure).\nUnless there is another JIRA ticket which has a patch fixing this which I didn't notice, I can whip up a patch (I've got a similar implementation over in decomposer I can pull stuff out of, although mine is simpler because it is immutable, so it's not just a copy and paste).\nWe don't have any benchmarking code anywhere yet, do we?  Is there a JIRA ticket open for that already?",
        "Issue Links": []
    },
    "MAHOUT-202": {
        "Key": "MAHOUT-202",
        "Summary": "Make Taste support HBase as data store",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Jeff Zhang",
        "Created": "18/Nov/09 07:44",
        "Updated": "03/Oct/12 22:54",
        "Resolved": "22/Dec/09 13:21",
        "Description": "I'd like to add hbase as another data store option for taste.",
        "Issue Links": []
    },
    "MAHOUT-203": {
        "Key": "MAHOUT-203",
        "Summary": "JWriterVectorWriter doesn't flush, content not necessarily written to disk.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Flo Leibert",
        "Created": "19/Nov/09 05:27",
        "Updated": "21/May/11 03:23",
        "Resolved": "03/Jan/10 16:10",
        "Description": "Just needs to flush before returning the number of vectors written...",
        "Issue Links": []
    },
    "MAHOUT-204": {
        "Key": "MAHOUT-204",
        "Summary": "Better integration of Mahout matrix capabilities with Colt Matrix additions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "23/Nov/09 15:44",
        "Updated": "21/May/11 03:23",
        "Resolved": "03/Jan/10 15:59",
        "Description": "Per MAHOUT-165, we need to refactor the matrix package structures a bit to be more coherent and clean.  For instance, there are two levels of matrix packages now, so those should be rectified.",
        "Issue Links": [
            "/jira/browse/MAHOUT-165"
        ]
    },
    "MAHOUT-205": {
        "Key": "MAHOUT-205",
        "Summary": "Pull Writable (and anything else hadoop dependent) out of the matrix module",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "23/Nov/09 18:29",
        "Updated": "21/May/11 03:24",
        "Resolved": "13/Jan/10 08:03",
        "Description": "Vector and Matrix extend Writable, and while that was merely poorly coupled before, it will be an actual problem now that we have a separate submodule for matrix: this module should not depend on hadoop at all, ideally.   Distributed matrix work, as well as simply Writable wrappers can go somewhere else (where?  core?  yet another submodule which depends on matrix?), but it would be really nice if we could produce an artifact which doesn't require Hadoop which has our core linear primitives.",
        "Issue Links": []
    },
    "MAHOUT-206": {
        "Key": "MAHOUT-206",
        "Summary": "Separate and clearly label different SparseVector implementations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "23/Nov/09 18:39",
        "Updated": "21/May/11 03:24",
        "Resolved": "14/Jan/10 17:52",
        "Description": "Shashi's last patch on MAHOUT-165 swapped out the int/double parallel array impl of SparseVector for an OpenIntDoubleMap (hash-based) one.  We actually need both, as I think I've mentioned a gazillion times.\nThere was a patch, long ago, on MAHOUT-165, in which Ted had OrderedIntDoubleVector, and OpenIntDoubleHashVector (or something to that effect), and neither of them are called SparseVector.  I like this, because it forces people to choose what kind of SparseVector they want (and they should: sparse is an optimization, and the client should make a conscious decision what they're optimizing for).  \nWe could call them RandomAccessSparseVector and SequentialAccessSparseVector, to be really obvious.\nBut really, the important part is we have both.",
        "Issue Links": []
    },
    "MAHOUT-207": {
        "Key": "MAHOUT-207",
        "Summary": "AbstractVector.hashCode() should not care about the order of iteration over elements",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Jake Mannix",
        "Created": "23/Nov/09 19:19",
        "Updated": "21/May/11 03:24",
        "Resolved": "21/Jan/10 22:43",
        "Description": "As was discussed in MAHOUT-165, hashCode can be implemented simply like this:\n\n \npublic int hashCode() {\n    final int prime = 31;\n    int result = prime + ((name == null) ? 0 : name.hashCode());\n    result = prime * result + size();\n    Iterator<Element> iter = iterateNonZero();\n    while (iter.hasNext()) {\n      Element ele = iter.next();\n      long v = Double.doubleToLongBits(ele.get());\n      result += (ele.index() * (int)(v^(v>>32)));\n    }\n    return result;\n  }\n\n\nwhich obviates the need to sort the elements in the case of a random access hash-based implementation.  Also, (ele.index() * (int)(v^(v>>32)) ) == 0 when v = Double.doubleToLongBits(0d), which avoids the wrong hashCode() for sparse vectors which have zero elements returned from the iterateNonZero() iterator.",
        "Issue Links": []
    },
    "MAHOUT-208": {
        "Key": "MAHOUT-208",
        "Summary": "Vector.getLengthSquared() is dangerously optimized",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "24/Nov/09 19:06",
        "Updated": "21/May/11 03:24",
        "Resolved": "28/Jan/10 06:16",
        "Description": "SparseVector and DenseVector both cache the value of lengthSquared, so that subsequent calls to it get the cached value.  Great, except the cache is never cleared - calls to set/setQuick or assign or anything, all leave the cached value unchanged.  \nMutating method calls should set lengthNorm to -1 so that the cache is cleared.\nThis could be a really nasty bug if hit.",
        "Issue Links": []
    },
    "MAHOUT-209": {
        "Key": "MAHOUT-209",
        "Summary": "Add aggregate() methods for Vector",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "24/Nov/09 19:49",
        "Updated": "21/May/11 03:23",
        "Resolved": "28/Jan/10 06:15",
        "Description": "As discussed in MAHOUT-165 at some point, Vector (and Matrix, but let's put that on a separate ticket) could do with a nice exposure of methods like the following:\n\n// this can get optimized, of course\n\n  public double aggregate(Vector other, BinaryFunction aggregator, BinaryFunction combiner) {\n    double result = 0;\n    for(int i=0; i<size(); i++) {\n      result = aggregator.apply(result, combiner.apply(getQuick(i), other.getQuick(i)));\n    }\n    return result;\n  }\n\n\nthis is good for generalized inner products and distances.  Also nice:\n\n  public double aggregate(BinaryFunction aggregator, UnaryFunction map) {\n    double result = 0;\n    for(int i=0; i<size(); i++) {\n      result = aggregator.apply(result, map.apply(getQuick(i)) );\n    }\n    return result;\n  }\n\n\nWhich generalizes norms and statistics (mean, median, stdDev) and things like that (number of positive values, or negative values, etc...).\nThese kind of thing exists in Colt, and we could just surface it up to the top.",
        "Issue Links": []
    },
    "MAHOUT-210": {
        "Key": "MAHOUT-210",
        "Summary": "Publish code quality reports through maven",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1,                                            0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Documentation",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "28/Nov/09 10:34",
        "Updated": "21/May/11 03:24",
        "Resolved": "18/Dec/09 09:17",
        "Description": "We should use mvn site:site to generate code reports and publish them online for users to review and developers to easily spot problems.\nFirst version that still needs checks adjusted to our needs is available online at:\nhttp://people.apache.org/~isabel/mahout_site/mahout-core/project-reports.html\nFurther discussion on-list at\nhttp://www.lucidimagination.com/search/document/a13aa5127b47fda3/publish_code_quality_reports_on_web_site##a13aa5127b47fda3",
        "Issue Links": []
    },
    "MAHOUT-211": {
        "Key": "MAHOUT-211",
        "Summary": "DenseMatrix.getRow() and getColumn() return deep copies of the data",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "01/Dec/09 23:28",
        "Updated": "21/May/11 03:24",
        "Resolved": "21/Jan/10 13:16",
        "Description": "As mentioned in the summary, DenseMatrix.getRow() returns a new DenseVector(values[row]) instead of just a shallow reference.  This is a) wasteful, and b) inconsistent with what SparseRowMatrix / SparseColumnMatrix do, which means that users who think they can mutate rows by calling getRow() are wrong for DenseMatrix, or conversely think they can modify the row at will are wrong for SparseRowMatrix.  \nEither way, we need to be consistent.  I would suggest the contract be that you get a shallow view reference (meaning, it may be an actual reference to a real Vector, or it may be a virtual Vector, dynamically constructed, which is still backed by entries in the matrix), so that calling Matrix.getRow(row).set(col, value) has the same affect as Matrix.set(row, col, value).  If we don't do it this way, we should at least provide an api to get a shallow reference to rows.\nThoughts?",
        "Issue Links": []
    },
    "MAHOUT-212": {
        "Key": "MAHOUT-212",
        "Summary": "Need random sampler for use in reducers",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Ted Dunning",
        "Created": "07/Dec/09 05:22",
        "Updated": "21/May/11 03:24",
        "Resolved": "10/Dec/09 19:12",
        "Description": "For a variety of mining algorithms, it helps to have a uniform way to only process a sub-set of the records in a reducer.\nAs such, I have written a simple generic sampler that filters an Iterator returning a fair sample of at most a specified size.",
        "Issue Links": []
    },
    "MAHOUT-213": {
        "Key": "MAHOUT-213",
        "Summary": "storeMapping should not been called when toLongID() is called",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jeff Zhang",
        "Created": "08/Dec/09 07:57",
        "Updated": "21/May/11 03:23",
        "Resolved": "08/Dec/09 12:19",
        "Description": "In the trunk, storeMapping is called always when toLongID() is called. In my opinion storeMapping should been called only in method initialize().\nstoreMapping will cost a lot when you use database to store the id mapping. I believe the code should like this:\n\n  public void initialize(Iterable<String> stringIDs) throws TasteException {\n    for (String stringID : stringIDs) {\n      long longID = hash(stringID);\n      storeMapping(longID, stringID);\n    }\n  }",
        "Issue Links": []
    },
    "MAHOUT-214": {
        "Key": "MAHOUT-214",
        "Summary": "Implement Stacked RBM",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.2",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Prasen Mukherjee",
        "Created": "08/Dec/09 09:16",
        "Updated": "21/May/11 03:27",
        "Resolved": "04/Jan/11 14:55",
        "Description": "Implement Stacked RBM based models for generic data.  Suggested paper is :  http://www.cs.toronto.edu/~rsalakhu/papers/semantic_final.pdf For a more indepth understanding of  Stacked RBMs refer to : http://www.cs.toronto.edu/~hinton/papers.html\nhttp://jarbm.sourceforge.net/ could be a good starting point.",
        "Issue Links": []
    },
    "MAHOUT-215": {
        "Key": "MAHOUT-215",
        "Summary": "Provide jars with mahout release.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Drew Farris",
        "Created": "09/Dec/09 13:58",
        "Updated": "02/Feb/10 06:00",
        "Resolved": "28/Jan/10 07:53",
        "Description": "The 0.2 release of mahout does not include pre-built jars for the various mahout modules. An end user must be able to build the jars using maven or figure out how to retrieve them from the maven repository, which can be easy or difficult depending on the build tool being used. It would be convenient to provide binary jars in future mahout releases.\nOne approach for doing this is described \"Maven: the Definitive Guide\", available on the web here: http://www.sonatype.com/books/maven-book/reference/assemblies-set-dist-assemblies.html\nThe problem with the current project structure is described succinctly in the second paragraph of this page. The dependencies are structured in such a way so that the top-level mahout project is always built before core, utils, examples, etc, so that it is not a simple matter of adding or modifying the assembly of this project to include the child artifacts.\nFollowing the approach described in the resource above, a new module would be created below the top-level mahout project whose sole purpose is to bundle the distributions. This module would depend on all other projects that would be included in the distribution and thus be build only after all of the other modules are built. This assembly would wrap together both the sources and binary artifacts for each of the modules included in the release.\nAdditionally, the release profile can be removed from the top level pom and the artifacts produced by the project assembly descriptor would no longer be needed.",
        "Issue Links": []
    },
    "MAHOUT-216": {
        "Key": "MAHOUT-216",
        "Summary": "Improve the results of MAHOUT-145 by uniformly distributing the classes in the partitioned data",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "10/Dec/09 19:11",
        "Updated": "21/May/11 03:23",
        "Resolved": "14/Jan/10 09:17",
        "Description": "the poor results of the partial decision forest implementation may be explained by the particular distribution of the partitioned data. For example, if a partition does not contain any instance of a given class, the decision trees built using this partition won't be able to classify this class. \nAccording to [CHAN, 95]:\n\nRandom Selection of the partitioned data sets with a uniform distribution of classes is perhaps the most sensible solution. Here we may attempt to maintain the same frequency distribution over the ''class attribute\" so that each partition represents a good but a smaller model of the entire training set\n[CHAN, 95]: Philip K. Chan, \"On the Accuracy of Meta-learning for Scalable Data Mining\"",
        "Issue Links": []
    },
    "MAHOUT-217": {
        "Key": "MAHOUT-217",
        "Summary": "Tidy up generated data after unit tests are run",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "11/Dec/09 13:45",
        "Updated": "21/May/11 03:24",
        "Resolved": "21/Jan/10 19:53",
        "Description": "I tried to compile Mahout on people.apache.org yesterday: The build failed at first, because tests could not generate test data. The reason: Some tests tried to generate test data at /tmp/<mahout-dir>/... - but those directories did exist already and belonged to Sean. Why? Probably because Sean had run the build earlier this year - but tests did not remove the data they generated.\nProposed solution: Tests come with setup and with shutdown hooks. We should remove any data when a test is finished and shut down.\nAny thoughts?",
        "Issue Links": []
    },
    "MAHOUT-218": {
        "Key": "MAHOUT-218",
        "Summary": "Update to Junit 4.5",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.3",
        "Component/s": "Integration",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "11/Dec/09 16:29",
        "Updated": "21/May/11 03:24",
        "Resolved": "11/Dec/09 19:13",
        "Description": "Junit 4.5 is back-compatible with the current 3.x, and allows using the new @annotation scheme. Since the compiler level is set to 1.6, this seems harmless, and I for one would rather write tests in the new pattern.",
        "Issue Links": []
    },
    "MAHOUT-219": {
        "Key": "MAHOUT-219",
        "Summary": "Unit test for GenericSorting",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "11/Dec/09 17:01",
        "Updated": "21/May/11 03:23",
        "Resolved": "11/Dec/09 19:23",
        "Description": "Just to check expectations, here is a patch with just one unit test.",
        "Issue Links": []
    },
    "MAHOUT-220": {
        "Key": "MAHOUT-220",
        "Summary": "Mahout Bayes Code cleanup",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "13/Dec/09 15:01",
        "Updated": "21/May/11 03:23",
        "Resolved": "05/Feb/10 09:38",
        "Description": "Following isabel's checkstyle, I am adding a whole slew of code cleanup with the following exceptions\n1.  Line length used is 120 instead of 80. \n2.  static final log is kept as is. not LOG.",
        "Issue Links": [
            "/jira/browse/MAHOUT-233"
        ]
    },
    "MAHOUT-221": {
        "Key": "MAHOUT-221",
        "Summary": "Implementation of FP-Bonsai Pruning for fast pattern mining",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "13/Dec/09 15:14",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "05/Feb/10 11:02",
        "Description": "FP Bonsai is a method to prune long chained FP-Trees for faster growth. \nhttp://win.ua.ac.be/~adrem/bibrem/pubs/fpbonsai.pdf\nThis implementation also adds a transaction preprocessing map/reduce job which converts a list of transactions \n{1, 2, 4, 5}\n, \n{1, 2, 3}\n, \n{1, 2}\n into a tree structure and thus saves space during fpgrowth map/reduce \nthe tree formed from above is. For typical this improves the storage space by a great amount and thus saves on time during shuffle and sort\n(1,3) -> (2,3) | - (4,1) - (5,1)\n                      (3,1)        \nAlso added a reducer to PFPgrowth (not part of the original paper) which does this compression and saves on space. \nThis patch also adds an example transaction dataset generator from flickr and delicious data set https://www.uni-koblenz.de/FB4/Institutes/IFI/AGStaab/Research/DataSets/PINTSExperimentsDataSets/\nBoth of them are GIG of tag data. Where \"date userid itemid tag\" is given. The example maker creates a transaction based on all the unique tags a user has tagged on an item.",
        "Issue Links": []
    },
    "MAHOUT-222": {
        "Key": "MAHOUT-222",
        "Summary": "Colt collections work in progress",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "14/Dec/09 02:49",
        "Updated": "21/May/11 03:23",
        "Resolved": "15/Dec/09 10:36",
        "Description": "I am attaching a patch that shows my current work state on polishing the colt collections code. My primary target is the int-int hash map, but I've also started work on extending it to an int-char hash map. No one should even think about committing this yet, I'm posting so that people can see what I'm up to.",
        "Issue Links": []
    },
    "MAHOUT-223": {
        "Key": "MAHOUT-223",
        "Summary": "Infinite loop in Vector.haveSharedCells(Vector)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jake Mannix",
        "Created": "15/Dec/09 04:57",
        "Updated": "21/May/11 03:24",
        "Resolved": "22/Dec/09 13:49",
        "Description": "SparseVector:\n\n  @Override\n  public boolean haveSharedCells(Vector other) {\n    if (other instanceof SparseVector) {\n      return other == this;\n    } else {\n      return other.haveSharedCells(this);\n    }\n  }\n\n\nDenseVector:\n\n  @Override\n  public boolean haveSharedCells(Vector other) {\n    if (other instanceof DenseVector) {\n      return other == this;\n    } else {\n      return other.haveSharedCells(this);\n    }\n  }\n\n\nI think the result is self-explanatory.\nI'd just check a fix in, but I don't have an svn account yet.  Actually, no I wouldn't because I'm not exactly sure what this method is supposed to do - it's clearly for VectorView instances somehow... but what about Sparse and Dense versions on the same vector (same meaning strictEquivalent() returns true for them)?",
        "Issue Links": []
    },
    "MAHOUT-224": {
        "Key": "MAHOUT-215 Provide jars with mahout release.",
        "Summary": "Dependency Cleanup",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "15/Dec/09 05:08",
        "Updated": "21/May/11 03:24",
        "Resolved": "22/Dec/09 21:04",
        "Description": "In preparation for the binary release work described in MAHOUT-215, here's a minor patch that does some some cleanup on the poms. \nThe hadoop and junit dependency versions are now established using the dependencyManagement section of the parent pom in mahout/maven/pom.xml\nA large number of transitive dependencies from the hadoop pom are now excluded there as well \u2013 these were not necessary previously because the hadoop dependency was hand-rolled and did not include them. With the update to the hadoop 0.20.2-SNAPSHOT, they now become required.\nAlso, the parent pom no longer has mahout/pom.xml as its parent, this allows binary packaging to be performed in mahout/pom.xml after the build of all of the other sub-modules is complete.\nAlso, removed the javamail dependency \u2013 was there a reason this was present?\nVerified that build and unit tests complete.",
        "Issue Links": []
    },
    "MAHOUT-225": {
        "Key": "MAHOUT-225",
        "Summary": "Rename mahout-matrix to mahout-math",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "15/Dec/09 15:01",
        "Updated": "21/May/11 03:24",
        "Resolved": "17/Dec/09 23:23",
        "Description": "Per discussion on the dev list, this patch renames:\no.a.m.matrix -> o.a.m.math\no.a.m.jet -> o.a.m.math.jet\nThings currently under o.a.m.matrix would retain their existing names,\nwith s/matrix/math, for example:\no.a.m.math.bitvector\no.a.m.math.buffer\no.a.m.math.function\no.a.m.math.list\no.a.m.math.map\no.a.m.math.matrix\nThe maven submodule is changed from mahout-matrix to mahout-math\nApply this patch with -E to delete empty files, and then 'svn delete matrix' and 'svn add math' \u2013 I couldn't figure out how to get patch to set that up automatically.\nVerified that patch applies cleanly to r890796, build completes and unit tests work.",
        "Issue Links": []
    },
    "MAHOUT-226": {
        "Key": "MAHOUT-226",
        "Summary": "Velocity-based code generation support to support more primitive type collections",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "19/Dec/09 02:17",
        "Updated": "21/May/11 03:24",
        "Resolved": "22/Dec/09 14:56",
        "Description": "We want the complete set of hash maps on the primitive types. The code can be generated. So here's a code generator to do it, and, to show it off, the TypeTypeFunction.java family set up to be generated.",
        "Issue Links": []
    },
    "MAHOUT-227": {
        "Key": "MAHOUT-227",
        "Summary": "Parallel SVM",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.2",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "zhao zhendong",
        "Created": "20/Dec/09 05:37",
        "Updated": "02/May/13 02:29",
        "Resolved": "22/Sep/10 07:16",
        "Description": "I wrote a proposal of parallel algorithm for SVM training. Any comment is welcome.",
        "Issue Links": [
            "/jira/browse/MAHOUT-232",
            "/jira/browse/MAHOUT-232"
        ]
    },
    "MAHOUT-228": {
        "Key": "MAHOUT-228",
        "Summary": "Need sequential logistic regression implementation using SGD techniques",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "23/Dec/09 20:00",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "20/Aug/10 06:22",
        "Description": "Stochastic gradient descent (SGD) is often fast enough for highly scalable learning (see Vowpal Wabbit, http://hunch.net/~vw/).\nI often need to have a logistic regression in Java as well, so that is a reasonable place to start.",
        "Issue Links": [
            "/jira/browse/MAHOUT-309",
            "/jira/browse/MAHOUT-479",
            "/jira/browse/MAHOUT-262"
        ]
    },
    "MAHOUT-229": {
        "Key": "MAHOUT-229",
        "Summary": "Clean up and templatize the primitive list containers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "23/Dec/09 21:50",
        "Updated": "21/May/11 03:23",
        "Resolved": "03/Jan/10 17:04",
        "Description": "Clean up the code in org.apache.mahout.math.list, including some testing and cleanup of some things that it calls.\nThis includes:\n1) More features in the Velocity code generator, including code to avoid generating a shadow of a checked-in file, and code to allow explicit suppression of individual outputs. Also an extra macro or two to make the template neater in some cases.\n2) The stack of code from AbstractList to AbstractXxxList to XxxArrayList generated with templates. AbstractCollection eliminated. ObjectArrayList preserved for now until the maps can be weaned from it.\n3) Removed 2 broken functions from the Sorting class.",
        "Issue Links": []
    },
    "MAHOUT-230": {
        "Key": "MAHOUT-230",
        "Summary": "Replace org.apache.mahout.math.Sorting with code of clear provenance",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Benson Margulies",
        "Created": "27/Dec/09 03:07",
        "Updated": "21/May/11 03:23",
        "Resolved": "31/Dec/09 20:38",
        "Description": "org.apache.mahout.math.Sorting looks as if the original author borrowed from the Sun JRE, based on the private internal function names and contents. That code has a restrictive license. We need to take the equivalent file (java.util.Arrays) from Apache Harmony and use it as the basis for a clean replacement.\nThe problematic code are the quickSort and mergeSort functions, which extend 'Arrays' by supporting slices of arrays and custom sorting predicate functions. \nOne might also wistfully note that the more recent JDKs from Sun have deployed different (and one hopes) better sort algorithms that 1.5 and/or Harmony, so a really energetic person might build implementations in here to match. However, expediency calls for just bashing on the Harmony implementation to solve the problem at hand.",
        "Issue Links": []
    },
    "MAHOUT-231": {
        "Key": "MAHOUT-231",
        "Summary": "Upgrade QM reports to use Clover 2.6",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Documentation",
        "Assignee": "Robin Anil",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "27/Dec/09 15:00",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "26/May/10 18:53",
        "Description": "Atlassian has donated a license for a new Clover version. The reports provide more information and are easier to read. We should upgrade to site reports to use that version.",
        "Issue Links": []
    },
    "MAHOUT-232": {
        "Key": "MAHOUT-232",
        "Summary": "Implementation of sequential SVM solver based on Pegasos",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "zhao zhendong",
        "Created": "28/Dec/09 04:41",
        "Updated": "02/May/13 02:29",
        "Resolved": "09/Feb/11 10:54",
        "Description": "After discussed with guys in this community, I decided to re-implement a Sequential SVM solver based on Pegasos  for Mahout platform (mahout command line style,  SparseMatrix and SparseVector etc.) , Eventually, it will support HDFS. \nSequential SVM based on Pegasos.\nMaxim zhao (zhaozhendong at gmail dot com)\n-------------------------------------------------------------------------------------------\nCurrently, this package provides (Features):\n-------------------------------------------------------------------------------------------\n1. Sequential SVM linear solver, include training and testing.\n2. Support general file system and HDFS right now.\n3. Supporting large-scale data set training.\nBecause of the Pegasos only need to sample certain samples, this package supports to pre-fetch\nthe certain size (e.g. max iteration) of samples to memory.\nFor example: if the size of data set has 100,000,000 samples, due to the default maximum iteration is 10,000,\nas the result, this package only random load 10,000 samples to memory.\n4. Sequential Data set testing, then the package can support large-scale data set both on training and testing.\n5. Supporting parallel classification (only testing phrase) based on Map-Reduce framework.\n6. Supoorting Multi-classfication based on Map-Reduce framework (whole parallelized version).\n7. Supporting Regression.\n-------------------------------------------------------------------------------------------\nTODO:\n-------------------------------------------------------------------------------------------\n1. Multi-classification Probability Prediction\n2. Performance Testing\n-------------------------------------------------------------------------------------------\nUsage:\n-------------------------------------------------------------------------------------------\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nClassification:\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ Training: @@\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nSVMPegasosTraining.java\nThe default argument is:\n-tr ../examples/src/test/resources/svmdataset/train.dat -m ../examples/src/test/resources/svmdataset/SVM.model\n~~~~~~~~~~~~~~~~~~~~~~\n@ For the case that training data set on HDFS:@\n~~~~~~~~~~~~~~~~~~~~~~\n1 Assure that your training data set has been submitted to hdfs\nhadoop-work-space# bin/hadoop fs -ls path-of-train-dataset\n2 revise the argument:\n-tr /user/hadoop/train.dat -m ../examples/src/test/resources/svmdataset/SVM.model -hdfs hdfs://localhost:12009\n~~~~~~~~~~~~~~~~~~~~~~\n@ Multi-class Training [Based on MapReduce Framework]:@\n~~~~~~~~~~~~~~~~~~~~~~\nbin/hadoop jar mahout-core-0.3-SNAPSHOT.job org.apache.mahout.classifier.svm.ParallelAlgorithms.ParallelMultiClassifierTrainDriver -if /user/maximzhao/dataset/protein -of /user/maximzhao/protein -m /user/maximzhao/proteinmodel -s 1000000 -c 3 -nor 3 -ms 923179 -mhs -Xmx1000M -ttt 1080\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n@@ Testing: @@\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nSVMPegasosTesting.java\nI have hard coded the arguments in this file, if you want to custom the arguments by youself, please uncomment the first line in main function.\nThe default argument is:\n-te ../examples/src/test/resources/svmdataset/test.dat -m ../examples/src/test/resources/svmdataset/SVM.model\n~~~~~~~~~~~~~~~~~~~~~~\n@ Parallel Testing (Classification): @\n~~~~~~~~~~~~~~~~~~~~~~\nParallelClassifierDriver.java\nbin/hadoop jar mahout-core-0.3-SNAPSHOT.job org.apache.mahout.classifier.svm.ParallelAlgorithms.ParallelClassifierDriver -if /user/maximzhao/dataset/rcv1_test.binary -of /user/maximzhao/rcv.result -m /user/maximzhao/rcv1.model -nor 1 -ms 241572968 -mhs -Xmx500M -ttt 1080\n~~~~~~~~~~~~~~~~~~~~~~\n@ Parallel multi-classification: @\n~~~~~~~~~~~~~~~~~~~~~~\nbin/hadoop jar mahout-core-0.3-SNAPSHOT.job org.apache.mahout.classifier.svm.ParallelAlgorithms.ParallelMultiClassPredictionDriver -if /user/maximzhao/dataset/protein.t -of /user/maximzhao/proteinpredictionResult -m /user/maximzhao/proteinmodel -c 3 -nor 1 -ms 2226917 -mhs -Xmx1000M -ttt 1080\nNote: the parameter -ms 241572968 is obtained by equation : ms = input files size / number of mapper.\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nRegression: \n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nSVMPegasosTraining.java\n-tr ../examples/src/test/resources/svmdataset/abalone_scale -m ../examples/src/test/resources/svmdataset/SVMregression.model -s 1\n-------------------------------------------------------------------------------------------\nExperimental Results:\n-------------------------------------------------------------------------------------------\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nClasssification:\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nData set:\nname\t          source\t    type\tclass\ttraining size\ttesting size\tfeature\n-----------------------------------------------------------------------------------------------\nrcv1.binary\t [DL04b]\tclassification\t2\t   20,242\t  677,399\t47,236\ncovtype.binary\t  UCI\t        classification\t2\t  581,012\t\t         54\na9a               UCI           classification\t2          32,561\t   16,281\t123\nw8a\t         [JP98a]\tclassification\t2\t   49,749\t   14,951\t300\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nData set                 |        Accuracy         |       Training Time      |    Testing Time     |\nrcv1.binary              |          94.67%         |         19 Sec           |     2 min 25 Sec    |\ncovtype.binary           |                         |         19 Sec           |                     |\na9a                      |          84.72%         |         14 Sec           |     12 Sec          |\nw8a                      |          89.8 %         |         14 Sec           |     8  Sec          |\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nParallel Classification (Testing)\nData set                 |        Accuracy         |       Training Time      |    Testing Time            |\nrcv1.binary              |          94.98%         |         19 Sec           |     3 min 29 Sec (one node)|\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nParallel Multi-classification Based on MapReduce Framework:\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nData set:\nname\t  |        source\t    | type\t| class\t| training size\t| testing size\t| feature\n-----------------------------------------------------------------------------------------------\npoker\t| UCI\t| classification\t| 10\t| 25,010\t| 1,000,000\t| 10\nprotein\t | [JYW02a] \t| classification\t| 3\t| 17,766\t| 6,621\t| 357\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nData set                 |        Accuracy  vs. (Libsvm with linear kernel)\npoker | 50.14 %  vs. ( 49.952% ) |\nprotein | 68.14% vs. ( 64.93% ) |\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nRegression:\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nData set:\nname\t|          source\t|    type |\tclass\t| training size |\ttesting size |\tfeature\n-----------------------------------------------------------------------------------------------\nabalone |\tUCI\t| regression\t\t| 4,177\t\t| | 8\ntriazines |\tUCI\t| regression\t\t| 186\t\t| | 60\ncadata\t| StatLib\t| regression\t\t| 20,640\t| | 8\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nData set                 |        Mean Squared error vs. (Libsvm with linear kernel)   |       Training Time      | Test Time |\nabalone | 6.01 vs. (5.25) | 13 Sec |\ntriazines | 0.031  vs. (0.0276) | 14 Sec |\ncadata | 5.61 e +10 vs. (1.40 e+10) | 20 Sec |",
        "Issue Links": [
            "/jira/browse/MAHOUT-227",
            "/jira/browse/MAHOUT-227"
        ]
    },
    "MAHOUT-233": {
        "Key": "MAHOUT-233",
        "Summary": "Modifying Mahout Check style to match our current coding style",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "28/Dec/09 18:57",
        "Updated": "21/May/11 03:23",
        "Resolved": "24/Jan/10 17:54",
        "Description": "Checkstyle currently throws a lot of errors for small things like \nprivate static Logger log \nif(booleanVar == false) // Required for readability \nThis task is created to track \nhow to match the checkstyle to relax certain rules and \nchanges in code for the strict ones",
        "Issue Links": [
            "/jira/browse/MAHOUT-220"
        ]
    },
    "MAHOUT-234": {
        "Key": "MAHOUT-234",
        "Summary": "permission denied error on unit test on clean tree on linux",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Benson Margulies",
        "Created": "28/Dec/09 19:05",
        "Updated": "21/May/11 03:22",
        "Resolved": "28/Dec/09 19:12",
        "Description": "-------------------------------------------------------------------------------\nTest set: org.apache.mahout.classifier.bayes.BayesFileFormatterTest\n-------------------------------------------------------------------------------\nTests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.041 sec <<< FAILURE!\ntestCollapse(org.apache.mahout.classifier.bayes.BayesFileFormatterTest)  Time elapsed: 0.007 sec  <<< ERROR!\njava.io.FileNotFoundException: /tmp/bayes/input/dog (Permission denied)\n        at java.io.FileOutputStream.open(Native Method)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:179)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:131)\n        at org.apache.mahout.classifier.bayes.BayesFileFormatterTest.setUp(BayesFileFormatterTest.java:59)\n        at junit.framework.TestCase.runBare(TestCase.java:132)\n        at junit.framework.TestResult$1.protect(TestResult.java:110)\n        at junit.framework.TestResult.runProtected(TestResult.java:128)\n        at junit.framework.TestResult.run(TestResult.java:113)\n        at junit.framework.TestCase.run(TestCase.java:124)\n        at junit.framework.TestSuite.runTest(TestSuite.java:232)\n        at junit.framework.TestSuite.run(TestSuite.java:227)\n        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)\n        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:62)\n        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:140)\n        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:165)\n        at org.apache.maven.surefire.Surefire.run(Surefire.java:107)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:289)\n        at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1005)",
        "Issue Links": []
    },
    "MAHOUT-235": {
        "Key": "MAHOUT-235",
        "Summary": "GenericSorting.java also needs replacing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Benson Margulies",
        "Created": "31/Dec/09 01:38",
        "Updated": "21/May/11 03:24",
        "Resolved": "05/Jan/10 19:49",
        "Description": "It looks like GenericSorting.java is more code of the same dubious parentage that needs the same treatment.",
        "Issue Links": []
    },
    "MAHOUT-236": {
        "Key": "MAHOUT-236",
        "Summary": "Cluster Evaluation Tools",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Grant Ingersoll",
        "Created": "03/Jan/10 22:02",
        "Updated": "21/May/11 03:19",
        "Resolved": "24/Sep/10 11:43",
        "Description": "Per http://www.lucidimagination.com/search/document/10b562f10288993c/validating_clustering_output#9d3f6a55f4a91cb6, it would be great to have some utilities to help evaluate the effectiveness of clustering.",
        "Issue Links": []
    },
    "MAHOUT-237": {
        "Key": "MAHOUT-237",
        "Summary": "Map/Reduce Implementation of Document Vectorizer",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "05/Jan/10 02:45",
        "Updated": "21/May/11 03:23",
        "Resolved": "05/Feb/10 09:31",
        "Description": "Current Vectorizer uses Lucene Index to convert documents into SparseVectors\nTed is working on a Hash based Vectorizer which can map features into Vectors of fixed size and sum it up to get the document Vector\nThis is a pure bag-of-words based Vectorizer written in Map/Reduce. \nThe input document is in SequenceFile<Text,Text> . with key = docid, value = content\nFirst Map/Reduce over the document collection and generate the feature counts.\nSecond Sequential pass reads the output of the map/reduce and converts them to SequenceFile<Text, LongWritable> where key=feature, value = unique id \n    Second stage should create shards of features of a given split size\nThird Map/Reduce over the document collection, using each shard and create Partial(containing the features of the given shard) SparseVectors \nFourth Map/Reduce over partial shard, group by docid, create full document Vector",
        "Issue Links": []
    },
    "MAHOUT-238": {
        "Key": "MAHOUT-215 Provide jars with mahout release.",
        "Summary": "Further Dependency Cleanup",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "07/Jan/10 03:09",
        "Updated": "21/May/11 03:23",
        "Resolved": "07/Jan/10 18:03",
        "Description": "Further dependency cleanup is required, mainly to set the right hadoop dependency for mahout-math and fix exclusions for the hadoop dependency in the parent pom. Other minor cleanups too.\nThe patch includes the following changes:\nmaven (parent pom)\n\nadded inceptionYear (2008)\nremoved some exclusions for hadoop dependency: avro, commons-codec, commons-httpclient in the dependendy management section.\nremoved javax.mail dependency\n\nmahout-math\n\nswitched from o.a.m.hadoop:hadoop-core dependency to new o.a.hadoop:hadoop-core dependency used in core, version specified in dependencyManagement section of parent pom.\nremoved unnecessary compile scope from gson dependency\n\nmahout-core\n\nremoved: kfs, jets3t, xmlenc, unused, originally added to support old o.a.mahout.hadoop:hadoop-core:0.20.1 dependency\nremoved: commons-httpclient, now added transitively from new o.a.hadoop:hadoop-core:0.20.2-SNAPSHOT dependency\nset slf4j-jcl to test scope.\nremoved: watchmaker-swing, added later in mahout-examples where it is actually used.\nfixed uncommons-maths groupId\nremoved unused lucene-analyzers dependency.\nadded easymock dependencies explicitly\n\nmahout-utils\n\nremoved unused easymock dependencies\n\nmahout-examples\n\nadded watchmaker-framework and watchmaker-swing",
        "Issue Links": []
    },
    "MAHOUT-239": {
        "Key": "MAHOUT-239",
        "Summary": "Complete set of open hash maps with primitive types as both key and value",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "09/Jan/10 22:53",
        "Updated": "21/May/11 03:23",
        "Resolved": "10/Jan/10 20:32",
        "Description": "Here is the template providing the hash map and the test for all the primitive type pairs.",
        "Issue Links": []
    },
    "MAHOUT-240": {
        "Key": "MAHOUT-240",
        "Summary": "Parallel version of Perceptron",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "10/Jan/10 11:18",
        "Updated": "21/May/11 03:27",
        "Resolved": "04/Jan/11 21:14",
        "Description": "So far Perceptron (as well as Winnow) training is still implemented to run w/o parallelization. The goal of this issue is to explore ways for parallelization and if possible to provide a parallel version, that is one that is based on map reduce.",
        "Issue Links": []
    },
    "MAHOUT-241": {
        "Key": "MAHOUT-241",
        "Summary": "Example for perceptron",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "10/Jan/10 11:20",
        "Updated": "21/May/11 03:27",
        "Resolved": "04/Jan/11 21:13",
        "Description": "The goal is to provide an end-to-end example based on the 20-newsgroups dataset to show how to get from a set of labelled training examples to a trained model that can later be reused.",
        "Issue Links": []
    },
    "MAHOUT-242": {
        "Key": "MAHOUT-242",
        "Summary": "LLR Collocation Identifier",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "11/Jan/10 04:35",
        "Updated": "11/Mar/10 02:14",
        "Resolved": "09/Feb/10 17:03",
        "Description": "Identifies interesting Collocations in text using ngrams scored via the LogLikelihoodRatio calculation. \nAs discussed in: \n\nhttp://www.lucidimagination.com/search/document/d051123800ab6ce7/collocations_in_mahout#26634d6364c2c0d2\nhttp://www.lucidimagination.com/search/document/b8d5bb0745eef6e8/n_grams_for_terms#f16fa54417697d8e\n\nCurrent form is a tar of a maven project that depends on mahout. Build as usual with 'mvn clean install', can be executed using:\n\nmvn -e exec:java  -Dexec.mainClass=\"org.apache.mahout.colloc.CollocDriver\" -Dexec.args=\"--input src/test/resources/article --colloc target/colloc --output target/output -w\"\n\n\nOutput will be placed in target/output and can be viewed nicely using:\n\nsort -rn -k1 target/output/part-00000\n\n\nIncludes rudimentary unit tests. Please review and comment. Needs more work to get this into patch state and integrate with Robin's document vectorizer work in MAHOUT-237\nSome basic TODO/FIXME's include:\n\nuse mahout math's ObjectInt map implementation when available\nmake the analyzer configurable\nbetter input validation + negative unit tests.\nmore flexible ways to generate units of analysis (n-1)grams.",
        "Issue Links": []
    },
    "MAHOUT-243": {
        "Key": "MAHOUT-243",
        "Summary": "Next collections step: {key}ObjectOpenHashMap<T> and tests",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "11/Jan/10 23:38",
        "Updated": "21/May/11 03:24",
        "Resolved": "14/Jan/10 00:13",
        "Description": "Here we have all of IntObjectOpenHashMap<T>, etc.",
        "Issue Links": []
    },
    "MAHOUT-244": {
        "Key": "MAHOUT-244",
        "Summary": "Add root log-likelihood method to LogLikehood class.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "14/Jan/10 05:13",
        "Updated": "21/May/11 03:24",
        "Resolved": "14/Jan/10 10:58",
        "Description": "Per discussion at: http://www.lucidimagination.com/search/document/6dc8709e65a7ced1/llr_scoring_question\nThis patch adds a method for root log-likelihood calculation to the existing LogLikelihood class + provides a unit test based on Shashi's numbers.",
        "Issue Links": []
    },
    "MAHOUT-245": {
        "Key": "MAHOUT-245",
        "Summary": "Better handling of Categorical attributes when building Decision Forests",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "14/Jan/10 09:27",
        "Updated": "21/May/11 03:24",
        "Resolved": "24/Jan/10 18:34",
        "Description": "When building a decision tree, at each node a random subset from all variables (attributes) is considered for the node split.\nIf a Categorical variable has been selected, the data available at the node is split such that each child node has the same value for the selected variable. In all sub-nodes the selected variable should not be selected again, but the current implementation does not account for that. The resulting tree may contain redundant nodes that does not impair its classification performance but are nonetheless unnecessary.",
        "Issue Links": []
    },
    "MAHOUT-246": {
        "Key": "MAHOUT-246",
        "Summary": "upgrade to new lucene TokenStream API to cleanup deprecation warnings",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Olivier Grisel",
        "Reporter": "Olivier Grisel",
        "Created": "14/Jan/10 14:42",
        "Updated": "21/May/11 03:24",
        "Resolved": "21/Jan/10 17:36",
        "Description": "The attached patch use the new ts.incrementToken() / TermAttribute API instead of the deprecated manual Token handling.\nIt also replaces to occurrences of the deprecated \"new StandardAnalyzer()\" to the more explicit \"new StandardAnalyzer(Version.LUCENE_CURRENT)\".",
        "Issue Links": []
    },
    "MAHOUT-247": {
        "Key": "MAHOUT-247",
        "Summary": "GenericUserBasedRecommender.recommend causes connection leak when called for user with no preferences",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tolga Oral",
        "Created": "14/Jan/10 17:17",
        "Updated": "21/May/11 03:23",
        "Resolved": "15/Jan/10 00:40",
        "Description": "UserSimilarity userSimilarity = new TanimotoCoefficientSimilarity(getBooleanPrefDataModel());\n\t\t\tUserNeighborhood neighborhood = new NearestNUserNeighborhood(3, userSimilarity, getBooleanPrefDataModel());\n\t\t\tRecommender recommender = new GenericBooleanPrefUserBasedRecommender(getBooleanPrefDataModel(), neighborhood, userSimilarity);\n                        recommender.recommend(userwithnopreferencesdata, 10);\ncode properly throws NoSuchUserException however one of the connections is hang on LongPrimitiveIterator backed by org.apache.mahout.cf.taste.impl.model.jdbc.AbstractJDBCDataModel$ResultSetIDIterator as Exception is thrown before TopItems.getTopUsers finishes the while loop \npublic static long[] getTopUsers(int howMany,\n                                   LongPrimitiveIterator allUserIDs,\n                                   Rescorer<Long> rescorer,\n                                   Estimator<Long> estimator) throws TasteException {\n    Queue<SimilarUser> topUsers = new PriorityQueue<SimilarUser>(howMany + 1, Collections.reverseOrder());\n    boolean full = false;\n    double lowestTopValue = Double.NEGATIVE_INFINITY;\n//HERE IS THE ITERATOR\n    while (allUserIDs.hasNext()) {\n      long userID = allUserIDs.next();\n      if (rescorer != null && rescorer.isFiltered(userID)) \n{\n        continue;\n      }\n\n//EXCEPTION THROWN HERE CAUSES THE CONNECTION LEAK\n      double similarity = estimator.estimate(userID);\n      double rescoredSimilarity = rescorer == null ? similarity : rescorer.rescore(userID, similarity);\n      if (!Double.isNaN(rescoredSimilarity) && (!full || rescoredSimilarity > lowestTopValue)) {\n        topUsers.add(new SimilarUser(userID, similarity));\n        if (full) \n{\n          topUsers.poll();\n        }\n else if (topUsers.size() > howMany) \n{\n          full = true;\n          topUsers.poll();\n        }\n        lowestTopValue = topUsers.peek().getSimilarity();\n      }\n    }\n    if (topUsers.isEmpty()) \n{\n      return NO_IDS;\n    }\n    List<SimilarUser> sorted = new ArrayList<SimilarUser>(topUsers.size());\n    sorted.addAll(topUsers);\n    Collections.sort(sorted);\n    long[] result = new long[sorted.size()];\n    int i = 0;\n    for (SimilarUser similarUser : sorted) \n{\n      result[i++] = similarUser.getUserID();\n    }\n    return result;\n  }\n============================================================================================================\nI currently fixed it in our application by checking first to see if user has preferences for the given dataset (user might exists and have preferences for a different dataset).\nHowever this edge case does not cause issues in some other recommenders as long as we handle the NoSuchUserException.\nEasy solution is to use AbstractJDBCDataModel$ResultSetIDIterator always with try/catch/finally and release the connection.",
        "Issue Links": []
    },
    "MAHOUT-248": {
        "Key": "MAHOUT-248",
        "Summary": "Next collections expansion kit: OpenObjectWhateverHashMap<T>",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "15/Jan/10 01:57",
        "Updated": "21/May/11 03:24",
        "Resolved": "16/Jan/10 17:51",
        "Description": "Here's the next slice.",
        "Issue Links": []
    },
    "MAHOUT-249": {
        "Key": "MAHOUT-249",
        "Summary": "Make WikipediaXmlSplitter able to write the chunks directly to HDFS or S3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": "Olivier Grisel",
        "Reporter": "Olivier Grisel",
        "Created": "15/Jan/10 16:53",
        "Updated": "21/May/11 03:23",
        "Resolved": "22/Jan/10 14:44",
        "Description": "By using the Hadoop FS abstraction it should be possible to avoid writing the chunks on the local hard drive before uploading them to HDFS or S3.",
        "Issue Links": []
    },
    "MAHOUT-250": {
        "Key": "MAHOUT-250",
        "Summary": "Make WikipediaXmlSplitter able to directly use the bzip2 compressed dump as input",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Olivier Grisel",
        "Created": "15/Jan/10 18:08",
        "Updated": "21/May/11 03:23",
        "Resolved": "22/Jan/10 04:41",
        "Description": "Wikipedia.org ships large bzip2 compressed archives hence it would make sense to be able to load the chunked XML into HDFS directly from the original file without having to uncompress a 25GB temporary file on the local hard drive. Reusing the Hadoop BZip2 codecs allows us to avoid having to introduce a new dependency.",
        "Issue Links": []
    },
    "MAHOUT-251": {
        "Key": "MAHOUT-251",
        "Summary": "Generalize Dirichlet models and model distributions to handle n-d and sparse vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "16/Jan/10 01:43",
        "Updated": "21/May/11 03:24",
        "Resolved": "18/Jan/10 19:30",
        "Description": "Users attempting to use Dirichlet Process Clustering on real life problems cannot use any of the existing models or model distributions as these have hard-coded assumptions of a 2-d DenseVector underlying data representation. These limitations are overly restrictive and the code needs to be generatlized.",
        "Issue Links": []
    },
    "MAHOUT-252": {
        "Key": "MAHOUT-252",
        "Summary": "Sets (primitive types)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "16/Jan/10 02:37",
        "Updated": "21/May/11 03:23",
        "Resolved": "16/Jan/10 17:51",
        "Description": "Here come the sets.",
        "Issue Links": []
    },
    "MAHOUT-253": {
        "Key": "MAHOUT-253",
        "Summary": "Proposal for high performance primitive collections.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": "Dawid Weiss",
        "Reporter": "Dawid Weiss",
        "Created": "16/Jan/10 13:51",
        "Updated": "21/May/11 03:22",
        "Resolved": "24/Sep/10 11:51",
        "Description": "A proposal for template-driven collections library (lists, sets, maps, deques), with specializations for Java primitive types to save memory and increase performance. The \"templates\" are regular Java classes written with generics and certain \"intrinsics\", that is blocks replaceable by a regexp-preprocessor. This lets one write the code once, immediately test it (tests are also templates) and generate primitive versions from a single source.\nAn additional interesting part is the benchmarking subsystem written on top of JUnit \nThere are major differences from the Java Collections API, most notably no interfaces and interface-compatible views over sub-collections or key/value sets. These classes also expose their internal implementation (buffers, addressing, etc.) so that the code can be optimized for a particular use case.\nThese motivations are further discussed here, together with an API overview.\nhttp://www.carrot-search.com/download/hppc/index.html\nI am curious what you think about it. If folks like it, Carrot Search will donate the code to Mahout (or Apache Commons-?) and will maintain it (because we plan to use it in our internal projects anyway).",
        "Issue Links": []
    },
    "MAHOUT-254": {
        "Key": "MAHOUT-254",
        "Summary": "Primitive set unit tests",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "16/Jan/10 17:38",
        "Updated": "21/May/11 03:23",
        "Resolved": "18/Jan/10 00:03",
        "Description": "The primitive sets need unit tests.",
        "Issue Links": []
    },
    "MAHOUT-255": {
        "Key": "MAHOUT-255",
        "Summary": "Open hash set and map that plug into java.util",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "16/Jan/10 17:40",
        "Updated": "21/May/11 03:24",
        "Resolved": "18/Jan/10 00:14",
        "Description": "Aside from the primitive type issues, the usual java.util.HashMap/Set classes suffer from horrible storage inefficiency.\nThe Colt code can be adapted to add OpenHashSet<T> and OpenHashMap<K,V> that use open hashing and implement the full Collections interfaces.",
        "Issue Links": []
    },
    "MAHOUT-256": {
        "Key": "MAHOUT-256",
        "Summary": "Clean up raw type usage",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "16/Jan/10 17:41",
        "Updated": "21/May/11 03:24",
        "Resolved": "18/Jan/10 02:55",
        "Description": "Turning the Object-related Colt maps into Generics has left a number of other classes referencing raw types. (e.g. matrices). These need to be made generic and cleaned up.",
        "Issue Links": []
    },
    "MAHOUT-257": {
        "Key": "MAHOUT-257",
        "Summary": "Get rid of GenericSorting.java",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "16/Jan/10 17:42",
        "Updated": "21/May/11 03:23",
        "Resolved": "18/Jan/10 00:26",
        "Description": "GenericSorting.java has one function left in it. Let's move that to Sorting.java and delete the class.",
        "Issue Links": []
    },
    "MAHOUT-258": {
        "Key": "MAHOUT-258",
        "Summary": "Unit test failure in CDInfo example",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "16/Jan/10 17:47",
        "Updated": "21/May/11 03:22",
        "Resolved": "24/Jan/10 17:55",
        "Description": "-------------------------------------------------------------------------------\nTest set: org.apache.mahout.ga.watchmaker.cd.tool.CDInfosToolTest\n-------------------------------------------------------------------------------\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 6.844 sec <<< FAILURE!\ntestGatherInfos(org.apache.mahout.ga.watchmaker.cd.tool.CDInfosToolTest)  Time elapsed: 6.731 sec  <<< ERROR!\njava.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n\tat org.apache.mahout.ga.watchmaker.cd.tool.CDInfosTool.gatherInfos(CDInfosTool.java:90)\n\tat org.apache.mahout.ga.watchmaker.cd.tool.CDInfosToolTest.testGatherInfos(CDInfosToolTest.java:220)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:168)\n\tat junit.framework.TestCase.runBare(TestCase.java:134)\n\tat junit.framework.TestResult$1.protect(TestResult.java:110)\n\tat junit.framework.TestResult.runProtected(TestResult.java:128)\n\tat junit.framework.TestResult.run(TestResult.java:113)\n\tat junit.framework.TestCase.run(TestCase.java:124)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:232)\n\tat junit.framework.TestSuite.run(TestSuite.java:227)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)\n\tat org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:62)\n\tat org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:140)\n\tat org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:165)\n\tat org.apache.maven.surefire.Surefire.run(Surefire.java:107)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:289)\n\tat org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1005)",
        "Issue Links": []
    },
    "MAHOUT-259": {
        "Key": "MAHOUT-259",
        "Summary": "Remove all code for Object matrices",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "18/Jan/10 02:59",
        "Updated": "21/May/11 03:23",
        "Resolved": "18/Jan/10 15:02",
        "Description": "No code in Mahout that does math, even code straight from Colt, uses the Object matrix classes. They are something of a nightmare from a Generic type standpoint. \nSo, 'When in doubt, cut it out.'",
        "Issue Links": []
    },
    "MAHOUT-260": {
        "Key": "MAHOUT-260",
        "Summary": "An alternative approach to RNG management",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Benson Margulies",
        "Created": "18/Jan/10 03:23",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "26/May/10 19:41",
        "Description": "Here's code to (a) use the JDK for random numbers when testing, and (b) make the RNG selectable.",
        "Issue Links": []
    },
    "MAHOUT-261": {
        "Key": "MAHOUT-261",
        "Summary": "Give the primitive-value maps an adjustOrPutValue call, like Trove.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Benson Margulies",
        "Created": "18/Jan/10 15:31",
        "Updated": "21/May/11 03:24",
        "Resolved": "18/Jan/10 15:37",
        "Description": "adjustOrPutValue is a useful efficiency trick.",
        "Issue Links": []
    },
    "MAHOUT-262": {
        "Key": "MAHOUT-262",
        "Summary": "Writable for labeled vectors for supervised learning algorithms",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": "Olivier Grisel",
        "Reporter": "Olivier Grisel",
        "Created": "19/Jan/10 14:04",
        "Updated": "21/May/11 03:24",
        "Resolved": "22/Jan/10 14:47",
        "Description": "Implement two new classes:\n\nSingleLabelVectorWritable for singly classified vectorized data item (one and only one label index per instance)\n\n\nMultiLabelVectorWritable for multi categorized vectorized data item (0 or more category indexes per instance)",
        "Issue Links": [
            "/jira/browse/MAHOUT-228"
        ]
    },
    "MAHOUT-263": {
        "Key": "MAHOUT-263",
        "Summary": "Matrix interface should extend Iterable<Vector> for better integration with distributed storage",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "19/Jan/10 17:57",
        "Updated": "21/May/11 03:23",
        "Resolved": "28/Jan/10 06:17",
        "Description": "Many sparse algorithms for dealing with Matrices just make sequential passes over the data, but don't need to see the entire matrix at once.  The way they would be implemented currently is:\n\nMatrix m = getInputCorpus();\nfor(int i=0; i<m.numRows(); i++) {\n  Vector v = m.getRow(i);\n  doStuffWithRow(v); \n}\n\n\nWhen the Matrix is backed essentially by a SequenceFile<Integer, Vector>, this algorithm outline doesn't make sense, because it requires lots of sequential random access reads.  What makes more sense, and works for in-memory matrices too, is something like the following:\n\npublic interface Matrix extends Iterable<Vector> { \n\n\nwhich allows for algorithms which only need iterators over Vectors do use them as such:\n\nMatrix m = getInputCorpus();\nIterator<Vector> it = m.iterator();\nVector v;\nwhile(it.hasNext() && (v = it.next()) != null) {\n  doStuffWithRow(v); \n}\n\n\nThe Iterator interface could be easily implemented in the AbstractMatrix base class, so implementing this idea would be transparent to all current Mahout code.  Additionally, pulling out two layers of AbstractMatrix - one which only knows how to do the things which can be done using iterators (like times(Vector), timesSquared(Vector), plus(Matrix), assignRow(), etc...), which would be the direct base class for DistributedMatrix (or HDFSMatrix), but all the random-access matrix methods currently in AbstractMatrix would go in another abstract base class of the first one (which could be called AbstractVectorIterable, say).\nI think Iteratable<Vector> could be made more flexible by extending that to a new interface VectorIterable, which provided iterateAll() and iterateNonEmpty(), in case document Ids were sparse, and could also allow for the possibility of adding other methods (things like skipTo(int rowNum), perhaps).  \nQuestion is: should this go for all Matrices, or just SparseRowMatrix?  It's really tricky to have a matrix which is iterable both as sparse rows and sparse columns.  I guess the point would be that by default, it iterates over rows, unless it's SparseColumnMatrix, which obviously iterates over columns.\nThoughts?  Having to rely on random-access to a distributed-backed matrix is making me jump through silly extra hoops on some of the stuff I'm working on patches for.",
        "Issue Links": []
    },
    "MAHOUT-264": {
        "Key": "MAHOUT-264",
        "Summary": "Make mahout-math compatible with Java 1.5 (bytecode and standard library).",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Dawid Weiss",
        "Created": "20/Jan/10 19:01",
        "Updated": "21/May/11 03:24",
        "Resolved": "22/Jan/10 20:35",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-265": {
        "Key": "MAHOUT-265",
        "Summary": "Error with creating MVC from Lucene Index or Arff",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Jerry Ye",
        "Created": "20/Jan/10 19:51",
        "Updated": "21/May/11 03:24",
        "Resolved": "28/Jan/10 06:16",
        "Description": "I'm getting the following error when trying to create vectors from a Solr index.  I've also tried using the arff to mvc utility and I'm getting the exact same error.\nException in thread \"main\" java.lang.NullPointerException\n    at org.apache.hadoop.io.serializer.SerializationFactory.getSerializer(SerializationFactory.java:73)\n    at org.apache.hadoop.io.SequenceFile$Writer.init(SequenceFile.java:910)\n    at org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1074)\n    at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:397)\n    at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:284)\n    at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:265)\n    at org.apache.mahout.utils.vectors.lucene.Driver.getSeqFileWriter(Driver.java:226)\n    at org.apache.mahout.utils.vectors.lucene.Driver.main(Driver.java:197)\nI'm getting this error with revision 901336 but not with revision 897299",
        "Issue Links": []
    },
    "MAHOUT-266": {
        "Key": "MAHOUT-266",
        "Summary": "Broken Sorting can result in AIOOB exception.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Dawid Weiss",
        "Reporter": "Dawid Weiss",
        "Created": "25/Jan/10 14:17",
        "Updated": "21/May/11 03:24",
        "Resolved": "25/Jan/10 21:43",
        "Description": "The sorting condition is checked too eagerly; probably a typo while porting from Harmony (all other sorting routines have similar pattern except this one).",
        "Issue Links": []
    },
    "MAHOUT-267": {
        "Key": "MAHOUT-267",
        "Summary": "Vector.norm(x) uses incorrect formula for both x == POSITIVE_INFINITY and x == 1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1,                                            0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "jakes old (non-committer) account",
        "Created": "27/Jan/10 06:58",
        "Updated": "21/May/11 03:24",
        "Resolved": "28/Jan/10 06:14",
        "Description": "Ugh.  This is the second time I've had to fix the definition of an L_p norm in an Apache project.\n\n  \n@Override\n  public double norm(double power)  {\n    if (power < 0.0) {\n      throw new IllegalArgumentException(\"Power must be >= 0\");\n    }\n    // we can special case certain powers\n    if (Double.isInfinite(power)) {\n      return maxValue();\n    } else if (power == 2.0) {\n      return Math.sqrt(dot(this));\n    } else if (power == 1.0) {\n      return zSum();\n    } else if (power == 0.0) {\n...\n}\n\n\nSpot the errors?  Hint: both maxValue() and zSum() can return negative values, which is a no-no for norms unless we're going to do Relativity and have Lorentzian metrics allowed in Mahout:\n\nmaxValue() returns the maximum (meaning: closest to POSITIVE_INFINITY) value, which means large negative numbers aren't noticed and factored in.\nzSum() adds all the values - not their absolute values.\n\nI'll check in a fix with some of my other code, but I wanted this tracked, as apparently the norm(double) method is so often used that this was never noticed in unit tests OR by users (the fact that nobody's ever noticed this makes me log this as a \"minor\" bug).",
        "Issue Links": []
    },
    "MAHOUT-268": {
        "Key": "MAHOUT-268",
        "Summary": "Vector.getDistanceSquared() is incorrect for both SparseVector varieties",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "jakes old (non-committer) account",
        "Created": "27/Jan/10 07:48",
        "Updated": "21/May/11 03:23",
        "Resolved": "28/Jan/10 06:13",
        "Description": "I'm pretty sure that getDistanceSquared() should just return as if an optimized implementation of:\n\n  public double getDistanceSquared(Vector v) { return this.minus(v).getLengthSquared(); }\n\n\nIn which case if some vector elements are negative, both SequentialAccessSparseVector (my fault!) and RandomAccessSparseVector return the wrong thing.  Very easy to write a failing unit test for this one.",
        "Issue Links": []
    },
    "MAHOUT-269": {
        "Key": "MAHOUT-269",
        "Summary": "Vector.maxValue() returns Double.MIN_VALUE for vectors with all negative entries.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1,                                            0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "27/Jan/10 16:31",
        "Updated": "21/May/11 03:23",
        "Resolved": "28/Jan/10 06:13",
        "Description": "@Override\n  public double maxValue() {\n    double result = Double.MIN_VALUE;\n    for (int i = 0; i < size(); i++) {\n      result = Math.max(result, getQuick(i));\n    }\n    return result;\n  }\n\n\nShould be:\n\n  @Override\n  public double maxValue() {\n    double result = 0;\n    for (int i = 0; i < size(); i++) {\n      result = Math.max(result, Math.abs(getQuick(i)));\n    }\n    return result;\n  }\n\n\nRight?  MaxValue should be returning the max of the absolute value, not the real max, right?  And the maxValue of the zero vector is 0, not MIN_VALUE, yes?",
        "Issue Links": []
    },
    "MAHOUT-270": {
        "Key": "MAHOUT-270",
        "Summary": "Make ClusterDumper dump Dirichlet clusters too",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "27/Jan/10 17:44",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "18/May/10 16:47",
        "Description": "Given the binary representation of models/clusters in Dirichlet, extend the ClusterDumper utility to dump out a printable representation of them too.",
        "Issue Links": []
    },
    "MAHOUT-271": {
        "Key": "MAHOUT-271",
        "Summary": "Make WikipediaDatasetCreatorMapper fuzzy category match respect word boundaries",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Olivier Grisel",
        "Reporter": "Olivier Grisel",
        "Created": "28/Jan/10 18:58",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "03/Oct/10 10:22",
        "Description": "WikipediaDatasetCreatorDriver is useful to create categorisation corpora out of wikipedia, however the category match just do a String#contains check which can catch a lot of unrelated categories.\nChecking the word boundaries with a regexp such as String.format(\"\\\\b%sb\", theCategoryNameIAmLookingFor); should fix the  issue.",
        "Issue Links": []
    },
    "MAHOUT-272": {
        "Key": "MAHOUT-272",
        "Summary": "Add licenses for 3rd party jars to mahout binary release and remove additional unused dependencies.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "02/Feb/10 21:55",
        "Updated": "21/May/11 03:24",
        "Resolved": "05/Feb/10 17:12",
        "Description": "The binary release produced by MAHOUT-215 includes some 3rd party jars that require licenses and other 3rd party jars (xpp3 + xstream) that are not required at all (eclipse core, a transitive dependency of hadoop, jfreechart a transitive dependency of watchmaker-swing).",
        "Issue Links": []
    },
    "MAHOUT-273": {
        "Key": "MAHOUT-273",
        "Summary": "RandomSeedGenerator doesnt estimate cluster centers when input path is a directory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "03/Feb/10 14:44",
        "Updated": "21/May/11 03:24",
        "Resolved": "03/Feb/10 21:38",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-274": {
        "Key": "MAHOUT-274",
        "Summary": "Use avro for serialization of structured documents.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "05/Feb/10 16:38",
        "Updated": "21/May/11 03:18",
        "Resolved": "17/Jan/11 15:52",
        "Description": "Explore the intersection between Writables and Avro to see how serialization can be improved within Mahout. \nAn intermediate goal is the provide a structured document format that can be serialized using Avro as an Input/OutputFormat and Writable",
        "Issue Links": []
    },
    "MAHOUT-275": {
        "Key": "MAHOUT-275",
        "Summary": "Create a separate collections library",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "07/Feb/10 00:54",
        "Updated": "21/May/11 03:23",
        "Resolved": "08/Feb/10 02:58",
        "Description": "Factor collections and as little else as possible into it's own library. Delete the AIOB test that uses matrixes until someone can refactor it to use the base collections API.",
        "Issue Links": []
    },
    "MAHOUT-276": {
        "Key": "MAHOUT-276",
        "Summary": "Alpha_0 mixture parameter is not implemented correctly in Dirichlet",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.2",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "07/Feb/10 18:16",
        "Updated": "21/May/11 03:24",
        "Resolved": "08/Feb/10 23:15",
        "Description": "I looked over the R reference code and alpha_0 is used in two places, not one as in the current implementation:\n\nin state initialization \"beta = rbeta(K, 1, alpha_0)\" [K is the number of models]\nduring state update \"beta[k] = rbeta(1, 1 + counts[k], alpha_0 + N-counts[k])\" [N is the cardinality of the sample vector and counts corresponds to totalCounts in the implementation]\n\nThe value of beta[k] is then used in the Dirichlet distribution calculation which results in the mixture probabilities pi[i], for the iteration:\n   other = 1                                     # product accumulator\n   for (k in 1:K) {\n     pi[k] = beta[k] * other;                    # beta_k * prod_\n{n<k}\n beta_n\n     other = other * (1-beta[k])\n     }\nAlpha_0 determines the probability a point will go into an empty cluster, mostly during the first iteration.  During the first iteration, the total counts of all prior clusters are zero. Thus the Beta calculation that drives the Dirichlet distribution that determines the mixture probabilities degenerates to beta = rBeta(1, alpha_0). Clusters that end up with points for the next iteration will overwhelm the small constants (alpha_0, 1) and subsequent new mixture probabilities will derive from beta ~=  rBeta(count, total) which is the current implementation. All empty clusters will subsequently be driven by beta ~= rBeta(1, total) as alpha_0 is insignificant and count is 0.\nThe current implementation ends up using beta = rBeta(alpha_0/k, alpha_0) as initial values during all iterations because the counts are all initialized to alpha_0/k. Close but no cigar.",
        "Issue Links": []
    },
    "MAHOUT-277": {
        "Key": "MAHOUT-277",
        "Summary": "Increase number of entries in memory per chunk of dictionary",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Integration",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "07/Feb/10 18:51",
        "Updated": "21/May/11 03:23",
        "Resolved": "07/Feb/10 23:26",
        "Description": "Java HashMap was having a huge overhead that was decreasing the number of entries per chunk hence increases number of passes over the data\nDictionary was using Text,LongWritable key. Integer is enough to keep the feature ids.",
        "Issue Links": []
    },
    "MAHOUT-278": {
        "Key": "MAHOUT-278",
        "Summary": "Make Cluster dumper read DictionaryVectorizer Dictionary format",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "07/Feb/10 19:10",
        "Updated": "21/May/11 03:24",
        "Resolved": "07/Feb/10 23:29",
        "Description": "Make ClusterDumper read the SequenceFile Text=>IntWritable format to print the cluster with an optional commandline  flag\n--dictionaryType or -dt (text|sequencefile) Defaults to text",
        "Issue Links": []
    },
    "MAHOUT-279": {
        "Key": "MAHOUT-279",
        "Summary": "Make RandomSeedGenerator a M/R Job",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "07/Feb/10 20:17",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "01/Mar/11 11:48",
        "Description": "Speedup Random Centroid Selection for clustering using Map/Reduce\nIncreasing the scope of this issue.\n\nRandom Seed Generator could take a distance measure and a threshold and use that information during random eviction and insertion to increase the distance between two centroids",
        "Issue Links": []
    },
    "MAHOUT-280": {
        "Key": "MAHOUT-280",
        "Summary": "Clean some redundant POM declarations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "08/Feb/10 03:08",
        "Updated": "21/May/11 03:27",
        "Resolved": "09/Feb/10 09:31",
        "Description": "I am about to attach a simple patch to clean up some redundant stuff in the poms.",
        "Issue Links": []
    },
    "MAHOUT-281": {
        "Key": "MAHOUT-281",
        "Summary": "scm urls are wrong in the poms",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "08/Feb/10 03:10",
        "Updated": "21/May/11 03:24",
        "Resolved": "12/Feb/10 13:47",
        "Description": "The scm urls in the poms are wrong. This must be fixed before running the release plugin to make an 0.3 release.",
        "Issue Links": []
    },
    "MAHOUT-282": {
        "Key": "MAHOUT-282",
        "Summary": "Remove assembly from core, re-add commons-cli 1.x (no longer exluced from hadoop dependency)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Drew Farris",
        "Created": "08/Feb/10 15:08",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/Feb/10 04:10",
        "Description": "Very small cleanup patch. \nThe assembly from core is no longer needed with the addition of the release artifacts that get built in MAHOUT-215, removing it because it slows down the build.\ncommons-cli 1.1 shouldn't be excluded from hadoop because it is needed by hadoop's GenericOptionsParser.",
        "Issue Links": []
    },
    "MAHOUT-283": {
        "Key": "MAHOUT-215 Provide jars with mahout release.",
        "Summary": "Update assemblies to include mahout-collections for release build",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "08/Feb/10 15:18",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/Feb/10 09:29",
        "Description": "The release assemblies need to be updated to include the new mahout-collections project.",
        "Issue Links": []
    },
    "MAHOUT-284": {
        "Key": "MAHOUT-284",
        "Summary": "In Fuzzy Kmeans, when the distance between centroid and the given point is zero, then it should belong to that cluster with probability 1 and rest with probability zero",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Pallavi Palleti",
        "Created": "09/Feb/10 10:23",
        "Updated": "21/May/11 03:27",
        "Resolved": "24/Sep/10 11:54",
        "Description": "In Fuzzy Kmeans, when the distance between centroid and the given point is zero, then the point should belong to that cluster with probability 1 and rest with probability zero. However, right now, we are not doing that.",
        "Issue Links": []
    },
    "MAHOUT-285": {
        "Key": "MAHOUT-285",
        "Summary": "Wrap up collocation and dictionary vectorizer integration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Drew Farris",
        "Created": "09/Feb/10 14:40",
        "Updated": "21/May/11 03:24",
        "Resolved": "11/Feb/10 07:21",
        "Description": "Final bit of work to integrate collocations into 0.3\n\nModify collocation finder to use dictionary vectorizer output as input (saves analysis step)\nGenerate input dictionary for dictionary vectorizer that includes unigrams and collocations.\n\nChatted with Robin this morning, I know what needs to be done it is just a matter of grinding out the code.",
        "Issue Links": []
    },
    "MAHOUT-286": {
        "Key": "MAHOUT-286",
        "Summary": "Need to be able to run classifiers from non-text input (such as ARFF data)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "09/Feb/10 18:39",
        "Updated": "21/May/11 03:18",
        "Resolved": "04/Jan/11 16:14",
        "Description": "Martin Haeger wrote this:\n\nWe're experimenting a bit with Weka and Mahout. Our input data is a\nrelation in ARFF format (see attached data.training.arff), and we'd\nlike to classify it using Mahout. However, it seems (to us, at first)\nthat the Mahout classifier.bayes.interfaces.Algorithm interface is\ncentered around documents of text, and not general attribute data.\nThus, running the classifier causes our ARFF data to be interpreted as\na document of words, with not very useful results (see attached\nmahout.log).\nWith Weka, we're able to get the results we want (see attached weka.log).\nAny suggestions for how to get this working?",
        "Issue Links": [
            "/jira/browse/MAHOUT-287"
        ]
    },
    "MAHOUT-287": {
        "Key": "MAHOUT-287",
        "Summary": "Bayes Classifier should use Vector as input",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "10/Feb/10 09:01",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "06/Oct/10 21:39",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-286"
        ]
    },
    "MAHOUT-288": {
        "Key": "MAHOUT-286 Need to be able to run classifiers from non-text input (such as ARFF data)",
        "Summary": "Select only Binary attributes from ARFF format for Bayes Classifier",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Integration",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "10/Feb/10 09:04",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "12/Feb/10 18:39",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-289": {
        "Key": "MAHOUT-289",
        "Summary": "Map/reduce framework accepts non Writables, Need to change Vector to VectorWritable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "12/Feb/10 19:48",
        "Updated": "21/May/11 03:24",
        "Resolved": "12/Feb/10 20:25",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-290": {
        "Key": "MAHOUT-290",
        "Summary": "Make SequenceFileFromDirectory input args consistent with others",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "13/Feb/10 17:38",
        "Updated": "21/May/11 03:24",
        "Resolved": "13/Feb/10 17:52",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-291": {
        "Key": "MAHOUT-291",
        "Summary": "Mahout Code Cleanup",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering,                                            Documentation,                                            Integration,                                            Math",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "13/Feb/10 17:53",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "17/Feb/10 10:43",
        "Description": "Code Cleanup\nOrganize imports\nRemove space in blank lines\nmake local variables final",
        "Issue Links": []
    },
    "MAHOUT-292": {
        "Key": "MAHOUT-292",
        "Summary": "Classifier Test Data and Self Tests",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "15/Feb/10 15:28",
        "Updated": "21/May/11 03:23",
        "Resolved": "15/Feb/10 18:28",
        "Description": "Till now there was no means to test if quality of classification suffered due to a code change. \nAdded Classifier data with 3 labels (mahout, lucene and spamassasin) with 4 long sentences in each of the labels. \nAdded a SelfTest which trains Bayes and CBayes model and classify the train dataset while testing and check accuracy and confusion matrix",
        "Issue Links": []
    },
    "MAHOUT-293": {
        "Key": "MAHOUT-293",
        "Summary": "Add more tunable parameters to PFPGrowth implementation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "15/Feb/10 16:28",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "31/Mar/11 13:52",
        "Description": "Objective is to add more tunable parameters to the PFPGrowth algorithm.\nFrom Neal on Mahout User list:\nI often use Christian Borgelt's itemset implementations for playing\nwith data.  He's implemented a nice set of switches, see below.\nSetting a minimum support threshold and mimimum itemset size are both\nconvenient and tend to make the algorithm run a bit faster.\nhttp://www.borgelt.net/software.html\nnealr@nrichter-laptop:~$ fpgrowth_fim\nusage: fpgrowth_fim [options] infile outfile\nfind frequent item sets with the fpgrowth algorithm\nversion 1.13 (2008.05.02)        (c) 2004-2008   Christian Borgelt\n-m#      minimal number of items per item set (default: 1)\n-n#      maximal number of items per item set (default: no limit)\n-s#      minimal support of an item set (default: 10%)\n        (positive: percentage, negative: absolute number)\n-d#      minimal binary logarithm of support quotient (default: none)\n-p#      output format for the item set support (default: \"%.1f\")\n-a       print absolute support (number of transactions)\n-g       write output in scanable form (quote certain characters)\n-q#      sort items w.r.t. their frequency (default: -2)\n        (1: ascending, -1: descending, 0: do not sort,\n         2: ascending, -2: descending w.r.t. transaction size sum)\n-u       use alternative tree projection method\n-z       do not prune tree projections to bonsai\n-j       use quicksort to sort the transactions (default: heapsort)\n-i#      ignore records starting with a character in the given string\n-b/f/r#  blank characters, field and record separators\n        (default: \" \\t\\r\", \" \\t\", \"\\n\")\ninfile   file to read transactions from\noutfile  file to write frequent item se",
        "Issue Links": []
    },
    "MAHOUT-294": {
        "Key": "MAHOUT-294",
        "Summary": "Uniform API behavior for Jobs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering,                                            Integration,                                            Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Robin Anil",
        "Created": "16/Feb/10 08:44",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "13/Sep/11 09:07",
        "Description": "Update \u2013 This issue is at this point open to track the mission to standardize, at least, on using one version of Hadoop's API. At the moment, it means using the .mapreduce.* classes instead of .mapred.* classes.",
        "Issue Links": [
            "/jira/browse/MAHOUT-301"
        ]
    },
    "MAHOUT-295": {
        "Key": "MAHOUT-295",
        "Summary": "Make Cluster Dumper read FuzzyKMeans clusters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "16/Feb/10 10:26",
        "Updated": "21/May/11 03:24",
        "Resolved": "01/Mar/10 06:09",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-307"
        ]
    },
    "MAHOUT-296": {
        "Key": "MAHOUT-296",
        "Summary": "TestClassifier takes correctLabel from filename instead of from the key",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "18/Feb/10 15:10",
        "Updated": "18/Feb/10 15:26",
        "Resolved": "18/Feb/10 15:25",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-297": {
        "Key": "MAHOUT-297",
        "Summary": "Canopy and Kmeans clustering slows down on using SeqAccVector for center",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Robin Anil",
        "Created": "18/Feb/10 20:43",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "18/May/10 17:25",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-298": {
        "Key": "MAHOUT-298",
        "Summary": "2 test case fails while trying to mvn clean install after checking out revision 911542 of trunk",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Anish Shah",
        "Created": "19/Feb/10 01:19",
        "Updated": "21/May/11 03:22",
        "Resolved": "27/May/10 07:58",
        "Description": "I checked out revision 911542 from trunk and seeing the following failed tests when I ran mvn clean install:\nResults :\nFailed tests:\n  testKMeansMRJob(org.apache.mahout.clustering.kmeans.TestKmeansClustering)\n  testKMeansWithCanopyClusterInput(org.apache.mahout.clustering.kmeans.TestKmean\nsClustering)\nTests run: 338, Failures: 2, Errors: 0, Skipped: 0\n[INFO] ------------------------------------------------------------------------\n[ERROR] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] There are test failures.\nI looked in the surefire-reports and see the following details on the failures:\ntestKMeansMRJob(org.apache.mahout.clustering.kmeans.TestKmeansClustering)  Time elapsed: 11.169 sec  <<< FAILURE!\njunit.framework.AssertionFailedError: clusters[3] expected:<4> but was:<2>\n\tat junit.framework.Assert.fail(Assert.java:47)\n...\ntestKMeansWithCanopyClusterInput(org.apache.mahout.clustering.kmeans.TestKmeansClustering)  Time elapsed: 3.35 sec  <<< FAILURE!\njunit.framework.AssertionFailedError: num points[0] expected:<4> but was:<1>\n\tat junit.framework.Assert.fail(Assert.java:47)",
        "Issue Links": []
    },
    "MAHOUT-299": {
        "Key": "MAHOUT-299",
        "Summary": "Collocations: improve performance by making Gram BinaryComparable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Integration",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "19/Feb/10 04:11",
        "Updated": "21/May/11 03:23",
        "Resolved": "20/Feb/10 18:56",
        "Description": "Robin's profiling indicated that a large portion of a run was spent in readFields() in Gram due to the deserialization occuring as a part of Gram comparions for sorting. He pointed me to BinaryComparable and the implementation in Text.\nLike Text, in this new implementation, Gram stores its string in binary form. When encoding the string at construction time we allocate an extra character's worth of data to hold the Gram type information. When sorting Grams, the binary arrays are compared instead of deserializing and comparing fields.",
        "Issue Links": []
    },
    "MAHOUT-300": {
        "Key": "MAHOUT-306 Profile and improve performance of algorithms based on vectors",
        "Summary": "Solve performance issues with Vector Implementations",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "19/Feb/10 20:46",
        "Updated": "21/May/11 03:24",
        "Resolved": "22/Feb/10 22:27",
        "Description": "AbstractVector operations like times\n  public Vector times(double x) {\n    Vector result = clone();\n    Iterator<Element> iter = iterateNonZero();\n    while (iter.hasNext()) \n{\n      Element element = iter.next();\n      int index = element.index();\n      result.setQuick(index, element.get() * x);\n    }\n    return result;\n  }\nshould be implemented as follows\n public Vector times(double x) {\n    Vector result = clone();\n    Iterator<Element> iter = result.iterateNonZero();\n    while (iter.hasNext()) \n{\n      Element element = iter.next();\n      element.set(element.get() * x);\n    }\n    return result;\n  }",
        "Issue Links": []
    },
    "MAHOUT-301": {
        "Key": "MAHOUT-301",
        "Summary": "Improve command-line shell script by allowing default properties files",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Integration",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "20/Feb/10 07:15",
        "Updated": "21/May/11 03:24",
        "Resolved": "02/Mar/10 18:11",
        "Description": "Snippet from javadoc gives the idea:\n\n/**\n * General-purpose driver class for Mahout programs.  Utilizes org.apache.hadoop.util.ProgramDriver to run\n * main methods of other classes, but first loads up default properties from a properties file.\n *\n * Usage: run on Hadoop like so:\n *\n * $HADOOP_HOME/bin/hadoop -jar path/to/job org.apache.mahout.driver.MahoutDriver [classes.props file] shortJobName \\\n *   [default.props file for this class] [over-ride options, all specified in long form: --input, --jarFile, etc]\n *\n * TODO: set the Main-Class to just be MahoutDriver, so that this option isn't needed?\n *\n * (note: using the current shell scipt, this could be modified to be just \n * $MAHOUT_HOME/bin/mahout [classes.props file] shortJobName [default.props file] [over-ride options]\n * )\n *\n * Works like this: by default, the file \"core/src/main/resources/driver.classes.prop\" is loaded, which\n * defines a mapping between short names like \"VectorDumper\" and fully qualified class names.  This file may\n * instead be overridden on the command line by having the first argument be some string of the form *classes.props.\n *\n * The next argument to the Driver is supposed to be the short name of the class to be run (as defined in the\n * driver.classes.props file).  After this, if the next argument ends in \".props\" / \".properties\", it is taken to\n * be the file to use as the default properties file for this execution, and key-value pairs are built up from that:\n * if the file contains\n *\n * input=/path/to/my/input\n * output=/path/to/my/output\n *\n * Then the class which will be run will have it's main called with\n *\n *   main(new String[] { \"--input\", \"/path/to/my/input\", \"--output\", \"/path/to/my/output\" });\n *\n * After all the \"default\" properties are loaded from the file, any further command-line arguments are taken in,\n * and over-ride the defaults.\n */\n\n\nCould be cleaned up, as it's kinda ugly with the whole \"file named in .props\", but gives the idea.  Really helps cut down on repetitive long command lines, lets defaults be put props files instead of locked into the code also.",
        "Issue Links": [
            "/jira/browse/MAHOUT-294"
        ]
    },
    "MAHOUT-302": {
        "Key": "MAHOUT-302",
        "Summary": "Change tests to use temp directories instead of output, testdata",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering,                                            Integration,                                            Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Robin Anil",
        "Created": "20/Feb/10 18:45",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "06/May/10 12:13",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-303": {
        "Key": "MAHOUT-303",
        "Summary": "Exhaustive Tests for Vector implementations",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "20/Feb/10 19:24",
        "Updated": "21/May/11 03:27",
        "Resolved": "22/Sep/10 06:36",
        "Description": "Must check borderline cases\nMust check with couple of levels of sparseness\nMust check all implementations with all possible combinations for the binary function\nOptional: Keep a performance threshold maybe as a percentage of dense vector to keep track of accidental regression",
        "Issue Links": []
    },
    "MAHOUT-304": {
        "Key": "MAHOUT-304",
        "Summary": "MeanShift doesn't read from VectorWritable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "20/Feb/10 21:48",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "01/Mar/10 06:09",
        "Description": "Need an M/R job for converting sequence file containing VectorWritable to MeanShiftCanopy before the MeanShift M/R",
        "Issue Links": [
            "/jira/browse/MAHOUT-307"
        ]
    },
    "MAHOUT-305": {
        "Key": "MAHOUT-305",
        "Summary": "Combine both cooccurrence-based CF M/R jobs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Ankur Bansal",
        "Reporter": "Sean R. Owen",
        "Created": "21/Feb/10 13:28",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "28/Apr/10 04:42",
        "Description": "We have two different but essentially identical MapReduce jobs to make recommendations based on item co-occurrence: org.apache.mahout.cf.taste.hadoop.\n{item,cooccurrence}\n. They ought to be merged. Not sure exactly how to approach that but noting this in JIRA, per Ankur.",
        "Issue Links": []
    },
    "MAHOUT-306": {
        "Key": "MAHOUT-306",
        "Summary": "Profile and improve performance of algorithms based on vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "22/Feb/10 22:26",
        "Updated": "21/May/11 03:18",
        "Resolved": "03/Feb/11 17:48",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-307": {
        "Key": "MAHOUT-307",
        "Summary": "Move Reference Clustering implementations to core",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "24/Feb/10 12:04",
        "Updated": "21/May/11 03:23",
        "Resolved": "01/Mar/10 06:09",
        "Description": "Current Reference K-Means, FuzzyK, Canopy etc  are having duplicate code in examples and in tests. \n\nIdea is to move everything to the Clusterer class and reuse it.\nIt can also serve as in-memory implementation",
        "Issue Links": [
            "/jira/browse/MAHOUT-295",
            "/jira/browse/MAHOUT-304"
        ]
    },
    "MAHOUT-308": {
        "Key": "MAHOUT-308",
        "Summary": "Improve Lanczos to handle extremely large feature sets (without hashing)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "24/Feb/10 23:01",
        "Updated": "02/May/12 02:16",
        "Resolved": "31/Mar/11 13:53",
        "Description": "DistributedLanczosSolver currently keeps all Lanczos vectors in memory on the driver (client) computer while Hadoop is iterating.  The memory requirements of this is (desiredRank) * (numColumnsOfInput) * 8bytes, which for desiredRank = a few hundred, starts to cap out usefulness at some-small-number * millions of columns for most commodity hardware.\nThe solution (without doing stochastic decomposition) is to persist the Lanczos basis to disk, except for the most recent two vectors.  Some care must be taken in the \"orthogonalizeAgainstBasis()\" method call, which uses the entire basis.  This part would be slower this way.",
        "Issue Links": []
    },
    "MAHOUT-309": {
        "Key": "MAHOUT-309",
        "Summary": "Implement Stochastic Decomposition",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Ted Dunning",
        "Reporter": "Jake Mannix",
        "Created": "24/Feb/10 23:53",
        "Updated": "21/May/11 03:18",
        "Resolved": "07/Mar/11 10:54",
        "Description": "Techniques reviewed in <a href=\"http://arxiv.org/abs/0909.4061\">Halko, Martinsson, and Tropp</a>.\nThe basic idea of the implementation is as follows: if the input matrix is represented as a DistributedSparseRowMatrix (backed by a sequence-file of <Writable,VectorWritable> - the values of which should be SequentialAccessSparseVector instances for best performance), and you optionally have a kernel function f(v) which maps sparse numColumns-dimensional (here numColumns is unconstrained in size) vectors to sparse numKernelizedFeatures-dimensional (also unconstrained in size) vectors (in the case where you want to do kernel-PCA, for example, for a kernel k(u,v) = f(u).dot( f(v) )), then take the MurmurHash (from MAHOUT-228) and maps the numKernelizedFeatures-dimensional vectors and projects down to some numHashedFeatures-dimensional space (reasonably-sized - no more than a 10^2 to 10^4).  \nThis is all done in the Mapper, and there are two outputs: the numHashedFeatures-dimensional vector itself (if the left-singular vectors are ever desired), which does not need to be Reduced, and the outer-product of this vector with itself, where the Reducer/Combiner just does the matrix sum on the partial outputs, eventually producing the kernel / gram matrix of your hashed features, which can then be run through a simple eigen-decomposition, the ((1/eigenvalue)-scaled) eigenvectors of which can be applied to project the (optional) numHashedFeatures-dimensional outputs mentioned earlier in this paragraph to get the left-singular vectors / reduced projections (which can be then run through clustering, etc...).\nGood fun will be had by all.",
        "Issue Links": [
            "/jira/browse/MAHOUT-228",
            "/jira/browse/MAHOUT-376"
        ]
    },
    "MAHOUT-310": {
        "Key": "MAHOUT-310",
        "Summary": "LanczosSolver and DistributedLanczosSolver always assume rectangular input, but should also handle symmetric eigensystems.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "25/Feb/10 10:05",
        "Updated": "21/May/11 03:24",
        "Resolved": "06/Mar/10 06:48",
        "Description": "LanczosSolver calls inputMatrix.timesSquared(Vector) as it's Krylov iteration, but for symmetric inputMatrix, it should chose to instead call inputMatrix.times(Vector).\nSimilarly for DistributedLanczosSolver, except in this case, DistributedSparseRowMatrix actually needs to properly MapReduce implement times(Vector) for this to work (which it should do anyways, instead of throw UnsupportedOperationException as it does currently).",
        "Issue Links": []
    },
    "MAHOUT-311": {
        "Key": "MAHOUT-301 Improve command-line shell script by allowing default properties files",
        "Summary": "Update assemblies to include components of launcher script from MAHOUT-301",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Drew Farris",
        "Created": "26/Feb/10 13:40",
        "Updated": "21/May/11 03:24",
        "Resolved": "02/Mar/10 18:13",
        "Description": "Update the release assemblies to include an executable bin/mahout script and the contents of the conf directory.",
        "Issue Links": []
    },
    "MAHOUT-312": {
        "Key": "MAHOUT-312",
        "Summary": "DistributedRowMatrix iterateAll() and iterate() don't work on multi-part SequenceFiles",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "28/Feb/10 22:02",
        "Updated": "21/May/11 03:23",
        "Resolved": "06/Mar/10 06:48",
        "Description": "DistributedRowMatrixIterator does not properly handle file glob paths of the various part-00000 files.",
        "Issue Links": []
    },
    "MAHOUT-313": {
        "Key": "MAHOUT-313",
        "Summary": "DistributedRowMatrix needs times(Vector) implementation as M/R job",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "28/Feb/10 22:03",
        "Updated": "21/May/11 03:24",
        "Resolved": "06/Mar/10 06:49",
        "Description": "pretty self-explanatory.",
        "Issue Links": []
    },
    "MAHOUT-314": {
        "Key": "MAHOUT-314",
        "Summary": "DistributedRowMatrix needs a sparse DistributedRowMatrix times(DistributedRowMatrix other) implementation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "28/Feb/10 22:06",
        "Updated": "21/May/11 03:24",
        "Resolved": "06/Mar/10 07:01",
        "Description": "If the matrix which is being multiplied by has been transformed into a column-sparse matrix backed by a SequenceFile<IntWritable,VectorWritable>, then doing a simple map-side join on the two, and taking the (sparse) outer product of each row-pair, and then doing a matrix-summing reducer (probably row-at-a-time, for memory constraints) would implement sparse matrix multiplication in one pass over the data.",
        "Issue Links": []
    },
    "MAHOUT-315": {
        "Key": "MAHOUT-315",
        "Summary": "VectorDumper should also do printing to simple {index : value, index : value, ... } output, if no dictionary is specified.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "28/Feb/10 22:08",
        "Updated": "21/May/11 03:24",
        "Resolved": "05/Mar/10 18:54",
        "Description": "I've got a patch for this, tied up in other code.",
        "Issue Links": []
    },
    "MAHOUT-316": {
        "Key": "MAHOUT-316",
        "Summary": "CardinalityException and IndexException should remove the default constructor, and always construct with arguments saying what the error was",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jake Mannix",
        "Created": "28/Feb/10 22:10",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "21/Apr/10 07:46",
        "Description": "CardinalityException already has another constructor: (int, int), which allows the stack trace to say why there was an exception.  This should be required, and IndexException should follow suit.",
        "Issue Links": []
    },
    "MAHOUT-317": {
        "Key": "MAHOUT-317",
        "Summary": "Collocations: Eliminate in-memory frequency calculation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "01/Mar/10 05:21",
        "Updated": "11/Mar/10 02:14",
        "Resolved": "06/Mar/10 16:39",
        "Description": "see: http://www.lucidimagination.com/search/document/ae484d53e969250e/who_owns_mahout_bucket_on_s3\nThe collocation code currently uses maps in the CollocCombiner and CollocReducer to perform frequency calculations which can cause the process to exceed the heap space if a large number of ngrams exist for any given subgram.\nConvert the code to use a composite key / secondary sort to avoid the need for in-memory map for frequency calculations.",
        "Issue Links": []
    },
    "MAHOUT-318": {
        "Key": "MAHOUT-318",
        "Summary": "Decision Tree Learning",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Meher Anand",
        "Created": "01/Mar/10 13:44",
        "Updated": "20/Feb/13 10:29",
        "Resolved": "26/Sep/10 09:51",
        "Description": "Is there any plan to implement ID3 and C4.5 algorithms in Mahout? I am an undergrad student who's working on Mahout as part of my senior year project. I plan to implement decision tree learning as part of my project and was wondering if it is a good enough idea to incorporate it into the mainstream application.\nRegards.\nMeher Anand",
        "Issue Links": []
    },
    "MAHOUT-319": {
        "Key": "MAHOUT-319",
        "Summary": "SVD solvers should be gracefully stoppable/restartable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "01/Mar/10 16:20",
        "Updated": "21/May/11 03:18",
        "Resolved": "06/May/11 04:24",
        "Description": "LanczosSolver, DistributedLanczosSolver, and HebbianSolver all keep copious amounts of memory-resident data which is lost if the app crashes or is killed (OOM, forgetting to run in a screen session, and losing net connectivity to the server running it, etc...).  \nThese algorithms (and many other Mahout processes!) should enable a pluggable \"persist state\" mechanism (to HDFS, RDBMS, local disk, key-value store, etc), and similarly, a way to pick up and start from such a state.",
        "Issue Links": []
    },
    "MAHOUT-320": {
        "Key": "MAHOUT-320",
        "Summary": "Modify IntPairWritable in LDA implementation to be binary comparable to improve performance.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "Drew Farris",
        "Created": "03/Mar/10 04:52",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/May/10 02:33",
        "Description": "Per discussion with Robin, modifying o.a.m.clustering.lda.IntPairWritable to be binary comparable will improve the performance of the comparison operations during a sort because no marshaling will need to occur to compare IntPairWritable instances.",
        "Issue Links": []
    },
    "MAHOUT-321": {
        "Key": "MAHOUT-321",
        "Summary": "Rationalize use of similarity metrics as weights in user-based, item-based recommendation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "03/Mar/10 11:26",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "18/Mar/10 12:15",
        "Description": "See this thread: http://old.nabble.com/weighted-score-td27686783.html\nIn short, using similarity values as weights in a weighted average is problematic since they can be negative. This can result in weighted averages well out of range of possible preference values, even infinite. The current solution simply moves the values from [-1,1] to [0,2] but this has undesirable effects like giving weight of 1 to entities with no similarity.\nTamas advances, and Ted refined, a convincing argument that negative weights can be handled in a different way which doesn't require them to be arbitrarily shifted. It is simply a matter of capping the estimated preference at the max or min value the preference value can take on.\nIt's possible for the framework to track this max/min value observed in the data, and do the capping, with little performance impact. Hence I want to make this change after 0.3 is released.",
        "Issue Links": []
    },
    "MAHOUT-322": {
        "Key": "MAHOUT-322",
        "Summary": "DistributedRowMatrix should live in SequenceFile<Writable,VectorWritable> instead of SequenceFile<IntWritable,VectorWritable>",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Danny Leshem",
        "Created": "04/Mar/10 09:53",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "15/Oct/11 10:05",
        "Description": "Class documentation for org.apache.mahout.math.hadoop.DistributedRowMatrix states that the matrix lives in SequenceFile<WritableComparable, VectorWritable>. Implementation, however, assumes SequenceFile<IntWritable, VectorWritable> is passed.\nCurrently, usage of this class inside Mahout is limited to Jake Mannix's SVD package, mainly to perform PCA on a massive document corpus. Given such corpus, it makes sense to not limit the user by forcing the document \"key\" to be integer. Instead, users should be able to use Text keys (document name or id) or keys made of any other arbitrary class. One may even argue that forcing a WritableComparable key is too limiting, and a simple Writable key should be assumed.\nIn fact, it would be best if DistributedRowMatrix did not read the SequenceFile key at all, to allow user-specific classes (unknown to Mahout) to be used as opaque keys even when their libraries are not available in runtime. Currently DistributedRowMatrix calls \"reader.next(i, v)\"... but reader has methods to query just the value, avoiding key deserialization altogether.",
        "Issue Links": []
    },
    "MAHOUT-323": {
        "Key": "MAHOUT-323",
        "Summary": "Classify new data using Decision Forest",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "06/Mar/10 07:52",
        "Updated": "21/Sep/10 16:01",
        "Resolved": "21/Sep/10 15:58",
        "Description": "When building a Decision Forest we should be able to store it somewhere and use it later to classify new datasets",
        "Issue Links": []
    },
    "MAHOUT-324": {
        "Key": "MAHOUT-324",
        "Summary": "conf.setMapOutputValueClass(vectorClass) should be conf.setMapOutputValueClass(Text.class); in KMeansDrive.runClustering(...)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.3",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Chad Chen",
        "Created": "09/Mar/10 23:28",
        "Updated": "21/May/11 03:24",
        "Resolved": "09/Mar/10 23:59",
        "Description": "It looks to me the MapOutputValueClass is not set correctly in the method KMeansDrive.runClustering(...). , which doesn't match the OutputCollector used by the KMeansClusterMapper. Although it doesn't affect the output in the current implementation since the mapred.reduce.tasks is set to 0, it should be corrected.\nPlease confirm if this bug is valid.\nThanks.",
        "Issue Links": []
    },
    "MAHOUT-325": {
        "Key": "MAHOUT-325",
        "Summary": "Switch from hadoop-0.20.2-SNAPSHOT to hadoop-0.20.2 (release)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "10/Mar/10 02:14",
        "Updated": "11/Mar/10 02:13",
        "Resolved": "11/Mar/10 02:13",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-326": {
        "Key": "MAHOUT-326",
        "Summary": "a possible bug with the isConverged() method in KMeansDriver.java",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Chad Chen",
        "Created": "11/Mar/10 00:15",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "31/Jul/10 15:09",
        "Description": "In one of my today's test runs using the clustering example from the book \"Mahout in Action\", I noticed the following exception thrown by  KMeansClusterMapper:\n----------------------------\njava.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:354) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307) at org.apache.hadoop.mapred.Child.main(Child.java:159) Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88) ... 5 more Caused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34) ... 10 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) at java.lang.reflect.Method.invoke(Method.java:597) at \n***\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88) ... 13 more Caused by: java.lang.NullPointerException: Cluster is empty!!! at \n***\norg.apache.mahout.clustering.kmeans.KMeansClusterMapper.configure(KMeansClusterMapper.java:63)\n---------------------------\nwhich says that the runClustering method didn't see the cluster ouput.  The same map task did finally succeed after a few failed attempts.\nAfter looking into KMeansDirver.java, I think may be a bug in the isConverged method. Basically, this method doesn't wait for the cluster output file to be fully populated. If the part-* file doesn't exist yet or has not been fully written, then this method can return true prematurally. I am not sure if this is a bug of hadoop itself because it may report successful job before the mapred output file is fully written. Meanwhile, a possible way to fix this problem is to force the isConverged method to wait for the existence of the cluster output file and make sure the file contains the 'converged' values for all the clusters.\nPlease note, I saw this problem only once in many test runs I had so far. It may be a little bit difficult to reproduce. If you need any further information, please let me know.\nThanks.",
        "Issue Links": []
    },
    "MAHOUT-327": {
        "Key": "MAHOUT-327",
        "Summary": "Implement a cool classifier over map/reduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "12/Mar/10 06:36",
        "Updated": "21/May/11 03:27",
        "Resolved": "23/May/10 01:19",
        "Description": "A student with a good proposal \n\nshould be free to work for Mahout in the summer and should be thrilled to work in this area \nshould be able to program in Java and be comfortable with datastructures and algorithms\nmust be clear about the classification algorithm, how it works, its strengths, its weaknesses and possible tweaks.\nmust have a plan on making it a map/reduce implementation\nmust have clear deadlines and pace it evenly across the span of 3 months.\nmay have a background in these area.  Past work, thesis etc counts, so show it in the proposal clearly\n\nIf you can do something extra it counts, but make sure the plan is reasonable within the specified time frame.",
        "Issue Links": []
    },
    "MAHOUT-328": {
        "Key": "MAHOUT-328",
        "Summary": "Implement a cool clustering algorithm on map/reduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "12/Mar/10 06:38",
        "Updated": "21/May/11 03:27",
        "Resolved": "26/May/10 20:06",
        "Description": "A student with a good proposal \n\nshould be free to work for Mahout in the summer and should be thrilled to work in this area \nshould be able to program in Java and be comfortable with datastructures and algorithms\nmust be clear about the clustering algorithm, how it works, its strengths, its weaknesses and possible tweaks.\nmust have a plan on making it a map/reduce implementation\nshould have a demo over standard datasets by the end of summer of code\nmust have clear deadlines and pace it evenly across the span of 3 months.\nmay have a background in these area.  Past work, thesis etc counts, so show it in the proposal clearly\n\nIf you can do something extra it counts, but make sure the plan is reasonable within the specified time frame.",
        "Issue Links": []
    },
    "MAHOUT-329": {
        "Key": "MAHOUT-329",
        "Summary": "Implement some recommendation ideas used by the Netflix top teams to boost the recommenders package",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "12/Mar/10 06:41",
        "Updated": "21/May/11 03:27",
        "Resolved": "27/Apr/10 08:49",
        "Description": "A student with a good proposal \n\nshould be free to work for Mahout in the summer and should be thrilled to work in this area \nshould be able to program in Java and be comfortable with datastructures and algorithms\nmust be clear about the algorithm or heuristic, how it works, its strengths, its weaknesses and possible tweaks.\nmust have a plan on making it a map/reduce implementation\nshould be able to show the improvement on the netflix data as compared to the baseline recommender package by the end of summer of code\nmust have clear deadlines and pace it evenly across the span of 3 months.\nmay have a background in these area.  Past work, thesis etc counts, so show it in the proposal clearly\n\nIf you can do something extra it counts, but make sure the plan is reasonable within the specified time frame.",
        "Issue Links": [
            "/jira/browse/MAHOUT-371",
            "/jira/browse/MAHOUT-375"
        ]
    },
    "MAHOUT-330": {
        "Key": "MAHOUT-330",
        "Summary": "Implement a meta-learner over the different classifiers in mahout",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "12/Mar/10 06:43",
        "Updated": "21/May/11 03:27",
        "Resolved": "23/May/10 01:18",
        "Description": "A student with a good proposal \n\nshould be free to work for Mahout in the summer and should be thrilled to work in this area \nshould be able to program in Java and be comfortable with datastructures and algorithms\nmust be clear about techniques in meta learning, how it works, its strengths, its weaknesses and possible tweaks.\nmust have a plan on making it a map/reduce implementation\nshould have a demo over standard datasets by the end of summer of code\nmust have clear deadlines and pace it evenly across the span of 3 months.\nmay have a background in these area.  Past work, thesis etc counts, so show it in the proposal clearly\n\nIf you can do something extra it counts, but make sure the plan is reasonable within the specified time frame.",
        "Issue Links": []
    },
    "MAHOUT-331": {
        "Key": "MAHOUT-331",
        "Summary": "Continuous performance benchmarking/dashboard w/ wrappers over EC2",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "12/Mar/10 06:48",
        "Updated": "21/May/11 03:27",
        "Resolved": "26/Sep/10 09:52",
        "Description": "A student with a good proposal \n\nshould be free to work for Mahout in the summer and should be thrilled to work in this area \nshould be able to program in Java and be comfortable with datastructures and algorithms\nmust sketch out parameters to keep track of. Keep track of open-source libaries(apache friendly) which can do the same and help plug it in.\nshould have a plan to make it scalable by benchmarking it parallely\nshould have clear apis for mahout algorithms to use\nshould focus more on getting a working version than to implement all functionalities. So its recommended that you divide features into milestones\nmust have clear deadlines and pace it evenly across the span of 3 months.\n\nIf you can do something extra it counts, but make sure the plan is reasonable within the specified time frame.",
        "Issue Links": []
    },
    "MAHOUT-332": {
        "Key": "MAHOUT-332",
        "Summary": "Create adapters for  MYSQL and NOSQL(hbase, cassandra) to access data for all the algorithms to use",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "12/Mar/10 06:54",
        "Updated": "21/May/11 03:27",
        "Resolved": "26/May/10 20:07",
        "Description": "A student with a good proposal \n\nshould be free to work for Mahout in the summer and should be thrilled to work in this area \nshould be able to program in Java and be comfortable with datastructures and algorithms\nmust explore SQL and NOSQL implementations, and design a framework with which data from them could be fetched and converted to mahout format or used directly as a matrix transparently\nshould have a plan to make it high performance with ample caching strategies or the ability to use it on a map/reduce job\nshould focus more on getting a working version than to implement all functionalities. So its recommended that you divide features into milestones\nmust have clear deadlines and pace it evenly across the span of 3 months.\n\nIf you can do something extra it counts, but make sure the plan is reasonable within the specified time frame.",
        "Issue Links": []
    },
    "MAHOUT-333": {
        "Key": "MAHOUT-333",
        "Summary": "Implement a visualization tool to help a user visualize the output of clustering and other algorithms",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "12/Mar/10 06:59",
        "Updated": "21/May/11 03:27",
        "Resolved": "26/May/10 20:08",
        "Description": "A student with a good proposal \n\nshould be free to work for Mahout in the summer and should be thrilled to work in this area \nshould be able to program in Java and be comfortable with datastructures and algorithms\nmust explore opensource visualization tools(apache license friendly) and decide on what to use(or even code one on your own)\nshould have a plan on how the output of various algorithms is processed as a map/reduce job for the visualization tool to use\nshould focus more on getting a working version than to implement all functionalities. So its recommended that you divide features into milestones\nmust have clear deadlines and pace it evenly across the span of 3 months.\n\nIf you can do something extra it counts, but make sure the plan is reasonable within the specified time frame.",
        "Issue Links": []
    },
    "MAHOUT-334": {
        "Key": "MAHOUT-334",
        "Summary": "Proposal for GSoC2010 (Linear SVM for Mahout)",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "zhao zhendong",
        "Created": "12/Mar/10 19:05",
        "Updated": "21/May/11 03:27",
        "Resolved": "26/Jan/11 15:01",
        "Description": "Title/Summary: Linear SVM Package (LIBLINEAR) for Mahout\nStudent: Zhen-Dong Zhao\nStudent e-mail: zhaozd@comp.nus.edu.sg\nStudent Major: Multimedia Information Retrieval /Computer Science\nStudent Degree: Master        Student Graduation: NUS'10           Organization: Hadoop\n0 Abstract\nLinear Support Vector Machine (SVM) is pretty useful in some applications with large-scale datasets or datasets with high dimension features. This proposal will port one of the most famous linear SVM solvers, say, LIBLINEAR [1] to mahout with unified interface as same as Pegasos [2] @ mahout, which is another linear SVM solver and almost finished by me. Two distinct con tributions would be: 1) Introduce LIBLINEAR to Mahout; 2) Uni\ufb01ed interfaces for linear SVM classi\ufb01er.\n1 Motivation\nAs one of TOP 10 algorithms in data mining society [3], Support Vector Machine is very powerful Machine Learning tool and widely adopted in Data Mining, Pattern Recognition and Information Retrieval domains.\nThe SVM training procedure is pretty slow, however, especially on the case with large-scale dataset. Nowadays, several literatures propose SVM solvers with linear kernel that can handle large-scale learning problem, for instance, LIBLINEAR [1] and Pegasos [2]. I have implemented a prototype of linear SVM classi\ufb01er based on Pegasos [2] for Mahout (issue: Mahout-232). Nevertheless, as the winner of ICML 2008 large-scale learning challenge (linear SVM track (http://largescale.first.fraunhofer.de/summary/), LIBLINEAR [1] suppose to be incorporated in Mahout too. Currently, LIBLINEAR package supports:\n  (1) L2-regularized classi\ufb01ers L2-loss linear SVM, L1-loss linear SVM, and logistic regression (LR)\n  (2) L1-regularized classi\ufb01ers L2-loss linear SVM and logistic regression (LR)\nMain features of LIBLINEAR are following:\n  (1) Multi-class classi\ufb01cation: 1) one-vs-the rest, 2) Crammer & Singer\n  (2) Cross validation for model selection\n  (3) Probability estimates (logistic regression only)\n  (4) Weights for unbalanced data\nAll the functionalities suppose to be implemented except probability estimates and weights for unbalanced data (If time permitting, I would like to do so).\n2 Unified Interfaces\nLinear SVM classi\ufb01er based on Pegasos package on Mahout already can provide such functionalities: (http://issues.apache.org/jira/browse/MAHOUT-232)\n  (1) Sequential Binary Classi\ufb01cation (Two-class Classi\ufb01cation), includes sequential training and prediction;\n  (2) Sequential Regression;\n  (3) Parallel & Sequential Multi-Classi\ufb01cation, includes One-vs.-One and One-vs.-Others schemes.\nApparently, the functionalities of Pegasos package on Mahout and LIBLINEAR are quite similar to each other. As aforementioned, in this section I will introduce an unified interfaces for linear SVM classi\ufb01er on Mahout, which will incorporate Pegasos, LIBLINEAR. \nThe unfied interfaces has two main parts: 1) Dataset loader; 2) Algorithms. I will introduce them separately.\n2.1 Data Handler\nThe dataset can be stored on personal computer or on Hadoop cluster. This framework provides high performance Random Loader, Sequential Loader for accessing large-scale data.\n2.2 Sequential Algorithms\nSequential Algorithms will include binary classi\ufb01cation, regression based on Pegasos and LIBLINEAR with uni\ufb01ed interface.\n2.3 Parallel Algorithms\nIt is widely accepted that to parallelize binary SVM classi\ufb01er is hard. For multi-classi\ufb01cation, however, the coarse-grained scheme (e.g. each Mapper or Reducer has one independent SVM binary classi\ufb01er) is easier to achieve great improvement. Besides, cross validation for model selection also can take advantage of such coarse-grained parallelism. I will introduce a uni\ufb01ed interface for all of them.\n3 Biography:\nI am a graduating masters student in Multimedia Information Retrieval System from National University of Singapore. My research has involved the large-scale SVM classifier.\nI have worked with Hadoop and Map Reduce since one year ago, and I have dedicated lots of my spare time to Sequential SVM (Pegasos) based on Mahout (http://issues.apache.org/jira/browse/MAHOUT-232). I have taken part in setting up and maintaining a Hadoop cluster with around 70 nodes in our group.\n4 Timeline:\nWeeks 1-4 (May 24 ~ June 18): Implement binary classifier \nWeeks 5-7 (June 21 ~ July 12): Implement parallel multi-class classification and Implement cross validation for model selection. \nWeeks 8 (July 12 ~ July 16): Summit of mid-term evaluation\nWeeks 9 - 11 (July 16 ~ August 9):  Interface re-factory and performance turning\nWeeks 11 - 12 (August 9 ~ August 16): Code cleaning, documents and testing. \n5 References\n[1] Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. Liblinear: A library for large linear classi\ufb01cation. J. Mach. Learn. Res., 9:1871-1874, 2008.\n[2] Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro. Pegasos: Primal estimated sub-gradient solver for svm. In ICML '07: Proceedings of the 24th international conference on Machine learning, pages 807-814, New York, NY, USA, 2007. ACM.\n[3] Xindong Wu, Vipin Kumar, J. Ross Quinlan, Joydeep Ghosh, Qiang Yang, Hiroshi Motoda, Geo\ufb00rey J. McLachlan, Angus Ng, Bing Liu, Philip S. Yu, Zhi-Hua Zhou, Michael Steinbach, David J. Hand, and Dan Steinberg. Top 10 algorithms in data mining. Knowl. Inf. Syst., 14(1):1-37, 2007.",
        "Issue Links": []
    },
    "MAHOUT-335": {
        "Key": "MAHOUT-335",
        "Summary": "Mahout Logo tweak",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Documentation",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "13/Mar/10 16:19",
        "Updated": "09/Aug/11 15:07",
        "Resolved": "15/Mar/10 10:38",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-336": {
        "Key": "MAHOUT-336",
        "Summary": "Update Site for the Release",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "15/Mar/10 03:29",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "20/Apr/10 11:48",
        "Description": "Few things:\nSite doesnt say maven as a requirement. \nNeed to add new logo and favicon to the site\nhudson code coverage and other things still point to Dec 21 version.",
        "Issue Links": []
    },
    "MAHOUT-337": {
        "Key": "MAHOUT-337",
        "Summary": "Don't serialize cached length squared in JSON vector representation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "15/Mar/10 11:01",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "26/May/10 19:06",
        "Description": "The cached length-squared field in vectors should be marked transient so that it is not part of the JSON serialized state.",
        "Issue Links": []
    },
    "MAHOUT-338": {
        "Key": "MAHOUT-338",
        "Summary": "Simplify maven structure",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "16/Mar/10 14:40",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "06/Sep/10 12:05",
        "Description": "As per some email to the dev list, I plan to streamline and simplify the maven structure, by eliminating the 'maven' directory and letting the top-level POM be the parent, moving release packaging to a separate distribution directory.\nI do not plan to post patches here.",
        "Issue Links": []
    },
    "MAHOUT-339": {
        "Key": "MAHOUT-339",
        "Summary": "Class Cast Exception Running Synthetic Control MeanShift Clustering Job",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "17/Mar/10 21:18",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "02/Apr/10 21:53",
        "Description": "Mar 17, 2010 2:15:00 PM org.apache.hadoop.mapred.LocalJobRunner$Job run\nWARNING: job_local_0002\njava.lang.ClassCastException: org.apache.mahout.clustering.meanshift.MeanShiftCanopy cannot be cast to org.apache.mahout.math.VectorWritable\n\tat org.apache.mahout.clustering.meanshift.MeanShiftCanopyCreatorMapper.map(MeanShiftCanopyCreatorMapper.java:1)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)",
        "Issue Links": []
    },
    "MAHOUT-340": {
        "Key": "MAHOUT-340",
        "Summary": "org.apache.mahout.cf.taste.hadoop.cooccurence can not support long as user_id and item_id",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "19/Mar/10 07:37",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "25/Mar/10 11:46",
        "Description": "I have preferences data using long as user_id and item_id,\nhadoop cooccurence arithmetic  can not support it",
        "Issue Links": []
    },
    "MAHOUT-341": {
        "Key": "MAHOUT-341",
        "Summary": "org.apache.mahout.cf.taste.hadoop.slopeone could have an off-line implementation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "19/Mar/10 07:54",
        "Updated": "21/May/11 03:27",
        "Resolved": "31/Jul/10 15:22",
        "Description": "slopeone arithmetic using hadoop is not complete .\ncan not use it do recommendation for dataset that has rating.\nhope to complete it and gave  a full solution",
        "Issue Links": []
    },
    "MAHOUT-342": {
        "Key": "MAHOUT-342",
        "Summary": "[GSOC] Implement Map/Reduce Enabled Neural Networks",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "19/Mar/10 13:15",
        "Updated": "21/May/11 03:22",
        "Resolved": "26/May/10 20:13",
        "Description": "Per the Ng. et. al. paper, implement a neural network with back propagation.",
        "Issue Links": []
    },
    "MAHOUT-343": {
        "Key": "MAHOUT-343",
        "Summary": "[GSOC] Implement Integration of Mahout Clustering or Classification with Apache Solr",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "19/Mar/10 13:19",
        "Updated": "21/May/11 03:27",
        "Resolved": "26/Sep/10 09:54",
        "Description": "There are many places where Mahout and Lucene/Solr can be hooked together.  For instance, automatically classifying documents to be indexed and adding fields contain said classification.  Another one is to write the Mahout Clustering Component integration that will take a Lucene index and go and cluster it (off line in the background) and then make the results available as part of a Solr request.  Further extension would allow for the results to be used as filters in Solr.",
        "Issue Links": [
            "/jira/browse/SOLR-1715"
        ]
    },
    "MAHOUT-344": {
        "Key": "MAHOUT-344",
        "Summary": "Minhash based clustering",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Ankur Bansal",
        "Reporter": "Ankur Bansal",
        "Created": "22/Mar/10 07:44",
        "Updated": "29/Nov/11 13:18",
        "Resolved": "01/Oct/10 07:40",
        "Description": "Minhash clustering performs probabilistic dimension reduction of high dimensional data. The essence of the technique is to hash each item using multiple independent hash functions such that the probability of collision of similar items is higher. Multiple such hash tables can then be constructed  to answer near neighbor type of queries efficiently.",
        "Issue Links": []
    },
    "MAHOUT-345": {
        "Key": "MAHOUT-345",
        "Summary": "integrate Mahout with Drupal/PHP",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Daniel Xiaodan Zhou",
        "Created": "22/Mar/10 19:51",
        "Updated": "09/Jun/11 14:28",
        "Resolved": "03/Jun/10 13:39",
        "Description": "Drupal is a very popular open source web content management system. It's been widely used in e-commerce sites, media sites, etc. This is a list of famous site using Drupal: http://socialcmsbuzz.com/45-drupal-sites-which-you-may-not-have-known-were-drupal-based-24092008/\nIntegrate Mahout with Drupal would greatly increase the impact of Mahout in web systems: any Drupal website can easily use Mahout to make content recommendations or cluster contents.\nI'm a PhD student at University of Michigan, with a research focus on recommender systems. Last year I participated GSOC 2009 with Drupal.org, and developed a recommender system for Drupal. But that module was not as sophisticated as Mahout. And I think it would be nice just to integrate Mahout into Drupal rather than developing a separate Mahout-like module for Drupal.\nAny comments? I can provide more information if people here are interested. Thanks.",
        "Issue Links": []
    },
    "MAHOUT-346": {
        "Key": "MAHOUT-346",
        "Summary": "[GSOC] Your Machine Learning Idea Here",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "23/Mar/10 15:37",
        "Updated": "21/May/11 03:27",
        "Resolved": "26/May/10 19:19",
        "Description": "Write up your GSOC idea for Mahout.  Make sure it is well referenced and well written.  Do not take on the world in your proposal.  In my experience (I've done it 2 years now) most students have time to implement 1 algorithm + documentation + examples + unit tests.  That is it.  I pretty much reject any proposal that says differently, as it simply implies you don't get what is involved in the task.  There are lots of opportunities out there for adding capabilities to Mahout.\nSee the Mahout mail archives in past years for tips: http://www.lucidimagination.com/search/?q=gsoc+Mahout",
        "Issue Links": []
    },
    "MAHOUT-347": {
        "Key": "MAHOUT-347",
        "Summary": "org.apache.mahout.math.IndexException: index(-521252222) is out of bounds of (0, 2147483647)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "26/Mar/10 14:29",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "26/Mar/10 20:26",
        "Description": "when run job org.apache.mahout.cf.taste.hadoop.item.RecommenderJob\nget following error:\n10/03/26 09:11:05 INFO mapred.JobClient: Task Id : attempt_201003221228_0170_r_000000_0, Status : FAILED\norg.apache.mahout.math.IndexException: index(-521252222) is out of bounds of (0, 2147483647)\n        at org.apache.mahout.math.AbstractVector.set(AbstractVector.java:353)\n        at org.apache.mahout.cf.taste.hadoop.item.ToUserVectorReducer.reduce(ToUserVectorReducer.java:75)\n        at org.apache.mahout.cf.taste.hadoop.item.ToUserVectorReducer.reduce(ToUserVectorReducer.java:58)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n        at org.apache.hadoop.mapred.Child.main(Child.java:170)",
        "Issue Links": []
    },
    "MAHOUT-348": {
        "Key": "MAHOUT-348",
        "Summary": "Trainer jobs should implement Hadoop's Tool",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Ferdy",
        "Created": "29/Mar/10 10:49",
        "Updated": "21/May/11 03:22",
        "Resolved": "26/Sep/10 09:55",
        "Description": "It would be nice if the Trainer jobs (and Mahout jobs in general, those not already doing so) would implement Tool. From the Hadoop's javadocs:\n\"Tool, is the standard for any Map-Reduce tool/application. The tool/application should delegate the handling of standard command-line options to ToolRunner.run(Tool, String[]) and only handle its custom arguments.\"\nThe problem we are running into currently is the fact that as of Mahout 0.3 there is no way to submit a CBayesDriver job with custom Configuration. Therefore it is not possible to set the classpath right for it's Mappers and Reducers, if one is to run the CBayesDriver with the generic \"-libjars\" option. Of course, this particular problem could be solved by just putting the required jars in the Hadoop lib dir, however this not always possible. For a custom Hadoop deployment (shared among many users and different types of jobs), every job should be able to specify it's own library dependencies.\nNote: I'm currently aware of issue MAHOUT-167, which has limited overlap with this issue: MAHOUT-167 states that the new API should be used (particulary for Clustering jobs). This issue addresses the needs for implementing a Hadoop Job interface at all, preferably Tool.\nAlso, there's issue MAHOUT-294, an effort to track all changes surrounding the Job API.\nLet me hear your thoughts, and I'll whip up a patch when needed.",
        "Issue Links": []
    },
    "MAHOUT-349": {
        "Key": "MAHOUT-349",
        "Summary": "has no 04-SNAPSHOT.jar file in the maven repositoty",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "29/Mar/10 11:14",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "27/Sep/10 06:53",
        "Description": "here is  no 04-SNAPSHOT.jar file in the maven repository.\nit's hard to import the snapshot to project",
        "Issue Links": []
    },
    "MAHOUT-350": {
        "Key": "MAHOUT-350",
        "Summary": "add  one \"JobName\" and reduceNumber parameter to org.apache.mahout.cf.taste.hadoop.item.RecommenderJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "29/Mar/10 15:35",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "02/Apr/10 09:12",
        "Description": "Can add one  \"JobName\" parameter to org.apache.mahout.cf.taste.hadoop.item.RecommenderJob?\nif there's a lot of RecommenderJob,it's hard to distinguish  those jobs.\nalso RecommenderJob has four sub jobs (or phase ) ,can add sub-job name to those phase ?\nBecause RecommenderJob has not setNumReduceTasks ,it seems that the performance is not good in reduce phase.",
        "Issue Links": []
    },
    "MAHOUT-351": {
        "Key": "MAHOUT-351",
        "Summary": "can not set usersFile as null when run org.apache.mahout.cf.taste.hadoop.item.RecommenderJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "30/Mar/10 09:04",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "30/Mar/10 10:18",
        "Description": "I\u3000want to get all users recommendations using RecommenderJob.\nit seems need not set the usersFile  parameters,\nIn Class:RecommenderMapper.java\n      String usersFilePathString = jobConf.get(USERS_FILE);\n      if (usersFilePathString == null) \n{\n        usersToRecommendFor = null;\n      }\n else {\n        usersToRecommendFor = new FastIDSet();\n        Path usersFilePath = new Path(usersFilePathString).makeQualified(fs);\n        FSDataInputStream in = fs.open(usersFilePath);\n        for (String line : new FileLineIterable(in)) \n{\n          usersToRecommendFor.add(Long.parseLong(line));\n        }\n      }\n    if ((usersToRecommendFor != null) && !usersToRecommendFor.contains(userID.get())) \n{\n      return;\n    }\n\nbut when I\u3000\uff49nore userFile parameters ,get following error:\nrun job :\nrandy@dan:~/app/steer/item$ hadoop  org.apache.mahout.cf.taste.hadoop.item.RecommenderJob -Dmapred.job.name=HADOOP_REC_tap_tag\\  -Dmapred.reduce.tasks=200 --input /steer/item/in  --tempDir /steer/item/temp --output /steer/item/out --jarFile mahout-0.4-SNAPSHOT.jar --numRecommendations 10\nException in thread \"main\" java.lang.NullPointerException\n        at java.util.Hashtable.put(Hashtable.java:394)\n        at java.util.Properties.setProperty(Properties.java:143)\n        at org.apache.hadoop.conf.Configuration.set(Configuration.java:403)\n        at org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.run(RecommenderJob.java:110)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.main(RecommenderJob.java:117)",
        "Issue Links": []
    },
    "MAHOUT-352": {
        "Key": "MAHOUT-352",
        "Summary": "not compression the final output of RecommenderJob",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "30/Mar/10 12:58",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "01/Apr/10 11:03",
        "Description": "in RecommenderJob Class:\nline 113 :recommenderConf.setClass(\"mapred.output.compression.codec\", GzipCodec.class, CompressionCodec.class);\nbecause the output here  is the final output ,the end user may operates the output through HDFS API,\nif compress here ,it not very convenient.\nbut it's convenient for operation through HDFS\u3000shell to get to local.\ncan provider options or not compress ?",
        "Issue Links": []
    },
    "MAHOUT-353": {
        "Key": "MAHOUT-353",
        "Summary": "java.lang.NullPointerException in RecommenderMapper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "30/Mar/10 13:48",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "01/Apr/10 11:20",
        "Description": "java.lang.NullPointerException\n\tat org.apache.mahout.cf.taste.hadoop.item.RecommenderMapper$CooccurrenceCache.get(RecommenderMapper.java:169)\n\tat org.apache.mahout.cf.taste.hadoop.item.RecommenderMapper$CooccurrenceCache.get(RecommenderMapper.java:154)\n\tat org.apache.mahout.cf.taste.impl.common.Cache.getAndCacheValue(Cache.java:125)\n\tat org.apache.mahout.cf.taste.impl.common.Cache.get(Cache.java:94)\n\tat org.apache.mahout.cf.taste.hadoop.item.RecommenderMapper.map(RecommenderMapper.java:111)\n\tat org.apache.mahout.cf.taste.hadoop.item.RecommenderMapper.map(RecommenderMapper.java:52)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)",
        "Issue Links": []
    },
    "MAHOUT-354": {
        "Key": "MAHOUT-354",
        "Summary": "make the output of RecommenderJob more readable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "31/Mar/10 10:17",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "27/Apr/10 08:49",
        "Description": "now the output of  RecommenderMapper as following:\noutput.collect(userID, new RecommendedItemsWritable(recommendations));\nCan we change it more readable like following:\n   private final Text user = new Text();\n    private final Text recomScore = new Text();\n    private static final String FIELD_SEPERATOR = \",\";\n    for (RecommendedItem recommendation : recommendations)\n    {\n    \tuser.set(String.valueOf(userId));\n    \trecomScore.set(recommendation.getItemID() + FIELD_SEPERATOR + recommendation.getValue());\n        output.collect(user, recomScore);\n    }\n\nthen user can read and verify the result more convenient and need not depend on the mahout  API",
        "Issue Links": []
    },
    "MAHOUT-355": {
        "Key": "MAHOUT-355",
        "Summary": "Misleading JavaDoc comment in FPGrowth",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2,                                            0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "31/Mar/10 12:16",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "01/Apr/10 11:11",
        "Description": "The JavaDoc for org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPGrowth.generateTopKFrequentPatterns(...) states that if the parameter returnableFeatures is null then patterns for every frequent item are generated. \nBut this works only if you give the method an empty set, submitting null results in a NullPointerException. Either change the comment or do a null check here.",
        "Issue Links": []
    },
    "MAHOUT-356": {
        "Key": "MAHOUT-356",
        "Summary": "ClassNotFoundException: org.apache.mahout.math.function.IntDoubleProcedure",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Kris Jack",
        "Created": "01/Apr/10 16:38",
        "Updated": "21/May/11 03:22",
        "Resolved": "19/Apr/10 12:25",
        "Description": "When running org.apache.mahout.cf.taste.hadoop.item.RecommenderJob in a pseudo-distributed hadoop, I get a java class not found exception.\nFull Output:\n10/04/01 16:50:42 INFO mapred.FileInputFormat: Total input paths to process : 1\n10/04/01 16:50:43 INFO mapred.JobClient: Running job: job_201004011631_0005\n10/04/01 16:50:44 INFO mapred.JobClient:  map 0% reduce 0%\n10/04/01 16:50:55 INFO mapred.JobClient:  map 2% reduce 0%\n10/04/01 16:50:58 INFO mapred.JobClient:  map 14% reduce 0%\n10/04/01 16:51:01 INFO mapred.JobClient:  map 24% reduce 0%\n10/04/01 16:51:04 INFO mapred.JobClient:  map 33% reduce 0%\n10/04/01 16:51:07 INFO mapred.JobClient:  map 41% reduce 0%\n10/04/01 16:51:10 INFO mapred.JobClient:  map 50% reduce 0%\n10/04/01 16:51:23 INFO mapred.JobClient:  map 63% reduce 0%\n10/04/01 16:51:26 INFO mapred.JobClient:  map 72% reduce 16%\n10/04/01 16:51:29 INFO mapred.JobClient:  map 83% reduce 16%\n10/04/01 16:51:32 INFO mapred.JobClient:  map 92% reduce 16%\n10/04/01 16:51:35 INFO mapred.JobClient:  map 98% reduce 16%\n10/04/01 16:51:38 INFO mapred.JobClient:  map 100% reduce 16%\n10/04/01 16:51:41 INFO mapred.JobClient:  map 100% reduce 25%\n10/04/01 16:51:59 INFO mapred.JobClient:  map 100% reduce 100%\n10/04/01 16:52:01 INFO mapred.JobClient: Job complete: job_201004011631_0005\n10/04/01 16:52:01 INFO mapred.JobClient: Counters: 18\n10/04/01 16:52:01 INFO mapred.JobClient:   Job Counters \n10/04/01 16:52:01 INFO mapred.JobClient:     Launched reduce tasks=1\n10/04/01 16:52:01 INFO mapred.JobClient:     Launched map tasks=4\n10/04/01 16:52:01 INFO mapred.JobClient:     Data-local map tasks=4\n10/04/01 16:52:01 INFO mapred.JobClient:   FileSystemCounters\n10/04/01 16:52:01 INFO mapred.JobClient:     FILE_BYTES_READ=603502320\n10/04/01 16:52:01 INFO mapred.JobClient:     HDFS_BYTES_READ=257007616\n10/04/01 16:52:01 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=846533316\n10/04/01 16:52:01 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=3417233\n10/04/01 16:52:01 INFO mapred.JobClient:   Map-Reduce Framework\n10/04/01 16:52:01 INFO mapred.JobClient:     Reduce input groups=168791\n10/04/01 16:52:01 INFO mapred.JobClient:     Combine output records=0\n10/04/01 16:52:01 INFO mapred.JobClient:     Map input records=17359346\n10/04/01 16:52:01 INFO mapred.JobClient:     Reduce shuffle bytes=179672560\n10/04/01 16:52:01 INFO mapred.JobClient:     Reduce output records=168791\n10/04/01 16:52:01 INFO mapred.JobClient:     Spilled Records=60466622\n10/04/01 16:52:01 INFO mapred.JobClient:     Map output bytes=208312152\n10/04/01 16:52:01 INFO mapred.JobClient:     Map input bytes=256995325\n10/04/01 16:52:01 INFO mapred.JobClient:     Combine input records=0\n10/04/01 16:52:01 INFO mapred.JobClient:     Map output records=17359346\n10/04/01 16:52:01 INFO mapred.JobClient:     Reduce input records=17359346\n10/04/01 16:52:01 INFO mapred.FileInputFormat: Total input paths to process : 1\n10/04/01 16:52:01 INFO mapred.JobClient: Running job: job_201004011631_0006\n10/04/01 16:52:02 INFO mapred.JobClient:  map 0% reduce 0%\n10/04/01 16:52:17 INFO mapred.JobClient:  map 15% reduce 0%\n10/04/01 16:52:20 INFO mapred.JobClient:  map 25% reduce 0%\n10/04/01 16:52:23 INFO mapred.JobClient:  map 34% reduce 0%\n10/04/01 16:52:26 INFO mapred.JobClient:  map 45% reduce 0%\n10/04/01 16:52:29 INFO mapred.JobClient:  map 50% reduce 0%\n10/04/01 16:52:41 INFO mapred.JobClient:  map 62% reduce 0%\n10/04/01 16:52:44 INFO mapred.JobClient:  map 70% reduce 16%\n10/04/01 16:52:48 INFO mapred.JobClient:  map 81% reduce 16%\n10/04/01 16:52:51 INFO mapred.JobClient:  map 91% reduce 16%\n10/04/01 16:52:53 INFO mapred.JobClient:  map 96% reduce 16%\n10/04/01 16:52:56 INFO mapred.JobClient:  map 100% reduce 16%\n10/04/01 16:53:02 INFO mapred.JobClient:  map 100% reduce 25%\n10/04/01 16:53:05 INFO mapred.JobClient:  map 100% reduce 0%\n10/04/01 16:53:07 INFO mapred.JobClient: Task Id : attempt_201004011631_0006_r_000000_0, Status : FAILED\nError: java.lang.ClassNotFoundException: org.apache.mahout.math.function.IntDoubleProcedure\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:200)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n\tat java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\n\tat org.apache.mahout.cf.taste.hadoop.item.ToUserVectorReducer.reduce(ToUserVectorReducer.java:71)\n\tat org.apache.mahout.cf.taste.hadoop.item.ToUserVectorReducer.reduce(ToUserVectorReducer.java:58)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\n10/04/01 16:53:22 INFO mapred.JobClient: Task Id : attempt_201004011631_0006_r_000000_1, Status : FAILED\nError: java.lang.ClassNotFoundException: org.apache.mahout.math.function.IntDoubleProcedure\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:200)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n\tat java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\n\tat org.apache.mahout.cf.taste.hadoop.item.ToUserVectorReducer.reduce(ToUserVectorReducer.java:71)\n\tat org.apache.mahout.cf.taste.hadoop.item.ToUserVectorReducer.reduce(ToUserVectorReducer.java:58)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\n10/04/01 16:53:33 INFO mapred.JobClient:  map 100% reduce 8%\n10/04/01 16:53:36 INFO mapred.JobClient:  map 100% reduce 0%\n10/04/01 16:53:38 INFO mapred.JobClient: Task Id : attempt_201004011631_0006_r_000000_2, Status : FAILED\nError: java.lang.ClassNotFoundException: org.apache.mahout.math.function.IntDoubleProcedure\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:200)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n\tat java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\n\tat org.apache.mahout.cf.taste.hadoop.item.ToUserVectorReducer.reduce(ToUserVectorReducer.java:71)\n\tat org.apache.mahout.cf.taste.hadoop.item.ToUserVectorReducer.reduce(ToUserVectorReducer.java:58)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\n10/04/01 16:53:53 INFO mapred.JobClient: Job complete: job_201004011631_0006\n10/04/01 16:53:53 INFO mapred.JobClient: Counters: 14\n10/04/01 16:53:53 INFO mapred.JobClient:   Job Counters \n10/04/01 16:53:53 INFO mapred.JobClient:     Launched reduce tasks=4\n10/04/01 16:53:53 INFO mapred.JobClient:     Launched map tasks=4\n10/04/01 16:53:53 INFO mapred.JobClient:     Data-local map tasks=4\n10/04/01 16:53:53 INFO mapred.JobClient:     Failed reduce tasks=1\n10/04/01 16:53:53 INFO mapred.JobClient:   FileSystemCounters\n10/04/01 16:53:53 INFO mapred.JobClient:     FILE_BYTES_READ=566454892\n10/04/01 16:53:53 INFO mapred.JobClient:     HDFS_BYTES_READ=257007616\n10/04/01 16:53:53 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=948360656\n10/04/01 16:53:53 INFO mapred.JobClient:   Map-Reduce Framework\n10/04/01 16:53:53 INFO mapred.JobClient:     Combine output records=0\n10/04/01 16:53:53 INFO mapred.JobClient:     Map input records=17359346\n10/04/01 16:53:53 INFO mapred.JobClient:     Spilled Records=43107276\n10/04/01 16:53:53 INFO mapred.JobClient:     Map output bytes=347186920\n10/04/01 16:53:53 INFO mapred.JobClient:     Map input bytes=256995325\n10/04/01 16:53:53 INFO mapred.JobClient:     Combine input records=0\n10/04/01 16:53:53 INFO mapred.JobClient:     Map output records=17359346\nException in thread \"main\" java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1293)\n\tat org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.run(RecommenderJob.java:92)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.main(RecommenderJob.java:116)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nIs it a configuration problem on my side or a problem with the code?\nMany thanks,\nKris",
        "Issue Links": []
    },
    "MAHOUT-357": {
        "Key": "MAHOUT-357",
        "Summary": "Implement a clustering algorithm on mapreduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "new user",
        "Created": "01/Apr/10 21:17",
        "Updated": "21/May/11 03:27",
        "Resolved": "31/Jul/10 14:18",
        "Description": "As I mentioned in my previous posts that I am interseted in implementing the clustering algorithm on mapreduce.So,now I am going to tell what I have thought to implement this. Thinking of the k-means algorithm for clustering,it appears that the whole set of data has to copied on each of the nodes of the hadoop framework to process the data in each iteration of the k-means clustering. But, this can be done without useless replication  of data on the clusters.First of all, we select a set of k elements as the initial clusters.This can be purely random or decided on the basis of some criteria.We maintain a file which stores the id of each cluster, the number of elements in each cluster, and the exact position of the cluster in terms of its co-ordinates.This file has to be shared by each of the nodes. During each iteration of the algorithm, the following steps are done:\n1. As each node has a part of the initial data,during the map phase, it calculates the distance of each of the element from the k cluster centroids. For each row,the smallest distance is chosen and the id of the cluster and the position of that element is stored.\n2.During the combine phase, for each of the cluster, the average of the co-ordinates for all the elements is calculated and the number of elements in that cluster. So, the combiner funnction outputs the cluster id and the average co-ordinates of the elements.\n3. During the reduce phase, the cluster centroid is re-calculated using the weighted averages of the co-ordinates.\nThus , after these 3 steps, the new value of centorid for each cluster and the number of elemnts in each cluster is updated.\nThe above three steps can be performed iteratively as long as the condition set for the convergence is not satisfied, by applying the map-combine-reduce phases again.\nI have proposed this as per my understanding of the probelem and my knowledge. If anybody have any doubts or want to add anything or suggest anything anything,then please respond as soon as possible. And, if you consider it a good idea, then please suggest how to proceed further in the Gsoc procedure.",
        "Issue Links": []
    },
    "MAHOUT-358": {
        "Key": "MAHOUT-358",
        "Summary": "the pref value  field of output of org.apache.mahout.cf.taste.hadoop.item.RecommenderJob has negative",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "02/Apr/10 06:07",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "07/Apr/10 16:20",
        "Description": "In my test the input pref values all is positive.\nthe output score value has negative value ,",
        "Issue Links": []
    },
    "MAHOUT-359": {
        "Key": "MAHOUT-359",
        "Summary": "org.apache.mahout.cf.taste.hadoop.item.RecommenderJob for Boolean recommendation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "02/Apr/10 08:42",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "27/Apr/10 08:52",
        "Description": "in some case there has no preference value in the input data ,the preference value is set to zero,then \nRecommenderMapper.class\n @Override\n  public void map(LongWritable userID,\n                  VectorWritable vectorWritable,\n                  OutputCollector<LongWritable,RecommendedItemsWritable> output,\n                  Reporter reporter) throws IOException {\n    if ((usersToRecommendFor != null) && !usersToRecommendFor.contains(userID.get())) \n{\n      return;\n    }\n    Vector userVector = vectorWritable.get();\n    Iterator<Vector.Element> userVectorIterator = userVector.iterateNonZero();\n    Vector recommendationVector = new RandomAccessSparseVector(Integer.MAX_VALUE, 1000);\n    while (userVectorIterator.hasNext()) {\n      Vector.Element element = userVectorIterator.next();\n      int index = element.index();\n      double value = element.get();     //here will get 0.0 for Boolean recommendation \n      Vector columnVector;\n      try \n{\n        columnVector = cooccurrenceColumnCache.get(new IntWritable(index));\n      }\n catch (TasteException te) {\n        if (te.getCause() instanceof IOException) \n{\n          throw (IOException) te.getCause();\n        }\n else \n{\n          throw new IOException(te.getCause());\n        }\n      }\n      if (columnVector != null) \n{\n        columnVector.times(value).addTo(recommendationVector); //here will set all score value to zero for Boolean recommendation\n      }\n    }\n    Queue<RecommendedItem> topItems = new PriorityQueue<RecommendedItem>(recommendationsPerUser + 1,\n        Collections.reverseOrder());\n    Iterator<Vector.Element> recommendationVectorIterator = recommendationVector.iterateNonZero();\n    LongWritable itemID = new LongWritable();\n    while (recommendationVectorIterator.hasNext()) {\n      Vector.Element element = recommendationVectorIterator.next();\n      int index = element.index();\n      if (userVector.get(index) == 0.0) {\n        if (topItems.size() < recommendationsPerUser) \n{\n          indexItemIDMap.get(new IntWritable(index), itemID);\n          topItems.add(new GenericRecommendedItem(itemID.get(), (float) element.get()));\n        }\n else if (element.get() > topItems.peek().getValue()) \n{\n          indexItemIDMap.get(new IntWritable(index), itemID);\n          topItems.add(new GenericRecommendedItem(itemID.get(), (float) element.get()));\n          topItems.poll();\n        }\n      }\n    }\n    List<RecommendedItem> recommendations = new ArrayList<RecommendedItem>(topItems.size());\n    recommendations.addAll(topItems);\n    Collections.sort(recommendations);\n    output.collect(userID, new RecommendedItemsWritable(recommendations));\n  }\nso maybe we need a option to distinguish boolean recommendation and slope one recommendation.\nin ToUserVectorReducer.class\nhere no need findTopNPrefsCutoff,maybe take all item.\nit's just my thinking ,maybe item is used for slope one only .",
        "Issue Links": []
    },
    "MAHOUT-360": {
        "Key": "MAHOUT-360",
        "Summary": "the pref score  field of output of org.apache.mahout.cf.taste.hadoop.item.RecommenderJob has been truncated",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "02/Apr/10 09:54",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "02/Apr/10 10:11",
        "Description": "the pref score  field of output of org.apache.mahout.cf.taste.hadoop.item.RecommenderJob has been truncated if the pref score exceed six byte.\nin RecommendedItemsWritable.class\n  @Override\n  public String toString() {\n    StringBuilder result = new StringBuilder(200);\n    result.append('[');\n    boolean first = true;\n    for (RecommendedItem item : recommended) {\n      if (first) \n{\n        first = false;\n      }\n else \n{\n        result.append(',');\n      }\n      result.append(item.getItemID());\n      result.append(':');\n      String valueString = String.valueOf(item.getValue());\n      // Is this rounding too crude?\n      if (valueString.indexOf('E') >= 0) \n{\n        valueString = \"0.0\";\n      }\n else if (valueString.length() > 6) \n{    \n        valueString = valueString.substring(0, 6);   //here may cause problem,we can reserve all the bytes ,or just one byte decimal fraction\n      }\n      result.append(valueString);\n    }\n    result.append(']');\n    return result.toString();\n  }",
        "Issue Links": []
    },
    "MAHOUT-361": {
        "Key": "MAHOUT-361",
        "Summary": "SLF4J dependency structure leads to unpleasant surproses",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Benson Margulies",
        "Created": "02/Apr/10 10:53",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "28/May/10 15:45",
        "Description": "Our poms declare a dependency on the slf4j core, but not on any of the implementation modules. Thus, if an unsuspecting user adds a dependency on our stuff, and runs, they get a exception from slf4j complaining that there's no implementation. I claim that it would be more better to declare a dependency on the JDK14 module, and those users who really care about using something else can exclude it and include their own.",
        "Issue Links": []
    },
    "MAHOUT-362": {
        "Key": "MAHOUT-362",
        "Summary": "Computation of pairwise cosine similarities for Item-Based Collaborative Filtering",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "04/Apr/10 17:24",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "05/Apr/10 15:56",
        "Description": "Provides a map/reduce job to precompute the pairwise cosine similarities between the item vectors of the user-item-matrix.  \nThe code uses a slightly modified version of the algorithm suggested in \"Elsayed et al: Pairwise Document Similarity in Large Collections with MapReduce\" (http://www.umiacs.umd.edu/~jimmylin/publications/Elsayed_etal_ACL2008_short.pdf)",
        "Issue Links": []
    },
    "MAHOUT-363": {
        "Key": "MAHOUT-363",
        "Summary": "Proposal for GSoC 2010 (EigenCuts clustering algorithm for Mahout)",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "05/Apr/10 04:21",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "30/Sep/10 21:43",
        "Description": "Proposal Title: EigenCuts spectral clustering implementation on map/reduce for Apache Mahout (addresses issue Mahout-328)\nStudent Name: Shannon Quinn\nStudent E-mail: magsol@gmail.com\nOrganization/Project:Assigned Mentor:\nProposal Abstract:\nClustering algorithms are advantageous when the number of classes are not known a priori. However, most techniques still require an explicit K to be chosen, and most spectral algorithms' use of piecewise constant approximation of eigenvectors breaks down when the clusters are tightly coupled. EigenCuts[1] solves both these problems by choosing an eigenvector to create a new cluster boundary and iterating until no more edges are cut.\nDetailed Description\nClustering techniques are extremely useful unsupervised methods, particularly within my field of computational biology, for situations where the number (and often the characteristics as well) of classes expressed in the data are not known a priori. K-means is a classic technique which, given some K, attempts to label data points within a cluster as a function of their distance (e.g. Euclidean) from the cluster's mean, iterating to convergence.\nAnother approach is spectral clustering, which models the data as a weighted, undirected graph in some n-dimensional space, and creates a matrix M of transition probabilities between nodes. By computing the eigenvalues and eigenvectors of this matrix, most spectral clustering techniques take advantage of the fact that, for data with loosely coupled clusters, the K leading eigenvectors will identify the roughly piecewise constant regions in the data that correspond to clusters.\nHowever, these techniques all suffer from drawbacks, the two most significant of which are having to choose an arbitrary K a priori, and in the situation of tightly coupled clusters where the piecewise constant approximation on the eigenvectors no longer holds.\nThe EigenCuts algorithm addresses both these issues. As a type of spectral clustering algorithm it works by constructing a Markov chain representation of the data and computing the eigenvectors and eigenvalues of the transition matrix. Eigenflows, or flow of probability by eigenvector, have an associated half life of flow decay called eigenflow. By perturbing the weights between nodes, it can be observed where bottlenecks exist in the eigenflow's halflife, allowing for the identification of boundaries between clusters. Thus, this algorithm iterates until no more cuts between clusters need to be made, eliminating the need for an a prior K, and conferring the ability to separate tightly coupled clusters.\nThe only disadvantage of EigenCuts is the need to recompute eigenvectors and eigenvalues at each iterative step, incurring a large computational overhead. This problem can be adequately addressed within the map/reduce framework and on a Hadoop cluster by parallelizing the computation of each eigenvector and its associated eigenvalue. Apache Hama in particular, with its specializations in graph and matrix data, will be crucial in parallelizing the computations of transition matrices and their corresponding eigenvalues and eigenvectors at each iteration.\nSince Dr Chennubhotla is currently a member of the faculty at the University of Pittsburgh, I have been in contact with him for the past few weeks, and we both envision and eagerly anticipate continued collaboration over the course of the summer and this project's implementation. His guidance in highlighting the finer points of the underlying theory, coupled with my experience in and knowledge of software engineering, makes this is a project we are both extremely excited about implementing.\nTimeline\nAt the end of each sprint, there should be a concrete, functional deliverable. It may not do much, but what it does will work and have full coverage accompanying unit tests.\nSprint 0 (April 26 - May 23): Work with mentor on any pre-coding tasks - familiarization with and dev deployments of Hadoop and Mahout; reading up on documentation, fine-tuning the project plan and requirements. This part will kick into high gear after May 6 (my last final exam and final academic obligation, prior to the actual graduation ceremony), but likely nothing before April 29 (the day of my thesis defense).\nSprint 1 (2 weeks; May 24 - June 6): Implement basic k-means clustering algorithm and parallelize on Mahout over Hadoop. Preliminary interface allows for dataset selection and visualization of resulting clusters.\nSprint 2 (3 weeks; June 7 - 27): Modify k-means algorithm to spectral clustering. Integrate map/reduce framework via Mahout and take advantage of existing core calculation of transition matrices and associated eigenvectors and eigenvalues.\nSprint 3 (3 weeks; June 28 - July 18): Augment spectral clustering to EigenCuts. Fully parallelize with Mahout. Also, mid-term evaluations.\nSprint 4 (3 weeks; July 19 - August 8): Fix any remaining issues with EigenCuts. Finalize interface for running the algorithm, selecting datasets and visualizing results.\nSprint 5 (1 week; August 9 - 15): Tidy up documentation, write final unit tests, fix outstanding bugs.\nOther Information\nI am finishing up my last semester as a master's student in computational biology at Carnegie Mellon, prior to beginning the PhD program in CompBio at Carnegie Mellon this coming fall. I have worked extensively with clustering techniques as a master's student, as a significant amount of my work has involved bioimage analysis within Dr Robert Murphy's lab. Previous work has involved using HMMs to detect patterns in tuberculosis genomes and use CLUSTALW to cluster those patterns according to geographic location. My master's thesis involves use of matrix completion to infer linear transformations of proteins and their associated subcellular locations across varying cell conditions (drugs, cell lines, etc). \nI unfortunately have limited experience with Apache Mahout and Hadoop, but with an undergraduate computer science degree from Georgia Tech, and after an internship with IBM ExtremeBlue, I feel I am extremely adept at picking up new frameworks quickly.\nReferences\n[1] Chakra Chennubhotla and Allan D. Jepson. Half-Lives of EigenFlows for Spectral Clustering. NIPS 2002.",
        "Issue Links": []
    },
    "MAHOUT-364": {
        "Key": "MAHOUT-364",
        "Summary": "[GSOC] Proposal to implement Neural Network with backpropagation learning on Hadoop",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zaid Md. Abdul Wahab Sheikh",
        "Created": "07/Apr/10 01:24",
        "Updated": "21/May/11 03:27",
        "Resolved": "31/Jul/10 14:29",
        "Description": "Proposal Title: Implement Multi-Layer Perceptrons with backpropagation learning on Hadoop (addresses issue Mahout-342)\nStudent Name: Zaid Md. Abdul Wahab Sheikh\nStudent E-mail: (gmail id) sheikh.zaid\nI. Brief Description\nA feedforward neural network (NN) reveals several degrees of parallelism within it such as weight parallelism, node parallelism, network parallelism, layer parallelism and training parallelism. However network based parallelism requires fine-grained synchronization and communication and thus is not suitable for map/reduce based algorithms. On the other hand, training-set parallelism is coarse grained. This can be easily exploited on Hadoop which can split up the input among different mappers. Each of the mappers will then propagate the 'InputSplit' through their own copy of the complete neural network.\nThe backpropagation algorithm will operate in batch mode. This is because updating a common set of parameters after each training example creates a bottleneck for parallelization. The overall error gradient vector calculation can be parallelized by calculating the gradients from each training vector in the Mapper, combining them to get partial batch gradients and then adding them in a reducer to get the overall batch gradient.\nIn a similiar manner, error function evaluations during line searches (for the conjugate gradient and quasi-Newton algorithms) can be efficiently parallelized.\nLastly, to avoid local minima in its error function, we can take advantage of training session parallelism to start multiple training sessions in parallel with different initial weights (simulated annealing).\nII. Detailed Proposal\nThe most important step is to design the base neural network classes in such a way that other NN architectures like Hopfield nets, Boltzman machines, SOM etc can be easily implemented by deriving from these base classes. For that I propose to implement a set of core classes that correspond to basic neural network concepts like artificial neuron, neuron layer, neuron connections, weight, transfer function, input function, learning rule etc. This architecture is inspired from that of the opensource Neuroph neural network framework (http://imgur.com/gDIOe.jpg). This design of the base architecture allows for great flexibility in deriving newer NNs and learning rules. All that needs to be done is to derive from the NeuralNetwork class, provide the method for network creation, create a new training method by deriving from LearningRule, and then add that learning rule to the network during creation. In addition, the API is very intuitive and easy to understand (in comparision to other NN frameworks like Encog and JOONE).\nThe approach to parallelization in Hadoop:\nIn the Driver class:\n\nThe input parameters are read and the NeuralNetwork with a specified LearningRule (training algorithm) created.\nInitial weight values are randomly generated and written to the FileSystem. If number of training sessions (for simulated annealing) is specified, multiple sets of initial weight values are generated.\nTraining is started by calling the NeuralNetwork's learn() method. For each iteration, every time the error gradient vector needs to be calculated, the method submits a Job where the input path to the training-set vectors and various key properties (like path to the stored weight values) are set. The gradient vectors calculated by the Reducers are written back to an output path in the FileSystem.\nAfter the JobClient.runJob() returns, the gradient vectors are retrieved from the FileSystem and tested to see if the stopping criterion is satisfied. The weights are then updated, using the method implemented by the particular LearningRule. For line searches, each error function evaluation is again done by submitting a job.\nThe NN is trained in iterations until it converges.\n\nIn the Mapper class:\n\nEach Mapper is initialized using the configure method, the weights are retrieved and the complete NeuralNetwork created.\nThe map function then takes in the training vectors as key/value pairs (the key is ignored), runs them through the NN to calculate the outputs and backpropagates the errors to find out the error gradients. The error gradient vectors are then output as key/value pairs where all the keys are set to a common value, such as the training session number (for each training session, all keys in the outputs of all the mappers have to be identical).\n\nIn the Combiner class:\n\nIterates through the all individual error gradient vectors output by the mapper (since they all have the same key) and adds them up to get a partial batch gradient.\n\nIn the Reducer class:\n\nThere's a single reducer class that will combine all the partial gradients from the Mappers to get the overall batch gradient.\nThe final error gradient vector is written back to the FileSystem\n\nI propose to complete all of the following sub-tasks during GSoC 2010:\nImplementation of the Backpropagation algorithm:\n\nInitialization of weights: using the Nguyen-Widrow algorithm to select the initial range of starting weight values.\nInput, transfer and error functions: implement basic input functions like WeightedSum and transfer functions like Sigmoid, Gaussian, tanh and linear. Implement the sum-of-squares error function.\nOptimization methods to update the weights: (a) Batch Gradient descent, with momentum and a variable learning rate method [2] (b) A Conjugate gradient method with Brent's line search.\n\nImproving generalization:\n\nValidating the network to test for overfitting (Early stopping method)\nRegularization (weight decay method)\n\nCreate examples for:\n\nClassification: using the Abalone Data Set  from UCI Machine Learning Repository\nClassification, Regression: Breast Cancer Wisconsin (Prognostic) Data Set\n\nIf time permits, also implement:\n\nResilient Backpropagation (RPROP)\n\nIII. Week Plan with list of deliverables\n\n(Till May 23rd, community bonding period)\nBrainstorm with my mentor and the Apache Mahout community to come up with the most optimal design for an extensible Neural Network framework. Code prototypes to identify potential problems and/or investigate new solutions.\nDeliverable: A detailed report or design document on how to implement the basic Neural Network framework and the learning algorithms.\n\n\n(May 24th, coding starts) Week 1:\nDeliverables: Basic Neural network classes (Neuron, Connection, Weight, Layer, LearningRule, NeuralNetwork etc) and the various input, transfer and error functions mentioned previously.\n\n\n(May 31st) Week 2 and Week 3:\nDeliverable: Driver, Mapper, Combiner and Reducer classes with basic functonality to run a feedforward Neural Network on Hadoop (no training methods yet, weights are generated using Nguyen-Widrow algorithm). \n\n\n(June 14th) Week 4:\nDeliverable: Backpropagation algorithm using standard Batch Gradient descent.\n\n\n(June 21st) Week 5:\nDeliverables: Variable learning rate and momentum during Batch Gradient descent. Validation tests support. Do some big tests.\n\n\n(June 28th) Week 6:\nDeliverable: Support for Early stopping and Regularization (weight decay) during training.\n\n\n(July 5th) Week 7 and Week 8:\nDeliverable: Conjugate gradient method with Brent's line search algorithm. \n\n\n(July 19th) Week 9:\nDeliverable: Write unit tests. Do bigger scale tests for both batch gradient descent and conjugate gradient method.\n\n\n(July 26th) Week 10 and Week 11:\nDeliverable: 2 examples of classification and regression on real-world datasets from UCI Machine Learning Repository. More tests.\n\n\n(August 9th, tentative 'pencils down' date) Week 12:\nDeliverable: Wind up the work. Scrub code. Improved documentation, tutorials (on the wiki) etc.\n\n\n(August 16: Final evaluation)\n\nIV. Additional Information\nI am a final year Computer Science student at NIT Allahabad (India) graduating in May. For my final year project/thesis, I am working on Open Domain Question Answering. I participated in GSoC last year for the Apertium machine translation system (http://google-opensource.blogspot.com/2009/11/apertium-projects-first-google-summer.html). I am familiar with the three major opensource Neural Network frameworks in Java, JOONE, Encog and Neuroph since I have used them in past projects on fingerprint recognition and face recognition (during a summer course on image and speech processing). My research interests are machine learning and statistical natural language processing and I will be enrolling for a Ph.D. next semester(i.e. next fall) in the same institute. \nI have no specific time constraints throughout the GSoC period. I will devote a minimum of 6 hours everyday to GSoC.\nTime offset: UTC+5:30 (IST)\nV. References\n[1] Fast parallel off-line training of multilayer perceptrons, S McLoone, GW Irwin - IEEE Transactions on Neural Networks, 1997\n[2] Optimization of the backpropagation algorithm for training multilayer perceptrons, W. Schiffmann, M. Joost and R. Werner, 1994\n[3] Map-Reduce for Machine Learning on Multicore, Cheng T. Chu, Sang K. Kim, Yi A. Lin, et al - in NIPS, 2006\n[4] Neural networks for pattern recognition, CM Bishop - 1995 [BOOK]",
        "Issue Links": []
    },
    "MAHOUT-365": {
        "Key": "MAHOUT-365",
        "Summary": "[GSoC] Proposal to implement SimHash clustering on MapReduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Cristi Prodan",
        "Created": "07/Apr/10 08:03",
        "Updated": "27/May/13 23:29",
        "Resolved": "26/May/10 20:12",
        "Description": "Application for Google Summer of Code 2010 - Mahout Project\nStudent: Cristian Prodan\n1. Synopsis\nI will add a map-reduce implementation of the SimHash clustering algorithm to the Mahout project. This algorithm provides an efficient way of finding similar/identical files in a large collection of files. \n2. Project\nStorage capacities become larger and thus it is difficult to organize and manage growing file systems. It is easy to loose track of identical copies or older versions of the files in a directory structure. Duplicate detection technologies do not extend well when files are not identical. A typical idea for detecting similar files is to see the features of a file into a high-dimensional space and then use distance within space as a measure of similarity. This may not be feasible since it involves a O(n^2) complexity of the algorithm. If these file-to-vector mappings are reduced ow a one-dimensional space, then the data points could be sorted in O(n log n) time - a big increase of detection speed. \nI will implement the SimHash algorithm presented in detail in [1]. The idea is the following: using a hash function that hashed similar files to similar values, file similarity could be determined simply by comparing pre-sorted hash key values. \nI will implement a family of similarity hash functions which will do this, as described in [1]. Furthermore, the performance will be enhanced by storing auxiliary data used to compute the hash keys. This data will be used as a second filter after a hash key comparison indicates that two files are potentially similar. \nProperties for the similarity function and the algorithm:\n\nvery similar files map to very similar or even the same hash key;\ndistance between keys should be some measure of the difference between files. This would lead to keys proportional to file sizes and this would create false positives. The auxiliary data mentioned above will provide an easy and efficient way of refining the similarity detection.\nthe metric used will be a binary metric (simhash operates at byte level).\ngiven a similarity metric, there needs to be a threshold to determine how close within the metric files need to be to count as similar.\n\nFrom a distributed point of view, the algorithm above is very suited for a MapReduce implementation. The sketch is the following:\nI. MAP phase\nIn this phase we compute the hash for a file along with additional info which serves as a second filter in similarity detection. \nIt outputs (File, simhash_key).\nII. REDUCE phase\nOnce every file has a simhash, group every file with the same simhash into a cluster, sorted by the simhashkey (the key is an integer) . \n\nThe similarity check can be done in main memory.\n\n3. Deliverables:\n1. Batch for sim-hashing the files. The hashes will be stored either on normal file system (for small tests), HDFS or on HBase.  \n2. A tool for answering the next type of queries:\n\nretrieving a set of files, similar to a given file;\nretrieving all pairs of similar files.\nThere will be the option to run this as a standalone program, for tests or smaller scale purposes.\n3. Documentation and unit tests for the written code. \n4. Getting started tutorials for SimHash.\n5. Demos for SimHash applied to 20newsgroups and Wikipedia data sets.\n\n4. Benefits for the Mahout community:\n1) A distributed tool for efficient similarity detection of files in large datasets. Algorithms for detecting similar files can also be useful for classification purposes and as an aid to search. Next release of Mahout will contain this functionality. \n2) This will be used at smaller scale also, by running it in an \"urn-distributed\" mode, for small scale datasets.\n3) Good tutorials on how to work with this tool. \n5. Roadmap\n1).  Community Bonding Period (21th April to 24rd May)\n\n1 week: Familiarize with Mahout, hadoop, and the existing clustering algorithms;\n2 weeks: Understand the data structures (Vector, ClusterBase, Writable and other interfaces and classes used in Mahout) and start initial planning of the system in terms of interactions and data structures.\n1 week: Setup a hadoop cluster formed of 2-3 nodes;\nDuring this time, I plan to speak with my mentor and ask him very specific questions about the plans I make for the implementation.\n\n2).  May 24th to Jul 12th\n\nWeek 1: Detailed planning on how the objects would interact (classes, methods, the Vector interfaces I plan to use etc.) and ask feedback from the mentor.\nWeek 2: Write the code for hashing files (a batch process which will work as an indexer), according to the algorithm, in a TDD style. At the end tests will accompany the code. The results will be stored on normal file system or HDFS.\nWeek 3: Half week: Test the program in \"single\" mode (urn-distributed). Start interacting with HBase if time permits.\nWeek 4: Interact with HBase; The algorithm will store the hashes in HBase for fast retrieval.\nWeek 5: Test the algorithm on different datasets: local file system, 20newsgroups and wikipedia.\nWeek 6, 7: Add the distributed MapReduce implementation (based on hadoop) and write unit tests for it;\nWeek 8: Continue with unit tests and test the distributed implementation on the data sets for local file system, HDFS and HBase. Fix outstanding bugs. Analyze results.\nWeek 9: Continue with testing and bug fixing and make sure everything is ready for mid-term evaluation;\n\nMid-term evaluation period.\n3). Jul 16th - Aug 9th\n\nWeek 10: Extend the batch tool with different options (different hash functions families, threshold for similarity). Experiment with different hash functions families; Give the user an option to specify hash functions.\nWeek 11: Finish off the demos for the 20newsgroups and wikipedia datasets. Write some tests for them. Publish the results from the demos on the wiki.\nWeek 12: Write the getting started guides on how to use the SimHash tool;\nWeek 13: Finish off everything, fix outstanding bugs, clean and document undocumented code.\n\n6. Biography \nMy name is Cristi Prodan, I am 23 years old and currently a 2nd year student pursuing a MSc degree in Computer Science. During the past year, I have been interested in the study of machine learning mainly Recommender Systems and clustering techniques. I have heard about hadoop and Mahout and I found challenging the idea of working with them  if I ever have the chance. My dissertation paper on Recommender Systems and I will use Mahout at it's core. \nSince I heard that The Apache Software Foundation is willing to participate  in this year edition of Google Summer of Code, I was eager to try my hand at contributing to the Mahout project. Being a subscriber to the list since December 2009, I started to interact with the community and presenting my ideas.  I was mostly interested in the MinHash algorithm [2] (which may also be used for similarity detection), whose implementation was started by a contributor. After analyzing his code and seeking for advice from the other members, I have committed my first patch to Mahout (on JIRA, MAHOUT-344).\nDuring this time I have also skimmed through the wiki and [3].\nAt university, I have extensively worked with Java. I am also a big fan of Ruby, Python and currently started digging into Erlang. In the past two years, in the free time, I did some freelancing, working on various web applications. \n7. References\n[1] Caitlin Sadowski, Greg Levin - SimHash: Hash-based Similarity Detection, December 13, 2007 (Manning MEAP).\n[2] Abhinandan Das, Mayur Datar, Ashutosh Garg, Shyam Rajaram - Google News Personalization: Scalable Online Collaborative Filtering, WWW 2007.\n[3] Owen, Anil - Mahout in Action. Manning, 2010. \n------------------------------------------------------\nThis proposal is partly inspired by the one of Konstantin Kafer http://drupal.org/files/application.pdf",
        "Issue Links": []
    },
    "MAHOUT-366": {
        "Key": "MAHOUT-366",
        "Summary": "Error: Java heap space",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Kris Jack",
        "Created": "07/Apr/10 11:21",
        "Updated": "21/May/11 03:22",
        "Resolved": "07/Apr/10 11:48",
        "Description": "I am trying to run the CF taste Hadoop recommender on the LibimSeTi data set (17,359,346 anonymous ratings of 168,791 profiles\nmade by 135,359 users).\nIt completes a couple of rounds of map reduce and then falls over with a \"Java heap space\" error.  I'm pretty sure that I must not have configured something correctly.  Any help would really be appreciated.\nI run:\n$ hadoop-0.20 jar mahout-0.3.jar org.apache.mahout.cf.taste.hadoop.item.RecommenderJob --jarFile mahout-0.3.jar --input input/data.cvs --output output --usersFile input/users.txt\nOutput:\n10/04/06 17:07:03 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n10/04/06 17:07:05 INFO mapred.FileInputFormat: Total input paths to process : 1\n10/04/06 17:07:06 INFO mapred.JobClient: Running job: job_201003311452_0110\n10/04/06 17:07:07 INFO mapred.JobClient:  map 0% reduce 0%\n10/04/06 17:07:20 INFO mapred.JobClient:  map 27% reduce 0%\n10/04/06 17:07:21 INFO mapred.JobClient:  map 38% reduce 0%\n10/04/06 17:07:23 INFO mapred.JobClient:  map 62% reduce 0%\n10/04/06 17:07:26 INFO mapred.JobClient:  map 85% reduce 0%\n10/04/06 17:07:29 INFO mapred.JobClient:  map 98% reduce 0%\n10/04/06 17:07:32 INFO mapred.JobClient:  map 100% reduce 0%\n10/04/06 17:07:38 INFO mapred.JobClient:  map 100% reduce 8%\n10/04/06 17:07:41 INFO mapred.JobClient:  map 100% reduce 25%\n10/04/06 17:07:47 INFO mapred.JobClient:  map 100% reduce 76%\n10/04/06 17:07:51 INFO mapred.JobClient:  map 100% reduce 90%\n10/04/06 17:07:57 INFO mapred.JobClient:  map 100% reduce 100%\n10/04/06 17:07:59 INFO mapred.JobClient: Job complete: job_201003311452_0110\n10/04/06 17:07:59 INFO mapred.JobClient: Counters: 19\n10/04/06 17:07:59 INFO mapred.JobClient:   Job Counters \n10/04/06 17:07:59 INFO mapred.JobClient:     Launched reduce tasks=1\n10/04/06 17:07:59 INFO mapred.JobClient:     Rack-local map tasks=2\n10/04/06 17:07:59 INFO mapred.JobClient:     Launched map tasks=4\n10/04/06 17:07:59 INFO mapred.JobClient:     Data-local map tasks=2\n10/04/06 17:07:59 INFO mapred.JobClient:   FileSystemCounters\n10/04/06 17:07:59 INFO mapred.JobClient:     FILE_BYTES_READ=603502320\n10/04/06 17:07:59 INFO mapred.JobClient:     HDFS_BYTES_READ=257007616\n10/04/06 17:07:59 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=846533316\n10/04/06 17:07:59 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=4399489\n10/04/06 17:07:59 INFO mapred.JobClient:   Map-Reduce Framework\n10/04/06 17:07:59 INFO mapred.JobClient:     Reduce input groups=168791\n10/04/06 17:07:59 INFO mapred.JobClient:     Combine output records=0\n10/04/06 17:07:59 INFO mapred.JobClient:     Map input records=17359346\n10/04/06 17:07:59 INFO mapred.JobClient:     Reduce shuffle bytes=177444782\n10/04/06 17:07:59 INFO mapred.JobClient:     Reduce output records=168791\n10/04/06 17:07:59 INFO mapred.JobClient:     Spilled Records=60466622\n10/04/06 17:07:59 INFO mapred.JobClient:     Map output bytes=208312152\n10/04/06 17:07:59 INFO mapred.JobClient:     Map input bytes=256995325\n10/04/06 17:07:59 INFO mapred.JobClient:     Combine input records=0\n10/04/06 17:07:59 INFO mapred.JobClient:     Map output records=17359346\n10/04/06 17:07:59 INFO mapred.JobClient:     Reduce input records=17359346\n10/04/06 17:07:59 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n10/04/06 17:08:00 INFO mapred.FileInputFormat: Total input paths to process : 1\n10/04/06 17:08:01 INFO mapred.JobClient: Running job: job_201003311452_0111\n10/04/06 17:08:02 INFO mapred.JobClient:  map 0% reduce 0%\n10/04/06 17:08:13 INFO mapred.JobClient:  map 3% reduce 0%\n10/04/06 17:08:15 INFO mapred.JobClient:  map 11% reduce 0%\n10/04/06 17:08:16 INFO mapred.JobClient:  map 26% reduce 0%\n10/04/06 17:08:17 INFO mapred.JobClient:  map 38% reduce 0%\n10/04/06 17:08:18 INFO mapred.JobClient:  map 43% reduce 0%\n10/04/06 17:08:19 INFO mapred.JobClient:  map 55% reduce 0%\n10/04/06 17:08:20 INFO mapred.JobClient:  map 62% reduce 0%\n10/04/06 17:08:21 INFO mapred.JobClient:  map 67% reduce 0%\n10/04/06 17:08:22 INFO mapred.JobClient:  map 79% reduce 0%\n10/04/06 17:08:23 INFO mapred.JobClient:  map 86% reduce 0%\n10/04/06 17:08:24 INFO mapred.JobClient:  map 92% reduce 0%\n10/04/06 17:08:25 INFO mapred.JobClient:  map 99% reduce 0%\n10/04/06 17:08:31 INFO mapred.JobClient:  map 100% reduce 0%\n10/04/06 17:08:35 INFO mapred.JobClient:  map 100% reduce 8%\n10/04/06 17:08:38 INFO mapred.JobClient:  map 100% reduce 16%\n10/04/06 17:08:44 INFO mapred.JobClient:  map 100% reduce 67%\n10/04/06 17:08:47 INFO mapred.JobClient:  map 100% reduce 71%\n10/04/06 17:08:50 INFO mapred.JobClient:  map 100% reduce 75%\n10/04/06 17:08:53 INFO mapred.JobClient:  map 100% reduce 79%\n10/04/06 17:08:56 INFO mapred.JobClient:  map 100% reduce 84%\n10/04/06 17:08:59 INFO mapred.JobClient:  map 100% reduce 87%\n10/04/06 17:09:02 INFO mapred.JobClient:  map 100% reduce 92%\n10/04/06 17:09:05 INFO mapred.JobClient:  map 100% reduce 96%\n10/04/06 17:09:11 INFO mapred.JobClient:  map 100% reduce 100%\n10/04/06 17:09:13 INFO mapred.JobClient: Job complete: job_201003311452_0111\n10/04/06 17:09:13 INFO mapred.JobClient: Counters: 19\n10/04/06 17:09:13 INFO mapred.JobClient:   Job Counters \n10/04/06 17:09:13 INFO mapred.JobClient:     Launched reduce tasks=1\n10/04/06 17:09:13 INFO mapred.JobClient:     Rack-local map tasks=2\n10/04/06 17:09:13 INFO mapred.JobClient:     Launched map tasks=4\n10/04/06 17:09:13 INFO mapred.JobClient:     Data-local map tasks=2\n10/04/06 17:09:13 INFO mapred.JobClient:   FileSystemCounters\n10/04/06 17:09:13 INFO mapred.JobClient:     FILE_BYTES_READ=948360528\n10/04/06 17:09:13 INFO mapred.JobClient:     HDFS_BYTES_READ=257007616\n10/04/06 17:09:13 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=1330266292\n10/04/06 17:09:13 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=33108188\n10/04/06 17:09:13 INFO mapred.JobClient:   Map-Reduce Framework\n10/04/06 17:09:13 INFO mapred.JobClient:     Reduce input groups=135359\n10/04/06 17:09:13 INFO mapred.JobClient:     Combine output records=0\n10/04/06 17:09:13 INFO mapred.JobClient:     Map input records=17359346\n10/04/06 17:09:13 INFO mapred.JobClient:     Reduce shuffle bytes=278841790\n10/04/06 17:09:13 INFO mapred.JobClient:     Reduce output records=135359\n10/04/06 17:09:13 INFO mapred.JobClient:     Spilled Records=60466622\n10/04/06 17:09:13 INFO mapred.JobClient:     Map output bytes=347186920\n10/04/06 17:09:13 INFO mapred.JobClient:     Map input bytes=256995325\n10/04/06 17:09:13 INFO mapred.JobClient:     Combine input records=0\n10/04/06 17:09:13 INFO mapred.JobClient:     Map output records=17359346\n10/04/06 17:09:13 INFO mapred.JobClient:     Reduce input records=17359346\n10/04/06 17:09:14 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n10/04/06 17:09:14 INFO mapred.FileInputFormat: Total input paths to process : 1\n10/04/06 17:09:15 INFO mapred.JobClient: Running job: job_201003311452_0112\n10/04/06 17:09:16 INFO mapred.JobClient:  map 0% reduce 0%\n10/04/06 17:09:30 INFO mapred.JobClient:  map 1% reduce 0%\n10/04/06 17:09:35 INFO mapred.JobClient:  map 2% reduce 0%\n10/04/06 17:09:41 INFO mapred.JobClient:  map 3% reduce 0%\n10/04/06 17:09:45 INFO mapred.JobClient:  map 4% reduce 0%\n10/04/06 17:09:51 INFO mapred.JobClient:  map 5% reduce 0%\n10/04/06 17:09:56 INFO mapred.JobClient:  map 6% reduce 0%\n10/04/06 17:10:09 INFO mapred.JobClient:  map 7% reduce 0%\n10/04/06 17:10:12 INFO mapred.JobClient:  map 8% reduce 0%\n10/04/06 17:10:21 INFO mapred.JobClient:  map 9% reduce 0%\n10/04/06 17:10:24 INFO mapred.JobClient:  map 10% reduce 0%\n10/04/06 17:10:30 INFO mapred.JobClient:  map 11% reduce 0%\n10/04/06 17:10:33 INFO mapred.JobClient:  map 12% reduce 0%\n10/04/06 17:10:39 INFO mapred.JobClient:  map 13% reduce 0%\n10/04/06 17:10:42 INFO mapred.JobClient:  map 14% reduce 0%\n10/04/06 17:10:50 INFO mapred.JobClient:  map 15% reduce 0%\n10/04/06 17:10:57 INFO mapred.JobClient:  map 16% reduce 0%\n10/04/06 17:11:06 INFO mapred.JobClient:  map 17% reduce 0%\n10/04/06 17:11:12 INFO mapred.JobClient:  map 18% reduce 0%\n10/04/06 17:11:15 INFO mapred.JobClient:  map 19% reduce 0%\n10/04/06 17:11:21 INFO mapred.JobClient:  map 20% reduce 0%\n10/04/06 17:11:26 INFO mapred.JobClient:  map 21% reduce 0%\n10/04/06 17:11:32 INFO mapred.JobClient:  map 22% reduce 0%\n10/04/06 17:11:36 INFO mapred.JobClient:  map 23% reduce 0%\n10/04/06 17:11:41 INFO mapred.JobClient:  map 24% reduce 0%\n10/04/06 17:11:47 INFO mapred.JobClient:  map 25% reduce 0%\n10/04/06 17:11:59 INFO mapred.JobClient:  map 26% reduce 0%\n10/04/06 17:12:12 INFO mapred.JobClient:  map 27% reduce 0%\n10/04/06 17:12:15 INFO mapred.JobClient:  map 28% reduce 0%\n10/04/06 17:12:24 INFO mapred.JobClient:  map 30% reduce 0%\n10/04/06 17:12:30 INFO mapred.JobClient:  map 31% reduce 0%\n10/04/06 17:12:36 INFO mapred.JobClient:  map 32% reduce 0%\n10/04/06 17:12:39 INFO mapred.JobClient:  map 33% reduce 0%\n10/04/06 17:12:44 INFO mapred.JobClient:  map 34% reduce 0%\n10/04/06 17:12:48 INFO mapred.JobClient:  map 35% reduce 0%\n10/04/06 17:12:54 INFO mapred.JobClient:  map 36% reduce 0%\n10/04/06 17:13:03 INFO mapred.JobClient:  map 37% reduce 0%\n10/04/06 17:13:09 INFO mapred.JobClient:  map 38% reduce 0%\n10/04/06 17:13:17 INFO mapred.JobClient:  map 39% reduce 0%\n10/04/06 17:13:27 INFO mapred.JobClient:  map 40% reduce 0%\n10/04/06 17:13:32 INFO mapred.JobClient:  map 41% reduce 0%\n10/04/06 17:13:36 INFO mapred.JobClient:  map 42% reduce 0%\n10/04/06 17:13:41 INFO mapred.JobClient:  map 43% reduce 0%\n10/04/06 17:13:45 INFO mapred.JobClient:  map 44% reduce 0%\n10/04/06 17:13:50 INFO mapred.JobClient:  map 45% reduce 0%\n10/04/06 17:13:56 INFO mapred.JobClient:  map 46% reduce 0%\n10/04/06 17:14:03 INFO mapred.JobClient:  map 47% reduce 0%\n10/04/06 17:14:09 INFO mapred.JobClient:  map 48% reduce 0%\n10/04/06 17:14:20 INFO mapred.JobClient:  map 49% reduce 0%\n10/04/06 17:14:26 INFO mapred.JobClient:  map 50% reduce 0%\n10/04/06 17:14:29 INFO mapred.JobClient:  map 51% reduce 0%\n10/04/06 17:14:33 INFO mapred.JobClient:  map 52% reduce 0%\n10/04/06 17:14:38 INFO mapred.JobClient:  map 53% reduce 0%\n10/04/06 17:14:42 INFO mapred.JobClient:  map 54% reduce 0%\n10/04/06 17:14:47 INFO mapred.JobClient:  map 55% reduce 0%\n10/04/06 17:14:50 INFO mapred.JobClient:  map 56% reduce 0%\n10/04/06 17:14:56 INFO mapred.JobClient:  map 57% reduce 0%\n10/04/06 17:14:59 INFO mapred.JobClient:  map 58% reduce 0%\n10/04/06 17:15:02 INFO mapred.JobClient:  map 59% reduce 0%\n10/04/06 17:15:05 INFO mapred.JobClient:  map 60% reduce 0%\n10/04/06 17:15:08 INFO mapred.JobClient:  map 61% reduce 0%\n10/04/06 17:15:14 INFO mapred.JobClient:  map 62% reduce 0%\n10/04/06 17:15:24 INFO mapred.JobClient:  map 63% reduce 0%\n10/04/06 17:15:30 INFO mapred.JobClient:  map 64% reduce 0%\n10/04/06 17:15:37 INFO mapred.JobClient:  map 65% reduce 0%\n10/04/06 17:15:45 INFO mapred.JobClient:  map 66% reduce 0%\n10/04/06 17:15:48 INFO mapred.JobClient:  map 67% reduce 0%\n10/04/06 17:15:54 INFO mapred.JobClient:  map 68% reduce 0%\n10/04/06 17:16:06 INFO mapred.JobClient:  map 69% reduce 0%\n10/04/06 17:16:09 INFO mapred.JobClient:  map 70% reduce 0%\n10/04/06 17:16:12 INFO mapred.JobClient:  map 71% reduce 0%\n10/04/06 17:16:18 INFO mapred.JobClient:  map 72% reduce 0%\n10/04/06 17:16:21 INFO mapred.JobClient:  map 73% reduce 0%\n10/04/06 17:16:24 INFO mapred.JobClient:  map 74% reduce 0%\n10/04/06 17:16:30 INFO mapred.JobClient:  map 75% reduce 0%\n10/04/06 17:16:36 INFO mapred.JobClient:  map 76% reduce 0%\n10/04/06 17:16:42 INFO mapred.JobClient:  map 77% reduce 0%\n10/04/06 17:16:54 INFO mapred.JobClient:  map 78% reduce 0%\n10/04/06 17:17:03 INFO mapred.JobClient:  map 79% reduce 0%\n10/04/06 17:17:06 INFO mapred.JobClient:  map 80% reduce 0%\n10/04/06 17:17:09 INFO mapred.JobClient:  map 81% reduce 0%\n10/04/06 17:17:18 INFO mapred.JobClient:  map 82% reduce 0%\n10/04/06 17:17:27 INFO mapred.JobClient:  map 83% reduce 0%\n10/04/06 17:17:30 INFO mapred.JobClient:  map 84% reduce 0%\n10/04/06 17:17:42 INFO mapred.JobClient:  map 85% reduce 0%\n10/04/06 17:17:51 INFO mapred.JobClient:  map 86% reduce 0%\n10/04/06 17:18:00 INFO mapred.JobClient:  map 87% reduce 0%\n10/04/06 17:18:06 INFO mapred.JobClient:  map 88% reduce 0%\n10/04/06 17:18:09 INFO mapred.JobClient:  map 89% reduce 0%\n10/04/06 17:18:12 INFO mapred.JobClient:  map 90% reduce 0%\n10/04/06 17:18:15 INFO mapred.JobClient:  map 91% reduce 0%\n10/04/06 17:18:24 INFO mapred.JobClient:  map 92% reduce 0%\n10/04/06 17:18:27 INFO mapred.JobClient:  map 93% reduce 0%\n10/04/06 17:18:30 INFO mapred.JobClient:  map 94% reduce 0%\n10/04/06 17:18:37 INFO mapred.JobClient:  map 95% reduce 0%\n10/04/06 17:18:46 INFO mapred.JobClient:  map 96% reduce 0%\n10/04/06 17:18:55 INFO mapred.JobClient:  map 97% reduce 0%\n10/04/06 17:19:12 INFO mapred.JobClient:  map 98% reduce 0%\n10/04/06 17:19:21 INFO mapred.JobClient:  map 99% reduce 0%\n10/04/06 17:19:27 INFO mapred.JobClient:  map 100% reduce 0%\n10/04/06 17:29:23 INFO mapred.JobClient:  map 100% reduce 16%\n10/04/06 17:30:54 INFO mapred.JobClient:  map 100% reduce 66%\n10/04/06 17:31:09 INFO mapred.JobClient:  map 100% reduce 67%\n10/04/06 17:31:36 INFO mapred.JobClient:  map 100% reduce 68%\n10/04/06 17:32:03 INFO mapred.JobClient:  map 100% reduce 69%\n10/04/06 17:32:27 INFO mapred.JobClient:  map 100% reduce 70%\n10/04/06 17:32:54 INFO mapred.JobClient:  map 100% reduce 71%\n10/04/06 17:33:21 INFO mapred.JobClient:  map 100% reduce 72%\n10/04/06 17:33:45 INFO mapred.JobClient:  map 100% reduce 73%\n10/04/06 17:34:09 INFO mapred.JobClient:  map 100% reduce 74%\n10/04/06 17:34:33 INFO mapred.JobClient:  map 100% reduce 75%\n10/04/06 17:34:57 INFO mapred.JobClient:  map 100% reduce 76%\n10/04/06 17:35:21 INFO mapred.JobClient:  map 100% reduce 77%\n10/04/06 17:35:45 INFO mapred.JobClient:  map 100% reduce 78%\n10/04/06 17:36:09 INFO mapred.JobClient:  map 100% reduce 79%\n10/04/06 17:36:36 INFO mapred.JobClient:  map 100% reduce 80%\n10/04/06 17:37:00 INFO mapred.JobClient:  map 100% reduce 81%\n10/04/06 17:37:24 INFO mapred.JobClient:  map 100% reduce 82%\n10/04/06 17:37:48 INFO mapred.JobClient:  map 100% reduce 83%\n10/04/06 17:38:15 INFO mapred.JobClient:  map 100% reduce 84%\n10/04/06 17:38:36 INFO mapred.JobClient:  map 100% reduce 85%\n10/04/06 17:39:03 INFO mapred.JobClient:  map 100% reduce 86%\n10/04/06 17:39:27 INFO mapred.JobClient:  map 100% reduce 87%\n10/04/06 17:39:51 INFO mapred.JobClient:  map 100% reduce 88%\n10/04/06 17:40:16 INFO mapred.JobClient:  map 100% reduce 89%\n10/04/06 17:40:40 INFO mapred.JobClient:  map 100% reduce 90%\n10/04/06 17:41:04 INFO mapred.JobClient:  map 100% reduce 91%\n10/04/06 17:41:31 INFO mapred.JobClient:  map 100% reduce 92%\n10/04/06 17:41:58 INFO mapred.JobClient:  map 100% reduce 93%\n10/04/06 17:42:28 INFO mapred.JobClient:  map 100% reduce 94%\n10/04/06 17:42:58 INFO mapred.JobClient:  map 100% reduce 95%\n10/04/06 17:43:28 INFO mapred.JobClient:  map 100% reduce 96%\n10/04/06 17:43:58 INFO mapred.JobClient:  map 100% reduce 97%\n10/04/06 17:44:25 INFO mapred.JobClient:  map 100% reduce 98%\n10/04/06 17:44:52 INFO mapred.JobClient:  map 100% reduce 99%\n10/04/06 17:45:19 INFO mapred.JobClient:  map 100% reduce 100%\n10/04/06 17:45:28 INFO mapred.JobClient: Job complete: job_201003311452_0112\n10/04/06 17:45:28 INFO mapred.JobClient: Counters: 19\n10/04/06 17:45:28 INFO mapred.JobClient:   Job Counters \n10/04/06 17:45:28 INFO mapred.JobClient:     Launched reduce tasks=1\n10/04/06 17:45:28 INFO mapred.JobClient:     Rack-local map tasks=1\n10/04/06 17:45:28 INFO mapred.JobClient:     Launched map tasks=2\n10/04/06 17:45:28 INFO mapred.JobClient:     Data-local map tasks=1\n10/04/06 17:45:28 INFO mapred.JobClient:   FileSystemCounters\n10/04/06 17:45:28 INFO mapred.JobClient:     FILE_BYTES_READ=30708483022\n10/04/06 17:45:28 INFO mapred.JobClient:     HDFS_BYTES_READ=33109381\n10/04/06 17:45:28 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=37855757698\n10/04/06 17:45:28 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=1252585143\n10/04/06 17:45:28 INFO mapred.JobClient:   Map-Reduce Framework\n10/04/06 17:45:28 INFO mapred.JobClient:     Reduce input groups=131869\n10/04/06 17:45:28 INFO mapred.JobClient:     Combine output records=0\n10/04/06 17:45:28 INFO mapred.JobClient:     Map input records=135359\n10/04/06 17:45:28 INFO mapred.JobClient:     Reduce shuffle bytes=3458456726\n10/04/06 17:45:28 INFO mapred.JobClient:     Reduce output records=131869\n10/04/06 17:45:28 INFO mapred.JobClient:     Spilled Records=3785573943\n10/04/06 17:45:28 INFO mapred.JobClient:     Map output bytes=5717819680\n10/04/06 17:45:28 INFO mapred.JobClient:     Map input bytes=33108047\n10/04/06 17:45:28 INFO mapred.JobClient:     Combine input records=0\n10/04/06 17:45:28 INFO mapred.JobClient:     Map output records=714727460\n10/04/06 17:45:28 INFO mapred.JobClient:     Reduce input records=714727460\n10/04/06 17:45:29 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n10/04/06 17:45:29 INFO mapred.FileInputFormat: Total input paths to process : 1\n10/04/06 17:45:33 INFO mapred.JobClient: Running job: job_201003311452_0113\n10/04/06 17:45:34 INFO mapred.JobClient:  map 0% reduce 0%\n10/04/06 17:45:46 INFO mapred.JobClient:  map 50% reduce 0%\n10/04/06 17:45:48 INFO mapred.JobClient: Task Id : attempt_201003311452_0113_m_000000_0, Status : FAILED\nError: Java heap space\n10/04/06 17:45:56 INFO mapred.JobClient:  map 50% reduce 16%\n10/04/06 17:45:59 INFO mapred.JobClient: Task Id : attempt_201003311452_0113_m_000000_1, Status : FAILED\nError: Java heap space\n10/04/06 17:46:10 INFO mapred.JobClient: Task Id : attempt_201003311452_0113_m_000000_2, Status : FAILED\nError: Java heap space\n10/04/06 17:46:25 INFO mapred.JobClient: Job complete: job_201003311452_0113\n10/04/06 17:46:25 INFO mapred.JobClient: Counters: 14\n10/04/06 17:46:25 INFO mapred.JobClient:   Job Counters \n10/04/06 17:46:25 INFO mapred.JobClient:     Launched reduce tasks=1\n10/04/06 17:46:25 INFO mapred.JobClient:     Rack-local map tasks=3\n10/04/06 17:46:25 INFO mapred.JobClient:     Launched map tasks=5\n10/04/06 17:46:25 INFO mapred.JobClient:     Data-local map tasks=2\n10/04/06 17:46:25 INFO mapred.JobClient:     Failed map tasks=1\n10/04/06 17:46:25 INFO mapred.JobClient:   FileSystemCounters\n10/04/06 17:46:25 INFO mapred.JobClient:     HDFS_BYTES_READ=16555091\n10/04/06 17:46:25 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=38\n10/04/06 17:46:25 INFO mapred.JobClient:   Map-Reduce Framework\n10/04/06 17:46:25 INFO mapred.JobClient:     Combine output records=0\n10/04/06 17:46:25 INFO mapred.JobClient:     Map input records=67595\n10/04/06 17:46:25 INFO mapred.JobClient:     Spilled Records=0\n10/04/06 17:46:25 INFO mapred.JobClient:     Map output bytes=0\n10/04/06 17:46:25 INFO mapred.JobClient:     Map input bytes=16553330\n10/04/06 17:46:25 INFO mapred.JobClient:     Combine input records=0\n10/04/06 17:46:25 INFO mapred.JobClient:     Map output records=0\nException in thread \"main\" java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1293)\n\tat org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.run(RecommenderJob.java:112)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.main(RecommenderJob.java:117)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
        "Issue Links": []
    },
    "MAHOUT-367": {
        "Key": "MAHOUT-367",
        "Summary": "IllegalArgumentException in AbstractJDBCModel when requesting mostSimilarItems for an item without preferences",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "07/Apr/10 11:26",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "07/Apr/10 12:08",
        "Description": "I'm using GenericItemBasedRecommender together with MySQLJDBCDataModel for generating recommendations for an online shop. Whenever the recommender tries to calculate the most similar items for an item without preferences (e.g. a brand new product), an IllegalArgumentException is thrown as AbstractJDBCModel tries to create a GenericItemPreferenceArray from the empty preference list.\nSample stacktrace:\njava.lang.IllegalArgumentException: size is less than 1\n\tat org.apache.mahout.cf.taste.impl.model.GenericItemPreferenceArray.<init>(GenericItemPreferenceArray.java:49)\n\tat org.apache.mahout.cf.taste.impl.model.GenericItemPreferenceArray.<init>(GenericItemPreferenceArray.java:56)\n\tat org.apache.mahout.cf.taste.impl.model.jdbc.AbstractJDBCDataModel.getPreferencesForItem(AbstractJDBCDataModel.java:441)\n\tat org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender.doMostSimilarItems(GenericItemBasedRecommender.java:169)\n\tat org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender.mostSimilarItems(GenericItemBasedRecommender.java:128)\n\tat org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender.mostSimilarItems(GenericItemBasedRecommender.java:121)\n\tat de.zalando.recommendation.recommender.cf.TasteRecommenderAdapter.mostSimilarItems(TasteRecommenderAdapter.java:38)\n\tat de.zalando.recommendation.recommender.taste.cache.CachingRecommender.mostSimilarItems(CachingRecommender.java:39)\n\tat de.zalando.recommendation.recoreco.rest.RecommendationsRestController.mostSimilarItems(RecommendationsRestController.java:53)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.springframework.web.bind.annotation.support.HandlerMethodInvoker.doInvokeMethod(HandlerMethodInvoker.java:710)\n\tat org.springframework.web.bind.annotation.support.HandlerMethodInvoker.invokeHandlerMethod(HandlerMethodInvoker.java:167)\n\tat org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter.invokeHandlerMethod(AnnotationMethodHandlerAdapter.java:414)\n\tat org.springframework.web.servlet.mvc.annotation.AnnotationMethodHandlerAdapter.handle(AnnotationMethodHandlerAdapter.java:402)\n\tat org.springframework.web.servlet.DispatcherServlet.doDispatch(DispatcherServlet.java:771)\n\tat org.springframework.web.servlet.DispatcherServlet.doService(DispatcherServlet.java:716)\n\tat org.springframework.web.servlet.FrameworkServlet.processRequest(FrameworkServlet.java:647)\n\tat org.springframework.web.servlet.FrameworkServlet.doGet(FrameworkServlet.java:552)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:617)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:717)\n\tat org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)\n\tat org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n\tat org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n\tat org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)\n\tat org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)\n\tat org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n\tat org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n\tat org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:298)\n\tat org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:852)\n\tat org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:588)\n\tat org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)\n\tat java.lang.Thread.run(Thread.java:619)",
        "Issue Links": []
    },
    "MAHOUT-368": {
        "Key": "MAHOUT-368",
        "Summary": "should package core ,math and collections to one Jar package for hadoop recommendations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "07/Apr/10 11:31",
        "Updated": "29/Mar/11 08:21",
        "Resolved": "07/Apr/10 11:48",
        "Description": "should package core ,math and collections to one Jar package for org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.\nbecause RecommenderJob used classes  (for example org.apache.mahout.math.VectorWritable )of the  math module of mahout project,\nbut math and core module is the separated jar package.\nso when work on the hadoop env ,the class of math module can not load to classloader in the datanode.\nit will cause class not found exception.\nthe work around is package all mahout classes to one package manually.",
        "Issue Links": []
    },
    "MAHOUT-369": {
        "Key": "MAHOUT-369",
        "Summary": "Issues with DistributedLanczosSolver output",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3,                                            0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Danny Leshem",
        "Created": "07/Apr/10 13:29",
        "Updated": "21/May/11 03:18",
        "Resolved": "05/Apr/11 00:24",
        "Description": "DistributedLanczosSolver (line 99) claims to persist eigenVectors.numRows() vectors.\n\n    log.info(\"Persisting \" + eigenVectors.numRows() + \" eigenVectors and eigenValues to: \" + outputPath);\n\n\nHowever, a few lines later (line 106) we have\n\n    for(int i=0; i<eigenVectors.numRows() - 1; i++) {\n        ...\n    }\n\n\nwhich only persists eigenVectors.numRows()-1 vectors.\nSeems like the most significant eigenvector (i.e. the one with the largest eigenvalue) is omitted... off by one bug?\nAlso, I think it would be better if the eigenvectors are persisted in reverse order, meaning the most significant vector is marked \"0\", the 2nd most significant is marked \"1\", etc.\nThis, for two reasons:\n1) When performing another PCA on the same corpus (say, with more principal componenets), corresponding eigenvalues can be easily matched and compared.  \n2) Makes it easier to discard the least significant principal components, which for Lanczos decomposition are usually garbage.",
        "Issue Links": []
    },
    "MAHOUT-370": {
        "Key": "MAHOUT-370",
        "Summary": "Mahout Installation Build Failure TopItems.java:[85,15] cannot find symbol",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Kris Jack",
        "Created": "07/Apr/10 14:06",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "07/Apr/10 14:16",
        "Description": "I have just checked out the latest version of Mahout from the repository (revision 931555) and it fails to build on install.  My $MAVEN_OPTS is -Xmx1024m.  Any ideas why it is failing to build?\nThanks very much for your help in advance.\nFrom the main directory:\n$ mvn install\nOutput:\n$ mvn install\n[INFO] Scanning for projects...\n[INFO] Reactor build order: \n[INFO]   Buildtools - jar file used to configure PMD and Checkstyle\n[INFO]   Package up files needed by Eclipse\n[INFO]   Maven Mojo to generate code for collections\n[INFO]   Apache Lucene Mahout\n[INFO]   Mahout Collections\n[INFO]   Mahout Math\n[INFO]   Mahout Core\n[INFO]   Mahout Taste Webapp\n[INFO]   Mahout Utilities\n[INFO]   Mahout Examples\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Buildtools - jar file used to configure PMD and Checkstyle\n[INFO]    task-segment: [install]\n[INFO] ------------------------------------------------------------------------\n[INFO] Setting property: classpath.resource.loader.class => 'org.codehaus.plexus.velocity.ContextClassLoaderResourceLoader'.\n[INFO] Setting property: velocimacro.messages.on => 'false'.\n[INFO] Setting property: resource.loader => 'classpath'.\n[INFO] Setting property: resource.manager.logwhenfound => 'false'.\n[INFO] [remote-resources:process \n{execution: default}\n]\n[INFO] inceptionYear not specified, defaulting to 2010\n[INFO] [resources:resources \n{execution: default-resources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 3 resources\n[INFO] Copying 3 resources\n[INFO] [compiler:compile \n{execution: default-compile}\n]\n[INFO] No sources to compile\n[INFO] [resources:testResources \n{execution: default-testResources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /home/kris/mahouts/mahout-0.4/buildtools/src/test/resources\n[INFO] Copying 3 resources\n[INFO] [compiler:testCompile \n{execution: default-testCompile}\n]\n[INFO] No sources to compile\n[INFO] [surefire:test \n{execution: default-test}\n]\n[INFO] Surefire report directory: /home/kris/mahouts/mahout-0.4/buildtools/target/surefire-reports\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nThere are no tests to run.\nResults :\nTests run: 0, Failures: 0, Errors: 0, Skipped: 0\n[INFO] [jar:jar \n{execution: default-jar}\n]\n[INFO] Building jar: /home/kris/mahouts/mahout-0.4/buildtools/target/mahout-buildtools-0.4-SNAPSHOT.jar\n[INFO] [install:install \n{execution: default-install}\n]\n[INFO] Installing /home/kris/mahouts/mahout-0.4/buildtools/target/mahout-buildtools-0.4-SNAPSHOT.jar to /home/kris/.m2/repository/org/apache/mahout/mahout-buildtools/0.4-SNAPSHOT/mahout-buildtools-0.4-SNAPSHOT.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Package up files needed by Eclipse\n[INFO]    task-segment: [install]\n[INFO] ------------------------------------------------------------------------\n[INFO] [remote-resources:process \n{execution: default}\n]\n[INFO] inceptionYear not specified, defaulting to 2010\n[INFO] [resources:resources \n{execution: default-resources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 3 resources\n[INFO] Copying 3 resources\n[INFO] [compiler:compile \n{execution: default-compile}\n]\n[INFO] No sources to compile\n[INFO] [resources:testResources \n{execution: default-testResources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /home/kris/mahouts/mahout-0.4/eclipse/src/test/resources\n[INFO] Copying 3 resources\n[INFO] [compiler:testCompile \n{execution: default-testCompile}\n]\n[INFO] No sources to compile\n[INFO] [surefire:test \n{execution: default-test}\n]\n[INFO] Surefire report directory: /home/kris/mahouts/mahout-0.4/eclipse/target/surefire-reports\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nThere are no tests to run.\nResults :\nTests run: 0, Failures: 0, Errors: 0, Skipped: 0\n[INFO] [jar:jar \n{execution: default-jar}\n]\n[INFO] Building jar: /home/kris/mahouts/mahout-0.4/eclipse/target/mahout-eclipse-support-0.4-SNAPSHOT.jar\n[INFO] [install:install \n{execution: default-install}\n]\n[INFO] Installing /home/kris/mahouts/mahout-0.4/eclipse/target/mahout-eclipse-support-0.4-SNAPSHOT.jar to /home/kris/.m2/repository/org/apache/mahout/mahout-eclipse-support/0.4-SNAPSHOT/mahout-eclipse-support-0.4-SNAPSHOT.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Maven Mojo to generate code for collections\n[INFO]    task-segment: [install]\n[INFO] ------------------------------------------------------------------------\n[INFO] [plugin:descriptor \n{execution: default-descriptor}\n]\n[INFO] Using 'UTF-8' encoding to read mojo metadata.\n[INFO] Applying mojo extractor for language: java\n[INFO] Mojo extractor for language: java found 1 mojo descriptors.\n[INFO] Applying mojo extractor for language: bsh\n[INFO] Mojo extractor for language: bsh found 0 mojo descriptors.\n[INFO] [remote-resources:process \n{execution: default}\n]\n[INFO] inceptionYear not specified, defaulting to 2010\n[INFO] [resources:resources \n{execution: default-resources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /home/kris/mahouts/mahout-0.4/collections-codegen-plugin/src/main/resources\n[INFO] Copying 3 resources\n[INFO] [compiler:compile \n{execution: default-compile}\n]\n[INFO] Compiling 1 source file to /home/kris/mahouts/mahout-0.4/collections-codegen-plugin/target/classes\n[INFO] [resources:testResources \n{execution: default-testResources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /home/kris/mahouts/mahout-0.4/collections-codegen-plugin/src/test/resources\n[INFO] Copying 3 resources\n[INFO] [compiler:testCompile \n{execution: default-testCompile}\n]\n[INFO] No sources to compile\n[INFO] [surefire:test \n{execution: default-test}\n]\n[INFO] Surefire report directory: /home/kris/mahouts/mahout-0.4/collections-codegen-plugin/target/surefire-reports\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nThere are no tests to run.\nResults :\nTests run: 0, Failures: 0, Errors: 0, Skipped: 0\n[INFO] [jar:jar \n{execution: default-jar}\n]\n[INFO] Building jar: /home/kris/mahouts/mahout-0.4/collections-codegen-plugin/target/mahout-collection-codegen-plugin-1.1-SNAPSHOT.jar\n[INFO] [plugin:addPluginArtifactMetadata \n{execution: default-addPluginArtifactMetadata}\n]\n[INFO] [install:install \n{execution: default-install}\n]\n[INFO] Installing /home/kris/mahouts/mahout-0.4/collections-codegen-plugin/target/mahout-collection-codegen-plugin-1.1-SNAPSHOT.jar to /home/kris/.m2/repository/org/apache/mahout/mahout-collection-codegen-plugin/1.1-SNAPSHOT/mahout-collection-codegen-plugin-1.1-SNAPSHOT.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Apache Lucene Mahout\n[INFO]    task-segment: [install]\n[INFO] ------------------------------------------------------------------------\n[INFO] [remote-resources:process \n{execution: default}\n]\n[INFO] [site:attach-descriptor \n{execution: default-attach-descriptor}\n]\n[INFO] Unable to load parent project from a relative path: Could not find the model file '/home/kris/mahouts/pom.xml'. for project unknown\n[INFO] Parent project loaded from repository.\n[INFO] [install:install \n{execution: default-install}\n]\n[INFO] Installing /home/kris/mahouts/mahout-0.4/pom.xml to /home/kris/.m2/repository/org/apache/mahout/mahout/0.4-SNAPSHOT/mahout-0.4-SNAPSHOT.pom\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Mahout Collections\n[INFO]    task-segment: [install]\n[INFO] ------------------------------------------------------------------------\n[INFO] [mahout-collection-codegen:generate \n{execution: default}\n]\n07-Apr-2010 14:51:31 org.apache.velocity.runtime.log.JdkLogChute log\nINFO: FileResourceLoader : adding path '/home/kris/mahouts/mahout-0.4/collections/src/main/java-templates'\n07-Apr-2010 14:51:31 org.apache.velocity.runtime.log.JdkLogChute log\nINFO: FileResourceLoader : adding path '/home/kris/mahouts/mahout-0.4/collections/src/test/java-templates'\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/buffer/ByteBufferConsumer.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/buffer/CharBufferConsumer.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/buffer/IntBufferConsumer.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/buffer/ShortBufferConsumer.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/buffer/LongBufferConsumer.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/buffer/FloatBufferConsumer.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/buffer/DoubleBufferConsumer.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/ByteArrayList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/CharArrayList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/IntArrayList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/ShortArrayList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/LongArrayList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/FloatArrayList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/DoubleArrayList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/AbstractByteList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/AbstractCharList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/AbstractIntList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/AbstractShortList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/AbstractLongList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/AbstractFloatList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/list/AbstractDoubleList.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractObjectByteMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractObjectCharMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractObjectIntMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractObjectShortMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractObjectLongMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractObjectFloatMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractObjectDoubleMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractByteObjectMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractCharObjectMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractIntObjectMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractShortObjectMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractLongObjectMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractFloatObjectMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractDoubleObjectMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenObjectByteHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenObjectCharHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenObjectIntHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenObjectShortHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenObjectLongHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenObjectFloatHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenObjectDoubleHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractByteByteMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractByteCharMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractByteIntMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractByteShortMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractByteLongMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractByteFloatMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractByteDoubleMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractCharByteMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractCharCharMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractCharIntMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractCharShortMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractCharLongMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractCharFloatMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractCharDoubleMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractIntByteMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractIntCharMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractIntIntMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractIntShortMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractIntLongMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractIntFloatMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractIntDoubleMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractShortByteMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractShortCharMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractShortIntMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractShortShortMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractShortLongMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractShortFloatMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractShortDoubleMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractLongByteMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractLongCharMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractLongIntMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractLongShortMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractLongLongMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractLongFloatMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractLongDoubleMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractFloatByteMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractFloatCharMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractFloatIntMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractFloatShortMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractFloatLongMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractFloatFloatMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractFloatDoubleMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractDoubleByteMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractDoubleCharMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractDoubleIntMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractDoubleShortMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractDoubleLongMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractDoubleFloatMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/AbstractDoubleDoubleMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenByteByteHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenByteCharHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenByteIntHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenByteShortHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenByteLongHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenByteFloatHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenByteDoubleHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenCharByteHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenCharCharHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenCharIntHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenCharShortHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenCharLongHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenCharFloatHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenCharDoubleHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenIntByteHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenIntCharHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenIntIntHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenIntShortHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenIntLongHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenIntFloatHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenIntDoubleHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenShortByteHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenShortCharHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenShortIntHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenShortShortHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenShortLongHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenShortFloatHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenShortDoubleHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenLongByteHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenLongCharHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenLongIntHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenLongShortHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenLongLongHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenLongFloatHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenLongDoubleHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenFloatByteHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenFloatCharHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenFloatIntHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenFloatShortHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenFloatLongHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenFloatFloatHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenFloatDoubleHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenDoubleByteHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenDoubleCharHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenDoubleIntHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenDoubleShortHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenDoubleLongHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenDoubleFloatHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenDoubleDoubleHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenByteObjectHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenCharObjectHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenIntObjectHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenShortObjectHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenLongObjectHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenFloatObjectHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/map/OpenDoubleObjectHashMap.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/AbstractByteSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/AbstractCharSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/AbstractIntSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/AbstractShortSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/AbstractLongSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/AbstractFloatSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/AbstractDoubleSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/OpenByteHashSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/OpenCharHashSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/OpenIntHashSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/OpenShortHashSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/OpenLongHashSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/OpenFloatHashSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/set/OpenDoubleHashSet.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteCharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteIntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteLongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteFloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteDoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharCharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharIntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharLongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharFloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharDoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntCharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntIntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntLongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntFloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntDoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortCharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortIntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortLongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortFloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortDoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongCharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongIntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongLongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongFloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongDoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatCharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatIntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatLongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatFloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatDoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleCharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleIntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleLongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleFloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleDoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteComparator.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharComparator.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntComparator.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortComparator.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongComparator.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatComparator.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleComparator.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ByteObjectProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/CharObjectProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/IntObjectProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ShortObjectProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/LongObjectProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/FloatObjectProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/DoubleObjectProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ObjectByteProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ObjectCharProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ObjectIntProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ObjectShortProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ObjectLongProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ObjectFloatProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-sources/org/apache/mahout/math/function/ObjectDoubleProcedure.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/list/ByteArrayListTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/list/CharArrayListTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/list/IntArrayListTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/list/ShortArrayListTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/list/LongArrayListTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/list/FloatArrayListTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/list/DoubleArrayListTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenByteByteHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenByteCharHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenByteIntHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenByteShortHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenByteLongHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenByteFloatHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenByteDoubleHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenCharByteHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenCharCharHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenCharIntHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenCharShortHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenCharLongHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenCharFloatHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenCharDoubleHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenIntByteHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenIntCharHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenIntIntHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenIntShortHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenIntLongHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenIntFloatHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenIntDoubleHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenShortByteHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenShortCharHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenShortIntHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenShortShortHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenShortLongHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenShortFloatHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenShortDoubleHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenLongByteHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenLongCharHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenLongIntHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenLongShortHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenLongLongHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenLongFloatHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenLongDoubleHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenFloatByteHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenFloatCharHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenFloatIntHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenFloatShortHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenFloatLongHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenFloatFloatHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenFloatDoubleHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenDoubleByteHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenDoubleCharHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenDoubleIntHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenDoubleShortHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenDoubleLongHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenDoubleFloatHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenDoubleDoubleHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenByteObjectHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenCharObjectHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenIntObjectHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenShortObjectHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenLongObjectHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenFloatObjectHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenDoubleObjectHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenObjectByteHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenObjectCharHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenObjectIntHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenObjectShortHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenObjectLongHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenObjectFloatHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/map/OpenObjectDoubleHashMapTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/set/OpenByteHashSetTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/set/OpenCharHashSetTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/set/OpenIntHashSetTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/set/OpenShortHashSetTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/set/OpenLongHashSetTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/set/OpenFloatHashSetTest.java\n[INFO] Writing to /home/kris/mahouts/mahout-0.4/collections/target/generated-test-sources/org/apache/mahout/math/set/OpenDoubleHashSetTest.java\n[INFO] [resources:resources \n{execution: default-resources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /home/kris/mahouts/mahout-0.4/collections/src/main/resources\n[INFO] [compiler:compile \n{execution: default-compile}\n]\n[INFO] Compiling 277 source files to /home/kris/mahouts/mahout-0.4/collections/target/classes\n[INFO] [resources:testResources \n{execution: default-testResources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /home/kris/mahouts/mahout-0.4/collections/src/test/resources\n[INFO] [compiler:testCompile \n{execution: default-testCompile}\n]\n[INFO] Compiling 80 source files to /home/kris/mahouts/mahout-0.4/collections/target/test-classes\n[INFO] [surefire:test \n{execution: default-test}\n]\n[INFO] Surefire report directory: /home/kris/mahouts/mahout-0.4/collections/target/surefire-reports\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.math.list.ShortArrayListTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.199 sec\nRunning org.apache.mahout.math.set.OpenLongHashSetTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 sec\nRunning org.apache.mahout.math.map.OpenDoubleFloatHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.047 sec\nRunning org.apache.mahout.math.map.OpenIntByteHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 sec\nRunning org.apache.mahout.math.list.CharArrayListTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.042 sec\nRunning org.apache.mahout.math.map.OpenShortByteHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.035 sec\nRunning org.apache.mahout.math.map.OpenIntObjectHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.037 sec\nRunning org.apache.mahout.math.map.OpenObjectIntHashMapTest\nTests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.024 sec\nRunning org.apache.mahout.math.map.OpenIntIntHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.02 sec\nRunning org.apache.mahout.math.set.OpenHashSetTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.01 sec\nRunning org.apache.mahout.math.map.OpenShortLongHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.03 sec\nRunning org.apache.mahout.math.map.OpenCharDoubleHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.019 sec\nRunning org.apache.mahout.math.map.OpenFloatIntHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.032 sec\nRunning org.apache.mahout.math.map.OpenByteDoubleHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.029 sec\nRunning org.apache.mahout.math.map.OpenHashMapTest\nTests run: 13, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.03 sec\nRunning org.apache.mahout.math.map.OpenByteLongHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.026 sec\nRunning org.apache.mahout.math.map.OpenCharShortHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.02 sec\nRunning org.apache.mahout.math.map.OpenFloatLongHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec\nRunning org.apache.mahout.math.map.OpenCharObjectHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.019 sec\nRunning org.apache.mahout.math.list.FloatArrayListTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.025 sec\nRunning org.apache.mahout.math.map.OpenCharCharHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 sec\nRunning org.apache.mahout.math.map.OpenIntDoubleHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.026 sec\nRunning org.apache.mahout.math.map.OpenFloatDoubleHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.018 sec\nRunning org.apache.mahout.math.map.OpenShortObjectHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.024 sec\nRunning org.apache.mahout.math.map.OpenLongShortHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.016 sec\nRunning org.apache.mahout.math.map.OpenFloatObjectHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec\nRunning org.apache.mahout.math.set.OpenFloatHashSetTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec\nRunning org.apache.mahout.math.map.OpenShortFloatHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.03 sec\nRunning org.apache.mahout.math.map.OpenDoubleCharHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.021 sec\nRunning org.apache.mahout.math.set.OpenByteHashSetTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec\nRunning org.apache.mahout.math.map.OpenLongObjectHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec\nRunning org.apache.mahout.math.map.OpenIntCharHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.028 sec\nRunning org.apache.mahout.math.set.OpenCharHashSetTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec\nRunning org.apache.mahout.math.map.OpenByteShortHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.048 sec\nRunning org.apache.mahout.math.map.OpenLongFloatHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.042 sec\nRunning org.apache.mahout.math.list.ByteArrayListTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.03 sec\nRunning org.apache.mahout.math.set.OpenDoubleHashSetTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec\nRunning org.apache.mahout.math.map.OpenIntFloatHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.list.DoubleArrayListTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.026 sec\nRunning org.apache.mahout.math.map.OpenDoubleIntHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.034 sec\nRunning org.apache.mahout.math.map.OpenDoubleLongHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.map.OpenFloatCharHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 sec\nRunning org.apache.mahout.math.map.OpenByteObjectHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec\nRunning org.apache.mahout.math.map.OpenDoubleByteHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.034 sec\nRunning org.apache.mahout.math.map.OpenFloatFloatHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.032 sec\nRunning org.apache.mahout.math.map.OpenIntShortHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.04 sec\nRunning org.apache.mahout.math.map.OpenShortDoubleHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.017 sec\nRunning org.apache.mahout.math.set.OpenIntHashSetTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec\nRunning org.apache.mahout.math.map.OpenLongIntHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.map.OpenCharFloatHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec\nRunning org.apache.mahout.math.map.OpenLongDoubleHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.022 sec\nRunning org.apache.mahout.math.list.LongArrayListTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.019 sec\nRunning org.apache.mahout.math.map.OpenCharLongHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.023 sec\nRunning org.apache.mahout.math.SortingTest\nTests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.803 sec\nRunning org.apache.mahout.math.map.OpenByteIntHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.029 sec\nRunning org.apache.mahout.math.list.IntArrayListTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.03 sec\nRunning org.apache.mahout.math.map.OpenObjectShortHashMapTest\nTests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.map.OpenDoubleObjectHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.047 sec\nRunning org.apache.mahout.math.map.OpenShortCharHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec\nRunning org.apache.mahout.math.map.OpenLongLongHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.021 sec\nRunning org.apache.mahout.math.map.OpenLongByteHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.028 sec\nRunning org.apache.mahout.math.map.OpenCharByteHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec\nRunning org.apache.mahout.math.map.OpenFloatByteHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.map.OpenShortIntHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 sec\nRunning org.apache.mahout.math.map.OpenByteFloatHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec\nRunning org.apache.mahout.math.set.OpenShortHashSetTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec\nRunning org.apache.mahout.math.map.OpenObjectLongHashMapTest\nTests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec\nRunning org.apache.mahout.math.map.OpenByteByteHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec\nRunning org.apache.mahout.math.map.OpenFloatShortHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.016 sec\nRunning org.apache.mahout.math.map.OpenShortShortHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.011 sec\nRunning org.apache.mahout.math.map.OpenByteCharHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.map.OpenDoubleShortHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec\nRunning org.apache.mahout.math.map.OpenObjectByteHashMapTest\nTests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.map.OpenObjectDoubleHashMapTest\nTests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec\nRunning org.apache.mahout.math.map.OpenDoubleDoubleHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.map.OpenCharIntHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec\nRunning org.apache.mahout.math.map.OpenObjectFloatHashMapTest\nTests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec\nRunning org.apache.mahout.math.map.OpenLongCharHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.013 sec\nRunning org.apache.mahout.math.map.OpenIntLongHashMapTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.014 sec\nRunning org.apache.mahout.math.map.OpenObjectCharHashMapTest\nTests run: 19, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.015 sec\nResults :\nTests run: 1314, Failures: 0, Errors: 0, Skipped: 0\n[INFO] [jar:jar \n{execution: default-jar}\n]\n[INFO] Building jar: /home/kris/mahouts/mahout-0.4/collections/target/mahout-collections-0.4-SNAPSHOT.jar\n[INFO] [install:install \n{execution: default-install}\n]\n[INFO] Installing /home/kris/mahouts/mahout-0.4/collections/target/mahout-collections-0.4-SNAPSHOT.jar to /home/kris/.m2/repository/org/apache/mahout/mahout-collections/0.4-SNAPSHOT/mahout-collections-0.4-SNAPSHOT.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Mahout Math\n[INFO]    task-segment: [install]\n[INFO] ------------------------------------------------------------------------\n[INFO] [resources:resources \n{execution: default-resources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /home/kris/mahouts/mahout-0.4/math/src/main/resources\n[INFO] [compiler:compile \n{execution: default-compile}\n]\n[INFO] Compiling 167 source files to /home/kris/mahouts/mahout-0.4/math/target/classes\n[INFO] [resources:testResources \n{execution: default-testResources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /home/kris/mahouts/mahout-0.4/math/src/test/resources\n[INFO] [compiler:testCompile \n{execution: default-testCompile}\n]\n[INFO] Compiling 16 source files to /home/kris/mahouts/mahout-0.4/math/target/test-classes\n[INFO] [surefire:test \n{execution: default-test}\n]\n[INFO] Surefire report directory: /home/kris/mahouts/mahout-0.4/math/target/surefire-reports\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.math.TestMatrixView\nTests run: 49, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.166 sec\nRunning org.apache.mahout.math.TestDenseVector\nTests run: 43, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.035 sec\nRunning org.apache.mahout.math.TestSparseMatrix\nTests run: 53, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.206 sec\nRunning org.apache.mahout.math.TestSparseRowMatrix\nTests run: 53, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.083 sec\nRunning org.apache.mahout.math.stats.LogLikelihoodTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.031 sec\nRunning org.apache.mahout.math.TestOrderedIntDoubleMapping\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.005 sec\nRunning org.apache.mahout.math.decomposer.hebbian.TestHebbianSolver\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.647 sec\nRunning org.apache.mahout.math.TestDenseMatrix\nTests run: 53, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.033 sec\nRunning org.apache.mahout.math.TestSparseVector\nTests run: 42, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.023 sec\nRunning org.apache.mahout.math.TestVectorView\nTests run: 38, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.019 sec\nRunning org.apache.mahout.math.VectorTest\nTests run: 16, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.03 sec\nRunning org.apache.mahout.math.TestSparseColumnMatrix\nTests run: 53, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.057 sec\nRunning org.apache.mahout.math.decomposer.lanczos.TestLanczosSolver\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.839 sec\nRunning org.apache.mahout.math.AIOOBInSortingTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.012 sec\nResults :\nTests run: 409, Failures: 0, Errors: 0, Skipped: 0\n[INFO] [jar:jar \n{execution: default-jar}\n]\n[INFO] Building jar: /home/kris/mahouts/mahout-0.4/math/target/mahout-math-0.4-SNAPSHOT.jar\n[INFO] [jar:test-jar \n{execution: default}\n]\n[INFO] Building jar: /home/kris/mahouts/mahout-0.4/math/target/mahout-math-0.4-SNAPSHOT-tests.jar\n[INFO] [install:install \n{execution: default-install}\n]\n[INFO] Installing /home/kris/mahouts/mahout-0.4/math/target/mahout-math-0.4-SNAPSHOT.jar to /home/kris/.m2/repository/org/apache/mahout/mahout-math/0.4-SNAPSHOT/mahout-math-0.4-SNAPSHOT.jar\n[INFO] Installing /home/kris/mahouts/mahout-0.4/math/target/mahout-math-0.4-SNAPSHOT-tests.jar to /home/kris/.m2/repository/org/apache/mahout/mahout-math/0.4-SNAPSHOT/mahout-math-0.4-SNAPSHOT-tests.jar\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Mahout Core\n[INFO]    task-segment: [install]\n[INFO] ------------------------------------------------------------------------\n[INFO] [resources:resources \n{execution: default-resources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 0 resource\n[INFO] [compiler:compile \n{execution: default-compile}\n]\n[INFO] Compiling 455 source files to /home/kris/mahouts/mahout-0.4/core/target/classes\n[INFO] ------------------------------------------------------------------------\n[ERROR] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Compilation failure\n/home/kris/mahouts/mahout-0.4/core/src/main/java/org/apache/mahout/cf/taste/impl/recommender/TopItems.java:[85,15] cannot find symbol\nsymbol  : method sort(java.util.List<org.apache.mahout.cf.taste.recommender.RecommendedItem>)\nlocation: class java.util.Collections\n[INFO] ------------------------------------------------------------------------\n[INFO] For more information, run Maven with the -e switch\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 43 seconds\n[INFO] Finished at: Wed Apr 07 14:52:08 BST 2010\n[INFO] Final Memory: 85M/390M\n[INFO] ------------------------------------------------------------------------",
        "Issue Links": []
    },
    "MAHOUT-371": {
        "Key": "MAHOUT-371",
        "Summary": "Proposal to implement Distributed SVD++ Recommender using Hadoop",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Richard Simon Just",
        "Created": "08/Apr/10 07:24",
        "Updated": "21/May/11 03:27",
        "Resolved": "26/Oct/10 16:03",
        "Description": "Proposal Title: MAHOUT-371 Proposal to implement Distributed SVD++ Recommender using Hadoop\nStudent Name: Richard Simon Just\nStudent E-mail:info@richardsimonjust.co.uk\nOrganization/Project: Apache Mahout\nAssigned Mentor:\nProposal Abstract: \nDuring the Netflix Prize Challenge one of the most popular forms of Recommender algorithm was that of Matrix Factorisation, in particular Singular Value Decomposition (SVD). As such this proposal looks to implement a distributed version of one of the most successful SVD-based recommender algorithms from the Netflix competition. Namely, the SVD++ algorithm. \nThe SVD++ improves upon other basic SVD algorithms by incorporating implicit feedback[1]. That is to say that it is able to take into account not just explicit user preferences, but also feedback such as, in the case of a company like Netflix, whether a movie has been rented. Implicit feedback assumes that the fact of there being some correlation between the user and the item is more important that whether the correlation is positive or negative. Implicit feedback would account for an item has being rated, but not what the rating was.\nThe implementation will include testing, in-depth documentation and a demo/tutorial. If there is time, I will also look to developing the algorithm into the timeSVD++ algorithm[2,3]. The timeSVD++ further improves the results of the SVD algorithm by taking into account temporal dynamics. Temporal dynamics addresses the way user preferences in items and their behaviour in how they rate items can change over time. According to [2] the gains in accuracy implementing timeSVD++ are significantly bigger than the gains going from SVD to SVD++. \nThe overall project will provide three deliverables:\n1. The basic framework for distributed SVD-based recommender\n2. A distributed SVD++ implementation and demo\n3. A distributed timeSVD++ implementation\nDetailed Description:\nThe SVD++ algorithm uses the principle of categorising users and items into factors, combined with regularisation and implicit feedback to predict how much a user is likely to match an item. Factors are abstract categorises that are created from comparing the data presented. Factor values are grades saying how much each user/item is related to that category. For example with the Netflix data a factor could loosely correspond to a movie genre, a director or story line. The more factors used the more detailed the categories are likely to become, and thus the more accurate the predictions are likely to become. \nImplicit feedback is based on the theory that a user having any sort of relationship to an item is more important that whether they rated it, or what rating they gave. The assumption is that even if a user does not like an item, or has not rated it, the very fact that they choose to have some interaction with it indicates something about their preferences. In the Netflix case this would be represented by whether a user has rated a movie or not,  it could also take into account the user's rental history. \nAs well as the actual implementation of the code the project has two other deliverable focuses. The readability, documentation, testing of the code; and a full tutorial and demo of the code. It is felt that without these things the implementation is not really complete or fully usable. \nThe recommender consists of 5 main parts. The job class that runs the code, an input conversion section, a training section, a re-trainer section and a prediction section. A brief overview of these sections follows.\nThe SVD++  Classes:\nThe Recommender Job class:\nThis class is the foundation of the recommender and allows it to run on Hadoop by implementing the Tool interface through AbstractJob. This class will parse any user arguments and setup the jobs that will run the algorithm on Map/Reduce, much in the same way Mahout's other distributed recommenders, do such as RecommenderJob.\nInput Mapper/Reducer Classes:\nThe Mapper will take the input data and convert it to key value pairs in the form of a hadoop Writeable. The Reducer will take the Mapper Writeables and create Sparse vectors. This is in keeping with Mahout's ToItemPrefsMapper and ToUserVectorReducer.\nTraining Mapper/Reducer Classes:\nThis phase creates the factor matrices that will be used to create the predictions later. It does this by making a prediction of a known rating using the SVD, comparing it against the known rating, calculating the error and updating the factors accordingly. The Mapper will loop through the training of the factor matrices. The Reducer will collect the outputs of the Mapper to create the dense factor matrices.\nRe-trainer Mapper/Reducer Classes:\nThis is a separate job that would be used to update the factor matrices taking into account any new user ratings. The Mapper/Reducer follow a similar pattern to the training section.\nPrediction Mapper/Reducer Classes:\nThis job would take the factor matrices from the training set and use them in the SVD to create the required predictions. Most likely this would be done for all user/item preferences, to create dense matrix of all user/item combinations. However potentially it could also be done for individual users.\nJust like training, the Mapper would handle the predictions the Reducer would collect them into a dense matrix.\nTimeline:\nThe Warm Up/Bonding Period (<=May 23rd):\n\nfamiliarise myself further with Mahout and Hadoop's code base and documentation\ndiscuss with community the proposal, design and implementation as well as related code tests, optimisations and documentation they would like to see incorporated into the project\nbuild a more detailed design of algorithm implementation and tweak timeline based on feedback\nfamiliarise myself more with unit testing\nfinish building 3-4 node Hadoop cluster and play with all the examples\n\nWeek 1 (May 24th-30th):\n\nstart writing the back bone of the code in the form of comments and skeleton code\nimplement SVDppRecommenderJob\nstart to integrate DistributedLanzcosSolver\n\nWeek 2 (May 31st - June 6th):\n\ncomplete DistributedLanzcosSolver integration\nstart implementing distributed training, prediction and regularisation\n\nWeek 3 - 5 (June 7th - 27th):\n\ncomplete implementation of distributed training, prediction and regularisation\nwork on testing, cleaning up code, and tying up any loose documentation ends\nwork on any documentation, tests and optimisation requested by community\nDeliverable : basic framework for distributed SVD-based recommender\n\nWeek 6 - 7 (June 28th-July 11th):\n\nstart implementation of SVD++ (keeping documentation and tests up-to-date)\nprepare demo\n\nWeek 8 (July 12th - 18th): Mid-Term Report by the 16th\n\ncomplete SVD++ and iron out bugs\nimplement and document demo\nwrite wiki articles and tutorial for what has been implemented including the demo\n\nWeek 9 (July 19th - 25th):\n\nwork on any documentation, tests and optimisation requested by community during project\nwork on testing, cleaning up code, and tying up any loose documentation ends\nDeliverable : Distributed SVD++ Recommender (including Demo)\n\nWeek 10 - 11 (July 26th - Aug 8th):\n\nincorporate temporal dynamics\nwrite temporal dynamics documentation, including wiki article\n\nWeek 12 (Aug 9th - 15th): Suggested Pencils Down\n\nlast optimisation and tidy up of code, documentation, tests and demo\ndiscuss with community what comes next, consider what JIRA issues to contribute to\nDeliverable: Distributed SVD++ Recommender with temporal dynamics\n\nFinal Evaluations Hand-in: Aug 16th-20th. \nAdditional Information:\nI am currently a final year undergraduate at the University of Reading, UK, studying BSc Applied Computer Science. As part of this degree I did a 9 month industrial placement programming in Java for an international non-profit organisation. While in this final year of study I have been fortunate enough to take modules in Distributed Computing, Evolutionary Computation and Concurrent Systems, which have proven to be the best of my University career. Each module required me to design and implement algorithms of either a distributed or machine learning nature [A], two in Java, one in C. For my final year project I have implemented a Gaming Persistence Platform in ActionScript, PHP and MySQL. It allows the user to play computer games integrated with the platform from different devices and/or different software platforms while retaining game progress across all instances. I am looking forward to the day when mobile computing goes distributed.\nI am a passionate user of open source software and now I am looking to become involved as a developer. Having enjoyed my Distributed Computing and Evolutionary Computation modules so much, and after reading all the introductory pages about the ASF I realised Mahout would be a great place to start. After graduation (and GSoC) I hope to continue contributing to Mahout while working in a related field. \nOutside of uni, when I'm not coding, I like to push my limits through meditation, mountaineering and running. I also love to cook!\nAfter my last exam on May 21st my only commitment over the summer will be GSoC. As such GSoC will be treated like a full-time job.\n[1] - Y. Koren, \"Factorization Meets the Neighborhood: a Mulitfaceted Collaborative Filtering Model\", ACM Press, 2008, http://public.research.att.com/~volinsky/netflix/kdd08koren.pdf\n[2] - Y. Koren, \"Collaborative Filtering with temporal Dynamics\", ACM Press, 2009, http://research.yahoo.com/files/kdd-fp074-koren.pdf\n[3] - R.M.Bell, Y. Koren, and C. Volinsky, \"The Bellkor 2008 Solution to the Netflix Prize\", http://www.netflixprize.com/assets/ProgressPrize2008_BellKor.pdf\n[A] - Example of code I've implemented as part of University degree, http://www.richardsimonjust.com/code/",
        "Issue Links": [
            "/jira/browse/MAHOUT-329"
        ]
    },
    "MAHOUT-372": {
        "Key": "MAHOUT-372",
        "Summary": "Partitioning Collaborative Filtering Job into Maps and Reduces",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Kris Jack",
        "Created": "09/Apr/10 09:59",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "09/Apr/10 12:26",
        "Description": "I am running the org.apache.mahout.cf.taste.hadoop.item.RecommenderJob main on my hadoop cluster and it partitions the job in 2 although I have more than 2 nodes available.  I was reading that the partitioning could be changed by setting the JobConf's conf.setNumMapTasks(int num) and conf.setNumReduceTasks(int num).\nWould I be right in assuming that this would speed up the processing by increasing these, say to 4)?  Can this code be partitioned into many reducers?  If so, would setting them in the protected AbstractJob::JobConf prepareJobConf() function be appropriate?",
        "Issue Links": []
    },
    "MAHOUT-373": {
        "Key": "MAHOUT-373",
        "Summary": "VectorDumper/VectorHelper doesn't dump values when dictionary is present",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Integration",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "09/Apr/10 14:50",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "03/Oct/10 10:23",
        "Description": "Dumping term vectors with a dictionary using:\nmahout vectordump -s vector-output/chunk-0 -dt sequencefile -d dictionary-output\ngives me output like the following with no values, just the indexes expanded into their dictionary entries:\n\nName: 0001-11055 elts: {513:discard, 7199:empty,...\n\n\nWhile dumping the same vector without a dictionary using\nmahout vectordump -s vector-output/chunk-0\ngives me output that includes indexes and values:\n\nName: 0001-11055 elts: {513:1.0, 7199:1.0...\n\n\nWould it make sense for the dictionary based output to include values as well? Anyone opposed to modifying VectorHelper.vectorToString(Vector, String[]) to do so?",
        "Issue Links": []
    },
    "MAHOUT-374": {
        "Key": "MAHOUT-374",
        "Summary": "GSOC 2010 Proposal Implement Map/Reduce Enabled Neural Networks (mahout-342)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Yinghua Hu",
        "Created": "09/Apr/10 16:00",
        "Updated": "21/May/11 03:27",
        "Resolved": "31/Jul/10 14:22",
        "Description": "--------------------\n\nIntroduction\n\nIn recent years, multicore computer becomes main stream. However, the potential of multicore has not been fully exploited for machine learning because the lack of good programming framework for multicore. Recently, Chu et. al. [CHU07] adapt Google's map-reduce paradigm [DEA04] and implemented 10 different machine learning algorithms on multicore processor. Their results show almost double speedup on average with a dual processor. Their work has inspired a lot of interesting projects under Mahout of Apache Software Foundation.\nAn artificial neural network (ANN) is a supervised learning tool inspired by the biological nervous system. It can capture and represent complex input/output relationships. The neural network has capability to represent both linear and non-linear relationships by learning the relationships directly from the data being modeled. Artificial neural network have been widely applied for classification, data processing and function approximation.\nWe propose to implement an artificial neural network with back-propagation under map-reduce framework. Success of delivery should include a fast neural network customized for multicore computer.\n--------------------\n\nMethodology\n\nIn this section, we briefly introduce map-reduce and back propagated neural network.\n\nMap-reduce\n\nMap-reduce [DEA04] is a programming model developed by Google. It gets its name from the map and reduce primitives in the functional language such as Lisp. Its inventors realize that most of their distributed computation involve applying a map operation and reduce operation. Where map operation is to compute a set of intermediate key/value pairs for each logical record in the input and reduce operation is applied on all the values that share the same key in order to combine the derived data appropriately. The reduce process allow users to handle large value list difficult to fit in memory.\nMap-reduce makes it possible to have a simple interface enabling automatic parallelization and distribution of large-scale computation. The programming model can be applied on multiple core personal computer as well as large clusters of commodity PCs.\nHere is an example of map-reduce from [DEA04]. To count the number of occurrences of each word in a large collection of documents, the user can write like the pseudo code below:\nmap(String key, String value):\n// key: document name\n// value: document contents\nfor each word w in value:\n       EmitIntermediate(w, \"1\");\nreduce(String key, Iterator values):\n// key: a word\n// values: a list of counts\nint result = 0;\nfor each v in values:\n       result += ParseInt(v);\n       Emit(AsString(result))\nThe map function above emits each word and its associated counts (one in this example). The reduce function sums together all counts emitted for a particular word.\nThe Map-reduce model has been successfully applied to a large variety of problems, such as Google's Web search service, sorting, data mining etc. It helps to reduce the amount of data sent across the network. In addition, its easiness to use makes the parallel and distributed computing achievable even for inexperienced programmer.\n[CHU07] provide a multicore map-reduce framework that is capable of implementing algorithm fitting the Statistical Query Model. Neural network is one of the ten machine learning algorithms fitting the requirement.\n\nNeural Network\n\nNeural network is one of many modern technology inspired by biology. It is a model which can perform simple computation such as linear regression as well as very complex nonlinear calculation. Without doubt, neural network is one of the most popular machine learning methodology.\nThe simplest artificial neural network is a single-layer perceptron as shown in Figure 1.  It contains a single layer of adaptive weights.  The output is calculated by applying an activation function f to the weighted sum of inputs:\nhttp://www.cs.ucf.edu/~yhu/gsoc/formula1.JPG             (1)\nhttp://www.cs.ucf.edu/~yhu/gsoc/fig1.JPG\nFigure 1 - Single-layer Perceptron\nThe limitation of a single-layer perceptron is that it can only model linear relationships between an output and the inputs. This is overcome by multi-layer perceptrons. Multi-layer perceptrons contain several layers of adaptive weights. In Figure 2, a hidden layer was added into the single layer perceptron in Figure 1  to form a two-layer perceptron. If there is an activation function g for the first layer of adaptive weights and f for the second layer of weights, then output can be calculated as in (2). Actually, even the two-layer perceptron is capable of approximating any continuous functional mapping [Bis95]. This is superior to a single-layer perceptron.\nhttp://www.cs.ucf.edu/~yhu/gsoc/formula2.JPG    (2)\nhttp://www.cs.ucf.edu/~yhu/gsoc/fig2.JPG\nFigure 2 - Two-layer Perceptron\nA neural network is called a feed-forward network if there is no feedback loops in the architecture. The most commonly used training method for feed-forward neural network is back-propagation [RUM86]. This method propagates errors backward through the network by evaluating the derivatives of the error with respect to the weights. The chain rule in calculus plays a important role to calculate all the derivatives. The calculated derivatives can then be used to find weight values minimizing the error function.\nOther than back propagation, there are other training methods such as hill climbing and conjugate gradient. However, none of these methods guarantee finding the global optimum. It is very possible that the result of neural network stuck in a local optimum. Additionally, neural network is also criticized as to be lacking of interpretability.\n--------------------\n\nImplementation\n\nA sequential back-propagated neural network in Java is not very difficult to implement. The UML diagram of a neural network is as Figure 3. The main hurdle for this project will be to apply map-reduce programming model. Scalability on large dataset will also likely be an issue.\nhttp://www.cs.ucf.edu/~yhu/ClassDiagram.gif\nFigure 3 UML Diagram of a Neural Network\nThe performance of module will be measured by using commonly used machine learning data sets from the UCI Machine Learning Repository. Firstly, we need an accurate neural network. Secondly, it should show reasonable speedup on multicore system. The results in [CHU07] serve as our reference.\n--------------------\n\nProject Timeline\n\nPreparation\n\nNow \u2014 May 23: set up development environment, read source code and development documentation of Mahout and previous map-reduce implementation.\n\nCoding\n\nMay 24 \u2014 June 10: start coding Neural Networks on map-reduce, implement a workable simple perceptron before adding hidden layer\n\n\nJune 11 \u2014 June 24: implement a three layer back-propagated Neural Network;\n\n\nJune 25 \u2014 July 1: Perform unit testing of the Neural Network module on benchmark dataset;\n\n\nJuly 2 \u2014 July 11: improve and finish documentation of Neural Network module;\n\n\nJuly 12: submit code, testing result and documentation for midterm evaluation;\n\nFinishing up\n\nJuly 13 \u2014 August 8: Perform large scale testing and performance turning\n\n\nAugust 9 \u2014 August 15: organize everything which has been finished, do minor modifications when necessary\n\n\nAugust 16: submit code, testing result and documentation for final evaluation\n\n-------------------\n\nDeliverables\n\n\nA back-propagated three layer Neural Network module on map-reduce;\n\n\nTest cases, results and related analysis;\n\n\nDevelopment documentation.\n\n--------------------\n\nAbout me\n\nI am interested in this project mainly because I have passion for machine learning. I have been working in a few different areas of Computer Science and find machine learning my favorite subject. If my proposal is approved, I will quit all the other obligations (Research Assistant, Courses) for the summer and focus only on this project. This project will be my full time summer job.\nI am currently a Ph.D. student at University of Central Florida, and majoring in Modeling and Simulation. I also hold master of Computer Science. I have (1). knowledge of machine learning and done numerous related projects.  (2). strong statistical and mathematical background, and (3). project experience on Neural Network with back propagation using Java. I have more than ten years of programming experience. The programming language I am most familiar with is Java. Although I have limited experience of Hadoop and Mahout, I am quick learner and ready to devote the whole summer to this project. For more information about me, please visit my homepage at http://www.cs.ucf.edu/~yhu/.\n--------------------\n\nReferences\n\n[BIS95] CM Bishop, Neural Networks for Pattern Recognition, Oxford University Press, New York City, 1995\n[CHU07] CT Chu, SK Kim, YA Lin, YY Yu, G Bradski, AY Ng & K Olukotun, Map-reduce for Machine Learning on Multicore, Advances in Neural Information Processing Systems 19, 2007\n[DEA04] J Dean, S Ghemawat, Mapreduce:simplified data processing on large clusters, ACM OSDI, 2004\n[RUM86] DE Rumelhart, GE Hinton, RJ Williams, Learning representations by back-propagating errors, Nature, 323(9), 533-536, 1986",
        "Issue Links": []
    },
    "MAHOUT-375": {
        "Key": "MAHOUT-375",
        "Summary": "[GSOC] Restricted Boltzmann Machines in Apache Mahout",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Sisir Koppaka",
        "Created": "11/Apr/10 08:49",
        "Updated": "11/Jan/12 15:55",
        "Resolved": "24/Sep/10 14:26",
        "Description": "Proposal Title: Restricted Boltzmann Machines in Apache Mahout (addresses issue Mahout-329)\nStudent Name: Sisir Koppaka\nStudent E-mail: sisir.koppaka@gmail.com\nOrganization/Project:Assigned Mentor:\nAbstract\nThis is a proposal to implement Restricted Boltzmann Machines in Apache Mahout as a part of Google Summer of Code 2010. The demo for the code would be built on the Netflix dataset.\n1 Introduction\nThe Grand Prize solution to the Netflix Prize offered several new lessons in the application of traditional machine learning techniques to very large scale datasets. The most significant among these were the impact of temporal models, the remarkable contribution of RBM's to the solution in the overall model, and the great success in applying ensemble models to achieve superior predictions. The present proposal seeks to implement a conditional factored RBM[4] in Apache Mahout as a project under Google Summer of Code 2010.\n2 Background\nThe Netflix dataset takes the form of a sparse matrix of a N X M ratings that N users assign to M movies. Matrix decompositions such as variants of Singular Value Decompositions(SVDs) form the first type of methods applied. This has also induced several recent works in applied mathematics relevant to the Netflix Prize, including [1, 2]. Another genre of techniques have been k-nearest neighbour approaches - user-user, movie-movie and using different\ndistance measures such as Pearson and Cosine. The third set of techniques that offers arguably the most divergent predictions that aid in the increase in prediction RMSE are RBM and it's variants.\n[4] demonstrates the algorithm that the author proposes to implement this summer in Apache Mahout. Parallelization can be done by updating all the hidden units, followed by the visible units in parallel, due to the conditional independence of the hidden units, given a visible binary indicator matrix. Rather than implementing a naive RBM, the conditional factored RBM is chosen due to it's useful combination of effectiveness and speed. Minor variations, in any case, could be developed later with little difficulty.\nThe training data set consists of nearly 100 million ratings from 480,000 users on 17,770 movie titles. As part of the training data, Netflix also pro- vides validation data(called the probe set), containing nearly 1.4 million rat- ings. In addition to the training and validation data, Netflix also provides a test set containing 2.8 million user/movie pairs(called the qualifying set) whose ratings were previously withheld, but have now been released post the conclusion of the Prize.\n3 Milestones \n3.1\tApril 26-May 24\nCommunity Bonding Period Certain boilerplate code for the Netflix dataset exists at org.apache.mahout.cf.taste.example.netflix. However, this code is non-distributed and is unrelated to Hadoop. Certain parts of this code, like the file read-in based on Netflix format will be reused to match the processed Netflix dataset file linked below.\nTest out any of the already-implemented Mahout algorithms like SVD or k-Means on the whole dataset to make sure that everything works as ad- vertised. Make a note of testing time. If testing time is very large, then make a 10% training set, and use the 10% probe, which already exists as a standardized Netflix Prize community contribution. This is only so that iterations can be faster/a multiple node Hadoop installation need not al- ways be required. Work on the map-reduce version of RBM and evaluate if parallelization beyond the hidden units and visible units alternate computa- tion can be implemented. Get the community's approval for the map-reduce version of RBM, and then proceed.\n3.2\tMay 24-July 12 Pre-midterm\t\nImplementation time! Write code, test code, rewrite code.\nShould have working code with decent predictions by end of this segment.\nDesign details\t\nThe RBM code would live at org.apache.mahout.classifier.rbm. Classify.java would need to be written to support the RBM similar to that in discriminative. An equivalent of BayesFileFormatter.java would not be required because of the pre-written Netflix read-in code as mentioned above. ConfusionMatrix.java, ResultAnalyzer.java and ClassifyResult.java would be reused as-is from discriminative.\nalgorithm would contain the actual conditional factored RBM algorithm. common would contain the relevant code common to various files in algo- rithm. mapreduce.rbm would contain the driver, mapper and reducer for the parallelized updating of the hidden units layer, followed by the visible units, and appropriate refactored code would be placed in mapreduce.common.\nThe algorithm would be implemented generically, and the demo would be on the Netflix dataset. \n3.3\tJuly 12-July 31 Post-midterm\t\nIf testing was on the 10% set, run multiple times on the whole dataset and ensure results match. Test a two-method ensemble of SVD(already in Mahout) and RBM and confirm that RBM offers a unique perpendicular dimensionality to the problem. Make sure unit tests are all in.\nTest on Netflix dataset linked above and prepare for demo.\n3.4\tJuly 31-August 16 Pencils down \nRefactor, tune and clean code. Final demo done. Write documentation and add a wiki page.\n4\tAbout Myself\nIm a 19-year old student hailing from the beautiful, sea-hugging, coastal city of Visakhapatnam in Andhra Pradesh, South India. In 2006, I was one of\nthe only 110 students in the country to be awarded a scholarship from the Indian Institute of Science and the Department of Science and Technology, Government of India, under the KVPY programme and I attended their summer camp that year.\nI interned in the Prediction Algorithms Lab at GE Research, Bangalore, last summer. I worked on custom-toolkit in C++ that implemented various Netflix algorithms and operated using data parallelization for some of the more lenghty algorithms like user-user kNN. Our team stood at rank 409 at the end of the two-month internship, when the Grand Prize was awarded.\nI have also published independent work in GECCO 2010[3]. GECCO is ACM SIGEVO's annual conference, and is ranked 11th out of 701 interna- tional conferences in AI, ML, Robotics, and HCI based on it's impact factor.\nI have also contributed code to FFmpeg, and was part of my Hall of Residence's award-winning Java-based Open Soft project team that we have now open sourced. I am also an avid quizzer and have won several prestigious quizzes during my schooling days. I also got the 4th rank in the regional qualifications for the Indian National Mathematics Olympiad.\nReferences\n[1] E. J. Candes and T. Tao. The Power of Convex Relaxation: Near-Optimal Matrix Completion. Arxiv, 2009.\n[2] R. H. Keshavan, A. Montanari, and S. Oh. Matrix Completion from Noisy Entries. Arxiv, 2009.\n[3] S. Koppaka and A. R. Hota. Superior Exploration-Exploitation Balance with Quantum-Inspired Hadamard Walks. Proceedings of the 12th Annual Conference on Genetic and Evolutionary computation - GECCO '10 to appear, 2010.\n[4] R. Salakhutdinov, A. Mnih, and G. Hinton. Restricted Boltzmann Machines for Collaborative Filtering. Proceedings of the 24th International Conference on Machine Learning, Corvallis, OR,2007, 6, 2007.\nOpen Source Java Project - http://blunet.googlecode.com\nFFmpeg patches \nhttp://git.ffmpeg.org/?p=ffmpeg;a=commitdiff;h=16a043535b91595bf34d7e044ef398067e7443e0\nhttp://git.ffmpeg.org/?p=ffmpeg;a=commitdiff;h=9dde37a150ce2e5c53e2295d09efe289cebea9cd",
        "Issue Links": [
            "/jira/browse/MAHOUT-329"
        ]
    },
    "MAHOUT-376": {
        "Key": "MAHOUT-376",
        "Summary": "Implement Map-reduce version of stochastic SVD",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "11/Apr/10 13:41",
        "Updated": "21/May/11 03:18",
        "Resolved": "07/Mar/11 10:53",
        "Description": "See attached pdf for outline of proposed method.\nAll comments are welcome.",
        "Issue Links": [
            "/jira/browse/MAHOUT-309",
            "/jira/browse/MAHOUT-623",
            "/jira/browse/MAHOUT-593"
        ]
    },
    "MAHOUT-377": {
        "Key": "MAHOUT-377",
        "Summary": "Clean up javadoc errors in collections",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "collections-1.0",
        "Fix Version/s": "collections-1.0",
        "Component/s": "collections",
        "Assignee": null,
        "Reporter": "Benson Margulies",
        "Created": "12/Apr/10 00:56",
        "Updated": "21/May/11 03:24",
        "Resolved": "27/May/10 08:01",
        "Description": "[INFO] [WARNING] Javadoc Warnings\n[INFO] [WARNING] /Users/benson/asf/mahout/collections/src/main/java/org/apache/mahout/math/list/ObjectArrayList.java:157: warning - @return tag cannot be used in method with void return type.\n[INFO] [WARNING] /Users/benson/asf/mahout/collections/src/main/java/org/apache/mahout/math/list/package.html: warning - Tag @link: reference not found: org.apache.mahout.math.matrix\n[INFO] [WARNING] /Users/benson/asf/mahout/collections/src/main/java/org/apache/mahout/math/list/package.html: warning - Tag @link: reference not found: AbstractDoubleList\n[INFO] [WARNING] /Users/benson/asf/mahout/collections/src/main/java/org/apache/mahout/math/list/package.html: warning - Tag @link: reference not found: org.apache.mahout.math.list.AbstractCollection\n[INFO] [WARNING] /Users/benson/asf/mahout/collections/src/main/java/org/apache/mahout/math/map/package.html: warning - Tag @link: reference not found: org.apache.mahout.math.map.AbstractMap",
        "Issue Links": []
    },
    "MAHOUT-378": {
        "Key": "MAHOUT-378",
        "Summary": "min support parameter ignored in FPGrowth",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robert Neumayer",
        "Created": "13/Apr/10 13:00",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "24/Sep/10 16:47",
        "Description": "The  minSupport parameter is never read in\ngenerateFList(Iterator<Pair<List<A>,Long>> transactions, int minSupport) \nOne solution is to drop the parameter, i.e.: \ngenerateFList(Iterator<Pair<List<A>,Long>> transactions)\nI guess this would do the filtering:\n\t\tList<Pair<A, Long>> fList = new ArrayList<Pair<A, Long>>();\n\t\tfor (Entry<A, MutableLong> e : attributeSupport.entrySet()) {\n\t\t\tif (e.getValue().intValue() >= minSupport) \n{\n\t\t\t\tfList.add(new Pair<A, Long>(e.getKey(), e.getValue()\n\t\t\t\t\t\t.longValue()));\n\t\t\t}\n\t\t}",
        "Issue Links": []
    },
    "MAHOUT-379": {
        "Key": "MAHOUT-379",
        "Summary": "SequentialAccessSparseVector.equals does not agree with AbstractVector.equivalent",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Danny Leshem",
        "Created": "14/Apr/10 09:08",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "22/Sep/10 07:23",
        "Description": "When a SequentialAccessSparseVector is serialized and deserialized using VectorWritable, the result vector and the original vector are equivalent, yet equals returns false.\nThe following unit-test reproduces the problem:\n\n@Test\npublic void testSequentialAccessSparseVectorEquals() throws Exception {\n    final Vector v = new SequentialAccessSparseVector(1);\n    final VectorWritable vectorWritable = new VectorWritable(v);\n    final VectorWritable vectorWritable2 = new VectorWritable();\n    writeAndRead(vectorWritable, vectorWritable2);\n    final Vector v2 = vectorWritable2.get();\n\n    assertTrue(AbstractVector.equivalent(v, v2));\n    assertEquals(v, v2); // This line fails!\n}\n\nprivate void writeAndRead(Writable toWrite, Writable toRead) throws IOException {\n    final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    final DataOutputStream dos = new DataOutputStream(baos);\n    toWrite.write(dos);\n\n    final ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());\n    final DataInputStream dis = new DataInputStream(bais);\n    toRead.readFields(dis);\n}\n\n\n\nThe problem seems to be that the original vector name is null, while the new vector's name is an empty string. The same issue probably also happens with RandomAccessSparseVector.\nSequentialAccessSparseVectorWritable (line 40):\n\ndataOutput.writeUTF(getName() == null ? \"\" : getName());\n\n\nRandomAccessSparseVectorWritable (line 42):\n\ndataOutput.writeUTF(this.getName() == null ? \"\" : this.getName());\n\n\nThe simplest fix is probably to change the default Vector's name from null to the empty string.",
        "Issue Links": []
    },
    "MAHOUT-380": {
        "Key": "MAHOUT-380",
        "Summary": "IllegalArgumentException from AbstractJDBCDataModel constructor which is extended by AbstractBooleanPrefJDBCDataModel",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Rashmi Paliwal",
        "Created": "16/Apr/10 08:44",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "16/Apr/10 09:15",
        "Description": "AbstractBooleanPrefJDBCDataModel extends the AbstractJDBCDataModel. It calls the super.constructor by intentionally setting the last two argument value as null because AbstractBooleanPrefJDBCDataModel is meant where we don' have the preference value. Below is the code snippet:\nprotected AbstractBooleanPrefJDBCDataModel(DataSource dataSource,\n                                             String preferenceTable,\n                                             String userIDColumn,\n                                             String itemIDColumn,\n                                             String preferenceColumn,\n                                             String getPreferenceSQL,\n                                             String getUserSQL,\n                                             String getAllUsersSQL,\n                                             String getNumItemsSQL,\n                                             String getNumUsersSQL,\n                                             String setPreferenceSQL,\n                                             String removePreferenceSQL,\n                                             String getUsersSQL,\n                                             String getItemsSQL,\n                                             String getPrefsForItemSQL,\n                                             String getNumPreferenceForItemSQL,\n                                             String getNumPreferenceForItemsSQL) \n{\n    super(dataSource, preferenceTable, userIDColumn, itemIDColumn, preferenceColumn, getPreferenceSQL,\n        getUserSQL, getAllUsersSQL, getNumItemsSQL, getNumUsersSQL, setPreferenceSQL, removePreferenceSQL,\n        getUsersSQL, getItemsSQL, getPrefsForItemSQL, getNumPreferenceForItemSQL, getNumPreferenceForItemsSQL,\n        null, null);\n    this.setPreferenceSQL = setPreferenceSQL;\n  }\n\nAbstractJDBCDataModel is checking for the null arguments and throwing the IlllegalArgumentException. Here is the code \nAbstractJDBCComponent.checkNotNullAndLog(\"getMaxPreferenceSQL\", getMaxPreferenceSQL);\nAbstractJDBCComponent.checkNotNullAndLog(\"getMinPreferenceSQL\", getMinPreferenceSQL);\nFor this case getMaxPreferenceSQL and getMinPreferenceSQL value is null as set in the AbstractBooleanPrefJDBCDataModel class.\nFix for this would be a great help.\nThanks,\nRashmi",
        "Issue Links": []
    },
    "MAHOUT-381": {
        "Key": "MAHOUT-381",
        "Summary": "org.apache.mahout.cf.taste.hadoop.item is more misleading",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "19/Apr/10 16:40",
        "Updated": "21/May/11 03:22",
        "Resolved": "19/Apr/10 17:05",
        "Description": "the package org.apache.mahout.cf.taste.hadoop.item is more misleading.\nmahout has two type recommendation model,\nUser based and item based,  org.apache.mahout.cf.taste.hadoop.item mislead people to consider that org.apache.mahout.cf.taste.hadoop.item is item based.\nbut truth is that :\nif the input format 's user_id,item_id,score ,then recommend user with similar items,\nif the  input format 's item_id,user_id,score ,then recommend item with similar user,it can be used for advertisement pushing.\nso the best way is  to change  package name.",
        "Issue Links": []
    },
    "MAHOUT-382": {
        "Key": "MAHOUT-382",
        "Summary": "implement other recommendation",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.3",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "19/Apr/10 16:50",
        "Updated": "21/May/11 03:22",
        "Resolved": "19/Apr/10 17:08",
        "Description": "I think that  recommendation has four types:\n1)  recommend user with item (use case:content subscriber, generate the more  interesting content for user )\n2)  recommend item to user (use case :advertisement pushing ,push the advertisement  to the people that is interested with)\n3)  recommend item to item (use case; related content) \n4) recommend user to user (use case : people connection )\nnow org.apache.mahout.cf.taste.hadoop.item can recommend user with item,also can recommend item to user .\nwe have no off-line implement for 3) and 4). although we already implement the code in 1) and 2).",
        "Issue Links": []
    },
    "MAHOUT-383": {
        "Key": "MAHOUT-383",
        "Summary": "Investigate possibility of integration with Neuroph neural-net library",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Jake Mannix",
        "Created": "19/Apr/10 19:46",
        "Updated": "28/May/15 14:37",
        "Resolved": "21/May/11 02:39",
        "Description": "The Neuroph project   is an open-source (currently LGPL, but voted to change to ASL to accomodate integration with Mahout) ANN library, complete with this feature-set:\n\nSupported neural network architectures\n\t\nAdaline\nPerceptron\nMulti Layer Perceptron with Backpropagation\nHopfield network\nBidirectional Associative Memory\nKohonen network\nHebbian network\nMaxnet\nCompetitive network\nInstar\nOutstar\nRBF network\nNeuro Fuzzy Reasoner\n\n\n\n*Other features\n\n\n\nSmall number of the essential base classes (only 10) which can be easily reused\nSupport for supervised and unsupervised learning rules\nAn easy-to-follow structure and logic.\nGUI tool for neural network development easyNeurons\nImage recognition support\n\n\n\nThey are interested in integration, so we should look into what we can do to augment Mahout's capabilities with this.",
        "Issue Links": []
    },
    "MAHOUT-384": {
        "Key": "MAHOUT-384",
        "Summary": "Implement of AVF algorithm",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "tony cui",
        "Created": "22/Apr/10 04:56",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "23/Jul/11 15:37",
        "Description": "This program realize a outlier detection algorithm called avf, which is kind of \nFast Parallel Outlier Detection for Categorical Datasets using Mapreduce and introduced by this paper : \nhttp://thepublicgrid.org/papers/koufakou_wcci_08.pdf\nFollowing is an example how to run this program under haodoop:\n$hadoop jar programName.jar avfDriver inputData interTempData outputData\nThe output data contains ordered avfValue in the first column, followed by original input data.",
        "Issue Links": []
    },
    "MAHOUT-385": {
        "Key": "MAHOUT-385",
        "Summary": "Unify Vector Writables",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "26/Apr/10 15:02",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "28/Apr/10 04:36",
        "Description": "Per the mailing list thread, creating an issue to track patches and discussion of unifying vector writables. The essence of my attempt will be attached.",
        "Issue Links": []
    },
    "MAHOUT-386": {
        "Key": "MAHOUT-386",
        "Summary": "org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob breaks when no usersFile is supplied",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "27/Apr/10 08:02",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "27/Apr/10 08:25",
        "Description": "When one tries to run org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob without a usersFile (the file containing the userIDs to generate recommendations for) it throws an Exception in the Reducer.\nI took a look at the code and it seems that there are some needless things done:\norg.apache.mahout.cf.taste.hadoop.pseudo.RecommenderReducer tries to create a set of userIDs to generate recommendations for and fails to do this if no usersFile is supplied, when it tries to parse the preferences file. As far as I understand the code the Mapper already maps out only the userIDs to generate recommendations for, so there the check in the reducer is not even necessary, right? Correct me if I'm wrong.\n---------------------------------------------------------------------------\nHow to reproduce the error:\n1) Create a file containing some sample preferences and call it mahout-testing.txt, mine looks like this\n1,2,1\n1,3,2\n2,1,1\n2,3,1\n2) Run the Job without a usersFile, e.g.:\nhadoop jar core/target/mahout-core-0.4-SNAPSHOT.job org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob -Dmapred.input.dir=./mahout-testing.txt -Dmapred.output.dir=/tmp/mahout/out --tempDir /tmp/mahout/tmp --recommenderClassName org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommender\njava.lang.RuntimeException: Error in configuring object\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:426)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:215)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n\t... 5 more\nCaused by: java.lang.NumberFormatException: For input string: \"1,2,1\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n\tat java.lang.Long.parseLong(Long.java:419)\n\tat java.lang.Long.parseLong(Long.java:468)\n\tat org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderReducer.configure(RecommenderReducer.java:88)\n\t... 10 more",
        "Issue Links": []
    },
    "MAHOUT-387": {
        "Key": "MAHOUT-387",
        "Summary": "Cosine item similarity implementation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "27/Apr/10 16:32",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "28/Apr/10 20:15",
        "Description": "I needed to compute the cosine similarity between two items when running org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob, I couldn't find an implementation (did I overlook it maybe?) so I created my own. I want to share it here, in case you find it useful.",
        "Issue Links": []
    },
    "MAHOUT-388": {
        "Key": "MAHOUT-388",
        "Summary": "Upgrade Lucene",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Grant Ingersoll",
        "Created": "28/Apr/10 10:45",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "11/May/10 02:41",
        "Description": "Upgrade Lucene version used to the latest release.",
        "Issue Links": []
    },
    "MAHOUT-389": {
        "Key": "MAHOUT-389",
        "Summary": "UncenteredCosineSimilarity",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "04/May/10 15:17",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "06/May/10 17:55",
        "Description": "org.apache.mahout.cf.taste.impl.similarity.UncenteredCosineSimilarity only computes the cosine distance between those components of the vectors where both vectors have a value greater zero.\nThis is inconsistent with the definition of the cosine (correct me if I'm wrong) and is inconsistent with the distributed cosine similarity computation.",
        "Issue Links": []
    },
    "MAHOUT-390": {
        "Key": "MAHOUT-390",
        "Summary": "Quickstart script for kmeans algorithm",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Robin Anil",
        "Reporter": "Sisir Koppaka",
        "Created": "04/May/10 18:40",
        "Updated": "21/May/11 03:18",
        "Resolved": "08/Feb/11 15:46",
        "Description": "Contains a quickstart shell script for kmeans algorithm on the Reuters dataset as described at https://cwiki.apache.org/MAHOUT/k-means.html \nThe script in JIRA is a slightly modified and cleaner version.",
        "Issue Links": []
    },
    "MAHOUT-391": {
        "Key": "MAHOUT-391",
        "Summary": "Make vector more space efficient with variable-length encoding, et al",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "05/May/10 11:34",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "11/May/10 19:23",
        "Description": "There are a few things we can do to make Vector representations smaller on disk:\n\nUse variable-length encoding for integer values like size and element indices in sparse representations\nFurther, delta-encode indices in sequential representations\nLet caller specify that precision isn't crucial in values, allowing it to store values as floats\n\nSince indices are usually small-ish, I'd guess this saves 2 bytes or so on average, out of 12 bytes per element now.\nUsing floats where applicable saves another 4. Not bad.",
        "Issue Links": []
    },
    "MAHOUT-392": {
        "Key": "MAHOUT-392",
        "Summary": "Test cases for logGamma, Distribution.normal and Distribution.beta, fix for Distribution.normal",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "08/May/10 05:56",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "26/May/10 19:15",
        "Description": "I need access to the normal distribution to write test cases for random matrices.  While testing that, I found it wasn't quite right so I provided an alternative implementation.",
        "Issue Links": []
    },
    "MAHOUT-393": {
        "Key": "MAHOUT-393",
        "Summary": "Distributed item similarity functions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "09/May/10 07:39",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "09/May/10 13:37",
        "Description": "To complete the work started in MAHOUT-389, I've created a distributed version of any item similarity function that is currently already available in a non-distributed manner. An additional M/R job was necessary to compute the number of all users which is needed by some similarity functions (like LogLikelihoodSimilarity for example).\nThere is still some optimization potential in the code as not every similarity function needs all information that is currently extracted (like the number of users e.g.), but the optimization would have made the code much less readable so I did not do any work on that.\nI hope you consider this a useful addition.",
        "Issue Links": []
    },
    "MAHOUT-394": {
        "Key": "MAHOUT-394",
        "Summary": "Correct URL for User Resources > Quickstart",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Jerry Chen",
        "Created": "09/May/10 18:41",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/Sep/10 11:55",
        "Description": "The current URL used for the \"User Resources > Quickstart\" navigation is:\nhttp://cwiki.apache.org/MAHOUT/quickstart.html\nFull formatting can be seen on the Confluence page:\nhttps://cwiki.apache.org/confluence/display/MAHOUT/QuickStart",
        "Issue Links": []
    },
    "MAHOUT-395": {
        "Key": "MAHOUT-395",
        "Summary": "Using KMeansDriver leaves open files and can lead to FileNotFoundException - \"too many open files\" error",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1,                                            0.2,                                            0.3,                                            0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Drew Farris",
        "Reporter": "Scott Ganyo",
        "Created": "15/May/10 01:13",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "15/May/10 02:30",
        "Description": "KMeansDriver uses isConverged() method to determine if the k-means clustering run is complete.  isConverged() has to open each SequenceFIle and read each cluster to see if the containing cluster is converged.  During this process the readers are not explicitly closed, so in the case where there are a large number of sequence files opened, the driving system may run out of file handles before they are eventually implicitly reclaimed.  I'm attaching a patch that explicitly closes these files as they are no longer needed to remain open.",
        "Issue Links": []
    },
    "MAHOUT-396": {
        "Key": "MAHOUT-396",
        "Summary": "Proposal for Implementing Hidden Markov Model",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Max Heimel",
        "Reporter": "Max Heimel",
        "Created": "16/May/10 19:31",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/Sep/10 11:17",
        "Description": "Overview\nThis is a project proposal for a summer-term university project to write a (sequential) HMM implementation for Mahout. Five students will work on this project as part of a course mentored by Isabel Drost.\nAbstract:\nHidden Markov Models are used in multiple areas of Machine Learning, such as speech recognition, handwritten letter recognition or natural language processing. A Hidden Markov Model (HMM) is a statistical model of a process consisting of two (in our case discrete) random variables O and Y, which change their state sequentially. The variable Y with states \n{y_1, ... , y_n}\n is called the \"hidden variable\", since its state is not directly observable. The state of Y changes sequentially with a so called - in our case first-order - Markov Property. This means, that the state change probability of Y only depends on its current state and does not change in time. Formally we write: P(Y(t+1)=y_i|Y(0)...Y(t)) = P(Y(t+1)=y_i|Y(t)) = P(Y(2)=y_i|Y(1)). The variable O with states \n{o_1, ... , o_m}\n is called the \"observable variable\", since its state can be directly observed. O does not have a Markov Property, but its state propability depends statically on the current state of Y. \nFormally, an HMM is defined as a tuple M=(n,m,P,A,B), where n is the number of hidden states, m is the number of observable states, P is an n-dimensional vector containing initial hidden state probabilities, A is the nxn-dimensional \"transition matrix\" containing the transition probabilities such that A[i,j]=P(Y(t)=y_i|Y(t-1)=y_j) and B is the mxn-dimensional \"observation matrix\" containing the observation probabilties such that B[i,j]= P(O=o_i|Y=y_j).\nRabiner [1 defined three main problems for HMM models:\n\nEvaluation: Given a sequence O of observations and a model M, what is the probability P(O|M)  that sequence O was generated by model M. The Evaluation problem can be efficiently solved using the Forward algorithm\nDecoding: Given a sequence O of observations and a model M, what is the most likely sequence Y*=argmax(Y) P(O|M,Y) of hidden variables to generate this sequence. The Decoding problem can be efficiently sovled using the Viterbi algorithm.\nLearning: Given a sequence O of observations, what is the most likely model M*=argmax(M)P(O|M) to generate this sequence.  The Learning problem can be efficiently solved using the Baum-Welch algorithm.\n\nThe target of each milestone is defined as the implementation for the given goals, the respective documentation and unit tests for the implementation.\nTimeline\nMid of May 2010 - Mid of July 2010\nMilestones\nI) Define an HMM class based on Apache Mahout Math package offering interfaces to set model parameters, perform consistency checks, perform output prediction.\n1 week from May 18th till May 25th.\nII) Write sequential implementations of forward (cf. problem 1 [1) and backward algorithm.\n2 weeks from May 25th till June 8th.\nIII) Write a sequential implementation of Viterbi algorithm (cf. problem 2 [1), based on existing forward algorithm implementation.\n2 weeks from June 8th till June 22nd\nIV) Have a running sequential implementation of Baum-Welch algorithm for model parameter learning (application II [ref]), based on existing forward/backward algorithm implementation.\n2 weeks from June 8th till June 22nd\nV) Provide a usage example of HMM implementation, demonstrating all three problems.\n2 weeks from June 22nd till July 6th\nVI) Finalize documentation and implemenation, clean up open ends.\n1 week from July 6th till July 13th\nReferences:\n[1    Lawrence R. Rabiner (February 1989). \"A tutorial on Hidden Markov Models and selected applications in speech recognition\". Proceedings of the IEEE 77 (2): 257-286. doi:10.1109/5.18626.\nPotential test data sets:\nhttp://www.cnts.ua.ac.be/conll2000/chunking/\nhttp://archive.ics.uci.edu/ml/datasets/Character+Trajectories",
        "Issue Links": []
    },
    "MAHOUT-397": {
        "Key": "MAHOUT-397",
        "Summary": "SparseVectorsFromSequenceFiles only outputs a single vector file",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Integration",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "20/May/10 01:29",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "22/Sep/10 07:44",
        "Description": "When running LDA via build-reuters.sh on a 3-node Hadoop cluster, I've noticed that there is only a single vector file produced by the utility preprocessing steps. This means LDA (and other clustering too) can only use a single mapper no matter how large the cluster is. Investigating, it seems that the program argument (-nr) for setting the number of reducers - and hence the number of output files - is not propagated to the final stages where the output vectors are created.",
        "Issue Links": []
    },
    "MAHOUT-398": {
        "Key": "MAHOUT-398",
        "Summary": "Seq2sparse outputs final vectors to different directories depending upon the TF/TFIDF weight switch. This is confusing to users.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Integration",
        "Assignee": "Drew Farris",
        "Reporter": "Jeff Eastman",
        "Created": "22/May/10 00:36",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "31/May/10 02:06",
        "Description": "In TF mode, seq2sparse puts the output vectors into <output>/vectors. In TFIDF mode; however, it puts the output vectors into <output>/tfidf/vectors. This happens because the IDF calculation - if it is selected - happens after TF and uses the TF vectors for its input.\nSeems like both modes ought to output to a consistent directory structure so changing the switch does not change the final output location: perhaps as simple as changing TF to output to <output>/tf/vectors so that the contents of both directories when present are more obvious from their nomenclature.",
        "Issue Links": []
    },
    "MAHOUT-399": {
        "Key": "MAHOUT-399",
        "Summary": "LDA on Mahout 0.3 does not converge to correct solution for overlapping pyramids toy problem.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.3,                                            0.4,                                            0.5",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Jake Mannix",
        "Reporter": "Michael Lazarus",
        "Created": "24/May/10 17:17",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "31/May/12 10:45",
        "Description": "Hello,\nApologies if I have not labeled this correctly.\nI have run a toy problem on Mahout 0.3 (locally) for LDA that I used to test Blei's c version of LDA that he posts on his site. It has an exact solution that the LDA should converge to.  Please see attached PDF that describes the intended output.\nIs LDA working?  The following output indicates some sort of collapsing behavior to me.\nT0 \tT1 \tT2 \tT3 \tT4\nx \tw \tx \tu \tx\nu \tu \tg \tj \tn\nl \tr \ti \tm \tl\nj \tq \th \th \tp\nv \tp \te \ti \tq\ne \tt \tf \tg \tv\nd \ts \td \tf \to\nb \tc \tb \tn \tk\ny \tf \tc \tl \tm\nw \tv \tu \tv \tu\nc \td \tp \ty \tt\nk \to \tl \tr \tr\ni \tb \tj \tk \tj\nf \te \tk \te \tf\ng \tx \ty \ts \ty\nt \ty \tw \tb \tw\nh \ti \ts \tp \ts\no \tl \tv \tx \td\nq \tj \tt \td \ti\nn \tk \to \tt \tb\nThe intended output is (again, please see attached):\nD \tI \tN \tS \tX\nd \ti \tn \ts \tx\nc \th \tm \tt \ty\ne \tj \to \tr \tw\nb \tk \tl \tu \tv\nf \tg \tp \tq \ta\na \tf \tk \tp \tb\ng \tl \tq \tv \tu\nh \tm \tj \tw \tt\ny \tu \tr \to \tc\nn \ts \td \td \ti\ns \te \tx \tf \tf\nr \tq \ti \ti \tn\nm \tv \tw \tc \to\no \tw \tu \ta \th\nq \tn \ts \th \tg\np \tt \tc \tx \td\nt \tx \tf \te \tl\nx \td \te \tj \ts\nw \ty \tg \tb \tj\ni \tr \ty \tn \tr\nu \to \th \ty \tm\nk \tb \tt \tl \te\nv \tc \ta \tm \tk\nj \ta \tb \tg \tp\nl \tp \tv \tk \tq\nWhat tests do you run to make sure the output is correct?\nThank you,\nMike.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1009"
        ]
    },
    "MAHOUT-400": {
        "Key": "MAHOUT-400",
        "Summary": "Move SequenceFileFromDirectory to Utils",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "27/May/10 13:55",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "27/May/10 14:12",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-401": {
        "Key": "MAHOUT-401",
        "Summary": "Use NamedVector in seq2sparse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Integration",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "27/May/10 16:01",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "27/Sep/10 02:14",
        "Description": "In seq2sparse, TFIDFPartialVectorReducer and TFPartialVectorReducer should write NamedVectors. It appears that a lack of labels on the vector input to k-means at least breaks the cluster-dumper in the sense that it no longer prints the original document ids for points.\nSee: http://lucene.472066.n3.nabble.com/where-are-the-points-in-each-cluster-kmeans-clusterdump-td838683.html#a845600\nI wonder if this is also an issue with the code that generates vectors from lucene indexes?",
        "Issue Links": []
    },
    "MAHOUT-402": {
        "Key": "MAHOUT-402",
        "Summary": "NamedVectors are not readily identifiable in vectordumper output",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "28/May/10 04:20",
        "Updated": "21/May/11 03:19",
        "Resolved": "19/Jan/11 00:49",
        "Description": "When dumping a sequence file of Writable,NamedVector using vectordumper in either JSON or standard format, it is not apparent in the output that the vectors are indeed named vectors.\nFor example, after applying MAHOUT-401 to produce NamedVectors from seq2sparse, I run:\n\n./bin/mahout vectordump -j -p -s ~/mahout/reuters-out-seqdir-sparse/tf-vectors/part-00000\n\n\nAnd get: \n\nInput Path: /home/drew/mahout/reuters-out-seqdir-sparse/tf-vectors/part-00000\n/reut2-000.sgm-0.txt    {\"class\":\"org.apache.mahout.math.RandomAccessSparseVector\",\"vector\" [...]\n\n\nor when removing the -j argument:\n\n/reut2-000.sgm-0.txt    elts: {1026:3.0, 16150:1.0, 3338:3.0, 16147:1.0, 3339:1.0, 12240:1.0, [...]\n\n\nThe first case, when dumping JSON, is due to the fact that NamedVector simply calls its delegate's asFormatString method. Granted the naive approach of implementing asFormatString in named vector also produces some nasty output:\n\n/reut2-001.sgm-468.txt\t{\"class\":\"org.apache.mahout.math.NamedVector\",\"vector\":\"{\\\"delegate\\\":{\\\"class\\\":\\\"org.apache.mahout.math.RandomAccessSparseVector\\\" [...]\n\n\nSo a little more thought needs to be given to that approach.\nFor the non-json format, VectorHelper.vectorToString(..) is the culprit. Would it be ok to do an instanceof NamedVector here and emit the name?",
        "Issue Links": []
    },
    "MAHOUT-403": {
        "Key": "MAHOUT-403",
        "Summary": "Regex to Various Output Formats",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "28/May/10 14:04",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "05/Nov/11 17:12",
        "Description": "Would be great to have a M/R job that took in a line, applied a regex to it and then used the capturing groups as output to various formats (FPG, Classifier, etc.)",
        "Issue Links": []
    },
    "MAHOUT-404": {
        "Key": "MAHOUT-404",
        "Summary": "AbstractJob improvements.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Integration",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "29/May/10 03:42",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "08/Jul/10 12:44",
        "Description": "Per discussion : http://lucene.472066.n3.nabble.com/Re-input-now-Dmapred-input-dir-td852297.html#a852297\nWith the advent of the parsedArgs map returned by AbstractJob.parseArguments is there\na need to pass Option arguments around anymore? Could AbstractJob maintain\nOptions state in a sense?\nFor example, from RecommenderJob:\n\n    Option numReccomendationsOpt = AbstractJob.buildOption(\"numRecommendations\", \"n\", \n      \"Number of recommendations per user\", \"10\");\n    Option usersFileOpt = AbstractJob.buildOption(\"usersFile\", \"u\",\n      \"File of users to recommend for\", null);\n    Option booleanDataOpt = AbstractJob.buildOption(\"booleanData\", \"b\",\n      \"Treat input as without pref values\", Boolean.FALSE.toString());\n\n    Map<String,String> parsedArgs = AbstractJob.parseArguments(\n        args, numReccomendationsOpt, usersFileOpt, booleanDataOpt);\n    if (parsedArgs == null) {\n      return -1;\n    }\n\n\nCould be changed to something like:\n\nbuildOption(\"numRecommendations\", \"n\", \"Number of recommendations per user\",\n\"10\");\nbuildOption(\"usersFile\", \"u\", \"File of users to recommend for\", null);\nbuildOption(\"booleanData\", \"b\", \"Treat input as without pref values\",\nBoolean.FALSE.toString());\nMap<String,String> parsedArgs = parseArguments();",
        "Issue Links": []
    },
    "MAHOUT-405": {
        "Key": "MAHOUT-405",
        "Summary": "Remove easy deprecations from math package",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "30/May/10 20:24",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "02/Jun/10 05:56",
        "Description": "A few cases like IntIntFunction are interfaces and don't need further tests to undeprecate.  Others like MersenneTwister are a little tricker.\nHere is a list of commonly used interfaces that can be undeprecated:\n[deprecation] org.apache.mahout.math.function.Double9Function in org.apache.mahout.math.function has been deprecated\n[deprecation] org.apache.mahout.math.function.IntIntDoubleFunction in org.apache.mahout.math.function has been deprecated\n[deprecation] org.apache.mahout.math.function.IntIntFunction in org.apache.mahout.math.function has been deprecated",
        "Issue Links": []
    },
    "MAHOUT-406": {
        "Key": "MAHOUT-406",
        "Summary": "Inconsistent return values of different ItemSimilarity implementations",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "31/May/10 12:03",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "31/May/10 13:49",
        "Description": "org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarity returns Double.NaN if you request the similarity for two itemIDs it doesn't know, while  org.apache.mahout.cf.taste.impl.similarity.jdbc.AbstractJDBCItemSimilarity throws a NoSuchItemException.\nThey should act consistently (and both return Double.NaN, I guess).",
        "Issue Links": []
    },
    "MAHOUT-407": {
        "Key": "MAHOUT-407",
        "Summary": "Limit the number of similar items per item in the ItemSimilarityJob",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "01/Jun/10 12:19",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "17/Jun/10 06:24",
        "Description": "In order to keep the item-similarity-matrix sparse, it would be a useful improvement to add an option like \"maxSimilaritiesPerItem\" to o.a.m.cf.taste.hadoop.similarity.item.ItemSimilarityJob, which would make it try to cap the number of similar items per item.\nHowever as we store each similarity pair only once it could happen that there are more than \"maxSimilaritiesPerItem\" similar items for a single item as we can't drop some of the pairs because the other item in the pair might have too little similarities otherwise.\nA default value of 100 co-occurrences (similarities) will be used because this is already the default in the distributed recommender.",
        "Issue Links": []
    },
    "MAHOUT-408": {
        "Key": "MAHOUT-408",
        "Summary": "Mahout CLI OptionExceptions when given \"-h\" or \"--help\" for specific programs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Shannon Quinn",
        "Created": "02/Jun/10 00:38",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "03/Jun/10 12:56",
        "Description": "The following Mahout programs incorrectly processed -h flags, throwing OptionExceptions:\nseq2sparse\nseqdirectory\nseqwiki\nsvd\ntrainclassifier\ntranspose\nThe first five are easily addressed by adding \"parser.setHelpOption(helpOpt)\" in the correct driver files (thanks to Jeff Eastman). The \"transpose\" program also contained a NullPointerException thrown when two integer arguments were not verified. See the patch file for a possible fix.",
        "Issue Links": []
    },
    "MAHOUT-409": {
        "Key": "MAHOUT-409",
        "Summary": "Find bugs finds some bugs in our code",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "03/Jun/10 22:54",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "05/Jun/10 07:05",
        "Description": "I will produce a patch shortly that fixes some low hanging fruit highlighted by findbugs.\nNotably this:\nhttp://hudson.zones.apache.org/hudson/job/Mahout-Quality/48/findbugsResult/HIGH/type.-1824009442/source.8285/#44\nand\nhttp://hudson.zones.apache.org/hudson/job/Mahout-Quality/48/findbugsResult/HIGH/type.-694648638/\nand\nhttp://hudson.zones.apache.org/hudson/job/Mahout-Quality/48/findbugsResult/HIGH/type.-1787470473/\nand\nhttp://hudson.zones.apache.org/hudson/job/Mahout-Quality/48/findbugsResult/HIGH/type.921339475/",
        "Issue Links": []
    },
    "MAHOUT-410": {
        "Key": "MAHOUT-410",
        "Summary": "NPE when calling hasPreferenceValues or getPreferenceValues on newly created FileDataModel",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "07/Jun/10 15:08",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "07/Jun/10 15:37",
        "Description": "An NPE gets thrown when calling hasPreferenceValues() or getPreferenceValue(long userID, long itemID) on a newly created FileDataModel. This happens because these methods do not call checkLoaded() before forwarding the method call to the delegate. Other overridden methods do call checkLoaded(). Seems like a bug.",
        "Issue Links": []
    },
    "MAHOUT-411": {
        "Key": "MAHOUT-411",
        "Summary": "Configurable reload interval in FileDataModel",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Jun/10 16:16",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "09/Jun/10 17:01",
        "Description": "Patch to enable the following usecase:\n\nall preferences are kept in a text file and loaded via a FileDataModel\nat some point in time the text file is completely overwritten with more recent preferences\nan explicit refresh() on the FileDataModel is called\n\nWith the current FileDataModel, one could not see the new preferences as it would only have refreshed the whole file if that was modified more than one minute ago. And I don't want to wait that long to trigger the explicit refresh.\nThe patch makes the interval configurable with keeping the old interval length as default.\nHope I understood the intention of the refreshing mechanism right.",
        "Issue Links": []
    },
    "MAHOUT-412": {
        "Key": "MAHOUT-412",
        "Summary": "file-backed ItemSimilarity",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "10/Jun/10 17:07",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "11/Jun/10 11:34",
        "Description": "a \"FileItemSimilarity\" making it possible to read (and reload) precomputed item-item-similarities from a textfile, similar to what is already possible for preferences with a FileDataModel",
        "Issue Links": []
    },
    "MAHOUT-413": {
        "Key": "MAHOUT-413",
        "Summary": "Sequential access vector has a bug in dot",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "11/Jun/10 21:12",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "11/Jun/10 21:15",
        "Description": "Zhao wrote:\n\nHi,\nOne strange thing, SequentialAccessSparseVector.dot() seems does NOT work\ncorrectly. When I change the Vector w = new\nRandomAccessSparseVector(Integer.MAX_VALUE, 12); Then the code works well.\nI check the source code of SequentialAccessSparseVector.dot(), it checks the\nindex of two Vectors(if both of them are instance of\nSequentialAccessSparseVector), once they mis-match, it will return 0.0.\n*Why do we need such constrain?\n*\nIn this case, what's the best way to get dot product between two mis-matched\nSequentialAccessSparseVector instances?\nCode:\n Vector w = new SequentialAccessSparseVector(Integer.MAX_VALUE, 12);\n w.set(1, 0.4);\n w.set(2, 0.4);\n w.set(3, -0.666666667);\n Vector v = new SequentialAccessSparseVector(Integer.MAX_VALUE, 12);\n v.set(3, 1);\n System.out.println(datastore.getFeatureRow(2).dot(w));\n The result is 0.0 (should be -0.666666667).",
        "Issue Links": []
    },
    "MAHOUT-414": {
        "Key": "MAHOUT-414",
        "Summary": "Usability: Mahout applications need a consistent API to allow users to specify desired map/reduce concurrency",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "13/Jun/10 18:07",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "30/Sep/10 14:50",
        "Description": "If specifying the number of mappers and reducers is a common activity which users need to perform in running Mahout applications on Hadoop clusters then we need to have a standard way of specifying them in our APIs without exposing the full set of Hadoop options, especially for our non-power-users. This is the case for some applications already but others require the use of Hadoop-level -D arguments to achieve reasonable out-of-the-box parallelism even when running our examples. The usability defect is that some of our algorithms won't scale without it and that we don't have a standard way to express this in our APIs.",
        "Issue Links": []
    },
    "MAHOUT-415": {
        "Key": "MAHOUT-415",
        "Summary": "Lucene filter for Collocations",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3,                                            0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "14/Jun/10 03:23",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "04/Jun/11 11:14",
        "Description": "Collocations generated using Mahout could be used to form a whitelist of terms to index into a Lucene index. This patch will provide a way to generate a serialized BloomFilter from CollocationsOutput and a Lucene filter that will take a BloomFilter and emit tokens that are members of that filter. This would allow a set of interesting collocations to be pre-computed for a corpus and then allow the documents to be indexed using only those collocations.",
        "Issue Links": []
    },
    "MAHOUT-416": {
        "Key": "MAHOUT-416",
        "Summary": "Input format of Random Forest in Mahout 0.3 is N*M matrix. It doesn't take sparse matrix as an input.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Karan Jindal",
        "Created": "16/Jun/10 04:40",
        "Updated": "21/May/11 03:22",
        "Resolved": "21/May/11 03:09",
        "Description": "Input format of Random Forest in Mahout 0.3 is N*M matrix. It doesn't take sparse matrix as an input.\nAfter converting sparse matrix into non-sparse matrix the size of file grow exponentially.",
        "Issue Links": []
    },
    "MAHOUT-417": {
        "Key": "MAHOUT-417",
        "Summary": "Update collocations code to hadoop 0.20 api",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "17/Jun/10 03:51",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "08/Jul/10 12:45",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-418": {
        "Key": "MAHOUT-418",
        "Summary": "Computing the pairwise similarities of the rows of a matrix",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "17/Jun/10 11:14",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "28/Jun/10 09:43",
        "Description": "In response to the wish from MAHOUT-362 and the latest discussion on the mailing list started by Kris Jack about computing a document similarity matrix, I tried to generalize the approach we're already using to compute the item-item-similarities for collaborative filtering.\nThe job in the patch computes the pairwise similarity of the rows of a matrix in a distributed manner, is uses a SequenceFile<IntWritable,VectorWritable> as input and outputs such a file too. Custom similarity implementations can be supplied, I've already implemented tanimoto and cosine for demo and testing purposes. The algorithm is based on the one presented here: http://www.umiacs.umd.edu/~jimmylin/publications/Elsayed_etal_ACL2008_short.pdf\nI'd be glad if someone could verify the applicability of this approach by running it with a reasonably large input, I'm also worried that it might buffer to much data in certain steps.\nIf you decide to include it in mahout, some more efforts and decisions (like more tests, more similarity measures, integration with DistributedRowMatrix) would need to be made, I guess.",
        "Issue Links": []
    },
    "MAHOUT-419": {
        "Key": "MAHOUT-419",
        "Summary": "Convert decomposer code to Hadoop 0.20 API",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.3,                                            0.4",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Danny Leshem",
        "Created": "20/Jun/10 14:38",
        "Updated": "21/May/11 03:22",
        "Resolved": "22/Sep/10 07:09",
        "Description": "org.apache.mahout.math.hadoop classes (MatrixMultiplicationJob, TimesSquaredJob, TransposeJob) all use the deprecated Hadoop API. In the spirit of MAHOUT-167 and MAHOUT-143, I suggest converting them to Hadoop's 0.20 API.\nThe reason I'm raising this now is that this code no longer runs on my Hadoop 0.22-SNAPSHOT cluster (not sure why really - it was running fine about a month ago, but after updating to the latest Mahout trunk a few days ago the code throws \"java.lang.RuntimeException: Error in configuring object\" at MapTask.runOldMapper).\nAlso, the documentation at https://cwiki.apache.org/MAHOUT/dimensionalreduction.html is no longer accurate - the command line parameters have changed (even without the new arguments from MAHOUT-308). This is partly due to using the new argument parser which receives the input/output directories differently.",
        "Issue Links": []
    },
    "MAHOUT-420": {
        "Key": "MAHOUT-420",
        "Summary": "Improving the distributed item-based recommender",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "21/Jun/10 09:38",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "08/Jul/10 19:20",
        "Description": "A summary of the discussion on the mailing list:\nExtend the distributed item-based recommender from using only simple cooccurrence counts to using the standard computations of an item-based recommender as defined in Sarwar et al \"Item-Based Collaborative Filtering Recommendation Algorithms\" (http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.144.9927&rep=rep1&type=pdf).\nWhat the distributed recommender generally does is that it computes the prediction values for all users towards all items those users have not rated yet. And the computation is done in the following way:\n u = a user\n i = an item not yet rated by u\n N = all items cooccurring with i\n Prediction(u,i) = sum(all n from N: cooccurrences(i,n) * rating(u,n))\nThe formula used in the paper which is used by GenericItemBasedRecommender.doEstimatePreference(...) too, looks very similar to the one above:\n u = a user\n i = an item not yet rated by u\n N = all items similar to i (where similarity is usually computed by pairwisely comparing the item-vectors of the user-item matrix)\n Prediction(u,i) = sum(all n from N: similarity(i,n) * rating(u,n)) / sum(all n from N: abs(similarity(i,n)))\nThere are only 2 differences:\n a) instead of the cooccurrence count, certain similarity measures like pearson or cosine can be used\n b) the resulting value is normalized by the sum of the similarities\nTo overcome difference a) we would only need to replace the part that computes the cooccurrence matrix with the code from ItemSimilarityJob or the code introduced in MAHOUT-418, then we could compute arbitrary similarity matrices and use them in the same way the cooccurrence matrix is currently used. We just need to separate steps up to creating the co-occurrence matrix from the rest, which is simple since they're already different MR jobs. \nRegarding difference b) from a first look at the implementation I think it should be possible to transfer the necessary similarity matrix entries from the PartialMultiplyMapper to the AggregateAndRecommendReducer to be able to compute the normalization value in the denominator of the formula. This will take a little work, yes, but is still straightforward. It canbe in the \"common\" part of the process, done after the similarity matrix is generated.\nI think work on this issue should wait until MAHOUT-418 is resolved as the implementation here depends on how the pairwise similarities will be computed in the future.",
        "Issue Links": []
    },
    "MAHOUT-421": {
        "Key": "MAHOUT-421",
        "Summary": "Item similarity in AbstractSimilarity always centers data",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Emerson Murphy-HIll",
        "Created": "22/Jun/10 00:16",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "22/Jun/10 06:45",
        "Description": "I'm trying to use UncenteredCosineSimilarity. When calculating similarity between users in AbstractSimilarity>>itemSimilarity, there's a condition to determine whether to center the data:\ndouble result;\nif (centerData) {\n  ... get result with centering\n} else {\n   ... get result without\n}\nIn AbstractSimilarity>>itemSimilarity, there's no conditional. It always centers the data. Shouldn't it only center the data when centerData is true?",
        "Issue Links": []
    },
    "MAHOUT-422": {
        "Key": "MAHOUT-422",
        "Summary": "Fix SVN links on website",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Documentation",
        "Assignee": "Robin Anil",
        "Reporter": "Jonathan Young",
        "Created": "22/Jun/10 13:39",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "31/Jul/10 14:24",
        "Description": "Some of the links to SVN from http://mahout.apache.org are out of date.  In particular,  http://mahout.apache.org/developer-resources.html , http://mahout.apache.org/versioncontrol.html , and the View source link on the LHS of all still refer to http://svn.apache.org/viewcvs.cgi/lucene/mahout/ instead of http://svn.apache.org/viewvc/mahout/ .",
        "Issue Links": []
    },
    "MAHOUT-423": {
        "Key": "MAHOUT-423",
        "Summary": "Optimize getNumUsersWithPreferenceFor(long... itemIDs)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jonathan Young",
        "Created": "22/Jun/10 13:51",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "22/Jun/10 17:17",
        "Description": "I ran a simple collaborative filtering application using a GenericBooleanPrefDataModel built from (a subset of) the Netflix data, Tanimoto similarity, and the GenericItemBasedRecommender, and then called recommender.mostSimilarItems() (a lot).  \nProfiling indicated that the majority of the time was spent in GenericBooleanPrefDataModel.getNumUsersWithPreferenceFor(long... itemIDs).  The version in GenericDataModel is optimized for the cases of one and two itemIDs, but the version in GenericBooleanPrefDataModel always computes the intersection set.\nI can create a patch which optimizes the two cases of itemIDs.length == 1 and itemIDs.length == 2 (similar to the version in GenericDataModel), but perhaps the code should be refactored if these are really the most common cases.",
        "Issue Links": []
    },
    "MAHOUT-424": {
        "Key": "MAHOUT-424",
        "Summary": "The Mahout mailing list page contains a number of outdated links, and doesn't contain links to archives of the new list",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Documentation",
        "Assignee": "Robin Anil",
        "Reporter": "Peter Goldstein",
        "Created": "22/Jun/10 17:46",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "31/Jul/10 15:00",
        "Description": "I just joined the Mahout user mailing list, and it was a bit of a process because the Mailing List page on the Mahout web site is incorrect.  Specifically, this page - http://mahout.apache.org/mailinglists.html - has a number of errors.  These include:\ni)\tOutdated email addresses for the Mahout User List\nii)\tOutdated email addresses for the Mahout Developer List\niii)\tLinks to the mahout-<...>@lucene.apache.org archives, but not links to the more recent <...>@mahout.apache.org lists.  The upshot is that it looks like this project has seen no activity since May.\nI'll submit a patch and atttach to this bug once I've finished downloading the source out of SVN and found the relevant page.",
        "Issue Links": []
    },
    "MAHOUT-425": {
        "Key": "MAHOUT-425",
        "Summary": "Should the release directory have changed when Apache Mahout was promoted to a top level project?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.3",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Peter Goldstein",
        "Created": "22/Jun/10 19:14",
        "Updated": "21/May/11 03:24",
        "Resolved": "31/Jul/10 14:40",
        "Description": "I noticed that the Apache Mahout releases are still found at this link - http://www.apache.org/dyn/closer.cgi/lucene/mahout/ - which is a subdirectory of the Lucene release directory.\nWhen I look at the top level Mahout release link - http://www.apache.org/dyn/closer.cgi/mahout/ - all I see is an empty directory.  Shouldn't historical releases be copied to this directory, and subsequent builds be created here?",
        "Issue Links": []
    },
    "MAHOUT-426": {
        "Key": "MAHOUT-426",
        "Summary": "The Mahout command script doesn't properly run when Hadoop is installed but $MAHOUT_JOB is not set or found in standard locations",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Peter Goldstein",
        "Created": "23/Jun/10 00:00",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "08/Jul/10 21:23",
        "Description": "In the case where no $MAHOUT_JOB is set, and no mahout-examples-.job file is found in the standard locations, the Mahout command script should default to the run locally option.  Instead, when the script is run one received an error that the non-existent file \"$MAHOUT_HOME/mahout-examples-.job\" cannot be found.\nThis is the result of two separate bugs in the Mahout command script:\ni) There is no existence check in the loop checking for mahout-examples-*.job files in $MAHOUT_HOME\nii) The execution if/else clause only checks if either $HADOOP_CONF_DIR or $HADOOP_HOME is empty when determining whether to run locally.  It also should check the $MAHOUT_JOB variable, as it is meaningless to run hadoop in this case.\nFixing these two issues allowed me to (almost) run the $MAHOUT_HOME/examples/bin/build-reuters.sh script.",
        "Issue Links": []
    },
    "MAHOUT-427": {
        "Key": "MAHOUT-427",
        "Summary": "$MAHOUT_HOME/examples/bin/build-reuters.sh doesn't run successfully because of classpath issues related to the -core argument",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Peter Goldstein",
        "Created": "23/Jun/10 00:19",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/Sep/10 11:56",
        "Description": "Once I resolved issue MAHOUT-426 , I was still not able to run the ./examples/bin/build-reuters.sh script without errors  These were ClassNotFoundExceptions, indicating a problem with the classpath.  \nThe issue appears to be related to the \"-core\" argument, which controls the classpath.  Placing '-core' as the first argument to the individual $MAHOUT_HOME/bin/mahout calls in the script solved the issue, but I'm not sure it was the correct solution.  It's unclear to me what the '-core' argument is supposed to signify.\nCan someone shed some light on this, and tell me whether this is the correct solution to the problem?  And if not, what is the correct solution to this classpath issue.",
        "Issue Links": []
    },
    "MAHOUT-428": {
        "Key": "MAHOUT-428",
        "Summary": "KMeansDriver: No job jar file set leads to ClassNotFoundException: org.apache.mahout.clustering.kmeans.KMeansMapper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Ted Dunning",
        "Reporter": "Peter Goldstein",
        "Created": "23/Jun/10 05:31",
        "Updated": "06/Sep/11 20:10",
        "Resolved": "06/Jul/10 17:05",
        "Description": "Running the $MAHOUT_HOME/examples/bin/build-reuters.sh script I encountered the following exception:\n10/06/23 04:41:00 WARN mapred.JobClient: No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).\n10/06/23 04:41:01 INFO input.FileInputFormat: Total input paths to process : 1\n10/06/23 04:41:01 INFO mapred.JobClient: Running job: job_201006222301_0019\n10/06/23 04:41:02 INFO mapred.JobClient:  map 0% reduce 0%\n10/06/23 04:41:11 INFO mapred.JobClient: Task Id : attempt_201006222301_0019_m_000000_0, Status : FAILED\njava.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.mahout.clustering.kmeans.KMeansMapper\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:960)\n\tat org.apache.hadoop.mapreduce.JobContext.getMapperClass(JobContext.java:158)\nThis appears to be identical in cause to MAHOUT-197 , and has an almost identical fix.  Adding a job.setJarByClass(KMeansDriver.class) line to the KMeansDriver.java file fixes the issue.",
        "Issue Links": []
    },
    "MAHOUT-429": {
        "Key": "MAHOUT-429",
        "Summary": "Add timestamp info to DataModel",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "25/Jun/10 18:29",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "13/Aug/10 18:26",
        "Description": "Per discussion on mailing list: would be good to add timestamp information to DataModel.",
        "Issue Links": []
    },
    "MAHOUT-430": {
        "Key": "MAHOUT-430",
        "Summary": "AbstractSimilarity improperly computes vector metrics",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Emerson Murphy-HIll",
        "Created": "28/Jun/10 01:23",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "29/Jun/10 08:10",
        "Description": "Looking at the userSimilarity and itemSimilarity methods in AbstractSimilarity, both compute metrics over each User's/Tool's PreferenceArrays, metrics like 'sumX' and 'sumY'. The algorithms go through each PreferenceArray in a single loop, comparing indexes to make sure we don't fall off the end. Eventually, we get to the end of an array, which is caught here:\nif (compare <= 0) {\n  if (++xPrefIndex >= xLength) \n{\n    break;\n  }\n...\nThe problem is, the metrics may not be correct when the break occurs. Specifically, for the other array, the one that we didn't fall off the end of, the metrics don't reflect the preferences we have not yet visited. In the example above, if yPrefLength<yLength, then sumY2 is too low. One fix is to do something like this:\nif (compare <= 0) {\n  if (++xPrefIndex >= xLength) \n{\n    sumY2 += squareSumRest(yPrefs,yPrefIndex);\n    break;\n  }\n...\nprivate double squareSumRest(Preference[] preferences, int startingFrom) {\n  double squareSum = 0;\n  for(int i = startingFrom; i < preferences.length; i++)\n{\n    double val = preferences[i].getValue();\n    squareSum += val*val;\n  }\n  return squareSum;\n}\nI believe that the problem affects the sumX and sumY variables (and probably sumXYdiff2), but not the sumXY, sumX2, or sumY2 variables.\nA couple of comments about these two methods:\n1) They're really hard to reason about. Isn't there a simpler implementation?\n2) The two methods are very similar. Can't they be combined somehow?",
        "Issue Links": []
    },
    "MAHOUT-431": {
        "Key": "MAHOUT-431",
        "Summary": "NPE in CBayesThetaNormalizerReducer when using HBase",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3,                                            0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Jeremy Handcock",
        "Created": "28/Jun/10 23:16",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "30/Jun/10 05:13",
        "Description": "I'm trying to use the complementary Bayes classifier and store the model in HBase (using BayesParameters.set(\"dataSource\", \"hbase\")).  When I run the MR jobs using CBayesDrvier, I get a NullPointerException in CBayesThetaNormalizerReducer at line 92:\nhBconf.set(new HBaseConfiguration(job));\n'hBconf' is a ThreadLocal<HBaseConfiguration> field and it looks like it has not been initialized.  I initialized it and the model built successfully.  I observed the same bug in the 0.3 release and in 0.4-SNAPSHOT.  I'll attach a patch that initializes the field.",
        "Issue Links": []
    },
    "MAHOUT-432": {
        "Key": "MAHOUT-432",
        "Summary": "Can add one parameter --itemsFile for org.apache.mahout.cf.taste.hadoop.item.RecommenderJob",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "01/Jul/10 13:21",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "05/Jul/10 12:01",
        "Description": "Can add one parameter --itemsFile for org.apache.mahout.cf.taste.hadoop.item.RecommenderJob ?\n--itemsFile some likes the the --usersFile.\nin some case, we want to calculate similarity using all preferences data ,\nbut some item in  preferences  are old ,not available,behind the times or retired ,\nwe want that those data can not recommend to users, can only recommend user with new or valid items.\nso we can add on paramete  --itemsFile for RecommenderJob, itemsFile includes all the new valid items.",
        "Issue Links": []
    },
    "MAHOUT-433": {
        "Key": "MAHOUT-433",
        "Summary": "ClusterLabels no longer works since moving to WeightedVectorWritables",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "05/Jul/10 18:31",
        "Updated": "21/May/11 03:27",
        "Resolved": "05/Jul/10 19:26",
        "Description": "See http://www.lucidimagination.com/search/document/66b5424997e05030/cluster_labels#3cde7b0e0660339d\nThe ClusterLabels program no longer works due to the lack of access to NamedVectors in the getClusterLabels method.",
        "Issue Links": [
            "/jira/browse/MAHOUT-434"
        ]
    },
    "MAHOUT-434": {
        "Key": "MAHOUT-434",
        "Summary": "LuceneIterable doesn't preserve the vector as a namedvector after normalization",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "05/Jul/10 19:02",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "05/Jul/10 19:28",
        "Description": "Per http://www.lucidimagination.com/search/document/713e7c8349727a29/reading_vectors_created_from_a_lucene_index#90f94a8d5bc78610, need to preserve NamedVector when normalizing",
        "Issue Links": [
            "/jira/browse/MAHOUT-433"
        ]
    },
    "MAHOUT-435": {
        "Key": "MAHOUT-435",
        "Summary": "Add assign(DenseVector)/assign(DenseMatrix) function to DenseVector/DenseMatrix",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Max Heimel",
        "Created": "06/Jul/10 13:07",
        "Updated": "21/May/11 03:19",
        "Resolved": "15/Jan/11 16:36",
        "Description": "This new feature would add a new overloaded assign function to (Dense)Vector/(Dense)Matrix, that allows to assign the content of another (Dense)Vector / (Dense)Matrix by overwriting the content of the internal double array. Compared to using .clone(), this feature would reduce the number of memory allocations. \nFor example in case of an iterative algorithm, that needs to check for convergence;\nConvergence check using .clone()\nDensematrix newMatrix = oldMatrix.clone();\nwhile(!converged)\n{\n    // perform iteration computation on newMatrix\n    converged=checkConvergence(newMatrix,oldMatrix);\n    oldMatrix = newMatrix.clone(); // results in memory allocation\n}\n\n\nConvergence check using .assign(Matrix)\nDensematrix newMatrix = oldMatrix.clone();\nwhile(!converged)\n{\n    // perform iteration computation on newMatrix\n    converged=checkConvergence(newMatrix,oldMatrix);\n    oldMatrix.assign(newMatrix); // no memory allocation\n}",
        "Issue Links": []
    },
    "MAHOUT-436": {
        "Key": "MAHOUT-436",
        "Summary": "Need minValue and minValueIndex in parallel with maxValue and maxValueIndex",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "10/Jul/10 22:59",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "11/Jul/10 06:48",
        "Description": "Some of the stuff I am doing needs to find minimum values similar to the way that we can currently find maximum values.",
        "Issue Links": []
    },
    "MAHOUT-437": {
        "Key": "MAHOUT-437",
        "Summary": "Naive bayes input preparation not compatible with new Lucene API",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "11/Jul/10 00:59",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "11/Jul/10 01:09",
        "Description": "Need a test for error during instantiation.\nI have a patch and will post and commit it shortly.",
        "Issue Links": []
    },
    "MAHOUT-438": {
        "Key": "MAHOUT-438",
        "Summary": "Wrong javadoc at org.apache.mahout.common.AbstractJob.addInputOption()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "11/Jul/10 13:16",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "11/Jul/10 22:56",
        "Description": "org.apache.mahout.common.AbstractJob.addInputOption() has a wrong javadoc comment that seems to have been copied from addOutputOption()",
        "Issue Links": []
    },
    "MAHOUT-439": {
        "Key": "MAHOUT-439",
        "Summary": "Misleading input description of ItemSimilarityJob and RecommenderJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "11/Jul/10 13:18",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "11/Jul/10 22:56",
        "Description": "ItemSimilarityJob and RecommenderJob both use addInputOption() from AbstractJob. The command line description for this option states that a SequenceFile of VectorWritable is expected, which is wrong for these jobs.",
        "Issue Links": []
    },
    "MAHOUT-440": {
        "Key": "MAHOUT-440",
        "Summary": "Convenient usage of RowSimilarityJob and ItemSimilarityJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "11/Jul/10 14:42",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "12/Jul/10 11:55",
        "Description": "I have added those two jobs to the MahoutDriver and made them accept shortcut names for the currently existing similarity implementations, so that users do not have to supply the full classname (e.g. you can call the job now with -s tanimoto-coefficient instead of -s org.apache.mahout.math.hadoop.similarity.vector.DistributedTanimotoCoefficientVectorSimilarity)",
        "Issue Links": []
    },
    "MAHOUT-441": {
        "Key": "MAHOUT-441",
        "Summary": "File-backed IDMigrator",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "16/Jul/10 15:42",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "17/Jul/10 21:18",
        "Description": "To complete taste's suite of file-related implementations (currently FileDataModel and FileItemSimilarity) I created an IDMigrator backed by a file. It can be refreshed and will reload it's data into memory in the same way FileDataModel and FileItemSimilarity are already doing this.\nI couldn't prevent having to split up the IDMigrator interface into 2 separate interfaces because the update methods (like storeMapping(...) and initialize(...)) did not fit in the concept of reloading data from an external source, hope that's ok.",
        "Issue Links": []
    },
    "MAHOUT-442": {
        "Key": "MAHOUT-442",
        "Summary": "Simple feature reduction options for Bayes classifiers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "19/Jul/10 01:14",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "29/Aug/10 03:23",
        "Description": "Adding options to the Bayes TrainClassifier driver to filter features using minimum df or tf. Features that only appear in a handful of documents or less than X times within the entire input set will be removed from the training feature set entirely. This will allow the Bayes classifiers to scale to larger corpora.\nMore background: \nWhen running the wikipedia example, I discovered that the number of features produced with -ng 1 was pretty outstanding: 9,500,000 using the default settings after running the following commands:\n\n./bin/mahout org.apache.mahout.classifier.bayes.WikipediaXmlSplitter -d wikipedia/enwiki-20100622-pages-articles.xml.bz2 -owikipedia/chunks -c 64\n./bin/mahout org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorDriver -i wikipedia/chunks -o wikipedia/bayes-input -c examples/src/test/resources/country.txt\n./bin/mahout org.apache.mahout.classifier.bayes.TrainClassifier -i wikipedia/bayes-input -o wikipedia/bayes-model -type cbayes -ng 1  -source hdfs\n\n\nThis if course makes testing the classifier tricky on machines of modest means because TestClassifier attempts to load all features into memory on the machines the mapper is running on.\nIt appears that Grant ran into a similar issue last year: \nhttp://www.lucidimagination.com/search/document/7fff9bc0b3350370/getting_started_with_classification#ba6838a9c8b9090c\nThis patch will add --minDf and --minSupport options to TrainClassifier. Also --skipCleanup to prevent the deletion of the output of the BayesFeatureDriver, which can be useful in order to allow inspection the resulting feature set in order to tune rules for feature production.",
        "Issue Links": []
    },
    "MAHOUT-443": {
        "Key": "MAHOUT-443",
        "Summary": "Need to undeprecate the colt gamma distribution",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "21/Jul/10 22:17",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/Sep/10 11:59",
        "Description": "I need this for testing some estimation code.",
        "Issue Links": []
    },
    "MAHOUT-444": {
        "Key": "MAHOUT-444",
        "Summary": "Need on-line distribution summary statistics ... mean, median, min, max q25, q75",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "22/Jul/10 00:56",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/Sep/10 12:00",
        "Description": "For the on-line learning algorithms it is very helpful to have robust on-line estimate of various summary statistics.",
        "Issue Links": []
    },
    "MAHOUT-445": {
        "Key": "MAHOUT-445",
        "Summary": "Customizable strategies for candidate item fetching",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Jul/10 10:55",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "31/Jul/10 14:15",
        "Description": "At the beginning of the recommendation process, a recommender has to identify a set of \"candidate items\" which are items that could possibly be recommended to the user, the final result of the recommender's computation will  be a subset of those.\nThe current approach in AbstractRecommender.getAllOtherItems(...) turns out to be very slow if there is a high number of cooccurrences in the data (like in the grouplens 1M dataset for example). The aim of this patch is to make the way in which these candidate items are identified customizable.",
        "Issue Links": []
    },
    "MAHOUT-446": {
        "Key": "MAHOUT-446",
        "Summary": "Mahalanobis Distance + Singular Value Decomposition",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Drew Farris",
        "Reporter": "Nicolas Maillot",
        "Created": "22/Jul/10 19:30",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "22/Sep/10 15:37",
        "Description": "This patch contains an implementation of the Mahalanobis distance + a unit test.\nAs explained in wikipedia (http://en.wikipedia.org/wiki/Mahalanobis_distance) ,  it is a useful way of determining similarity of an unknown sample set to a known one. It differs from Euclidean distance in that it takes into account the correlations of the data set and is scale-invariant.\nAlso contained in the patch:\n-A port of the SingularValueDecomposition Class to the Matrix data structure + unit tests.\n-An embryonic port of the matrix.linalg Algebra class to the Matrix/Vector data structure.",
        "Issue Links": []
    },
    "MAHOUT-447": {
        "Key": "MAHOUT-447",
        "Summary": "ClassNotFound exception when running mahout",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Randy Prager",
        "Created": "23/Jul/10 17:46",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "27/Jul/10 18:28",
        "Description": "We built 0.4-SNAPSHOT from source\nWhen running $MAHOUT_HOME/bin/mahout we get the following exception (and we can't find the class in the source tree)\n\njava.lang.NoClassDefFoundError: org/apache/mahout/math/PersistentObject\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:621)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:124)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:260)\n        at java.net.URLClassLoader.access$000(URLClassLoader.java:56)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:195)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:621)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:124)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:260)\n        at java.net.URLClassLoader.access$000(URLClassLoader.java:56)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:195)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\n        at java.lang.ClassLoader.defineClass1(Native Method)\n        at java.lang.ClassLoader.defineClass(ClassLoader.java:621)\n        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:124)\n        at java.net.URLClassLoader.defineClass(URLClassLoader.java:260)\n        at java.net.URLClassLoader.access$000(URLClassLoader.java:56)\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:195)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\n        at java.lang.Class.forName0(Native Method)\n        at java.lang.Class.forName(Class.java:169)\n        at org.apache.mahout.driver.MahoutDriver.addClass(MahoutDriver.java:206)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:108)\nCaused by: java.lang.ClassNotFoundException: org.apache.mahout.math.PersistentObject\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\n        ... 40 more",
        "Issue Links": []
    },
    "MAHOUT-448": {
        "Key": "MAHOUT-448",
        "Summary": "Prepare 20news needs some user interface smoothing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "24/Jul/10 03:58",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/Jul/10 04:00",
        "Description": "driver.props doesn't know about the program for preparing the 20newsgroup data.  Also the program is a bit brittle in the face of non-existent output directory and such.\nI will commit the change shortly.",
        "Issue Links": []
    },
    "MAHOUT-449": {
        "Key": "MAHOUT-449",
        "Summary": "negative binomial distribution afflicted by confused conventions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "27/Jul/10 06:26",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/Sep/10 12:00",
        "Description": "There are several was that the negative binomial distribution can be defined.  Our current implementation uses all of them with predictable problems resulting.\nI have fairly comprehensive test cases and bug fixes that make this right.",
        "Issue Links": []
    },
    "MAHOUT-450": {
        "Key": "MAHOUT-450",
        "Summary": "RowSimilarityJob  error",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "27/Jul/10 16:30",
        "Updated": "21/May/11 03:22",
        "Resolved": "28/Jul/10 17:12",
        "Description": "when ran RowSimilarityJob job as following:\n hadoop jar ../../singlejar/prefetch-hadoop-1.0-full.jar org.apache.mahout.math.hadoop.similarity.RowSimilarityJob --input /steer/item/tap/temp/maybePruneItemUserMatrixPath --output /steer/item/tap/temp/similarityMatrix -s SIMILARITY_COOCCURRENCE -r 10000 -m 100\ngot following error:\nError: java.lang.ClassNotFoundException: org.apache.commons.cli2.OptionException\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)\n        at org.apache.mahout.math.hadoop.similarity.RowSimilarityJob$SimilarityReducer.setup(RowSimilarityJob.java:284)\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n        at org.apache.hadoop.mapred.Child.main(Child.java:170)\n10/07/27 12:14:31 INFO mapred.JobClient: Task Id : attempt_201007261131_0026_r_000000_1, Status : FAILED\nError: java.lang.ClassNotFoundException: org.apache.commons.cli2.OptionException\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)\n        at org.apache.mahout.math.hadoop.similarity.RowSimilarityJob$SimilarityReducer.setup(RowSimilarityJob.java:284)\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n        at org.apache.hadoop.mapred.Child.main(Child.java:170)\n10/07/27 12:14:44 INFO mapred.JobClient: Task Id : attempt_201007261131_0026_r_000000_2, Status : FAILED\nError: java.lang.ClassNotFoundException: org.apache.commons.cli2.OptionException\n        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)\n        at org.apache.mahout.math.hadoop.similarity.RowSimilarityJob$SimilarityReducer.setup(RowSimilarityJob.java:284)\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:566)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n        at org.apache.hadoop.mapred.Child.main(Child.java:170)\nIt seemed that here error:\npublic static class SimilarityReducer\n      extends Reducer<WeightedRowPair,Cooccurrence,SimilarityMatrixEntryKey, MatrixEntryWritable> {\n    private DistributedVectorSimilarity similarity;\n    private int numberOfColumns;\n    @Override\n    protected void setup(Context ctx) throws IOException, InterruptedException {\n      super.setup(ctx);\n      similarity = instantiateSimilarity(ctx.getConfiguration().get(DISTRIBUTED_SIMILARITY_CLASSNAME));  //this line,seems can not load the class.\n      numberOfColumns = ctx.getConfiguration().getInt(NUMBER_OF_COLUMNS, -1);\n      if (numberOfColumns < 1) \n{\n        throw new IllegalStateException(\"Number of columns was not correctly set!\");\n      }\n    }",
        "Issue Links": []
    },
    "MAHOUT-451": {
        "Key": "MAHOUT-451",
        "Summary": "Simple utility to split bayes input into training/test sets",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "28/Jul/10 04:33",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "03/Oct/10 10:26",
        "Description": "Provides a simply utility that you point at a directory containing files in Bayes classifier input format. Given the number of documents to write to the test set, for each input file it will produce files in two output directories, one containing training data with the test documents removed and a second containing the test documents.",
        "Issue Links": []
    },
    "MAHOUT-452": {
        "Key": "MAHOUT-452",
        "Summary": "Need row and column views and aggregates",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "28/Jul/10 22:36",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "03/Aug/10 00:20",
        "Description": "It is common to need to sum up the rows of a matrix or to operate on a view of a row rather than a copy.\nI just ran into this big time so I implemented these operations.",
        "Issue Links": [
            "/jira/browse/MAHOUT-453"
        ]
    },
    "MAHOUT-453": {
        "Key": "MAHOUT-453",
        "Summary": "Need on-line computation of AUC for evaluation of on-line classifiers",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "28/Jul/10 23:47",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "24/Sep/10 11:59",
        "Description": "It turns out that AUC is easy to compute on-line.  Good news for M-228.",
        "Issue Links": [
            "/jira/browse/MAHOUT-452"
        ]
    },
    "MAHOUT-454": {
        "Key": "MAHOUT-454",
        "Summary": "Bad link in java doc for  LogLikelihoodSimilarity",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "30/Jul/10 22:08",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "31/Jul/10 14:28",
        "Description": "Better to point to the blog entry that talks about the subject (and get an updated citeseer link as well)",
        "Issue Links": []
    },
    "MAHOUT-455": {
        "Key": "MAHOUT-455",
        "Summary": "NearestNUserNeighborhood problems with large Ns",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Yanir Seroussi",
        "Created": "05/Aug/10 05:34",
        "Updated": "31/Oct/10 15:49",
        "Resolved": "10/Aug/10 17:23",
        "Description": "I set a large n for NearestNUserNeighborhood, with the intention of including all users in the neighbourhood. However, I encountered the following problems:\n(1) If n is set to Integer.MAX_VALUE, the program crashes with the following stack trace:\nException in thread \"main\" java.lang.IllegalArgumentException\n\tat java.util.PriorityQueue.<init>(PriorityQueue.java:152)\n\tat org.apache.mahout.cf.taste.impl.recommender.TopItems.getTopUsers(TopItems.java:93)\n\tat org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood.getUserNeighborhood(NearestNUserNeighborhood.java:111)\nThis is because TopItems.getTopUsers() tries to create a PriorityQueue with a capacity of Integer.MAX_VALUE + 1.\n(2) If n is set to a large integer value (e.g., 1 billion), it crashes with the following stack trace:\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n\tat java.util.PriorityQueue.<init>(PriorityQueue.java:153)\n\tat org.apache.mahout.cf.taste.impl.recommender.TopItems.getTopUsers(TopItems.java:93)\n\tat org.apache.mahout.cf.taste.impl.neighborhood.NearestNUserNeighborhood.getUserNeighborhood(NearestNUserNeighborhood.java:111)\nThis is due to the same reason - trying to create a PriorityQueue with size n + 1.\nIn my opinion, this should be fixed by changing n to the number of users in the DataModel when NearestNUserNeighborhood is created, or by letting users specify n = -1 (or a similar value) when they want the user neighbourhood to include all users.",
        "Issue Links": []
    },
    "MAHOUT-456": {
        "Key": "MAHOUT-456",
        "Summary": "RowSimilarityJob should not produce SequentialAccessSparseVectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "05/Aug/10 20:50",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "10/Aug/10 17:24",
        "Description": "RowSimilarityJob currently produces SequentialAccessSparseVectors with cardinality Integer.MAX_VALUE wrapped inside VectorWritables.\nIt should better produce RandomAccessSparseVectors as some methods like assign(Vector) are very slow on such SequentialAccessSparseVectors.",
        "Issue Links": []
    },
    "MAHOUT-457": {
        "Key": "MAHOUT-457",
        "Summary": "ItemSimilarityJob and RecommenderJob don't work on Amazon ElasticMapReduce",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "05/Aug/10 21:02",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "10/Aug/10 14:43",
        "Description": "I'm currently evaluating ItemSimilarityJob and RecommenderJob on ElasticMapReduce, it seems we have some small problems with S3, mostly due to the fact that we need to use Filesystem.get(path.toUri(), conf) instead of Filesystem.get(conf) in the code. I will create a patch for that the next days.\nI'm writing this mail because I encountered another problem I currently can't solve. RecommenderJob is emulating MultipleInputs (which is currently missing in Hadoop 0.20 AFAIK) by reading data from a combined path that is built like that:\n    new Path(prePartialMultiplyPath1 + \",\" + prePartialMultiplyPath2)\nMy Job always fails with this exception here:\n    java.lang.IllegalArgumentException: Invalid hostname in URI s3:/testingbucket-12345/tmp/prePartialMultiply2",
        "Issue Links": []
    },
    "MAHOUT-458": {
        "Key": "MAHOUT-458",
        "Summary": "The LDA output does not include the topic-probability distribution per document (p(z|d)). It outputs only the topics and corresponding words.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Jake Mannix",
        "Reporter": "Himanshu Gahlot",
        "Created": "06/Aug/10 19:55",
        "Updated": "06/Jun/11 12:21",
        "Resolved": "30/Sep/10 14:29",
        "Description": "The current implementation of LDA outputs only topics and their words. Many applications need the p(z|d) values of a document to use this vector as a reduced representation of the document (dimensionality reduction of document). We need to introduce a new key which would keep track of the gamma values for each document (as obtained from the document.infer() method) and writes these to the output stream and finally, PrintLDATopics should output these values per document id. Also, outputting the probabilities of words in a topic would also provide a more meaningful output.",
        "Issue Links": [
            "/jira/browse/MAHOUT-682"
        ]
    },
    "MAHOUT-459": {
        "Key": "MAHOUT-459",
        "Summary": "Reading an Index from Lucene/Solr 4.0-dev",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": "Ted Dunning",
        "Reporter": "Stephen McGill",
        "Created": "06/Aug/10 20:27",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "26/May/11 07:42",
        "Description": "It is not possible to read indexes created by Lucene/Solr 4.0-dev (the trunk development) with the Lucene libraries that are included with Mahout-dev.  When adding the new Lucene/Solr 4.0-dev, there are API changes that do not allow Mahout to compile.\nBy adapting mahout-utils to fit Lucene/Solr 4.0-dev's API changes, it is possible to read its index.",
        "Issue Links": []
    },
    "MAHOUT-460": {
        "Key": "MAHOUT-460",
        "Summary": "Add \"maxPreferencesPerItemConsidered\" option to o.a.m.cf.taste.hadoop.similarity.item.ItemSimilarityJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "08/Aug/10 09:38",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "18/Aug/10 06:17",
        "Description": "Because \"coocurrence algorithms ... scale in the square of the number of occurrences most popular item\" (Ted wrote that in a recent mail) we should offer a parameter to the ItemSimilarity job that makes it limit the number of considered preferences per item. RecommenderJob already has such an option.",
        "Issue Links": [
            "/jira/browse/MAHOUT-467",
            "/jira/browse/MAHOUT-468"
        ]
    },
    "MAHOUT-461": {
        "Key": "MAHOUT-461",
        "Summary": "DistributedLanczosSolver doesn't check/validate output dir",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "08/Aug/10 12:11",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "29/Aug/10 11:59",
        "Description": "I ran /mahout svd -Dmapred.input.dir=/tmp/solr-clust-n2/part-out.vec --tempDir /tmp/solr-clust-n2/svdTemp --rank 200 --numCols 62072 --numRows  130103\nand got\nINFO: Persisting 200 eigenVectors and eigenValues to: null\nException in thread \"main\" java.lang.IllegalArgumentException: Can not create a Path from a null string\n       at org.apache.hadoop.fs.Path.checkPathArg(Path.java:78)\n       at org.apache.hadoop.fs.Path.<init>(Path.java:90)\n       at org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.serializeOutput(DistributedLanczosSolver.java:110)\n       at org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.run(DistributedLanczosSolver.java:97)\n       at org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.run(DistributedLanczosSolver.java:76)\n       at org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver$DistributedLanczosSolverJob.run(DistributedLanczosSolver.java:166)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n       at org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.main(DistributedLanczosSolver.java:172)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n       at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n       at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:175)\nThe solver doesn't ask for nor require the output value and it isn't documented either.",
        "Issue Links": []
    },
    "MAHOUT-462": {
        "Key": "MAHOUT-462",
        "Summary": "RecommenderJob should use simple cooccurrence as default similarity measure",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Aug/10 21:53",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "13/Aug/10 03:40",
        "Description": "To make the new version of RecommenderJob more consistent with the past behavior we should use simple cooccurrence (org.apache.mahout.math.hadoop.similarity.vector.DistributedCooccurrenceVectorSimilarity) as default similarity measure when no similarity classname is supplied.",
        "Issue Links": []
    },
    "MAHOUT-463": {
        "Key": "MAHOUT-463",
        "Summary": "Boolean Data can not get any recommendation by running RecommnenderJob",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "11/Aug/10 16:24",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "13/Aug/10 18:22",
        "Description": "Boolean Preference Data can not get any recommendation by run RecommnenderJob,\nbut It can get data using the build in about Aprl.\nIt seems that it can not get any data by running RowSimilarityJob",
        "Issue Links": []
    },
    "MAHOUT-464": {
        "Key": "MAHOUT-464",
        "Summary": "Web-site still points to old svn repo",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Ted Dunning",
        "Created": "11/Aug/10 18:07",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "14/Oct/10 17:12",
        "Description": "This page is the top hit on google for the query [mahout svn]\nhttp://mahout.apache.org/developer-resources.html\nBut it points to \nhttp://svn.apache.org/repos/asf/lucene/mahout/trunk lucene/mahout/trunk\nI don't know how to change this or I would.  Can somebody who knows, take a look?",
        "Issue Links": []
    },
    "MAHOUT-465": {
        "Key": "MAHOUT-465",
        "Summary": "Vector aggregation uses incorrect initial value",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "12/Aug/10 02:49",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "13/Aug/10 06:05",
        "Description": "Tests and patch forthcoming",
        "Issue Links": []
    },
    "MAHOUT-466": {
        "Key": "MAHOUT-466",
        "Summary": "simplify or alternative  Similarity arithmetic(AbstractDistributedVectorSimilarity) for boolean data",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "12/Aug/10 12:41",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "13/Aug/10 03:45",
        "Description": "For boolean data ,the prefValue  is  always 1.0f, We need simplify Similarity arithmetic\nfor example:\n1) DistributedEuclideanDistanceVectorSimilarity \npackage org.apache.mahout.math.hadoop.similarity.vector;\nimport org.apache.mahout.math.hadoop.similarity.Cooccurrence;\n/**\n\ndistributed implementation of euclidean distance as vector similarity measure\n      */\n      public class DistributedEuclideanDistanceVectorSimilarity extends AbstractDistributedVectorSimilarity {\n\n@Override\nprotected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,\ndouble weightOfVectorB, int numberOfColumns) {\ndouble n = 0.0;\ndouble sumXYdiff2 = 0.0;\nfor (Cooccurrence cooccurrence : cooccurrences) \n{ double diff = cooccurrence.getValueA() - cooccurrence.getValueB(); sumXYdiff2 += diff * diff; n++; }\n\nreturn n / (1.0 + Math.sqrt(sumXYdiff2));\n}\n}\nthis one is always return n (=cooccurrence.size())\n2) DistributedUncenteredCosineVectorSimilarity \n/**\n\ndistributed implementation of cosine similarity that does not center its data\n      */\n      public class DistributedUncenteredCosineVectorSimilarity extends AbstractDistributedVectorSimilarity {\n\n@Override\nprotected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,\ndouble weightOfVectorB, int numberOfColumns) {\nint n = 0;\ndouble sumXY = 0.0;\ndouble sumX2 = 0.0;\ndouble sumY2 = 0.0;\nfor (Cooccurrence cooccurrence : cooccurrences) \n{ double x = cooccurrence.getValueA(); double y = cooccurrence.getValueB(); sumXY += x * y; sumX2 += x * x; sumY2 += y * y; n++; }\n\nif (n == 0) \n{ return Double.NaN; }\ndouble denominator = Math.sqrt(sumX2) * Math.sqrt(sumY2);\nif (denominator == 0.0) \n{ // One or both vectors has -all- the same values; // can't really say much similarity under this measure return Double.NaN; }\nreturn sumXY / denominator;\n}\n}\nthis one will always return 1.0\n3) DistributedUncenteredZeroAssumingCosineVectorSimilarity \nIf n users like ItemA, m users like ItemB,p users like both ItemA and ItemB,\nDistributedUncenteredZeroAssumingCosineVectorSimilarity return p/(m*n).\nit also can use for Boolean data, but we can provide a simple one , return (p*p)/(m*n),no so much computing.",
        "Issue Links": []
    },
    "MAHOUT-467": {
        "Key": "MAHOUT-467",
        "Summary": "Change Iterable<Cooccurrence> in  org.apache.mahout.math.hadoop.similarity.RowSimilarityJob.SimilarityReducer  to list or array to improve the performance",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "12/Aug/10 13:17",
        "Updated": "21/May/11 03:22",
        "Resolved": "22/Sep/10 07:12",
        "Description": "In Class AbstractDistributedVectorSimilarity\n      protected int countElements(Iterator<?> iterator)\n      { int count = 0;\n          while (iterator.hasNext()) \n\n{\n                  count++; \n                  iterator.next(); \n           }\n return count; \n    }\nThe method countElements is used continually and is called continually ,but it has bad performance.\nIf the iterator has million elements ,we have to iterate million  times to just get the count of the iterator.\nthis methods used in many pacles:\n1) DistributedCooccurrenceVectorSimilarity \npublic class DistributedCooccurrenceVectorSimilarity extends AbstractDistributedVectorSimilarity {\n  @Override\n  protected double doComputeResult(int rowA, int rowB, Iterable<Cooccurrence> cooccurrences, double weightOfVectorA,\n      double weightOfVectorB, int numberOfColumns) \n{\n    return countElements(cooccurrences);\n  }\n\n}\none items may be liked by many people, we has system ,one items may be liked by  hundred thousand persons,\nHere doComputeResult just returned the count of elements in  cooccurrences,but It has to iterate for hundred thousand times.\nIf we use List or Array type,we can get the result in one call. because it already sets the size of the Array or list when system constructs the List or Array.\n2)  DistributedLoglikelihoodVectorSimilarity\n3)  DistributedTanimotoCoefficientVectorSimilarity\nI have doing a test using DistributedCooccurrenceVectorSimilarity \nit used 4.5 hours to run RowSimilarityJob-CooccurrencesMapper-SimilarityReducer",
        "Issue Links": [
            "/jira/browse/MAHOUT-460"
        ]
    },
    "MAHOUT-468": {
        "Key": "MAHOUT-468",
        "Summary": "Performance of RowSimilarityJob is not good",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "12/Aug/10 14:07",
        "Updated": "21/May/11 03:22",
        "Resolved": "14/Aug/10 18:35",
        "Description": "I have done a test ,\nPreferences records: 680,194\ndistinct users: 23,246\ndistinct items:437,569 \nSIMILARITY_CLASS_NAME=SIMILARITY_COOCCURRENCE\nmaybePruneItemUserMatrixPath:16.50M\nweights:13.80M\npairwiseSimilarity:18.81G\nJob RowSimilarityJob-RowWeightMapper-WeightedOccurrencesPerColumnReducer:used 32 sec\nJob RowSimilarityJob-CooccurrencesMapper-SimilarityReducer:used 4.30 hours\nI think the reason may be following:\n1) We used SequenceFileOutputFormat,it cause job can only be run by n ( n= Hadoop node counts ) mappers or reducers concurrently.\n2)  We stored redundant info.\nfor example :\nthe output of CooccurrencesMapper: (ItemIndexA,similarity),(ItemIndexA,ItemIndexB,similarity)\n3) Some frequently used code \nhttps://issues.apache.org/jira/browse/MAHOUT-467\n4) allocate many local variable in loop (need confirm )\nIn Class DistributedUncenteredZeroAssumingCosineVectorSimilarity\n  @Override\n  public double weight(Vector v) {\n    double length = 0.0;\n    Iterator<Element> elemIterator = v.iterateNonZero();\n    while (elemIterator.hasNext()) \n{\n\n      double value = elemIterator.next().get();  //this one\n\n      length += value * value;\n\n    }\n    return Math.sqrt(length);\n  }\n5) Maybe we need control the size of cooccurrences",
        "Issue Links": [
            "/jira/browse/MAHOUT-460"
        ]
    },
    "MAHOUT-469": {
        "Key": "MAHOUT-469",
        "Summary": "Need to update and test QR decomposition",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "12/Aug/10 22:08",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "12/Aug/10 22:37",
        "Description": "I need to use QR for explaining the SGD model coefficients.\nWill commit the tests and rewritten QR class shortly.",
        "Issue Links": []
    },
    "MAHOUT-470": {
        "Key": "MAHOUT-470",
        "Summary": "Knock out some checkstyle warnings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "13/Aug/10 00:31",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "13/Aug/10 03:05",
        "Description": "This is just another maintenance patch that addresses some of the nuisance warnings in checkstyle.\nI won't post a patch but rather will commit directly.",
        "Issue Links": []
    },
    "MAHOUT-471": {
        "Key": "MAHOUT-471",
        "Summary": "RowSimilarityJob-Mapper-EntriesToVectorsReducer  failure",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "13/Aug/10 02:23",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "22/Sep/10 07:45",
        "Description": "I used Boolean Data and SIMILARITY_TANIMOTO_COEFFICIENT\njava.io.IOException: Task: attempt_201008101359_0084_r_000000_0 - The reduce copier failed\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:380)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\nCaused by: java.io.IOException: Intermediate merge failed\n\tat org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2576)\n\tat org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.run(ReduceTask.java:2501)\nCaused by: java.lang.RuntimeException: java.io.EOFException\n\tat org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:103)\n\tat org.apache.hadoop.mapred.Merger$MergeQueue.lessThan(Merger.java:373)\n\tat org.apache.hadoop.util.PriorityQueue.upHeap(PriorityQueue.java:123)\n\tat org.apache.hadoop.util.PriorityQueue.put(PriorityQueue.java:50)\n\tat org.apache.hadoop.mapred.Merger$MergeQueue.merge(Merger.java:447)\n\tat org.apache.hadoop.mapred.Merger$MergeQueue.merge(Merger.java:381)\n\tat org.apache.hadoop.mapred.Merger.merge(Merger.java:107)\n\tat org.apache.hadoop.mapred.Merger.merge(Merger.java:93)\n\tat org.apache.hadoop.mapred.ReduceTask$ReduceCopier$InMemFSMergeThread.doInMemMerge(ReduceTask.java:2551)\n\t... 1 more\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readByte(DataInputStream.java:250)\n\tat org.apache.mahout.math.Varint.readUnsignedVarInt(Varint.java:159)\n\tat org.apache.mahout.math.Varint.readSignedVarInt(Varint.java:140)\n\tat org.apache.mahout.math.hadoop.similarity.SimilarityMatrixEntryKey.readFields(SimilarityMatrixEntryKey.java:65)\n\tat org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:97)\n\t... 9 more",
        "Issue Links": []
    },
    "MAHOUT-472": {
        "Key": "MAHOUT-472",
        "Summary": "some of the pom.xml referencing old svn repository url",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Joe Prasanna Kumar",
        "Created": "13/Aug/10 02:58",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "14/Aug/10 00:14",
        "Description": "I realized that the pom.xml in mahout/examples is referencing the old repository url.\nCurrently it is\n <scm>\n    <connection>scm:svn:https://svn.apache.org/repos/asf/lucene/mahout/trunk/examples</connection>\n    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/lucene/mahout/trunk/examples </developerConnection>\n    <url>https://svn.apache.org/repos/asf/lucene/mahout/mahout-examples</url>\n  </scm>\nand I guess it should be changed to\n <scm>\n    <connection>scm:svn:https://svn.apache.org/repos/asf/mahout/trunk/examples</connection>\n    <developerConnection>scm:svn:https://svn.apache.org/repos/asf/mahout/trunk/examples </developerConnection>\n    <url>https://svn.apache.org/repos/asf/mahout/mahout-examples</url>\n  </scm>\nI saw similar references in \nmahout/utils/pom.xml\nmahout/taste-web/pom.xml\nmahout/math/pom.xml\nmahout/core/pom.xml\nmahout/buildtools/pom.xml\nTried searching in http://www.lucidimagination.com/search/ to see if there were previous discussion about this and couldnt find any.",
        "Issue Links": []
    },
    "MAHOUT-473": {
        "Key": "MAHOUT-473",
        "Summary": "add parameter -Dmapred.reduce.tasks when call job RowSimilarityJob in RecommenderJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "13/Aug/10 05:07",
        "Updated": "21/May/11 03:22",
        "Resolved": "13/Aug/10 11:13",
        "Description": "In RecommenderJob\nRecommenderJob.java\n    int numberOfUsers = TasteHadoopUtils.readIntFromFile(getConf(), countUsersPath);\n\n    if (shouldRunNextPhase(parsedArgs, currentPhase)) {\n      /* Once DistributedRowMatrix uses the hadoop 0.20 API, we should refactor this call to something like\n       * new DistributedRowMatrix(...).rowSimilarity(...) */\n      try {\n        RowSimilarityJob.main(new String[] { \"-Dmapred.input.dir=\" + maybePruneItemUserMatrixPath.toString(),\n            \"-Dmapred.output.dir=\" + similarityMatrixPath.toString(), \"--numberOfColumns\",\n            String.valueOf(numberOfUsers), \"--similarityClassname\", similarityClassname, \"--maxSimilaritiesPerRow\",\n            String.valueOf(maxSimilaritiesPerItemConsidered + 1), \"--tempDir\", tempDirPath.toString() });\n      } catch (Exception e) {\n        throw new IllegalStateException(\"item-item-similarity computation failed\", e);\n      }\n    }\n\n\nWe have not passed parameter -Dmapred.reduce.tasks when job RowSimilarityJob.\nIt caused all three  RowSimilarityJob sub-jobs run using 1 map and 1 reduce, so the sub jobs can not be scalable.",
        "Issue Links": []
    },
    "MAHOUT-474": {
        "Key": "MAHOUT-474",
        "Summary": "Should compress output of Job pairwiseSimilarity and Job asMatrix",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "13/Aug/10 05:25",
        "Updated": "21/May/11 03:22",
        "Resolved": "13/Aug/10 11:15",
        "Description": "From above picture ,we can see that the output of pairwiseSimilarity is very large ,we should compress them.\n      SequenceFileOutputFormat.setOutputCompressionType(job, style);\n      SequenceFileOutputFormat.setCompressOutput(job, compress);\n      SequenceFileOutputFormat.setOutputCompressorClass(job, codecClass)",
        "Issue Links": []
    },
    "MAHOUT-475": {
        "Key": "MAHOUT-475",
        "Summary": "Replace Mapper with  MultithreadedMapper  to run job pairwiseSimilarity",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "13/Aug/10 06:08",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "13/Aug/10 11:10",
        "Description": "Because CooccurrencesMapper has huge computing,\nMaybe we can replace  Mapper with  MultithreadedMapper.\nAnd call the mapper\noriginal:\n\n    if (shouldRunNextPhase(parsedArgs, currentPhase)) {\n      Job pairwiseSimilarity = prepareJob(weightsPath,\n                               pairwiseSimilarityPath,\n                               SequenceFileInputFormat.class,\n                               CooccurrencesMapper.class,\n                               WeightedRowPair.class,\n                               Cooccurrence.class,\n                               SimilarityReducer.class,\n                               SimilarityMatrixEntryKey.class,\n                               MatrixEntryWritable.class,\n                               SequenceFileOutputFormat.class);\n\n      Configuration pairwiseConf = pairwiseSimilarity.getConfiguration();\n      pairwiseConf.set(DISTRIBUTED_SIMILARITY_CLASSNAME, distributedSimilarityClassname);\n      pairwiseConf.setInt(NUMBER_OF_COLUMNS, numberOfColumns);\n      pairwiseSimilarity.waitForCompletion(true);\n    }\n\n\nnew:\n\n    if (shouldRunNextPhase(parsedArgs, currentPhase)) {\n      Job pairwiseSimilarity = prepareJob(weightsPath,\n                               pairwiseSimilarityPath,\n                               SequenceFileInputFormat.class,\n                               CooccurrencesMapper.class,\n                               WeightedRowPair.class,\n                               Cooccurrence.class,\n                               SimilarityReducer.class,\n                               SimilarityMatrixEntryKey.class,\n                               MatrixEntryWritable.class,\n                               SequenceFileOutputFormat.class);\n\n      \n      Configuration pairwiseConf = pairwiseSimilarity.getConfiguration();\n      pairwiseConf.set(DISTRIBUTED_SIMILARITY_CLASSNAME, distributedSimilarityClassname);\n      pairwiseConf.setInt(NUMBER_OF_COLUMNS, numberOfColumns);\n      MultithreadedMapper.setMapperClass(pairwiseSimilarity, CooccurrencesMapper.class);\n      MultithreadedMapper.setNumberOfThreads(pairwiseSimilarity, numMapThreads);\n      SequenceFileOutputFormat.setCompressOutput(pairwiseSimilarity, true);\n      SequenceFileOutputFormat.setOutputCompressorClass(pairwiseSimilarity, GzipCodec.class);\n      SequenceFileOutputFormat.setOutputCompressionType(pairwiseSimilarity, CompressionType.BLOCK);\n\n      pairwiseSimilarity.waitForCompletion(true);\n    }",
        "Issue Links": []
    },
    "MAHOUT-476": {
        "Key": "MAHOUT-476",
        "Summary": "bug when running org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorDriver on hadoop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.3",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "leon lee",
        "Created": "13/Aug/10 06:50",
        "Updated": "21/May/11 03:22",
        "Resolved": "20/Aug/10 06:24",
        "Description": "when I follow wiki instruction: https://cwiki.apache.org/MAHOUT/wikipedia-bayes-example.html \n(by the way, the bayes examples document in wiki  need update to 0.3 )\nto run step 5:\nCreate the countries based Split of wikipedia dataset. \nI use the following command:\n$HADOOP_HOME/bin/hadoop jar $MAHOUT_HOME/examples/target/mahout-examples-0.3.job  org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorDriver -i $MAHOUT_HOME/examples/work/wikipedia/chunks -o $MAHOUT_HOME/examples/work/wikipediainput  -c  $MAHOUT_HOME/examples/src/test/resources/country.txt\nand failed on hadoop.\nsee hadoop log, it hint:\nError: org.apache.lucene.wikipedia.analysis.WikipediaTokenizer.addAttribute(Ljava/lang/Class;)Lorg/apache/lucene/util/Attribute",
        "Issue Links": []
    },
    "MAHOUT-477": {
        "Key": "MAHOUT-477",
        "Summary": "SimilarityMatrixEntryKeyPartitioner sometimes produces illegal partition numbers",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "13/Aug/10 13:16",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "13/Aug/10 19:34",
        "Description": "My job failed on EMR today with the exception below. I think this happens only when when there are negative row/column indices in a matrix given to RowSimilarityJob, still  have to figure out the details.\njava.io.IOException: Illegal partition for org.apache.mahout.math.hadoop.similarity.SimilarityMatrixEntryKey@3532b21 (-3)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:921)\n\tat org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:549)\n\tat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n\tat org.apache.hadoop.mapreduce.Mapper.map(Mapper.java:124)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:629)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:310)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)",
        "Issue Links": []
    },
    "MAHOUT-478": {
        "Key": "MAHOUT-478",
        "Summary": "Do we need  normalize SimilarityMatrixEntryKey?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "13/Aug/10 13:32",
        "Updated": "21/May/11 03:22",
        "Resolved": "13/Aug/10 19:16",
        "Description": "In org.apache.mahout.math.hadoop.similarity.SimilarityMatrixEntryKey\n\npublic static class SimilarityMatrixEntryKeyComparator extends WritableComparator {\n\n    protected SimilarityMatrixEntryKeyComparator() {\n      super(SimilarityMatrixEntryKey.class, true);\n    }\n\n    @Override\n    public int compare(WritableComparable a, WritableComparable b) {\n      SimilarityMatrixEntryKey key1 = (SimilarityMatrixEntryKey) a;\n      SimilarityMatrixEntryKey key2 = (SimilarityMatrixEntryKey) b;\n\n      int result = compare(key1.row, key2.row);\n      if (result == 0) {\n        result = -1 * compare(key1.value, key2.value);\n      }\n      return result;\n    }\n\n    protected static int compare(long a, long b) {\n      return (a == b) ? 0 : (a < b) ? -1 : 1;\n    }\n\n    protected static int compare(double a, double b) {\n      return (a == b) ? 0 : (a < b) ? -1 : 1;\n    }\n  }\n\n\nWe used double as one part of the key, \nbecause of double has many possible value ,it will cause pairwiseSimilarity may has may group,\nthe count of group also is out of our control.\nfor example (ItemA ,0.1),(ItemA ,0.11),(ItemA ,0.01),(ItemA ,0.1),(ItemA ,0.001),(ItemA ,0.0011) is different group.\nAlso double is inaccurate,it hard to compare the equal of double .\nSo can we normalize the similarityValue ?\nmultiply all similarityValue  with 100,1000 ,or other numer,and make it to a integer.\nIf necessary we can transform them to double in the end.",
        "Issue Links": []
    },
    "MAHOUT-479": {
        "Key": "MAHOUT-479",
        "Summary": "Streamline classification/ clustering data structures",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1,                                            0.2,                                            0.3,                                            0.4",
        "Fix Version/s": "0.6",
        "Component/s": "Classification,                                            Clustering",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "13/Aug/10 14:52",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "15/Oct/11 10:21",
        "Description": "Opening this JIRA issue to collect ideas on how to streamline our classification and clustering algorithms to make integration for users easier as per mailing list thread http://markmail.org/message/pnzvrqpv5226twfs\n\nJake and Robin and I were talking the other evening and a common lament was that our classification (and clustering) stuff was all over the map in terms of data structures.  Driving that to rest and getting those comments even vaguely as plug and play as our much more advanced recommendation components would be very, very helpful.\nThis issue probably also realates to MAHOUT-287 (intention there is to make naive bayes run on vectors as input).\nTed, Jake, Robin: Would be great if someone of you could add a comment on some of the issues you discussed \"the other evening\" and (if applicable) any minor or major changes you think could help solve this issue.",
        "Issue Links": [
            "/jira/browse/MAHOUT-228"
        ]
    },
    "MAHOUT-480": {
        "Key": "MAHOUT-480",
        "Summary": "Replace manual precondition checking with Precondition utility class from Guava",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Eugen Paraschiv",
        "Created": "13/Aug/10 14:54",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "03/Oct/10 20:53",
        "Description": "Replace manual precondition checking with the Precondition class. This will affect the following classes: \nFileDataModel\nMemoryDiffStorage\nSlopeOneRecommender\nFileDiffStorage\nAbstractJDBCDiffStorage",
        "Issue Links": []
    },
    "MAHOUT-481": {
        "Key": "MAHOUT-481",
        "Summary": "Option --sizeOnly required in VectorDumper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "16/Aug/10 15:05",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "16/Aug/10 15:20",
        "Description": "$ bin/mahout vectordump  -s /home/ssc/Desktop/hui/maybePruneItemUserMatrixPath/_steer_item_dogear_temp_maybePruneItemUserMatrixPath_part-r-0000\nno HADOOP_CONF_DIR or HADOOP_HOME set, running locally\n16.08.2010 17:02:38 org.slf4j.impl.JCLLoggerAdapter error\nSCHWERWIEGEND: Exception\norg.apache.commons.cli2.OptionException: Missing required option --sizeOnly",
        "Issue Links": []
    },
    "MAHOUT-482": {
        "Key": "MAHOUT-482",
        "Summary": "Defaulting $HADOOP_CONF_DIR to $HADOOP_HOME/conf",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Joe Prasanna Kumar",
        "Created": "18/Aug/10 01:11",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "18/Aug/10 13:21",
        "Description": "Hi all,\nwhile trying to understand the clusterdump utility, I encountered a message \"no HADOOP_CONF_DIR or HADOOP_HOME set, running locally\". As per some of the discussions in Apr'10 (http://lucene.472066.n3.nabble.com/Dealing-with-kmean-and-meanshift-output-td708824.html#a709066), it would be good to default the hadoop conf directory to hadoop_home/conf. \nSo I've changed the script to accommodate this.\nExisting code:\nif [ \"$HADOOP_CONF_DIR\" = \"\" ] || [ \"$HADOOP_HOME\" = \"\" ] || [ \"$MAHOUT_LOCAL\" != \"\" ] ; then\n  if [ \"$HADOOP_CONF_DIR\" = \"\" ] || [ \"$HADOOP_HOME\" = \"\" ] ; then\n    echo \"no HADOOP_CONF_DIR or HADOOP_HOME set, running locally\"\n  elif [ \"$MAHOUT_LOCAL\" != \"\" ] ; then \n    echo \"MAHOUT_LOCAL is set, running locally\"\n  fi\n  exec \"$JAVA\" $JAVA_HEAP_MAX $MAHOUT_OPTS -classpath \"$CLASSPATH\" $CLASS \"$@\"\nelse\n  echo \"running on hadoop, using HADOOP_HOME=$HADOOP_HOME and HADOOP_CONF_DIR=$HADOOP_CONF_DIR\"\nNew code:\nif [ \"$HADOOP_HOME\" = \"\" ] || [ \"$MAHOUT_LOCAL\" != \"\" ] ; then\n  if [ \"$HADOOP_HOME\" = \"\" ] ; then\n    echo \"no HADOOP_HOME set, running locally\"\n  elif [ \"$MAHOUT_LOCAL\" != \"\" ] ; then \n    echo \"MAHOUT_LOCAL is set, running locally\"\n  fi\n  exec \"$JAVA\" $JAVA_HEAP_MAX $MAHOUT_OPTS -classpath \"$CLASSPATH\" $CLASS \"$@\"\nelse\n  if [ \"$HADOOP_CONF_DIR\" = \"\" ] ; then\n    HADOOP_CONF_DIR=$HADOOP_HOME/conf\n    echo \"no HADOOP_CONF_DIR set. so defaulting HADOOP_CONF_DIR to $HADOOP_HOME/conf \"\n  fi\necho \"running on hadoop, using HADOOP_HOME=$HADOOP_HOME and HADOOP_CONF_DIR=$HADOOP_CONF_DIR\"\nIn my local machine, I've just set HADOOP_HOME and the script defaults the $HADOOP_CONF_DIR directory and runs on hadoop.\nIf this issue / fix looks valid, I'll open a JIRA issue and attach the patch.\nAppreciate your thoughts / feedbacks,\nJoe.\nDrew Farris to dev\nshow details 8:53 AM (12 hours ago)\nOn Tue, Aug 17, 2010 at 12:26 AM, Joe Kumar <joekumar@gmail.com> wrote:\n> Hi all,\n>\n> while trying to understand the clusterdump utility, I encountered a message\n> \"no HADOOP_CONF_DIR or HADOOP_HOME set, running locally\". As per some of the\n> discussions in Apr'10 (\n> http://lucene.472066.n3.nabble.com/Dealing-with-kmean-and-meanshift-output-td708824.html#a709066),\n> it would be good to default the hadoop conf directory to hadoop_home/conf.\nAgreed, and your solution looks good. I'll keep an eye out for the patch.\nJake Mannix to dev\nshow details 3:46 PM (5 hours ago)\n+1\nGood idea.  I thought we already did this...\n -jake",
        "Issue Links": []
    },
    "MAHOUT-483": {
        "Key": "MAHOUT-483",
        "Summary": "Job RowSimilarityJob-Mapper-EntriesToVectorsReducer improvement",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "18/Aug/10 15:58",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "22/Sep/10 15:05",
        "Description": "the Mapper phase of Job RowSimilarityJob-Mapper-EntriesToVectorsReducer uses too long time but do nothing actually.\nCan we move the EntriesToVectorsReducer task to Mapper phase ? set the Reducer phase is null,\nSo can improve the performance.",
        "Issue Links": []
    },
    "MAHOUT-484": {
        "Key": "MAHOUT-484",
        "Summary": "The RecommenderJob exit ,some sub-jobs can not be run.",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "18/Aug/10 16:52",
        "Updated": "21/May/11 03:22",
        "Resolved": "18/Aug/10 16:57",
        "Description": "I have done a few test today, \nThe RecommenderJob exit in middle.\nThe first time it exited when it finished RowSimilarityJob-CooccurrencesMapper-SimilarityReducer\nthe second time it exited when it finished RECOMMENDATION_dogear_bookmark-Mapper-EntriesToVectorsReducer",
        "Issue Links": []
    },
    "MAHOUT-485": {
        "Key": "MAHOUT-485",
        "Summary": "NullPointerException when using GenericItemBasedRecommender with FileDataModel",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "20/Aug/10 15:28",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "22/Aug/10 22:26",
        "Description": "When GenericItemBasedRecommender is instantiated it calls getMinPreference on DataModel to see if it has to build a capper. When used with a FileDataModel this results in a NullPointerException as FileDataModel tries to delegate this call to its not yet existing delegate (which is a GenericDataModel that is yet to be loaded)",
        "Issue Links": []
    },
    "MAHOUT-486": {
        "Key": "MAHOUT-486",
        "Summary": "Null Pointer Exception running DictionaryVectorizer with ngram=2 on Reuters dataset",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Integration",
        "Assignee": "Drew Farris",
        "Reporter": "Robin Anil",
        "Created": "23/Aug/10 11:45",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "23/Aug/10 14:12",
        "Description": "java.io.IOException: Spill failed\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:860)\n\tat org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:541)\n\tat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n\tat org.apache.mahout.utils.nlp.collocations.llr.CollocMapper$1.apply(CollocMapper.java:127)\n\tat org.apache.mahout.utils.nlp.collocations.llr.CollocMapper$1.apply(CollocMapper.java:114)\n\tat org.apache.mahout.math.map.OpenObjectIntHashMap.forEachPair(OpenObjectIntHashMap.java:186)\n\tat org.apache.mahout.utils.nlp.collocations.llr.CollocMapper.map(CollocMapper.java:114)\n\tat org.apache.mahout.utils.nlp.collocations.llr.CollocMapper.map(CollocMapper.java:41)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)\nCaused by: java.lang.NullPointerException\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:86)\n\tat java.io.DataOutputStream.write(DataOutputStream.java:90)\n\tat org.apache.mahout.utils.nlp.collocations.llr.Gram.write(Gram.java:181)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)\n\tat org.apache.hadoop.mapred.IFile$Writer.append(IFile.java:179)\n\tat org.apache.hadoop.mapred.Task$CombineOutputCollector.collect(Task.java:880)\n\tat org.apache.hadoop.mapred.Task$NewCombinerRunner$OutputConverter.write(Task.java:1201)\n\tat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n\tat org.apache.mahout.utils.nlp.collocations.llr.CollocCombiner.reduce(CollocCombiner.java:40)\n\tat org.apache.mahout.utils.nlp.collocations.llr.CollocCombiner.reduce(CollocCombiner.java:25)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)\n\tat org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1222)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1265)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:686)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1173)",
        "Issue Links": []
    },
    "MAHOUT-487": {
        "Key": "MAHOUT-487",
        "Summary": "Issues with memory use and inconsistent or state-influenced results when using CBayesAlgorithm",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.3,                                            0.4,                                            0.5",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Drew Farris",
        "Created": "24/Aug/10 12:56",
        "Updated": "20/Nov/12 17:49",
        "Resolved": "21/Aug/11 19:57",
        "Description": "Came across this digging through the mailing list archives for something else, probably worth tracking as an issue.\n\nDuring classification, every word still unknown is added to \nfeatureDictionary. This leads to the excessive growth if lots of texts \nwith unknown words are to be classified. The inconsistency is caused by \nusing a \"vocabCount\" that is not reset after each classification. \nIndeed, featureDictionary.size() is used for \"vocabCount\", which \nincreases every time new unknown words are discovered.\nSee: http://www.lucidimagination.com/search/document/7dabe3efec8d136d/issues_with_memory_use_and_inconsistent_or_state_influenced_results_when_using_cbayesalgorit#8853165db260bf75\nAlternately per Robin:\n\nWe can remove the addition features to the\ndictionary altogether. Will yield better performance, and lock down the\nmodel. Will require a bit more modification",
        "Issue Links": []
    },
    "MAHOUT-488": {
        "Key": "MAHOUT-488",
        "Summary": "cannot run BayesFileFormatter from the command line",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Mathieu Sauve-Frankel",
        "Created": "27/Aug/10 05:48",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "27/Aug/10 17:56",
        "Description": "the --help option is listed as mandatory making it impossible to run org.apache.mahout.classifier.BayesFileFormatter from the command line",
        "Issue Links": []
    },
    "MAHOUT-489": {
        "Key": "MAHOUT-489",
        "Summary": "SequentialAccessSparseVector iterators skip items",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Laszlo Dosa",
        "Created": "27/Aug/10 14:34",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "24/Sep/10 16:48",
        "Description": "The following test cases fail, which iterates through the elements of the SequentialAccessSparseVector, because not every value is given back.\nSequentialAccessSparseVector.iterator() skips the last element, while the SequentialAccessSparseVector.iterateNonZero() skips the first element.\nint [] index = new int[] \n{ 0, 1, 2, 3, 4, 5 };\nint [] values = new int[] { 0, 1, 2, 3, 4, 5 }\n;\nSequentialAccessSparseVector vector;\n@Before\npublic void setUp() {\n\tvector = new SequentialAccessSparseVector(6);\n\tfor(int i = 0; i < Math.min(index.length, values.length); i++ ) \n{\n\t\tvector.set(index[i], values[i]);\n\t}\n}\n@Test\npublic void testIteratorAll() {\n\tint elements = 0;\n\tIterator<Element> it = vector.iterator();\n\twhile (it.hasNext()) \n{\n\t\tSystem.out.println(it.next().get());\n\t\telements++;\n\t}\n\tassertEquals((int)vector.get(Math.min(index.length,values.length)-1),values[Math.min(index.length, values.length)-1]);\n\tassertEquals(Math.min(index.length, values.length),elements);\n}\n\n@Test\npublic void testIteratorNonNull() {\n\tint elements = 0;\n\tIterator<Element> it = vector.iterateNonZero();\n\twhile (it.hasNext()) {\t\tSystem.out.println(it.next().get());\t\telements++;\t}\n\tassertEquals((int)vector.get(Math.min(index.length,values.length)-1),values[Math.min(index.length, values.length)-1]);\n\tassertEquals(Math.min(index.length,values.length),elements);  \n}",
        "Issue Links": []
    },
    "MAHOUT-490": {
        "Key": "MAHOUT-490",
        "Summary": "unable to find resource",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zhen Guo",
        "Created": "28/Aug/10 01:50",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "24/Sep/10 12:01",
        "Description": "I use the latested code checked out from trunk and there are  the following error when I build it using \"mvn clean install\"\n[INFO] [clean:clean \n{execution: default-clean}\n]\n[INFO] Deleting file set: /teoma/dx-9d31y00/ccenterq/zguo/mahout/trunk/core/target (included: [**], excluded: [])\n[INFO] [resources:resources \n{execution: default-resources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 0 resource\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/hadoop/hadoop-core/0.20.2/hadoop-core-0.20.2.pom\n[INFO] Unable to find resource 'org.apache.hadoop:hadoop-core:pom:0.20.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.20.2/hadoop-core-0.20.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-cli/commons-cli/1.2/commons-cli-1.2.pom\n[INFO] Unable to find resource 'commons-cli:commons-cli:pom:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/commons/commons-parent/11/commons-parent-11.pom\n[INFO] Unable to find resource 'org.apache.commons:commons-parent:pom:11' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/commons/commons-parent/11/commons-parent-11.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.pom\n[INFO] Unable to find resource 'commons-httpclient:commons-httpclient:pom:3.0.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-codec/commons-codec/1.2/commons-codec-1.2.pom\n[INFO] Unable to find resource 'commons-codec:commons-codec:pom:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-codec/commons-codec/1.2/commons-codec-1.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-codec/commons-codec/1.3/commons-codec-1.3.pom\n[INFO] Unable to find resource 'commons-codec:commons-codec:pom:1.3' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-codec/commons-codec/1.3/commons-codec-1.3.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/mahout/hbase/hbase/0.20.0/hbase-0.20.0.pom\n[INFO] Unable to find resource 'org.apache.mahout.hbase:hbase:pom:0.20.0' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/mahout/hbase/hbase/0.20.0/hbase-0.20.0.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-dbcp/commons-dbcp/1.2.2/commons-dbcp-1.2.2.pom\n[INFO] Unable to find resource 'commons-dbcp:commons-dbcp:pom:1.2.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-dbcp/commons-dbcp/1.2.2/commons-dbcp-1.2.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-pool/commons-pool/1.3/commons-pool-1.3.pom\n[INFO] Unable to find resource 'commons-pool:commons-pool:pom:1.3' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-pool/commons-pool/1.3/commons-pool-1.3.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-pool/commons-pool/1.4/commons-pool-1.4.pom\n[INFO] Unable to find resource 'commons-pool:commons-pool:pom:1.4' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-pool/commons-pool/1.4/commons-pool-1.4.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-lang/commons-lang/2.4/commons-lang-2.4.pom\n[INFO] Unable to find resource 'commons-lang:commons-lang:pom:2.4' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-lang/commons-lang/2.4/commons-lang-2.4.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/commons/commons-parent/9/commons-parent-9.pom\n[INFO] Unable to find resource 'org.apache.commons:commons-parent:pom:9' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/commons/commons-parent/9/commons-parent-9.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/uncommons/watchmaker/watchmaker-framework/0.6.2/watchmaker-framework-0.6.2.pom\n[INFO] Unable to find resource 'org.uncommons.watchmaker:watchmaker-framework:pom:0.6.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/uncommons/watchmaker/watchmaker-framework/0.6.2/watchmaker-framework-0.6.2.pom\n[INFO] Unable to find resource 'org.uncommons.watchmaker:watchmaker-framework:pom:0.6.2' in repository maven2-repository.maven.org (http://repo1.maven.org/maven2)\nDownloading: http://download.java.net/maven/2/org/uncommons/watchmaker/watchmaker-framework/0.6.2/watchmaker-framework-0.6.2.pom\n1K downloaded  (watchmaker-framework-0.6.2.pom)\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/uncommons/maths/uncommons-maths/1.2/uncommons-maths-1.2.pom\n[INFO] Unable to find resource 'org.uncommons.maths:uncommons-maths:pom:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/uncommons/maths/uncommons-maths/1.2/uncommons-maths-1.2.pom\n[INFO] Unable to find resource 'org.uncommons.maths:uncommons-maths:pom:1.2' in repository maven2-repository.maven.org (http://repo1.maven.org/maven2)\nDownloading: http://download.java.net/maven/2/org/uncommons/maths/uncommons-maths/1.2/uncommons-maths-1.2.pom\n726b downloaded  (uncommons-maths-1.2.pom)\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/com/google/collections/google-collections/1.0-rc2/google-collections-1.0-rc2.pom\n[INFO] Unable to find resource 'com.google.collections:google-collections:pom:1.0-rc2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/com/google/collections/google-collections/1.0-rc2/google-collections-1.0-rc2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/com/google/google/1/google-1.pom\n[INFO] Unable to find resource 'com.google:google:pom:1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/com/google/google/1/google-1.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/com/thoughtworks/xstream/xstream/1.3.1/xstream-1.3.1.pom\n[INFO] Unable to find resource 'com.thoughtworks.xstream:xstream:pom:1.3.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/com/thoughtworks/xstream/xstream/1.3.1/xstream-1.3.1.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/com/thoughtworks/xstream/xstream-parent/1.3.1/xstream-parent-1.3.1.pom\n[INFO] Unable to find resource 'com.thoughtworks.xstream:xstream-parent:pom:1.3.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/com/thoughtworks/xstream/xstream-parent/1.3.1/xstream-parent-1.3.1.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.pom\n[INFO] Unable to find resource 'xpp3:xpp3_min:pom:1.1.4c' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/lucene/lucene-core/3.0.1/lucene-core-3.0.1.pom\n[INFO] Unable to find resource 'org.apache.lucene:lucene-core:pom:3.0.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/3.0.1/lucene-core-3.0.1.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/lucene/lucene-parent/3.0.1/lucene-parent-3.0.1.pom\n[INFO] Unable to find resource 'org.apache.lucene:lucene-parent:pom:3.0.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-parent/3.0.1/lucene-parent-3.0.1.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/lucene/lucene-analyzers/3.0.1/lucene-analyzers-3.0.1.pom\n[INFO] Unable to find resource 'org.apache.lucene:lucene-analyzers:pom:3.0.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-analyzers/3.0.1/lucene-analyzers-3.0.1.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/lucene/lucene-contrib/3.0.1/lucene-contrib-3.0.1.pom\n[INFO] Unable to find resource 'org.apache.lucene:lucene-contrib:pom:3.0.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-contrib/3.0.1/lucene-contrib-3.0.1.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/mahout/commons/commons-cli/2.0-mahout/commons-cli-2.0-mahout.pom\n[INFO] Unable to find resource 'org.apache.mahout.commons:commons-cli:pom:2.0-mahout' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/mahout/commons/commons-cli/2.0-mahout/commons-cli-2.0-mahout.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-math/commons-math/1.2/commons-math-1.2.pom\n[INFO] Unable to find resource 'commons-math:commons-math:pom:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-math/commons-math/1.2/commons-math-1.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/easymock/easymock/2.5.2/easymock-2.5.2.pom\n[INFO] Unable to find resource 'org.easymock:easymock:pom:2.5.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/easymock/easymock/2.5.2/easymock-2.5.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/easymock/easymockclassextension/2.5.2/easymockclassextension-2.5.2.pom\n[INFO] Unable to find resource 'org.easymock:easymockclassextension:pom:2.5.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/easymock/easymockclassextension/2.5.2/easymockclassextension-2.5.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/cglib/cglib-nodep/2.2/cglib-nodep-2.2.pom\n[INFO] Unable to find resource 'cglib:cglib-nodep:pom:2.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/cglib/cglib-nodep/2.2/cglib-nodep-2.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/objenesis/objenesis/1.2/objenesis-1.2.pom\n[INFO] Unable to find resource 'org.objenesis:objenesis:pom:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/objenesis/objenesis/1.2/objenesis-1.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/objenesis/objenesis-parent/1.2/objenesis-parent-1.2.pom\n[INFO] Unable to find resource 'org.objenesis:objenesis-parent:pom:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/objenesis/objenesis-parent/1.2/objenesis-parent-1.2.pom\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-math/commons-math/1.2/commons-math-1.2.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/uncommons/watchmaker/watchmaker-framework/0.6.2/watchmaker-framework-0.6.2.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/lucene/lucene-core/3.0.1/lucene-core-3.0.1.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-dbcp/commons-dbcp/1.2.2/commons-dbcp-1.2.2.jar\n[INFO] Unable to find resource 'org.uncommons.watchmaker:watchmaker-framework:jar:0.6.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\n[INFO] Unable to find resource 'commons-math:commons-math:jar:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-math/commons-math/1.2/commons-math-1.2.jar\nDownloading: http://repo1.maven.org/maven2/org/uncommons/watchmaker/watchmaker-framework/0.6.2/watchmaker-framework-0.6.2.jar\n[INFO] Unable to find resource 'commons-httpclient:commons-httpclient:jar:3.0.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-httpclient/commons-httpclient/3.0.1/commons-httpclient-3.0.1.jar\n[INFO] Unable to find resource 'commons-dbcp:commons-dbcp:jar:1.2.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-dbcp/commons-dbcp/1.2.2/commons-dbcp-1.2.2.jar\n[INFO] Unable to find resource 'org.apache.lucene:lucene-core:jar:3.0.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-core/3.0.1/lucene-core-3.0.1.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/com/thoughtworks/xstream/xstream/1.3.1/xstream-1.3.1.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/uncommons/maths/uncommons-maths/1.2/uncommons-maths-1.2.jar\n[INFO] Unable to find resource 'com.thoughtworks.xstream:xstream:jar:1.3.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/com/thoughtworks/xstream/xstream/1.3.1/xstream-1.3.1.jar\n[INFO] Unable to find resource 'org.uncommons.maths:uncommons-maths:jar:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/uncommons/maths/uncommons-maths/1.2/uncommons-maths-1.2.jar\n[INFO] Unable to find resource 'org.uncommons.watchmaker:watchmaker-framework:jar:0.6.2' in repository maven2-repository.maven.org (http://repo1.maven.org/maven2)\nDownloading: http://download.java.net/maven/2/org/uncommons/watchmaker/watchmaker-framework/0.6.2/watchmaker-framework-0.6.2.jar\n74K downloaded  (watchmaker-framework-0.6.2.jar)\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-cli/commons-cli/1.2/commons-cli-1.2.jar\n[INFO] Unable to find resource 'xpp3:xpp3_min:jar:1.1.4c' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/xpp3/xpp3_min/1.1.4c/xpp3_min-1.1.4c.jar\n[INFO] Unable to find resource 'org.uncommons.maths:uncommons-maths:jar:1.2' in repository maven2-repository.maven.org (http://repo1.maven.org/maven2)\nDownloading: http://download.java.net/maven/2/org/uncommons/maths/uncommons-maths/1.2/uncommons-maths-1.2.jar\n[INFO] Unable to find resource 'commons-cli:commons-cli:jar:1.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-cli/commons-cli/1.2/commons-cli-1.2.jar\n46K downloaded  (uncommons-maths-1.2.jar)\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/hadoop/hadoop-core/0.20.2/hadoop-core-0.20.2.jar\n[INFO] Unable to find resource 'org.apache.hadoop:hadoop-core:jar:0.20.2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-core/0.20.2/hadoop-core-0.20.2.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-pool/commons-pool/1.4/commons-pool-1.4.jar\n[INFO] Unable to find resource 'commons-pool:commons-pool:jar:1.4' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-pool/commons-pool/1.4/commons-pool-1.4.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-lang/commons-lang/2.4/commons-lang-2.4.jar\n[INFO] Unable to find resource 'commons-lang:commons-lang:jar:2.4' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-lang/commons-lang/2.4/commons-lang-2.4.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/commons-codec/commons-codec/1.3/commons-codec-1.3.jar\n[INFO] Unable to find resource 'commons-codec:commons-codec:jar:1.3' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/commons-codec/commons-codec/1.3/commons-codec-1.3.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/com/google/collections/google-collections/1.0-rc2/google-collections-1.0-rc2.jar\n[INFO] Unable to find resource 'com.google.collections:google-collections:jar:1.0-rc2' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/com/google/collections/google-collections/1.0-rc2/google-collections-1.0-rc2.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/mahout/hbase/hbase/0.20.0/hbase-0.20.0.jar\n[INFO] Unable to find resource 'org.apache.mahout.hbase:hbase:jar:0.20.0' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/mahout/hbase/hbase/0.20.0/hbase-0.20.0.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/mahout/commons/commons-cli/2.0-mahout/commons-cli-2.0-mahout.jar\n[INFO] Unable to find resource 'org.apache.mahout.commons:commons-cli:jar:2.0-mahout' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/mahout/commons/commons-cli/2.0-mahout/commons-cli-2.0-mahout.jar\nDownloading: http://google-gson.googlecode.com/svn/mavenrepo/org/apache/lucene/lucene-analyzers/3.0.1/lucene-analyzers-3.0.1.jar\n[INFO] Unable to find resource 'org.apache.lucene:lucene-analyzers:jar:3.0.1' in repository gson (http://google-gson.googlecode.com/svn/mavenrepo)\nDownloading: http://repo1.maven.org/maven2/org/apache/lucene/lucene-analyzers/3.0.1/lucene-analyzers-3.0.1.jar\nAlso there are two failed test\nRunning org.apache.mahout.math.hadoop.TestDistributedRowMatrix\nTests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 8.046 sec <<< FAILURE!\nRunning org.apache.mahout.ep.ThreadedEvolutionaryProcessTest\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.239 sec <<< FAILURE!\nI also tried the release 0.3 and the \"unable to find resource\" error also appears.\nI use the hadoop 0.20.1.",
        "Issue Links": []
    },
    "MAHOUT-491": {
        "Key": "MAHOUT-491",
        "Summary": "AdaptiveLogisticRegression test is not determinstic",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "29/Aug/10 05:13",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "29/Aug/10 05:17",
        "Description": "The problem is that OnlineAuc doesn't allow injection of a PRNG.",
        "Issue Links": []
    },
    "MAHOUT-492": {
        "Key": "MAHOUT-492",
        "Summary": "Need better handling of interaction variables in *ValueEncoder",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "30/Aug/10 19:23",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "30/Aug/10 19:35",
        "Description": "We need to be able to have generic interactions between anything and anything.  Text and something is the tricky case.\nCommit and tests coming shortly.",
        "Issue Links": []
    },
    "MAHOUT-493": {
        "Key": "MAHOUT-493",
        "Summary": "Explicit filter for user/item pairs in o.a.m.cf.taste.hadoop.item.RecommenderJob",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "31/Aug/10 07:05",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "08/Sep/10 05:47",
        "Description": "It should be possible to explicitly prevent having a given item being recommended to a given user.\nThis filter is necessary because that user might have stated to never be shown a recommended item again.",
        "Issue Links": []
    },
    "MAHOUT-494": {
        "Key": "MAHOUT-494",
        "Summary": "Need to serialize and restore SGD models",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "01/Sep/10 00:14",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "30/Sep/10 07:32",
        "Description": "This will be a GSON based serialization of OnlineLogisticRegression, CrossFoldLearner and AdaptiveLogisticRegression and all related classes.",
        "Issue Links": []
    },
    "MAHOUT-495": {
        "Key": "MAHOUT-495",
        "Summary": "Undeprecate Normal and Exponential distributions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "01/Sep/10 02:16",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "21/Sep/10 17:48",
        "Description": "I am putting together tests for these distributions as we speak.",
        "Issue Links": []
    },
    "MAHOUT-496": {
        "Key": "MAHOUT-496",
        "Summary": "Quickstart scripts go out of date",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "05/Sep/10 22:56",
        "Updated": "21/May/11 03:22",
        "Resolved": "21/May/11 03:13",
        "Description": "The various quickstart instructions and scripts go out of date quickly. Approaching the Mahout project as a beginner is often frustrating.\nI suggest that quickstart scripts should be part of the build and tested regularly.",
        "Issue Links": []
    },
    "MAHOUT-497": {
        "Key": "MAHOUT-497",
        "Summary": "Matrix get/set causes allocation (?!)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "08/Sep/10 17:57",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "08/Sep/10 20:00",
        "Description": "Matrix get and set check the array bounds.  But checking the array bounds calls size.  But size allocates a small integer\narray.\nThis is evil and KILLs performance.\nThe first fix is to avoid this in SGD based classifiers.  Second will be a real change to the matrix routines to avoid the allocation.",
        "Issue Links": []
    },
    "MAHOUT-498": {
        "Key": "MAHOUT-498",
        "Summary": "Need improvements to feature hashed vectorization for speed and flexibility",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "08/Sep/10 17:59",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "15/Oct/10 17:16",
        "Description": "Need to have a text vectorizer that handles a lucene analyzer.\nAlso need byte[] short-cuts to avoid converting to a string and then back to bytes.",
        "Issue Links": []
    },
    "MAHOUT-499": {
        "Key": "MAHOUT-499",
        "Summary": "Implement LSMR in-memory",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "09/Sep/10 00:25",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "21/Aug/11 19:41",
        "Description": "I need this for reverse engineering SGD models.\nThis will interact with the recent work for large sparse solvers.",
        "Issue Links": [
            "/jira/browse/MAHOUT-672"
        ]
    },
    "MAHOUT-500": {
        "Key": "MAHOUT-500",
        "Summary": "Make it easy to run Mahout on Amazon's Elastic Map Reduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "12/Sep/10 14:20",
        "Updated": "22/May/11 15:42",
        "Resolved": "22/May/11 15:36",
        "Description": "There are command line tools for interacting with S3 and EMR, a script that made it easy to (optionally) upload data and run Mahout commands similar to, or possibly integrated into, bin/mahout would be really useful for people wanting to run Mahout without having to learn too much more than they already know from the bin/mahout command line.",
        "Issue Links": [
            "/jira/browse/MAHOUT-588"
        ]
    },
    "MAHOUT-501": {
        "Key": "MAHOUT-501",
        "Summary": "Property file lucenevector.props doesn't get loaded when running 'mahout lucene.vector'",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Frank Scholten",
        "Created": "13/Sep/10 20:51",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "22/Sep/10 13:05",
        "Description": "Running 'mahout lucene.vector' results in the following stacktrace:\nfrank@frankthetank:~$ mahout lucene.vector\nRunning on hadoop, using HADOOP_HOME=/home/frank/software/dist/hadoop\nHADOOP_CONF_DIR=/home/frank/software/dist/hadoop/conf\n10/09/13 22:44:43 WARN driver.MahoutDriver: No lucene.vector.props found on classpath, will use command-line arguments only\n10/09/13 22:44:43 ERROR lucene.Driver: Exception\norg.apache.commons.cli2.OptionException: Missing required option --dir\n        at org.apache.commons.cli2.option.DefaultOption.validate(DefaultOption.java:172)\n        at org.apache.commons.cli2.option.GroupImpl.validate(GroupImpl.java:265)\n        at org.apache.commons.cli2.commandline.Parser.parse(Parser.java:104)\n        at org.apache.mahout.utils.vectors.lucene.Driver.main(Driver.java:133)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:175)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nUsage:                                                                          \n [--dir <dir> --idField <idField> --output <output> --delimiter <delimiter>     \n--help --field <field> --max <max> --dictOut <dictOut> --norm <norm>            \n--outputWriter <outputWriter> --maxDFPercent <maxDFPercent> --weight <weight>   \n--minDF <minDF>]                                                                \nOptions                                                                         \n  --dir (-d) dir                      The Lucene directory                      \n  --idField (-i) idField              The field in the index containing the     \n                                      index.  If null, then the Lucene internal \n                                      doc id is used which is prone to error if \n                                      the underlying index changes              \n  --output (-o) output                The output file                           \n  --delimiter (-l) delimiter          The delimiter for outputing the           \n                                      dictionary                                \n  --help (-h)                         Print out help                            \n  --field (-f) field                  The field in the index                    \n  --max (-m) max                      The maximum number of vectors to output.  \n                                      If not specified, then it will loop over  \n                                      all docs                                  \n  --dictOut (-t) dictOut              The output of the dictionary              \n  --norm (-n) norm                    The norm to use, expressed as either a    \n                                      double or \"INF\" if you want to use the    \n                                      Infinite norm.  Must be greater or equal  \n                                      to 0.  The default is not to normalize    \n  --outputWriter (-e) outputWriter    The VectorWriter to use, either seq       \n                                      (SequenceFileVectorWriter - default) or   \n                                      file (Writes to a File using JSON format) \n  --maxDFPercent (-x) maxDFPercent    The max percentage of docs for the DF.    \n                                      Can be used to remove really high         \n                                      frequency terms.  Expressed as an integer \n                                      between 0 and 100. Default is 99.         \n  --weight (-w) weight                The kind of weight to use. Currently TF   \n                                      or TFIDF                                  \n  --minDF (-md) minDF                 The minimum document frequency.  Default  \n                                      is 1                                      \n10/09/13 22:44:43 INFO driver.MahoutDriver: Program took 54 ms\nThis is because the program shortname lucene.vector from driver.classes.props has a dot, while the props file is called lucenevector.props, which doesn't have a dot between lucene and vector.\nFix: change lucenevector.props filename to lucene.vector.props.",
        "Issue Links": []
    },
    "MAHOUT-502": {
        "Key": "MAHOUT-502",
        "Summary": "Adding footer note to command line utility",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Joe Prasanna Kumar",
        "Created": "15/Sep/10 05:06",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "24/Sep/10 12:32",
        "Description": "Hi all,\nSince ClusterDumper doesnt seem to have elaborate documentation, just created a page https://cwiki.apache.org/confluence/display/MAHOUT/Cluster+Dumper\nWhile playing around with clusterdump utility, I learned that it can be run on hadoop or as a standalone java program.\nAs most of you are aware, when executed on hadoop, the seqFileDir and pointsDir should be the HDFS location else the local system path location. Since some of the clustering related wiki pages specified that we can get the output from HDFS and then run clusterdump, I was assuming that the clusterdump would always read data from local FS.\nI am not sure if newbies would have this same thought process.. So I was thinking if we'd need to make this explicit by changing the help list of clusterdump\nCurrently ClusterDumper.java has \n addOption(SEQ_FILE_DIR_OPTION, \"s\", \"The directory containing Sequence Files for the Clusters\", true);\nShould we specify something like\n addOption(SEQ_FILE_DIR_OPTION, \"s\", \"The directory (HDFS if using Hadoop / Local filesystem if on standalone mode) containing Sequence Files for the Clusters\", true);\nand so on..\nThe problem with this approach is itz repetitive in that we'd need to change in quite a few places.. (I believe vectordump also follows the same principle)\nor \nshould we modify CommandLineUtil to have a generic message in the help specifying the fact that while running hadoop, the directories should reference HDFS location else local FS.\nHow about adding it to the footer like \nformatter.setFooter(\"Specify HDFS directories while running hadoop; else specify local File System directories\");\nformatter.printFooter();\nAppreciate your feedbacks / thots.\nthanks\nJoe.\nfrom\tJeff Eastman <jdog@windwardsolutions.com>\nreply-to\tdev@mahout.apache.org\nto\tdev@mahout.apache.org\ndate\tFri, Sep 3, 2010 at 2:45 PM\nsubject\tRe: ClusterDumper - Hadoop or standalone ?\nmailed-by\tmahout.apache.org\nhide details Sep 3 (12 days ago)\n\nShow quoted text -\n+1 to generic message approach",
        "Issue Links": []
    },
    "MAHOUT-503": {
        "Key": "MAHOUT-503",
        "Summary": "Bad murmur hash implementation ?!?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Thomas Sauzedde",
        "Created": "16/Sep/10 08:29",
        "Updated": "21/May/11 03:22",
        "Resolved": "17/Sep/10 18:47",
        "Description": "It looks like the murmur hash implementation is coming from the original C to Java port (see http://www.getopt.org/murmur/MurmurHash.java)\nAccording to http://dmy999.com/article/50/murmurhash-2-java-port, (not verified myself), this port doesn't produce the same results than the original C code.",
        "Issue Links": []
    },
    "MAHOUT-504": {
        "Key": "MAHOUT-504",
        "Summary": "Kmeans clustering error",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Zhen Guo",
        "Created": "20/Sep/10 19:47",
        "Updated": "23/Mar/12 11:23",
        "Resolved": "30/Sep/10 14:31",
        "Description": "I tried the Kmeans algorithm on the Synthetic Control data. The following error appears. I tried the Canopy algorithm, it is fine. This error is from Mapper. I am using Trunk.\n10/09/20 19:40:06 INFO mapred.JobClient: Task Id : attempt_201008261432_1324_m_000000_0, Status : FAILED\njava.lang.IllegalStateException: Cluster is empty!\n\tat org.apache.mahout.clustering.kmeans.KMeansClusterMapper.setup(KMeansClusterMapper.java:57)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:583)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)",
        "Issue Links": []
    },
    "MAHOUT-505": {
        "Key": "MAHOUT-505",
        "Summary": "Alternate mechanism for assembling job files.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "21/Sep/10 00:55",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "04/Oct/10 03:44",
        "Description": "Per discussion here:\nhttp://www.lucidimagination.com/search/document/6082249e667f2a1b/problem_with_our_jar_publishing\nAssembly job files using the assembly plugin/descriptor instead of an ant taks and the build helper plugin. This results in job files being named {-job.jar} instead of \n{.job}\n, but that seems to be necessary to prevent nexus from overwriting the main jar artifacts with the jar files.",
        "Issue Links": []
    },
    "MAHOUT-506": {
        "Key": "MAHOUT-506",
        "Summary": "Deploy a classifier as a server",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Saikat Kanjilal",
        "Created": "21/Sep/10 04:38",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "21/Sep/10 15:02",
        "Description": "Deploy a classifier, supervised as well unsupervised as a recommendation engine to be run as a server",
        "Issue Links": []
    },
    "MAHOUT-507": {
        "Key": "MAHOUT-507",
        "Summary": "Add some docs/testing to the SVM's work by Zhao Zhendong",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Saikat Kanjilal",
        "Created": "21/Sep/10 04:41",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "22/May/11 15:03",
        "Description": "Add some testing/documentation to the SVM work done by Zhao Zhendong.",
        "Issue Links": []
    },
    "MAHOUT-508": {
        "Key": "MAHOUT-508",
        "Summary": "ARFF Driver configuration for mahout script missing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Frank Scholten",
        "Created": "21/Sep/10 19:56",
        "Updated": "21/May/11 03:19",
        "Resolved": "12/Dec/10 18:28",
        "Description": "I noticed that org.apache.mahout.utils.vectors.arff.Driver was missing from the list of classes in driver.classes.props and that there was no props file so I added a patch.\nYou can now run the driver via mahout arff.vector",
        "Issue Links": []
    },
    "MAHOUT-509": {
        "Key": "MAHOUT-509",
        "Summary": "Options in Bayes TrainClassifier and TestClassifier",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Joe Prasanna Kumar",
        "Created": "22/Sep/10 04:44",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "26/Sep/10 10:18",
        "Description": "Hi all,\nAs I was going through wikipedia example, I encountered a situation with TrainClassifier wherein some of the options with default values are actually mandatory. \nThe documentation / command line help says that \ndefault source (--datasource) is hdfs but TrainClassifier has withRequired(true) while building the --datasource option. We are checking if the dataSourceType is hbase else set it to hdfs. so ideally withRequired should be set to false\ndefault --classifierType is bayes but withRequired is set to true and we have code like\nif (\"bayes\".equalsIgnoreCase(classifierType)) \n{\n        log.info(\"Training Bayes Classifier\");\n        trainNaiveBayes(inputPath, outputPath, params);\n        \n      }\n else if (\"cbayes\".equalsIgnoreCase(classifierType)) \n{\n        log.info(\"Training Complementary Bayes Classifier\");\n        // setup the HDFS and copy the files there, then run the trainer\n        trainCNaiveBayes(inputPath, outputPath, params);\n      }\n\nwhich should be changed to\nif (\"cbayes\".equalsIgnoreCase(classifierType)) \n{\n        log.info(\"Training Complementary Bayes Classifier\");\n        trainCNaiveBayes(inputPath, outputPath, params);\n        \n      }\n else  \n{\n        log.info(\"Training  Bayes Classifier\");\n        // setup the HDFS and copy the files there, then run the trainer\n        trainNaiveBayes(inputPath, outputPath, params);\n      }\n\nPlease let me know if this looks valid and I'll submit a patch for a JIRA issue.\nreg\nJoe.",
        "Issue Links": []
    },
    "MAHOUT-510": {
        "Key": "MAHOUT-510",
        "Summary": "Standardize serialization mechanisms",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "22/Sep/10 14:49",
        "Updated": "21/May/11 03:19",
        "Resolved": "05/Apr/11 21:13",
        "Description": "At the moment this is tracking a broader concern: to standardize as much as possible how we approach serialization. The long-term goal is notionally to use the following \"encodings\" as the input/output of Mahout stuff, and by extension, probably internally too.\n\nText\nVector Writable\n(maybe Avro)\n\nnot\n\nSerializable\nGSON / JSON",
        "Issue Links": []
    },
    "MAHOUT-511": {
        "Key": "MAHOUT-511",
        "Summary": "TrainLogistic regressed due to encoder changes.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "22/Sep/10 17:27",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "22/Sep/10 17:30",
        "Description": "Isabel wrote:\nI just tried running the SGD example with the following command line (adapted\nfrom the corresponding JIRA issue):\n./bin/mahout org.apache.mahout.classifier.sgd.TrainLogistic --passes 100 --rate\n50 --lambda 0.001 --input examples/src/main/resources/donut.csv --features 21 \u2013\noutput donut.model --target color --categories 2 --predictors x y xx xy yy a b c\n--types n n\nWhen running the code above I ran into a few NullPointerExceptions - I was able\nto fix them with a few tiny changes. If not stripped they should be attached to\nthis mail to highlight the lines of code that caused the trouble. However I was\nwondering whether I simply used the wrong command line.",
        "Issue Links": []
    },
    "MAHOUT-512": {
        "Key": "MAHOUT-512",
        "Summary": "Principal Component Analysis for mahout-examples",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Raphael Cendrillon",
        "Reporter": "Max Heimel",
        "Created": "25/Sep/10 13:48",
        "Updated": "11/Jan/12 01:17",
        "Resolved": "21/May/11 02:43",
        "Description": "Overview:\nPrinciple Component Analysis is a widely used statistical method for decorrelation and dimension-reduction. It is typically applied for denoising data, data compression and as a preprocessing step for more advanced statistical analysis tools (like e.g. independent component analysis). The main idea of PCA is to apply a linear transformation of the given data points into a - typically less-dimensional - feature space, such that the direction of greatest variance within the data lies on the first dimension within the feature space.\nOne approach to performing PCA is by transforming the d-dimensional data into the space spanned by the p largest eigenvectors of the covariance matrix of the (centered) data. This is done in the following steps (assume the data is given by a nxd data matrix containing n data rows with dimensionality d):\n\nCompute the d-dimensional empirical mean-vector m of the data points\nCenter the data by subtracting m from each data point.\nCompute the covariance matrix C = E' * E, where E is the centered data matrix and E' is its transpose.\nCompute the eigenvalue decomposition of C, such that e_i is the eigenvector corresponding to eigenvalue lambda_i, order i such that lambda_i > lambda_j implies i<j.\nCreate the transformation matrix T as the p largest eigenvectors (e_0 ... e_p) of C.\nTransforming a data matrix into the PCA space: R=(D-M)*T\nTransforming data from PCA to original space: D=(R*T')+M\n\nImplementation overview:\nI would suggest implementing PCA for the mahout-examples subproject. Maybe some of the jobs could also go into mahout-core/math, if wished. The implementation would consist of a set of M/R jobs and some gluing code. I assume data is given in HDFS as a file of d-dimensional data points, where each row corresponds to a data point. The map-reduce based implementation of PCA for Mahout would then consist of the following code:\n\nDistributed Data centering code, that computes the emprical mean vector and centers the data\nDistributed Code for computing the covariance matrix.\nSequential ( ? )  glue code for computing the eigenvalue decomposition of the covariance matrix and constructing the transformation matrices T and T'. This code would use the existing eigensolvers of Mahout.\nDistributed code to transform data into the PCA space. This code would use the existing distributed matrix multiplication of mahout & the data centering job\nDistributed code to transforms PCA  data back into the original space. This code would use the existing distributed matrix multiplication of mahout & the data centering job.\nGlue code that combines all jobs into a coherent implementation of PCA.\n\nImplementation details:\nFor a complete implementation the following three map/reduce jobs have to be written:\n\nA M/R job to compute the empirical mean vector. Each row gets a row (data point) and emits (dimension, [value,1]) tuples. A local combiner preaggregates the values for each dimension to (dimension, [sum of values, nr of datapoints]). Finally, a (single) reducer computes the average for each dimension and stores back the mean vector.\nA M/R job to center (uncenter) the data. The job consists of a single map-phase that reads in a row (data point) of the original matrix, subtracts (or adds) the empirical mean vector and emits (row number, new ro) pairs that are written back to disk.\nA M/R job to compute the matrix product of a large matrix with its transpose. Principle idea: each mapper reads in a row of the matrix and computes the products of all combinations (e.g. [a,b,c] --> (0,aa)(1,ab)(2,ac)(3,bb)(4,bc)(5,cc)). The key corresponds to the position within the symmetric result matrix. A local combiner sums up the local products for each key, the reducer sums up the sum of products for each mapper per key and stores (position, value) tuples to disk. Finally a (local) clean-up phase constructs the covariance matrix from the (position, value) tuples. Since the covariance matrix is dxd and d is typically low compared to the number of data points, I assume the final local step should be fine.",
        "Issue Links": []
    },
    "MAHOUT-513": {
        "Key": "MAHOUT-513",
        "Summary": "ClusterEvaluator inter-cluster density returns NaN",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "28/Sep/10 15:20",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "30/Sep/10 14:54",
        "Description": "Hi Jeff,\nI've been trying out the ClusterEvaluator class today since your recent changes, and I'm running into a problem whereby the average intra-cluster density can be set to NaN. Looking into it, it seems to happen for clusters containing points which are very close to the centroid.  For example, I have a cluster with:\nCentroid:\n\n{0:0.6075199543688895,1:-0.3165058387409551,2:0.2027106147825682,3:-21.246338574215706,4:-5.875047828899212,5:-0.9835694086952028,6:0.2794019939470805,7:-0.36402079609289717,8:0.5201946127074457,9:-0.47084217746293855,10:-0.14380397719670499,11:-0.10441028152861193,12:0.0698485086335405,13:0.014286758874801297}\n\nand one of the representative points (3 per cluster):\n[0.6075199543688894, -0.31650583874095506, 0.2027106147825682, -21.2463385742157, -5.875047828899212, -0.9835694086952026, 0.27940199394708054, -0.36402079609289706, 0.5201946127074457, -0.47084217746293855, -0.14380397719670499, -0.10441028152861194, 0.06984850863354047, 0.014286758874801297]\nAs far as I can tell from debugging, the representative points look identical to the centroid of this cluster, but I'm assuming there's some small difference as \"if (!vector.equals(clusterI.getCenter()))\" in ClusterEvaluator.invalidCluster() is always returning false for these points, and so the cluster isn't pruned from the list.\nLater on, in ClusterEvaluator.intraClusterDensity(), the \"min\" and \"max\" distances are ending up with the same value, and the density from \"double density = (sum / count - min) / (max - min);\" is calculated as NaN, e.g. here are the values I'm getting:\nmin = max = 1.5397509610616733E-7\ncount = 3\nsum = 4.61925288318502E-7\nmax - min: 0.0\ncount - min: 2.9999998460249038\n(sum / count - min) = 0.0\nThis then causes avgDensity to be calculated as NaN. I'm not sure what the solution is here, should invalidCluster() check that the the difference between the centroid and the candidate representative point is greater than a certain threshold, which would cause such a cluster to be pruned? Or is the fix in the intraClusterDensity() calculation to handle the case where min = max?\nBTW would you prefer that I create a Jira to record these issues, or is it okay to send them to the dev list as I've been doing?\nThanks,\nDerek",
        "Issue Links": []
    },
    "MAHOUT-514": {
        "Key": "MAHOUT-514",
        "Summary": "Improve search box at Mahout site.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Documentation",
        "Assignee": "Sean R. Owen",
        "Reporter": "Alex Baranau",
        "Created": "30/Sep/10 10:36",
        "Updated": "21/May/11 03:18",
        "Resolved": "01/Dec/10 17:29",
        "Description": "Use \"Lucid Find\" and \"Search-Lucene\" as search providers fro project's artefacts.\nInitially it was discussed at http://search-lucene.com/m/dYNqq1BRJf91.\nNotes: It makes sense to follow the same rationales as those in TIKA-488 and NUTCH-909, since they are applicable here too.",
        "Issue Links": []
    },
    "MAHOUT-515": {
        "Key": "MAHOUT-515",
        "Summary": "PFPGrowthTest has a hard-wired reference to an existing output directory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Jeff Eastman",
        "Created": "30/Sep/10 17:43",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "03/Oct/10 10:37",
        "Description": "This test began failing when another test was added which had the same dependency. That test has been corrected but is not yet checked-in.\nThe correct pattern is to use getTestTempDirPath() to allocate temporary files. I was unable to quickly do this in the test (it caused other of its tests to fail) so I'm opening an issue for somebody more knowledgeable in this code.",
        "Issue Links": []
    },
    "MAHOUT-516": {
        "Key": "MAHOUT-516",
        "Summary": "Eigencuts produces unexpected results",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "30/Sep/10 21:27",
        "Updated": "03/Jun/12 18:30",
        "Resolved": "18/Jan/11 16:29",
        "Description": "Shannon reports he suspects a logic error in Eigencuts since it evidently does not produce exactly the expected results. It passes all current unit tests so we need to characterize the results differences and produce a test for it. Marking for 0.5 for now though we will fix it as soon as possible.",
        "Issue Links": []
    },
    "MAHOUT-517": {
        "Key": "MAHOUT-517",
        "Summary": "Eigencuts needs an output format",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Jeff Eastman",
        "Created": "30/Sep/10 21:30",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "13/Sep/11 09:16",
        "Description": "Need to define and implement an output format for Eigencuts spectral clustering. Marking for 0.5 but will consider an interim fix for 0.4 during polishing phase.",
        "Issue Links": []
    },
    "MAHOUT-518": {
        "Key": "MAHOUT-518",
        "Summary": "Implement Affinity Preprocessing for Eigencuts and Spectral KMeans",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Jeff Eastman",
        "Created": "30/Sep/10 21:33",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "20/Oct/11 10:29",
        "Description": "The input format for these clustering algorithms is currently affinity tuples. It would be very nice to have this process automated. Marking for 0.5 as this will require some investigation.",
        "Issue Links": []
    },
    "MAHOUT-519": {
        "Key": "MAHOUT-519",
        "Summary": "Spectral KMeans Lacks All the Normal KMeans Arguments",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "30/Sep/10 21:37",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "30/Sep/10 23:08",
        "Description": "This is a simple fix which is targeted for 0.5 but very likely for 0.4 during polishing phase",
        "Issue Links": []
    },
    "MAHOUT-520": {
        "Key": "MAHOUT-520",
        "Summary": "Add example scripts / integration tests for various algorithms.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Drew Farris",
        "Reporter": "Drew Farris",
        "Created": "02/Oct/10 00:45",
        "Updated": "21/May/11 03:19",
        "Resolved": "03/Feb/11 12:52",
        "Description": "Scripts like build-reuters.sh are useful in that they both demonstrate typical usage of Mahout from the command-line but also serve as integration tests. We should add additional scripts that drive the algorithms so new users can quickly run the examples. \nPerhaps these can also be run from hudson as a part of the nightly builds and can serve as integration tests.\nAs a start towards this goal, provide build-20news-bayes.sh example (in the same vein as build-reuters.sh, that follows https://cwiki.apache.org/MAHOUT/twenty-newsgroups.html",
        "Issue Links": []
    },
    "MAHOUT-521": {
        "Key": "MAHOUT-521",
        "Summary": "Add option to DictionaryVectorizer to create (tf and tfidf) vectors on-the-fly using a given dictionary",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "02/Oct/10 20:18",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "09/Oct/10 10:58",
        "Description": "Current dictionary vectorizer takes a set of text-files, creates the dictionary and convert them to text vectors. In a classification scenario, the vectorizer needs to take a Already existing dictionary and use the ids to convert text to vectors and optionally do the following\n1. Choose between tf|tfidf weights (need to take the document frequency as an input for this)\n2. Add new words to the dictionary and provide options to write it to the disk and read it back\n3. Add option to normalize/lognormalize",
        "Issue Links": []
    },
    "MAHOUT-522": {
        "Key": "MAHOUT-522",
        "Summary": "Using different data sources for input/output",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Alex Petrov",
        "Created": "04/Oct/10 07:47",
        "Updated": "21/May/11 03:22",
        "Resolved": "21/May/11 02:51",
        "Description": "Hi,\nMahout is currently bound to the file system, at least from my feeling. Most of the time data structures i'm working with aren't located on the file system, the same way as output isn't bound to the FS, most of time i'm forced to export my datasets from DB to FS, and then load them back to DB afterwards.\nMost likely, it's not quite interesting for the core developers, who're working on the algorithms implementation to start writing adapters to DBs or anything like that.\nFor instance,  SequenceFilesFromDirectory is a simple way to get your files from directory and convert it all to Sequence Files. Some people would be extremely grateful if there would be an interface they may implement to throw their files from DB straight to the Sequence File without a medium of a File System. If anyone's interested, i can provide a patch.\nSecond issue is related to the workflow process itself. For instance, what if i already do have Dictionary, TF-IDF, and TF in some particular format that was created by other things in my infrastructure. Again, I need to convert those to the Mahout data-structures. Can't we just allow other jobs to accept more generic types (or interfaces, for instance) when working with TF-IDF, TF and Dictionaries, without binding those to Hadoop FS. \nI do realize that Mahout is a part of Lucene/Hadoop infrastructure, but it's also an independent project, so it may benefit and get a more wide adoption, if it allows to work with any format. I do have an idea of how to implement it, and partially implemented it for our infrastructure needs, but i really want to hear some output from users and hadoop developers, whether it's suitable and if anyone may benefit out of that.\nThank you!",
        "Issue Links": []
    },
    "MAHOUT-523": {
        "Key": "MAHOUT-523",
        "Summary": "LDADriver not being initialised properly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Mat Kelcey",
        "Created": "08/Oct/10 12:20",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "08/Oct/10 13:00",
        "Description": "all the clustering drivers under o.a.m.clustering, eg  CanopyDriver, DirichletDriver, FuzzyKMeansDriver  etc\nare bootstrapped in main() with ToolRunner\nfor some reason o.a.m.clustering.lda.LDADriver wasn't being boostrapped like this so job conf was null\nfound while trying to run LDADriver as part of examples/bin/build-reuters.sh",
        "Issue Links": []
    },
    "MAHOUT-524": {
        "Key": "MAHOUT-524",
        "Summary": "DisplaySpectralKMeans example fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Jeff Eastman",
        "Created": "12/Oct/10 03:31",
        "Updated": "12/Feb/12 13:55",
        "Resolved": "04/Jan/12 14:58",
        "Description": "I've committed a new display example that attempts to push the standard mixture of models data set through spectral k-means. After some tweaking of configuration arguments and a bug fix in EigenCleanupJob it runs spectral k-means to completion. The display example is expecting 2-d clustered points and the example is producing 5-d points. Additional I/O work is needed before this will play with the rest of the clustering algorithms.",
        "Issue Links": []
    },
    "MAHOUT-525": {
        "Key": "MAHOUT-525",
        "Summary": "Implement LatentFactorLogLinear models",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "14/Oct/10 17:15",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "02/Mar/14 20:00",
        "Description": "The approach is based on the paper http://arxiv.org/abs/1006.2156\nFor integration into Mahout, the idea would be to build an OnlineLearner that accept feature vectors where the first two elements are assumed to be the row\nand column id's.  The learning will proceed by stochastic gradient descent.",
        "Issue Links": []
    },
    "MAHOUT-526": {
        "Key": "MAHOUT-526",
        "Summary": "Infinite Recursion in DefaultTreeBuilder",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "16/Oct/10 15:48",
        "Updated": "21/May/11 03:18",
        "Resolved": "01/Nov/10 16:45",
        "Description": "In some cases, building a random forest with a dataset that contains many numerical attributes, the DefaultTreeBuilder enters in an infinite recursion and throws a StackOverflowError.",
        "Issue Links": []
    },
    "MAHOUT-527": {
        "Key": "MAHOUT-527",
        "Summary": "Add optional max depth pruning to DefaultTreeBuilder",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "16/Oct/10 16:05",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "21/May/11 02:59",
        "Description": "The user should be able to set a limit on the maximum depth of the grown trees. By default the Tree builder does not set any limit to the depth.",
        "Issue Links": []
    },
    "MAHOUT-528": {
        "Key": "MAHOUT-528",
        "Summary": "Perverse serialization format ... JSON contain strings that parse into JSON that contain strings that parse into JSON",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "17/Oct/10 04:52",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "17/Oct/10 07:56",
        "Description": "The current use of gson has some very odd use of strings that could be eliminated to allow much faster serialization with less memory use.\nBackwards compatibility should be a mild goal with this fix.",
        "Issue Links": []
    },
    "MAHOUT-529": {
        "Key": "MAHOUT-529",
        "Summary": "Implement LinearRegression",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Frank Wang",
        "Created": "21/Oct/10 04:21",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "15/Oct/11 10:13",
        "Description": "The implementation would an OnlineLearner that would be to similar to AbstractOnlineLogisticRegression for logistic regression.\nThe learning will proceed by stochastic gradient descent.",
        "Issue Links": []
    },
    "MAHOUT-530": {
        "Key": "MAHOUT-530",
        "Summary": "seqdirectory fails silently on encountering  empty documents",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.3",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Sidharth",
        "Created": "23/Oct/10 09:15",
        "Updated": "21/May/11 03:22",
        "Resolved": "19/Jan/11 00:38",
        "Description": "When some empty documents are present in the set the utility should be ignoring them. It fails silently and terminates the sequencing process... Removing the empty docs fixes the problems. There are no errors no logs on the screen or the hadoop logs",
        "Issue Links": []
    },
    "MAHOUT-531": {
        "Key": "MAHOUT-531",
        "Summary": "MatrixWritable doesn't actually write/read anything",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Ted Dunning",
        "Reporter": "Alexander Hans",
        "Created": "23/Oct/10 15:26",
        "Updated": "21/May/11 03:19",
        "Resolved": "07/Mar/11 23:41",
        "Description": "The write() and readFields() methods of MatrixWritable write/read only the classname, they don't write/read actual data.",
        "Issue Links": []
    },
    "MAHOUT-532": {
        "Key": "MAHOUT-532",
        "Summary": "Update plugin versions in the POM structure",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "23/Oct/10 17:27",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "25/Oct/10 02:19",
        "Description": "I suspect that some of Sean's problems with pushing a release have to do with the staleness of the versions of various critical plugins, such as the maven release plugin. Under this JIRA, I propose to move everything up to current released versions. As a side-benefit, parallel builds under maven 3 should become possible.",
        "Issue Links": []
    },
    "MAHOUT-533": {
        "Key": "MAHOUT-533",
        "Summary": "Clustering Standard Deviation Calculations Are Inaccurate",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "23/Oct/10 17:53",
        "Updated": "21/May/11 03:18",
        "Resolved": "18/Jan/11 16:28",
        "Description": "Mahout has two classes that compute Gaussian statistics: RunningSumsGaussianAccumulator and OnlineGaussianAccumulator. The first uses sum-of-squares and the second Welford's method. There is also a unit test (TestGaussianAccumulators) which compares their results over a sample dataset and illustrates the large differences in standard deviation produced. The Online accumulator is used in the CDbwEvaluator to compute its metrics. The RunningSums accumulator is only used by the unit test for comparison purposes.\nToday, the sum-of-squares method is used in AbstractCluster to compute mean and stdDev statistics for all Clusters. The stdDev values are not used by most of the clustering algorithms except for graphical displays so this does not cause an accuracy problem with the clustering results themselves. For Dirichlet process clustering; however, stdDev is relevant in computing pdf() and so it needs to be changed in those models. Even with this numerical error; however, Dirichlet performs pretty well. This is probably due to its sampling behavior not requiring precise standard deviations.\nMoving the AbstractCluster implementation to use OnlineGaussianAccumulator is in my plans for 0.5; however, Fuzzy K-Means requires that weighted observations be correctly handled and both K-Means algorithms require that observation statistics (see ClusterObservations) be passed from mapper to combiner to reducer and I have not been able to figure out how to do this yet with Online's state variables as opposed to RunningSums. There is also a performance difference between the two algorithms, since Online does a complete computation for each observe() and none in compute() whereas RunningSums has minimal per-observation math and does all the heavy lifting in compute().",
        "Issue Links": []
    },
    "MAHOUT-534": {
        "Key": "MAHOUT-534",
        "Summary": "ditch the use of org.apache:apache at the parent POM",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Benson Margulies",
        "Created": "27/Oct/10 12:25",
        "Updated": "31/Oct/10 15:50",
        "Resolved": "27/Oct/10 19:50",
        "Description": "org.apache:apache:7, used as a parent, does define distribution management to point to the Apache Nexus. That's about 5 lines. The rest of it is composed of someone's opinion as to appropriate versions and default for the many projects of, really, the apache maven project. In particular, their configuration of the release plugin is not what we want.\nTo get 0.4 done I add a -P!apache-release to the release plugin (actually, any setting of <arguments> would have done it), but I propose post-0.4 to just remove the parent spec from our toplevel and put the distribution management in there by hand.",
        "Issue Links": []
    },
    "MAHOUT-535": {
        "Key": "MAHOUT-535",
        "Summary": "mahout seqdirectory reads only from the local filesystem, even when running over Hadoop",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Matt Spitz",
        "Created": "27/Oct/10 22:08",
        "Updated": "21/May/11 03:19",
        "Resolved": "20/Jan/11 23:37",
        "Description": "It seems as if seqdirectory only reads from the local filesystem, though it writes correctly to the HDFS.\nConsider 'myurls-local' and 'myurls-dfs', the former existing in the working directory and the latter existing on the home directory of the HDFS.\nRunning:\nMAHOUT_HOME=. ./bin/mahout seqdirectory -i myurls-local -o myurls-seqdir -c UTF-8 -chunk \nacts as expected (myurls-seqdir is created on the local filesystem)\nRunning:\nMAHOUT_HOME=. HADOOP_HOME=/usr/lib/hadoop-0.20 HADOOP_CONF_DIR=/etc/hadoop-0.20/conf ./bin/mahout seqdirectory -i myurls-dfs -o myurls-seqdir -c UTF-8 -chunk \ncreates a 12kb myurls-seqdir directory on the DFS.  Presumably, it couldn't read myurls-dfs from the DFS and ended up creating a nearly-empty sequence directory.\nRunning:\nMAHOUT_HOME=. HADOOP_HOME=/usr/lib/hadoop-0.20 HADOOP_CONF_DIR=/etc/hadoop-0.20/conf ./bin/mahout seqdirectory -i myurls-local -o myurls-seqdir -c UTF-8 -chunk \nacts as expected, creating a substantial myurls-seqdir on the DFS.",
        "Issue Links": []
    },
    "MAHOUT-536": {
        "Key": "MAHOUT-536",
        "Summary": "ClusterDumper has some private static methods that are generally useful",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "30/Oct/10 19:26",
        "Updated": "21/May/11 03:19",
        "Resolved": "19/Jan/11 07:37",
        "Description": "getTopFeatures, readPoints, etc. are useful outside of the scope of dumping clusters and we should expose them as such.",
        "Issue Links": []
    },
    "MAHOUT-537": {
        "Key": "MAHOUT-537",
        "Summary": "Bring DistributedRowMatrix into compliance with Hadoop 0.20.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "03/Nov/10 21:26",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "13/Sep/11 09:13",
        "Description": "Convert the current DistributedRowMatrix to use the newer Hadoop 0.20.2 API, in particular eliminate dependence on the deprecated JobConf, using instead the separate Job and Configuration objects.",
        "Issue Links": []
    },
    "MAHOUT-538": {
        "Key": "MAHOUT-538",
        "Summary": "neaten pom profiles for release production",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "04/Nov/10 17:25",
        "Updated": "21/May/11 03:18",
        "Resolved": "08/Nov/10 12:51",
        "Description": "The profile structure for the POMS is error prone, as the distribution module is conditional, and so can be left out when it needs to be in.\nSwitch to a structure where all the modules are in, but slow plugin executions are disabled when we don't want them.",
        "Issue Links": []
    },
    "MAHOUT-539": {
        "Key": "MAHOUT-539",
        "Summary": "Need example code for fast encoding",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "05/Nov/10 03:17",
        "Updated": "21/May/11 03:18",
        "Resolved": "05/Nov/10 03:25",
        "Description": "The variable encoding stuff needs some examples for how to do encoding fast.",
        "Issue Links": []
    },
    "MAHOUT-540": {
        "Key": "MAHOUT-540",
        "Summary": "Integration test for the DocumentProcessor",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Frank Scholten",
        "Created": "05/Nov/10 17:30",
        "Updated": "21/May/11 03:18",
        "Resolved": "12/Dec/10 18:27",
        "Description": "Added an integration test for the DocumentProcessor",
        "Issue Links": []
    },
    "MAHOUT-541": {
        "Key": "MAHOUT-541",
        "Summary": "Incremental SVD Implementation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tamas Jambor",
        "Created": "08/Nov/10 23:57",
        "Updated": "21/May/11 03:18",
        "Resolved": "08/Mar/11 08:58",
        "Description": "I thought I'd put up this implementation of the popular SVD algorithm for recommender systems. It is based on the SVD implementation, but instead of computing each user and each item matrix, it trains the model iteratively, which was the original version that Simon Funk proposed.  The advantage of this implementation is that you don't have to recalculate the dot product of each user-item pair for each training cycle, they can be cached, which speeds up the algorithm considerably.",
        "Issue Links": []
    },
    "MAHOUT-542": {
        "Key": "MAHOUT-542",
        "Summary": "MapReduce implementation of ALS-WR",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "13/Nov/10 22:16",
        "Updated": "15/Nov/11 13:18",
        "Resolved": "23/Mar/11 22:34",
        "Description": "As Mahout is currently lacking a distributed collaborative filtering algorithm that uses matrix factorization, I spent some time reading through a couple of the Netflix papers and stumbled upon the \"Large-scale Parallel Collaborative Filtering for the Net\ufb02ix Prize\" available at http://www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf.\nIt describes a parallel algorithm that uses \"Alternating-Least-Squares with Weighted-\u03bb-Regularization\" to factorize the preference-matrix and gives some insights on how the authors distributed the computation using Matlab.\nIt seemed to me that this approach could also easily be parallelized using Map/Reduce, so I sat down and created a prototype version. I'm not really sure I got the mathematical details correct (they need some optimization anyway), but I wanna put up my prototype implementation here per Yonik's law of patches.\nMaybe someone has the time and motivation to work a little on this with me. It would be great if someone could validate the approach taken (I'm willing to help as the code might not be intuitive to read) and could try to factorize some test data and give feedback then.",
        "Issue Links": []
    },
    "MAHOUT-543": {
        "Key": "MAHOUT-543",
        "Summary": "classifier type and data source not taken into account in org.apache.mahout.classifier.bayes.TestClassifier",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.4",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Robin Swezey",
        "Created": "14/Nov/10 14:27",
        "Updated": "21/May/11 03:23",
        "Resolved": "15/Nov/10 00:31",
        "Description": "This concerns the latest version of Mahout checked out from the SVN repo (I believe 0.4)\nWhen launching TestClassifier using:\n$MAHOUT_HOME/bin/mahout testclassifier -m model -d test-input -type cbayes -ng 1 -source hbase\nThe output says that the classifier type is Bayes (not Complementary) and the data source is HDFS.\nAfter a quick investigation in TestClassifer.java source, there are mistakes on lines: 142, 147, 148\nwith classifierType instead of typeOpt, and dataSource instead of dataSourceOpt",
        "Issue Links": []
    },
    "MAHOUT-544": {
        "Key": "MAHOUT-544",
        "Summary": "lucene.vector -e file NPE happens",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "beneo",
        "Created": "15/Nov/10 06:30",
        "Updated": "21/May/11 03:22",
        "Resolved": "19/Jan/11 07:50",
        "Description": "in Mahout/bin\n./mahout lucene.vector -d /home/beneo/temp/index/ -i id -o /home/beneo/temp/vector/vector -f content -t /home/beneo/temp/vector/dict -e file -n 2\nNPE\nthe class LuceneIterable#next, the result = mapper.getVecter() may return null;\nhowever, in the JWriterVectorWriter#write(Iterable<Vector>, long),  in the loop for(Vector vector : iterable), the vector may be null.\nso, NPE throwed.\nthe solutions is \nif(vector!=null){\n    writer.writer ......\n}",
        "Issue Links": []
    },
    "MAHOUT-545": {
        "Key": "MAHOUT-545",
        "Summary": "GSON not a good fit for deserializing SGD models",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "15/Nov/10 15:31",
        "Updated": "21/May/11 03:19",
        "Resolved": "30/Dec/10 02:30",
        "Description": "The problem is that GSON uses JavaCC which does LL(k) parsing.  The way that the JSON grammar is expressed causes stack recursion for each element of\na vector.\nThis is evil for parsing large models.\nI will be changing ModelSerializer to fix this and will provide a reverse compatible mode for anybody who has been able to serialize/deserialize these models in the past.  My thought is to just make all the SGD models Writables.",
        "Issue Links": [
            "/jira/browse/MAHOUT-556"
        ]
    },
    "MAHOUT-546": {
        "Key": "MAHOUT-546",
        "Summary": "Bug creating vector from Solr index with TrieFields",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "G Fernandes",
        "Created": "16/Nov/10 15:09",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "26/Jun/11 08:12",
        "Description": "A Solr index with an Id field of type TrieLong causes NPE when trying to extract vectors from the lucene index\nCommand line used:\n../bin/mahout lucene.vector -d ~/solr/data/index/ -o  vectors.vec -t d.dic -f text -i articleId\nwhere 'articleId' is a Stored numeric field declared as 'tlong' in Solr.  Output:\nno HADOOP_HOME set, running locally\nNov 16, 2010 2:59:50 PM org.slf4j.impl.JCLLoggerAdapter info\nINFO: Output File: moreover.vec\nNov 16, 2010 2:59:51 PM org.apache.hadoop.util.NativeCodeLoader <clinit>\nWARNING: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nNov 16, 2010 2:59:51 PM org.apache.hadoop.io.compress.CodecPool getCompressor\nINFO: Got brand-new compressor\nException in thread \"main\" java.lang.NullPointerException\n\tat java.io.DataOutputStream.writeUTF(DataOutputStream.java:330)\n\tat java.io.DataOutputStream.writeUTF(DataOutputStream.java:306)\n\tat org.apache.mahout.math.VectorWritable.write(VectorWritable.java:125)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)\n\tat org.apache.hadoop.io.SequenceFile$RecordCompressWriter.append(SequenceFile.java:1128)\n\tat org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:977)\n\tat org.apache.mahout.utils.vectors.io.SequenceFileVectorWriter.write(SequenceFileVectorWriter.java:46)\n\tat org.apache.mahout.utils.vectors.lucene.Driver.main(Driver.java:226)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:184)\nThe problem is at class LuceneIteratable line 130:\n  name = indexReader.document(doc, idFieldSelector).get(idField);\nIt does not work with the way Solr stores numeric fields in the index.",
        "Issue Links": []
    },
    "MAHOUT-547": {
        "Key": "MAHOUT-547",
        "Summary": "http://mahout.apache.org/javadoc/core/ points to the documentation for 0.3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Documentation",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "16/Nov/10 16:11",
        "Updated": "21/May/11 03:19",
        "Resolved": "01/Dec/10 17:42",
        "Description": "http://mahout.apache.org/javadoc/core/ should be updated to point to the javadoc of the latest release",
        "Issue Links": []
    },
    "MAHOUT-548": {
        "Key": "MAHOUT-548",
        "Summary": "Add support for CSV for Vector IO",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "19/Nov/10 14:19",
        "Updated": "21/May/11 03:19",
        "Resolved": "25/Mar/11 14:30",
        "Description": "We should be able to read in and write out CSV files to Vectors",
        "Issue Links": []
    },
    "MAHOUT-549": {
        "Key": "MAHOUT-549",
        "Summary": "\".numberOfRows\" should be \".numberOfColumns\" in RowSimilarityJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jingguo Yao",
        "Created": "20/Nov/10 13:50",
        "Updated": "21/May/11 03:18",
        "Resolved": "01/Dec/10 17:16",
        "Description": "In line 80 of RowSimilarityJob, \".numberOfRows\" should be \".numberOfColumns\" since NUMBER_OF_COLUMNS is about column not row.\n  public static final String NUMBER_OF_COLUMNS = RowSimilarityJob.class.getName() + \".numberOfRows\";",
        "Issue Links": []
    },
    "MAHOUT-550": {
        "Key": "MAHOUT-550",
        "Summary": "Add RandomVector and RandomMatrix",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "21/Nov/10 04:39",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "04/Apr/11 07:32",
        "Description": "Add Vector and Matrix implementations that generate a unique and reproducible random number for each index.",
        "Issue Links": []
    },
    "MAHOUT-551": {
        "Key": "MAHOUT-551",
        "Summary": "Kmeans example with space delimited data",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Jeff Eastman",
        "Reporter": "Djellel Eddine Difallah",
        "Created": "21/Nov/10 20:42",
        "Updated": "29/Feb/16 02:14",
        "Resolved": "15/Jan/11 16:43",
        "Description": "The provided example for Kmeans clustering using the synthetic control data asks for t1 and t2 measures because it runs the Canopy Driver to determine the initial clusters. Kmeans originally requires a K variable to generate random centers from the input data. I propose to add another example in the package which will serve for any space delimited numerical input to cluster with Kmeans in its original form and not using Canopy. The modification is quite simple and is mostly based on the synthetic control Job.",
        "Issue Links": []
    },
    "MAHOUT-552": {
        "Key": "MAHOUT-552",
        "Summary": "AbstractCluster eliminates NamedVectors by replacing them with RandomAccessSparseVector always",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Pere Ferrera Bertran",
        "Created": "24/Nov/10 09:18",
        "Updated": "21/May/11 03:18",
        "Resolved": "10/Apr/11 18:11",
        "Description": "When clustering using NamedVectors as input - after running seq2sparse with patch https://issues.apache.org/jira/browse/MAHOUT-401 - names are lost because AbstractCluster replaces vectors coming in the constructor with RandomAccessSparseVector.",
        "Issue Links": []
    },
    "MAHOUT-553": {
        "Key": "MAHOUT-553",
        "Summary": "Unify ranking of boolean recommendations in distributed and non-distributed recommenders",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "26/Nov/10 20:09",
        "Updated": "21/May/11 03:18",
        "Resolved": "26/Nov/10 21:24",
        "Description": "When using a weighted sum for preference estimation on boolean data, the predicted preferences can only be 1 or NaN which is mathematically correct but not very useful for ranking them. The distributed recommender should therefore adapt the behavior of GenericBooleanPrefItemBasedRecommender in that case: use the sums of similarities to rank the recommended items.",
        "Issue Links": []
    },
    "MAHOUT-554": {
        "Key": "MAHOUT-554",
        "Summary": "Javadoc syntax error in AbstractJob's parseArguments method",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jingguo Yao",
        "Created": "28/Nov/10 02:37",
        "Updated": "21/May/11 03:18",
        "Resolved": "01/Dec/10 17:16",
        "Description": "\"<code>get(optionName</code>\" should be \"<code>get(optionName)</code>.\"",
        "Issue Links": []
    },
    "MAHOUT-555": {
        "Key": "MAHOUT-555",
        "Summary": "Wrong description of input in ToItemPrefsMapper's javadoc",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Jingguo Yao",
        "Created": "28/Nov/10 03:48",
        "Updated": "21/May/11 03:18",
        "Resolved": "01/Dec/10 17:14",
        "Description": "{@link org.apache.mahout.math.VarLongWritable}\n/\n{@link org.apache.hadoop.io.Text}\nshould be\n{@link org.apache.hadoop.io.LongWritable}/{@link org.apache.hadoop.io.Text}",
        "Issue Links": []
    },
    "MAHOUT-556": {
        "Key": "MAHOUT-556",
        "Summary": "In the trainlogistic example the JSON model file which is created is missing commas and making it unusable with runLogistic.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Rohan Anil",
        "Created": "29/Nov/10 11:42",
        "Updated": "21/May/11 03:18",
        "Resolved": "17/Jan/11 01:03",
        "Description": "Bug related to creation of the model when you run trainlogistic \n Its creating the JSON model file  using the toJson function as illustrated below\n--------------------------------\nIn,\n LogisticModelParameters.java\nFunction\nvoid saveTo(Writer out)\n{\n...\n..\nString savedForm = gson.toJson(this);\n...\n}\n--------------------------------\nBut this is not working as expected : -  String savedForm = gson.toJson(this);\nFor my experiment using a different dataset - \nI get the following model file : \n{\"targetVariable\":\"customer\",\"typeMap\":\n{\"feature2\":\"n\",\"feature3\":\"n\",\n    \"feature1\":\"n\"}\n,\"numFeatures\":334,\"useBias\":true,\"maxTargetCategories\":\n  2,\"targetCategories\":[\"0\",\"1\"],\"lambda\":1.0E-4,\"learningRate\":0.001,\"lr\":{\n    \"mu0\":0.001,\"decayFactor\":0.999,\"stepOffset\":10,\"forgettingExponent\":\n    -0.5,\"perTermAnnealingOffset\":20,\"beta\":\n{\"rows\":1,\"cols\":334,\"data\":[[\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,6.741887291022263E-4,0.0,0.0,-53.6076187622054,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,5.031178185395536E-5,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.04383410529689268,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,\n          0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]]}\n,\n    \"numCategories\":2,\"step\":260951,\"updateSteps\":{}\"updateCounts\":{}\n    \"lambda\":1.0E-4,\"prior\":{}\"sealed\":true,\"gradient\":{}}}\nIf you notice the last part,\n  \"numCategories\":2,\"step\":260951,\"updateSteps\":{}\"updateCounts\":{}\n    \"lambda\":1.0E-4,\"prior\":{}\"sealed\":true,\"gradient\":{}}}\nare missing commas between updateSteps,updateCounts  and Sealed variables\nInvestigating further, \nThese come from the  AbstractOnlineLogisticRegression.java and the above variables are not initialized hence the wrong output by the toJson function. \nThis is a bug with  - > gson.toJson function,  I see that I am using gson-1.3 and upgrading to 1.4  by modifying core/pom.xml fixes things, But runLogistic then complains about \n10/11/29 03:29:43 WARN conf.Configuration: DEPRECATED: hadoop-site.xml found in the classpath. Usage of hadoop-site.xml is deprecated. Instead use core-site.xml, mapred-site.xml and hdfs-site.xml to override properties of core-default.xml, mapred-default.xml and hdfs-default.xml respectively\nException in thread \"main\" java.lang.RuntimeException: No-args constructor for interface org.apache.mahout.math.Vector does not exist. Register an InstanceCreator with Gson for this type to fix this problem.\n\tat com.google.gson.MappedObjectConstructor.constructWithNoArgConstructor(MappedObjectConstructor.java:64)\n\tat com.google.gson.MappedObjectConstructor.construct(MappedObjectConstructor.java:53)\n\tat com.google.gson.JsonObjectDeserializationVisitor.constructTarget(JsonObjectDeserializationVisitor.java:41)\n\tat com.google.gson.JsonDeserializationVisitor.getTarget(JsonDeserializationVisitor.java:56)\n\tat com.google.gson.ObjectNavigator.accept(ObjectNavigator.java:101)\n\tat com.google.gson.JsonDeserializationVisitor.visitChild(JsonDeserializationVisitor.java:107)\n\tat com.google.gson.JsonDeserializationVisitor.visitChildAsObject(JsonDeserializationVisitor.java:95)\n\tat com.google.gson.JsonObjectDeserializationVisitor.visitObjectField(JsonObjectDeserializationVisitor.java:62)\n\tat com.google.gson.ObjectNavigator.navigateClassFields(ObjectNavigator.java:156)\n\tat com.google.gson.ObjectNavigator.accept(ObjectNavigator.java:123)\n\tat com.google.gson.JsonDeserializationVisitor.visitChild(JsonDeserializationVisitor.java:107)\n\tat com.google.gson.JsonDeserializationVisitor.visitChildAsObject(JsonDeserializationVisitor.java:95)\n\tat com.google.gson.JsonObjectDeserializationVisitor.visitObjectField(JsonObjectDeserializationVisitor.java:62)\n\tat com.google.gson.ObjectNavigator.navigateClassFields(ObjectNavigator.java:156)\n\tat com.google.gson.ObjectNavigator.accept(ObjectNavigator.java:123)\n\tat com.google.gson.JsonDeserializationContextDefault.fromJsonObject(JsonDeserializationContextDefault.java:73)\n\tat com.google.gson.JsonDeserializationContextDefault.deserialize(JsonDeserializationContextDefault.java:51)\n\tat com.google.gson.Gson.fromJson(Gson.java:495)\n\tat com.google.gson.Gson.fromJson(Gson.java:444)\n\tat com.google.gson.Gson.fromJson(Gson.java:419)\n\tat org.apache.mahout.classifier.sgd.LogisticModelParameters.loadFrom(LogisticModelParameters.java:142)\n\tat org.apache.mahout.classifier.sgd.LogisticModelParameters.loadFrom(LogisticModelParameters.java:155)\n\tat org.apache.mahout.classifier.sgd.RunLogistic.main(RunLogistic.java:56)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:616)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:182)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:616)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nWhich I haven't had the time to investigate yet, Will post more results tomorrow.",
        "Issue Links": [
            "/jira/browse/MAHOUT-545"
        ]
    },
    "MAHOUT-557": {
        "Key": "MAHOUT-557",
        "Summary": "Bug in Auc computation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "30/Nov/10 04:58",
        "Updated": "21/May/11 03:18",
        "Resolved": "12/Dec/10 22:41",
        "Description": "From Anil:\nAnother bug: in Auc.java\nClassifier Evaluation :\nwhen the scores are tied -\n...\n\n       // how many negatives are tied?\n       int k0 = 0;\n       while (i0 < n0 && v0 == tieScore) \nUnknown macro: {         k0++;         i0++;         v0 = scores[0].get(i0);       } \n       // and how many positives\n       int k1 = 0;\n       while (i1 < n1 && v1 == tieScore) \nUnknown macro: {         k1++;         i1++;         v1 = scores[1].get(i1);       } \nIt sometimes goes Index out of bounds: Increment operation should be after accessing it\n\n while (i0 < n0 && v0 == tieScore) \nUnknown macro: {         v0 = scores[0].get(i0);         k0++;         i0++;\n       } \n       // and how many positives\n       int k1 = 0;\n       while (i1 < n1 && v1 == tieScore) \nUnknown macro: {         v1 = scores[1].get(i1);         k1++;         i1++;\n       } \n\nVerily, he speaks truth.",
        "Issue Links": []
    },
    "MAHOUT-558": {
        "Key": "MAHOUT-558",
        "Summary": "Extend ItembasedRecommender to offer different \"exclusion modes\" when computing most similar items to a collection of input items",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "05/Dec/10 09:38",
        "Updated": "21/May/11 03:18",
        "Resolved": "12/Dec/10 19:23",
        "Description": "GenericItembased Recommender currently excludes all items that are not similar at least one of the input items when computing the most similar items to a collection of items. We should introduce a way to have the user decide whether he/she wants this behavior or he/she wants to have all items included that are similar to at least one of the input items, which is more useful in practice in my experience.",
        "Issue Links": []
    },
    "MAHOUT-559": {
        "Key": "MAHOUT-559",
        "Summary": "Compare Recommender output by order of recommendations.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "07/Dec/10 09:36",
        "Updated": "21/May/11 03:19",
        "Resolved": "10/Jan/11 00:00",
        "Description": "The existing RecommenderEvaluator (AverageAbsoluteDifferenceRecommenderEvaluator.java) has a very limited API. It evaluates a Recommender's performance on a training v.s. test scenario. It does not allow comparing the outputs of different recommenders against the same data model. Also, I could not figure out how its comparison criteria.\nOrderBasedRecommenderEvaluator compares the output of two recommenders. It only checks the order of the items in the recommendations, ignoring the returned preference values.",
        "Issue Links": [
            "/jira/browse/MAHOUT-586"
        ]
    },
    "MAHOUT-560": {
        "Key": "MAHOUT-560",
        "Summary": "Support for more flexible file handling in text to sequence file conversion",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "07/Dec/10 22:36",
        "Updated": "21/May/11 03:18",
        "Resolved": "16/Dec/10 09:31",
        "Description": "Currently SequenceFilesFromDirectory supports for conversion of texts to sequence file. The exact file (and potentially text from file) selection is not configurable. I'd like to re-use most of the conversion logic but change the exact text selection. (More information on what exactly I want to do: http://tinyurl.com/35pv8jg )\nI slightly changed SequenceFilesFromDirectory to make that possible. (Added one additional optional parameter, but by default the current behaviour is used).",
        "Issue Links": []
    },
    "MAHOUT-561": {
        "Key": "MAHOUT-561",
        "Summary": "TestClassifier does not correctly handle 'type' and 'source' command line parameters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Oleg Kalnichevski",
        "Created": "11/Dec/10 20:02",
        "Updated": "21/May/11 03:18",
        "Resolved": "16/Jan/11 23:39",
        "Description": "TestClassifier does not correctly handle 'type' and 'source' command line parameters",
        "Issue Links": []
    },
    "MAHOUT-562": {
        "Key": "MAHOUT-562",
        "Summary": "Results produced by Complementary Bayes Classifier seem odd",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Oleg Kalnichevski",
        "Created": "11/Dec/10 20:26",
        "Updated": "21/May/11 03:22",
        "Resolved": "11/Dec/10 20:34",
        "Description": "The 20newsgroups example produces expected results (95% correctness rate) when using the Naive Bayes algorithm. When switching the algorithm to the Complementary Bayes while all other parameters remain the same the rate of correctly classified documents drops to 5%. This seems odd to me. \nI admit I know next to nothing about the Bayes theorem and possibly my expectations are totally off. \n\u2014\nDec 11, 2010 8:47:47 PM org.apache.mahout.classifier.bayes.TestClassifier classifySequential\nINFO: Loading model from: \n{basePath=/home/oleg/data/mahout/20news-bayes-model, classifierType=cbayes, alpha_i=1, dataSource=hdfs, gramSize=1, verbose=false, encoding=UTF-8, defaultCat=unknown, testDirPath=/home/oleg/data/mahout/20news-bayes-train-input}\nDec 11, 2010 8:47:47 PM org.apache.mahout.classifier.bayes.TestClassifier classifySequential\nINFO: Testing Complementary Bayes Classifier\n...\nINFO: =======================================================\nSummary\n-------------------------------------------------------\nCorrectly Classified Instances          :        578\t    5.1087%\nIncorrectly Classified Instances        :      10736\t   94.8913%\nTotal Classified Instances              :      11314\n=======================================================\nConfusion Matrix\n-------------------------------------------------------\na    \tb    \tc    \td    \te    \tf    \tg    \th    \ti    \tj    \tk    \tl    \tm    \tn    \to    \tp    \tq    \tr    \ts    \tt    \t<--Classified as\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t597  \t0    \t0    \t0    \t0    \t0    \t0    \t |  597   \ta     = rec.sport.baseball\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t595  \t0    \t0    \t0    \t0    \t0    \t0    \t |  595   \tb     = sci.crypt\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t600  \t0    \t0    \t0    \t0    \t0    \t0    \t |  600   \tc     = rec.sport.hockey\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t546  \t0    \t0    \t0    \t0    \t0    \t0    \t |  546   \td     = talk.politics.guns\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t599  \t0    \t0    \t0    \t0    \t0    \t0    \t |  599   \te     = soc.religion.christian\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t591  \t0    \t0    \t0    \t0    \t0    \t0    \t |  591   \tf     = sci.electronics\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t591  \t0    \t0    \t0    \t0    \t0    \t0    \t |  591   \tg     = comp.os.ms-windows.misc\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t585  \t0    \t0    \t0    \t0    \t0    \t0    \t |  585   \th     = misc.forsale\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t377  \t0    \t0    \t0    \t0    \t0    \t0    \t |  377   \ti     = talk.religion.misc\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t480  \t0    \t0    \t0    \t0    \t0    \t0    \t |  480   \tj     = alt.atheism\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t593  \t0    \t0    \t0    \t0    \t0    \t0    \t |  593   \tk     = comp.windows.x\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t564  \t0    \t0    \t0    \t0    \t0    \t0    \t |  564   \tl     = talk.politics.mideast\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t590  \t0    \t0    \t0    \t0    \t0    \t0    \t |  590   \tm     = comp.sys.ibm.pc.hardware\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t578  \t0    \t0    \t0    \t0    \t0    \t0    \t |  578   \tn     = comp.sys.mac.hardware\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t593  \t0    \t0    \t0    \t0    \t0    \t0    \t |  593   \to     = sci.space\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t598  \t0    \t0    \t0    \t0    \t0    \t0    \t |  598   \tp     = rec.motorcycles\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t594  \t0    \t0    \t0    \t0    \t0    \t0    \t |  594   \tq     = rec.autos\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t584  \t0    \t0    \t0    \t0    \t0    \t0    \t |  584   \tr     = comp.graphics\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t465  \t0    \t0    \t0    \t0    \t0    \t0    \t |  465   \ts     = talk.politics.misc\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t594  \t0    \t0    \t0    \t0    \t0    \t0    \t |  594   \tt     = sci.med\nDefault Category: unknown: 20",
        "Issue Links": []
    },
    "MAHOUT-563": {
        "Key": "MAHOUT-563",
        "Summary": "CanopyEstimator - Estimate T1/T2 for CanopyClusterer",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "12/Dec/10 03:34",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "22/May/11 15:32",
        "Description": "Hunting for T1/T2 values that make an interesting Canopy set is a singularly unsatisfying task. This class estimates T1 and T2 numbers given the original set.",
        "Issue Links": []
    },
    "MAHOUT-564": {
        "Key": "MAHOUT-564",
        "Summary": "KMeansClusterer does not use distanceThreshold parameter in testConvergence(Iterable<Cluster> clusters, double distanceThreshold) method",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "12/Dec/10 18:24",
        "Updated": "21/May/11 03:18",
        "Resolved": "10/Jan/11 17:30",
        "Description": "While running KMeansClusterer#runKMeansIteration sequentially I noticed that the distanceThreshold parameter is not used. The convergenceDelta field is used instead. However, it's initialized at 0 and only set when creating a KMeansClusterer with a Configuration object, which is only used in a MapReduce setting.",
        "Issue Links": []
    },
    "MAHOUT-565": {
        "Key": "MAHOUT-565",
        "Summary": "Features incorrectly hashed in Minhash",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Ankur Bansal",
        "Reporter": "Ankur Bansal",
        "Created": "16/Dec/10 08:44",
        "Updated": "21/May/11 03:18",
        "Resolved": "16/Dec/10 14:20",
        "Description": "Given a feature vector for which minhash signature is desired, each feature id (an integer) is converted to a byte array through a series of bit shift operations. Current implementation of these operations doesn't mask the bits being shifted resulting in sign bit being shifted.",
        "Issue Links": []
    },
    "MAHOUT-566": {
        "Key": "MAHOUT-566",
        "Summary": "Job-specific boolean options such as --clustering cannot be set through program props file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "17/Dec/10 20:04",
        "Updated": "21/May/11 03:18",
        "Resolved": "21/Jan/11 19:59",
        "Description": "When I run mahout kmeans or the MahoutDriver with a kmeans.props that has all required parameters filled in and contains:\ncl|clustering= true\nI will get the following error message in the log: SEVERE: Unexpected true while processing Job-Specific Options\nRunning kmeans and specifying -cl or --clustering does work however.",
        "Issue Links": []
    },
    "MAHOUT-567": {
        "Key": "MAHOUT-567",
        "Summary": "Infrastructure Todo list brocken",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Documentation",
        "Assignee": "Sean R. Owen",
        "Reporter": "Oliver B. Fischer",
        "Created": "21/Dec/10 22:34",
        "Updated": "21/May/11 03:18",
        "Resolved": "17/Jan/11 10:09",
        "Description": "The infrastructure todo list at https://cwiki.apache.org/confluence/display/MAHOUT/TODO is brocken.",
        "Issue Links": []
    },
    "MAHOUT-568": {
        "Key": "MAHOUT-568",
        "Summary": "Online Classification using Cassandra",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Mubarak Seyed",
        "Created": "27/Dec/10 09:43",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "21/May/11 02:56",
        "Description": "As like HBase integration (https://issues.apache.org/jira/browse/MAHOUT-124), Mahout can be integrated with Cassandra. Cassandra supports Map/Reduce integration out-of-the-box.\nWe can implement CassandraBayesDatastore to read Cassandra based model.",
        "Issue Links": []
    },
    "MAHOUT-569": {
        "Key": "MAHOUT-569",
        "Summary": "Category names with spaces cause NullPointerException in ConfusionMatrix when testing classifier",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Raviv M-G",
        "Created": "28/Dec/10 20:27",
        "Updated": "21/May/11 03:18",
        "Resolved": "17/Jan/11 10:25",
        "Description": "If you try to perform classification using categories with spaces in their names then the classifier will happily train and create a model for you, but when you go to test the model you get:\nException in thread \"main\" java.lang.NullPointerException\n\tat org.apache.mahout.classifier.ConfusionMatrix.getCount(ConfusionMatrix.java:102)\n\tat org.apache.mahout.classifier.ConfusionMatrix.incrementCount(ConfusionMatrix.java:118)\n\tat org.apache.mahout.classifier.ConfusionMatrix.incrementCount(ConfusionMatrix.java:122)\n\tat org.apache.mahout.classifier.ConfusionMatrix.addInstance(ConfusionMatrix.java:90)\n\tat org.apache.mahout.classifier.ResultAnalyzer.addInstance(ResultAnalyzer.java:69)\n\tat org.apache.mahout.classifier.bayes.TestClassifier.classifySequential(TestClassifier.java:266)\n\tat org.apache.mahout.classifier.bayes.TestClassifier.main(TestClassifier.java:186)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:182)",
        "Issue Links": []
    },
    "MAHOUT-570": {
        "Key": "MAHOUT-570",
        "Summary": "Make the retrieval of candidate items for the most-similar-items computation customizable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "31/Dec/10 10:02",
        "Updated": "21/May/11 03:19",
        "Resolved": "03/Jan/11 08:56",
        "Description": "When retrieving the initial set of possibly similar items in GenericItemBasedRecommender.doMostSimilarItems(...) only co-occurring items are selected. We need an exchangable strategy (similar to the existing CandidateItemsStrategy) in order to make this behavior customizable.",
        "Issue Links": []
    },
    "MAHOUT-571": {
        "Key": "MAHOUT-571",
        "Summary": "Minkowski and Chebyshev DistanceMeasure",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "01/Jan/11 05:19",
        "Updated": "21/May/11 03:19",
        "Resolved": "04/Feb/11 17:48",
        "Description": "Implementations of Minkowski and Chebyshev distance measures. \nMinkowski distance is a generalization of the L-space measures, where L1 is Manhattan distance and L2 is Euclidean distance. Uses Math.pow to calculate coordinate distances. Math.pow has a fast-path for integer-valued arguments, so Minkowski with 3.0 is much faster than Minkowski with 3.1.\nChebyshev distance is \"chessboard\" distance, based on the moves that a king can make: any direction include diagonals. The Manhattan or taxicab distances can only traverse in right angles.",
        "Issue Links": []
    },
    "MAHOUT-572": {
        "Key": "MAHOUT-572",
        "Summary": "Non-distributed implementation of ALS-WR matrix factorization",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "01/Jan/11 10:31",
        "Updated": "21/May/11 03:18",
        "Resolved": "03/Jan/11 09:22",
        "Description": "I created a non-distributed implementation of the algorithm described in \"Large-scale Parallel Collaborative Filtering for the Net\ufb02ix Prize\" available at http://www.hpl.hp.com/personal/Robert_Schreiber/papers/2008%20AAIM%20Netflix/netflix_aaim08(submitted).pdf.\nThis gives more choice for users that need an online SVD Recommender and might be a useful starting point to implementing code that automatically finds a near-optimal regularization parameter for the distributed version of the algorithm in MAHOUT-542.\nAlong with the patch goes a refactoring and polishing of the SVD Recommender code that separates the factorization computation from the Recommender implementation. This way it should easily be possible to try out different factorization approaches with our SVDRecommender.",
        "Issue Links": []
    },
    "MAHOUT-573": {
        "Key": "MAHOUT-573",
        "Summary": "hadoop job config parameter,e.g., -Dmapred.cache.archives, support in mahout wrapper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Shige Takeda",
        "Created": "03/Jan/11 07:33",
        "Updated": "21/May/11 03:22",
        "Resolved": "03/Jan/11 11:19",
        "Description": "In order to specify a custom analyzer that utilizes a Japanese Morphological Analyzer \"Igo\" referring to dictionary files on HDFS for seq2sparse, I needed to pass the following job config:\nmapred.cache.archives=\"hdfs://localhost:9000/user/stakeda/ipadic.zip#ipadic\nmapred.create.symlink=yes\nThis way, the IgoAnalyzer can read dictionaries from \"./ipadic\" as follows:\nhttps://github.com/smtakeda/mahout/blob/project101210/examples/src/main/java/org/apache/mahout/analysis/IgoAnalyzer.java\nOther use case is I needed to specify mapred.job.queue.name to something to get appropriate priority for running jobs in  the work environment:\nhttps://github.com/smtakeda/mahout/blob/yahoo/core/src/main/java/org/apache/mahout/clustering/canopy/CanopyDriver.java\n...\nconf.set(\"mapred.job.queue.name\", \"unfunded\"); \nBased on these two use cases, I would like to request/propose to add hadoop job option support, i.e., -Dmapred.cache.archives=... to mahout wrapper.\nChanges are roughly expected in two ends; \"bin/mahout\" and all main functions that parse command lines. Here is a quick patch for \"bin/mahout\":\nlocalhost ~/workspace/mahout_git/bin: git diff -r f13e517408f20f75009e05e6c72c5fbb836e3f66 mahout \ndiff --git a/bin/mahout b/bin/mahout\nindex 774fa11..9d78ceb 100755\n\u2014 a/bin/mahout\n+++ b/bin/mahout\n@@ -116,6 +116,14 @@ CLASSPATH=${CLASSPATH}:$JAVA_HOME/lib/tools.jar\n\nso that filenames w/ spaces are handled correctly in loops below\n IFS=\n\n+\n+# JAVA_PROPERTIES\n+JAVA_PROPERTIES=\n+while [ $1 ] && [ ${1:0:2} == \"-D\" ] ; do \n+    JAVA_PROPERTIES=\"$1 $JAVA_PROPERTIES\"\n+    shift\n+done\n+\n if [ $IS_CORE == 0 ] \n then\n\nadd release dependencies to CLASSPATH\n@@ -198,7 +206,7 @@ if [ \"$HADOOP_HOME\" = \"\" ] || [ \"$MAHOUT_LOCAL\" != \"\" ] ; then\n   elif [ \"$MAHOUT_LOCAL\" != \"\" ] ; then \n     echo \"MAHOUT_LOCAL is set, running locally\"\n   fi\n\n\nexec \"$JAVA\" $JAVA_HEAP_MAX $MAHOUT_OPTS -classpath \"$CLASSPATH\" $CLASS \"$@\"\n+  exec \"$JAVA\" $JAVA_HEAP_MAX $JAVA_PROPERTIES $MAHOUT_OPTS -classpath \"$CLASSPATH\" $CLASS \"$@\"\n else\n   echo \"Running on hadoop, using HADOOP_HOME=$HADOOP_HOME\"\n   if [ \"$HADOOP_CONF_DIR\" = \"\" ] ; then\n@@ -213,7 +221,7 @@ else\n     exit 1\n   else\n   export HADOOP_CLASSPATH=$MAHOUT_CONF_DIR:${HADOOP_CLASSPATH}\nexec \"$HADOOP_HOME/bin/hadoop\" jar $MAHOUT_JOB $CLASS \"$@\"\n+  exec \"$HADOOP_HOME/bin/hadoop\" jar $MAHOUT_JOB $CLASS \"$@\" $JAVA_PROPERTIES\n   fi \n fi",
        "Issue Links": []
    },
    "MAHOUT-574": {
        "Key": "MAHOUT-574",
        "Summary": "\"seqdirectory\" command fails to notice -Ddefault.fs.name",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "04/Jan/11 01:17",
        "Updated": "21/May/11 03:19",
        "Resolved": "29/Jan/11 06:32",
        "Description": "Have problem targeting Filesystem used with utility. If hadoop setup is present, there's no way to force the utility to process using local filesystem (or, i guess, vice versa): \n\nbin/mahout seqdirectory -Dmapred.job.tracker=local -Dfs.default.name=file:/// -c UTF-8 -i /home/dmitriy/projects/testcollections/reuters-extracted/ -o /home/dmitriy/projects/testcollections/reuters-seqfiles\nRunning on hadoop, using HADOOP_HOME=/home/dmitriy/tools/hadoop\nNo HADOOP_CONF_DIR set, using /home/dmitriy/tools/hadoop/conf\n11/01/03 15:16:13 ERROR text.SequenceFilesFromDirectory: Exception\norg.apache.commons.cli2.OptionException: Unexpected -Dfs.default.name=file:/// while processing Options\n\n        at org.apache.commons.cli2.commandline.Parser.parse(Parser.java:99)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.main(SequenceFilesFromDirectory.java:201)\n\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:183)\n\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:186)\n\n\nThe reason seems to be in the fact that this job is not a Tool and hence does not recognize any hadoop params.",
        "Issue Links": []
    },
    "MAHOUT-575": {
        "Key": "MAHOUT-575",
        "Summary": "Remove most deprecated/unused mahout-math code",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "04/Jan/11 14:51",
        "Updated": "21/May/11 03:19",
        "Resolved": "08/Jan/11 13:55",
        "Description": "Per a conversation on the mailing list, like Ted, I'd like to remove most of the remaining unused/deprecated code in mahout-math.",
        "Issue Links": []
    },
    "MAHOUT-576": {
        "Key": "MAHOUT-576",
        "Summary": "AbstractJDBCDiffStorage.updateItemPref is updating the AVG incorrectly in most cases",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Renaud Bruyeron",
        "Created": "05/Jan/11 10:49",
        "Updated": "21/May/11 03:18",
        "Resolved": "15/Jan/11 16:30",
        "Description": "the JDBC version of the DiffStorage is not using a RunningAverage in the removePreference case, and ends up making incorrect calculations.\nIn a scenario where users are setting and removing a lot of preferences, the AVG stored in the diff table quickly diverges from the correct value because of this.\nRight now, the input to updateItemPref comes from SlopeOneRecommender, and in the case of removePreference, it is the old preference value, not a delta. However, the code uses it as if it were a delta. Thus the calculation is off by PEER(removedpreference,userid)/count everytime a user removes a preference.\nAt first glance, the code should compute the old delta instead of the old preference, and use this in the updateItemPref",
        "Issue Links": []
    },
    "MAHOUT-577": {
        "Key": "MAHOUT-577",
        "Summary": "RowSimilarityJob hangs during CooccurrencesMapper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Maya Hristakeva",
        "Created": "05/Jan/11 19:37",
        "Updated": "21/May/11 03:18",
        "Resolved": "31/Jan/11 07:50",
        "Description": "Hello,\nWhen trying to run a RowSimilarityJob on a matrix ( 146682 x 138351 ), the job gets through the RowWeightMapper and WeightedOccurrencesPerColumnReducer, and hangs during the CooccurrencesMapper although it shows that the map tasks are 100% complete. \nThe command I use to run the job is: \nhadoop jar mahout-core-0.4-job.jar org.apache.mahout.math.hadoop.similarity.RowSimilarityJob -Dmapred.input.dir=/user/maya.hristakeva/mahout/core4/tf/1/0.001/title/12_07_10/lda/5/lda-sim/ldaCompressedDocumentsMatrix -Dmapred.output.dir=/user/maya.hristakeva/mahout/core4/tf/1/0.001/title/12_07_10/lda/5/lda-sim/ldaDocumentSimilarityMatrix -Dmapred.reduce.tasks=8 -Dmapred.map.tasks=200 -Dmapred.job.name=LDA_ROW_SIMILARITY_TEST --tempDir /user/maya.hristakeva/temp/lda/5 --numberOfColumns 138351 --similarityClassname org.apache.mahout.math.hadoop.similarity.vector.DistributedEuclideanDistanceVectorSimilarity --maxSimilaritiesPerRow 10\nAnd the output of the mappers which are 100% complete, but hanging is: \nsyslog logs\n01-05 18:30:00,835 INFO org.apache.hadoop.mapred.MapTask: bufstart = 29085149; bufend = 39038598; bufvoid = 99614720\n2011-01-05 18:30:00,835 INFO org.apache.hadoop.mapred.MapTask: kvstart = 65461; kvend = 327605; length = 327680\n2011-01-05 18:30:06,241 INFO org.apache.hadoop.mapred.MapTask: Finished spill 94\n2011-01-05 18:30:09,208 INFO org.apache.hadoop.mapred.MapTask: Spilling map output: record full = true\n2011-01-05 18:30:09,208 INFO org.apache.hadoop.mapred.MapTask: bufstart = 39038598; bufend = 48983989; bufvoid = 99614720\n2011-01-05 18:30:09,208 INFO org.apache.hadoop.mapred.MapTask: kvstart = 327605; kvend = 262068; length = 327680\n2011-01-05 18:30:14,528 INFO org.apache.hadoop.mapred.MapTask: Finished spill 95\n2011-01-05 18:30:17,328 INFO org.apache.hadoop.mapred.MapTask: Spilling map output: record full = true\n2011-01-05 18:30:17,328 INFO org.apache.hadoop.mapred.MapTask: bufstart = 48983989; bufend = 58929384; bufvoid = 99614720\n2011-01-05 18:30:17,328 INFO org.apache.hadoop.mapred.MapTask: kvstart = 262068; kvend = 196531; length = 327680\n2011-01-05 18:30:22,615 INFO org.apache.hadoop.mapred.MapTask: Finished spill 96\n.\n.\n.\nThis problem does not occur when I use a toy matrix of 100 x 100, but once I give it the original matrix of ..... the problem is always reproducible. \nAny ideas on what could be causing this? \nThanks, \nMaya Hristakeva",
        "Issue Links": []
    },
    "MAHOUT-578": {
        "Key": "MAHOUT-578",
        "Summary": "canopy clustering fails if --Dmapred.job.queue.name=unfunded is specified to mahout driver command line",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Shige Takeda",
        "Created": "05/Jan/11 20:28",
        "Updated": "21/May/11 03:22",
        "Resolved": "05/Jan/11 22:57",
        "Description": "Hi, I would like to demonstrate -D option issues by showing one concrete example, and would like to propose the fix.\nWhen I want to run canopy clustering using mahout driver, the command line is something like this. NOTE: -Dmapred.job.queue.name is required Job config in the company's environment.\n$MAHOUT_HOME/bin/mahout canopy \\\n        -i input \\\n        -o output \\\n        -dm org.apache.mahout.common.distance.EuclideanDistanceMeasure \\\n        -t1 2.0 \\\n        -t2 0.05 \\\n        -cl \\\n        -Dmapred.job.queue.name=unfunded\nand I get the error:\nRunning on hadoop, using HADOOP_HOME=/grid/0/gs/hadoop/current\nHADOOP_CONF_DIR=/grid/0/gs/conf/current\n11/01/05 20:19:15 ERROR common.AbstractJob: Unexpected -Dmapred.job.queue.name=unfunded while processing Job-Specific Options:\nThis is because -D parameter is NOT parsed properly by ToolRunner.run but passed through to CanopyDriver's command line option parsers.\nToolRunner.run(Tool,String[]) should be used rather than ToolRunner.run(Configuraiton,Tool,String[]) to get -D parameter processed.",
        "Issue Links": []
    },
    "MAHOUT-579": {
        "Key": "MAHOUT-579",
        "Summary": "group Id should be included in clusterId for MinHash clustering",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Ankur Bansal",
        "Reporter": "Forest Tan",
        "Created": "07/Jan/11 07:04",
        "Updated": "21/May/11 03:19",
        "Resolved": "14/Jan/11 09:24",
        "Description": "Current implementation of MinHash clustering use N groups of hash value as clusterid, e.g., 10003226-1109023\nAnd the code(MinHashMapper.java) is as following:\nfor (int i = 0; i < this.numHashFunctions; i += this.keyGroups)\n        {\n            StringBuilder clusterIdBuilder = new StringBuilder();\n            for (int j = 0; (j < this.keyGroups) && (i + j < this.numHashFunctions); j++)\n            {\n                clusterIdBuilder.append(this.minHashValues[(i + j)]).append('-');\n            }\n            String clusterId = clusterIdBuilder.toString();\n            clusterId = clusterId.substring(0, clusterId.lastIndexOf('-'));\n            Text cluster = new Text(clusterId);\n            Writable point;\n            if (this.debugOutput)\n                point = new VectorWritable(featureVector.clone());\n            else\n            {\n                point = new Text(item.toString());\n            }\n            context.write(cluster, point);\n        }\nFor example, when KEY_GROUPS=1, NUM_HASH_FUNCTIONS=2, and minhash result is:\nuserid, minhash1, minhash2\nA, 100, 200\nB, 200, 100\nthe clustering result will be:\nclusterid, userid\n100, A\n200, A\n200, B\n100, B\nAnd user A, B will be in the same cluster 100 and 200. \nHowever, the first and the second hash functions are different, so, it doesn't mean the two users are similar even if minhash1 of A equals to minhash2 of B.\nThe fix is easy, just change the line\nclusterId = clusterId.substring(0, clusterId.lastIndexOf('-'));\nto\nclusterId = clusterId + i;\nAfter the fix, the clustering result will be:\nclusterid, userid\n100-0, A\n200-1, A\n200-0, B\n100-1, B",
        "Issue Links": []
    },
    "MAHOUT-580": {
        "Key": "MAHOUT-580",
        "Summary": "Missing basePath in Bayes job / InMemoryDataStore",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Sean R. Owen",
        "Created": "10/Jan/11 19:48",
        "Updated": "21/May/11 03:18",
        "Resolved": "11/Jan/11 20:53",
        "Description": "Reported on user@ by Pierre Mage:\nI found the following issue in the trunk: If the dataSource option is \"hdfs\"\nin org.apache.mahout.classifier.\nClassify, there is a problem with InMemoryBayesDatastore because its\nbasePath param is not defined :\nTo fix it, I edited Classify like this:\n122.  if (\"hdfs\".equals(dataSource)) {\n123.    params.set(\"basePath\", modelBasePath);\nYes, looks like InMemoryDataStore expects this to be set \u2013 so does HBaseDataStore. But that takes the base path (table) separately as a param. And InMemoryDataStore doesn't take it at all. I adjusted this since it seems correct. And then tried to centralized handling of Bayes params in BayesParameters since it seems to get half processed in this class only.\nI have attached my own rather more elaborate patch that reflects this.",
        "Issue Links": []
    },
    "MAHOUT-581": {
        "Key": "MAHOUT-581",
        "Summary": "Remove lines in FileDataModel update files not parsed correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "10/Jan/11 22:26",
        "Updated": "21/May/11 03:19",
        "Resolved": "10/Jan/11 22:27",
        "Description": "Reported by Vasil Vangelovski on user@. Lines of the form \"userID,itemID,\" are valid but cause an IllegalArgumentException when used with FileDataModel.",
        "Issue Links": []
    },
    "MAHOUT-582": {
        "Key": "MAHOUT-582",
        "Summary": "Add check for empty files passed to FileDataModel constructor in order to generate a better error message.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Donald Bleyl",
        "Created": "11/Jan/11 04:01",
        "Updated": "21/May/11 03:18",
        "Resolved": "11/Jan/11 11:44",
        "Description": "The FileDataModel constructor checks for a null arg for dataFile, and a few other conditions, but not for an empty file.  If an empty file is passed in, it triggers a NullPointerException when the first line is evaluated.\nFor an empty file, an NPE is raised when firstLine.length() is called:\n    FileLineIterator iterator = new FileLineIterator(dataFile, false);\n    String firstLine = iterator.peek();\n    while ((firstLine.length() == 0) || (firstLine.charAt(0) == COMMENT_CHAR)) {\nProposed fix is to add a check for a zero-length file:\n    Preconditions.checkArgument(dataFile != null, \"dataFile is null\");\n    if (!dataFile.exists() || dataFile.isDirectory()) \n{\n      throw new FileNotFoundException(dataFile.toString());\n    }\n    Preconditions.checkArgument(dataFile.length() > 0L, \"dataFile is empty\");\nA unit test has been included in the patch.",
        "Issue Links": []
    },
    "MAHOUT-583": {
        "Key": "MAHOUT-583",
        "Summary": "Loss some data when create sequence files from directory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "yumeng",
        "Created": "13/Jan/11 11:10",
        "Updated": "21/May/11 03:18",
        "Resolved": "17/Jan/11 09:54",
        "Description": "Loss some data when create sequence files from directory. It will happen when we need more than one output chunk file. It create chunk-0 twice. The first chunk-0 file is overwrite by the second chunk-0 file. That's because the name of the second chunk file starts from 0 not 1.\nFor example, it creates files in the sequence, chunk-0, chunk-0, chunk-1, chunk-2, chunk-3, chunk-*.  So we loss the first chunk-0 file if we create more than one chunk files.",
        "Issue Links": []
    },
    "MAHOUT-584": {
        "Key": "MAHOUT-584",
        "Summary": "MahoutDriver throws NPE if driver.classes.props is not on the classpath",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "13/Jan/11 22:10",
        "Updated": "21/May/11 03:18",
        "Resolved": "18/Jan/11 00:41",
        "Description": "Running MahoutDriver from an IDE run configuration with a program name such as kmeans and help as arguments, will result in an NPE because it won't be able to find the driver.classes.props file.\nBy changing the catch block so it catches Exception instead of IOException, MahoutDriver will use the driver.classes.default.props file so it can still be run with or without the driver.classes.props. This is useful if one wants to call the MahoutDriver from custom code outside mahout instead of using the mahout script. However, if one wishes to run MahoutDriver from outside mahout, the mahout-core, -math, -utils and example jars still need to be on the classpath or else ClassNotFoundExceptions will be thrown when it tries to add the classes for all the programs.",
        "Issue Links": []
    },
    "MAHOUT-585": {
        "Key": "MAHOUT-585",
        "Summary": "Need simpler ways to use log-likelihood",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "16/Jan/11 23:18",
        "Updated": "21/May/11 03:19",
        "Resolved": "17/Jan/11 01:03",
        "Description": "The log-likelihood score that we have is pretty raw.  It would help to have some convenience routines that package common uses.",
        "Issue Links": []
    },
    "MAHOUT-586": {
        "Key": "MAHOUT-586",
        "Summary": "Redo RecommenderEvaluator for modularity",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "17/Jan/11 04:51",
        "Updated": "11/Sep/11 02:16",
        "Resolved": "25/Mar/11 17:07",
        "Description": "The RecommenderEvaluator implementation is hard-coded around the standard algorithm for comparing a recommender's performance on training v.s. test datasets.\nThis is a rewrite supplying other algorithms, and the ability to directly compare DataModels. It was born out of a desire to check that a new recommender made sense compared to existing recommenders, and to directly work with DataModels that encompass the entire recommendation space.",
        "Issue Links": [
            "/jira/browse/MAHOUT-559"
        ]
    },
    "MAHOUT-587": {
        "Key": "MAHOUT-587",
        "Summary": "-Dmapred.job.queue.name=unfunded is not counted in for seq2sparse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Shige Takeda",
        "Created": "21/Jan/11 20:25",
        "Updated": "21/May/11 03:19",
        "Resolved": "24/Jan/11 21:44",
        "Description": "I revisited this and found the -D problem still remains in seq2sparse... \n$ $MAHOUT_HOME/bin/mahout seq2sparse --input text_output --output seq_output -Dmapred.job.queue.name=unfunded\nRunning on hadoop, using HADOOP_HOME=/grid/0/gs/hadoop/current\nHADOOP_CONF_DIR=/grid/0/gs/conf/current\n11/01/21 20:12:39 ERROR vectorizer.SparseVectorsFromSequenceFiles: Exception\norg.apache.commons.cli2.OptionException: Unexpected -Dmapred.job.queue.name=unfunded while processing Options\n\tat org.apache.commons.cli2.commandline.Parser.parse(Parser.java:99)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.main(SparseVectorsFromSequenceFiles.java:137)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:174)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nUsage:                                                                          \n...\nThe cause is obvious; as somebody mentioned (as well as I see from source code), ./core/src/main/java/org/apache/mahout/vectorizer/SparseVectorsFromSequenceFiles.java doesn't use ToolRunner, and an appropriate propagation of config object to MR jobs is missing.\nAlthough this may be a known issue, since it is not filed in JIRA, I've done just in case.",
        "Issue Links": []
    },
    "MAHOUT-588": {
        "Key": "MAHOUT-588",
        "Summary": "Benchmark Mahout's clustering performance on EC2 and publish the results",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "22/Jan/11 12:32",
        "Updated": "05/Jan/12 23:45",
        "Resolved": "22/May/11 16:02",
        "Description": "For Taming Text, I've commissioned some benchmarking work on Mahout's clustering algorithms.  I've asked the two doing the project to do all the work in the open here.  The goal is to use a publicly reusable dataset (for now, the ASF mail archives, assuming it is big enough) and run on EC2 and make all resources available so others can reproduce/improve.\nI'd like to add the setup code to utils (although it could possibly be done as a Vectorizer) and the publication of the results will be put up on the Wiki as well as in the book.  This issue is to track the patches, etc.",
        "Issue Links": [
            "/jira/browse/MAHOUT-598",
            "/jira/browse/MAHOUT-500",
            "/jira/browse/MAHOUT-670"
        ]
    },
    "MAHOUT-589": {
        "Key": "MAHOUT-589",
        "Summary": "More math deletions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "23/Jan/11 01:45",
        "Updated": "21/May/11 03:18",
        "Resolved": "25/Jan/11 00:37",
        "Description": "The LU decomposition code is not likely to be useful to us and it's ally the Algebra static class also adds little to our normal sparse problem set.\nI will post a patch shortly that deletes both of these and does a few other cleanups as well.",
        "Issue Links": []
    },
    "MAHOUT-590": {
        "Key": "MAHOUT-590",
        "Summary": "add TSV (Tab Separate Value) input file support to SequenceFilesFromDirectory",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Shige Takeda",
        "Created": "24/Jan/11 02:46",
        "Updated": "16/Aug/12 05:42",
        "Resolved": "23/Mar/11 14:52",
        "Description": "I would like to add TSV (Tab Separated Value) input file type support to SequenceFilesFromDirectory.\nHere is my real use case:\nI have 36M records of input, each of which consists of ID and CONTENT and various other attributes, and I wanted to convert them to sequence files for clustering records by term vectors of CONTENT. However the problem is since I cannot create 36M files under my home directory due to quota limit that is up to 50k files, I was not able to convert them to sequence files by SequenceFilesFromDirectory utility... Meanwhile, source data format is TSV where each line of a file includes ID\\tCONTENT\\t... as it is suitable for Pig and most hadoop stream programs to process as input and output. NOTE: CONTENT size is up to around 2k bytes. Hence I feel better TSV support by SequenceFilesFromDirectory directly instead of taking two steps; TSV to text files and text files to Sequence files.\nI'm attaching the patch.\nHope this makes sense to other folks.",
        "Issue Links": []
    },
    "MAHOUT-591": {
        "Key": "MAHOUT-591",
        "Summary": "Model Dissector got broken over time.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "24/Jan/11 06:42",
        "Updated": "21/May/11 03:19",
        "Resolved": "03/Feb/11 12:56",
        "Description": "Chris Schilling reports:\n\nI changed the Weight subclass of the ModelDissector to sort by true value (rather than absolute value) and reran over the 20 newsgroups data.  Here are the results of the dissector function:\nbody=rt 0.042   comp.sys.mac.hardware\nbody=computer   0.039   sci.electronics\nbody=seem       0.035   talk.religion.misc\nbody=mike       0.035   misc.forsale\nbody=windows    0.034   misc.forsale\nbody=just       0.032   sci.crypt\nbody=supports   0.032   talk.politics.mideast\nbody=x  0.032   talk.religion.misc\nbody=do 0.029   rec.motorcycles\nbody=university 0.028   comp.sys.mac.hardware\nbody=slagle     0.028   rec.sport.hockey",
        "Issue Links": []
    },
    "MAHOUT-592": {
        "Key": "MAHOUT-592",
        "Summary": "Multiplication of two matrices that are distributed one of them is transpose A*A' gives the result of A*A",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Ahmed Nagy",
        "Created": "24/Jan/11 11:26",
        "Updated": "21/May/11 03:22",
        "Resolved": "24/Jan/11 16:55",
        "Description": "The problem basically is the result of two distributed matrices A and A' gives always A * A. I am attaching the code that produces the issue. If A is \n1.000 2.000 3.000\n4.000 5.000 6.000\n7.000 8.000 9.000 \nB=A'\n1.000 4.000 7.000\n2.000 5.000 8.000\n3.000 6.000 9.000\nResult should be \n14.000 32.000 50.000\n32.000 77.000 122.000\n50.000 122.000 194.000\nBut Result is \n30.0 36.0 42.0 \n66.0 81.0 96.0 \n102.0 126.0 150.0 \nwhich is the result of A*A",
        "Issue Links": []
    },
    "MAHOUT-593": {
        "Key": "MAHOUT-593",
        "Summary": "Backport of Stochastic SVD patch (Mahout-376) to hadoop 0.20 to ensure compatibility with current Mahout dependencies.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "25/Jan/11 01:02",
        "Updated": "21/May/11 03:18",
        "Resolved": "28/Mar/11 17:01",
        "Description": "Current Mahout-376 patch requries 'new' hadoop API.  Certain elements of that API (namely, multiple outputs) are not available in standard hadoop 0.20.2 release. As such, that may work only with either CDH or 0.21 distributions. \n In order to bring it into sync with current Mahout dependencies, a backport of the patch to 'old' API is needed. \nAlso, some work is needed to resolve math dependencies. Existing patch relies on apache commons-math 2.1 for eigen decomposition of small matrices. This dependency is not currently set up in the mahout core. So, certain snippets of code are either required to go to mahout-math or use Colt eigen decompositon (last time i tried, my results were mixed with that one. It seems to produce results inconsistent with those from mahout-math eigensolver, at the very least, it doesn't produce singular values in sorted order).\nSo this patch is mainly moing some Mahout-376 code around.",
        "Issue Links": [
            "/jira/browse/MAHOUT-376",
            "/jira/browse/MAHOUT-623"
        ]
    },
    "MAHOUT-594": {
        "Key": "MAHOUT-594",
        "Summary": "FileWriter may garble non-ASCII output if the environment variable LANG/LC_ALL is not appropriate.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Shige Takeda",
        "Created": "25/Jan/11 08:09",
        "Updated": "21/May/11 03:18",
        "Resolved": "26/Jan/11 23:22",
        "Description": "For non-ASCII output data, java.io.FileWriter should be replaced with java.io.OutputStreamWriter in UTF-8.\nFor example, if you dump centroids of clusters using ClusterDumper, you may get the following output:\n\n...\nC-0{n=2 c=[brown:2.099, c?t:1.957, dogs:1.916, fox:0.652, jumped:2.099, l?zy:1.884, over:2.099, quick:2.099, red:1.916, ?:0.871, ?:0.871, ?:0.871, ?:0.871] r=[c?t:0.652, fox:0.652, l?zy:1.131, ?:0.871, ?:0.871, ?:0.871, ?:0.871]}\n    Top Terms:\n        quick                                   =>  2.0986123085021973\n        over                                    =>  2.0986123085021973\n        jumped                                  =>  2.0986123085021973\n        brown                                   =>  2.0986123085021973\n        c?t                                     =>   1.957078456878662\n        red                                     =>  1.9162907600402832\n        dogs                                    =>  1.9162907600402832\n        l?zy                                    =>  1.8843144178390503\n        ?                                       =>  0.8706584572792053\n        ?                                       =>  0.8706584572792053\n    Weight:  Point:\n    1.0: P(0) = [brown:2.099, dogs:1.916, fox:2.609, jumped:2.099, over:2.099, quick:2.099, red:1.916, ?:2.322, ?:2.322, ?:2.322, ?:2.322]\n    1.0: P(1) = [brown:2.099, dogs:1.916, fox:2.609, jumped:2.099, over:2.099, quick:2.099, red:1.916, ?:2.322, ?:2.322, ?:2.322, ?:2.322]\n    1.0: P(2) = [brown:2.099, c?t:2.609, dogs:1.916, jumped:2.099, over:2.099, quick:2.099, red:1.916, ?:2.322, ?:2.322, ?:2.322, ?:2.322]\n...\n\n\nwhere \"?\" characters were garbled by FileWriter. NOTE: this test case is a tweaked version of TestClusterDumper. E.g., lazy => l\u00e4zy\nThe cause of this is the line in ClusterDumper.java:\n\nWriter writer = this.outputFile == null ? new OutputStreamWriter(System.out) : new FileWriter(this.outputFile);\n\n\nThis can be around by setting the environment variables LC_ALL/LANG to en_US.UTF-8, but many environments have LC_ALL/LANG=C by default, and in some cases, you even may not have choices but C for various reasons.\nTo address this issue, I would like to propose to hard code the output encoding to UTF-8 as follows:\n\nWriter writer = this.outputFile == null ? new OutputStreamWriter(System.out) : new OutputStreamWriter(new FileInputStream(this.outputFile), UTF8);\n\n\nThis way, the output file encoding will not be affected by environments.\nAnd if this proposal is agreed, a similar fix should be applied to the following files:\n\n./core/src/main/java/org/apache/mahout/classifier/sgd/ModelSerializer.java\n./core/src/test/java/org/apache/mahout/fpm/pfpgrowth/PFPGrowthTest.java\n./examples/src/main/java/org/apache/mahout/classifier/sgd/TrainLogistic.java\n./examples/src/main/java/org/apache/mahout/clustering/display/DisplaySpectralKMeans.java\n./utils/src/main/java/org/apache/mahout/clustering/lda/LDAPrintTopics.java\n./utils/src/main/java/org/apache/mahout/utils/SequenceFileDumper.java\n./utils/src/main/java/org/apache/mahout/utils/clustering/ClusterDumper.java\n./utils/src/main/java/org/apache/mahout/utils/vectors/VectorDumper.java\n./utils/src/main/java/org/apache/mahout/utils/vectors/arff/Driver.java\n./utils/src/main/java/org/apache/mahout/utils/vectors/lucene/ClusterLabels.java\n./utils/src/main/java/org/apache/mahout/utils/vectors/lucene/Driver.java\n\nHope not many folks prefer ISO-8859-1 or other 'legacy' character sets.",
        "Issue Links": []
    },
    "MAHOUT-595": {
        "Key": "MAHOUT-595",
        "Summary": "Installing and deploying source.jars with maven",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Manuel Me\u00dfner",
        "Created": "25/Jan/11 14:54",
        "Updated": "21/May/11 03:19",
        "Resolved": "25/Jan/11 15:31",
        "Description": "Hi Mahouts,\nI was missing source jars for mahout in my maven repository, so I'm adding a patch to this issue, which does the following:\n\nadds an <execution> to maven-source-plugin's configuration in /project/build/pluginManagement in the main pom.xml\nadds maven-source-plugin to core/pom.xml, examples/pom.xml, math/pom.xml and utils/pom.xml, without any further configuration.\n\nfor each mvn install or mvn deploy this will add an ${artifactId}-{$version}-sources.jar to the repository.\nSo Eclipse and other IDEs will be able to use those source jars to make \"source-code-hopping\" possible.\nThe given patch was created using revision 1063288.\nKind Regards,\nManuel Me\u00dfner",
        "Issue Links": []
    },
    "MAHOUT-596": {
        "Key": "MAHOUT-596",
        "Summary": "Testing if the weight assigned to points when calling the observe method in AbstractCluster incorrectly affect the number of points in a cluster",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Yuval Merhav",
        "Created": "27/Jan/11 06:00",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "17/Aug/11 20:45",
        "Description": "See the observe method in AbstractCluster:\npublic void observe(Vector x, double weight) \n{\n    s0 += weight;\n    Vector weightedX = x.times(weight);\n ....\n  }\n\nAnd then the computeParameters method:\npublic void computeParameters() {\n    ...\n numPoints = (int) s0;\n    ...\n}\nSo if someone changes the weight from the default value 1.0, it affects the number of points in the cluster. It does not\nhowever affect the centroid (which I'm not sure if that's correct or not \u2013 depends on what the author meant to use the weight for).\nI attached a few simple test cases that fail.",
        "Issue Links": []
    },
    "MAHOUT-597": {
        "Key": "MAHOUT-597",
        "Summary": "Kernels in Mean Shift",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Vasil Vasilev",
        "Created": "27/Jan/11 11:00",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 23:41",
        "Description": "Improve currently implemented variant of the mean shift algorithm in Mahout so that kernels are supported.",
        "Issue Links": []
    },
    "MAHOUT-598": {
        "Key": "MAHOUT-598",
        "Summary": "Downstream steps in the seq2sparse job flow looking in wrong location for output from previous steps when running in Elastic MapReduce (EMR) cluster",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Timothy Potter",
        "Created": "27/Jan/11 16:20",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 19:44",
        "Description": "While working on MAHOUT-588, I've discovered an issue with the seq2sparse job running on EMR. From what I can tell this job is made up of multiple MR steps and downstream steps are expecting output from previous steps to be in HDFS, but the output is in S3 (see errors below). For example, the DictionaryVectorizer wrote \"dictionary.file.0\" to S3 but TFPartialVectorReducer is looking for it in HDFS.\nTo run this job, I spin up an EMR cluster and then add the following step to it (this is using the elastic-mapreduce-ruby tool):\nelastic-mapreduce --jar s3n://thelabdude/mahout-core-0.4-job.jar \\\n--main-class org.apache.mahout.driver.MahoutDriver \\\n--arg seq2sparse \\\n--arg -i --arg s3n://thelabdude/asf-mail-archives/mahout-0.4/sequence-files-sm/ \\\n--arg -o --arg s3n://thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/ \\\n--arg --weight --arg tfidf \\\n--arg --chunkSize --arg 200 \\\n--arg --minSupport --arg 2 \\\n--arg --minDF --arg 1 \\\n--arg --maxDFPercent --arg 90 \\\n--arg --norm --arg 2 \\\n--arg --maxNGramSize --arg 2 \\\n--arg --overwrite \\\n-j JOB_ID\nWith these parameters, I see the following errors in the hadoop logs:\njava.io.FileNotFoundException: File does not exist: /asf-mail-archives/mahout-0.4/vectors-sm/dictionary.file-0\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:457)\n\tat org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:716)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1476)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1471)\n\tat org.apache.mahout.vectorizer.term.TFPartialVectorReducer.setup(TFPartialVectorReducer.java:126)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:575)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\njava.io.FileNotFoundException: File does not exist: /asf-mail-archives/mahout-0.4/vectors-sm/dictionary.file-0\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:457)\n\tat org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:716)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1476)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1471)\n\tat org.apache.mahout.vectorizer.term.TFPartialVectorReducer.setup(TFPartialVectorReducer.java:126)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:575)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\njava.io.FileNotFoundException: File does not exist: /asf-mail-archives/mahout-0.4/vectors-sm/dictionary.file-0\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:457)\n\tat org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:716)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1476)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1471)\n\tat org.apache.mahout.vectorizer.term.TFPartialVectorReducer.setup(TFPartialVectorReducer.java:126)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:575)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\nException in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: s3n://thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/partial-vectors-0\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:224)\n\tat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:55)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:241)\n\tat org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:933)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:827)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:432)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447)\n\tat org.apache.mahout.vectorizer.common.PartialVectorMerger.mergePartialVectors(PartialVectorMerger.java:126)\n\tat org.apache.mahout.vectorizer.DictionaryVectorizer.createTermFrequencyVectors(DictionaryVectorizer.java:176)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.main(SparseVectorsFromSequenceFiles.java:253)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:184)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nI don't think this is a \"config\" error on my side because if I change the -o argument to:\n/thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/ \nthen the job completes successfully, except the output is now stored in the hdfs and not S3. After the job completes successfully, if I SSH into the EMR master server, then I see the following output as expected:\nhadoop@ip-10-170-93-177:~$ hadoop fs -lsr /thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/\ndrwxr-xr-x   - hadoop supergroup          0 2011-01-24 23:44 /thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/df-count\nrw-rr-   1 hadoop supergroup      26893 2011-01-24 23:43 /thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/df-count/part-r-00000\nrw-rr-   1 hadoop supergroup      26913 2011-01-24 23:43 /thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/df-count/part-r-00001\nrw-rr-   1 hadoop supergroup      26893 2011-01-24 23:43 /thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/df-count/part-r-00002\nrw-rr-   1 hadoop supergroup     104874 2011-01-24 23:42 /thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/dictionary.file-0\nrw-rr-   1 hadoop supergroup      80493 2011-01-24 23:44 /thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/frequency.file-0\ndrwxr-xr-x   - hadoop supergroup          0 2011-01-24 23:43 /thelabdude/asf-mail-archives/mahout-0.4/vectors-sm/tf-vectors\n/part-r-00000\n...\nThe work-around is to just write all output to HDFS and then SSH into the master server once the job completes and then copy the output to S3.",
        "Issue Links": [
            "/jira/browse/MAHOUT-588"
        ]
    },
    "MAHOUT-599": {
        "Key": "MAHOUT-599",
        "Summary": "AbstractMatrix uses clone() where it should use like()",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "29/Jan/11 05:41",
        "Updated": "21/May/11 03:19",
        "Resolved": "29/Jan/11 05:59",
        "Description": "The AbstractMatrix.java class uses clone() instead of like() in the methods which create a new matrix and populate it with calculated data. This means that the contents of 'this' is copied into the return matrix, then promptly overwritten.\nThis patch fixes this problem by substituting like() and changing the code to not rely on having the values of 'this' in the clone.\nAlso, this allows generated matrices like a random matrix to reuse the code from AbstractMatrix; read-only matrices cannot populate their clones.",
        "Issue Links": []
    },
    "MAHOUT-600": {
        "Key": "MAHOUT-600",
        "Summary": "Reported test failure due to exact floating point comparison",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "29/Jan/11 19:45",
        "Updated": "21/May/11 03:19",
        "Resolved": "31/Jan/11 17:49",
        "Description": "Jeremy Lewi kindly pointed this result from his machine out to us:\n\nTest set: org.apache.mahout.math.VectorTest\n-------------------------------------------------------------------------------\nTests run: 20, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.045 sec <<< FAILURE!\ntestLogNormalize(org.apache.mahout.math.VectorTest)  Time elapsed: 0.002 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<{2:0.5345224838248488,1:0.4235990463273581,0:0.2672612419124244}> but was:<{2:0.5345224838248488,1:0.423599046327358,0:0.2672612419124244}>\n\tat org.junit.Assert.fail(Assert.java:91)\n\tat org.junit.Assert.failNotEquals(Assert.java:645)\n\tat org.junit.Assert.assertEquals(Assert.java:126)\n\tat org.junit.Assert.assertEquals(Assert.java:145)\n\tat org.apache.mahout.math.VectorTest.testLogNormalize(VectorTest.java:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\n\nI have a patch that cleans this up.  Will attach forthwith.",
        "Issue Links": []
    },
    "MAHOUT-601": {
        "Key": "MAHOUT-601",
        "Summary": "Syntax error when running build-reuters.sh script",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Frank Scholten",
        "Created": "30/Jan/11 13:56",
        "Updated": "21/May/11 03:19",
        "Resolved": "30/Jan/11 15:02",
        "Description": "When running examples/bin/build-reuters.sh from the root of the checked out mahout source tree I get the following syntax error:\nexamples/bin/build-reuters.sh: 28: Syntax error: \"(\" unexpected (expecting \"fi\")\nThis is because the script uses array initialization syntax supported by the bash shell, algorithm=( kmeans lda ), but it's shebang line uses the bourne shell: #!/bin/sh",
        "Issue Links": []
    },
    "MAHOUT-602": {
        "Key": "MAHOUT-602",
        "Summary": "\"Partial Implementation\" throws exceptions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Lance Norskog",
        "Created": "31/Jan/11 07:09",
        "Updated": "21/May/11 03:18",
        "Resolved": "04/Feb/11 23:13",
        "Description": "The \"Partial Implementation\" described on the wiki page Partial Implementation fails with the given dataset and operations.",
        "Issue Links": []
    },
    "MAHOUT-603": {
        "Key": "MAHOUT-603",
        "Summary": "Standardize implementation of log-likelihood",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3,                                            0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "31/Jan/11 07:37",
        "Updated": "21/May/11 03:18",
        "Resolved": "03/Feb/11 12:51",
        "Description": "We should use Ted's formulation of log-likelihood instead of the one that exists twice in the CF module. It doesn't change the resulting numbers, though does fix some symmetry issues in corner cases.",
        "Issue Links": []
    },
    "MAHOUT-604": {
        "Key": "MAHOUT-604",
        "Summary": "Bayes Classifier fails on data other than training data",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "02/Feb/11 04:39",
        "Updated": "21/May/11 03:18",
        "Resolved": "15/Feb/11 07:57",
        "Description": "The Bayes Classifier throws an exception when tested with different data than the training data.",
        "Issue Links": []
    },
    "MAHOUT-605": {
        "Key": "MAHOUT-605",
        "Summary": "Array returned by classifier.bayes.algorithm.CBayesAlgorithm.classifyDocument is sorted ascendant",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Swezey",
        "Created": "03/Feb/11 12:47",
        "Updated": "21/May/11 03:18",
        "Resolved": "26/Mar/11 13:51",
        "Description": "The array returned for a n-best call to classifyDocument is sorted ascendant instead of descendant. \nEx:\n\n47-best: [ClassifierResult{category='\u9999\u5ddd\u770c', score=32.28281232047167},\nClassifierResult{category='\u5bae\u5d0e\u770c', score=32.28969992600906}, ......,\nClassifierResult{category='\u611b\u77e5\u770c', score=32.487981016587796},\nClassifierResult{category='\u6771\u4eac\u90fd', score=32.49189358054859},\nClassifierResult{category='\u5317\u6d77\u9053', score=32.49811200756193}]\n(classification of documents for Japanese prefectures)\nInside the classifyDocument method, just before the return statement we found this line:\n\nCollections.reverse(result);\nIs this a mistake or a design choice? (we are not sure, hence the \"Minor\" priority)",
        "Issue Links": []
    },
    "MAHOUT-606": {
        "Key": "MAHOUT-606",
        "Summary": "Parallelize non-distributed ALSWRFactorizer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "03/Feb/11 20:47",
        "Updated": "09/Dec/11 09:17",
        "Resolved": "14/Feb/11 20:08",
        "Description": "Add multithreading to ALSWRFactorizer so that all available cores are used for the computation of the factorization",
        "Issue Links": []
    },
    "MAHOUT-607": {
        "Key": "MAHOUT-607",
        "Summary": "Count used and neglected elements in MaybePruneRowsMapper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "04/Feb/11 17:20",
        "Updated": "12/Aug/20 21:18",
        "Resolved": "09/Feb/11 11:10",
        "Description": "MaybePruneRowsMapper is used to prune the overall number of preferences/ratings our distributed recommender looks at. The patch adds counters to the code so we can see the number of used and neglected preferences in the jobtracker.",
        "Issue Links": []
    },
    "MAHOUT-608": {
        "Key": "MAHOUT-608",
        "Summary": "Collect various data directories in Mahout dir structure",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "05/Feb/11 11:40",
        "Updated": "21/May/11 03:18",
        "Resolved": "14/Feb/11 08:48",
        "Description": "The top-level project directory has collected, over time, a number of directories that have a generally similar purpose: to collect various config files, data files, and scripts. In addition toWe have, at first glance:\nbin/\n mahout\nconf/\n (various .props files)\netc/\n build.xml (reusable  Ant tasks?)\n findbugs-exclude.xml\n mahout.importorder\nmahout/\n conf/\n  arff.vector.props (wrong place?)\nsrc/\n main/\n  appended-resources/\n   META-INF/\n    NOTICE\n   supplemental-models.xml\n site/\n  site.xml\nThere are a few top-level generated directories:\ninput/\n ...\noutput/\n ...\ntestdata/\n transactions\n  test.txt\nI'd like to prune whatever isn't needed anymore, and rationalize one directory structure as a start.\nCan anyone help by suggesting things to be removed, or a directory structure?",
        "Issue Links": []
    },
    "MAHOUT-609": {
        "Key": "MAHOUT-609",
        "Summary": "Add an option to make RecommenderJob write out it's computed item similarities",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "08/Feb/11 16:55",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "18/Jan/13 10:59",
        "Description": "As RecommenderJob already includes the computation of similar items, we should an add option that makes it write out those similar items in the same format as ItemSimilarityJob does, so that users interested in both recommendations and similar items do not have to run two jobs.",
        "Issue Links": []
    },
    "MAHOUT-610": {
        "Key": "MAHOUT-610",
        "Summary": "Not all Coocurrences provided to SimilarityReducer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering,                                            Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Joris Geessels",
        "Created": "14/Feb/11 19:19",
        "Updated": "21/May/11 03:18",
        "Resolved": "22/Mar/11 13:11",
        "Description": "While doing some tests with the RecommenderJob, and more specifically the RowSimilarityJob, I noticed that in some cases not all cooccurences are used in the similarity calculations ( done in the SimilarityReducer class ).\nA RowPair object with (RowA=1,RowB=2) isn't considered the same as (RowA=2,RowB=1). This causes problems as CoocurencesMapper sometimes emits rowpairs in the first form and sometimes in the second form thus separating the cooccurences. If I'm right, this is due to the fact that ordering of the WeightedCoocurrenceArray for one column isn't guaranteed to be the same as for another column.\nThe solution is very simple, either you can change the compare method of the RowPair class or you can adapt the CooccurencesMapper to enforce that RowA < RowB.\nHope I've not missed something obvious, and that this is intended behavior. If this is the case, please enlighten me \nAlso, slightly off topic. While doing these tests, I've noticed that the predictions are all remarkably high and the RMSE on the movielens 100k dataset lies around 1,6.\nA bit to high if you ask me. Are these normal values or am I doing something wrong?",
        "Issue Links": []
    },
    "MAHOUT-611": {
        "Key": "MAHOUT-611",
        "Summary": "Please make taste-web working with mvn jetty:run (org.apache.mahout.cf.taste.common.TasteException: java.lang.ClassNotFoundException: ${recommender.class})",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Manuel Blechschmidt",
        "Created": "19/Feb/11 17:46",
        "Updated": "21/May/11 03:22",
        "Resolved": "20/Feb/11 16:00",
        "Description": "When trying to start the integrated jetty server in taste-web it can not load the class: \"${recommender.class}\". When checking out trunk it should be possible to directly start up the taste-web application with: mvn jetty:run\nComplete command line output:\n$ pwd\n/Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web\ndynamic218:taste-web manuel$ mvn jetty:run\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Building Mahout Taste Webapp\n[INFO]    task-segment: [jetty:run]\n[INFO] ------------------------------------------------------------------------\n[INFO] Preparing jetty:run\n[INFO] [resources:resources \n{execution: default-resources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] Copying 1 resource\n[INFO] Copying 0 resource to /Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web/target/mahout-taste-webapp-0.5-SNAPSHOT/WEB-INF/lib\n[INFO] [compiler:compile \n{execution: default-compile}\n]\n[INFO] Nothing to compile - all classes are up to date\n[INFO] [resources:testResources \n{execution: default-testResources}\n]\n[INFO] Using 'UTF-8' encoding to copy filtered resources.\n[INFO] skip non existing resourceDirectory /Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web/src/test/resources\n[INFO] [compiler:testCompile \n{execution: default-testCompile}\n]\n[INFO] Nothing to compile - all classes are up to date\n[INFO] [jetty:run \n{execution: default-cli}\n]\n[INFO] Configuring Jetty for project: Mahout Taste Webapp\n[INFO] webAppSourceDirectory /Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web/src/main/webapp does not exist. Defaulting to /Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web/src/main/webapp\n[INFO] Reload Mechanic: automatic\n[INFO] Classes = /Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web/target/classes\n2011-02-19 18:41:49.989:INFO::Logging to StdErrLog::DEBUG=false via org.eclipse.jetty.util.log.StdErrLog\n[INFO] Context path = /\n[INFO] Tmp directory = /Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web/target/tmp\n[INFO] Web defaults = org/eclipse/jetty/webapp/webdefault.xml\n[INFO] Web overrides =  none\n[INFO] web.xml file = file:/Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web/src/main/webapp/WEB-INF/web.xml\n[INFO] Webapp directory = /Users/manuel/Projects/Masterarbeit/workspace/Mahout/taste-web/src/main/webapp\n[INFO] Starting jetty 7.1.2.v20100523 ...\n2011-02-19 18:41:50.077:INFO::jetty-7.1.2.v20100523\n2011-02-19 18:41:50.770:INFO::No Transaction manager found - if your webapp requires one, please configure one.\n2011-02-19 18:41:50.846:WARN:/:unavailable\norg.apache.mahout.cf.taste.common.TasteException: java.lang.ClassNotFoundException: ${recommender.class}\n\tat org.apache.mahout.cf.taste.web.RecommenderSingleton.<init>(RecommenderSingleton.java:53)\n\tat org.apache.mahout.cf.taste.web.RecommenderSingleton.initializeIfNeeded(RecommenderSingleton.java:42)\n\tat org.apache.mahout.cf.taste.web.RecommenderServlet.init(RecommenderServlet.java:74)\n\tat org.eclipse.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:421)\n\tat org.eclipse.jetty.servlet.ServletHolder.doStart(ServletHolder.java:245)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:691)\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:192)\n\tat org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:995)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:579)\n\tat org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:381)\n\tat org.mortbay.jetty.plugin.JettyWebAppContext.doStart(JettyWebAppContext.java:114)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:165)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.doStart(ContextHandlerCollection.java:162)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.doStart(HandlerCollection.java:165)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:92)\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:226)\n\tat org.mortbay.jetty.plugin.JettyServer.doStart(JettyServer.java:67)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:55)\n\tat org.mortbay.jetty.plugin.AbstractJettyMojo.startJetty(AbstractJettyMojo.java:433)\n\tat org.mortbay.jetty.plugin.AbstractJettyMojo.execute(AbstractJettyMojo.java:377)\n\tat org.mortbay.jetty.plugin.JettyRunMojo.execute(JettyRunMojo.java:577)\n\tat org.apache.maven.plugin.DefaultPluginManager.executeMojo(DefaultPluginManager.java:490)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoals(DefaultLifecycleExecutor.java:694)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeStandaloneGoal(DefaultLifecycleExecutor.java:569)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoal(DefaultLifecycleExecutor.java:539)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeGoalAndHandleFailures(DefaultLifecycleExecutor.java:387)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.executeTaskSegments(DefaultLifecycleExecutor.java:348)\n\tat org.apache.maven.lifecycle.DefaultLifecycleExecutor.execute(DefaultLifecycleExecutor.java:180)\n\tat org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:328)\n\tat org.apache.maven.DefaultMaven.execute(DefaultMaven.java:138)\n\tat org.apache.maven.cli.MavenCli.main(MavenCli.java:362)\n\tat org.apache.maven.cli.compat.CompatibleMain.main(CompatibleMain.java:60)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.codehaus.classworlds.Launcher.launchEnhanced(Launcher.java:315)\n\tat org.codehaus.classworlds.Launcher.launch(Launcher.java:255)\n\tat org.codehaus.classworlds.Launcher.mainWithExitCode(Launcher.java:430)\n\tat org.codehaus.classworlds.Launcher.main(Launcher.java:375)\n2011-02-19 18:41:50.873:INFO::Started SelectChannelConnector@0.0.0.0:8080\n[INFO] Started Jetty Server\n^C[INFO] Jetty server exiting.\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD SUCCESSFUL\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 14 seconds\n[INFO] Finished at: Sat Feb 19 18:42:00 CET 2011\n$",
        "Issue Links": []
    },
    "MAHOUT-612": {
        "Key": "MAHOUT-612",
        "Summary": "Simplify configuring and running Mahout MapReduce jobs from Java using Java bean configuration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "None",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Frank Scholten",
        "Created": "20/Feb/11 15:21",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 16:24",
        "Description": "Most of the Mahout features require running several jobs in sequence. This can be done via the command line or using one of the driver classes.\nRunning and configuring a Mahout job from Java requires using either the Driver's static methods or creating a String array of parameters and pass them to the main method of the job. If we can instead configure jobs through a Java bean or factory we it will be type safe and easier to use in by DI frameworks such as Spring and Guice.\nI have added a patch where I factored out a KMeans MapReduce job plus a configuration Java bean, from KMeansDriver.buildClustersMR(...)\n\nThe KMeansMapReduceConfiguration takes care of setting up the correct values in the Hadoop Configuration object and initializes defaults. I copied the config keys from KMeansConfigKeys.\nThe KMeansMapReduceJob contains the code for the actual algorithm running all iterations of KMeans and returns the KMeansMapReduceConfiguration, which contains the cluster path for the final iteration.\n\nI like to extend this approach to other Hadoop jobs for instance the job for creating points in KMeansDriver, but I first want some feedback on this. \nOne of the benefits of this approach is that it becomes easier to chain jobs. For instance we can chain Canopy to KMeans by connecting the output dir of Canopy's configuration to the input dir of the configuration of the KMeans job next in the chain. Hadoop's JobControl class can then be used to connect and execute the entire chain.\nThis approach can be further improved by turning the configuration bean into a factory for creating MapReduce or sequential jobs. This would probably remove some duplicated code in the KMeansDriver.",
        "Issue Links": []
    },
    "MAHOUT-613": {
        "Key": "MAHOUT-613",
        "Summary": "Added descriptive messages to calls to Preconditions.checkNotNull()",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "20/Feb/11 15:42",
        "Updated": "21/May/11 03:18",
        "Resolved": "21/Feb/11 06:36",
        "Description": "Noticed a few Preconditions.notNullCheck calls without a message so I added them.",
        "Issue Links": []
    },
    "MAHOUT-614": {
        "Key": "MAHOUT-614",
        "Summary": "org.apache.mahout.classifier.baytes.MultipleOutputFormat not working as intended with Hadoop 0.20?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Sean R. Owen",
        "Created": "21/Feb/11 11:00",
        "Updated": "21/May/11 03:18",
        "Resolved": "22/Feb/11 21:17",
        "Description": "I believe there might be an error in org.apache.mahout.classifier.baytes.MultipleOutputFormat. It overrides the Hadoop class FileOutputFormat, and most of its work is done in getRecordWriter(FileSystem, Configuration, String, Progressable). However this is not the method that one must override to control how FileOutputFormat writes records; that's getRecordWriter(TaskAttemptContext). My hunch is that this used to work, but against the Hadoop 0.19.x APIs. (@Override is our friend!)\nI've attached a patch that I believe addresses this and along the way is able to clean things up slightly. Am I on track here?",
        "Issue Links": []
    },
    "MAHOUT-615": {
        "Key": "MAHOUT-615",
        "Summary": "unable to instanciate FastByIDMap<PreferenceArray>, ClassNotFoundException for com.google.common.base.Preconditions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "collections",
        "Assignee": "Sean R. Owen",
        "Reporter": "bernd hopp",
        "Created": "01/Mar/11 12:00",
        "Updated": "21/May/11 03:22",
        "Resolved": "01/Mar/11 12:03",
        "Description": "this code:\n FastByIDMap<PreferenceArray> map = new FastByIDMap<PreferenceArray>();   \nthrows an java.lang.ClassNotFoundException, class com.google.common.base.Preconditions cannot be found.\nThe error is thrown in FastByIdMap, ln 76, where \"Preconditions\" cannot be found. In this project, im using the google web toolkit, which is shipped with the Preconditions-class, what propably confuses the class-loader. Anyhow, when im trying the line shown above in netbeans, without any references to gwt, it fails with the very same exception. \nAny questions, suggestions, workarounds etc. appreciated, thanks.",
        "Issue Links": []
    },
    "MAHOUT-616": {
        "Key": "MAHOUT-616",
        "Summary": "Cannot use MahalanobisDistanceMeasure with Dirichlet clustering",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Vasil Vasilev",
        "Created": "06/Mar/11 13:09",
        "Updated": "21/May/11 03:19",
        "Resolved": "20/Mar/11 15:54",
        "Description": "When Dirichlet clustering is run with DistanceMeasureClusterDistribution and MahalanobisDistanceMeasure the configure method of the distance measure is not called. In this way the configuration data cannot be supplied to the distance measure. In addition there is a bug in the MahalanobisDistanceMeasure that does not take into account the matrix class",
        "Issue Links": []
    },
    "MAHOUT-617": {
        "Key": "MAHOUT-617",
        "Summary": "FPGrowth/PFPGrowth giving out wrong results.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Vipul Pandey",
        "Created": "07/Mar/11 03:10",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "13/Mar/11 18:56",
        "Description": "FPGrowth reports the support of itemsets individually - in that - if Item X appears \"individually\" 12 times and appears with item Y 10 times (a total of 22 times) AND item Y appears \"individually\" 4 times (a total of 14 times) then this is what the output will be (say for min-support 2)\n12 X\n10 XY\n4  Y\nInstead of \n22 X\n10 XY\n14 Y\nAlso, because of this If the minimum support is 5 then the output will look like : \n12 X\n10 X Y\nThus totally Ignoring Y\nif the minimum support is 11 then the output will look like \n12 X\nagain Ignoring Y\nif the minimum support is 13 then there will be NO output. even though all the way along Xs support was 22 and Y's was 14\nEven if we want to show just the maximal itemsets (although i would like to see ALL the frequent itemsets - maximal or not) this output is wrong as with a support of 13 we should still have seen X(22) and Y(14)\nNow Say you add XYZ 11 times\nfor support 1 you'd see\n12 X\n10 X Y\n11 X Y Z\n4   Y\nAnd for support 11 you'd see\n12 X\n11 X Y Z\nAlthough I'd expect the output (for both s=1 & s=11) to be \n33 X\n25 Y \n21 XY\n11 Z\n11 XZ\n11 YZ\n11 XYZ\nattached are the sample inputs:",
        "Issue Links": []
    },
    "MAHOUT-618": {
        "Key": "MAHOUT-618",
        "Summary": "mahout-math's pom.xml should be configured to compile in Java6",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Daisuke Miyamoto",
        "Created": "07/Mar/11 07:20",
        "Updated": "21/May/11 03:19",
        "Resolved": "08/Mar/11 01:51",
        "Description": "Though Mahout requires JDK6, mahout-math's pom.xml contains plugin configuration which direct compile for Java5 as following:\n\n     <plugin>\n       <groupId>org.apache.maven.plugins</groupId>\n       <artifactId>maven-compiler-plugin</artifactId>\n       <configuration>\n         <source>1.5</source>\n         <target>1.5</target>\n       </configuration>\n     </plugin>\n\n\nFurthermore RandomWrapper and DistributionChecks uses String#getBytes(Charset) and Arrays#copyOf() method.\nThese methods are not supported in JDK5.",
        "Issue Links": []
    },
    "MAHOUT-619": {
        "Key": "MAHOUT-619",
        "Summary": "Add normalized discounted cumulative gain to recommender evaluator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "07/Mar/11 08:35",
        "Updated": "21/May/11 03:19",
        "Resolved": "07/Mar/11 08:36",
        "Description": "Just tracking a simple improvement: add nDCG to precision and recall computations in GenericRecommenderIRStatsEvaluator",
        "Issue Links": []
    },
    "MAHOUT-620": {
        "Key": "MAHOUT-620",
        "Summary": "Follow up on MAHOUT-541",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tamas Jambor",
        "Created": "09/Mar/11 11:39",
        "Updated": "21/May/11 03:22",
        "Resolved": "09/Mar/11 11:42",
        "Description": "I have made some of the changes I suggested for MAHOUT-541.\n1, some bug fixes\n2, changed the way random noise was added\n3, when to shuffle the preference values",
        "Issue Links": []
    },
    "MAHOUT-621": {
        "Key": "MAHOUT-621",
        "Summary": "Support more data import mechanisms",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "09/Mar/11 13:05",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "15/Oct/11 10:07",
        "Description": "We should have more ways of getting data in:\n1. ARFF (MAHOUT-155)\n2. CSV (MAHOUT-548)\n3. Databases\n4. Behemoth (Tika, Map-Reduce)\n5. Other",
        "Issue Links": []
    },
    "MAHOUT-622": {
        "Key": "MAHOUT-622",
        "Summary": "Mahout dependencies are unified under dependency management in parent pom",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "10/Mar/11 06:44",
        "Updated": "10/Jul/13 11:46",
        "Resolved": "05/Apr/11 10:27",
        "Description": "As far as I understand, Maven encourages \"best practice\" of unified view of dependency versions specified under <dependencyManagement> usually in a parent pom, instead of under <dependencies>. \nIn Mahout, this practice is only partially followed. Some dependencies have concrete versions under <dependencies> tag in submodule poms. Proposed change is to raid those and move version declarations into parent pom. \nThis (as far as i understand) achieves 2 things: \n\nMahout assembly would include same versions for all modules thus ensuring runtime module dependencies are the same as compile time;\nSomebody who uses Mahout as a dependency, could import Mahout dependencies using <scope>import</scope> spec thus inheriting Mahout's versions for shared dependencies.\n\nFor most part the change would be nominal although in certain cases we'd need to sort out through cross-module conflicts (if any). Commons-math was one, not sure if there are more. If there are none, the changes would be rather mechanistic.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1277"
        ]
    },
    "MAHOUT-623": {
        "Key": "MAHOUT-623",
        "Summary": "Bug/improvement: add \"overwrite\" option to Stochastic SVD command line and API",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "10/Mar/11 23:58",
        "Updated": "21/May/11 03:19",
        "Resolved": "20/Mar/11 20:51",
        "Description": "I actually stumbled on the fact that SSVD code doesn't have 'overwrite results' option. Which might not be not a big deal if it weren't for the fact that it also turns out to be the default behavior as well (i.e. by default it would overwrite target path if it already exists). Which promotes it to a verge of a bug. \nI think i need to attend to it by introducing overwrite option and disabling this by default. (i.e. by default it would fail if the output path already exists). I think this is consistent with some other CLI in Mahout I saw.",
        "Issue Links": [
            "/jira/browse/MAHOUT-593",
            "/jira/browse/MAHOUT-376"
        ]
    },
    "MAHOUT-624": {
        "Key": "MAHOUT-624",
        "Summary": "Documentation links broken on mahout.apache site",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Ted Dunning",
        "Created": "11/Mar/11 05:13",
        "Updated": "21/May/11 03:19",
        "Resolved": "11/Mar/11 10:38",
        "Description": "All of our links to hudson have failed.  The new link for the documentation should be based at\nhttps://builds.apache.org/hudson/job/Mahout-Quality/\nI am not sure how to change or push the site.  Otherwise, I would have.",
        "Issue Links": []
    },
    "MAHOUT-625": {
        "Key": "MAHOUT-625",
        "Summary": "Some of generated patterns have support higher than in reality",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Jaroslaw Odzga",
        "Created": "11/Mar/11 15:24",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "21/May/11 03:06",
        "Description": "It turnes out that some of generated patterns have incorrect support. The returned support is slightly higher than the true one.\nI attached the test, which proves that FPGrowth has a bug. Test is using data (retail) found here: http://fimi.ua.ac.be/data/\nThe pattern (36, 39, 41) occurs in the transactions 572 times (this is also calculated in test), but the FPGrowth returns pattern (36, 39, 41) with support 573.\nPlease note that mentioned pattern is not the only one with incorrect support - the test only point out one example to hace something to focus on. There is plenty more patterns with support higher than the real one. The biggest difference I noticed was support 8 higher than the real one for one of patterns.\nPlease find attached failing unit test - it's actually a maven project, which contains test data and is ready to run.",
        "Issue Links": []
    },
    "MAHOUT-626": {
        "Key": "MAHOUT-626",
        "Summary": "T1 and T2 Values in Canopy (& MeanShift)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "13/Mar/11 00:23",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "17/Aug/11 20:49",
        "Description": "Users are reporting that the T1 and T2 threshold values which work in sequential mode don't work as well in the mapreduce mode because both the mapper and reducer are using the same values. The effect of coalescing a number of points into a single centroid done by the mapper changes the distances enough that independent threshold values are needed in the reducer. \nHere is a patch which implements optional T3 and T4 threshold values which are only used by the canopy reducer. Convenience methods have been added for API compatibility and defaults included so that these values will default to T1 and T2. A new unit test confirms the thresholds are being set correctly.\nIf this works out as a positive improvement, I will make the same changes to MeanShift and commit them",
        "Issue Links": []
    },
    "MAHOUT-627": {
        "Key": "MAHOUT-627",
        "Summary": "Baum-Welch Algorithm on Map-Reduce for Parallel Hidden Markov Model Training.",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Dhruv Kumar",
        "Created": "17/Mar/11 15:59",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "08/Mar/14 12:03",
        "Description": "Proposal Title: Baum-Welch Algorithm on Map-Reduce for Parallel Hidden Markov Model Training. \nStudent Name: Dhruv Kumar \nStudent E-mail: dkumar@ecs.umass.edu \nOrganization/Project: Apache Mahout \nAssigned Mentor: \nProposal Abstract: \nThe Baum-Welch algorithm is commonly used for training a Hidden Markov Model because of its superior numerical stability and its ability to guarantee the discovery of a locally maximum,  Maximum Likelihood Estimator, in the presence of incomplete training data. Currently, Apache Mahout has a sequential implementation of the Baum-Welch which cannot be scaled to train over large data sets. This restriction reduces the quality of training and constrains generalization of the learned model when used for prediction. This project proposes to extend Mahout's Baum-Welch to a parallel, distributed version using the Map-Reduce programming framework for enhanced model fitting over large data sets. \nDetailed Description: \nHidden Markov Models (HMMs) are widely used as a probabilistic inference tool for applications generating temporal or spatial sequential data. Relative simplicity of implementation, combined with their ability to discover latent domain knowledge have made them very popular in diverse fields such as DNA sequence alignment, gene discovery, handwriting analysis, voice recognition, computer vision, language translation and parts-of-speech tagging. \nA HMM is defined as a tuple (S, O, Theta) where S is a finite set of unobservable, hidden states emitting symbols from a finite observable vocabulary set O according to a probabilistic model Theta. The parameters of the model Theta are defined by the tuple (A, B, Pi) where A is a stochastic transition matrix of the hidden states of size |S| X |S|. The elements a_(i,j) of A specify the probability of transitioning from a state i to state j. Matrix B is a size |S| X |O| stochastic symbol emission matrix whose elements b_(s, o) provide the probability that a symbol o will be emitted from the hidden state s. The elements pi_(s) of the |S| length vector Pi determine the probability that the system starts in the hidden state s. The transitions of hidden states are unobservable and follow the Markov property of memorylessness. \nRabiner [1] defined three main problems for HMMs: \n1. Evaluation: Given the complete model (S, O, Theta) and a subset of the observation sequence, determine the probability that the model generated the observed sequence. This is useful for evaluating the quality of the model and is solved using the so called Forward algorithm. \n2. Decoding: Given the complete model (S, O, Theta) and an observation sequence, determine the hidden state sequence which generated the observed sequence. This can be viewed as an inference problem where the model and observed sequence are used to predict the value of the unobservable random variables. The backward algorithm, also known as the Viterbi decoding algorithm is used for predicting the hidden state sequence. \n3. Training: Given the set of hidden states S, the set of observation vocabulary O and the observation sequence, determine the parameters (A, B, Pi) of the model Theta. This problem can be viewed as a statistical machine learning problem of model fitting to a large set of training data. The Baum-Welch (BW) algorithm (also called the Forward-Backward algorithm) and the Viterbi training algorithm are commonly used for model fitting. \nIn general, the quality of HMM training can be improved by employing large training vectors but currently, Mahout only supports sequential versions of HMM trainers which are incapable of scaling.  Among the Viterbi and the Baum-Welch training methods, the Baum-Welch algorithm is superior, accurate, and a better candidate for a parallel implementation for two reasons:\n(1) The BW is numerically stable and provides a guaranteed discovery of the locally maximum, Maximum Likelihood Estimator (MLE) for model's parameters (Theta). In Viterbi training, the MLE is approximated in order to reduce computation time. \n(2) The BW belongs to the general class of Expectation Maximization (EM) algorithms which naturally fit into the Map-Reduce framework [2], such as the existing Map Reduce implementation of k-means in Mahout. \nHence, this project proposes to extend Mahout's current sequential implementation of the Baum-Welch HMM trainer to a scalable, distributed case. Since the distributed version of the BW will use the sequential implementations of the Forward and the Backward algorithms to compute the alpha and the beta factors in each iteration, a lot of existing HMM training code will be reused. Specifically, the parallel implementation of the BW algorithm on Map Reduce has been elaborated at great length in [3] by viewing it as a specific case of the Expectation-Maximization algorithm and will be followed for implementation in this project. \nThe BW EM algorithm iteratively refines the model's parameters and consists of two distinct steps in each iteration--Expectation and Maximization. In the distributed case, the Expectation step is computed by the mappers and the reducers, while the Maximization is handled by the reducers. Starting from an initial Theta^(0), in each iteration i, the model parameter tuple Theta^i is input to the algorithm, and the end result Theta^(i+1) is fed to the next iteration i+1. The iteration stops on a user specified convergence condition expressed as a fixpoint or when the number of iterations exceeds a user defined value. \nExpectation computes the posterior probability of each latent variable for each observed variable, weighed by the relative frequency of the observed variable in the input split. The mappers process independent training instances and emit expected state transition and emission counts using the Forward and Backward algorithms. The reducers finish Expectation by aggregating the expected counts. The input to a mapper consists of (k, v_o) pairs where k is a unique key and v_o is a string of observed symbols. For each training instance, the mappers emit the same set of keys corresponding to the following three optimization problems to be solved during the Maximization, and their values in a hash-map:\n(1) Expected number of times a hidden state is reached (Pi).\n(2) Number of times each observable symbol is generated by each hidden state (B).\n(3) Number of transitions between each pair of states in the hidden state space (A). \nThe M step computes the updated Theta(i+1) from the values generated during the E part. This involves aggregating the values (as hash-maps) for each key corresponding to one of the optimization problems. The aggregation summarizes the statistics necessary to compute a subset of the parameters for the next EM iteration. The optimal parameters for the next iteration are arrived by computing the relative frequency of each event with respect to its expected count at the current iteration. The emitted optimal parameters by each reducer are written to the HDFS and are fed to the mappers in the next iteration. \nThe project can be subdivided into distinct tasks of programming, testing and documenting the driver, mapper, reducer and the combiner with the Expectation and Maximization parts split between them. For each of these tasks, a new class will be programmed, unit tested and documented within the org.apache.mahout.classifier.sequencelearning.hmm package. Since k-means is also an EM algorithm, particular attention will be paid to its code at each step for possible reuse.\nA list of milestones, associated deliverable and high level implementation details is given below. \nTime-line: April 26 - Aug 15. \nMilestones: \nApril 26 - May 22 (4 weeks): Pre-coding stage. Open communication with my mentor, refine the project's plan and requirements, understand the community's code styling requirements, expand the knowledge on Hadoop and Mahout internals. Thoroughly familiarize with the classes within the classifier.sequencelearning.hmm, clustering.kmeans, common, vectorizer and math packages. \nMay 23 - June 3 (2 weeks): Work on Driver. Implement, test and document the class HmmDriver by extending the AbstractJob class and by reusing the code from the KMeansDriver class. \nJune 3 - July 1 (4 weeks): Work on Mapper. Implement, test and document the class HmmMapper. The HmmMapper class will include setup() and map() methods. The setup() method will read in the HmmModel and the parameter values obtained from the previous iteration. The map() method will call the HmmAlgorithms.backwardAlgorithm() and the HmmAlgorithms.forwardAlgorithm() and complete the Expectation step partially. \nJuly 1 - July 15 (2 weeks): Work on Reducer. Implement, test and document the class HmmReducer. The reducer will complete the Expectation step by summing over all the occurences emitted by the mappers for the three optimization problems. Reuse the code from the HmmTrainer.trainBaumWelch() method if possible. Also, mid-term review.\nJuly 15 - July 29 (2 weeks): Work on Combiner. Implement, test and document the class HmmCombiner. The combiner will reduce the network traffic and improve efficiency by aggregating the values for each of the three keys corresponding to each of the optimization problems for the Maximization stage in reducers. Look at the possibility of code reuse from the KMeansCombiner class. \nJuly 29 - August 15 (2 weeks): Final touches. Test the mapper, reducer, combiner and driver together. Give an example demonstrating the new parallel BW algorithm by employing the parts-of-speech tagger data set also used by the sequential BW [4]. Tidy up code and fix loose ends, finish wiki documentation. \nAdditional Information: \nI am in the final stages of finishing my Master's degree in Electrical and Computer Engineering from the University of Massachusetts Amherst. Working under the guidance of Prof. Wayne Burleson, as part of my Master's research work, I have applied the theory of Markov Decision Process (MDP) to increase the duration of service of mobile computers. This semester I am involved with two course projects involving machine learning over large data sets. In the Bioinformatics class, I am mining the RCSB Protein Data Bank [5] to learn the dependence of side chain geometry on a protein's secondary structure, and comparing it with the Dynamic Bayesian Network approach used in [6]. In another project for the Online Social Networks class, I am using reinforcement learning to build an online recommendation system by reformulating MDP optimal policy search as an EM problem [7] and employing Map Reduce (extending Mahout) to arrive at it in a scalable, distributed manner. \nI owe much to the open source community as all my research experiments have only been possible due to the freely available Linux distributions, performance analyzers, scripting languages and associated documentation. After joining the Apache Mahout's developer mailing list a few weeks ago,  I have found the community extremely vibrant, helpful and welcoming. If selected, I feel that the GSOC 2011 project will be a great learning experience for me from both a technical and professional standpoint and will also allow me to contribute within my modest means to the overall spirit of open source programming and Machine Learning. \nReferences: \n[1] A tutorial on hidden markov models and selected applications in speech recognition by Lawrence R. Rabiner. In Proceedings of the IEEE, Vol. 77 (1989), pp. 257-286. \n[2] Map-Reduce for Machine Learning on Multicore by Cheng T. Chu, Sang K. Kim, Yi A. Lin, Yuanyuan Yu, Gary R. Bradski, Andrew Y. Ng, Kunle Olukotun. In NIPS (2006), pp. 281-288. \n[3] Data-Intensive Text Processing with MapReduce by Jimmy Lin, Chris Dyer. Morgan & Claypool 2010. \n[4] http://flexcrfs.sourceforge.net/#Case_Study \n[5] http://www.rcsb.org/pdb/home/home.do\n[6] Beyond rotamers: a generative, probabilistic model of side chains in proteins by Harder T, Boomsma W, Paluszewski M, Frellsen J, Johansson KE, Hamelryck T. BMC Bioinformatics. 2010 Jun 5.\n[7] Probabilistic inference for solving discrete and continuous state Markov Decision Processes by M. Toussaint and A. Storkey. ICML, 2006.",
        "Issue Links": []
    },
    "MAHOUT-628": {
        "Key": "MAHOUT-628",
        "Summary": "Add an option to prune away users with less than a given number of preferences to ItemSimilarityJob and RecommenderJob",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "18/Mar/11 19:23",
        "Updated": "21/May/11 03:18",
        "Resolved": "22/Mar/11 21:48",
        "Description": "Some real-world datasets (especially those created from implicit feedback) might include users with only a tiny number of preferences (like one-time-visitors only viewing a single item) that a users of ItemSimilarityJob or RecommenderJob might want to prune away. I added a new parameter \"minPrefsPerUser\" that makes those jobs throw out users with less than a given number of preferences. It is per default set to 1 so that the input data stays untouched.\nIt's just a small patch to make those jobs more usable in real-world scenarios.",
        "Issue Links": []
    },
    "MAHOUT-629": {
        "Key": "MAHOUT-629",
        "Summary": "FP Growth performance improvement",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Jaroslaw Odzga",
        "Created": "21/Mar/11 10:32",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "09/Feb/12 13:54",
        "Description": "Instead of calculating patterns ending with given attribute multiple times they can be calculated just once. Depending on for how many features patterns are generated, speedup can be huge. More feature included - greater speedup. For test data set (88162 real life 'basket' transactions), if all features were selected (i.e. we want to generate patterns for all items in transactions), patterns generation time dropped from 1h 15min to 8sec. For parallel fpgrowth, where the number of requested features is limited the speedup is not that dramatic, but still noticeable. Basically work done is always smaller than before the patch (as patterns for each item are calculated at most once).\nThe improvement is a variation of base algorithm in situation where we want to generate patterns for only subset of items (let's call this subset A). Given that items are ordered by descending frequency it is enough to calculate only patterns ending on any item with frequency smaller or equal to the most frequent item in the subset A. The heaps for each item are initialized upfront and merged after processing every item.",
        "Issue Links": [
            "/jira/browse/MAHOUT-709"
        ]
    },
    "MAHOUT-630": {
        "Key": "MAHOUT-630",
        "Summary": "Small bug in WeightedRunningAverage; add WeightedRunningAverageAndStdDev",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "22/Mar/11 15:35",
        "Updated": "21/May/11 03:18",
        "Resolved": "22/Mar/11 16:15",
        "Description": "There's a problem in the WeightedRunningAverage handling of a new weighted datum. While looking at that I said, hey, we could use WeightedRunningAverageAndStdDev to go with it. And some tests.",
        "Issue Links": []
    },
    "MAHOUT-631": {
        "Key": "MAHOUT-631",
        "Summary": "Small bug in WeightedRunningAverage; add WeightedRunningAverageAndStdDev",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "22/Mar/11 15:35",
        "Updated": "21/May/11 03:18",
        "Resolved": "22/Mar/11 15:42",
        "Description": "There's a problem in the WeightedRunningAverage handling of a new weighted datum. While looking at that I said, hey, we could use WeightedRunningAverageAndStdDev to go with it. And some tests.",
        "Issue Links": []
    },
    "MAHOUT-632": {
        "Key": "MAHOUT-632",
        "Summary": "PFPGrowth : Exceeded max jobconf size",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Vipul Pandey",
        "Created": "22/Mar/11 20:01",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "22/Jun/11 22:29",
        "Description": "I'm getting this error right after startParallelCounting finishes :\n11/03/21 19:06:40 INFO mapred.JobClient:     Map output records=164272900\n11/03/21 19:06:40 INFO mapred.JobClient:     SPLIT_RAW_BYTES=2860\n11/03/21 19:06:40 INFO mapred.JobClient:     Reduce input records=67087840\n11/03/21 19:07:02 INFO pfpgrowth.PFPGrowth: No of Features: 1788471\n11/03/21 19:07:09 WARN mapred.JobClient: Use GenericOptionsParser for\nparsing the arguments. Applications should implement Tool for the same.\n11/03/21 19:07:12 INFO input.FileInputFormat: Total input paths to process :\n20\n11/03/21 19:07:17 INFO mapred.JobClient: Cleaning up the staging area\nhdfs://nccc001:54310/mnt/analytics/data/hadoop/tmp/mapred/staging/isapps/.staging/job_201103101218_0287\nException in thread \"main\" org.apache.hadoop.ipc.RemoteException:\njava.io.IOException: java.io.IOException: Exceeded max jobconf size:\n72276915 limit: 52428800\nat org.apache.hadoop.mapred.JobTracker.submitJob(JobTracker.java:3759)\nat sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)\nat\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:557)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1416)\nat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1412)\nQuoting Robin :  \"I guess we just hit the limit of storing flist in the conf. Moving it do the distributed cache should fix this.\"",
        "Issue Links": []
    },
    "MAHOUT-633": {
        "Key": "MAHOUT-633",
        "Summary": "Add SequenceFileIterable; put Iterable stuff in one place",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "23/Mar/11 18:44",
        "Updated": "21/May/11 03:19",
        "Resolved": "31/Mar/11 09:25",
        "Description": "In another project I have a useful little class, SequenceFileIterable, which simplifies iterating over a sequence file. It's like FileLineIterable. I'd like to add it, then use it throughout the code. See patch, which for now merely has the proposed new classes. \nWell it also moves some other iterator-related classes that seemed to be outside their rightful home in common.iterator.",
        "Issue Links": []
    },
    "MAHOUT-634": {
        "Key": "MAHOUT-634",
        "Summary": "Need more online averagers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "24/Mar/11 17:18",
        "Updated": "21/May/11 03:18",
        "Resolved": "25/Mar/11 00:09",
        "Description": "I am occasionally seeing a need to do exponential averaging of values or rates.\nHbase guys want this as well.\nSo it is time to do it.  I have a patch that does the averaging of values according to\nhttp://tdunning.blogspot.com/2011/03/exponential-weighted-averages-with.html\nI will attach that as a patch now and do the rate averaging as well before committing.",
        "Issue Links": []
    },
    "MAHOUT-635": {
        "Key": "MAHOUT-635",
        "Summary": "RMS variant of RunningAverage classes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "25/Mar/11 08:55",
        "Updated": "21/May/11 03:19",
        "Resolved": "25/Mar/11 17:03",
        "Description": "Added as classes because I don't know if this is interesting.",
        "Issue Links": []
    },
    "MAHOUT-636": {
        "Key": "MAHOUT-636",
        "Summary": "Remove CompactRunningAverage",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "26/Mar/11 11:14",
        "Updated": "21/May/11 03:18",
        "Resolved": "27/Mar/11 14:03",
        "Description": "CompactRunningAverage doesn't help on 64-bit machines, and makes little difference on 32-bit machines. It's probably not worth keeping going forward. Instead, saving memory could be accomplished better through use of parallel arrays and such.",
        "Issue Links": []
    },
    "MAHOUT-637": {
        "Key": "MAHOUT-637",
        "Summary": "Remove direct HBase dependency",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "27/Mar/11 14:05",
        "Updated": "21/May/11 03:18",
        "Resolved": "30/Mar/11 15:50",
        "Description": "As discussed on the mailing list, seems desirable to remove the direct dependence on HBase for now. The integration only exists for the Naive Bayes Classifier, and is based on an old version. A more comprehensive strategy for integrating with data sources, such as via Gora, is viewed as a desirable goal for later. This is a step in that direction.",
        "Issue Links": []
    },
    "MAHOUT-638": {
        "Key": "MAHOUT-638",
        "Summary": "Stochastic svd's is not handling well all cases of sparse vectors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "28/Mar/11 17:07",
        "Updated": "29/Aug/11 22:31",
        "Resolved": "19/Apr/11 18:29",
        "Description": "The Mahout patch of the algorithm is not handling all types of sparse input efficiently. BtJob doesn't handle SequentialSparseVector in a way to pick only non-zero elements from initial input and QJob doesn't iterate over RandomAccessSparseVector correctly. With extremely sparse inputs (0.05% non-zero elements) that leads to a terrible inefficiency in the aforementioned jobs (QJob, BtJob).",
        "Issue Links": []
    },
    "MAHOUT-639": {
        "Key": "MAHOUT-639",
        "Summary": "Need special case to handle creating a new SequentialAccessSparseVector from a large (> 1M dims) random/hashed vector",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Timothy Potter",
        "Created": "29/Mar/11 13:12",
        "Updated": "21/May/11 03:24",
        "Resolved": "30/Apr/11 04:43",
        "Description": "When trying to transpose a matrix of tfidf vectors created from text documents (ASF mail archives in this case), there is a bottleneck in the TransposeJob's reducer when Mahout creates a new SequentialAccessSparseVector from a RandomAccessSparseVector after the while loop completes:\n      SequentialAccessSparseVector outVector = new SequentialAccessSparseVector(tmp);\nFor high-frequency terms (some of which occur over ~1M times in my data), the code to create a SequentialAccessSparseVector from a RandomAccessSparseVector bogs down completely .... \nFrom Jake Mannix:\n\"Suspicion confirmed:\n  public SequentialAccessSparseVector(Vector other) {\n    this(other.size(), other.getNumNondefaultElements());\n    Iterator<Element> it = other.iterateNonZero();\n    Element e;\n    while (it.hasNext() && (e = it.next()) != null) \n{\n      set(e.index(), e.get());\n    }\n  }\nwe iterate over the other vector (which is in random/hashed order), adding it to the sequential access vector (which always tries to stay in sequential order).  So actually, this may be worse than O(n^2), but I'd prefer to just not know how much worse, and instead we should fix it.\nShould be fairly straightforward: make an array of structs (essentially) with the index and the double, of size other.getNumNonDefaultElements() (what a horrible method name), fill it up on one iteration over the other vector, sort it in place, then make your new OrderedIntDoubleMapping out of the indexes and values (unless someone has a cleverer idea to sort a pair of two arrays at the same time, shuffling one based on the ordering criterion of the other).\"",
        "Issue Links": []
    },
    "MAHOUT-640": {
        "Key": "MAHOUT-640",
        "Summary": "Implementation of refresh in SVDRecommender",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Chris Newell",
        "Created": "29/Mar/11 13:40",
        "Updated": "21/May/11 03:18",
        "Resolved": "12/Apr/11 18:58",
        "Description": "SVDRecommender (in package org.apache.mahout.cf.taste.impl.recommender.svd) and associated classes do not properly implement refresh.\nPlan:\n\nmake the AbstractFactorizer class implement refreshable.\ncomplete the implementation of refresh in SVDRecommender.",
        "Issue Links": []
    },
    "MAHOUT-641": {
        "Key": "MAHOUT-641",
        "Summary": "DistributedRowMatrix hadoop jobs ignore Configuration set via setConf()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Jonathan Traupman",
        "Created": "29/Mar/11 18:36",
        "Updated": "21/May/11 03:18",
        "Resolved": "31/Mar/11 10:39",
        "Description": "I am using the Distributed Lanczos solver which uses the DistributedRowMatrix class for it's internal calculation. In our environment, I need to set some Configuration properties (specifically hadoop.job.ugi & hadoop.queue.name), which are when set via the setConf() method on DistributedLanczosSolver. These are correctly passed to DistributedRowMatrix via its setConf() method, but are not passed into the Hadoop JobConfs created by the various static routines in MatrixMultiplicationJob, TimesSquaredJob, and TransposeJob.",
        "Issue Links": []
    },
    "MAHOUT-642": {
        "Key": "MAHOUT-642",
        "Summary": "Wrong parameter order in LoglikelihoodSimilarity and DistributedLoglikelihoodVectorSimilarity",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "30/Mar/11 14:14",
        "Updated": "21/May/11 03:18",
        "Resolved": "30/Mar/11 19:37",
        "Description": "org.apache.mahout.math.stats.LogLikelihood.logLikelihoodRatio expects the following counts:\n\nA and B together (k_11)\nB without A (k_12)\nA without B (k_21)\nNeither A nor B (k_22)\n\nIt seems to me that in org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarity and org.apache.mahout.math.hadoop.similarity.vector.DistributedLoglikelihoodVectorSimilarity the counts of k_12 and k_21 are given in the wrong order (B without A should come before A without B)\nCan someone confirm that?",
        "Issue Links": []
    },
    "MAHOUT-643": {
        "Key": "MAHOUT-643",
        "Summary": "Adding CityBlockSimilarity and DistributedCityBlockDistanceVectorSimilarity",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering,                                            Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Daniel McEnnis",
        "Created": "30/Mar/11 15:23",
        "Updated": "21/May/11 03:18",
        "Resolved": "31/Mar/11 09:28",
        "Description": "adding a new distance metric to the 0.5 branch",
        "Issue Links": []
    },
    "MAHOUT-644": {
        "Key": "MAHOUT-644",
        "Summary": "LuceneIterable does not throw exception if field does not have term vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "31/Mar/11 11:25",
        "Updated": "21/May/11 03:18",
        "Resolved": "02/Apr/11 10:14",
        "Description": "LuceneIterable does not throw an exception when it tries to create Mahout vectors from a field without term vectors. It should throw an IllegalStateException and not silently continue and return null vectors.",
        "Issue Links": []
    },
    "MAHOUT-645": {
        "Key": "MAHOUT-645",
        "Summary": "Elkan distance optimization for VectorBenchmarks class",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Gustavo Salazar Torres",
        "Created": "31/Mar/11 13:40",
        "Updated": "21/May/11 03:18",
        "Resolved": "31/Mar/11 19:00",
        "Description": "Implementation of first lemma of Elkan's optimization:\nGiven three points x, b, c (where b and c are centroids):\n                                           d(b,c)>=2d(x.b) then d(x,c)>=d(x,b)\nin which case we wouldn't need to calculate d(x,c). This is used to find the closest centroid for every point x.",
        "Issue Links": []
    },
    "MAHOUT-646": {
        "Key": "MAHOUT-646",
        "Summary": "Cannot run Wikipedia example on Amazon Elastic MapReduce (EMR)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Martin Provencher",
        "Created": "31/Mar/11 18:18",
        "Updated": "21/May/11 03:18",
        "Resolved": "03/Apr/11 20:40",
        "Description": "When I tried to run the Wikipedia example on EMR with all the categories existing in the Wikipedia dump, I got this error :\norg.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /yatter.tagger/wikipedia/input/temporary/_attempt_0000_r_000000_0/part-r-00000 for DFSClient_attempt_201103292134_0010_r_000000_0 on client 10.240.10.157 because current leaseholder is trying to recreate file.\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1045)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:981)\n    at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:377)\n    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:961)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:957)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:955)\n    at org.apache.hadoop.ipc.Client.call(Client.java:740)\n    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)\n    at $Proxy1.create(Unknown Source)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\n    at $Proxy1.create(Unknown Source)\n    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:2709)\n    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:491)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:195)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:524)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:505)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:412)\n    at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:128)\n    at org.apache.mahout.classifier.bayes.MultipleTextOutputFormat.getBaseRecordWriter(MultipleTextOutputFormat.java:41)\n    at org.apache.mahout.classifier.bayes.MultipleOutputFormat$1.write(MultipleOutputFormat.java:81)\n    at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:517)\n    at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n    at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:35)\n    at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:28)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:575)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)\n    at org.apache.hadoop.mapred.Child.main(Child.java:170)\norg.apache.hadoop.ipc.RemoteException: java.io.IOException: failed to create file /yatter.tagger/wikipedia/input/temporary/_attempt_0000_r_000000_0/part-r-00000 on client 10.240.10.157 either because the filename is invalid or the file exists\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1092)\n    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:981)\n    at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:377)\n    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:508)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:961)\n    at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:957)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:955)\n    at org.apache.hadoop.ipc.Client.call(Client.java:740)\n    at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:220)\n    at $Proxy1.create(Unknown Source)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\n    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\n    at $Proxy1.create(Unknown Source)\n    at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:2709)\n    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:491)\n    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:195)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:524)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:505)\n    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:412)\n    at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:128)\n    at org.apache.mahout.classifier.bayes.MultipleTextOutputFormat.getBaseRecordWriter(MultipleTextOutputFormat.java:41)\n    at org.apache.mahout.classifier.bayes.MultipleOutputFormat$1.write(MultipleOutputFormat.java:81)\n    at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:517)\n    at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n    at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:35)\n    at org.apache.mahout.classifier.bayes.WikipediaDatasetCreatorReducer.reduce(WikipediaDatasetCreatorReducer.java:28)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:575)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:412)\n    at org.apache.hadoop.mapred.Child.main(Child.java:170)\n    4 more :\n   org.apache.hadoop.ipc.RemoteException: java.io.IOException: failed to create file /yatter.tagger/wikipedia/input/temporary/_attempt_0000_r_000000_0/part-r-00000 on client 10.240.10.157 either because the filename is invalid or the file exists",
        "Issue Links": []
    },
    "MAHOUT-647": {
        "Key": "MAHOUT-647",
        "Summary": "Two small bugs in seq2sparse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Vasil Vasilev",
        "Created": "31/Mar/11 20:26",
        "Updated": "21/May/11 03:19",
        "Resolved": "31/Mar/11 21:38",
        "Description": "From Vasil on the mailing list:\n1. the minLLR parameter is not taken into account. The problem is that in\nthe CollocDriver class\nJob job = new Job(conf);\nis executed before\nconf.setFloat(LLRReducer.MIN_LLR, minLLRValue);\nsee CollocDriver.computeNGramsPruneByLLR method\n2. maxDFPercent is not taken into account. The problem is that in\nTFIDFPartialVectorReducer.reduce the check is\nif (df / vectorCount > maxDfPercent) {\n         if (log.isInfoEnabled()) {\n               log.info(\"ommiting {}\", e.index());\n             }\n       continue;\n     }\nand should be:\nif (df*100 / vectorCount > maxDfPercent) {\n         if (log.isInfoEnabled()) {\n               log.info(\"ommiting {}\", e.index());\n             }\n       continue;\n     }",
        "Issue Links": []
    },
    "MAHOUT-648": {
        "Key": "MAHOUT-648",
        "Summary": "API-changes for optimizing recommender performance in some usecases",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "02/Apr/11 07:05",
        "Updated": "21/May/11 03:18",
        "Resolved": "02/Apr/11 16:41",
        "Description": "I'd like to propose a set of small API changes in our recommender code. \n\nadd a method allSimilarItemIDs(long itemID) to ItemSimilarity, which returns the ids of all similar items\nmake sure that GenericItemBasedRecommender.recommend(...) only makes a single call to the DataModel with which it retrieves all preferences for the user to recommend items for\n\n\nadd a new strategy for finding candidate items for the most-similar-items and recommendation computation that only calls ItemSimilarity.allSimilarItemIDs(...) and doesn't need to call anything on the DataModel\nand an option to GenericItemSimilarity to make it create an in-memory-index to allow retrieval of all similar items per item in constant time\n\nThe purpose of these changes is to make it possible to run a very efficient recommender for usecases, where the major purpose of the recommender is to answer requests for most-similar-items and it you only have to compute \"real\" recommendations from time to time. A typical scenario where these conditions are met is e-commerce, you have lots of most-similar-items calls as users browse product pages and fill their shopping carts and for the minority of users that log in you have to provide personalized product recommendations.\nWith the proposed changes, you need to precompute the item-similarities and load them into memory, either from a file with FileItemSimilarity or from a database with the new MySQLJDBCInMemoryItemSimilarity and use a GenericItemBasedRecommender with the AllSimilarItemsCandidateItemsStrategy. Requests for most-similar-items can be completely answered from memory (in nearly constant time) without having to touch the DataModel. Answering 100 requests per second on a single machine are no problem using this approach.\nWe can then use a DataModel that does not need to reside in memory because its only task is to act as a repository for the users' preferences. When we compute personalized recommendations we need to do exactly one single call to the datastore to retrieve all the preferences for the user we wanna compute recommendations for. This single call should be very fast with our already existing jdbc-backed DataModel's and it should be easy to implement it equally fast in other datastores like Solr for example. One could even start thinking about sharded DataModels with this approach.\nAnother very big advantage of this approach is that user preferences can now be updated in realtime as we never need to refresh the datamodel. We only need to refresh the item-similarities from time to time. Memory requirements for the recommender machines would drop drastically as we only have to store the item-similarities in RAM whose number should be orders of magnitude smaller than the number of preferences.\nThe API changes in the patch should be fully backwards compatible, so that this new approach is only an additional way to use our recommender code and all currently existing approaches still work as before.\nHere is an example how such a setup would work using a MySQL database:\n\nDataSource dataSource = ...\nDataModel dataModel = new MySQLJDBCDataModel(dataSource);\n\n/* load all item-similarities into memory, create an index for fast retrieval of all-similar-item-ids */\nItemSimilarity itemSimilarity = MySQLJDBCInMemoryItemSimilarity(dataSource, true);\n\n/* the candidate items for recommendation and most-similar-items are only fetched from our in-memory data structures by this strategy*/\nAllSimilarItemsCandidateItemsStrategy allSimilarItemsStrategy = new AllSimilarItemsCandidateItemsStrategy(itemSimilarity);\n\nItemBasedRecommender recommender = new GenericItemBasedRecommender(dataModel, itemSimilarity, allSimilarItemsStrategy, allSimilarItemsStrategy);",
        "Issue Links": []
    },
    "MAHOUT-649": {
        "Key": "MAHOUT-649",
        "Summary": "Test org.apache.mahout.text.SequenceFilesFromMailArchivesTest fails on windows",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "03/Apr/11 22:48",
        "Updated": "21/May/11 03:19",
        "Resolved": "04/Apr/11 07:11",
        "Description": "org.apache.mahout.text.SequenceFilesFromMailArchivesTest seems to be platform-dependent in assertion (path comparisons). Assertion fails on Windows\nstack trace: \nTest set: org.apache.mahout.text.SequenceFilesFromMailArchivesTest\n-------------------------------------------------------------------------------\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.144\nsec <<< FAILURE!\ntestMain(org.apache.mahout.text.SequenceFilesFromMailArchivesTest)\nTime elapsed: 0.14 sec  <<< FAILURE!\norg.junit.ComparisonFailure:\nexpected:<TEST[/subdir/mail-messages.gz/]user@example.com> but\nwas:<TEST[\\subdir\\mail-messages.gz]user@example.com>\n       at org.junit.Assert.assertEquals(Assert.java:123)\n       at org.junit.Assert.assertEquals(Assert.java:145)\n       at org.apache.mahout.text.SequenceFilesFromMailArchivesTest.testMain(SequenceFilesFromMailArchivesTest.java:112)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n       at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n       at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n       at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n       at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n       at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n       at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n       at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n       at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n       at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n       at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n       at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n       at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n       at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n       at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)\n       at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:115)\n       at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:102)\n       at org.apache.maven.surefire.Surefire.run(Surefire.java:180)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:350)\n       at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1021)\nintroduced with commit :\n\n$ git log --stat 86e6e1d64901 -1\ncommit 86e6e1d64901cc0ce436d43a56fcadb8a2cb6c1d\nAuthor: Grant Ingersoll <gsingers@apache.org>\nDate:   Fri Mar 25 14:39:25 2011 +0000\n\n   MAHOUT-588: partial commit, not including shell script yet\n\n   git-svn-id: https://svn.apache.org/repos/asf/mahout/trunk@1085408 13f79535-4\n\n .../text/MailArchivesClusteringAnalyzer.java       |  171 +++++++++++\n .../mahout/text/SequenceFilesFromMailArchives.java |  295 ++++++++++++++++++++\n .../text/MailArchivesClusteringAnalyzerTest.java   |   61 ++++\n .../text/SequenceFilesFromMailArchivesTest.java    |  214 ++++++++++++++\n 4 files changed, 741 insertions(+), 0 deletions(-)",
        "Issue Links": []
    },
    "MAHOUT-650": {
        "Key": "MAHOUT-650",
        "Summary": "java.io.Serializable should be implemented by org.apache.mahout.cf.taste.impl.common.LongPrimitiveArrayIterator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Manuel Blechschmidt",
        "Created": "04/Apr/11 10:36",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "04/Apr/11 11:34",
        "Description": "I am using Mahout in a Java EE 6 application. I implemented a DataModel as an EJB 3.1 Singelton Service Bean. I wanted to write some test cases but unfortunately some of the classes of Taste are not serializable.\nAt the moment LongPrimitiveArrayIterator stops my test cases from working.\nSee attached patch, files and images for details",
        "Issue Links": []
    },
    "MAHOUT-651": {
        "Key": "MAHOUT-651",
        "Summary": "Pass hadoop configuration to methods that use FileSystem operations, even if they don't invoke map/reduce jobs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Robert Mahfoud",
        "Created": "04/Apr/11 18:28",
        "Updated": "21/May/11 03:19",
        "Resolved": "05/Apr/11 10:21",
        "Description": "Some classes in the Classification component internally use the hadoop's FileSystem class, however, they instantiate the hadoop configuration locally in the method using new Configuration(). This limits the ability to integrate these tools within applications that manage and enrich their own configuration rather than rely on the default hadoop resources that get loaded when calling new Configuration().\nThe fix is simply to make these methods take a Configuration parameter rather than creating a new instance when needed. An example for an that creates a new Configuration instances is: org.apache.mahout.clustering.kmeans.KMeansUtil.configureWithClusterInfo(Path, List<Cluster>)\nThis problem could also exists beyond the Clustering module, but this issue only addresses the Clustering code.",
        "Issue Links": []
    },
    "MAHOUT-652": {
        "Key": "MAHOUT-652",
        "Summary": "[GSoC Proposal] Parallel Viterbi algorithm for HMM",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Sergey Bartunov",
        "Created": "06/Apr/11 20:42",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jun/12 08:26",
        "Description": "Proposal Title: Parallel Viterbi algorithm for HMM\nStudent Name: Sergey Bartunov\nStudent E-mail: sbos.net@gmail.com\nOrganization/Project: Apache Mahout\nAssigned Mentor:\nProposal Abstract:\nThe Viterbi Algorithm is an evaluating algorithm for Hidden Markov Model [1]. It estimates the most likely sequence of hidden states called \"Viterbi path\" from given sequence of observed states. Viterbi algorithm is also used in Viterbi training which is less numerical stable than Baum-Welch algorithm but faster and can be adjusted to have near the same accuracy [2]. Hidden Markov Model is widely used for speech and gesture recognition, part-of-speech tagging, bioinformatics etc. At the moment Apache Mahout contains only sequential HMM functionality, and this project is intended to extend it by implementing Map-Reduce version of Viterbi algorithm which would make Mahout able to evaluate HMM on big amounts of data in parallel mode. \nDetailed Description:\nThe Viterbi algorithms is a quite common dynamic programming algorithm, it's well described in many sources such as [3], [4], [5]. As being popular and needed, Viterbi algorithm already have parallel versions which became a subject of several studies, for example, this paper about implementation for GPU [6]. Some of these papers may contain useful ideas for map-reduce implementation.\nThere are several possible strategies for parallelization (which one is\nbetter is an open question for the project) I discovered and posted to the mailing list.\nSee this thread\nThe most reasonable one is:\nAssume we have O - set of oberved state sequences (that's our input data), $O_i$ is i-th sequence with length \nlen($O_i$), K is number of hidden states in our model. {$O_\n{i, t}\n$} is the observed state of i-th sequence at the moment of time t. Let the N be the maximum of len($O_i$).\nLet's write all the seqences one above other. We will get the matrix which rows are the input seqeunces. Then take the first L columns of this matrix and name them the \"first chunk\", then next L columns and so on. Thus we divide our input into the chunks of size L (less or equal to L). L is the parameter of the algorithm. So the n-th chunk contain subsequences with $t$ in the iterval [L*n, L*(n+1)). \nThe key idea is to process each chunk by standard sequential Viterbi algorithm (the existing Mahout code could be reused here) in parallel mode one by one using results of previous chunk processing.\nIt will require about O((M/P) * K^2) time where M is sum of all sequence lengths if most of input sequences have near the same length (the good case). P here is the number of \"cores\" / computational nodes. This time complexity means that such strategy scales well. \nHere is Map-Reduce scheme for the good case:\n\nMap emits (subsequence, probabilites, paths) vector for each subsequence in the chunk, where probabilities is the initial probabilites in case of the first chunk and optimal-path probabilities in all other cases, paths means optimal paths to the first observations of each subsequence.\nReduce function just performs sequential Viterbi algorithm on these data and returns (probabilities, paths).\nDriver class runs the Map then Reduce for the first chunk, then Map and Reduce for the second, and so on. Then provide the results (probably using another map-reduce combination).\n\nProbably, it's possible to use lazy viterbi [7,8] instead of the standard Viterbi for chunk processing.\nLet's focus now on all other cases when first T chunks contain the subsequences with near the same length and all other N-T chunks do not. Well, then they could be processed using the next technique (which is strategy number 2 in the list):\nAt first, denote $V_\n{t,k}$ as the probabilty that optimal hidden state path goes through hidden state $k$ at the moment $t$. We need such probabilities to select the most likely (or optimal) path so we need to compute $V_{t,k}\n$ for every $t$ and $k$ (that is the part of standard Viterbi). I omit here the $i$ index of the sequence, because each seqence is the separate task.\n1) process each chunk separately. $t$ lie in the [L*n, L*(n+1)) interval, where n is the number of chunk and to compute  Actually to do this, we need to compute $V_\n{t,k}\n$ we need to know $V_\n{L*n-1,k1}\n$ for each $k1$. But this value belong to the chunk the previous number (n-1) which is processed separately. We need to obtain the max and arg-max of $V_\n{L*n-1, k1}$ for each k1. \n2) Since we don't know the max's and arg-max's let's just assume that optimal path goes through the k1=1 then through k2=2 and so on up to the K, and $V_{L*n-1, k1}\n$ = 1 (or 0 if we use log(prob) adding instead of prob multiplying) and argmax is k1=1, 2 ... K. Later we may multiply the $V_\n{L*(n+1)-1, k1}\n$ by the precise probability max{ $V_\n{L*n-1, k1}\n$ } (or add the log of it). Thus we get  K^2 possibly optimal paths instead of K.\n3) After we process all the chunks we start \"connecting\" phase. \nThe first of these N-T chunks could be processed by using usual Viterbi processing since we know the results for the previous \"good\" chunk. All others need to be connected each to other starting from connecting second this first then 3th with 4th and so on. During connection process we throw away all non-optimal paths (know we know which are optimal and which are not) and leave just K possibly optimal paths.\nTo handle such difficult cases the Driver class should know the sequence lengths to recognize which Map-Reduce combination it should use. The resulting time complexity for difficult case is \nO((T*L / N) / P) * K^2 + (((T-N) * L / N)) / P * K^3). The first term of this sum is time for computing the first T chunks using simple case approach, and the second term is time for computing the rest using difficult case approach. When T -> N overall time tend to O((M/P) * K^2). T * L / N is about total length of subsequences in the first T chunks and (((T-N) * L / N)) is total length of the rest.\nThe only problem not discussed here is how to deal with obtained most likely paths. Path length is equal to length of a sequence, so it's nood a good idea to emit optimal path in Map stage. They could be stored separately in the files or HBase since they are required only in getting the results to the user.\nThis implementation should work (or be compatible) with org.apache.mahout.classifier.sequencelearning.hmm.HmmModel\nAs you see, Viterbi algorithm use very common dynamic approach. It computes the next computation layer by combining values the previous, that's all we need to know how to implement it in the Map-Reduce and it shares the mentioned difficulties with all other such dynamic algorithms, so the implentation may be highly resuable.\nTimeline:\n1. 1 week. Discover Mahout internals, sequential HMM code and code best practices. Think on problem with path storing. I'll work on this stage actually before GSoC starts but I leave here 1 week just to be sure.\n2. 5 days. Write the chunk division routines. \n3. 5 days. Write the SimpleMapper class for simple good case of data.\n4. 1 week. Write the SimpleReducer class for simple good case.\n5. 1 week. Write the Driver class for simple good case which will use SimpleMapper and SimpleReducer. \n6. 1 week. Collect debug dataset, write tests for the code and find possible problems, fix them.\n7. 10 days. Write the HardReducer class for difficult bad case and rewrite Driver class to handle such cases. Perform tests, ensure that everything works.\n7. 10 days. Try to get a large dataset to test performance, scalability etc, write and perform such a test (here a community help might be needed).\nIf there are any problems discuss and analyze them, then fix.\n8. 1 week. Try to use some optimized Viterbi implentation (i.e. Lazy Viterbi) in chunk processing phase, measure speed improvement especially for difficult cases.\n9. 1 week. Write documentation for all classes, description of algorithm in\nwiki and complete example of usage.\nAdditional Information:\nThe project relates to this proposal which is about implementing parallel Baum-Welch traning algorithm for HMM.\nMy motivation for this project is that I need such a functionality as a regular Mahout user since I face HMM often in my research work. Also I'm very interested in what kinds or classes of algorithms could be efficiently implemented in Map-Reduce, I also think that this is important question for the community and such an experience is significant.\nThe important detail is that I have exams in the university until 16th June. After that I will be ready to work on the project.\nReferences:\n[1] Lawrence R. Rabiner (February 1989). \"A tutorial on Hidden Markov Models and selected applications in speech recognition\". Proceedings of the IEEE 77 (2): 257-286. doi:10.1109/5.18626. \n[2] Adjusted Viterbi Training. A proof of concept.\n[3] http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.2081&rep=rep1&type=pdf\n[4] http://www.cambridge.org/resources/0521882672/7934_kaeslin_dynpro_new.pdf\n[5] http://www.kanungo.com/software/hmmtut.pdf\n[6] http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=5470903\n[7] http://www.scribd.com/doc/48568001/Lazy-Viterbi-Slides\n[8] http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1040367",
        "Issue Links": []
    },
    "MAHOUT-653": {
        "Key": "MAHOUT-653",
        "Summary": "Approximations to standard functions",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "07/Apr/11 03:56",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "21/May/11 02:40",
        "Description": "These give approximate versions of pow(value, exponent), exp(value), and natural log(value).\nlog() and exp() stolen from:\nhttp://martin.ankerl.com/2007/02/11/optimized-exponential-functions-for-java/\npow() stolen from:\nhttp://martin.ankerl.com/2007/10/04/optimized-pow-approximation-for-java-and-c-c/",
        "Issue Links": []
    },
    "MAHOUT-654": {
        "Key": "MAHOUT-654",
        "Summary": "Clean out redundant material from the poms, use standard apache parent.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "07/Apr/11 16:30",
        "Updated": "21/May/11 03:18",
        "Resolved": "07/Apr/11 20:16",
        "Description": "There are scm elements and repository elements in the poms that are redundant or unhelpful, and the top pom does not use the standard apache parent.\nThe patch addresses this.",
        "Issue Links": []
    },
    "MAHOUT-655": {
        "Key": "MAHOUT-655",
        "Summary": "Need the ability to specify the output path for the Transpose Job",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Timothy Potter",
        "Created": "07/Apr/11 16:46",
        "Updated": "21/May/11 03:22",
        "Resolved": "07/Apr/11 17:52",
        "Description": "Currently, the transpose job does not allow us to specify the output path. It creates it's own path internally, something like: /asf-mail-archives/mahout/svd/transpose-##. This makes it hard to script matrixmult jobs (which need to know the output path of the transpose job).",
        "Issue Links": []
    },
    "MAHOUT-656": {
        "Key": "MAHOUT-656",
        "Summary": "Test failure in AbstractJDBCInmemoryItemSimiliarityTest",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "07/Apr/11 17:03",
        "Updated": "21/May/11 03:22",
        "Resolved": "07/Apr/11 17:27",
        "Description": "The test in the subject fails on the trunk on my mac. My analysis so far is that hasNext returns false when called from next() after returning true when called from the loop (this is JDBCSimilaritiesIterator).",
        "Issue Links": []
    },
    "MAHOUT-657": {
        "Key": "MAHOUT-657",
        "Summary": "Sample code to apply SVD to the KDD data",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "07/Apr/11 18:47",
        "Updated": "21/May/11 03:19",
        "Resolved": "07/Apr/11 21:16",
        "Description": "I was incited by some comments on twitter to make our SVD-based recommendation code work on the KDD data. Here's the results so far:\nThe patch contains a tweaked version of ExpectationMaximizationSVDFactorizer (org.apache.mahout.cf.taste.example.kddcup.track1.svd.ParallelArraysSGDFactorizer) in the examples module, that is able to load and process the KDD dataset with a constant memory usage of approximately 7 gb (by using primitive arrays for everything). \nIt's still very slow unfortunately, a factorization using 40 features and 25 iterations took 10 hours on my desktop PC. As far as I understand the math behind it, the algorithm is not parallelizable but maybe someone might be able to improve my implementation or make it compute several factorizations at once.\nI took a wild guess on the parameters and got an RMSE of 23.35 to the validation set and and RMSE of 26.1287 to the secret test ratings (that's rank 63 by the time of this writing).\nWould love to see people play with this code and improve it!\nIn order to use this, have a look at the parameters in org.apache.mahout.cf.taste.example.kddcup.track1.svd.Track1SVDRunner, change them as you see fit and run that class with the path to the kdd data directory and the path to the file you wanna have the results stored in as arguments. In my tests I used -Xms6700M -Xmx6700M to give the JVM enough memory for 40 features.",
        "Issue Links": []
    },
    "MAHOUT-658": {
        "Key": "MAHOUT-658",
        "Summary": "UH-Mine algorithm for frequent pattern mining of uncertain data",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yarco Hayduk",
        "Created": "08/Apr/11 04:23",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "22/May/11 15:30",
        "Description": "Proposal Title: UH-Mine algorithm for frequent pattern mining of uncertain data\nStudent Name: Yaroslav Hayduk\nStudent E-mail: yarcoh@gmail.com\nOrganization/Project: Apache Mahout\nAssigned Mentor:\nAbstract\nFrequent pattern mining detects frequent patterns in data, which in turn permits data analysts determine interesting correlations in the dataset. In uncertain data mining, we suspect, but cannot guarantee, the presence or absence of an item. Currently, Apache\nMahout does not contain algorithm implementations capable of mining frequent patterns from uncertain data. Hence, I propose to implement UH-Mine during GSoC 2011, used for frequent pattern mining of uncertain data.\nMotivation\nThere are many real-life situations, where we observe uncertain data, such as in temperature and wind speed readings, patient diagnosis, and satellite imaging. Due to the probabilistic nature of this data, it takes more time and resources to mine it. Current state of-the-art frequent pattern mining of uncertain data algorithms do not provide sufficient performance results, as most of them are not crafted to execute in parallel.\nIn uncertain data mining, we suspect, but cannot guarantee, the presence or absence of an item. For example, when a doctor is 90% sure that a patient suffers from a particular disease. A video lecture by Jian Pei [3] contains a thorough overview of the uncertain data problem domain as well as elaborates on the process of frequent pattern mining of uncertain data.\nComparison of UF-Growth and UH-Mine\nThe UF-Growth [4] algorithm modifies the FP-Growth [2] algorithm in the way the transaction tree is built. FP-Growth uses the FP-Tree, a tree-based data structure, to store a compact representation of the transaction database, which contains frequency information of all frequent items. The FP-Tree, however, does not store existential probabilities, associated with items. Hence, Leung et al. [4] created a data structure, called UF-Tree, which is based on the FP-tree. Each node in the UF-Tree stores an item, its expected support, as well as the number of occurrence of such expected support for each item. To merge the transaction with the child node in UF-Tree, UF-Growth requires both the item and its corresponding existential probability to match. Hence, the algorithm arrives with the UF-Tree, having a much lower compression ratio, that in the FP-Tree, constructed by FP-Growth.\nAs such, I propose to adapt and implement the UH-Mine algorithms to the MapReduce programming model. The UH-Struct structure uses the linkage behaviour among transactions corresponding to a branch of the FP-Tree(UF-Tree) without actually creating a projected database.\nThis approach is better than FP-Tree even in the deterministic case, when compression from FP-Tree is not high. This turns out to be particularly true for the uncertain case, as discussed earlier. The UH-mine [1] algorithm provides the best trade-offs both in terms of running time and memory usage.\nThe UH-Mine algorithm works as follows: it\n1. prunes the initial DB such that all singleton infrequent items are removed; \n2. divides the pruned DB into equal chunks; \n3. mines these chunks separately using the UH-Mine(mem) algorithm. To mine frequent patterns, UH-Mine(mem) maintains an H-Struct, which contains pointers to transaction items. At each step, the algorithm adjusts these pointers and does not incur the overheads, associated with the FP(UF)-Tree construction;\n4. joins the results and \n5. scans the pruned DB once again to remove false positives and obtain the actual counts.\nBenefits for the Mahout community\na) My work adds an algorithm implementation for discovering frequent patterns in uncertain data \nb) If my algorithm implementation proves to be fast, it would permit future algorithm developers to retrofit my UH-Mine implementation to create H-Mine[5]. H-Mine is an alternative algorithm, which mines frequent patterns from precise data.\nTimeline\nWeeks 1-4: Implement UH-Mine in Java, set up a local Hadoop cluster composed of 3 machines Weeks 5-7: Investigate Mahout code structure, start Adopting UH-Mine to MapReduce. \nWeeks 8: Summit for mid-term evaluation \nWeeks 9 - 11: Finish-up with my implementation, refactor code smells, identify and fix performance issues \nWeeks 11 - 12: Code cleaning, documenting and testing.\nBiography \nMy name is Yarco Hayduk and I am a Comp Sci graduate student at the University of Manitoba, Canada. I'm very interested in concurrency and data mining. Currently, I am working on a similar project, which adopts the UFP-Growth[6] algorithm to MapReduce. I'm using the Parallel FP-Growth algorithm as a starting point for implementing UFP-Growth. I also implemented UH-Mine and H-Mine algorithms in C. Thus, I would use it as a reference for my proposed UH-Mine implementation on MapReduce.\nReferences\n[1] Jianyong Wang Charu C. Aggarwal, Yan Li and Jing Wang. Frequent pattern mining\nwith uncertain data. In KDD '09: 15th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pages 29--38, Paris, France, June 2010. ACM.\n10.1145/1557019.1557030.\n[2] Jiawei Han, Jian Pei, and Yiwen Yin. Mining frequent patterns without candidate generation. In SIGMOD '00: 2000 ACM SIGMOD international conference on management\nof data, pages 1--12, Dallas, TX, USA, May 2000. ACM. 10.1145/342009.335372.\n[3] Ming Hua Jian Pei. Mining uncertain and probabilistic data: problems, challenges,\nmethods, and applications. http://videolectures.net/kdd08_pei_mupd, Accessed on\nMarch 11, 2011.\n[4] Carson Kai-Sang Leung, Christopher Lee Carmichael, and Boyu Hao. Efficient mining\nof frequent patterns from uncertain data. In ICDMW '07: 7th IEEE International Conference on Data Mining Workshops, pages 489--494, Omaha, NE, USA, October 2007.\nIEEE. 10.1109/ICDMW.2007.84.\n[5] Jian Pei, Jiawei Han, Hongjun Lu, Shojiro Nishio, Shiwei Tang, and Dongqing Yang.\nH-Mine: Hyper-structure mining of frequent patterns in large databases. In ICDM '01:\n1st IEEE International Conference on Data Mining, pages 441--448, San Jose, California,\nUSA, November 2001. IEEE. 10.1109/ICDM.2001.989550.\n[6] Calin Garboni Toon Calders and Bart Goethals. Efficient pattern mining from uncertain data with sampling. In PAKDD 2010: 14th Pacific-Asia Conference on Knowledge\nDiscovery and Data Mining, pages 480--487, Hyderabad, India, June 2010. Springer.\n10.1007/978-3-642-13657-351.",
        "Issue Links": []
    },
    "MAHOUT-659": {
        "Key": "MAHOUT-659",
        "Summary": "Remove use of java.net repository from build",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "08/Apr/11 11:36",
        "Updated": "21/May/11 03:19",
        "Resolved": "08/Apr/11 15:48",
        "Description": "Using java.net is irritating and inconvenient for users. Stop.\nThe first known problem is uncommons-math; I've arranged publication to central, and I'll fix the poms when I know it's there.",
        "Issue Links": []
    },
    "MAHOUT-660": {
        "Key": "MAHOUT-660",
        "Summary": "Fix fastinstall profile to work without prior build with tests",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "build",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "08/Apr/11 11:42",
        "Updated": "21/May/11 03:18",
        "Resolved": "08/Apr/11 12:13",
        "Description": "just what the subject says.",
        "Issue Links": []
    },
    "MAHOUT-661": {
        "Key": "MAHOUT-661",
        "Summary": "Finish iterator overhaul by using Google Guava",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering,                                            Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "08/Apr/11 16:23",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "10/Apr/11 18:30",
        "Description": "To complete this truly interesting overhaul of the use of Iterator \u2013 I rightly guessed that Guava already has some of these common classes and more. Indeed. I can see easily another 1,000 lines of code of savings by adapting to use their iterator primitives. I'm in progress now and have removed 600 lines so far. I think this will go a long way to avoiding subtle and various bugs in the iterator implementations to date. I can definitley get it in before 0.5.",
        "Issue Links": []
    },
    "MAHOUT-662": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-663": {
        "Key": "MAHOUT-663",
        "Summary": "Rationalize hadoop job creation with respect to setJarByClass",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "build",
        "Assignee": "Sean R. Owen",
        "Reporter": "Benson Margulies",
        "Created": "08/Apr/11 18:00",
        "Updated": "03/Jun/13 15:57",
        "Resolved": "13/Sep/11 09:11",
        "Description": "Mahout includes a series of driver classes that create hadoop jobs via static methods.\nEach one of these calls job.setJarByClass(itself.class).\nUnfortunately, this subverts the hadoop support for putting additional jars in the lib directory of a job jar, since the class passed in is not a class that lives in the ordinary section of the job jar.\nThe effect of this is to force users of Mahout (and Mahout's own example job jar) to unpack the mahout-core jar into the main section, instead of just treating it as a 'lib' dependency.\nIt seems to me that all the static job creators should be refactored into a public function that returns a job object (and does NOT call waitForCompletion), and then the existing wrapper. Users could call the new functions, and make their own call to setJarByClass.",
        "Issue Links": []
    },
    "MAHOUT-664": {
        "Key": "MAHOUT-664",
        "Summary": "Remove usage of XStream string serialization too?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "10/Apr/11 21:58",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "05/Jun/11 16:52",
        "Description": "In the spirit of MAHOUT-510 and continuing to standardize approaches without going too nuts \u2013 I notice that there is only one use of XStream left in the code, in StringUtils, which is merely a wrapper for serializing/deserializing an object to a String.\nThere are two key mahout-core usages:\n\norg.apache.mahout.ga.watchmaker. Here it seems to be used to serialize/deserialize a candidate solution to a file on HDFS and fed into a mapper. Surely it's more appropriate to do something with Writable here?\norg.apache.mahout.df.mapreduce. It's used to serialize/deserialize a DefaultTreeBuilder. Does it need to be done? it seems like DefaultTreeBuilder is parameterized entirely by its param \"m\" but I haven't looked hard.\n\nAll of the other usages are in mahout-examples/, which suggests they're of secondary importance.\nSome usages are serialization of longs and char[], which seems pointless.\nAny thoughts? putting this on the map for 1.0.",
        "Issue Links": []
    },
    "MAHOUT-665": {
        "Key": "MAHOUT-665",
        "Summary": "Do we need org.apache.mahout.common.cache?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "10/Apr/11 22:08",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "12/Apr/11 18:59",
        "Description": "org.apache.mahout.common.cache is unused except for in org.apache.mahout.fpm.pfpgrowth.fpgrowth.FPTreeDepthCache. It in turn could be replaced with a simple Map, if the requirement to cache only the smallest 5 attributes is relaxed. Is this assumption needed?",
        "Issue Links": []
    },
    "MAHOUT-666": {
        "Key": "MAHOUT-666",
        "Summary": "DistributedSparseMatrix should clean up after itself when doing times(Vector) and timesSquared(Vector)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Math",
        "Assignee": "Jake Mannix",
        "Reporter": "Jonathan Traupman",
        "Created": "12/Apr/11 02:23",
        "Updated": "21/May/11 03:19",
        "Resolved": "15/Apr/11 10:24",
        "Description": "The directories created during the times() and timesSquared() methods in DistributedSparseMatrix leave behind a lot of cruft. While the individual files are tagged with deleteOnExit, but the directories are not. Also, but not deleting them until JVM exit, a job that does repeated matrix/vector multiplies, like DistributedLanczosSolver, creates a lot of temp files that stick around for the whole run, even though the results they contain are read once and then never again. \nOur cluster admins enforce both file count and size quotas, so since 5 temp files/directories are created on each iteration of DistributedLanczosSolver, we're constantly bumping into the quota with large SVDs.",
        "Issue Links": []
    },
    "MAHOUT-667": {
        "Key": "MAHOUT-667",
        "Summary": "Persistent storage of factorizations in SVDRecommender",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Chris Newell",
        "Created": "12/Apr/11 12:18",
        "Updated": "21/May/11 03:18",
        "Resolved": "17/Apr/11 10:11",
        "Description": "As discussed previously (https://issues.apache.org/jira/browse/MAHOUT-640) it would be beneficial to provide a persistent storage mechanism for factorizations created by SVDRecommender (in package org.apache.mahout.cf.taste.impl.recommender.svd) as these can be time consuming to produce. It would also allow factorizations to be computed on one machine then distributed to other machines providing predictions, improving efficiency and scalability.\nHaving a \"persistence strategy\" interface has been suggested that could be implemented as required. I'll try to post a outline proposal for discussion purposes in the next few days but any comments or suggestions would be very welcome.",
        "Issue Links": []
    },
    "MAHOUT-668": {
        "Key": "MAHOUT-668",
        "Summary": "Adding knn support to Mahout classifiers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Daniel McEnnis",
        "Created": "13/Apr/11 00:05",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "08/Mar/14 12:03",
        "Description": "Initial implementation of the knn.  This is a minimum base set with many more possible add-ons including support for text and weka input as well as a classify only (no confusion matrix) back end.  The system was tested on the 20 newsgroup data set.",
        "Issue Links": []
    },
    "MAHOUT-669": {
        "Key": "MAHOUT-669",
        "Summary": "Scripts should specify bash instead of /bin/sh",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "13/Apr/11 17:32",
        "Updated": "21/May/11 03:19",
        "Resolved": "14/Apr/11 11:13",
        "Description": "Ken Williams said:\n\nJust out of interest, I also reckon your 'build-cluster-syntheticcontrol.sh'\nscript should be a bash script (#!/bin/bash) rather than a standard\nshell (#!/bin/sh) script.\n$ trunk/examples/bin/build-cluster-syntheticcontrol.sh\ntrunk/examples/bin/build-cluster-syntheticcontrol.sh: 28: Syntax error: \"(\"\nunexpected (expecting \"fi\")\n$",
        "Issue Links": []
    },
    "MAHOUT-670": {
        "Key": "MAHOUT-670",
        "Summary": "Provide a performance measurement framework for Mahout",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Oliver B. Fischer",
        "Created": "14/Apr/11 13:32",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "01/Jun/13 16:06",
        "Description": "At the moment Mahout lacks the existence of a performance test framework. The framework should be able to execute user defined performace test of distributed and non-distributed algorithms, generate reports and to detect regressions in the performace of mahout.",
        "Issue Links": [
            "/jira/browse/MAHOUT-875",
            "/jira/browse/MAHOUT-588"
        ]
    },
    "MAHOUT-671": {
        "Key": "MAHOUT-671",
        "Summary": "Refactor org.apache.mahout.utils.vectors.lucene.Driver into a POJO",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Chris Jordan",
        "Created": "15/Apr/11 15:58",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "02/Jun/11 13:11",
        "Description": "The Driver class in org.apache.mahout.utils.vectors.lucene is currently a monolithic main method. While it does serve the purposes of running the vector dumping utility from command line well, it would beneficial to refactor this object into a POJO that can be easily used by applications that use Mahout; line 131 even indicates that this change should be done.",
        "Issue Links": []
    },
    "MAHOUT-672": {
        "Key": "MAHOUT-672",
        "Summary": "Implementation of Conjugate Gradient for solving large linear systems",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Jonathan Traupman",
        "Created": "16/Apr/11 02:31",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "25/Oct/11 02:00",
        "Description": "This patch contains an implementation of conjugate gradient, an iterative algorithm for solving large linear systems. In particular, it is well suited for large sparse systems where a traditional QR or Cholesky decomposition is infeasible. Conjugate gradient only works for matrices that are square, symmetric, and positive definite (basically the same types where Cholesky decomposition is applicable). Systems like these commonly occur in statistics and machine learning problems (e.g. regression). \nBoth a standard (in memory) solver and a distributed hadoop-based solver (basically the standard solver run using a DistributedRowMatrix a la DistributedLanczosSolver) are included.\nThere is already a version of this algorithm in taste package, but it doesn't operate on standard mahout matrix/vector objects, nor does it implement a distributed version. I believe this implementation will be more generically useful to the community than the specialized one in taste.\nThis implementation solves the following types of systems:\nAx = b, where A is square, symmetric, and positive definite\nA'Ax = b where A is arbitrary but A'A is positive definite. Directly solving this system is more efficient than computing A'A explicitly then solving.\n(A + lambda * I)x = b and (A'A + lambda * I)x = b, for systems where A or A'A is singular and/or not full rank. This occurs commonly if A is large and sparse. Solving a system of this form is used, for example, in ridge regression.\nIn addition to the normal conjugate gradient solver, this implementation also handles preconditioning, and has a sample Jacobi preconditioner included as an example. More work will be needed to build more advanced preconditioners if desired.",
        "Issue Links": [
            "/jira/browse/MAHOUT-499",
            "/jira/browse/MAHOUT-772"
        ]
    },
    "MAHOUT-673": {
        "Key": "MAHOUT-673",
        "Summary": "Stochastic projection (SSVD) to use 64bit murmur hash to produce uniform distribution matrix elements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5,                                            0.6",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "17/Apr/11 20:46",
        "Updated": "21/May/11 03:19",
        "Resolved": "25/Apr/11 18:00",
        "Description": "So, per earlier discussion on the list: for random matrix Omega in stochastic projection, let's use murmur hash to generate uniformly distributed elements in a closed interval (-1,+1] instead of using Random.nextGaussian(). \nI am not sure if there's really compelling mathematical reason to do this but maybe it's just faster and more inline with practice accepted in Mahout for all this. \nThe murmur 64bit value is already in the code. I just need to figure the optimal way to convert it into a uniform distribution.\nGithub url for this issue tree: https://github.com/dlyubimov/mahout-commits/branches/MAHOUT-673, pull requests are welcome.",
        "Issue Links": []
    },
    "MAHOUT-674": {
        "Key": "MAHOUT-674",
        "Summary": "Let the TrainLogistic class could print the category feature weight probably",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Examples",
        "Assignee": "Sean R. Owen",
        "Reporter": "Stanley Xu",
        "Created": "18/Apr/11 11:03",
        "Updated": "21/May/11 03:18",
        "Resolved": "18/Apr/11 21:32",
        "Description": "In the example of trainlogistic, if we add a category feature. It will throw a NullPointerException while printing out the model.\nThe root cause is for category feature, the code try to use the feature name as the predictor rather than feature=value as the predictor while get the weight from the trained LR model.",
        "Issue Links": []
    },
    "MAHOUT-675": {
        "Key": "MAHOUT-675",
        "Summary": "LuceneIterator throws an IllegalStateException when a null TermFreqVector is encountered for a document instead of skipping to the next one",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Integration",
        "Assignee": "Ted Dunning",
        "Reporter": "Chris Jordan",
        "Created": "18/Apr/11 17:27",
        "Updated": "21/May/11 03:19",
        "Resolved": "25/Apr/11 11:39",
        "Description": "The org.apache.mahout.utils.vectors.lucene.LuceneIterator currently throws an IllegalStateException if it encounters a document with a null term frequency vector for the target field in the computeNext() method. That is problematic for people who are developing text mining applications on top of lucene as it forces them to check that the documents that they are adding to their lucene indexes actually have terms for the target field. While that check may sound reasonable, it actually is not in practice.\nLucene in most cases will apply an analyzer to a field in a document as it is added to the index. The StandardAnalyzer is pretty lenient and barely removes any terms. In most cases though, if you want to have better text mining performance, you will create your own custom analyzer. For example, in my current work with document clustering, in order to generate tighter clusters and have more human readable top terms, I am using a stop word list specific to my subject domain and I am filtering out terms that contain numbers. The net result is that some of my documents have no terms for the target field which is a desirable outcome. When I attempt to dump the lucene vectors though, I encounter an IllegalStateException because of those documents.\nNow it is possible for me to check the TokenStream of the target field before I insert into my index however, if we were to follow that approach, it means for each of my applications, I would have to perform this check. That isn't a great practice as someone could be experimenting with custom analyzers to improve text mining performance and then encounter this exception without any real indication that it was due to the custom analyzer.\nI believe a better approach is to log a warning with the field id of the problem document and then skip to the next one. That way, a warning will be in the logs and the lucene vector dump process will not halt.",
        "Issue Links": []
    },
    "MAHOUT-676": {
        "Key": "MAHOUT-676",
        "Summary": "Random samplers in a modular library",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "19/Apr/11 06:26",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "31/May/11 17:32",
        "Description": "This is a modular suite of samplers. It supplies the ability to throw away samples in a useful way. \nHere is a use case: for my recommendations, I want user activity to decide the amount of influence on the results. For the number of users who watch X number of movies: 1-5 is 20%, 6-15 is 50%, 15-30 is 30 %, and users who watch over 30 movies are not useful.\n\nIf I know the input distribution, I can supply a function to the Slice sampler to give this distribution.\nIf I don't know the distribution, I can create a Reservoir sampler for each of the three buckets. After reading the whole set, I check the sizes of the various buckets and solve for my distribution. This gives the number of users to pull from each bucket.",
        "Issue Links": []
    },
    "MAHOUT-677": {
        "Key": "MAHOUT-677",
        "Summary": "The SimpleCsvExamples didn't really parsed the double correctly with the FastLine and FastLineReader",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Examples",
        "Assignee": "Ted Dunning",
        "Reporter": "Stanley Xu",
        "Created": "20/Apr/11 12:13",
        "Updated": "21/May/11 03:19",
        "Resolved": "20/Apr/11 16:57",
        "Description": "The FastLineReader in SimpleCsvExamples.java try to parse the line quickly through parse the bytes directly from the stream without the cost of copy Strings. But it didn't parse the line correctly and will get all double values as zero in fast parsing mode",
        "Issue Links": []
    },
    "MAHOUT-678": {
        "Key": "MAHOUT-678",
        "Summary": "NullPointerException while using MixedGradient with SGD algorithm",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Stanley Xu",
        "Created": "22/Apr/11 14:03",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "21/Aug/11 21:44",
        "Description": "I am trying to use the MixedGradient in OnlineLogisticRegression algorithm. But I will get an NullPointerException randomly if I set the alpha larger than 0. \nI checked the code and found that in the RankingGradient used by MixedGradient, it assume that the target category should be only 2, rather than multiple. And the rank gradient should only be used once the Gradient object knew both the positive and negative targets. I created a simple patch to make it workable, but I am not really understand the MixedGradient method deeply, please check the patch carefully to see if it really works correctly.",
        "Issue Links": []
    },
    "MAHOUT-679": {
        "Key": "MAHOUT-679",
        "Summary": "ClusterDumper closes System.out",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "22/Apr/11 18:31",
        "Updated": "17/Aug/11 16:46",
        "Resolved": "22/Apr/11 18:35",
        "Description": "if the output file passed to ClusterDumper is null, it uses System.out.  But it always closes its input.\nClosing System.out confuses lots of stuff.",
        "Issue Links": []
    },
    "MAHOUT-680": {
        "Key": "MAHOUT-680",
        "Summary": "Running the Hadoop script through bin/mahout to set up classpath",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Frank Scholten",
        "Created": "22/Apr/11 23:03",
        "Updated": "19/Jun/11 17:45",
        "Resolved": "23/Apr/11 08:44",
        "Description": "Added a patch which allows you to run the $HADOOP_HOME/bin/hadoop command script through the bin/mahout script.\nThis way the Mahout script adds the Mahout classes to the $HADOOP_CLASSPATH so you can view sequencefiles generated by Mahout jobs with\nbin/mahout hadoop fs -text <sequencefile>\nwithout having to specify Mahout classes manually or getting ClassNotFoundExceptions",
        "Issue Links": []
    },
    "MAHOUT-681": {
        "Key": "MAHOUT-681",
        "Summary": "Remove AsymmetricSampledNormal, SampledNormal, L1, Normal Models and Distributions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Sean R. Owen",
        "Created": "24/Apr/11 09:41",
        "Updated": "21/May/11 03:19",
        "Resolved": "25/Apr/11 11:36",
        "Description": "As discussed on the mailing list, it looks like these classes are deprecated, and actually unused outside of tests. Here's my proposal to remove them.",
        "Issue Links": []
    },
    "MAHOUT-682": {
        "Key": "MAHOUT-682",
        "Summary": "The LDA output does not include the topic-probability distribution per document (p(z|d)). It outputs only the topics and corresponding words.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Jake Mannix",
        "Reporter": "Himanshu Gahlot",
        "Created": "27/Apr/11 22:45",
        "Updated": "02/Jun/11 07:26",
        "Resolved": "23/May/11 14:36",
        "Description": "The current implementation of LDA outputs only topics and their words. Many applications need the p(z|d) values of a document to use this vector as a reduced representation of the document (dimensionality reduction of document). We need to introduce a new key which would keep track of the gamma values for each document (as obtained from the document.infer() method) and writes these to the output stream and finally, PrintLDATopics should output these values per document id. Also, outputting the probabilities of words in a topic would also provide a more meaningful output.",
        "Issue Links": [
            "/jira/browse/MAHOUT-458"
        ]
    },
    "MAHOUT-683": {
        "Key": "MAHOUT-683",
        "Summary": "LDA Vectorization",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Vasil Vasilev",
        "Created": "28/Apr/11 07:49",
        "Updated": "02/Jun/11 07:26",
        "Resolved": "23/May/11 14:35",
        "Description": "Currently the result of LDA clustering algorithm is a state which describes the probability of words, part of a corpus of documents, to belong to given topics. This probability is calculated for the whole corpus\nIt is interesting, however, what is the average number of words of a given document that comes from a given topic. This information comes from the gamma vector in the LDA inference process. This vector can be used as representation of the given document for further clustering purposes (using algorithms like KMeans, Dirichlet, etc.). In this manner the dimensions of a document get reduced to the number of topics that is specified to the LDA clustering algorithm.\nWith the proposed implementation from a corpus of documents described as vectors and from the last state of LDA inference process a set of vectors with reduced dimensions is produced (a vector per a document) which represent the set of documents",
        "Issue Links": []
    },
    "MAHOUT-684": {
        "Key": "MAHOUT-684",
        "Summary": "Topics regularization for LDA",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Jake Mannix",
        "Reporter": "Vasil Vasilev",
        "Created": "28/Apr/11 13:28",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 21:53",
        "Description": "Implementation provided for the alpha parameters estimation as described in the paper of Blei, Ng and Jordan (http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf).\nRemark: there is a mistake in the last formula in A.4.2 (the signs are wrong). The correct version is described here: http://www.cs.cmu.edu/~jch1/research/dirichlet/dirichlet.pdf (page 6).",
        "Issue Links": []
    },
    "MAHOUT-685": {
        "Key": "MAHOUT-685",
        "Summary": "Make the maven release process work for 0.5",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "build",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "29/Apr/11 12:35",
        "Updated": "21/May/11 03:18",
        "Resolved": "06/May/11 14:20",
        "Description": "This is a bit of an omnibus to keep track of sorting out release problems.\nThe first one is that the release profile suppresses tests, so the test jar doesn't get built, so the build fails.",
        "Issue Links": []
    },
    "MAHOUT-686": {
        "Key": "MAHOUT-686",
        "Summary": "Upgrade to Lucene/Solr 3.1.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "03/May/11 15:48",
        "Updated": "21/May/11 03:18",
        "Resolved": "06/May/11 01:11",
        "Description": "Now that 3.1.0 is out, we should upgrade the Lucene dependencies as this should give some new analysis capabilities.",
        "Issue Links": []
    },
    "MAHOUT-687": {
        "Key": "MAHOUT-687",
        "Summary": "Random generator objects- slight refactor",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "05/May/11 05:38",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "02/Jun/11 13:13",
        "Description": "Problems:\n\nThe uncommons RepeatableRNG classes are the basis of RandomUtils.\n\t\nThese classes cheerfully ignore setSeed.\n\n\nSome people in the project want to move off Uncommons anyway.\n\nThis patch uses the org.apache.commons.math.random.RandomGenerator classes instead of org.apache.uncommons.maths.RepeatableRNG classes.\n.",
        "Issue Links": []
    },
    "MAHOUT-688": {
        "Key": "MAHOUT-688",
        "Summary": "High Document Frequency pruning for seq2sparse",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Vasil Vasilev",
        "Created": "05/May/11 13:02",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "08/Dec/11 11:12",
        "Description": "This improvement allows to prune the words with high document frequencies from the tf and tf-idf vectors produced by seq2sparse, based on the standard deviation of the words' document frequencies and specifying which rods to be pruned in a means of times this standard deviation. One good option is 3 times the standard deviation",
        "Issue Links": []
    },
    "MAHOUT-689": {
        "Key": "MAHOUT-689",
        "Summary": "runlogisti\u200bc in Mahout 0.4 does not work",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "XiaoboGu",
        "Created": "06/May/11 12:06",
        "Updated": "21/May/11 03:18",
        "Resolved": "06/May/11 12:27",
        "Description": "[gpadmin@lixsvr1 mahtest]$ mahout trainlogistic --input donut.csv --output ./model --target color --categories 2 --predictors x y --types numeric --features 20 --passes 100 --rate 50 \nRunning on hadoop, using HADOOP_HOME=/usr/local/hadoop\nNo HADOOP_CONF_DIR set, using /usr/local/hadoop/conf \n20\ncolor ~ -0.149*Intercept Term + -0.701*x + -0.427*y\n      Intercept Term -0.14885\n                   x -0.70136\n                   y -0.42740\n    0.000000000     0.000000000     0.000000000     0.000000000     0.000000000    -0.148846792     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000    -0.427403872    -0.701362221     0.000000000     0.000000000     0.000000000     0.000000000     0.000000000 \n11/05/06 15:56:48 INFO driver.MahoutDriver: Program took 2014 ms\n[gpadmin@lixsvr1 mahtest]$ mahout runlogistic --input donut.csv --model ./model --auc --confusion \nRunning on hadoop, using HADOOP_HOME=/usr/local/hadoop\nNo HADOOP_CONF_DIR set, using /usr/local/hadoop/conf \nException in thread \"main\" com.google.gson.JsonParseException: Failed parsing JSON source: java.io.FileReader@2c19e15c to Json\n        at com.google.gson.JsonParser.parse(JsonParser.java:57)\n        at com.google.gson.Gson.fromJson(Gson.java:376)\n        at com.google.gson.Gson.fromJson(Gson.java:352)\n        at org.apache.mahout.classifier.sgd.LogisticModelParameters.loadFrom(LogisticModelParameters.java:141)\n        at org.apache.mahout.classifier.sgd.LogisticModelParameters.loadFrom(LogisticModelParameters.java:154)\n        at org.apache.mahout.classifier.sgd.RunLogistic.main(RunLogistic.java:56)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:184)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:186)\nCaused by: com.google.gson.ParseException: Encountered \"\\\"updateCounts\\\"\" at line 8, column 51.\nWas expecting one of:\n    \"}\" ...\n    \",\" ...\n        at com.google.gson.JsonParserJavacc.generateParseException(JsonParserJavacc.java:658)\n        at com.google.gson.JsonParserJavacc.jj_consume_token(JsonParserJavacc.java:540)\n        at com.google.gson.JsonParserJavacc.JsonObject(JsonParserJavacc.java:59)\n        at com.google.gson.JsonParserJavacc.JsonValue(JsonParserJavacc.java:169)\n        at com.google.gson.JsonParserJavacc.Pair(JsonParserJavacc.java:89)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:72)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:76)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:76)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:76)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:76)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:76)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:76)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:76)\n        at com.google.gson.JsonParserJavacc.Members(JsonParserJavacc.java:76)\n        at com.google.gson.JsonParserJavacc.JsonObject(JsonParserJavacc.java:53)\n        at com.google.gson.JsonParserJavacc.parse(JsonParserJavacc.java:19)\n        at com.google.gson.JsonParser.parse(JsonParser.java:53)\n        ... 17 more\n[gpadmin@lixsvr1 mahtest]$",
        "Issue Links": []
    },
    "MAHOUT-690": {
        "Key": "MAHOUT-690",
        "Summary": "LanczosSolver tests take forever.  No fun.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "06/May/11 19:01",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "22/Jun/11 22:28",
        "Description": "Long test runs long.\nSample output:\n...\nRunning org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolver\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 282.859 sec\n...\nThis is very long, and should be improved, if possible.",
        "Issue Links": []
    },
    "MAHOUT-691": {
        "Key": "MAHOUT-691",
        "Summary": "\"job\" file dependent jars in lib/ aren't found by Hadoop?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.5",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "09/May/11 09:18",
        "Updated": "21/May/11 03:19",
        "Resolved": "15/May/11 16:26",
        "Description": "(marking for 0.5 until we decide it's fixed or not actually a problem)\nAs discussed on the mailing list, there's some apparent issue with packaging the \"job\" jar files with dependent jars within the lib/ directory. Changing the config to repackage everything into one jar can fix this. Patch attached for discussion.",
        "Issue Links": []
    },
    "MAHOUT-692": {
        "Key": "MAHOUT-692",
        "Summary": "OnlineSummarizer does not tolerate fewer than 100 samples",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Paul Baclace",
        "Created": "10/May/11 23:39",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 16:15",
        "Description": "If fewer than 100 samples are add()ed to an instance of org.apache.mahout.math.stats.OnlineSummarizer an exception will be thrown during a sort when getQuartile() is called:\nCaused by: java.lang.IndexOutOfBoundsException: from: 0, to: 99, size=89\n    at org.apache.mahout.math.list.AbstractList.checkRangeFromTo(AbstractList.java:87)\n    at org.apache.mahout.math.list.DoubleArrayList.sortFromTo(DoubleArrayList.java:573)\n    at org.apache.mahout.math.stats.OnlineSummarizer.sort(OnlineSummarizer.java:116)\n    at org.apache.mahout.math.stats.OnlineSummarizer.getQuartile(OnlineSummarizer.java:129)\nThe problem is that sort is on index range 0,99 but 0,n-1 should be used.",
        "Issue Links": []
    },
    "MAHOUT-693": {
        "Key": "MAHOUT-693",
        "Summary": "Locally Weighted Linear Regression implementation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Shingo Yamagami",
        "Created": "11/May/11 17:55",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "15/Oct/11 10:14",
        "Description": "Is there any time line to integrate Locally Weighted Linear Regression implimentation? \nhttps://cwiki.apache.org/confluence/display/MAHOUT/Algorithms (May 12, 2011)\nRegression > Locally Weighted Linear Regression (open)\nif it is still stated as open status, I would like to request / integrate this under\norg.apache.mahout.regression\nThank you very much,\nShingo Yamagami",
        "Issue Links": []
    },
    "MAHOUT-694": {
        "Key": "MAHOUT-694",
        "Summary": "IndexOutOfBoundException using build-reuters.sh",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.5",
        "Component/s": "Clustering",
        "Assignee": "Drew Farris",
        "Reporter": "Allan BLANCHARD",
        "Created": "12/May/11 15:04",
        "Updated": "02/Jun/11 07:26",
        "Resolved": "23/May/11 14:48",
        "Description": "I run Hadoop-0.20 on distributed mode on 10 VMs (NameNode + JobTracker + 8 DataNodes/TaskTrackers) with Mahout trunk.\nI tried to test kmeans example with build-reuters.sh but I got an IndexOutOfBoundException when it starts kmeans.\nI don't know which operation fails ... ExtractReuters, seqdirectory, seq2sparse or kmeans. Maybe I forgot a configuration ? I searched on the web and didn't find solutions ... \n------------------------ UPDATE == 05/16 -------------------------\nNameNode:/usr/local/mahout/trunk/examples/bin# ./build-reuters.sh \nPlease select a number to choose the corresponding clustering algorithm\n1. kmeans clustering\n2. lda clustering\nEnter your choice : 1\nok. You chose 1 and we'll use kmeans Clustering\n./build-reuters.sh: line 39: cd: examples/bin/: No such file or directory\nDownloading Reuters-21578\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 7959k  100 7959k    0     0   121k      0  0:01:05  0:01:05 -::-   99k\nExtracting...\nRunning on hadoop, using HADOOP_HOME=/usr/lib/hadoop-0.20\nNo HADOOP_CONF_DIR set, using /usr/lib/hadoop-0.20/src/conf \n11/05/16 09:31:20 WARN driver.MahoutDriver: No org.apache.lucene.benchmark.utils.ExtractReuters.props found on classpath, will use command-line arguments only\nDeleting all files in ./examples/bin/work/reuters-out/-tmp\n11/05/16 09:31:24 INFO driver.MahoutDriver: Program took 3471 ms\nRunning on hadoop, using HADOOP_HOME=/usr/lib/hadoop-0.20\nNo HADOOP_CONF_DIR set, using /usr/lib/hadoop-0.20/src/conf \n11/05/16 09:31:26 INFO common.AbstractJob: Command line arguments: {--charset=UTF-8, --chunkSize=5, --endPhase=2147483647, --fileFilterClass=org.apache.mahout.text.PrefixAdditionFilter, --input=./examples/bin/work/reuters-out/, --keyPrefix=, --output=./examples/bin/work/reuters-out-seqdir, --startPhase=0, --tempDir=temp}\n11/05/16 09:31:26 INFO driver.MahoutDriver: Program took 398 ms\nRunning on hadoop, using HADOOP_HOME=/usr/lib/hadoop-0.20\nNo HADOOP_CONF_DIR set, using /usr/lib/hadoop-0.20/src/conf \n11/05/16 09:31:28 INFO vectorizer.SparseVectorsFromSequenceFiles: Maximum n-gram size is: 1\n11/05/16 09:31:28 INFO vectorizer.SparseVectorsFromSequenceFiles: Minimum LLR value: 1.0\n11/05/16 09:31:28 INFO vectorizer.SparseVectorsFromSequenceFiles: Number of reduce tasks: 1\n11/05/16 09:31:29 INFO input.FileInputFormat: Total input paths to process : 1\n11/05/16 09:31:29 INFO mapred.JobClient: Running job: job_201105160929_0001\n11/05/16 09:31:30 INFO mapred.JobClient:  map 0% reduce 0%\n11/05/16 09:31:40 INFO mapred.JobClient:  map 100% reduce 0%\n11/05/16 09:31:42 INFO mapred.JobClient: Job complete: job_201105160929_0001\n[...]\n11/05/16 09:33:58 INFO common.HadoopUtil: Deleting examples/bin/work/reuters-out-seqdir-sparse/partial-vectors-0\n11/05/16 09:33:58 INFO driver.MahoutDriver: Program took 149846 ms\nRunning on hadoop, using HADOOP_HOME=/usr/lib/hadoop-0.20\nNo HADOOP_CONF_DIR set, using /usr/lib/hadoop-0.20/src/conf \n11/05/16 09:34:00 INFO common.AbstractJob: Command line arguments: {--clusters=./examples/bin/work/clusters, --convergenceDelta=0.5, --distanceMeasure=org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure, --endPhase=2147483647, --input=./examples/bin/work/reuters-out-seqdir-sparse/tfidf-vectors/, --maxIter=10, --method=mapreduce, --numClusters=20, --output=./examples/bin/work/reuters-kmeans, --overwrite=null, --startPhase=0, --tempDir=temp}\n11/05/16 09:34:00 INFO util.NativeCodeLoader: Loaded the native-hadoop library\n11/05/16 09:34:00 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\n11/05/16 09:34:00 INFO compress.CodecPool: Got brand-new compressor\nException in thread \"main\" java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n\tat java.util.ArrayList.RangeCheck(ArrayList.java:547)\n\tat java.util.ArrayList.get(ArrayList.java:322)\n\tat org.apache.mahout.clustering.kmeans.RandomSeedGenerator.buildRandom(RandomSeedGenerator.java:108)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:101)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.main(KMeansDriver.java:58)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:187)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:186)\n------------------------------------------------------------------\nEDIT : I just tried this on Mahout 0.4 and it seems to work (I use the same VM configuration). \nPS : Sorry for my very bad english",
        "Issue Links": [
            "/jira/browse/LUCENE-929"
        ]
    },
    "MAHOUT-695": {
        "Key": "MAHOUT-695",
        "Summary": "Have LDADriver determine numWords from input vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jake Mannix",
        "Reporter": "Mat Kelcey",
        "Created": "13/May/11 05:51",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "30/Sep/11 11:44",
        "Description": "It bugged me that you needed to specify the number of words directly to the LDADriver \neg ./bin/mahout lda \\\n     -i ./examples/bin/work/reuters-out-seqdir-sparse/tf-vectors \\\n     -o ./examples/bin/work/reuters-lda -k 20 -v 50000 -ow -x 20 \nwith this patch the ldadriver just checks a vector from the input to determine the size\neg ./bin/mahout lda \\\n     -i ./examples/bin/work/reuters-out-seqdir-sparse/tf-vectors \\\n     -o ./examples/bin/work/reuters-lda -k 20 -ow -x 20",
        "Issue Links": []
    },
    "MAHOUT-696": {
        "Key": "MAHOUT-696",
        "Summary": "Command line program for AdaptiveLogiscticRegression",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "XiaoboGu",
        "Created": "15/May/11 14:46",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "06/Jul/11 20:18",
        "Description": "Suggested by Ted, I'll try to write a command line program for AdaptiveLogicticRegression, but as I am not familir with the algorithm, I'll try to write a prototype for the program from a Java developer's perspactive, hope anyone else will help with the details of the algorithm.",
        "Issue Links": []
    },
    "MAHOUT-697": {
        "Key": "MAHOUT-697",
        "Summary": "How to setup the developing and debuging environment of mahout trunk with Elipse",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "15/May/11 14:59",
        "Updated": "21/May/11 03:22",
        "Resolved": "15/May/11 16:26",
        "Description": "Is there any guides about this already?",
        "Issue Links": []
    },
    "MAHOUT-698": {
        "Key": "MAHOUT-698",
        "Summary": "Hook up Automated Patch Checking for Mahout",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Grant Ingersoll",
        "Created": "16/May/11 21:43",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 16:09",
        "Description": "It would be great if we could get feedback to contributors on the basics of patches (tests exist/pass, applies cleanly, formatting, etc.)\nFrom Nigel Daley on builds@a.o\n\nI revamped the precommit testing in the fall so that it doesn't use Jira email anymore to trigger a build.  The process is controlled by\nhttps://builds.apache.org/hudson/job/PreCommit-Admin/\nwhich has some documentation up at the top of the job.  You can look at the config of the job (do you have access?) to see what it's doing.  Any project could use this same admin job \u2013 you just need to ask me to add the project to the Jira filter used by the admin job (https://issues.apache.org/jira/sr/jira.issueviews:searchrequest-xml/12313474/SearchRequest-12313474.xml?tempMax=100 ) once you have the downstream job(s) setup for your specific project.  For Hadoop we have 3 downstream builds configured which also have some documentation:\nhttps://builds.apache.org/hudson/job/PreCommit-HADOOP-Build/\nhttps://builds.apache.org/hudson/job/PreCommit-MAPREDUCE-Build/\nhttps://builds.apache.org/hudson/job/PreCommit-HDFS-Build/",
        "Issue Links": []
    },
    "MAHOUT-699": {
        "Key": "MAHOUT-699",
        "Summary": "Rename taste-webapp module to integration; move integration code there from examples",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "18/May/11 09:55",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 07:18",
        "Description": "Per discussion on the mailing list we need a more rationalized approach to \"integration\" code of the sort that lives in bits in examples, in taste-webapp, and of the sort mentioned on the mailing list in the context of MongoDB.",
        "Issue Links": []
    },
    "MAHOUT-700": {
        "Key": "MAHOUT-700",
        "Summary": "When running 'seqdirectory' on Amazon Elastic MapReduce, FileSystem.get() fails due to use of FileSystem.get(conf) instead of FileSystem.get(uri, conf)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Dave Lewis",
        "Created": "18/May/11 12:36",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 11:04",
        "Description": "After submitting a seqdirectory job to EMR:\n./elastic-mapreduce --enable-debugging -j $JOB_NAME --jar $JAR_URL --main-class org.apache.mahout.driver.MahoutDriver --arg seqdirectory --arg --input --arg $INPUT_URL --arg --output --arg $OUTPUT_URL --arg --charset --arg utf-8\nwhen $INPUT_URL or $OUTPUT_URL are s3n:// URLs, SequenceFilesFromDirectory throws an exception like this:\nException in thread \"main\" java.lang.IllegalArgumentException: This file system object (hdfs://ip-10-84-247-151.ec2.internal:9000) does not support access to the request path 's3n://dall-emr-bucket/output/out-subjot-seqfiles/chunk-0' You possibly called FileSystem.get(conf) when you should have called FileSystem.get(uri, conf) to obtain a file system supporting your path.\n\tat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:351)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.checkPath(DistributedFileSystem.java:99)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:155)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:195)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:524)\n\tat org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:871)\n\tat org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:859)\n\tat org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:851)\n\tat org.apache.mahout.text.ChunkedWriter.<init>(ChunkedWriter.java:47)\n\tat org.apache.mahout.text.SequenceFilesFromDirectory.run(SequenceFilesFromDirectory.java:63)\n\tat org.apache.mahout.text.SequenceFilesFromDirectory.run(SequenceFilesFromDirectory.java:106)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.text.SequenceFilesFromDirectory.main(SequenceFilesFromDirectory.java:81)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:187)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nI fixed the problem by changing FileSystem.get(conf) in SequenceFilesFromDirectory and ChunkedWriter to FileSystem.get(input.toUri(), conf) and FileSystem.get(output.toUri(), conf), respectively.  I also had to make a couple of changes to SequenceFilesFromDirectoryFilter and PrefixAdditionFilter to properly use those FileSystems.  I'm building with tests now, then I'll add the patch to this issue.",
        "Issue Links": []
    },
    "MAHOUT-701": {
        "Key": "MAHOUT-701",
        "Summary": "ClusterDumper writes to System.out or local filesystem only (I would like to write to s3 when running on Elastic MapReduce)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Dave Lewis",
        "Created": "18/May/11 12:41",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "15/Oct/11 10:12",
        "Description": "At the end of a kmeans job at EMR, I like to look through the clusters.  Unfortunately ClusterDumper writes to System.out or a local file.  I added a small conditional to examine the filename to see if it starts with s3n://, and if so to open up a FileSystem to write to s3 instead of to the local filesystem so that the output file is available after the cluster is shut down.  I am creating the patch now (tests are still running from the change in MAHOUT-700), so I will add the patch to this issue shortly.",
        "Issue Links": []
    },
    "MAHOUT-702": {
        "Key": "MAHOUT-702",
        "Summary": "Implement Online Passive Aggressive learner",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Hector Yee",
        "Created": "18/May/11 14:43",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 19:38",
        "Description": "Implements online passive aggressive learner that minimizes label ranking loss.",
        "Issue Links": []
    },
    "MAHOUT-703": {
        "Key": "MAHOUT-703",
        "Summary": "Implement Gradient machine",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Hector Yee",
        "Created": "19/May/11 02:23",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "04/Jun/11 19:47",
        "Description": "Implement a gradient machine (aka 'neural network) that can be used for classification or auto-encoding.\nIt will just have an input layer, identity, sigmoid or tanh hidden layer and an output layer.\nTraining done by stochastic gradient descent (possibly mini-batch later).\nSparsity will be optionally enforced by tweaking the bias in the hidden unit.\nFor now it will go in classifier/sgd and the auto-encoder will wrap it in the filter unit later on.",
        "Issue Links": []
    },
    "MAHOUT-704": {
        "Key": "MAHOUT-704",
        "Summary": "Refactor PredictionJob to use MultipleInputs for reduce side joins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5,                                            0.6",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "19/May/11 09:59",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "17/Aug/11 10:05",
        "Description": "The code in org.apache.mahout.cf.taste.hadoop.als.PredictionJob should be refactored to use org.apache.hadoop.mapreduce.lib.input.MultipleInputs for the reduce-side joins. This should spare us some M/R cycles and greatly simplify the code.\nWe'd need to add another prepareJob() method to AbstractJob in order to make this work.\nThis is a rather cosmetic feature request that can wait till after the 0.5 release.",
        "Issue Links": []
    },
    "MAHOUT-705": {
        "Key": "MAHOUT-705",
        "Summary": "MongoDB DataModel support",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Fernando Tapia Rico",
        "Created": "20/May/11 11:24",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 08:16",
        "Description": "Support for mongoDB in taste DataModels",
        "Issue Links": []
    },
    "MAHOUT-706": {
        "Key": "MAHOUT-706",
        "Summary": "reuse lucene tokenstreams",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Robert Muir",
        "Created": "20/May/11 17:00",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 17:47",
        "Description": "Currently, mahout uses Lucene's non-reusable analysis API.\nThis means that per-\"document\", a lot of objects are recreated (e.g. every TokenStream in the analysis chain, every Attribute).\nThis can create a lot of unnecessary overhead, particularly if \"documents\" are short.\nIt looks like an easy win to use the reusable API (reusableTokenStream) instead.",
        "Issue Links": []
    },
    "MAHOUT-707": {
        "Key": "MAHOUT-707",
        "Summary": "Setup Jenkins Jobs to validate our Examples/bin Scripts",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "20/May/11 18:07",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 17:07",
        "Description": "We should setup Jenkins to run our example scripts on a regular basis (See MAHOUT-694) and check for breakage.",
        "Issue Links": []
    },
    "MAHOUT-708": {
        "Key": "MAHOUT-708",
        "Summary": "Update to Hadoop 0.20.203.0",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "21/May/11 09:53",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "23/Jun/11 21:08",
        "Description": "I suggest we should move to Hadoop 0.20.203.0 for the next release. (Not 0.21 or further.) It is a much more recent branch of 0.20.x and is compile-time compatible with 0.20.2 in our code already.\nHowever I know already that switching to it causes some failures, in the Lanczos jobs for instances. Looks like something's expecting a file somewhere that isn't where it used to be. I bet it's an easy fix, but don't know what it is yet.",
        "Issue Links": []
    },
    "MAHOUT-709": {
        "Key": "MAHOUT-709",
        "Summary": "FP-Growth Redundant patterns",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Yarco Hayduk",
        "Created": "22/May/11 05:11",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "22/Jun/12 16:02",
        "Description": "The algorithm outputs more patterns that it is needed. \nI have tested Mahout's PFP-Growth algorithm with the http://www.borgelt.net/fpgrowth.html FP-Growth implementation. This implementation has an option to generate closed patterns too. \nWhen I filtered out the sub patterns from the output of Parallel FP-Growth I arrived to the same result, as in http://www.borgelt.net/fpgrowth.html\nSuccinctly, you are not outputting closed items\nI am attaching the dummy DB along with the output of both algorithms",
        "Issue Links": [
            "/jira/browse/MAHOUT-629"
        ]
    },
    "MAHOUT-710": {
        "Key": "MAHOUT-710",
        "Summary": "Implementing K-Trusses",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Tillmann Fiehn",
        "Created": "23/May/11 13:20",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "15/Oct/11 15:37",
        "Description": "We are Tillmann Fiehn and Sebastian Arnold, IT students from TU Berlin. As Sebastian Schelter already announced, we are atteding Isabel's and Sebastian's class \"Large scale data analysis and data mining\" and picked an interesting project that we want to implement in Mahout. We are open for any hints and suggestions and would appreciate if you could share your thoughts on our proposal.\nOur goal is to implement a map/reduce algorithm for finding k-trusses in a given graph. A k-truss is a nontrivial, single-component maximal subgraph, such that every edge is contained in at least k-2 triangles in the subgraph. The algorithm was proposed in the IEEE paper J. Cohen 2009: \"Graph Twiddling in a MapReduce World\" (http://www.csee.usf.edu/~anda/CIS6930-S11/papers/graph-processing-w-mapreduce.pdf) and involves a number of graph algorithms that are to our knowledge currently not present in Mahout:\nGoal: finding K-Trusses\n\nrelaxation of k-member clique\nnon-trivial, single-component maximal subgraph, s.t.\nevery edge is contained in at least k-2 triangles in the subgraph\n\nAlgorithms to be implemented on top of Mahout / Hadoop:\nsimplifyGraph: Edges -> RepresentativeEdges\n\nremoves Loops (not cycles)\naggregate duplicate edges\n\naugmentGraphWithDegrees: RepresentativeEdges -> AugmentedEdges = (Edge (v, u) , d(v), d(u))\n\naugements the edges with degree information for both nodes d(v) = |\n{E | E = (x,y) a. (x = v o. y = v) }\n|\n\nenumerateTriangles: AugmentedEdges  = (Edge, d(v), d(u)) -> Triangles (v, u, s) \n\nfinds all triangles in a Graph\n\nfindComponents: RepresentativeEdges -> ZoneAssignments (v, z)\n\nfinds all components of a graph, each identified as the order number of the lowest-order vertex contained\nconsists of:\n\t\nstep 1: find adjacent zones: Edges x Zones -> InterzoneEdges (z, z)\nstep 2: merge adjacent zones into one (the lowest-order neighbouring zone): InterzoneEdges, ZoneAssignments (v, z) -> Pairs (v, z)\n\n\n\n\nwhile true do:\n  step 1\n  if empty set interzone edges break;\n  step 2\ndone\n\n\nfindKTrusses: Edges, k -> ZoneAssignments (v, z)\n\nfinds all k-trusses of the graph\neach returned vertex v is part of a truss z\n\n\nsimplifyGraph\nwhile true do:\n  augmentGraphWithDegrees\n  enumerateTriangles\n  keep only edges contained in k-2 triangles\n  if all edges kept break;\ndone\nfindComponents\n\n\nWe suppose to create the package org.apache.mahout.graph.trusses and  org.apache.mahout.graph.components in the core module.",
        "Issue Links": []
    },
    "MAHOUT-711": {
        "Key": "MAHOUT-711",
        "Summary": "outputs miss some right frequent itemsets",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.4,                                            0.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "jinyongbo",
        "Created": "25/May/11 07:31",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "21/Aug/11 19:44",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-712": {
        "Key": "MAHOUT-524 DisplaySpectralKMeans example fails",
        "Summary": "DisplaySpectralKMeans Example Surfaces FileNotFoundException in DistributedRowMatrix.times() Usage/Implementation",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Jeff Eastman",
        "Created": "25/May/11 22:04",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "13/Sep/11 09:15",
        "Description": "Not clear if this is due to SpectralKMeansDriver usage or an implementation issue deeper inside. The error is repeatable:\nException in thread \"main\" java.lang.IllegalStateException: java.io.FileNotFoundException: File file:/home/dev/workspace/mahout/output/calculations/laplacian-3/tmp/data does not exist.\n        at org.apache.mahout.math.hadoop.DistributedRowMatrix.times(DistributedRowMatrix.java:217)\n        at org.apache.mahout.math.decomposer.lanczos.LanczosSolver.solve(LanczosSolver.java:104)\n        at org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.runJob(DistributedLanczosSolver.java:70)\n        at org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:155)\n        at org.apache.mahout.clustering.display.DisplaySpectralKMeans.main(DisplaySpectralKMeans.java:71)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:187)\nCaused by: java.io.FileNotFoundException: File file:/home/dev/workspace/mahout/output/calculations/laplacian-3/tmp/data does not exist.\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:361)\n        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:245)\n        at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:51)\n        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:201)\n        at org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n        at org.apache.mahout.math.hadoop.DistributedRowMatrix.times(DistributedRowMatrix.java:209)",
        "Issue Links": []
    },
    "MAHOUT-713": {
        "Key": "MAHOUT-713",
        "Summary": "Random Forest Prototypes",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Oleg Levchenko",
        "Created": "26/May/11 07:36",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "15/Oct/11 10:15",
        "Description": "Below is an explanation by Breinman (http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#prototype):\nPrototypes are a way of getting a picture of how the variables relate to the classification. \nFor the jth class, we find the case that has the largest number of class j cases among its k nearest neighbors, determined using the proximities. Among these k cases we find the median, 25th percentile, and 75th percentile for each variable. \nThe medians are the prototype for class j and the quartiles give an estimate of is stability. \nFor the second prototype, we repeat the procedure but only consider cases that are not among the original k, and so on. \nPrototypes for continuous variables are standardized by subtractng the 5th percentile and dividing by the difference between the 95th and 5th percentiles. \nFor categorical variables, the prototype is the most frequent value.",
        "Issue Links": []
    },
    "MAHOUT-714": {
        "Key": "MAHOUT-714",
        "Summary": "CollocDriver not runnable with ToolRunner due to private Constructor",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "29/May/11 16:35",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "04/Jun/11 08:28",
        "Description": "I noticed that CollocDriver has a private Constructor, which means that I can't run it via ToolRunner\nConfiguration configuration = ...\nString[] collocArgs =  ...\nToolRunner.run(configuration, new CollocDriver(), args);",
        "Issue Links": []
    },
    "MAHOUT-715": {
        "Key": "MAHOUT-715",
        "Summary": "Use Kryo for serializing vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "collections",
        "Assignee": "Benson Margulies",
        "Reporter": "Gustavo Salazar Torres",
        "Created": "31/May/11 02:30",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "31/May/11 17:01",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-716": {
        "Key": "MAHOUT-716",
        "Summary": "Implement Boosting",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Hector Yee",
        "Created": "01/Jun/11 01:32",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "08/Mar/14 12:04",
        "Description": "Implement boosting (grad boost variant) with l1-regularization and induction.\nThe gradient part is scalable and parallel and the induction part allows stochastic hypothesis generation for speed.",
        "Issue Links": []
    },
    "MAHOUT-717": {
        "Key": "MAHOUT-717",
        "Summary": "LDAPrintTopics only prints first topic when outputting to stdout",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Mat Kelcey",
        "Created": "01/Jun/11 05:37",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "05/Jun/11 09:39",
        "Description": "LDAPrintTopics uses a PrintWriter to output topics.\nThis PrintWriter wraps a File when an output directory is specified and System.out otherwise.\nThis PrintWriter is closed and recreated for each topic; which doesn't work for the System.out case (since it closes System.out)\nFix is to flush the stream and close only in the output directory case.",
        "Issue Links": []
    },
    "MAHOUT-718": {
        "Key": "MAHOUT-718",
        "Summary": "Small refactoring to broaden the use of Google Guava",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "02/Jun/11 09:03",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 09:09",
        "Description": "a little refactoring that introduces util classes to fetch the top k and min k elements of a collection and broadens the use of Google Guava",
        "Issue Links": []
    },
    "MAHOUT-719": {
        "Key": "MAHOUT-719",
        "Summary": "Rename current runLogistic command line program to  validateLogistic and let runLogistic do predicting against new production data",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "02/Jun/11 10:07",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "05/Jul/11 13:23",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-720": {
        "Key": "MAHOUT-720",
        "Summary": "Javadoc: non-core javadoc, and by version?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sean R. Owen",
        "Created": "02/Jun/11 17:24",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Oct/12 08:59",
        "Description": "Is the javadoc for anything but mahout-core generated in Jenkins, which we link to? it seems that core is all that's found in Mahout-Quality. We're missing other javadoc as a result.\nWe could go back to a fixed deployment of the latest release's javadoc in the static web site. That's easier to control but then again means you're focusing people on the last release.\nI don't have enough knowledge of how any of that is set up to answer this.",
        "Issue Links": []
    },
    "MAHOUT-721": {
        "Key": "MAHOUT-721",
        "Summary": "Override org.apache.mahout.math.VectorWritable#toString()",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Daisuke Miyamoto",
        "Created": "04/Jun/11 02:16",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Jun/11 08:22",
        "Description": "VectorWritable#toString method is currently default implementation.\nIt should be implemented appropriately.\nFor instance, after running RecommenderJob, I inspect the temporary files on HDFS.\n\n$ hadoop fs -libjars /path/to/mahout-core-0.5-job.jar -text temp/userVectors/part-r-00000\n\nBut this command returns\n\n2       org.apache.mahout.math.VectorWritable@15f48262\n4       org.apache.mahout.math.VectorWritable@15f48262\n\n\nIf the toString is not default implementation, we can see this result.\n\n2       {101:2.0,104:2.0,103:5.0,102:2.5}\n4       {101:5.0,106:4.0,104:4.5,103:3.0}",
        "Issue Links": []
    },
    "MAHOUT-722": {
        "Key": "MAHOUT-722",
        "Summary": "Ignore the line when input text file contains irregular entry",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Daisuke Miyamoto",
        "Created": "04/Jun/11 09:23",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "04/Jun/11 10:48",
        "Description": "RecommenderJob with usersFile/itemsFile which contains newline at end of file is failed.\n\njava.lang.NumberFormatException: For input string: \"\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n\tat java.lang.Long.parseLong(Long.java:431)\n\tat java.lang.Long.parseLong(Long.java:468)\n\tat org.apache.mahout.cf.taste.hadoop.item.UserVectorSplitterMapper.setup(UserVectorSplitterMapper.java:61)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:629)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:310)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\n\n\nI think lines which cause parse error should be ignored.",
        "Issue Links": []
    },
    "MAHOUT-723": {
        "Key": "MAHOUT-723",
        "Summary": "Incorrect handling of NUM_CLUSTERS_OPTION in FuzzyKMeansDriver class",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5,                                            0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Alex Ott",
        "Created": "05/Jun/11 14:04",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "05/Jun/11 16:03",
        "Description": "the NUM_CLUSTERS_OPTION isn't handled correctly in the FuzzyKMeansDriver, so it leads to NPE when running fuzzy k-means clustering through command-line tool with following command:\nbin/mahout fkmeans -i reuters-vectors/tfidf-vectors/ -c reuters-fkmeans-centroids -o reuters-fkmeans-clusters -dm org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure -x 10 -cl -k 21 -cd 1.0 -m 3 -ow",
        "Issue Links": []
    },
    "MAHOUT-724": {
        "Key": "MAHOUT-724",
        "Summary": "Merge eclipse module into buildtools module",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "build",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "05/Jun/11 16:14",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "06/Jun/11 10:15",
        "Description": "Having gotten away with merging two modules, I am emboldened to try another.\nRight now, we have a module for Eclipse settings, and another tiny module called buildtools that seems to support Clover in the Jenkins build. buildtools only has about one thing: checkstyle rules. And they are duplicates of what's in the Eclipse module. And \"Eclipse\" is a build tool.\nSo... what if I merged these two guys into the buildtools module?",
        "Issue Links": []
    },
    "MAHOUT-725": {
        "Key": "MAHOUT-725",
        "Summary": "Where is org.apache.mahout.cf.taste.impl.model.jdbc in v0.6?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Daniel Xiaodan Zhou",
        "Created": "09/Jun/11 14:26",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "09/Jun/11 15:55",
        "Description": "Hello, I'm working on a GSoC2011 project which is to integrate Mahout into Drupal to provide content recommender services. I'm now considering whether to use the JDBCDataModel provided by Mahout, or retrieve data into memory myself and user GenericDataModel. I see org.apache.mahout.cf.taste.impl.model.jdbc is missing in Mahout v0.6. If JDBCDataModel is removed in future release of Mahout, I'll just ignore it. Any suggestions? Thanks!",
        "Issue Links": []
    },
    "MAHOUT-726": {
        "Key": "MAHOUT-726",
        "Summary": "IntWritable / VectorWritable cast problem in classifier/clustering implementations",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "Classification,                                            Clustering",
        "Assignee": null,
        "Reporter": "Sean R. Owen",
        "Created": "09/Jun/11 16:05",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "05/Jul/11 13:27",
        "Description": "I think we've seen this sort of error, most recently reported by Hector:\nException in thread \"main\" java.lang.ClassCastException:\norg.apache.hadoop.io.IntWritable cannot be cast to\norg.apache.mahout.math.VectorWritable\n at\norg.apache.mahout.clustering.kmeans.RandomSeedGenerator.buildRandom(RandomSeedGenerator.java:90)\nat\norg.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:102)\nHector indicates it's also in LDA and Dirichlet, so probably a common issue to all, though also I suspect an easy fix whatever it is.\nTracking it formally with a JIRA to record the resolution.",
        "Issue Links": []
    },
    "MAHOUT-727": {
        "Key": "MAHOUT-727",
        "Summary": "Does bin/mahout work for binary distro?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "build",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "09/Jun/11 16:09",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "23/Jul/11 15:28",
        "Description": "Mark reports that bin/mahout doesn't work out of the box from the binary distribution.\n11/06/08 21:17:00 INFO mapred.JobClient: Task Id : attempt_201106061352_0066_r_000001_1, Status : FAILED\nError: java.lang.ClassNotFoundException: org.apache.lucene.analysis.TokenStream\nThis suggests the script isn't finding the job file, and indeed it is not in the same place in the binary distro as in the source tree. I am guessing this is the issue.",
        "Issue Links": []
    },
    "MAHOUT-728": {
        "Key": "MAHOUT-728",
        "Summary": "Migrate from collections to fastutil?",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "collections",
        "Assignee": "Benson Margulies",
        "Reporter": "Sean R. Owen",
        "Created": "09/Jun/11 16:11",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jun/12 10:48",
        "Description": "Filing this placeholder on behalf of Benson, who suggested we might use fastutil instead of continuing to maintain mahout-collecitons.",
        "Issue Links": []
    },
    "MAHOUT-729": {
        "Key": "MAHOUT-729",
        "Summary": "Refactoring: Use Maps.newHashMap() and Lists.newArrayList() from Guava globally",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Jun/11 23:13",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "11/Jun/11 11:25",
        "Description": "I put on my refactoring hat today and started to replace all calls to \"new HashMap\" with Maps.newHashMap() as well as all calls to \"new ArrayList\" with Lists.newArrayList() from Guava. This should make the code much more readable as the generic type of a variable needs to be written only once per line. I also refactored some minor things I encountered along the way.",
        "Issue Links": []
    },
    "MAHOUT-730": {
        "Key": "MAHOUT-730",
        "Summary": "Replace EasyMock with Mockito",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Jun/11 23:43",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "13/Jun/11 08:52",
        "Description": "I'd like to switch our mock library from EasyMock to Mockito. The later one needs less code (and doesn't need to be \"replayed\") and it's more intuitive to write tests with it in my experience. Switching shouldn't be a big effort.",
        "Issue Links": []
    },
    "MAHOUT-731": {
        "Key": "MAHOUT-731",
        "Summary": "Add SQL92JDBCDataModel",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Daniel Xiaodan Zhou",
        "Created": "11/Jun/11 14:18",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "16/Jun/11 09:47",
        "Description": "I'm thinking to work on a patch that adds SQL92JDBCDataModel based on the sample code of PostgreSQLJDBCDataModel. This class will only use standard SQL92. It won't be high performance, but would have maximum compatibility to other popular DBMS, such as Oracle, SQLlite, etc. This is quite important to Drupal/Mahout integration.\nBefore I actually write the code, is there any suggestions or concerns? Thanks!",
        "Issue Links": []
    },
    "MAHOUT-732": {
        "Key": "MAHOUT-732",
        "Summary": "Implement ranking autoencoder on top of gradient machine",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Ted Dunning",
        "Reporter": "Hector Yee",
        "Created": "13/Jun/11 21:03",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "08/Mar/14 12:00",
        "Description": "Implement a ranking autoencoder clusterer based on top of gradient machine.\nSee https://docs.google.com/present/edit?id=0AQC247eq7Jp5ZGZ6NXpyOWhfMjlmM2pzdjRkZw&authkey=CNj2h98P&hl=en_US\nfor details",
        "Issue Links": []
    },
    "MAHOUT-733": {
        "Key": "MAHOUT-733",
        "Summary": "how to use the closed patterns to mining the association rules in PFPG algorithm",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.4",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "jinyongbo",
        "Created": "14/Jun/11 03:26",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "16/Jun/11 09:49",
        "Description": "I have got the output of the PFPG algorithm.But the output are closed patterns, not the frequent patterns.\nHow can I mining the association rule use the closed patterns?\nIs there any reference papers?",
        "Issue Links": []
    },
    "MAHOUT-734": {
        "Key": "MAHOUT-734",
        "Summary": "Command-line utils for HMM",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sergey Bartunov",
        "Created": "14/Jun/11 22:32",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "27/Jun/11 18:46",
        "Description": "Mahout already have HMM functionality, but it presents only in API.\nCommand-line tools should be added and registered in driver.classes.props\nThese patches are get from git against trunk of mahout's github\n[this is my \"traning\" issue in Jira to learn how to commit patches to the Mahout, so please be merficul]",
        "Issue Links": []
    },
    "MAHOUT-735": {
        "Key": "MAHOUT-735",
        "Summary": "Command line for Decision tree",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "15/Jun/11 15:23",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "19/Jun/11 11:25",
        "Description": "Can we try to build a command line for decision tree algorithm",
        "Issue Links": []
    },
    "MAHOUT-736": {
        "Key": "MAHOUT-736",
        "Summary": "Save one pass through the data in ItemSimilarityJob and RecommenderJob by intelligently using counters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "17/Jun/11 14:26",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "20/Jun/11 09:56",
        "Description": "Both ItemSimilarityJob and RecommenderJob have to find the overall number of users. This was done with an M/R pass over the data. Instead of that we can use a counter in a previous pass over the data (similar to the way the total number of ngrams if collected in CollocDriver) and thereby save one pass through the data.",
        "Issue Links": []
    },
    "MAHOUT-737": {
        "Key": "MAHOUT-737",
        "Summary": "Implicit Alternating Least Squares SVD",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Tamas Jambor",
        "Created": "20/Jun/11 07:52",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "27/Apr/12 15:34",
        "Description": "I am sharing this Java implementation of mine that is based on the paper - Collaborative Filtering with Implicit Datasets. The implementation is multi-treading and can be easily extended to use it on Hadoop. In fact this approach would possibly work with non-implicit datasets, but further testing is needed. The algorithm is tried and tested on an implicit TV-viewing dataset, and the performance was pretty good (details to follow).",
        "Issue Links": []
    },
    "MAHOUT-738": {
        "Key": "MAHOUT-738",
        "Summary": "Collocation driver has long being statically cast to an int",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "peter andrews",
        "Created": "21/Jun/11 15:09",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "22/Jun/11 17:05",
        "Description": "org.apache.mahout.vectorizer.collocations.llr.LLRReducer, which is part of the collocation driver, statically casts a long to an int.\nprivate long ngramTotal;\n...\nint k11 = ngram.getFrequency(); /* a&b */\nint k12 = gramFreq[0] - ngram.getFrequency(); /* a&!b */\nint k21 = gramFreq[1] - ngram.getFrequency(); /* !b&a */\nint k22 = (int) (ngramTotal - (gramFreq[0] + gramFreq[1] - ngram.getFrequency())); /* !a&!b */\nThese numbers are then fed into \norg.apache.mahout.math.stats.LogLikelihood\nspecifically the function below.\npublic static double logLikelihoodRatio(int k11, int k12, int k21, int k22) {\n  // note that we have counts here, not probabilities, and that the entropy is not normalized.\n  double rowEntropy = entropy(k11, k12) + entropy(k21, k22);\n  double columnEntropy = entropy(k11, k21) + entropy(k12, k22);\n  double matrixEntropy = entropy(k11, k12, k21, k22);\n  if (rowEntropy + columnEntropy > matrixEntropy) \n{\n    // round off error\n    return 0.0;\n  }\n  return 2.0 * (matrixEntropy - rowEntropy - columnEntropy);\n}\nIn short if the long ngramTotal is larger than Integer.MAX_VALUE (which will happen in large datasets), then the driver will either crash or in the case that it casts to a negative int, will continue as usual but produce no output due to error checking.",
        "Issue Links": []
    },
    "MAHOUT-739": {
        "Key": "MAHOUT-739",
        "Summary": "MapReduce job to compute the degree distribution of an undirected graph",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "21/Jun/11 19:34",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "22/Jun/11 07:59",
        "Description": "The distribution of node degrees in a graph is an interesting measurement that is easy to compute with MapReduce. As a first step we compute it for undirected graphs.",
        "Issue Links": []
    },
    "MAHOUT-740": {
        "Key": "MAHOUT-740",
        "Summary": "add SQL92JDBCItemSimilarity",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Daniel Xiaodan Zhou",
        "Created": "21/Jun/11 20:32",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "26/Jun/11 08:09",
        "Description": "I'm thinking to add org.apache.mahout.cf.taste.impl.similarity.jdbc.SQL92JDBCItemSimilarity. It would directly extends AbstractJDBCItemSimilarity. Lots of code would be copied from MySQLJDBCItemSimilarity.\nAny comments/suggestions before I starts the patch?",
        "Issue Links": []
    },
    "MAHOUT-741": {
        "Key": "MAHOUT-741",
        "Summary": "MapReduce job to compute the local clustering coefficient in an undirected graph",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Jun/11 12:22",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "23/Jun/11 12:26",
        "Description": "Distributed computation of the local clustering coefficient of the vertices of an undirected graph. The local clustering coefficient is a measure for the \"connectedness\" of a vertex in its neighborhood and is computed by dividing the number of closed triangles with a vertex neighbors by the number of possible triangles of this vertex with it's neighbours.\nhttp://en.wikipedia.org/wiki/Clustering_coefficient#Local_clustering_coefficient\nThis measure is easy to compute when we already have enumerated the triangles of the graph.",
        "Issue Links": []
    },
    "MAHOUT-742": {
        "Key": "MAHOUT-742",
        "Summary": "Pagerank implementation in Map/Reduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Christoph Nagel",
        "Created": "22/Jun/11 16:18",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "11/Jul/11 15:49",
        "Description": "Hi,\nmy name is Christoph Nagel. I'm student on technical university Berlin and participating on the course of Isabel Drost and Sebastian Schelter.\nMy work is to implement the pagerank-algorithm, where the pagerank-vector fits in memory.\nFor the computation I used the naive algorithm shown in the book 'Mining of Massive Datasets' from Rajaraman & Ullman (http://www-scf.usc.edu/~csci572/2012Spring/UllmanMiningMassiveDataSets.pdf).\nMatrix- and vector-multiplication are done with mahout methods.\nMost work is the transformation the input graph, which has to consists of a nodes- and edges file.\nFormat of nodes file: <node>\\n\nFormat of edges file: <startNode>\\t<endNode>\\n\nTherefore I created the following classes:\n\nLineIndexer: assigns each line an index\nEdgesToIndex: indexes the nodes of the edges\nEdgesIndexToTransitionMatrix: creates the transition matrix\nPagerank: computes PR from transition matrix\nJoinNodesWithPagerank: creates the joined output\nPagerankExampleJob: does the complete job\n\nEach class has a test (not PagerankExampleJob) and I took the example of the book for evaluating.",
        "Issue Links": []
    },
    "MAHOUT-743": {
        "Key": "MAHOUT-743",
        "Summary": "Allow use of random seeds during unit tests",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "27/Jun/11 06:03",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "27/Jun/11 10:54",
        "Description": "The RandomUtils package has a trick for running unit tests repeatably. All random generators created with RandomUtils.getRandom() are started with seed 0. Unfortunately, all random generators started with RandomUtils.getRandom(seed) effectively run from a seed of 0. Thus, code which deliberately creates Random objects with seeds does not work correctly under unit tests.\nThe problem is that the logic in RandomWrapper tests for unit test mode and settable seed mode in the wrong order.",
        "Issue Links": []
    },
    "MAHOUT-744": {
        "Key": "MAHOUT-744",
        "Summary": "SparseMatrix should iterate over rows sparsely",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "27/Jun/11 10:59",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "27/Jun/11 16:09",
        "Description": "iterate() ought to iterate over rows that exist only, not every row (that's the job of iterateAll()).",
        "Issue Links": []
    },
    "MAHOUT-745": {
        "Key": "MAHOUT-745",
        "Summary": "HADOOP_CONF_DIR is not passed to the hadoop script correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Alex Kozlov",
        "Created": "28/Jun/11 00:42",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "28/Jun/11 07:14",
        "Description": "When one does not set the HADOOP_CONF_DIR, it is set to HADOOP_HOME/src/conf (usually the config files are in the $HADOOP_HOME/conf directory).  But even then the location is not passed correctly to the hadoop script as we either need to export this setting or give it explicitly to the hadoop command.  Here is a patch for the latter.",
        "Issue Links": []
    },
    "MAHOUT-746": {
        "Key": "MAHOUT-746",
        "Summary": "Refactoring of the parallel Naive Bayes implementation in org.apache.mahout.classifier.naivebayes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "28/Jun/11 09:59",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "04/Jul/11 06:50",
        "Description": "I refactored the code in org.apache.mahout.classifier.naivebayes to extend AbstractJob, decoupled the model serialization from the job output, extracted trainer classes and tried to clarify naming and reduce code complexity. I also added tests for the training M/R code as well as a toy integration test.\nIt would be great if someone could review my patch to make sure I didn't break anything.",
        "Issue Links": []
    },
    "MAHOUT-747": {
        "Key": "MAHOUT-747",
        "Summary": "Entropy implementation in Map/Reduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Christoph Nagel",
        "Created": "29/Jun/11 09:32",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "05/Jul/11 13:54",
        "Description": "Hi again,\nbecause I got much to work with entropy and information gain ratio, I want to implement the following distributed algorithms:\n\nEntropy (https://secure.wikimedia.org/wikipedia/en/wiki/Entropy_%28information_theory%29)\nConditional Entropy (https://secure.wikimedia.org/wikipedia/en/wiki/Conditional_entropy)\nInformation Gain\nInformation Gain Ratio (https://secure.wikimedia.org/wikipedia/en/wiki/Information_gain_ratio)\n\nThis issue is at first only for entropy.\nSome questions:\n\nIn which package do the classes belong. I put them first at 'org.apache.mahout.math.stats', don't know if this is right, because they are components of information retrieval.\nEntropy only reads a set of elements. As input i took a sequence file with keys of type Text and values anyone, because I only work with the keys. Is this the best practise?\nIs there a generic solution, so that the type of keys can be anything inherited from Writable?\n\nIn Hadoop is a TokenCounterMapper, which emits each value with an IntWritable(1). I added a KeyCounterMapper into 'org.apache.mahout.common.mapreduce' which does the same with the keys.\nWill append my patch soon.\nRegards, Christoph.",
        "Issue Links": []
    },
    "MAHOUT-748": {
        "Key": "MAHOUT-748",
        "Summary": "WikipediaAnalyzer in 0.5 would fail due to lucene3.1's CharArraySet.iterator() returns an \"char[]\"  iterator instead of a \"String\" iterator",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "Examples",
        "Assignee": "Sean R. Owen",
        "Reporter": "steven zhuang",
        "Created": "29/Jun/11 10:50",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "01/Jul/11 07:50",
        "Description": "in mahout0.5, the class org.apache.mahout.analysis.WikipediaAnalyzer would fail to be constructed.\nthe statement around WikipediaAnalyzer.java line 38:\n   stopSet = (CharArraySet) StopFilter.makeStopSet(Version.LUCENE_31,\n        StopAnalyzer.ENGLISH_STOP_WORDS_SET.toArray(new String[StopAnalyzer.ENGLISH_STOP_WORDS_SET.size()]));\n  will raise an ArrayStoreException exception due to \n          StopAnalyzer.ENGLISH_STOP_WORDS_SET.toArray(String[] ) will throw such an exception.\n   the cause is that in lucene3.1, when version number is bigger than 3.0, the CharArraySet.iterator() method returns an 'char[]' iterator instead of an \"String\" list.\nsee code from CharArraySet.java:\n  @Override @SuppressWarnings(\"unchecked\")\n  public Iterator<Object> iterator() \n{\n    // use the AbstractSet#keySet()'s iterator (to not produce endless recursion)\n    return map.matchVersion.onOrAfter(Version.LUCENE_31) ?\n      map.originalKeySet().iterator() : (Iterator) stringIterator();\n  }\n\nso in WikipediaAnalyzer() we may need to make a transform from char[] to String to make it work.",
        "Issue Links": []
    },
    "MAHOUT-749": {
        "Key": "MAHOUT-749",
        "Summary": "MeanShift Cannot Use Multiple Reducers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "29/Jun/11 16:47",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "23/Jul/11 14:47",
        "Description": "The MeanShiftCanopy clustering job sets the numReducers=1 and this severely limits its scalability for larger jobs.",
        "Issue Links": []
    },
    "MAHOUT-750": {
        "Key": "MAHOUT-750",
        "Summary": "IndexOutOfBoundsException within sort method of OnlineSummarizer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "XiaoboGu",
        "Created": "30/Jun/11 02:40",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "30/Jun/11 07:45",
        "Description": "When the total number of samples added to OnlineSummarizer is less than 100 , then sort() will thrown this :\n  private void sort() {\n    if (!sorted && starter != null) \n{\n      starter.sortFromTo(0, 99);\n      sorted = true;\n    }\n  }\nAnd I think it should be\n private void sort() {\n    if (!sorted && starter != null) \n{\n      starter.sortFromTo(0, Math.min(starter.size() - 1, 99));\n      sorted = true;\n    }\n  }",
        "Issue Links": []
    },
    "MAHOUT-751": {
        "Key": "MAHOUT-751",
        "Summary": "(Mapreduce)ClassCastException: org.apache.hadoop.io.LongWritable cannot be cast to ....io.Text",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "patrick.J",
        "Created": "02/Jul/11 07:41",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "02/Jul/11 07:58",
        "Description": "Could somebody help me ?\nMy mapreduce program cast a Exception like:\n11/07/01 20:18:06 INFO mapreduce.Job: Task Id : attempt_201107011635_0005_m_000000_0, Status : FAILED\njava.lang.ClassCastException: org.apache.hadoop.io.LongWritable cannot be cast to org.apache.hadoop.io.Text\n        at Sum$MapClass.map(Sum.java:15)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:652)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:328)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:217)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:742)\n        at org.apache.hadoop.mapred.Child.main(Child.java:211)\nThere's the code:\nimport java.io.*;\nimport java.util.*;\nimport org.apache.hadoop.fs.Path;\nimport org.apache.hadoop.conf.*;\nimport org.apache.hadoop.io.*;\nimport org.apache.hadoop.mapreduce.*;\nimport org.apache.hadoop.mapreduce.split.*;\nimport org.apache.hadoop.mapreduce.lib.input.*;\nimport org.apache.hadoop.mapreduce.lib.output.*;\nimport org.apache.hadoop.util.*;\npublic class Sum extends Configured implements Tool{\n        public static class MapClass extends \n        Mapper<Text, Text, Text, IntWritable>{\n        private  static int i;\n        private  static int j;\n        private  static int a;\n        private  static int b;\n        private String user;\n                public void map(Text key,Text value,Context context) \n                                throws IOException, InterruptedException{\n                                String[] user_score = value.toString().split(\" \");\n                                for(i=1;i<user_score.length-1;i+=2)\n                                   for(j=i+2;j<user_score.length-1;j+=2)\n{\n                                   user = user_score[i]+\" \"+user_score[j];\n                                   a = Integer.parseInt(user_score[i+1]);\n                                   b = Integer.parseInt(user_score[j+1]);\n                                   context.write(new Text(user),new IntWritable(a*b));\n                                   }\n                            }\n                            }\n        public static class Reduce extends\n        Reducer<Text,IntWritable,Text,IntWritable>{\n                public void reduce(Text key,Iterable<IntWritable> values, Context context) throws IOException, InterruptedException{\n                           int sum = 0;  \n             for(IntWritable val:values)\n{\n                             sum += val.get();     \n                                }\n \n             context.write(key, new IntWritable(sum));  \n                           }\n                           }\n     public int run(String[] args) throws Exception\n{\n               Configuration conf = getConf();\n                Job job = new Job(conf,\"Myjob\");\n                job.setJarByClass(Sum.class);\n\n                Path in = new Path(args[0]);\n                Path out = new Path(args[1]);\n                FileInputFormat.setInputPaths(job,in);\n                FileOutputFormat.setOutputPath(job,out);\n\n                job.setMapperClass(MapClass.class);\n                job.setCombinerClass(Reduce.class); \n                job.setReducerClass(Reduce.class);\n\n                job.setInputFormatClass(TextInputFormat.class);\n                job.setOutputFormatClass(TextOutputFormat.class);\n              \n\n                job.setMapOutputKeyClass(Text.class);  \n                job.setMapOutputValueClass(IntWritable.class);\n                job.setOutputKeyClass(Text.class);\n                job.setOutputValueClass(IntWritable.class);\n\n                System.exit(job.waitForCompletion(true)?0:1);\n                \n                return 0;\n                }\n        public static void main (String[] args) throws Exception\n{\n                int res = ToolRunner.run(new Configuration(),new Sum(),args);\n                System.exit(res);\n                }\n\n }",
        "Issue Links": []
    },
    "MAHOUT-752": {
        "Key": "MAHOUT-752",
        "Summary": "Semantic Vectors: generate and use vectors from User/Item Taste data models",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "02/Jul/11 07:45",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "05/Jul/11 13:26",
        "Description": "This package has two parts:\n\nSemanticVectorFactory creates geometric vectors based on non-geometric User/Item ratings.\nVectorDataModel stores these and does preference evaluation based on the vectors and a given DistanceMeasure\n\nThis is a large exploration of the Semantic Vectors concept: http://code.google.com/p/semanticvectors/. And was the inspiration for this project.",
        "Issue Links": []
    },
    "MAHOUT-753": {
        "Key": "MAHOUT-753",
        "Summary": "MurmurHashRandom class: subclass of java.util.Random that uses MurmurHash",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "03/Jul/11 21:36",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "12/Jul/11 07:53",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-754": {
        "Key": "MAHOUT-754",
        "Summary": "add option abbreviation error in RecommenderJob.java",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "lufeng",
        "Created": "04/Jul/11 07:32",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "04/Jul/11 08:23",
        "Description": "/core/src/main/java/org/apache/mahout/cf/taste/hadoop/item/RecommenderJob.java\n110 line\n--------------\naddOption(\"maxPrefsPerUser\", \"mp\",\n        \"Maximum number of preferences considered per user in final recommendation phase\",\n        String.valueOf(UserVectorSplitterMapper.DEFAULT_MAX_PREFS_PER_USER_CONSIDERED));\n    addOption(\"minPrefsPerUser\", \"mp\", \"ignore users with less preferences than this in the similarity computation \"\n        + \"(default: \" + DEFAULT_MIN_PREFS_PER_USER + ')', String.valueOf(DEFAULT_MIN_PREFS_PER_USER));\n--------------\nhave the same otion abbreviation.",
        "Issue Links": []
    },
    "MAHOUT-755": {
        "Key": "MAHOUT-755",
        "Summary": "RecommenderServlet response content types for XML and JSON",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Julian Bell",
        "Created": "06/Jul/11 15:54",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "06/Jul/11 19:03",
        "Description": "In my opinion I believe the following changes should be made:\nFor the writeJSON method, the content type should be changed from \"text/plain\" to \"application/json\".  It's not as 'friendly' to some browsers but is more technically correct.  See http://www.ietf.org/rfc/rfc4627.txt\nFor the writeXML method, if you don't put the character encoding type into the same line at the content type, then the encoding type can be ignored and decoded as ASCII (see http://annevankesteren.nl/2005/03/text-xml).  So you could change the content type from \"text/xml\" to \"text/xml; charset=utf-8\" but it's probably better to change it to \"application/xml\".  See http://www.grauw.nl/blog/entry/489",
        "Issue Links": []
    },
    "MAHOUT-756": {
        "Key": "MAHOUT-756",
        "Summary": "VectorList (Matrix implementation) does not maintain cardinality getters correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "10/Jul/11 22:17",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "12/Jul/11 22:15",
        "Description": "VectorList (implements Matrix) is dynamically expandable, row-wise. There are three different ways to query the size of a Matrix, and VectorList does not correctly supply these values.",
        "Issue Links": []
    },
    "MAHOUT-757": {
        "Key": "MAHOUT-757",
        "Summary": "RowIdJob does not use Mahout's standard CLI parameters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "11/Jul/11 19:29",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "23/Jul/11 14:51",
        "Description": "RowIdJob doesn't use --input and --output and should for taking in it's arguments",
        "Issue Links": []
    },
    "MAHOUT-758": {
        "Key": "MAHOUT-758",
        "Summary": "seqdirectory does not preserve full, correct pathnames in sequence files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "Examples",
        "Assignee": "Sean R. Owen",
        "Reporter": "Carson Holgate",
        "Created": "11/Jul/11 19:44",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/Feb/12 13:53",
        "Description": "When I run seqdirectory on a complex directory structure in HDFS, the sequence files produced do not contain the correct path to the files.",
        "Issue Links": []
    },
    "MAHOUT-759": {
        "Key": "MAHOUT-759",
        "Summary": "improve the output for ItemSimilarityJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Han Hui Wen",
        "Created": "13/Jul/11 06:07",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "24/Jun/12 06:10",
        "Description": "Now the output of ItemSimilarityJob like following:\n-7757148334301255842\t8179634876330318523\t0.003430531732418525\n-7748456450926673883\t-4835531939219667484\t0.2\n-7748456450926673883\t-4314955996498817413\t0.5\n-7748456450926673883\t2808714190706572296\t0.16666666666666666\n-7748456450926673883\t6553837338030757853\t0.14285714285714285\n-7748456450926673883\t8751415108300656176\t0.25\n-7747582778903926086\t-7015341798833970389\t0.05\n-7745456649800833279\t-4355275072474512298\t4.2444821731748726E-4\n-7743453627722079138\t-3667977661496669483\t0.0625\n-7743453627722079138\t5506208171850960507\t0.0625\n-7743453627722079138\t7221367701058721462\t0.0625\n-7721326863046534787\t4345458182369739840\t0.1111111111111111\nIt's hard to store and view those similar items for one item. can we traverse   them same as RecommenderJob like following:\n-9220680374247203656\t[1352180348488328600:2.5,-7757148334301255842:2.5,-7490490145790861630:2.5,-2522983126042570313:2.5,-6799281597153282746:2.5,2068144185705723774:2.5,-6007350693723349387:2.5,-6926986971196173463:2.5,5406899818760113425:2.5,-1490410533166829581:2.5,-27094582027403342:2.5,5665136340246000627:2.5]\n-9218599019595753787\t[7535853797920985421:2.5,6375444791143058470:2.5,-6278686364859964742:2.5,4842183991621375854:2.5,-5371123101058190798:2.5,8606934083257321678:2.5,8043580185091202137:2.5,5264973095582397115:2.5,1990532764981555035:2.5,5406899818760113425:2.5,-5208048021997301514:2.5,-5565838412826072017:2.5]",
        "Issue Links": []
    },
    "MAHOUT-760": {
        "Key": "MAHOUT-760",
        "Summary": "\"org.apache.mahout.fpm.pfpgrowth.PFPGrowthTest\" test fails during install",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Chintamani",
        "Created": "13/Jul/11 18:17",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "14/Jul/11 05:28",
        "Description": "mvn install core fails because of a single failed test - \"org.apache.mahout.fpm.pfpgrowth.PFPGrowthTest\" with the following error (extracted from target/surefire-reports/org.apache.mahout.fpm.pfpgrowth.PFPGrowthTest.txt)\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.615 sec <<< FAILURE!\ntestStartParallelFPGrowth(org.apache.mahout.fpm.pfpgrowth.PFPGrowthTest)  Time elapsed: 6.587 sec  <<< FAILURE!\norg.junit.ComparisonFailure: expected:<\n{[D=0, E=1, A=0, B=0, C]=1}\n> but was:<\n{[A=0, B=0, C=1, D=0, E]=1}\n>\n        at org.junit.Assert.assertEquals(Assert.java:123)\n        at org.junit.Assert.assertEquals(Assert.java:145)\n        at org.apache.mahout.fpm.pfpgrowth.PFPGrowthTest.testStartParallelFPGrowth(PFPGrowthTest.java:95)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:60)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37)\n        at java.lang.reflect.Method.invoke(Method.java:611)\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)\n        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n        at org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:119)\n        at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:101)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:60)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:37)\n        at java.lang.reflect.Method.invoke(Method.java:611)\n        at org.apache.maven.surefire.booter.ProviderFactory$ClassLoaderProxy.invoke(ProviderFactory.java:103)\n        at $Proxy0.invoke(Unknown Source)\n        at org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:150)\n        at org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcess(SurefireStarter.java:91)\n        at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:69)\nEvery other test in all the components succeed.",
        "Issue Links": []
    },
    "MAHOUT-761": {
        "Key": "MAHOUT-761",
        "Summary": "Emitting cluster points should have the option of emitting the distance and potentially other related metrics",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "13/Jul/11 20:41",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "15/Oct/11 10:20",
        "Description": "See http://www.lucidimagination.com/search/document/c5502e401f59f799/emitting_distance_from_centroid_for_k_means",
        "Issue Links": []
    },
    "MAHOUT-762": {
        "Key": "MAHOUT-762",
        "Summary": "VectorDumper fails to parse --help option",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Dhruv Kumar",
        "Created": "14/Jul/11 15:34",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "14/Jul/11 16:25",
        "Description": "In VectorDumper.java, the Group Builder does not add help as a command line argument. This raises an exception when the user tries \"bin/mahout vectordump --help.\"",
        "Issue Links": []
    },
    "MAHOUT-763": {
        "Key": "MAHOUT-763",
        "Summary": "Map-Side Distance Comparison",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "14/Jul/11 17:20",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "02/Aug/11 21:19",
        "Description": "KMeans currently on the map-side calculates the distance between a set of seeds and all other vectors.  It would be handy to have a generalization of this that, given a set of vectors that fits in memory (the seeds) and other points, emit <seed id, other id, distance> according to the distance measure.  This is similar to the RowSimilarityJob, but much simpler and not as general purpose.",
        "Issue Links": []
    },
    "MAHOUT-764": {
        "Key": "MAHOUT-764",
        "Summary": "Rationalize DataModel.getNumUsersWithPreferenceFor() API",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "16/Jul/11 19:04",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "17/Jul/11 11:53",
        "Description": "DataModel has a method getNumUsersWithPreferenceFor(long... itemIDs). It does what it says. However I think this is a suboptimal API (for which we have me to blame).\n\nAll calls to this involve 1 or 2 arguments only\nNo implementation supports more than 2 arguments, which makes the signature misleading\nAll implementations internally have some logic like \"if it's 1, do X, if it's args, do Y, otherwise fail\"\nCalling this method, which happens frequently, always incurs the overhead of allocating a long[]\n\nThe change is simple: make this two methods, with one and two args respectively. Implementations would then just split their current implementation into these two methods.\nThere is no API change for callers, at all, if invoked with 1 or 2 args. I assume callers are not trying 3+ args now, as it has never been supported.\nIt does involve a straightforward change to implementors of DataModel.\nOpen for comment?",
        "Issue Links": []
    },
    "MAHOUT-765": {
        "Key": "MAHOUT-765",
        "Summary": "Upgrade Lucene to latest release or wait for LUCENE-3151",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "18/Jul/11 21:38",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "17/Aug/11 11:03",
        "Description": "Lucene is up to 3.3, we should upgrade to that, or we should wait for LUCENE-3151 and move to that.  Either way, we should upgrade Lucene.",
        "Issue Links": []
    },
    "MAHOUT-766": {
        "Key": "MAHOUT-766",
        "Summary": "fuzzy kmeans - all cluster with the same top terms",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering,                                            Examples",
        "Assignee": "Jeff Eastman",
        "Reporter": "Paulo Magalhaes",
        "Created": "18/Jul/11 22:17",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "18/Oct/11 00:19",
        "Description": "believe there is something wrong with fkmeans in trunk. \nI am using code from trunk (last checkout 6/30/11). To recreate is very simple:\n1) change examples/bin/build-reuters.sh to use fkmeans and set -m 2\n2) run build-reuters.sh\n3) Dump the cluster. I'm doing: ../../bin/mahout clusterdump -dt sequencefile -s ./mahout-work/reuters-kmeans/clusters-6 -b 100 -o ./reuters-clusterdump.txt  -d ./mahout-work/reuters-out-seqdir-sparse-kmeans/dictionary.file-0\nhere is what the clusters look like:\nSV-15898{n=34 c=[0:0.020, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.7254762602900604\n\t\tmln                                     =>  1.2510936664951733\n\t\tdlrs                                    =>  1.1340145215097008\n\t\t3                                       =>  1.0643797240793276\n\t\tpct                                     =>  1.0422760712239152\n\t\treuter                                  =>  1.0202689935247569\n\t\tits                                     =>  0.9997771992646881\n\t\tfrom                                    =>  0.9903731234557381\n\t\tyear                                    =>  0.8855389859684145\n\t\tvs                                      =>  0.8291746545786391\n:SV-14766{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6406710289350412\n\t\tmln                                     =>  1.2174993414858022\n\t\tdlrs                                    =>  1.0937941570322955\n\t\t3                                       =>  1.0334420773050856\n\t\tpct                                     =>   0.991539915235039\n\t\treuter                                  =>   0.990042452019326\n\t\tits                                     =>  0.9508638527143669\n\t\tfrom                                    =>  0.9403885495991262\n\t\tvs                                      =>   0.865437130369746\n\t\tyear                                    =>  0.8463503194752994\n:SV-14854{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>   1.641260962665307\n\t\tmln                                     =>   1.217806578134094\n\t\tdlrs                                    =>  1.0941157210136143\n\t\t3                                       =>  1.0336934328877394\n\t\tpct                                     =>   0.991895013999163\n\t\treuter                                  =>  0.9902889592990656\n\t\tits                                     =>  0.9512076670014483\n\t\tfrom                                    =>  0.9407384847445094\n\t\tvs                                      =>  0.8653426311034671\n\t\tyear                                    =>  0.8466407590692175\n:SV-14890{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6410352907185948\n\t\tmln                                     =>    1.21769021136256\n\t\tdlrs                                    =>  1.0939933408434481\n\t\t3                                       =>  1.0335977297579235\n\t\tpct                                     =>   0.991759193577722\n\t\treuter                                  =>  0.9901951250301172\n\t\tits                                     =>  0.9510761761632947\n\t\tfrom                                    =>  0.9406047832581563\n\t\tvs                                      =>  0.8653814488835572\n\t\tyear                                    =>  0.8465301083353372\n:SV-14972{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>   1.640981249652196\n\t\tmln                                     =>  1.2176595452829564\n\t\tdlrs                                    =>   1.093962519439548\n\t\t3                                       =>  1.0335737897463568\n\t\tpct                                     =>  0.9917266257955816\n\t\treuter                                  =>  0.9901715950801396\n\t\tits                                     =>  0.9510446208123859\n\t\tfrom                                    =>  0.9405723357372776\n\t\tvs                                      =>  0.8653843699725567\n\t\tyear                                    =>   0.846502466267153\n:SV-15023{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6399319888551425\n\t\tmln                                     =>   1.217099157115808\n\t\tdlrs                                    =>  1.0933830369192543\n\t\t3                                       =>   1.033121271434882\n\t\tpct                                     =>   0.991094828319561\n\t\treuter                                  =>  0.9897275313905611\n\t\tits                                     =>  0.9504327303592046\n\t\tfrom                                    =>  0.9399480272494183\n\t\tvs                                      =>  0.8655203514280634\n\t\tyear                                    =>  0.8459804922897428\n:SV-15330{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6411480082558068\n\t\tmln                                     =>   1.217746071140758\n\t\tdlrs                                    =>  1.0940532425506244\n\t\t3                                       =>  1.0336447143638317\n\t\tpct                                     =>  0.9918269975797083\n\t\treuter                                  =>   0.990241145450359\n\t\tits                                     =>  0.9511417993006985\n\t\tfrom                                    =>  0.9406712099799636\n\t\tvs                                      =>  0.8653569180999117\n\t\tyear                                    =>  0.8465844425179013\n:SV-15403{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6493270418577013\n\t\tmln                                     =>   1.221708475489808\n\t\tdlrs                                    =>  1.0983489300320377\n\t\t3                                       =>  1.0370024996153944\n\t\tpct                                     =>  0.9967446058994232\n\t\treuter                                  =>   0.993528974793619\n\t\tits                                     =>  0.9558988111209523\n\t\tfrom                                    =>  0.9454911460774864\n\t\tvs                                      =>  0.8633642497287671\n\t\tyear                                    =>  0.8505083085439775\n:SV-15514{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6414524586689534\n\t\tmln                                     =>  1.2179029815366167\n\t\tdlrs                                    =>   1.094218299808865\n\t\t3                                       =>   1.033773769117182\n\t\tpct                                     =>  0.9920102286561391\n\t\treuter                                  =>  0.9903676795676004\n\t\tits                                     =>  0.9513191861395162\n\t\tfrom                                    =>  0.9408515920762511\n\t\tvs                                      =>   0.865304353452142\n\t\tyear                                    =>  0.8467337135094862\n:SV-15549{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>   1.640632892454694\n\t\tmln                                     =>  1.2174764812983898\n\t\tdlrs                                    =>  1.0937717467869699\n\t\t3                                       =>   1.033424727632325\n\t\tpct                                     =>    0.99151691360307\n\t\treuter                                  =>  0.9900253758026865\n\t\tits                                     =>  0.9508415534060888\n\t\tfrom                                    =>  0.9403654699584985\n\t\tvs                                      =>   0.865436402399392\n\t\tyear                                    =>  0.8463303217162843\n:SV-15616{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6402745961421197\n\t\tmln                                     =>   1.217287104215781\n\t\tdlrs                                    =>  1.0935749393200054\n\t\t3                                       =>  1.0332709291683844\n\t\tpct                                     =>  0.9913012005612369\n\t\treuter                                  =>  0.9898744911012118\n\t\tits                                     =>  0.9506326562835085\n\t\tfrom                                    =>  0.9401525895225771\n\t\tvs                                      =>  0.8654873596392523\n\t\tyear                                    =>  0.8461528918952358\n:SV-15674{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6402335213893247\n\t\tmln                                     =>  1.2172651791725515\n\t\tdlrs                                    =>  1.0935522610806727\n\t\t3                                       =>  1.0332532137000938\n\t\tpct                                     =>   0.991276468108388\n\t\treuter                                  =>  0.9898571070574692\n\t\tits                                     =>  0.9506087026962596\n\t\tfrom                                    =>  0.9401281555632803\n\t\tvs                                      =>  0.8654927058873914\n\t\tyear                                    =>  0.8461324681573653\n:SV-15720{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>   1.641454220566282\n\t\tmln                                     =>  1.2179063418879368\n\t\tdlrs                                    =>  1.0942205822099829\n\t\t3                                       =>  1.0337754035575257\n\t\tpct                                     =>  0.9920113271819195\n\t\treuter                                  =>  0.9903693325123661\n\t\tits                                     =>  0.9513202705619623\n\t\tfrom                                    =>  0.9408530174807668\n\t\tvs                                      =>  0.8653096216062077\n\t\tyear                                    =>  0.8467355860669477\n:SV-15732{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6418679366988789\n\t\tmln                                     =>   1.218118262616823\n\t\tdlrs                                    =>  1.0944441677361394\n\t\t3                                       =>  1.0339502052648608\n\t\tpct                                     =>  0.9922602967957669\n\t\treuter                                  =>  0.9905406967751569\n\t\tits                                     =>  0.9515612774046113\n\t\tfrom                                    =>   0.941098001639954\n\t\tvs                                      =>   0.865235154416334\n\t\tyear                                    =>  0.8469379811534101\n:SV-15825{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6403540331112847\n\t\tmln                                     =>  1.2173302824011656\n\t\tdlrs                                    =>  1.0936192179118565\n\t\t3                                       =>  1.0333054698476525\n\t\tpct                                     =>  0.9913490440255205\n\t\treuter                                  =>  0.9899084014354236\n\t\tits                                     =>  0.9506790000021428\n\t\tfrom                                    =>  0.9401999656754023\n\t\tvs                                      =>  0.8654787849286104\n\t\tyear                                    =>  0.8461927112339609\n:SV-15888{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>   1.641852069569193\n\t\tmln                                     =>   1.218106579705691\n\t\tdlrs                                    =>  1.0944336674208315\n\t\t3                                       =>  1.0339422184421034\n\t\tpct                                     =>  0.9922506923700831\n\t\treuter                                  =>  0.9905327937543529\n\t\tits                                     =>   0.951551949990525\n\t\tfrom                                    =>  0.9410880514065464\n\t\tvs                                      =>  0.8652299423273659\n\t\tyear                                    =>  0.8469287549740471\n:SV-15944{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6406094746503062\n\t\tmln                                     =>  1.2174640910103491\n\t\tdlrs                                    =>  1.0937588768380255\n\t\t3                                       =>  1.0334146735611798\n\t\tpct                                     =>  0.9915028147402405\n\t\treuter                                  =>  0.9900155118531778\n\t\tits                                     =>  0.9508279001565995\n\t\tfrom                                    =>  0.9403515526055797\n\t\tvs                                      =>   0.865439705916966\n\t\tyear                                    =>   0.846318717539638\n:SV-15952{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>   1.641608350634413\n\t\tmln                                     =>  1.2179827157677379\n\t\tdlrs                                    =>   1.094302484756082\n\t\t3                                       =>   1.033839606583586\n\t\tpct                                     =>  0.9921040410110572\n\t\treuter                                  =>   0.990432219413613\n\t\tits                                     =>  0.9514099986904929\n\t\tfrom                                    =>  0.9409438763575203\n\t\tvs                                      =>  0.8652760331837802\n\t\tyear                                    =>  0.8468099163160301\n:SV-15954{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6429205353451672\n\t\tmln                                     =>  1.2186434984636658\n\t\tdlrs                                    =>  1.0950054459143779\n\t\t3                                       =>  1.0343894404834142\n\t\tpct                                     =>   0.992893505149969\n\t\treuter                                  =>  0.9909710261706427\n\t\tits                                     =>  0.9521740690117075\n\t\tfrom                                    =>  0.9417194634871013\n\t\tvs                                      =>  0.8650137662755684\n\t\tyear                                    =>  0.8474476266423354\n:SV-16007{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>  1.6401767760282457\n\t\tmln                                     =>  1.2172339691485916\n\t\tdlrs                                    =>   1.093520432998812\n\t\t3                                       =>  1.0332284013507513\n\t\tpct                                     =>  0.9912422858233993\n\t\treuter                                  =>  0.9898327402827573\n\t\tits                                     =>  0.9505755879363272\n\t\tfrom                                    =>  0.9400942591120444\n\t\tvs                                      =>  0.8654979916098049\n\t\tyear                                    =>  0.8461038772989482\n:SV-16037{n=36 c=[0:0.019, 0.003:0.001, 0.006913:0.001, 0.01:0.004, 0.02:0.002, 0.03:0.001, 0.046:0.0\n\tTop Terms: \n\t\tsaid                                    =>   1.640610618380475\n\t\tmln                                     =>  1.2174645746382695\n\t\tdlrs                                    =>  1.0937594396319776\n\t\t3                                       =>  1.0334151203058977\n\t\tpct                                     =>  0.9915035014016228\n\t\treuter                                  =>  0.9900159476830741\n\t\tits                                     =>  0.9508285640147016\n\t\tfrom                                    =>  0.9403522136131415\n\t\tvs                                      =>  0.8654392679742507\n\t\tyear                                    =>   0.846319234572972",
        "Issue Links": []
    },
    "MAHOUT-767": {
        "Key": "MAHOUT-767",
        "Summary": "Improve RowSimilarityJob performance",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Grant Ingersoll",
        "Created": "19/Jul/11 20:56",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "09/Sep/11 07:56",
        "Description": "(See http://www.lucidimagination.com/search/document/40c4f124795c6b5/rowsimilarity_s#42ab816c27c6a9e7 for background)\nCurrently, the RowSimilarityJob defers the calculation of the similarity metric until the reduce phase, while emitting many Cooccurrence objects.  For similarity metrics that are algebraic (http://pig.apache.org/docs/r0.8.1/udf.html#Aggregate+Functions) we should be able to do much of the computation during the Mapper part of this phase and also take advantage of a Combiner.  \nWe should use a marker interface to know whether a similarity metric is algebraic and then make use of an appropriate Mapper implementation, otherwise we can fall back on our existing implementation.",
        "Issue Links": []
    },
    "MAHOUT-768": {
        "Key": "MAHOUT-768",
        "Summary": "Duplicated DoubleFunction in mahout and mahout-collections (mahout.math package).",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5,                                            collections-1.0",
        "Fix Version/s": "0.7",
        "Component/s": "collections,                                            Math",
        "Assignee": "Ted Dunning",
        "Reporter": "Dawid Weiss",
        "Created": "24/Jul/11 07:38",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "04/Jun/12 13:47",
        "Description": "DoubleFunction is duplicated in Mahout Math and Collections. There are also other things inside m.math.* package that are kept there to avoid circular dependencies... Simply removing DoubleFunction from collections is not going to work because it is needed for compilation (again, circular dependency between collections and math). I see two solutions:\n1) extract the common definitions inside math.function.* into a separate module. This is a clean solution, but obviously scatters the code even further.\n2) create a compilation-time, optional dependency on mahout 0.5 in collections, remove the entire mahout.math.* subpackage from collections and live with this. I don't know how Maven handles circular dependencies of this type:\ncollections [trunk] -> [optional, required at build time] mahout.math 0.5\nmahout.math [trunk] -> collections [trunk]\nit seems tricky and error-prone, but should also work.\nLooking for other ideas of cleaning this mess up, of course.\nDawid",
        "Issue Links": []
    },
    "MAHOUT-769": {
        "Key": "MAHOUT-769",
        "Summary": "Implementation of ALS -WR.... error caused by log file not found",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "pragati meena",
        "Created": "24/Jul/11 18:13",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "17/Aug/11 09:34",
        "Description": "Implementation of ALS -WR..... logs file not found ...but the file is there in the same directory \nException in thread \"main\" java.lang.IllegalStateException: java.io.FileNotFoundException: File does not exist: /user/hadoop/temp/errors/_logs\nat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator$1.apply(SequenceFileDirIterator.java:73)\nat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirIterator$1.apply(SequenceFileDirIterator.java:67)\nat com.google.common.collect.Iterators$8.next(Iterators.java:730)\nat com.google.common.collect.Iterators$5.hasNext(Iterators.java:508)\nat com.google.common.collect.ForwardingIterator.hasNext(ForwardingIterator.java:40)\nat org.apache.mahout.utils.eval.ParallelFactorizationEvaluator.computeRmse(ParallelFactorizationEvaluator.java:111)",
        "Issue Links": []
    },
    "MAHOUT-770": {
        "Key": "MAHOUT-770",
        "Summary": "Add CassandraDataModel",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "24/Jul/11 20:47",
        "Updated": "11/Sep/12 13:42",
        "Resolved": "25/Jul/11 13:41",
        "Description": "In the spirit of MongoDBDataModel I'd like to add a CassandraDataModel as a backing store for non-distributed recommenders. \nIt takes a somewhat different approach, heavily reliant on in-memory caching, as Cassandra is a different beast.\nPatch is attached. This goes in integration, not core. It uses Hector, not the raw Thrift API.",
        "Issue Links": []
    },
    "MAHOUT-771": {
        "Key": "MAHOUT-771",
        "Summary": "Random Projection using sampled values",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "25/Jul/11 02:13",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "17/Aug/11 09:51",
        "Description": "Random Projection implementation which follows two deterministic guarantees:\n\nThe same data projected multiple times produces the same output\nDense and sparse data with the same contents produce the same output\n\nCustom class that does Random Projection based on Johnson-Lindenstrauss. This implementation uses Achlioptas's results, which allow using method other than a full-range random multiplier per sample:\n\nuse 1 random bit to add or subtract a sample to a row sum\nuse a random value from 1/6 to add (1/6), subtract (1/6), or ignore (4 out of 6) a sample to a row sum\n\nCustom implementations for both dense and sparse vectors are included. The sparse vector implementation assumes the active values will fit in memory.\nAn implementation using full-range random multipliers made by java.util.Random is included for reference/research. \nDatabase-friendly random projections: Johnson-Lindenstrauss with binary coins\nDimitris Achlioptas\nhttp://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.84.4546&rep=rep1&type=pdf",
        "Issue Links": []
    },
    "MAHOUT-772": {
        "Key": "MAHOUT-772",
        "Summary": "Refactor Matrix/Vector implementation with linear operators",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Jonathan Traupman",
        "Created": "25/Jul/11 06:13",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "02/Mar/14 20:00",
        "Description": "As part of the implementation of MAHOUT-672, we refactored the various linear algebra classes to include a new superclass for matrices, the LinearOperator, which is more or less a matrix that implements various arithmetic operations but does not allow per-element or per-row/col access. Linear operators allowed us to implement things like diagonal offsets (e.g. ridge regression) and the \"timesSquared\" operation efficiently and generally, removing some special-case code in various algorithms. \nSince in its current state, this refactor isn't ready for inclusion, and because it's fairly separate for other stuff in MAHOUT-672, I'm pulling it out into its own issue.",
        "Issue Links": [
            "/jira/browse/MAHOUT-672"
        ]
    },
    "MAHOUT-773": {
        "Key": "MAHOUT-773",
        "Summary": "Implement Random Walk with Restarts",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "25/Jul/11 11:56",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "19/Sep/11 16:58",
        "Description": "I'll create an implementation of Random Walk with Restarts as described in Kang, Tsourakakis, Faloutsos, \"PEGASUS: A Peta-Scale Graph Mining System - Implementation and Observations\" http://www.cs.cmu.edu/~christos/PUBLICATIONS/icdm09-pegasus.pdf\nThe algorithm is a random walk similar to PageRank with the difference that you start at and teleport to a certain node. The probabilities it computes can be seen as a measure of proximity between the start node and a reached node. To my knowledge RWR can be e.g used for link predicition in social networks.\nI will try to create an implementation that is able to do several walks in parallel and I will assume that a steadystate probability vector fits in memory.\nI don't plan to use the implementation details from the paper but I'll model the algorithm as an iterative multiplication between the adjacency matrix of the graph and the matrix created from the steadystate probability vectors for the vertices we compute the random walks for.",
        "Issue Links": []
    },
    "MAHOUT-774": {
        "Key": "MAHOUT-774",
        "Summary": "Seq2Sparse overwrites output even without --overwrite option",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "25/Jul/11 13:05",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "25/Jul/11 13:41",
        "Description": "I noticed that SparseVectorsFromSequenceFiles overwrites the output directory on line 241 via HadoopUtil. It should only overwrite the output directory if the --overwrite option is set, as it currently does on lines 176-178",
        "Issue Links": []
    },
    "MAHOUT-775": {
        "Key": "MAHOUT-775",
        "Summary": "L2 does not work with TrainAdaptiveLogisticRegression",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "26/Jul/11 01:29",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 19:48",
        "Description": "I have post the problem to the dev list, see the following message\nhttp://mail-archives.apache.org/mod_mbox/mahout-dev/201106.mbox/%3cBANLkTik6153pjgCFNAyuPrWbV9jzCXPPJA@mail.gmail.com%3e",
        "Issue Links": []
    },
    "MAHOUT-776": {
        "Key": "MAHOUT-776",
        "Summary": "What about a universal input data handling mechanism for Mahout?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "26/Jul/11 01:32",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "17/Aug/11 09:49",
        "Description": "please see the following message\nhttp://mail-archives.apache.org/mod_mbox/mahout-dev/201107.mbox/%3cCACOCgc=8W=OBAU8p9hNaDJL9CDWnBrOzzwP01Xoax+Ce2zJ+Tg@mail.gmail.com%3e",
        "Issue Links": []
    },
    "MAHOUT-777": {
        "Key": "MAHOUT-777",
        "Summary": "Improve TransposeJob to use a Combiner",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "26/Jul/11 19:50",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "23/Aug/11 09:13",
        "Description": "I saw that TransposeJob has a comment that it needs a rewrite to use a combiner. If we emit vectors instead of MatrixEntryWritables we should be able to \"merge\" them in a combiner.",
        "Issue Links": []
    },
    "MAHOUT-778": {
        "Key": "MAHOUT-778",
        "Summary": "Mark folder name of final clustering iteration with pattern such as 'cluster-n-last'",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "28/Jul/11 12:45",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "30/Sep/11 19:21",
        "Description": "It would be useful if the KMeans, FuzzyKMeans would specify the last cluster iteration folder with a pattern such as 'cluster-n-last'.\nAt the moment it is difficult to configure other programs to process clustering results since the number of actual iterations is not known up front.\nA PathFilder similar to ClustersFilter could be created which filters folders on the pattern 'cluster-*-last' in order to determine the folder.",
        "Issue Links": []
    },
    "MAHOUT-779": {
        "Key": "MAHOUT-779",
        "Summary": "Hadoop 0.21.0 support",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "30/Jul/11 12:58",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "31/Jul/11 19:06",
        "Description": "Hi, \nSince 0.21.0 are going to be a mature version, are we going to support it.",
        "Issue Links": []
    },
    "MAHOUT-780": {
        "Key": "MAHOUT-780",
        "Summary": "job jars fail on OS X due to case-insensitive name conflict on 'license'",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "build",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "01/Aug/11 11:05",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "03/Aug/11 13:14",
        "Description": "Dan explains it well below. The workaround is to make the 'license' folder into a 'licenses' folder, but, where does this come from? anyone know?\nWith SVN 'At revision 1152597.', and freshly rebuilt:\njar -tvf /Users/danbri/Documents/workspace/trunk/examples/target/mahout-examples-0.6-SNAPSHOT-job.jar\n\n\n\n grep -i license\n\n\n\n 19355 Sat Feb 26 19:16:30 CET 2011 META-INF/LICENSE.txt\n 11358 Sun Apr 11 21:45:12 CEST 2010 META-INF/LICENSE\n 1596 Mon Dec 20 15:47:30 CET 2010 LICENSE\n    0 Sun Dec 01 11:57:24 CET 2002 license/\n 4083 Sun Dec 01 11:57:24 CET 2002 license/LICENSE.dom-documentation.txt\n 3595 Sun Dec 01 11:57:24 CET 2002 license/LICENSE.dom-software.txt\n  804 Sun Dec 01 11:57:24 CET 2002 license/LICENSE.sax.txt\n 2827 Sun Dec 01 11:57:24 CET 2002 license/LICENSE.txt\n 1274 Sun Dec 01 11:57:24 CET 2002 license/README.dom.txt\n  715 Sun Dec 01 11:57:24 CET 2002 license/README.sax.txt\n  672 Sun Dec 01 11:57:24 CET 2002 license/README.txt\nThis situation seems to quite confuse Hadoop. The underlying OSX\nfilesystem doesn't support file and directory names differing only by\ncase; see http://developer.apple.com/library/mac/#documentation/Java/Conceptual/Java14Development/01-JavaOverview/JavaOverview.html\nmahout  lucene.vector --dir solr/data/index/ --output bar/vecs --field\nlabel --idField id --dictOut bar/dict.out --norm 2\nRunning on hadoop, using HADOOP_HOME=/Users/danbri/working/hadoop/hadoop-0.20.2\nHADOOP_CONF_DIR=/Users/danbri/working/hadoop/hadoop-0.20.2/conf\nMAHOUT-JOB: /Users/danbri/Documents/workspace/trunk/examples/target/mahout-examples-0.6-SNAPSHOT-job.jar\nException in thread \"main\" java.io.IOException: Mkdirs failed to\ncreate /tmp/hadoop/hadoop-unjar5018665014541152120/license\n       at org.apache.hadoop.util.RunJar.unJar(RunJar.java:48)\n       at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\nThat Hadoop error message is somewhat unhelpful, especially for those\nwho doubt their hadoop knowhow; but technically correct. The\n/tmp/hadoop and its subdirectory exist and are writeable. The problem\nis the specific file/dir names being written into it. That wasn't so\nobvious. So I went chasing around configuring hadoop tmp dirs,\nchecking it existed and was writable in local and in hdfs dirs, ...\nthen ... I finally, belatedly tried unzipping the jar with 'jar -xvf '\nto see what was special about 'license', and got the same error from\ncommandline 'jar' that upset !file.getParentFile().isDirectory() in\nHadoop's ./src/core/org/apache/hadoop/util/RunJar.java:\njava.io.IOException: license : could not create directory\n       at sun.tools.jar.Main.extractFile(Main.java:909)\n       at sun.tools.jar.Main.extract(Main.java:852)\n       at sun.tools.jar.Main.run(Main.java:242)\n       at sun.tools.jar.Main.main(Main.java:1149)\n(this is the same error that trips up hadoop)\nThis seems to be reproducible; I did an svn up, mvn clean and mvn\npackage, let all the tests run and pass, and confirm that the same\nthing happens.\nI compared an early job .jar from 0.5, where all was fine. Any\nsuggestions for best quick fix?",
        "Issue Links": []
    },
    "MAHOUT-781": {
        "Key": "MAHOUT-781",
        "Summary": "universal map-reduce job to convert csv file to vectorwritable sequencefile",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "07/Aug/11 09:50",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Oct/12 08:59",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-782": {
        "Key": "MAHOUT-782",
        "Summary": "Build error with Java JDK 1.7",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.7",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Ran Hamada",
        "Created": "10/Aug/11 00:42",
        "Updated": "02/May/12 08:56",
        "Resolved": "10/Aug/11 08:22",
        "Description": "I get an error building mahout with JDK1.7.0 at\nintegration/src/main/java/org/apache/mahout/cf/taste/impl/model/jdbc/ConnectionPoolDataSource.java",
        "Issue Links": []
    },
    "MAHOUT-783": {
        "Key": "MAHOUT-783",
        "Summary": "AUC ConfusionMatrix do not work probably",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "XiaoboGu",
        "Created": "10/Aug/11 14:40",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "15/Oct/11 10:24",
        "Description": "I have reported the problem to the dev list, just creating this jira to remind the authors of AUC and ConfusionMatrix to help identify the root problem, you can see abnormal output of ConfusionMatrix and AUC at here \nhttp://mail-archives.apache.org/mod_mbox/mahout-dev/201108.mbox/%3cCACOCgcnRbH=xHW6TqkZV_iFdbDPOcqEZ_aHMP8npE7_utE_1=Q@mail.gmail.com%3e",
        "Issue Links": []
    },
    "MAHOUT-784": {
        "Key": "MAHOUT-784",
        "Summary": "Exception at 20 Newsgroups examples",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "patrick.J",
        "Created": "13/Aug/11 02:39",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/Feb/12 13:57",
        "Description": "somebody may help,\n   I run\"Twenty Newsgroups Classification Example\"(from https://cwiki.apache.org/confluence/display/MAHOUT/Twenty+Newsgroups) step by step.And the Environment variables(HADOOP_HOME,MAHOUT_HOME,HADOOP_CONF_DIR) all set in ~/.bashrc.\n   But when I going the step train the classifier on Hadoop, Exception was cast like:\n$> $MAHOUT_HOME/bin/mahout trainclassifier   -i 20news-input/bayes-train-input   -o newsmodel   -type bayes   -ng 3   -source hdfs:\n11/08/13 09:37:54 INFO mapred.JobClient:  map 0% reduce 0%\n11/08/13 09:38:04 INFO mapred.JobClient: Task Id : attempt_201108130934_0001_m_000000_0, Status : FAILED\njava.lang.RuntimeException: Error in configuring object\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:354)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\nCaused by: java.lang.reflect.InvocationTargetException\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n\t... 5 more\nCaused by: java.lang.NoClassDefFoundError: org/apache/lucene/analysis/TokenStream\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:247)\n\tat org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:762)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:807)\n\tat org.apache.hadoop.conf.Configuration.getClass(Configuration.java:833)\n\tat org.apache.hadoop.mapred.JobConf.getMapperClass(JobConf.java:772)\n\tat org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)\n\t... 10 more\nCaused by: java.lang.ClassNotFoundException: org.apache.lucene.analysis.TokenStream\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:306)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:247)\n\t... 17 more\n   What happened? I just walked through the tutorials.......",
        "Issue Links": []
    },
    "MAHOUT-785": {
        "Key": "MAHOUT-785",
        "Summary": "Universal input file format for classifier algorithms in Mahout",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "14/Aug/11 04:40",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "15/Oct/11 10:18",
        "Description": "I think a universal input file format is much more convinient for users, especially command line users, and we should even consider use some universal command line options for the classification algorithms, such as options for target/predictor variables and their types. Then users can prepare their data once, and build different models to get the best one. Currentlly we should consider the following:\n1. SGD LogisticRegression\n2. NaiveBayes\n3. Bayes\n4. Random Forest",
        "Issue Links": []
    },
    "MAHOUT-786": {
        "Key": "MAHOUT-786",
        "Summary": "SSVD+CDH3u0: Upper Triangular matrix overrun reported by front end",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "14/Aug/11 18:44",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "17/Aug/11 09:30",
        "Description": "SSVD error When running with cdh3u0 and reduceTasks >1  :\n\nException in thread \"main\" java.io.IOException: Unexpected overrun in upper triangular matrix files\n       at org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.loadUpperTriangularMatrix(SSVDSolver.java:471)\n       at org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.run(SSVDSolver.java:268)\n       at com.mozilla.SSVDCli.run(SSVDCli.java:89)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n       at com.mozilla.SSVDCli.main(SSVDCli.java:129)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.apache.hadoop.util.RunJar.main(RunJar.java:186)",
        "Issue Links": []
    },
    "MAHOUT-787": {
        "Key": "MAHOUT-787",
        "Summary": "When running  ./examples/bin/build-reuters.sh after mahout installation, getting  java.io.IOException: Broken pipe",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Arsen Zahray",
        "Created": "16/Aug/11 20:31",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "17/Aug/11 09:24",
        "Description": "I've installed mahout at bitnami AMI ami-02fb006b, (as well as several other ami's, otherwise I won't be asking the question)\naccording to instructions provided here: http://bickson.blogspot.com/2011/01/how-to-install-mahout-on-amazon-ec2.html\nand here: https://cwiki.apache.org/MAHOUT/mahout-on-amazon-ec2.html\nI'm always getting stuck when trying to run ./examples/bin/build-reuters.sh\nHere's the output of the command:\nPlease select a number to choose the corresponding clustering algorithm\n1. kmeans clustering\n2. lda clustering\nEnter your choice : 1\nok. You chose 1 and we'll use kmeans Clustering\nDownloading Reuters-21578\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100 7959k  100 7959k    0     0   294k      0  0:00:26  0:00:26 -::-  305k\nExtracting...\nRunning on hadoop, using HADOOP_HOME=/usr/local/hadoop-0.20.2\nHADOOP_CONF_DIR=/usr/local/hadoop-0.20.2/conf\nMAHOUT-JOB: /usr/local/mahout-0.4/examples/target/mahout-examples-0.6-SNAPSHOT-job.jar\n11/08/16 20:10:25 WARN driver.MahoutDriver: No org.apache.lucene.benchmark.utils.ExtractReuters.props found on classpath, will use command-line arguments only\nDeleting all files in mahout-work/reuters-out-tmp\n11/08/16 20:10:30 INFO driver.MahoutDriver: Program took 4906 ms\nMAHOUT_LOCAL is set, running locally\nCLASSPATH: :/usr/local/mahout-0.4/src/conf:/usr/local/hadoop-0.20.2/conf:/usr/lib/jvm/java-6-openjdk//lib/tools.jar:/usr/local/mahout-0.4/mahout-.jar:/usr/local/mahout-0.4/examples/target/mahout-examples-0.6-SNAPSHOT-job.jar:/usr/local/mahout-0.4/mahout-examples--job.jar:/usr/local/mahout-0.4/lib/*.jar:/usr/local/mahout-0.4/examples/target/dependency/antlr-2.7.7.jar:/usr/local/mahout-0.4/examples/target/dependency/antlr-3.2.jar:/usr/local/mahout-0.4/examples/target/dependency/antlr-runtime-3.2.jar:/usr/local/mahout-0.4/examples/target/dependency/avro-1.4.0-cassandra-1.jar:/usr/local/mahout-0.4/examples/target/dependency/bson-2.5.jar:/usr/local/mahout-0.4/examples/target/dependency/cassandra-all-0.8.1.jar:/usr/local/mahout-0.4/examples/target/dependency/cassandra-thrift-0.8.1.jar:/usr/local/mahout-0.4/examples/target/dependency/cglib-nodep-2.2.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-beanutils-1.7.0.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-beanutils-core-1.8.0.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-cli-1.2.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-cli-2.0-mahout.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-codec-1.4.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-collections-3.2.1.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-compress-1.1.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-configuration-1.6.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-dbcp-1.4.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-digester-1.7.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-httpclient-3.0.1.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-lang-2.6.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-logging-1.1.1.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-math-2.2.jar:/usr/local/mahout-0.4/examples/target/dependency/commons-pool-1.5.6.jar:/usr/local/mahout-0.4/examples/target/dependency/concurrentlinkedhashmap-lru-1.1.jar:/usr/local/mahout-0.4/examples/target/dependency/easymock-3.0.jar:/usr/local/mahout-0.4/examples/target/dependency/google-collections-1.0-rc2.jar:/usr/local/mahout-0.4/examples/target/dependency/guava-r09.jar:/usr/local/mahout-0.4/examples/target/dependency/hadoop-core-0.20.203.0.jar:/usr/local/mahout-0.4/examples/target/dependency/hector-core-0.8.0-2.jar:/usr/local/mahout-0.4/examples/target/dependency/high-scale-lib-1.1.2.jar:/usr/local/mahout-0.4/examples/target/dependency/httpclient-4.0.1.jar:/usr/local/mahout-0.4/examples/target/dependency/httpcore-4.0.1.jar:/usr/local/mahout-0.4/examples/target/dependency/jackson-core-asl-1.8.2.jar:/usr/local/mahout-0.4/examples/target/dependency/jackson-mapper-asl-1.8.2.jar:/usr/local/mahout-0.4/examples/target/dependency/jakarta-regexp-1.4.jar:/usr/local/mahout-0.4/examples/target/dependency/jamm-0.2.2.jar:/usr/local/mahout-0.4/examples/target/dependency/jcommon-1.0.12.jar:/usr/local/mahout-0.4/examples/target/dependency/jetty-6.1.22.jar:/usr/local/mahout-0.4/examples/target/dependency/jetty-util-6.1.22.jar:/usr/local/mahout-0.4/examples/target/dependency/jfreechart-1.0.13.jar:/usr/local/mahout-0.4/examples/target/dependency/jline-0.9.94.jar:/usr/local/mahout-0.4/examples/target/dependency/json-simple-1.1.jar:/usr/local/mahout-0.4/examples/target/dependency/jul-to-slf4j-1.6.1.jar:/usr/local/mahout-0.4/examples/target/dependency/junit-4.8.2.jar:/usr/local/mahout-0.4/examples/target/dependency/libthrift-0.6.1.jar:/usr/local/mahout-0.4/examples/target/dependency/log4j-1.2.16.jar:/usr/local/mahout-0.4/examples/target/dependency/lucene-analyzers-3.1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/lucene-benchmark-3.1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/lucene-core-3.1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/lucene-highlighter-3.1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/lucene-memory-3.1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/lucene-queries-3.1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/lucene-xercesImpl-3.1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/mahout-collections-1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/mahout-core-0.6-SNAPSHOT.jar:/usr/local/mahout-0.4/examples/target/dependency/mahout-core-0.6-SNAPSHOT-tests.jar:/usr/local/mahout-0.4/examples/target/dependency/mahout-integration-0.6-SNAPSHOT.jar:/usr/local/mahout-0.4/examples/target/dependency/mahout-math-0.6-SNAPSHOT.jar:/usr/local/mahout-0.4/examples/target/dependency/mahout-math-0.6-SNAPSHOT-tests.jar:/usr/local/mahout-0.4/examples/target/dependency/mongo-java-driver-2.5.jar:/usr/local/mahout-0.4/examples/target/dependency/objenesis-1.2.jar:/usr/local/mahout-0.4/examples/target/dependency/servlet-api-2.5-20081211.jar:/usr/local/mahout-0.4/examples/target/dependency/servlet-api-2.5.jar:/usr/local/mahout-0.4/examples/target/dependency/slf4j-api-1.6.1.jar:/usr/local/mahout-0.4/examples/target/dependency/slf4j-jcl-1.6.1.jar:/usr/local/mahout-0.4/examples/target/dependency/slf4j-log4j12-1.6.1.jar:/usr/local/mahout-0.4/examples/target/dependency/snakeyaml-1.6.jar:/usr/local/mahout-0.4/examples/target/dependency/solr-commons-csv-3.1.0.jar:/usr/local/mahout-0.4/examples/target/dependency/speed4j-0.9.jar:/usr/local/mahout-0.4/examples/target/dependency/stringtemplate-3.2.jar:/usr/local/mahout-0.4/examples/target/dependency/uncommons-maths-1.2.2.jar:/usr/local/mahout-0.4/examples/target/dependency/uuid-3.2.0.jar:/usr/local/mahout-0.4/examples/target/dependency/watchmaker-framework-0.6.2.jar:/usr/local/mahout-0.4/examples/target/dependency/watchmaker-swing-0.6.2.jar:/usr/local/mahout-0.4/examples/target/dependency/xml-apis-1.0.b2.jar:/usr/local/mahout-0.4/examples/target/dependency/xpp3_min-1.1.4c.jar:/usr/local/mahout-0.4/examples/target/dependency/xstream-1.3.1.jar\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/mahout-0.4/examples/target/mahout-examples-0.6-SNAPSHOT-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/mahout-0.4/examples/target/dependency/slf4j-jcl-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/mahout-0.4/examples/target/dependency/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nWARNING: org.apache.hadoop.metrics.jvm.EventCounter is deprecated. Please use org.apache.hadoop.log.metrics.EventCounter in all the log4j.properties files.\n11/08/16 20:10:32 INFO common.AbstractJob: Command line arguments: {--charset=UTF-8, --chunkSize=5, --endPhase=2147483647, --fileFilterClass=org.apache.mahout.text.PrefixAdditionFilter, --input=mahout-work/reuters-out, --keyPrefix=, --output=mahout-work/reuters-out-seqdir, --startPhase=0, --tempDir=temp}\nException in thread \"main\" java.io.IOException: Call to localhost/127.0.0.1:9000 failed on local exception: java.io.IOException: Broken pipe\n        at org.apache.hadoop.ipc.Client.wrapException(Client.java:1065)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1033)\n        at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:224)\n        at $Proxy1.getProtocolVersion(Unknown Source)\n        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:364)\n        at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:106)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:208)\n        at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:175)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:89)\n        at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1310)\n        at org.apache.hadoop.fs.FileSystem.access$100(FileSystem.java:65)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1328)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:226)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:109)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:210)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.run(SequenceFilesFromDirectory.java:59)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.run(SequenceFilesFromDirectory.java:110)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.main(SequenceFilesFromDirectory.java:85)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:616)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\nCaused by: java.io.IOException: Broken pipe\n        at sun.nio.ch.FileDispatcher.write0(Native Method)\n        at sun.nio.ch.SocketDispatcher.write(SocketDispatcher.java:47)\n        at sun.nio.ch.IOUtil.writeFromNativeBuffer(IOUtil.java:122)\n        at sun.nio.ch.IOUtil.write(IOUtil.java:93)\n        at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:352)\n        at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:55)\n        at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)\n        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:146)\n        at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:107)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n        at java.io.DataOutputStream.flush(DataOutputStream.java:123)\n        at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:746)\n        at org.apache.hadoop.ipc.Client.call(Client.java:1011)\n        ... 25 more\nrmr: cannot remove mahout-work/reuters-out-seqdir: No such file or directory.\nput: File mahout-work/reuters-out-seqdir does not exist.\nthis is a consistent error and I am getting it in every single installation I attempt.\nWhat do I do to fix this?",
        "Issue Links": []
    },
    "MAHOUT-788": {
        "Key": "MAHOUT-788",
        "Summary": "ClusterDumper is never flushing output stream",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering,                                            Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jeff Hansen",
        "Created": "16/Aug/11 22:03",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "17/Aug/11 09:32",
        "Description": "ClusterDumper utility never calls flush on the OutputStreamWriter.  As of issue https://issues.apache.org/jira/browse/MAHOUT-679, the output stream is never being closed when it defaults to System.out \u2013 while that's a good thing, it would be nice to flush the stream before exiting the program.  As is, when I run cluster dumper with the -b (substring) option set to something like 50, the stream never gets big enough to overflow the default buffer on my machine, so I see no output.  Even when it does get big enough to overflow the buffer, I still miss the last cluster's summary.  When the output is written to a file, the close() method usually flushes the buffer by default, but it shouldn't hurt to call the flush method either way \u2013 therefore I'd suggest adding in an unconditional call to writer.flush(); in the finally block just before conditionally closing the writer. (line 199 in the org.apache.mahout.utils.clustering.ClusterDumper.run(String[] args) method)\n    } finally {\n      writer.flush();\n      if (shouldClose) \n{\n        Closeables.closeQuietly(writer);\n      }\n    }",
        "Issue Links": []
    },
    "MAHOUT-789": {
        "Key": "MAHOUT-789",
        "Summary": "testclassifier seems does not work using kdd data set",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "XiaoboGu",
        "Created": "20/Aug/11 02:18",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jun/12 12:04",
        "Description": "I am now testing the trainclassifier and testclassifier commands in\nmabout, I prepaired a nbdf-train.csv and nbdf-test.csv file with the\nfollowing R commands:\ndf <- read.arff(file = \"d:/temp/kdd/KDDTrain+.arff\")\nnbdf <- data.frame(class=df[\"class\"],\nprotocol_type=df[\"protocol_type\"],service=df[\"service\"],flag=df[\"flag\"],land=df[\"land\"],logged_in=df[\"logged_in\"],is_host_login=df[\"is_host_login\"],is_guest_login=df[\"is_guest_login\"])\nnbdf[[\"logged_in\"]] <- as.factor(nbdf[[\"logged_in\"]])\nnbdf[[\"is_guest_login\"]] <- as.factor(nbdf[[\"is_guest_login\"]])\nnbdf[[\"is_host_login\"]] <- as.factor(nbdf[[\"is_host_login\"]])\nnbdf[[\"land\"]] <- as.factor(nbdf[[\"land\"]])\nwrite.table(nbdf, file=\"D:/nbdf-train.csv\", row.names=FALSE,\ncol.names=FALSE, quote=FALSE, sep=\"\\t\")\ndf <- read.arff(file = \"d:/temp/kdd/KDDTest+.arff\")\nnbdf <- data.frame(class=df[\"class\"],\nprotocol_type=df[\"protocol_type\"],service=df[\"service\"],flag=df[\"flag\"],land=df[\"land\"],logged_in=df[\"logged_in\"],is_host_login=df[\"is_host_login\"],is_guest_login=df[\"is_guest_login\"])\nnbdf[[\"logged_in\"]] <- as.factor(nbdf[[\"logged_in\"]])\nnbdf[[\"is_guest_login\"]] <- as.factor(nbdf[[\"is_guest_login\"]])\nnbdf[[\"is_host_login\"]] <- as.factor(nbdf[[\"is_host_login\"]])\nnbdf[[\"land\"]] <- as.factor(nbdf[[\"land\"]])\nwrite.table(nbdf, file=\"D:/nbdf-test.csv\", row.names=FALSE,\ncol.names=FALSE, quote=FALSE, sep=\"\\t\")\nand put them under nbtest/train and nbtest/test in HDFS\nthen issue\nmahout trainclassifier --input nbtest/train --output nbtest/output\nmahout testclassifier --testDir nbtest/test --model nbtest/output\ntrainclassifier seems succed, but testclassifier failed with this:\n[gpadmin@linuxsvr2 mahtest]$ mahout testclassifier --testDir\nnbtest/test --model nbtest/output\nRunning on hadoop, using HADOOP_HOME=/usr/local/hadoop\nHADOOP_CONF_DIR=/usr/local/hadoop/conf\nMAHOUT-JOB: /usr/local/mahout/mahout-examples-0.6-SNAPSHOT-job.jar\n11/08/15 18:06:20 WARN driver.MahoutDriver: No testclassifier.props\nfound on classpath, will use command-line arguments only\n11/08/15 18:06:20 INFO bayes.TestClassifier: Loading model from:\n{basePath=nbtest/output, classifierType=bayes, alpha_i=1.0,\ndataSource=hdfs, gramSize=1, verbose=false, encoding=UTF-8,\ndefaultCat=unknown, testDirPath=nbtest/test}\n11/08/15 18:06:20 INFO bayes.TestClassifier: Testing Bayes Classifier\n11/08/15 18:06:20 INFO bayes.SequenceFileModelReader: 77319.90481464032\n11/08/15 18:06:20 INFO bayes.InMemoryBayesDatastore: normal\n-213.05542661827678 442.8886516970405 -0.48105867197522617\n11/08/15 18:06:20 INFO bayes.InMemoryBayesDatastore: anomaly\n-442.8886516970405 442.8886516970405 -1.0\n11/08/15 18:06:20 INFO bayes.TestClassifier:\n=======================================================\nSummary\n-------------------------------------------------------\nCorrectly Classified Instances          :          0             \u951f\nIncorrectly Classified Instances        :          0                 \u951f\nTotal Classified Instances              :          0\n=======================================================\nConfusion Matrix\n-------------------------------------------------------\na       b       c       <--Classified as\n0       0       0        |  0           a     = normal\n0       0       0        |  0           b     = anomaly\n0       0       0        |  0           c     = unknown\nDefault Category: unknown: 2\n11/08/15 18:06:20 INFO driver.MahoutDriver: Program took 746 ms",
        "Issue Links": []
    },
    "MAHOUT-790": {
        "Key": "MAHOUT-790",
        "Summary": "Redundancy in Matrix API, view or get?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "20/Aug/11 21:28",
        "Updated": "02/May/13 02:29",
        "Resolved": "13/Sep/11 09:12",
        "Description": "We have a bunch of redundant methods in our matrix interface.  These include things that return views of parts of the matrix:\n\n  Matrix viewPart(int[] offset, int[] size);\n  Matrix viewPart(int rowOffset, int rowsRequested, int columnOffset, int columnsRequested);\n  Vector viewRow(int row);\n  Vector viewColumn(int column);\n\n\nand things that do the same but call refer to getting stuff\n\n  Vector getColumn(int column);\n  Vector getRow(int row);\n  double getQuick(int row, int column);\n  int[] getNumNondefaultElements();\n  Map<String, Integer> getColumnLabelBindings();\n  Map<String, Integer> getRowLabelBindings();\n  double get(String rowLabel, String columnLabel);\n\n\nTo my mind, get implies a get-by-value whereas view implies get-by-reference.  As such, I would suggest that getColumn and getRow should disappear.  On the other hand, getQuick and get are both correctly named.  \nThis raises the question of what getNumNondefaultElements really does.  I certainly can't tell just from the signature.  Is it too confusing to keep?\nAdditionally, what do people think that getColumnLabelBindings and getRowLabelBindings return?  A mutable map?  Or an immutable one?\nUnder the covers, viewRow and viewColumn (and the upcoming viewDiagonal) have default implementations that use MatrixVectorView, but AbstractMatrix doesn't have an implementation for getRow and getColumn. \nIn sum, I suggest that:\n\ngetRow and getColumn go away\n\n\nthe fancy fast implementations fo getRow and getColumn that exist be migrated to be over-rides of viewRow and viewColumn\n\n\nthere be a constructor for AbstractMatrix that sets the internal size things correctly.\n\n\nthat the internal cardinality array in AbstractMatrix goes away to be replaced by two integers.\n\n\nviewDiagonal() and viewDiagonal(length) and viewDiagonal(row, column) and viewDiagonal(int row, column, length) be added.\n\nI will produce a patch shortly.",
        "Issue Links": [
            "/jira/browse/MAHOUT-792",
            "/jira/browse/MAHOUT-793",
            "/jira/browse/MAHOUT-796",
            "/jira/browse/MAHOUT-797"
        ]
    },
    "MAHOUT-791": {
        "Key": "MAHOUT-791",
        "Summary": "Redundancy in Matrix API, view or get?",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "20/Aug/11 21:28",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "20/Aug/11 21:37",
        "Description": "We have a bunch of redundant methods in our matrix interface.  These include things that return views of parts of the matrix:\n\n  Matrix viewPart(int[] offset, int[] size);\n  Matrix viewPart(int rowOffset, int rowsRequested, int columnOffset, int columnsRequested);\n  Vector viewRow(int row);\n  Vector viewColumn(int column);\n\n\nand things that do the same but call refer to getting stuff\n\n  Vector getColumn(int column);\n  Vector getRow(int row);\n  double getQuick(int row, int column);\n  int[] getNumNondefaultElements();\n  Map<String, Integer> getColumnLabelBindings();\n  Map<String, Integer> getRowLabelBindings();\n  double get(String rowLabel, String columnLabel);\n\n\nTo my mind, get implies a get-by-value whereas view implies get-by-reference.  As such, I would suggest that getColumn and getRow should disappear.  On the other hand, getQuick and get are both correctly named.  \nThis raises the question of what getNumNondefaultElements really does.  I certainly can't tell just from the signature.  Is it too confusing to keep?\nAdditionally, what do people think that getColumnLabelBindings and getRowLabelBindings return?  A mutable map?  Or an immutable one?\nUnder the covers, viewRow and viewColumn (and the upcoming viewDiagonal) have default implementations that use MatrixVectorView, but AbstractMatrix doesn't have an implementation for getRow and getColumn. \nIn sum, I suggest that:\n\ngetRow and getColumn go away\n\n\nthe fancy fast implementations fo getRow and getColumn that exist be migrated to be over-rides of viewRow and viewColumn\n\n\nthere be a constructor for AbstractMatrix that sets the internal size things correctly.\n\n\nthat the internal cardinality array in AbstractMatrix goes away to be replaced by two integers.\n\n\nviewDiagonal() and viewDiagonal(length) and viewDiagonal(row, column) and viewDiagonal(int row, column, length) be added.\n\nI will produce a patch shortly.",
        "Issue Links": []
    },
    "MAHOUT-792": {
        "Key": "MAHOUT-792",
        "Summary": "Add new stochastic decomposition code",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "20/Aug/11 21:33",
        "Updated": "02/May/13 02:29",
        "Resolved": "16/Jan/12 06:01",
        "Description": "I have figured out some simplification for our SSVD algorithms.  This eliminates the QR decomposition and makes life easier.\nI will produce a patch that contains the following:\n\na CholeskyDecomposition implementation that does pivoting (and thus rank-revealing) or not.  This should actually be useful for solution of large out-of-core least squares problems.\n\n\nan in-memory SSVD implementation that should work for matrices up to about 1/3 of available memory.\n\n\nan out-of-core SSVD threaded implementation that should work for very large matrices.  It should take time about equal to the cost of reading the input matrix 4 times and will require working disk roughly equal to the size of the input.",
        "Issue Links": [
            "/jira/browse/MAHOUT-790",
            "/jira/browse/MAHOUT-797"
        ]
    },
    "MAHOUT-793": {
        "Key": "MAHOUT-793",
        "Summary": "Move MurmurHash to math",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "21/Aug/11 04:02",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "22/Aug/11 03:28",
        "Description": "I am finding a need to use MurmurHash from within math.  So far, it has lived where it started in the encoding stuff.  I would like to move it down.",
        "Issue Links": [
            "/jira/browse/MAHOUT-790"
        ]
    },
    "MAHOUT-794": {
        "Key": "MAHOUT-794",
        "Summary": "Eigencuts produces unexpected results, part 2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Shannon Quinn",
        "Reporter": "Sean R. Owen",
        "Created": "21/Aug/11 19:37",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 17:01",
        "Description": "See MAHOUT-516, which was closed. Looks like Shannon believes there is a follow-on issue. I'm just opening a new issue to track this for 0.6.\nThis is an issue in the workflow of the Eigencuts algorithm; some part of it is not implemented correctly. More details to follow.",
        "Issue Links": []
    },
    "MAHOUT-795": {
        "Key": "MAHOUT-795",
        "Summary": "Change prep_asf_mail_archives to not download archives",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "22/Aug/11 12:16",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "07/Sep/11 20:21",
        "Description": "I've turned off access to my old storage of the ASF archives in favor of using the publicly hosted data set (http://aws.amazon.com/datasets/7791434387204566).  So, instead of downloading the files, one should just point the script at the location of the untarred files.",
        "Issue Links": []
    },
    "MAHOUT-796": {
        "Key": "MAHOUT-796",
        "Summary": "Modified power iterations in existing SSVD code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "25/Aug/11 23:38",
        "Updated": "02/May/13 02:29",
        "Resolved": "07/Sep/11 21:42",
        "Description": "Nathan Halko contacted me and pointed out importance of availability of power iterations and their significant effect on accuracy of smaller eigenvalues and noise attenuation. \nEssentially, we would like to introduce yet another job parameter, q, that governs amount of optional power iterations. The suggestion how to modify the algorithm is outlined here : https://github.com/dlyubimov/ssvd-lsi/wiki/Power-iterations-scratchpad .\nNote that it is different from original power iterations formula in the paper in the sense that additional orthogonalization performed after each iteration. Nathan points out that that improves errors in smaller eigenvalues a lot (If i interpret it right).",
        "Issue Links": [
            "/jira/browse/MAHOUT-790"
        ]
    },
    "MAHOUT-797": {
        "Key": "MAHOUT-797",
        "Summary": "MapReduce SSVD: provide alternative B-pipeline per B=R' ^{-1} Y'A",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5,                                            0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "29/Aug/11 15:31",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "10/Jul/13 17:45",
        "Description": "Since alternative flow using Cholesky decomposition is extremely easy to add to existing computations of BB', I am thinking of just adding an option that chooses between B-pipeline with QR step and B-pipeline with Y'Y-Cholesky step. \nOngoing work and some initial code (Y'Y step) is here https://github.com/dlyubimov/mahout-commits/tree/MAHOUT-797.\nI also want to fix what's left unfixed in MAHOUT-638.",
        "Issue Links": [
            "/jira/browse/MAHOUT-792",
            "/jira/browse/MAHOUT-790"
        ]
    },
    "MAHOUT-798": {
        "Key": "MAHOUT-798",
        "Summary": "Add Examples for the ASF Mail Archive",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "29/Aug/11 20:57",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "07/Oct/11 14:13",
        "Description": "Per http://www.lucidimagination.com/search/document/c6ea889edb9ad0fe/email_and_collab_filtering, I am working on a variety of examples based on the ASF email archive.  WIP will be at https://github.com/lucidimagination/mahout.\nI intend to have at least three examples, one for classification, clustering and collab filtering.",
        "Issue Links": []
    },
    "MAHOUT-799": {
        "Key": "MAHOUT-799",
        "Summary": "Cannot run SequenceFilesFromCsvFilter, ever",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Examples",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jack Tanner",
        "Created": "02/Sep/11 05:26",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "28/Sep/11 19:37",
        "Description": "As described here:\nhttp://mail-archives.apache.org/mod_mbox/mahout-user/201106.mbox/%3C4DED5DCD.6050107@gmail.com%3E\nSequenceFilesFromCsvFilter cannot be invoked with default parameter values, because it dies like so:\nbin/mahout seqdirectory -i input -o output -filter \norg.apache.mahout.text.SequenceFilesFromCsvFilter\n...\nCaused by: java.lang.NumberFormatException: null\n     at java.lang.Integer.parseInt(Integer.java:417)\n     at java.lang.Integer.parseInt(Integer.java:499)\n     at org.apache.mahout.text.SequenceFilesFromCsvFilter.<init>(SequenceFilesFromCsvFilter.java:56)\nIf one adds the parameters -kcol 0 -vcol 0 (or their long-form versions), it dies like so:\nUnexpected -kcol while processing Job-Specific Options\nCommenting out SequenceFilesFromCsvFilter:56 and SequenceFilesFromCsvFilter:57, like so, allows the run to proceed\n//    this.keyColumn = Integer.parseInt(options.get(KEY_COLUMN_OPTION[0]));\n//    this.valueColumn = Integer.parseInt(options.get(VALUE_COLUMN_OPTION[0]));",
        "Issue Links": []
    },
    "MAHOUT-800": {
        "Key": "MAHOUT-800",
        "Summary": "bin/mahout attempts cluster mode  if HADOOP_CONF_DIR is set plausibly (and hence appended to classpath), even with MAHOUT_LOCAL set and no HADOOP_HOME",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Examples,                                            Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Dan Brickley",
        "Created": "02/Sep/11 08:41",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "23/Sep/11 09:00",
        "Description": "(This began as a build-reuters.sh bug report, but the problem seemed deeper; please excuse the narrative format here)\nSummary: both examples/bin/build-reuters.sh and bin/mahout will attempt cluster mode if HADOOP_CONF_DIR env variable points at a Hadoop conf/ directory, because bin/mahout appends it to Java's classpath. This seems to trigger something in Mahout Java that will to try to use the cluster, without this being explicitly requested.\nThere have been reports (Jeff Eastman, myself; http://mail-archives.apache.org/mod_mbox/mahout-user/201108.mbox/%3CCAFNgM+Y4twNVL_RSyNb+hGhoAu0xW917YfUTW3a5-m=Z0dynDA@mail.gmail.com%3E ) of build-reuters.sh attempting cluster mode, even while claiming - \"MAHOUT_LOCAL is set, running locally\". (or for that matter in slight variant conditions, \"no HADOOP_HOME set, running locally\").\nExperimenting here with a fresh trunk install, clean ~/.m2/ on a laptop with a pseudo-cluster Hadoop configuration available, I find HADOOP_CONF_DIR seems to be the key.\nWhen HADOOP_CONF_DIR is set to a working value (regardless of whether cluster is running), and regardless of HADOOP_HOME and MAHOUT_LOCAL, build-reuters.sh tries to use the cluster. Aside: this is not the same as it using non-clustering local Hadoop, since I see errors such as \"11/09/02 09:27:10 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:9000. Already tried 1 time(s).\" unless the cluster is up. If the cluster is up and accessible, I'll see java.io.IOException instead, presumably since the files aren't there.\nIf I do 'export HADOOP_CONF_DIR=' then build-reuters.sh (both kmeans and lda modes) runs OK without real Hadoop.\nIf I retry with a bogus value for HADOOP_CONF_DIR e.g. /foo, this also seems fine. Only when it finds a Hadoop installation does it get confused.\nMinimally I'd consider this a documentation issue. Nothing in build_reuters.sh script mentions role of HADOOP_CONF_DIR. Reading build-reuters.sh I get the impression both clustered and local modes are possible; however mailing list discussion leave me ensure whether clustered mode is still supposed to work in trunk.\nTests: (with no HADOOP_HOME set)\nRunning these extracts from build-reuters.sh in examples/bin/ after having previously run build-reuters.sh to fetch data...\n#this one runs OK\nMAHOUT_LOCAL=true HADOOP_CONF_DIR=/foo ../../bin/mahout seqdirectory \\\n        -i mahout-work/reuters-out -o mahout-work/reuters-out-seqdir -c UTF-8 -chunk 5\n#this fails (assuming there's a Hadoop there) by attempting clustered mode: 'Call to localhost/127.0.0.1:9000 failed...'\nMAHOUT_LOCAL=true HADOOP_CONF_DIR=$HOME/working/hadoop/hadoop-0.20.2/conf ../../bin/mahout seqdirectory \\\n        -i mahout-work/reuters-out -o mahout-work/reuters-out-seqdir -c UTF-8 -chunk 5\nSame thing with seq2sparse\n#fails, localhost:9000\nHADOOP_CONF_DIR=$HOME/working/hadoop/hadoop-0.20.2/conf MAHOUT_LOCAL=true ../../bin/mahout seq2sparse \\\n    -i mahout-work/reuters-out-seqdir/ -o mahout-work/reuters-out-seqdir-sparse-kmeans\n#runs locally just fine (because of bad hadoop conf path)\nHADOOP_CONF_DIR=$HOME/bad/path/working/hadoop/hadoop-0.20.2/conf MAHOUT_LOCAL=true ../../bin/mahout seq2sparse \\\n    -i mahout-work/reuters-out-seqdir/ -o mahout-work/reuters-out-seqdir-sparse-kmeans\nI get same behaviour from '../../bin/mahout kmeans' too, so the problem seems general, not driver-specific. \nAll this seems to contradict the notes in ../../bin/mahout, i.e.\n\nMAHOUT_LOCAL       set to anything other than an empty string to force\nmahout to run locally even if\nHADOOP_CONF_DIR and HADOOP_HOME are set\n\nDigging into bin/mahout it seems the accidental clustering happens deeper into java-land, not in the .sh; it's not invoking hadoop directly there. We get this far:\n  exec \"$JAVA\" $JAVA_HEAP_MAX $MAHOUT_OPTS -classpath \"$CLASSPATH\" $CLASS \"$@\"\nI compared the Java commandlines generated by successful vs accidentally-cluster-invoking runs of bin/mahout ...it seems the only difference is whether a hadoop conf directory is on the classpath that's passed to Java.\nIf I blank out with 'HADOOP_CONF_DIR=', and 'HADOOP_HOME=' and then run \nMAHOUT_LOCAL=true ../../bin/mahout kmeans \\\n    -i mahout-work/reuters-out-seqdir-sparse-kmeans/tfidf-vectors/ \\\n    -c mahout-work/reuters-kmeans-clusters \\\n    -o mahout-work/reuters-kmeans \\\n    -x 10 -k 20 -ow\n...against an edited version of bin/mahout that appends a hadoop conf dir to the classpath, i.e.\n  exec \"$JAVA\" $JAVA_HEAP_MAX $MAHOUT_OPTS -classpath \"$CLASSPATH:/Users/danbri/working/hadoop/hadoop-0.20.2/conf\" $CLASS \"$@\"\nThis is enough to get \"Exception in thread \"main\" java.io.IOException: Call to localhost/127.0.0.1:9000 failed on local exception: java.io.EOFException\"\n(...and if I remove the /conf path from classpath, we're back to expected behaviours).\nNot sure whether it's best to patch this in bin/mahout, or in the Java (perhaps the former might mask issues that'll cause later confusion?)\nPerhaps only do \n  CLASSPATH=${CLASSPATH}:$HADOOP_CONF_DIR\nif we're not seeing MAHOUT_LOCAL?",
        "Issue Links": []
    },
    "MAHOUT-801": {
        "Key": "MAHOUT-801",
        "Summary": "MongoDBDataModel  getID() throws a class cast exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Vishnu",
        "Created": "07/Sep/11 15:55",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "12/Sep/11 14:58",
        "Description": "I imported GroupLense ratings.dat to a mongodb collection \"ratings\" using mongoimport \nOutput of db.ratings.find() query\n\"_id\" : ObjectId(\"4e6733621497ef8543c1e144\"), \"userId\" : 1, \"movieId\" : 1193, \"rating\" : 5, \"timestamp\" : NumberLong(978300760) \n\"_id\" : ObjectId(\"4e6733621497ef8543c1e145\"), \"userId\" : 1, \"movieId\" : 661, \"rating\" : 3, \"timestamp\" : NumberLong(978302109) \n\"_id\" : ObjectId(\"4e6733621497ef8543c1e146\"), \"userId\" : 1, \"movieId\" : 914, \"rating\" : 3, \"timestamp\" : NumberLong(978301968) \n\"_id\" : ObjectId(\"4e6733621497ef8543c1e147\"), \"userId\" : 1, \"movieId\" : 3408, \"rating\" : 4, \"timestamp\" : NumberLong(978300275) \n\"_id\" : ObjectId(\"4e6733621497ef8543c1e148\"), \"userId\" : 1, \"movieId\" : 2355, \"rating\" : 5, \"timestamp\" : NumberLong(978824291) \nSpring Bean creating MongoDBDataModel instance\n\n  <bean id=\"mongodbDataModel\" class=\"org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel\">\n        <constructor-arg index=\"0\" type=\"java.lang.String\" value=\"127.0.0.1\"/>\n        <constructor-arg index=\"1\" type=\"int\" value=\"27017\"/>\n        <constructor-arg index=\"2\" type=\"java.lang.String\" value=\"grouplens\"/>\n        <constructor-arg index=\"3\" type=\"java.lang.String\" value=\"ratings\" />\n        <constructor-arg index=\"4\" type=\"boolean\" value=\"false\"/>\n        <constructor-arg index=\"5\" type=\"boolean\" value=\"false\"/>\n        <constructor-arg index=\"6\" type=\"java.text.DateFormat\" value=\"#{ null }\"/>\n        <constructor-arg index=\"7\" type=\"java.lang.String\" value=\"user\"/>\n        <constructor-arg index=\"8\" type=\"java.lang.String\" value=\"password\"/>\n        <constructor-arg index=\"9\" type=\"java.lang.String\" value=\"userId\"/>\n        <constructor-arg index=\"10\" type=\"java.lang.String\" value=\"movieId\"/>\n        <constructor-arg index=\"11\" type=\"java.lang.String\" value=\"rating\"/>\n    </bean>\n\n\nWhen MongDBDataModel loads the rating from mongdb, method getID() throws ClassCastException\n\nCaused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String\n\tat org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel.getID(MongoDBDataModel.java:742)\n\tat org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel.buildModel(MongoDBDataModel.java:564)\n\tat org.apache.mahout.cf.taste.impl.model.mongodb.MongoDBDataModel.<init>(MongoDBDataModel.java:347)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\n\n\nException is being thrown by last return statement \"return (String) id;\"\n\nprivate String getID(Object id, boolean isUser) {\n    if (id.getClass().getName().contains(\"ObjectId\")) {\n      if (isUser) {\n        userIsObject = true;\n      } else {\n        itemIsObject = true;\n      }\n      return ((ObjectId) id).toStringMongod();\n    } else {\n      return (String) id;\n    }\n  }",
        "Issue Links": []
    },
    "MAHOUT-802": {
        "Key": "MAHOUT-802",
        "Summary": "Start Phase doesn't properly work in RecommenderJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "07/Sep/11 20:20",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "27/Sep/11 12:24",
        "Description": "I'm trying to run RecommenderJob and do --startPhase 2 since I have my prefs already in the right format.  Unfortunately, when I do that, I get:\n\njava.lang.IllegalArgumentException: Number of columns was not correctly set!\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)\n\tat org.apache.mahout.math.hadoop.similarity.RowSimilarityJob$SimilarityReducer.setup(RowSimilarityJob.java:296)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:648)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:256)\nThis appears to be due to the fact that the numberOfUsers variable defaults to 0 and is only set when phase 1 is run.",
        "Issue Links": []
    },
    "MAHOUT-803": {
        "Key": "MAHOUT-803",
        "Summary": "Complete minsize constraints for similarity measures used in RowSimilarityJob",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Sep/11 08:02",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "11/Mar/13 08:38",
        "Description": "The latest implementation of RowSimilarityJob allows specifying a threshold for the minimum similarity value of the resulting row pairs.\nA measure can specify a minsize constraints via VectorSimilarityMeasure.consider(...) to prune some candidate pairs very early by looking at some statistics computed for the single rows.\nFor example if cooccurrence count is used as similarity measure and a threshold of 5 is set, then all row pairs where one of the vectors has less than 5 non-zero components can be discarded.\nThese min-size constraints are still missing for CityBlockSimilarity, LoglikelihoodSimilarity and EuclideanDistanceSimilarity",
        "Issue Links": []
    },
    "MAHOUT-804": {
        "Key": "MAHOUT-804",
        "Summary": "Each page in Mahout's Confluence Wiki has 2 URLs, with differing page styles and search behaviours",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Dan Brickley",
        "Created": "12/Sep/11 07:14",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 20:01",
        "Description": "There are two styles of URL in circulation for URLs into Mahout's Wiki (presumably an Apache-wide configuration issue):\nhttps://cwiki.apache.org/MAHOUT/svd-singular-value-decomposition.html vs\nhttps://cwiki.apache.org/confluence/display/MAHOUT/SVD+-+Singular+Value+Decomposition\nThey appear to be the self-same confluence 3.4.9 installation (or its raw filetree). Each has a different search box at the top of the page. The version with 'confluence/' in the path does a confluence search, and returns similar URLs as results. The one with '.html' suffixes does a domain-constrained Google search. \nDespite markup canonicalising the confluence variant, ie.  <link rel=\"canonical\" href=\"https://cwiki.apache.org/confluence/display/MAHOUT/SVD+-+Singular+Value+Decomposition\"> appearing in the confluence pages, it seems the Google search results typically throw people into the other version of the Wiki site.\nThis is all mildly confusing, mildly annoying but overall mostly harmless. It could be having some negative impact on google rank & suchlike, since incoming links will be split between the two styles. Maybe this could be passed along to the Wiki admins? \nWhich version does the Mahout team consider canonical URLs (for external links etc)?",
        "Issue Links": []
    },
    "MAHOUT-805": {
        "Key": "MAHOUT-805",
        "Summary": "Mahout script sets HADOOP_CONF_DIR to $HADOOP_HOME/src/conf instead of $HADOOP_HOME/conf",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Examples",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "12/Sep/11 20:17",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "13/Sep/11 07:53",
        "Description": "When the HADOOP_CONF_DIR is not set, the Mahout scripts sets this environment variable to $HADOOP_HOME/src/conf\ninstead of $HADOOP_HOME/conf, which is the default location for Hadoop configuration.",
        "Issue Links": []
    },
    "MAHOUT-806": {
        "Key": "MAHOUT-806",
        "Summary": "Seq2Sparse not runnable with ToolRunner due to private Constructor",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Frank Scholten",
        "Created": "12/Sep/11 21:46",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "13/Sep/11 07:48",
        "Description": "Similar to MAHOUT-714",
        "Issue Links": []
    },
    "MAHOUT-807": {
        "Key": "MAHOUT-807",
        "Summary": "Wrong prefixes in PrefixAdditionFilter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "F. Bausch",
        "Created": "13/Sep/11 15:28",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "14/Sep/11 09:22",
        "Description": "The org.apache.mahout.text.PrefixAdditionFilter is not working as intended. The prefix is computed wrong when there is recursion (this is when there is at least one subdirectory).\nI think the line:\nfs.listStatus(fst.getPath(), new PrefixAdditionFilter(getConf(), getPrefix() + Path.SEPARATOR + current.getName(), getOptions(), writer, fs));\nshould better be:\nfs.listStatus(fst.getPath(), new PrefixAdditionFilter(getConf(), getPrefix() + Path.SEPARATOR + current.getName() + Path.SEPARATOR + fst.getPath().getName(), getOptions(), writer, fs));",
        "Issue Links": []
    },
    "MAHOUT-808": {
        "Key": "MAHOUT-808",
        "Summary": "logical error with term counting in org.apache.mahout.vectorizer.DictionaryVectorizer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Phil",
        "Created": "14/Sep/11 08:12",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "14/Sep/11 09:22",
        "Description": "when using mahout lda for topic modeling, creating vectors from SequenceFile is essential, (refer to https://cwiki.apache.org/confluence/display/MAHOUT/Creating+Vectors+from+Text) but when the --minSupport was set a little bit larger, I found the term counting not right \u2014 there is a logical error at org.apache.mahout.vectorizer.DictionaryVectorizer.java:line 335\n    job.setCombinerClass(TermCountReducer.class);\nNow turn to line 41 at org.apache.mahout.vectorizer.term.TermCountReducer.java\n    if (sum >= minSupport) \n{\n      context.write(key, new LongWritable(sum));\n    }\n\nso some terms would be filtered at Combiner even though they actually could pass through, absolutely this is not what we've expected.",
        "Issue Links": []
    },
    "MAHOUT-809": {
        "Key": "MAHOUT-809",
        "Summary": "Bad bug in ChunkedWriter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5,                                            0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "F. Bausch",
        "Created": "14/Sep/11 13:53",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "14/Sep/11 14:00",
        "Description": "org.apache.mahout.text.ChunkedWriter has a bug, that causes data loss, if the maximal chunk size is reached. The first chunk is overwritten, then it continues normally.\nThis is caused in line 58:\nwriter = new SequenceFile.Writer(fs, conf, getPath(currentChunkID++), Text.class, Text.class);\nThe fix should look like this:\nwriter = new SequenceFile.Writer(fs, conf, getPath(++currentChunkID), Text.class, Text.class);",
        "Issue Links": []
    },
    "MAHOUT-810": {
        "Key": "MAHOUT-810",
        "Summary": "Create EnsembleRecommender",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Daniel Xiaodan Zhou",
        "Created": "14/Sep/11 14:22",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 16:23",
        "Description": "Q: Is there an EnsembleRecommender or CompoundRecommender that takes input\nfrom other recommender algorithms and combine them to generate better\nresults? \nTed Dunning:\nThere isn't really any such thing although the SGD models are easy to glue\ntogether in this way.\nThere is a guy named Praneet at UCI who is doing some feature sharding work\nthat might relate to what you are doing.  His email is\npraneetmhatre@gmail.com\nSean Owen:\nThere isn't. For the recommenders that work by computing an estimated\npreference value for items, I suppose you could average their\nestimates and rank by that.\nMore crudely, you could stitch together the recommendations of\nrecommender 1 and 2 by taking the top 10 amongst each of their top\nrecommendations \u2013 averaging estimates where an item appears in both\nlists. That's much less work for you; it's not quite as \"accurate\".\nDanny Bickson:\nIn terms of papers about ensemble methods/blending I suggest looking at the\nBigChaos Netflix paper:\nhttp://www.*netflixprize*.com/assets/*GrandPrize2009*_BPC_*BigChaos*.pdf\nSee section 7.",
        "Issue Links": []
    },
    "MAHOUT-811": {
        "Key": "MAHOUT-811",
        "Summary": "Mahout examples try to write to examples/bin/work, which may not be writeable by current user",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Examples",
        "Assignee": "Drew Farris",
        "Reporter": "Andrew Bayer",
        "Created": "14/Sep/11 15:40",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "15/Oct/11 10:26",
        "Description": "The examples in examples/bin create subdirectories (either work or mahout-work) in that directory and write to those subdirectories. This works fine if the current user has write access to examples/bin, but if not (such as in the case of the package generated by Bigtop, in which the files are installed to /usr/lib/mahout and owned by root), the examples can't run. This is causing BIGTOP-96, but needs to be fixed in Mahout. The patch I'm attaching changes all the references to work, examples/bin/work, and mahout-work to instead use /tmp/mahout-work-${USER}, which will be writeable.",
        "Issue Links": []
    },
    "MAHOUT-812": {
        "Key": "MAHOUT-812",
        "Summary": "Allow ConfusionMatrix to be Writable (via MatrixWritable)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "18/Sep/11 02:23",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "03/Oct/11 07:51",
        "Description": "ConfusionMatrix does not support Writable. This patch adds that feature. ConfusionMatrix is a subclass of MatrixWritable.\nSince ConfusionMatrix is somewhat less than useful without the row/column labels, and MatrixWritable does not support writing bindings (it only saves numbers), this patch fixes both.\nIncludes unit test for ConfusionMatrix (previously missing) which includes exercise of MatrixWritable support for numbers and labels. (There is no independent unit test for MatrixWritable.)",
        "Issue Links": [
            "/jira/browse/MAHOUT-838"
        ]
    },
    "MAHOUT-813": {
        "Key": "MAHOUT-813",
        "Summary": "RecommenderJob incorrectly sets io.sort.mb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "18/Sep/11 13:31",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "18/Sep/11 13:33",
        "Description": "The RecommenderJob can incorrectly set the io.sort.mb value to values higher than what's allowed, due to MAPREDUCE-2308.  It's not clear whether the RJ should even set the factor or just rely on documenting that users will need to set it higher.",
        "Issue Links": []
    },
    "MAHOUT-814": {
        "Key": "MAHOUT-814",
        "Summary": "SSVD local tests should use their own tmp space to avoid collisions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Grant Ingersoll",
        "Created": "18/Sep/11 14:34",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "27/Sep/11 12:26",
        "Description": "Running Mahout in an environment with Jenkins also running and am getting:\n\njava.io.FileNotFoundException: /tmp/q-temp.seq (Permission denied)\n        at java.io.FileOutputStream.open(Native Method)\n        at java.io.FileOutputStream.<init>(FileOutputStream.java:209)\n        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:187)\n        at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:183)\n        at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:241)\n        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:335)\n        at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:368)\n        at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:528)\n        at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.<init>(SequenceFile.java:1198)\n        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:401)\n        at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:284)\n        at org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.getTempQw(QRFirstStep.java:263)\n        at org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.flushSolver(QRFirstStep.java:104)\n        at org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.map(QRFirstStep.java:175)\n        at org.apache.mahout.math.hadoop.stochasticsvd.qr.QRFirstStep.collect(QRFirstStep.java:279)\n        at org.apache.mahout.math.hadoop.stochasticsvd.QJob$QMapper.map(QJob.java:142)\n        at org.apache.mahout.math.hadoop.stochasticsvd.QJob$QMapper.map(QJob.java:71)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)\nAlso seeing the following tests fail:\n\nTests in error: \n  testSSVDSolverSparse(org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest): Q job unsuccessful.\n  testSSVDSolverPowerIterations1(org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest): Q job unsuccessful.\n  testSSVDSolverPowerIterations1(org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest): Q job unsuccessful.\n  testSSVDSolverDense(org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest): Q job unsuccessful.\nI haven't checked all of them, but I suspect they are all due to the same reason.  We should dynamically create a temp area for each test using temporary directories under the main temp dir.",
        "Issue Links": []
    },
    "MAHOUT-815": {
        "Key": "MAHOUT-815",
        "Summary": "LDA Inference Corrections, Alpha (Dirichlet) Estimation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Christoph Boden",
        "Created": "21/Sep/11 17:56",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "30/Oct/11 20:29",
        "Description": "Hi, I am a PhD Student at TU Berlin DIMA. I am currently working on Mahouts LDA Implementation together with Sebastian Schelter. We identified a couple of points that can be fixed or improved in the current version.\nWe propose to fix the inference in the expectation step of EM in accordance with [1], implement maximum likelihood estimation of the dirichlet distribution (alpha) as presented in [1] and some refacoring.\n[1]Blei, David M.; Ng, Andrew Y.; Jordan, Michael I (January 2003). Lafferty, John. ed. \"Latent Dirichlet allocation\". Journal of Machine Learning Research 3 (4-5): pp. 993-1022. doi:10.1162/jmlr.2003.3.4-5.993",
        "Issue Links": []
    },
    "MAHOUT-816": {
        "Key": "MAHOUT-816",
        "Summary": "Power iterations -q=2 directory mess-up",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "21/Sep/11 18:10",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "23/Sep/11 00:15",
        "Description": "Not sure it this is related, but sounds similar. I can't run more than one power iteration, ie q=2 produces\n11/09/21 11:25:46 INFO mapred.LocalJobRunner: reduce > reduce\n11/09/21 11:25:46 INFO mapred.Task: Task 'attempt_local_0004_r_000000_0' done.\n11/09/21 11:25:50 INFO mapred.JobClient: Cleaning up the staging area file:/tmp/hadoop-nathanhalko/mapred/staging/nathanhalko-200181280/.staging/job_local_0005\nException in thread \"main\" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory temp/ABt-job-1 already exists\nat org.apache.hadoop.mapreduce.lib.output.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:134)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:830)\nat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:791)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:396)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\nat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:791)\nat org.apache.hadoop.mapreduce.Job.submit(Job.java:465)\nat org.apache.mahout.math.hadoop.stochasticsvd.ABtJob.run(ABtJob.java:454)\nat org.apache.mahout.math.hadoop.stochasticsvd.SSVDSolver.run(SSVDSolver.java:312)\nat org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.run(SSVDCli.java:118)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\nat org.apache.mahout.math.hadoop.stochasticsvd.SSVDCli.main(SSVDCli.java:163)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\nat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\nat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nfor q=0,1 everything works fine. I am running with --overwrite and I rm -rf the temp dir before running.",
        "Issue Links": []
    },
    "MAHOUT-817": {
        "Key": "MAHOUT-817",
        "Summary": "Add PCA options to SSVD code",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "21/Sep/11 18:14",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "07/Mar/12 22:54",
        "Description": "It seems that a simple solution should exist to integrate PCA mean subtraction into SSVD algorithm without making it a pre-requisite step and also avoiding densifying the big input. \nSeveral approaches were suggested:\n1) subtract mean off B\n2) propagate mean vector deeper into algorithm algebraically where the data is already collapsed to smaller matrices\n3) --?\nIt needs some math done first . I'll take a stab at 1 and 2 but thoughts and math are welcome.",
        "Issue Links": []
    },
    "MAHOUT-818": {
        "Key": "MAHOUT-818",
        "Summary": "Canopy Emits Too Many Trivial Clusters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "26/Sep/11 22:07",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "28/Sep/11 20:15",
        "Description": "Users of Canopy clustering report that the single reducer used in the mapreduce version often takes dispropportately long to process the results of multiple mappers. This patch introduces a new Canopy CLI argument, cf (-clusterFilter), which if present establishes a lower bound on the numPoints of canopies output from the algorithm. The default value for this filter is 0, and all canopies are output. Setting -cf 1 would eliminate any canopies which contain only 1 point from subsequent processing steps.",
        "Issue Links": []
    },
    "MAHOUT-819": {
        "Key": "MAHOUT-819",
        "Summary": "NullPointerException in matrix vector multiplication in TimesSquaredJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Christoph Nagel",
        "Created": "27/Sep/11 12:52",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "27/Sep/11 13:35",
        "Description": "TimesSquaredMapper causes a NullPointerException in method close() if member out isn't initiated.\nAdding a null-check fixes the issue.",
        "Issue Links": []
    },
    "MAHOUT-820": {
        "Key": "MAHOUT-820",
        "Summary": "Dependency on sjl4j-jcl should not have compile scope",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "build",
        "Assignee": "Sean R. Owen",
        "Reporter": "Aaron Kaplan",
        "Created": "27/Sep/11 16:09",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "27/Sep/11 16:16",
        "Description": "The top-level pom.xml and integration/pom.xml specify a dependency on slf4j-jcl with default (compile) scope. The point of slf4j is to allow libraries to work with any underlying logging system, leaving the choice up to the application programmer. If my application uses jcl-over-slf4j, and also uses your library which uses slf4j-jcl, initialization fails because there's a logging loop.\ncore/pom.xml and math/pom.xml specify slf4j-jcl with test scope only. That's fine, because dependencies with test scope aren't propagated to dependents.",
        "Issue Links": []
    },
    "MAHOUT-821": {
        "Key": "MAHOUT-821",
        "Summary": "MemoryDiffStorage mistake in recommendable items method",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "28/Sep/11 07:27",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "28/Sep/11 19:37",
        "Description": "The MemoryDiffStorage class has a \"positive/negative\" bug in building a list of \"recommendable items\". Attached patch fixes the bug and includes updates for various unit tests.",
        "Issue Links": []
    },
    "MAHOUT-822": {
        "Key": "MAHOUT-822",
        "Summary": "Mahout needs to be made compatible with Hadoop .23 releases",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "build",
        "Assignee": "Tom Pierce",
        "Reporter": "Roman Shaposhnik",
        "Created": "29/Sep/11 22:37",
        "Updated": "14/Feb/13 23:26",
        "Resolved": "12/Mar/12 19:19",
        "Description": "As part of the Hadoop stack integration project (Apache Bigtop) we are now trying to compile Mahout's upcoming 0.6 release against Hadoop 0.22 and 0.23.\nI'm attaching the patch to Mahout's Maven build system that made it possible. I would also like to request help in solving the real issues that poped\nup when we tried to compile Mahout: http://bigtop01.cloudera.org:8080/job/Bigtop-hadoop22/COMPONENT=mahout,label=centos5/6/console\n\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/df/mapreduce/partial/Step0JobTest.java:[182,33] org.apache.hadoop.mapreduce.TaskAttemptContext is abstract; cannot be instantiated\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/df/mapreduce/partial/Step0JobTest.java:[218,9] org.apache.mahout.df.mapreduce.partial.Step0JobTest.Step0Context is not abstract and does not override abstract method getInputSplit() in org.apache.hadoop.mapreduce.MapContext\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/df/mapreduce/partial/Step0JobTest.java:[229,12] cannot find symbol\n[ERROR] symbol  : constructor Context(org.apache.hadoop.conf.Configuration,org.apache.hadoop.mapreduce.TaskAttemptID,<nulltype>,<nulltype>,<nulltype>,<nulltype>,<nulltype>)\n[ERROR] location: class org.apache.hadoop.mapreduce.Mapper.Context\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java:[68,18] org.apache.hadoop.mapreduce.Mapper.Context is abstract; cannot be instantiated\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/common/DummyRecordWriter.java:[77,19] org.apache.hadoop.mapreduce.Reducer.Context is abstract; cannot be instantiated\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/df/mapreduce/partial/PartialSequentialBuilder.java:[110,30] org.apache.hadoop.mapreduce.TaskAttemptContext is abstract; cannot be instantiated\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/df/mapreduce/partial/PartialSequentialBuilder.java:[206,28] org.apache.hadoop.mapreduce.JobContext is abstract; cannot be instantiated\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/df/mapreduce/partial/PartialSequentialBuilder.java:[227,30] org.apache.hadoop.mapreduce.TaskAttemptContext is abstract; cannot be instantiated\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/df/mapreduce/partial/MockContext.java:[30,6] org.apache.mahout.df.mapreduce.partial.MockContext is not abstract and does not override abstract method getInputSplit() in org.apache.hadoop.mapreduce.MapContext\n[ERROR] /mnt/jenkins/workspace/workspace/Bigtop-hadoop22/COMPONENT/mahout/label/centos5/build/mahout/rpm/BUILD/apache-mahout-c298f70/core/src/test/java/org/apache/mahout/df/mapreduce/partial/MockContext.java:[38,10] cannot find symbol",
        "Issue Links": [
            "/jira/browse/MRUNIT-31",
            "/jira/browse/MAHOUT-950"
        ]
    },
    "MAHOUT-823": {
        "Key": "MAHOUT-823",
        "Summary": "RandomAccessSparseVector.dot with another non-sequential vector can be extremely non-symmetric in its performance",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Eugene Kirpichov",
        "Created": "30/Sep/11 11:21",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "01/Oct/11 13:30",
        "Description": "http://codesearch.google.com/#6LK_nEANBKE/math/src/main/java/org/apache/mahout/math/RandomAccessSparseVector.java&l=172\nThe complexity of the algorithm is O(num nondefault elements in this), while it could clearly be O(min(num nondefault in this, num nondefault in x)).\nThis can be fixed by adding this code before line 189.\n\nif(x.getNumNondefaultElements() < this.getNumNondefaultElements()) {\n  return x.dot(this);\n}\n\n\nAn easy case where this asymmetry is very apparent and makes a huge difference in performance is K-Means clustering.\nIn K-Means for high-dimensional points (e.g. those that arise in text retrieval problems), the centroids often have a huge number of non-zero components, whereas points have a small number of them.\nSo, if you make a mistake and use centroid.dot(point) in your code for computing the distance, instead of point.dot(centroid), you end up with orders of magnitude worse performance (which is what we actually observed - the clustering time was a couple of minutes with this fix and over an hour without it).\nSo, perhaps, if you make this fix, quite a few people who had a similar case but didn't notice it will suddenly have a dramatic performance increase",
        "Issue Links": []
    },
    "MAHOUT-824": {
        "Key": "MAHOUT-824",
        "Summary": "FastByIDRunningAverage: Optimize SlopeOneRecommender by optimizing MemoryDiffStorage",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "02/Oct/11 01:49",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "02/Oct/11 07:04",
        "Description": "The SlopeOneRecommender has by far the best RMS of all of the online recommenders in Mahout (that I've found). Unfortunately the implementation also uses much more memory and is unuseable on my laptop.\nThis patch optimizes memory (and speed) by folding FastByIDMap<RunningAverage> into one class: FastByIDRunningAverage. This is what it sounds like: a Long-addressable array of running averages (and optionally standard deviation).",
        "Issue Links": []
    },
    "MAHOUT-825": {
        "Key": "MAHOUT-825",
        "Summary": "Canopies grouping records outside t1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Paritosh Ranjan",
        "Created": "03/Oct/11 08:10",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "16/Dec/11 19:43",
        "Description": "While finding closest canopy, there is no check to ensure that it returns canopies which are within distance t1 from the point. This results in incorrect result i.e. Points outside t1 are grouped in canopies.",
        "Issue Links": []
    },
    "MAHOUT-826": {
        "Key": "MAHOUT-826",
        "Summary": "Bayes/CBayes classification on a non-existing feature",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Andre-Philippe Paquet",
        "Created": "03/Oct/11 19:09",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "03/Jun/12 16:43",
        "Description": "(see http://comments.gmane.org/gmane.comp.apache.mahout.user/9597)\nUsing CBayes or Bayes, when trying to classify a feature/word that doesn't exist in the model, instead of returning the default/unknown label, the algorithm returns all labels with a constant score (ex: 12.386649147018964). After a quick look in CBayesAlgorithm, I found the problem in the featureWeight function that returns the theta normalized weight even if the feature didn't have any match (result=0).\nAs a fix, I overrided the function in a subclass and return 0 if the weight of the current feature in the current label is 0.",
        "Issue Links": []
    },
    "MAHOUT-827": {
        "Key": "MAHOUT-827",
        "Summary": "Another version of RecommenderJob that broadcasts the similarity matrix",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "04/Oct/11 11:46",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 08:37",
        "Description": "Add another version of RecommenderJob that computes the item similarities via RowSimilarityJob but assumes that the resulting similarity matrix fits into the memory of the mappers in the cluster. After the item similarity computation is done, the similarities are broadcasted via Hadoop's distributed cache and the recommendations are computed in a map-only pass over the data afterwards.",
        "Issue Links": []
    },
    "MAHOUT-828": {
        "Key": "MAHOUT-828",
        "Summary": "bin/mahout should only print classpath on request, not all the time",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "06/Oct/11 00:44",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "06/Oct/11 18:26",
        "Description": "This is a trivial change, but it changes the user experience.  Anybody care to keep the current (very) verbose output?",
        "Issue Links": []
    },
    "MAHOUT-829": {
        "Key": "MAHOUT-829",
        "Summary": "bin/mahout doesn't match the way the packaged forms of Mahout are arranged",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "06/Oct/11 00:46",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "06/Oct/11 18:19",
        "Description": "bin/mahout assumes that conf is in $MAHOUT_HOME/src/conf if MAHOUT_CONF_DIR isn't set.  In the packaged version of Mahout, however, it is in $MAHOUT_HOME/conf.\nThis is easy to handle with a couple of if's.  Patch incoming.",
        "Issue Links": [
            "/jira/browse/MAHOUT-830"
        ]
    },
    "MAHOUT-830": {
        "Key": "MAHOUT-830",
        "Summary": "Distribution should create .deb and .rpm packages",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "06/Oct/11 01:22",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "20/Oct/11 10:26",
        "Description": "Sometimes we need standardized install packages.  The distribution module does almost all of the work necessary for this so it is a piece of cake to finish the job.  The debian packages in particular can be generated in a portable fashion thanks to jdeb.\nI will track this work in the debian-package branch of https://github.com/tdunning/mahout\nComments are very welcome.",
        "Issue Links": [
            "/jira/browse/MAHOUT-829"
        ]
    },
    "MAHOUT-831": {
        "Key": "MAHOUT-831",
        "Summary": "@Experimental annotation to indicate which implementations are not intended for production use",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "07/Oct/11 16:41",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 08:16",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-832": {
        "Key": "MAHOUT-832",
        "Summary": "clusterdump job: bug and usability problems",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "09/Oct/11 03:20",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jun/12 16:08",
        "Description": "The clusterdump job has a bug and some usability problems.\nBug: file type 'GRAPH_ML' is described in the arguments as 'GML' instead.\nUsability:\n\nIf you do not give the --pointsDir argument, the CSV and GRAPH_ML outputs are empty\nThe GRAPH_ML (xml) output would benefit from pretty-printing.\nThe gml output is missing the final </graphml> line.\n--startPhase and --endPhase: what are the names of the phases?",
        "Issue Links": []
    },
    "MAHOUT-833": {
        "Key": "MAHOUT-833",
        "Summary": "Make conversion to sequence files map-reduce",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Grant Ingersoll",
        "Created": "09/Oct/11 12:57",
        "Updated": "29/Jul/13 02:09",
        "Resolved": "23/Jun/13 18:16",
        "Description": "Given input that is on HDFS, the SequenceFilesFrom****.java classes should be able to do their work in parallel.",
        "Issue Links": []
    },
    "MAHOUT-834": {
        "Key": "MAHOUT-834",
        "Summary": "rowsimilarityjob doesn't clean it's temp dir, and fails when seeing it again",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dan Brickley",
        "Created": "09/Oct/11 13:37",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "07/May/12 14:16",
        "Description": "If I do this:\nmahout rowsimilarity --input matrixified/matrix --output sims/ --numberOfColumns 27684 --similarityClassname SIMILARITY_LOGLIKELIHOOD --excludeSelfSimilarity\nthen clean my output and rerun,\nrm -rf sims/ # (though this step doesn't even seem needed)\nthen try again:\nmahout rowsimilarity --input matrixified/matrix --output sims/ --numberOfColumns 27684 --similarityClassname SIMILARITY_LOGLIKELIHOOD --excludeSelfSimilarity\nThe temp files left from the first run make a re-run impossible - we get: \"Exception in thread \"main\" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory temp/weights already exists\".\nManually deleting the temp directory fixes this.\nI get same behaviour if I explicitly pass in a --tempdir path, e.g.:\nmahout rowsimilarity --input matrixified/matrix --output sims/ --numberOfColumns 27684 --similarityClassname SIMILARITY_LOGLIKELIHOOD --excludeSelfSimilarity --tempDir tmp2/\nPresumably something like HadoopUtil.delete(getConf(),tempDirPath) is needed somewhere?  (and maybe --overwrite too ?)",
        "Issue Links": []
    },
    "MAHOUT-835": {
        "Key": "MAHOUT-835",
        "Summary": "Don't compute out of bag error for Decision Forests",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "09/Oct/11 17:53",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "11/Oct/11 05:00",
        "Description": "The current implementation of Decision Forests makes a lot of assumption about how Hadoop works when computing the out-of-bag error. Removing these calculations not only makes the code run faster, it's also less buggier and won't break easily when a new version of Hadoop is used.",
        "Issue Links": []
    },
    "MAHOUT-836": {
        "Key": "MAHOUT-836",
        "Summary": "On donating my Robust PCA Java code to Mahout",
        "Type": "New JIRA Project",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sujit Nair",
        "Created": "09/Oct/11 22:03",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Mar/14 21:47",
        "Description": "Hi All,\nI have an implementation of Robust PCA (a.k.a low rank and sparse decomposition) in Java which I would like to donate to Mahout. I am a MATLAB expert, comfortable with C++ and have just started with Java. I am completely new to Mahout but am very excited to participate and contribute. \nI have tested my code exhaustively and there does not seem to be any issues. The results are very good but the code definitely needs some optimization. \nPlease let me know if there is interest. \nThanks,\nSujit",
        "Issue Links": []
    },
    "MAHOUT-837": {
        "Key": "MAHOUT-837",
        "Summary": "Make build-asf-email.sh HDFS aware",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Raphael Cendrillon",
        "Reporter": "Grant Ingersoll",
        "Created": "10/Oct/11 19:55",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "08/Dec/11 10:42",
        "Description": "In MAHOUT-798, build-asf-email.sh was added and it checks to see if certains tasks are already done in order to skip costly calculations.  The problem is, the checking is not HDFS aware, so in a Hadoop cluster, it repeats some unnecessary tasks (such as conversion to vectors)",
        "Issue Links": []
    },
    "MAHOUT-838": {
        "Key": "MAHOUT-838",
        "Summary": "Make the confusion matrix writable to a file when testing classifiers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "10/Oct/11 22:17",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Nov/11 11:20",
        "Description": "If you have a lot of labels for a classifier, the confusion matrix is hard to fit in terminal window.  Would be nice if we could write it out to a file.",
        "Issue Links": [
            "/jira/browse/MAHOUT-812"
        ]
    },
    "MAHOUT-839": {
        "Key": "MAHOUT-839",
        "Summary": "rowid job failing (when parsing options)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Dan Brickley",
        "Created": "11/Oct/11 14:34",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "24/Oct/11 07:27",
        "Description": "Although MAHOUT-757 moved towards standard option naming, it uses different APIs for option parsing than other jobs.\nOn my system, it died reliably with null pointer error. Reported in mail here, but not reconfirmed by anyone else yet: http://permalink.gmane.org/gmane.comp.apache.mahout.user/9659\nExample: \nTellyClub:bin danbri$ ./mahout rowid --help\nMAHOUT_LOCAL is set, so we don't add HADOOP_CONF_DIR to classpath.\nMAHOUT_LOCAL is set, running locally\n[skipping some hopefully unrelated SLF4J errors re same thing on classpath twice]\nException in thread \"main\" java.lang.NullPointerException\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:61)\n\tat org.apache.hadoop.fs.Path.<init>(Path.java:50)\n\tat org.apache.mahout.utils.vectors.RowIdJob.run(RowIdJob.java:49)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.utils.vectors.RowIdJob.main(RowIdJob.java:89)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)",
        "Issue Links": []
    },
    "MAHOUT-840": {
        "Key": "MAHOUT-840",
        "Summary": "Decision Forests should support Regression problems",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "11/Oct/11 16:31",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "23/Dec/11 09:32",
        "Description": "Improve Decision Forest code in order to handle numerical targets, thus supporting regression problems",
        "Issue Links": [
            "/jira/browse/MAHOUT-954",
            "/jira/browse/MAHOUT-926",
            "/jira/browse/MAHOUT-928"
        ]
    },
    "MAHOUT-841": {
        "Key": "MAHOUT-841",
        "Summary": "mailing list link on home page is broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Documentation",
        "Assignee": "Sean R. Owen",
        "Reporter": "Patrick D. Hunt",
        "Created": "14/Oct/11 16:25",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "14/Oct/11 16:49",
        "Description": "The TLP web site's \"Mailing Lists\" link is broken http://mahout.apache.org/\nLinks to: https://cwiki.apache.org/confluence/display/MAHOUT/Mailing+Lists",
        "Issue Links": []
    },
    "MAHOUT-842": {
        "Key": "MAHOUT-842",
        "Summary": "Inconsistent and conflicting use of '-i' flag",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "15/Oct/11 10:29",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "15/Oct/11 15:36",
        "Description": "As the Cloudera folk noted on the mailing list, the \"i\" option doesn't work for \"-input\" on RecommenderJob as the job actually overwrites \"-i\" to mean something else. We can fix that by removing the short name for this option. And removing similarly conflicting short names elsewhere.\nAlong the way I noticed how much code doesn't use standard input/output flags. I've tried to standardize as much of that as possible.",
        "Issue Links": []
    },
    "MAHOUT-843": {
        "Key": "MAHOUT-843",
        "Summary": "Top Down Clustering",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Paritosh Ranjan",
        "Created": "15/Oct/11 19:27",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "10/Dec/11 19:23",
        "Description": "Top Down Clustering works in multiple steps. The first step is to find comparative bigger clusters. The second step is to cluster the bigger chunks into meaningful clusters. This can performance while clustering big amount of data. And, it also removes the dependency of providing input clusters/numbers to the clustering algorithm.\nThe \"big\" is a relative term, as well as the smaller \"meaningful\" terms. So, the control of this \"bigger\" and \"smaller/meaningful\" clusters will be controlled by the user.\nWhich clustering algorithm to be used in the top level and which to use in the bottom level can also be selected by the user. Initially, it can be done for only one/few clustering algorithms, and later, option can be provided to use all the algorithms ( which suits the case ).",
        "Issue Links": []
    },
    "MAHOUT-844": {
        "Key": "MAHOUT-844",
        "Summary": "RowSimilarityJob in Item Similarity workflow does not pickup default similarity measure",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Josh Patterson",
        "Created": "17/Oct/11 20:53",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "17/Oct/11 21:06",
        "Description": "If you omit the \"-s\" flag for running \"mahout itemsimilarity ....\" the following error occurs after a bit:\nException in thread \"main\" java.lang.NullPointerException\n\tat java.lang.String.<init>(String.java:147)\n\tat org.apache.commons.cli2.commandline.Parser.parse(Parser.java:66)\n\tat org.apache.mahout.common.AbstractJob.parseArguments(AbstractJob.java:263)\n\tat org.apache.mahout.math.hadoop.similarity.RowSimilarityJob.run(RowSimilarityJob.java:103)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.run(ItemSimilarityJob.java:199)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.main(ItemSimilarityJob.java:93)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:187)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:186)\nAdding back in the \"-s\" flag allows the job to complete.",
        "Issue Links": []
    },
    "MAHOUT-845": {
        "Key": "MAHOUT-845",
        "Summary": "Make cluster top terms code more reusable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Jake Mannix",
        "Reporter": "Frank Scholten",
        "Created": "19/Oct/11 14:06",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/Feb/12 13:51",
        "Description": "When working with Mahout text clustering I find that I keep writing code similar to the contents of\npublic static String getTopFeatures(Cluster cluster, String[] dictionary, int numTerms)\nin ClusterDumper in order to determine cluster labels.\nI think it would be useful if (parts of) this code are added to the cluster or vector API so that you could do something like\nCluster cluster = ... // get the cluster from seq file iterable\nString clusterLabel = cluster.getTopTerms(1, dictionary); // Do something with the label  \nI think this would make it easier to export and post-process clustering results, like indexing or storing them elsewhere.\nThoughts?",
        "Issue Links": []
    },
    "MAHOUT-846": {
        "Key": "MAHOUT-846",
        "Summary": "Improve Scalability of Gaussian Cluster For Wide Vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "19/Oct/11 17:06",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "26/Dec/11 14:24",
        "Description": "The pdf() implementation in GaussianCluster is pretty lame. It is computing a running product of the element pdfs which, for wide input vectors (Reuters is 41,807), always underflows and returns 0. Here's the code:\n\n  public double pdf(VectorWritable vw) {\n    Vector x = vw.get();\n    // return the product of the component pdfs\n    // TODO: is this reasonable? correct? It seems to work in some cases.\n    double pdf = 1;\n    for (int i = 0; i < x.size(); i++) {\n      // small prior on stdDev to avoid numeric instability when stdDev==0\n      pdf *= UncommonDistributions.dNorm(x.getQuick(i),\n          getCenter().getQuick(i), getRadius().getQuick(i) + 0.000001);\n    }\n    return pdf;\n\n\n  }",
        "Issue Links": []
    },
    "MAHOUT-847": {
        "Key": "MAHOUT-847",
        "Summary": "Improve Euclidean distance similarity calculation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "20/Oct/11 13:21",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "21/Oct/11 09:20",
        "Description": "In the non-distributed recommender world, the Euclidean distance similarity is calculated as n/(1+d), where d is distance and n is dimension. 1/(1+d) is a valid mapping from distance [0,infinity) to similarity (0,1]. n is there to \"correct\" for the fact that things are farther apart in higher dimensions. It would be right-er, after some discussion, to use a factor of sqrt, and apply directly to the distance; 1/(1+d/sqrt).\nI propose fixing the calculation accordingly.\nIn the distributed similarity, the formula is 1-1/(1+d), which is the wrong way around. That will be fixed. I'd apply the same heuristic, except that at the moment we don't have access to the value of n at that point. I don't like the inconsistency but it's minor; would rather get this change in now, which definitely improves things.",
        "Issue Links": []
    },
    "MAHOUT-848": {
        "Key": "MAHOUT-848",
        "Summary": "M/R job launching code should add Oozie's action.xml as a configuration resource of the Hadoop Configuration object",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Timothy Potter",
        "Created": "20/Oct/11 20:30",
        "Updated": "19/Jun/12 13:59",
        "Resolved": "31/May/12 16:04",
        "Description": "Here's an overview of what is happening:\nOozie workflow has a sub-workflow (and in my case a sub-workflow to the sub-workflow, so 3 levels down) that launches a Mahout job, such as the vectorizer as a Java action. This job fails due to class loading issues, e.g. vectorizer code cannot load a Lucene class, which it's definitely in the job jar and definitely gets found just fine if launched from a simple Oozie (1-level) workflow.\nThe solution is to include Oozie's action.xml as a configuration resource of the Hadoop Configuration object, i.e.\n        String oozieActionConfXml = System.getProperty(\"oozie.action.conf.xml\");            \n        if (oozieActionConfXml != null) \n{\n            conf.addResource(new Path(\"file:///\", oozieActionConfXml));\n        }\n\nAs you can see, there's no adverse affects if not running in an Oozie workflow. This code could be added to AbstractJob with minimal impact and much benefit to those of us using Mahout in our Oozie workflows.",
        "Issue Links": []
    },
    "MAHOUT-849": {
        "Key": "MAHOUT-849",
        "Summary": "Wrong error messages in AbstractMatrix",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "22/Oct/11 04:08",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "22/Oct/11 11:44",
        "Description": "AbstractMatrix prints out the wrong number when checking cardinality in times(Vector) and times(Matrix).",
        "Issue Links": []
    },
    "MAHOUT-850": {
        "Key": "MAHOUT-850",
        "Summary": "Random Forest Partial implementation in training stage throw EOF exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Wangda Tan (No longer used)",
        "Created": "25/Oct/11 02:00",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "25/Oct/11 08:44",
        "Description": "when running next command line to training:\nbin/hadoop jar mahout-examples-0.5-gphdce-1.1.0.0-job.jar org.apache.mahout.df.mapred.BuildForest -Dmapred.max.split.size=1874231 -oob -d rf_input/KDDTrain_20Percent.arff -ds rf_input/KDDTrain_20Percent.info -sl 5 -p -t 100\nit will throw an EOF exception.\n\u2013\nReason:\nIn previous stage, _SUCCESS and _logs file will in the output folder, when parse the output folder, _SUCCESS and _logs will cause an EOF exception",
        "Issue Links": []
    },
    "MAHOUT-851": {
        "Key": "MAHOUT-851",
        "Summary": "Add SGD to build-asf-email.sh example",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "25/Oct/11 13:05",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "12/Nov/11 08:20",
        "Description": "It would be nice to be able to do classification over the ASF emails using Stochastic Gradient Descent, much like it does for the 20 news groups.",
        "Issue Links": [
            "/jira/browse/MAHOUT-873"
        ]
    },
    "MAHOUT-852": {
        "Key": "MAHOUT-852",
        "Summary": "Upgrade Lucene dependency to 3.4",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "26/Oct/11 16:03",
        "Updated": "25/Jul/13 12:45",
        "Resolved": "26/Oct/11 17:02",
        "Description": "As the title says, commit coming shortly once the tests are done running",
        "Issue Links": []
    },
    "MAHOUT-853": {
        "Key": "MAHOUT-853",
        "Summary": "Most recent version depends on Lucene 3.3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Mark Rosenberg",
        "Created": "28/Oct/11 20:39",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "29/Oct/11 12:49",
        "Description": "Encountered a Lucene version incompatibility when using Mahout 0.6 and Solr 3.4. Solr 3.4 has a Lucene 3.4 dependency making it incompatible with Mahout 0.6. In particular, the term vector formats between Lucene 3.3 and 3.4 cause Mahout 0.6 trouble. Currently have to revert to Solr 3.3.",
        "Issue Links": []
    },
    "MAHOUT-854": {
        "Key": "MAHOUT-854",
        "Summary": "Add MinHash to build-reuters.sh example",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering,                                            Examples",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Varun Thacker",
        "Created": "30/Oct/11 17:57",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "18/Jan/12 16:08",
        "Description": "We can use the Reuters data set for MinHash clustering. Thus adding the MinHash algorithm to the build-reuters.sh would be nice.",
        "Issue Links": []
    },
    "MAHOUT-855": {
        "Key": "MAHOUT-855",
        "Summary": "LuceneTextValueEncoder doesn't properly set internal buffers, causing BufferUnderflowException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "31/Oct/11 15:39",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "31/Oct/11 16:20",
        "Description": "The LuceneTextValueEncoder throws an BufferUnderflowException when used.  See the code below.  The problem appears to be due to the CharBuffer not getting values, but I'm not sure yet.\n\n@Test\n  public void testLucene() throws Exception {\n    LuceneTextValueEncoder enc = new LuceneTextValueEncoder(\"text\");\n    enc.setAnalyzer(new WhitespaceAnalyzer(Version.LUCENE_34));\n    Vector v1 = new DenseVector(200);\n    enc.addToVector(\"test1 and more\", v1);\n    enc.flush(1, v1);\n}\n\n\nHere's the exception:\n\njava.nio.BufferUnderflowException\n\tat java.nio.HeapCharBuffer.get(HeapCharBuffer.java:127)\n\tat org.apache.mahout.vectorizer.encoders.LuceneTextValueEncoder$CharSequenceReader.read(LuceneTextValueEncoder.java:87)\n\tat org.apache.lucene.analysis.CharReader.read(CharReader.java:54)\n\tat org.apache.lucene.util.CharacterUtils$Java5CharacterUtils.fill(CharacterUtils.java:181)\n\tat org.apache.lucene.analysis.CharTokenizer.incrementToken(CharTokenizer.java:273)\n\tat org.apache.mahout.common.lucene.TokenStreamIterator.computeNext(TokenStreamIterator.java:41)\n\tat org.apache.mahout.common.lucene.TokenStreamIterator.computeNext(TokenStreamIterator.java:30)\n\tat com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:141)\n\tat com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:136)\n\tat org.apache.mahout.vectorizer.encoders.TextValueEncoder.addText(TextValueEncoder.java:78)\n\tat org.apache.mahout.vectorizer.encoders.TextValueEncoder.addText(TextValueEncoder.java:69)\n\tat org.apache.mahout.vectorizer.encoders.TextValueEncoder.addToVector(TextValueEncoder.java:59)\n\tat org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder.addToVector(FeatureVectorEncoder.java:86)\n\tat org.apache.mahout.vectorizer.encoders.FeatureVectorEncoder.addToVector(FeatureVectorEncoder.java:63)\n\tat org.apache.mahout.vectorizer.encoders.TextValueEncoderTest.testLucene(TextValueEncoderTest.java:75)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n\tat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\n\tat org.junit.runner.JUnitCore.run(JUnitCore.java:157)\n\tat com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:62)",
        "Issue Links": []
    },
    "MAHOUT-856": {
        "Key": "MAHOUT-856",
        "Summary": "build-20news-bayes.sh doesn't work when downloading content",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "01/Nov/11 14:25",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "01/Nov/11 15:28",
        "Description": "The build-20news-bayes.sh script doesn't work when downloading the content for the first time.  The issue is that it changes the directory to the temp directory and then later tries to do \"cd ../..\" to get back to MAHOUT_HOME, but that doesn't work if it changes to the /tmp directory.",
        "Issue Links": []
    },
    "MAHOUT-857": {
        "Key": "MAHOUT-857",
        "Summary": "Rework 20 NewsGroup shell script example to include SGD Example",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "01/Nov/11 15:32",
        "Updated": "13/May/13 13:07",
        "Resolved": "04/Nov/11 13:29",
        "Description": "We have build-20news-bayes.sh that runs our NB stuff on 20 news groups.  We also have an SGD example that works on 20 news groups, but no script to run it.  I'm going to rename build-20news-bayes.sh to classify-20news.sh and incorporate the two.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1212"
        ]
    },
    "MAHOUT-858": {
        "Key": "MAHOUT-858",
        "Summary": "ConnectionPoolDataSource is not abstract error while compiling",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Shern Shiou Tan",
        "Created": "01/Nov/11 16:05",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "01/Nov/11 16:14",
        "Description": "While compiling the latest mahout using openjdk-7-jdk\nError: ConnectionPoolDataSource is not abstract and does not override abstract method getParentLogger() in CommonDataSource\nBut compiling the source with openjdk-6-jdk is successful\nPlease refer to \nhttp://download.oracle.com/javase/7/docs/api/java/sql/Connection.html#getNetworkTimeout\nand\nhttp://download.oracle.com/javase/6/docs/api/java/sql/Connection.html\nI think the difference causes the bug.",
        "Issue Links": []
    },
    "MAHOUT-859": {
        "Key": "MAHOUT-859",
        "Summary": "Move Decision Forests to classifier package",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "02/Nov/11 01:51",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "03/Nov/11 12:29",
        "Description": "The Decision/Random forest code is a classifier and should be located under the classifier package.",
        "Issue Links": []
    },
    "MAHOUT-860": {
        "Key": "MAHOUT-860",
        "Summary": "Create minimalist maven module for *Writable classes for export",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jake Mannix",
        "Created": "02/Nov/11 02:19",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 09:29",
        "Description": "Other projects (and developers) who want to interact with Mahout file-formats currently need to depend on mahout-core, which is big.  It would be nice if we had a slim and trim mahout dependency which just has code necessary to work with our Writables.\nIn particular, this would allow other projects which Mahout would possibly like to depend on (like Giraph) to depend on just a part of Mahout (ie. file format stuff) without introducing nasty circular dependencies.",
        "Issue Links": []
    },
    "MAHOUT-861": {
        "Key": "MAHOUT-861",
        "Summary": "Simultaneously update left and right vectors while training ExpectationMaximizationSVDFactorizer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Gokhan Capan",
        "Created": "02/Nov/11 09:15",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "02/Nov/11 09:32",
        "Description": "I guess the updates on left and right vectors in ExpectationMaximizationSVDFactorizer#train(int i, int j, int f, SVDPreference pref) should be done simultaneously according to the original algorithm (http://sifter.org/~simon/journal/20061211.html)",
        "Issue Links": []
    },
    "MAHOUT-862": {
        "Key": "MAHOUT-862",
        "Summary": "MurmurHash 3.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "02/Nov/11 13:03",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "06/Dec/11 02:03",
        "Description": "Yonik has ported an implementation of MurmurHash 3.0 and put it in the public domain: http://www.lucidimagination.com/blog/2011/09/15/murmurhash3-for-java/\nIt's a port of https://sites.google.com/site/murmurhash/ which says: \n\n(I reserve the right to tweak the constants after people have had a chance to bang on it). Murmur3 has better performance than MurmurHash2, no repetition flaw, comes in 32/64/128-bit versions for both x86 and x64 platforms, and the 128-bit x64 version is blazing fast - over 5 gigabytes per second on my 3 gigahertz Core 2.\nIn addition, the library of test code that I use to test MurmurHash (called SMHasher) has been released - it's still rough (and will only compile under VC++ at the moment), but it contains everything needed to verify hash functions of arbitrary output bit-lengths.\nMurmur3 and all future versions will be hosted on Google Code here - http://code.google.com/p/smhasher/ - you can access the codebase via the 'Source' tab at the top.\nSee also http://code.google.com/p/smhasher/\nWe should add support for it and hook into MinHash",
        "Issue Links": []
    },
    "MAHOUT-863": {
        "Key": "MAHOUT-863",
        "Summary": "Add DisplayMinhash clustering example",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "02/Nov/11 15:20",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "13/Jan/12 13:53",
        "Description": "We've got simple GUI tools for many of the clustering algorithms, we should add one for Minhash, too",
        "Issue Links": []
    },
    "MAHOUT-864": {
        "Key": "MAHOUT-864",
        "Summary": "DisplayCanopy doesn't show any clusters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering,                                            Examples",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "02/Nov/11 15:22",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "02/Nov/11 15:30",
        "Description": "The DisplayCanopy program doesn't show any actual clusters, just the original points",
        "Issue Links": []
    },
    "MAHOUT-865": {
        "Key": "MAHOUT-865",
        "Summary": "Refactor Sequential Clustering algorithms",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "02/Nov/11 18:20",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 16:16",
        "Description": "We have a lot of implementations of sequential clustering algorithms that are kind of treated as an afterthought by sticking them into the *Driver classes.  We should pull them out into their own classes with real APIs so that people can use them.",
        "Issue Links": []
    },
    "MAHOUT-866": {
        "Key": "MAHOUT-866",
        "Summary": "Move Precondition checks out of Mahalanobis.distance method and into configuration/setup",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "03/Nov/11 00:15",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "03/Nov/11 00:22",
        "Description": "The Mahalanobis distance currently checks certain preconditions on member variables for every call to distance().  These should be done as part of setup, not as part of the distance call.",
        "Issue Links": []
    },
    "MAHOUT-867": {
        "Key": "MAHOUT-867",
        "Summary": "Add ClusterEvaluator capabilities to ClusterDumper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "03/Nov/11 03:24",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "03/Nov/11 04:02",
        "Description": "It would be nice if the ClusterDumper spit out some of our cluster evaluation metrics.",
        "Issue Links": []
    },
    "MAHOUT-868": {
        "Key": "MAHOUT-868",
        "Summary": "Rename build*.sh examples to be more indicative of what they actually do, i.e. classify-20newsgroups.sh",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "03/Nov/11 20:55",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "29/Nov/11 13:19",
        "Description": "The build*.sh scripts in examples/bin are a bit weird naming wise.  We should deprecate them and rename them to something more explanatory.\nSince we have a fair amount of stuff pointing to the old ones, however, I would suggest we keep them around for a release or two and have them simply call the new one, along with printing out a short message saying they should use the other one next time.",
        "Issue Links": []
    },
    "MAHOUT-869": {
        "Key": "MAHOUT-869",
        "Summary": "driver.classes.props is getting unwieldy",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "03/Nov/11 20:57",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "27/Nov/11 12:44",
        "Description": "We should clean up and organize driver.classes.props.  At least group things like clustering and classification together.  But also try to standardize some of the short names (trainclassifier is really train NB classifier, which also conflicts with trainnb)\nWould also be nice if bin/mahout could spit things out in alphabetical order.",
        "Issue Links": []
    },
    "MAHOUT-870": {
        "Key": "MAHOUT-870",
        "Summary": "Driver or Job?  Let's pick one and be consistent.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Grant Ingersoll",
        "Created": "04/Nov/11 00:57",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jun/12 09:17",
        "Description": "Some things are Drivers, some are Jobs, but they all do the same thing.  Since we often use these as the main interface into our stuff, let's make it easier for people to find things by being consistent w/ our naming.\nMy vote is Driver.",
        "Issue Links": []
    },
    "MAHOUT-871": {
        "Key": "MAHOUT-871",
        "Summary": "LDA job \"mahout lda\" fails- attempts to read _SUCCESS file in Hadoop output",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Lance Norskog",
        "Created": "04/Nov/11 01:12",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "28/Nov/11 12:23",
        "Description": "The bin/mahout \"lda\" job throw an exception. It seems to be reading the _SUCCESS file in from the seq2sparse output, but of course _SUCCESS files are empty.\n------------------------------\n11/11/03 15:09:01 INFO common.HadoopUtil: Deleting /tmp/mahout-work-lancenorskog/reuters-out-seqdir-sparse-lda/partial-vectors-0\n11/11/03 15:09:01 INFO driver.MahoutDriver: Program took 60008 ms (Minutes: 1.0001333333333333)\n+ ../../bin/mahout lda -i /tmp/mahout-work-lancenorskog/reuters-out-seqdir-sparse-lda/tf-vectors -o /tmp/mahout-work-lancenorskog/reuters-lda -k 20 -ow -x 20\nMAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\nno HADOOP_HOME set, running locally\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/Users/lancenorskog/svn/training/lucid/mahout/labs/tools/mahout/examples/target/mahout-examples-0.6-SNAPSHOT-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/lancenorskog/svn/training/lucid/mahout/labs/tools/mahout/examples/target/dependency/slf4j-jcl-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/lancenorskog/svn/training/lucid/mahout/labs/tools/mahout/examples/target/dependency/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n11/11/03 15:09:04 INFO common.AbstractJob: Command line arguments: {--endPhase=2147483647, --input=/tmp/mahout-work-lancenorskog/reuters-out-seqdir-sparse-lda/tf-vectors, --maxIter=20, --numTopics=20, --output=/tmp/mahout-work-lancenorskog/reuters-lda, --overwrite=null, --startPhase=0, --tempDir=temp, --topicSmoothing=-1.0}\nException in thread \"main\" java.lang.IllegalStateException: file:/tmp/mahout-work-lancenorskog/reuters-out-seqdir-sparse-lda/tf-vectors/_SUCCESS\n       at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator$1.apply(SequenceFileDirValueIterator.java:82)\n       at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator$1.apply(SequenceFileDirValueIterator.java:73)\n       at com.google.common.collect.Iterators$8.next(Iterators.java:765)\n       at com.google.common.collect.Iterators$5.hasNext(Iterators.java:526)\n       at com.google.common.collect.ForwardingIterator.hasNext(ForwardingIterator.java:43)\n       at org.apache.mahout.clustering.lda.LDADriver.determineNumberOfWordsFromFirstVector(LDADriver.java:204)\n       at org.apache.mahout.clustering.lda.LDADriver.run(LDADriver.java:164)\n       at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n       at org.apache.mahout.clustering.lda.LDADriver.main(LDADriver.java:90)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n       at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n       at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\nCaused by: java.io.EOFException\n       at java.io.DataInputStream.readFully(DataInputStream.java:180)\n       at java.io.DataInputStream.readFully(DataInputStream.java:152)\n       at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1450)\n       at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1428)\n       at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1417)\n       at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1412)\n       at org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator.<init>(SequenceFileValueIterator.java:51)\n       at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator$1.apply(SequenceFileDirValueIterator.java:77)\n       ... 15 more",
        "Issue Links": []
    },
    "MAHOUT-872": {
        "Key": "MAHOUT-872",
        "Summary": "Revisit the parallel ALS matrix factorization",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "04/Nov/11 08:00",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Nov/11 08:55",
        "Description": "Our current code for computing a decomposition of a rating matrix with Alternating Least Squares (ALS) uses a lot of highly unefficient reduce side joins. \nThe rating matrix A is decomposed into a matrix U of users x features and a matrix M of items x features. Each of these matrices is iteratively recomputed until a maximum number of iterations is reached\nIf we assume that U and M fit into the memory of a single mapper instance, each iteration can be implemented as single map-only job, which greatly improves the runtime of this job.\nNote that in spite of these improvements this job is still rather slow as Hadoop is a poor fit for iterative algorithms. Each iteration has to be scheduled again and data is always read from and written to disk.",
        "Issue Links": []
    },
    "MAHOUT-873": {
        "Key": "MAHOUT-873",
        "Summary": "Provide MapReduce job for creating Encoded Vectors from sequence files",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "04/Nov/11 17:13",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "20/Nov/11 12:47",
        "Description": "Similar to SparseVectorsFromSequenceFiles, provide a version that can do encoded vectors.  Start simple by handling basic text, but this could easily evolve to handle pluggable Vectorizer's that can better deal with features (numerics, etc.).",
        "Issue Links": [
            "/jira/browse/MAHOUT-851"
        ]
    },
    "MAHOUT-874": {
        "Key": "MAHOUT-874",
        "Summary": "Extract Writables into a separate module to allow smaller dependencies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "05/Nov/11 00:05",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "13/Apr/14 14:37",
        "Description": "The theory is that we can have a smaller jar if we only include writable classes and their exact dependencies.\nI have a prototype, but it has some funky characteristics which I would like to discuss.",
        "Issue Links": []
    },
    "MAHOUT-875": {
        "Key": "MAHOUT-875",
        "Summary": "Allow to obtain Mahout version information through the Java API",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Oliver B. Fischer",
        "Created": "06/Nov/11 23:55",
        "Updated": "31/Dec/11 18:34",
        "Resolved": "30/Dec/11 18:20",
        "Description": "It would be nice to be able to obtain the used Mahout version through the Java API. This enables users to output the version information for debugging purposes or simple to document which Mahout version is used.",
        "Issue Links": [
            "/jira/browse/MAHOUT-670",
            "/jira/browse/MAHOUT-892"
        ]
    },
    "MAHOUT-876": {
        "Key": "MAHOUT-876",
        "Summary": "Add build information to README",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "07/Nov/11 22:07",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "18/Nov/11 19:08",
        "Description": "When comparing our README.txt to the one Lucene provides it seems the Lucene one has some essential build information that new Mahout contributors might find useful as well. Added the information (see attached patch). Will commit in 72h if no-one objects.",
        "Issue Links": []
    },
    "MAHOUT-877": {
        "Key": "MAHOUT-877",
        "Summary": "Enable the parallel ALS recommender to use implicit feedback data",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "08/Nov/11 08:17",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "08/Nov/11 10:13",
        "Description": "Mahout's ParallelALSFactorizationJob offers a distributed matrix factorization for computing recommendations. The current implementation is only suited for explicit feedback data (ratings) unfortunately. \nThe majority of usecases has to work with implicit feedback however. The paper \"Collaborative Filtering for Implicit Feedback Datasets\" http://research.yahoo.com/pub/2433 describes a closely related approach that is aimed at implicit feedback data and should easily be integratable into the current ParallelALSJob.",
        "Issue Links": []
    },
    "MAHOUT-878": {
        "Key": "MAHOUT-878",
        "Summary": "Provide better examples for the parallel ALS recommender code",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "08/Nov/11 10:18",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "10/Nov/11 15:09",
        "Description": "We should provide examples that show how to apply the parallel ALS recommender to the Netflix or KDD2011 datasets.",
        "Issue Links": []
    },
    "MAHOUT-879": {
        "Key": "MAHOUT-879",
        "Summary": "Remove all graph algorithms with the exception of PageRank",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Nov/11 11:13",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "13/Nov/11 17:04",
        "Description": "As discussed on the mailinglist we will remove the graph algorithms from Mahout (except PageRank). It has become pretty clear that the Map/Reduce is not suitable for most \"classic\" graph algorithms.\nI will attach a file containing the code of all algorithms that will be removed so that they don't get lost.",
        "Issue Links": []
    },
    "MAHOUT-880": {
        "Key": "MAHOUT-880",
        "Summary": "Add some matrix method(like addition, subtraction, norm ... etc) to DistributedRowMatrix",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Wangda Tan (No longer used)",
        "Created": "11/Nov/11 08:40",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "08/Mar/14 12:05",
        "Description": "I'm a new to Mahout, I didn't find some basic matrix functions. This make users cannot do many tasks by CLI or API, if user get some result through existing map-reduce matrix operation (like svd), he cannot do farther steps. I make a list for it:\n1) Addition, Subtraction \n2) Norm (like norm-1, norm-2, norm-frobenius)\n3) Matrix compare\n4) Get lower triangle, upper triangle and diagonal\n5) Get identity and zero matrix\n6) Put two or matrix to together: A = [A1, A2]\n7) More linear equations solver method, like Gaussian elimination (maybe it's hard to implement)\n8) import and export CSV, ARFF ... (this will very useful when user want to reuse result from or to other applications like MATLAB)\nI want to know is there any plan to do this, if so, I can make some efforts to implement these.",
        "Issue Links": []
    },
    "MAHOUT-881": {
        "Key": "MAHOUT-881",
        "Summary": "Refactor TopItems to use Lucene's PriorityQueue and remove excessive sorting",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "12/Nov/11 08:10",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "20/Nov/11 13:25",
        "Description": "TopItems.getTop*() all do a fair number of excessive operations that can be replaced by switching to using Lucene's PriorityQueue implementation, which is more efficient and faster than Java's built in PQ implementation.",
        "Issue Links": []
    },
    "MAHOUT-882": {
        "Key": "MAHOUT-882",
        "Summary": "TopItems.getTopUsers ignores rescoring",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "12/Nov/11 08:14",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "12/Nov/11 11:42",
        "Description": "I believe TopUsers.getTopUsers() is not properly handling the rescoredSimilarity:\n\nTopItems.getTopUsers:\ndouble rescoredSimilarity = rescorer == null ? similarity : rescorer.rescore(userID, similarity);\n      if (!Double.isNaN(rescoredSimilarity) && (!full || rescoredSimilarity > lowestTopValue)) {\n        topUsers.add(new SimilarUser(userID, similarity));\n\n\nIt is checking rescoredSimilarity for everything, but then not passing it in like getTopItems does.",
        "Issue Links": []
    },
    "MAHOUT-883": {
        "Key": "MAHOUT-883",
        "Summary": "Add an example that computes PageRank on the wikipedia page link graph",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "13/Nov/11 17:11",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "22/Jun/12 05:52",
        "Description": "Use the link graph of the english wikipedia (available from http://users.on.net/~henry/home/wikipedia.htm) to demonstrate Mahout's pagerank implementation.",
        "Issue Links": []
    },
    "MAHOUT-884": {
        "Key": "MAHOUT-884",
        "Summary": "Matrix Concatenate utility",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Lance Norskog",
        "Created": "14/Nov/11 06:53",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "09/Jun/13 22:32",
        "Description": "Utility to concatenate matrices stored as SequenceFiles of vectors.\nEach pair in the SequenceFile is the IntWritable row number and a VectorWritable.\nThe input and output files may skip rows.",
        "Issue Links": []
    },
    "MAHOUT-885": {
        "Key": "MAHOUT-885",
        "Summary": "Freq pattern growth advertises wrong value for default",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tom Pierce",
        "Created": "14/Nov/11 18:23",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "14/Nov/11 21:27",
        "Description": "FPG advertises that numgroups will default to 1000, but it actually uses 50.  If you do not override the default, only 50 reducers get work even if you have many more.",
        "Issue Links": []
    },
    "MAHOUT-886": {
        "Key": "MAHOUT-886",
        "Summary": "FPtree nodes multiply-added (becoming siblings in tree)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tom Pierce",
        "Created": "14/Nov/11 19:39",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "15/Nov/11 08:49",
        "Description": "In FPGrowth#traverseAndBuildConditionalFPTreeData, while creating a conditional FPtree sometimes nodes are multiply-added as children of the same node, becoming siblings in the conditional tree.",
        "Issue Links": []
    },
    "MAHOUT-887": {
        "Key": "MAHOUT-887",
        "Summary": "Bottom Up Clustering",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Paritosh Ranjan",
        "Created": "15/Nov/11 04:40",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "15/Nov/11 18:54",
        "Description": "Bottom up clustering is achieved by starting with small clusters/single points and then merging clusters recursively which are closer than a specified control constraint.",
        "Issue Links": []
    },
    "MAHOUT-888": {
        "Key": "MAHOUT-888",
        "Summary": "Start using newly acquired reviewboard instance!",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "17/Nov/11 07:18",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "26/Nov/11 16:55",
        "Description": "It's easy: go to https://reviews.apache.org/ and submit a review by clicking \"new review request\", then picking the project to be \"mahout\", base directory to be \"trunk\", and upload your patch.  Add \"mahout\" to the reviewer group of your reviewboard, pick some people you want to aim the review at (if you think you know the \"owner\" of the code area you're hitting), set the \"bug number\" in the appropriate field so that JIRA and RB will magically communicate, and don't forget to click the \"publish\" button when you're ready for people to look at it.  You can also look at your review directly for sanity checking before doing so by clicking on the \"view diff\" button.\nReviewBoard can be slow and clunky, but inline comments in the diff are A+ goodness.",
        "Issue Links": []
    },
    "MAHOUT-889": {
        "Key": "MAHOUT-889",
        "Summary": "size() returns wrong value (10) on freshly instantiated ObjectArrayList.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "collections-1.0",
        "Fix Version/s": "0.8",
        "Component/s": "collections",
        "Assignee": "Benson Margulies",
        "Reporter": "Claudio Martella",
        "Created": "17/Nov/11 14:32",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 10:43",
        "Description": "ObjectArrayList returns 10 for a freshly instantiated object:\n\nObjectArrayList<String> list = new ObjectArrayList<String>();\nSystem.out.println(list.size());\n\n\nprints 10.",
        "Issue Links": []
    },
    "MAHOUT-890": {
        "Key": "MAHOUT-890",
        "Summary": "Performance issue in FPGrowth",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Tom Pierce",
        "Created": "20/Nov/11 03:39",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "17/Jan/12 05:20",
        "Description": "I've encountered a dataset which indicates there is probably a performance bug lurking in the FPGrowth implementation.  This set may be a bit of an unusual target for FPG - there's a relatively modest number itemsets, and many items with a Zipfy distribution.  I am attaching a patch (addSynth.patch) to add a similar dataset as core/src/test/resources/FPGsynth.dat.\nFPGsynth.dat can take minutes or a few hours to process, depending on how it is grouped out to machines.  If run in sequential mode, or with \"-g 50\" it will take considerable time.  Most reducers/\"anchor items\" are processed quickly, but a small number take a handful of minutes, and one or two take a long time.  If you experiment with this data, I suggest using  '-s 50 -regex \"[ ]+\"'. \nDigging into this, I've found that the tree pruning code sometimes creates surprising trees.  One oddity I've observed is 0-count nodes, sometimes with non-zero children.  The other is that sometimes subtrees seem to get repeated.  I'm attaching a sample input file (smallexample.dat, use the whitespace regex with this one, too) and a patch which adds some logging in pruneFPTree and growthBottomUp which will print out some interesting trees when run with the smallexample.dat input.",
        "Issue Links": [
            "/jira/browse/MAHOUT-920"
        ]
    },
    "MAHOUT-891": {
        "Key": "MAHOUT-891",
        "Summary": "LoadEvaluationRunner and Recommender stats",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Grant Ingersoll",
        "Created": "20/Nov/11 13:27",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "22/Nov/11 06:48",
        "Description": "Per MAHOUT-881, it would be nice to implement a easy to use test driver (CFLoadEvaluationRunner) that can take in a data model for CF and run the LoadEvaluator multiple times and bring back stats about how long it took to run, etc.  This likely means being able to return the StatsCallable results out to the runner, but doesn't have to.",
        "Issue Links": []
    },
    "MAHOUT-892": {
        "Key": "MAHOUT-892",
        "Summary": "Allow to obtain used Mahout version via ./bin/mahout",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Oliver B. Fischer",
        "Created": "22/Nov/11 12:32",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "04/Jan/12 16:19",
        "Description": "Allow the user to obtain the used Mahout version via  the ./bin/mahout script. Please add for this an --version|-v command line option. The version information should be identical to the version information which will be available via the Java API. For details on obtaining version information via the Java API see MAHOUT-875.",
        "Issue Links": [
            "/jira/browse/MAHOUT-875"
        ]
    },
    "MAHOUT-893": {
        "Key": "MAHOUT-893",
        "Summary": "Dependency Clash : Google Collections and Guava",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "collections",
        "Assignee": "Sean R. Owen",
        "Reporter": "Paritosh Ranjan",
        "Created": "23/Nov/11 16:04",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "23/Nov/11 17:18",
        "Description": "I tried to run the Recommender example in Mahout in Action Book.\n        DataModel model = new FileDataModel(new File(\"intro.csv\"));\n        UserSimilarity similarity = new PearsonCorrelationSimilarity(model);\nThe dependencies from google collections and guava are clashing while execution. Its picking some wrong class like Preconditions and few others which is giving error on runtime. I fixed that by excluding google-collections. The patch is attached which helped me fix this problem.",
        "Issue Links": []
    },
    "MAHOUT-894": {
        "Key": "MAHOUT-894",
        "Summary": "NB testclassifier runs in sequential mode by default",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tom Pierce",
        "Created": "24/Nov/11 01:55",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "28/Nov/11 10:46",
        "Description": "NB classifiers can only be trained in MR mode, but evaluation happens in sequential mode by default.  I think this violates the principle of least surprise - anyone trying this out is likely to expect the opposite.\nI'm attaching a patch to flip the default.",
        "Issue Links": []
    },
    "MAHOUT-895": {
        "Key": "MAHOUT-895",
        "Summary": "Make Wikipedia example set maker easier to mod",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Classification,                                            Examples",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tom Pierce",
        "Created": "24/Nov/11 01:58",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "28/Nov/11 10:45",
        "Description": "The WikipediaDatasetCreator uses 2 mechanisms to scrape out the text of articles; first an XmlInputFormat is used with the \"text\" tags as start/end markers (which demarcate the article content), then the content inside the text tags is pattern matched out in the Mapper.\nThis means a newcomer must discover both pruning steps before modifying this program to create a dataset including other fields from the article.\nI am attaching a patch which mods the Driver to split on entire articles and changes the mapper to accommodate the extra input without allowing spurious new category matches outside the text element.",
        "Issue Links": []
    },
    "MAHOUT-896": {
        "Key": "MAHOUT-896",
        "Summary": "Improve readability of AbstractDifferenceRecommenderEvaluator class",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Anatoliy Kats",
        "Created": "25/Nov/11 13:47",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "25/Nov/11 16:37",
        "Description": "Change the variable and private method names so that they are internally consistent, and their purpose is more clear.",
        "Issue Links": []
    },
    "MAHOUT-897": {
        "Key": "MAHOUT-897",
        "Summary": "New implementation for LDA: Collapsed Variational Bayes (0th derivative approximation), with map-side model caching",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "27/Nov/11 07:34",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "09/Jan/12 17:51",
        "Description": "Current LDA implementation in Mahout suffers from a few issues:\n  1) it's based on the original Variational Bayes E/M training methods of Blei et al (http://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf), which are a) significantly more complex to implement/maintain, and b) significantly slower than subsequently discovered techniques\n  2) the entire \"current working model\" is held in memory in each Mapper, which limits the scalability of the implementation by numTerms in vocabulary * numTopics * 8bytes per double being less than the mapper heap size.\n  3) the sufficient statistics which need to be emitted by the mappers scale as numTopics * numNonZeroEntries in the corpus.  Even with judicious use of Combiners (currently implemented), this can get prohibitively expensive in terms of network + disk usage.\nIn particular, point 3 looks like: a 1B nonzero entry corpus in Mahout would take up about 12GB of RAM in total, but if you wanted 200 topics, you'd be using 2.5TB if disk+network traffic per E/M iteration.  Running a moderate 40 iterations we're talking about 100TB.  Having tried this implementation on a 6B nonzero entry input corpus with 100 topics (500k term vocabulary, so memory wasn't an issue), I've seen this in practice: even with our production Hadoop cluster with many thousands of map slots available, even one iteration was taking more than 3.5hours to get to 50% completion of the mapper tasks.\nPoint 1) was simple to improve: switch from VB to an algorithm labeled CVB0 (\"Collapsed Variational Bayes, 0th derivative approximation\") in Ascuncion, et al ( http://www.datalab.uci.edu/papers/uai_2009.pdf ).  I tried many approaches to get the overall distributed side of the algorithm to scale better, originally aiming at removing point 2), but it turned out that point 3) was what kept rearing its ugly head.  The way that YahooLDA ( https://github.com/shravanmn/Yahoo_LDA ) and many others have achieved high scalability is by doing distributed Gibbs sampling, but that requires that you hold onto the model in distributed memory and query it continually via RPC.  This could be done in something like Giraph or Spark, but not in vanilla Hadoop M/R.\nThe end result was to actually make point 2) even worse, and instead of relying on Hadoop combiners to aggregate sufficient statistics for the model, you instead do a full map-side cache of (this mapper's slice of) the next iteration's model, and emit nothing in each map() call, emitting the entire model at cleanup(), and then the reducer simply sums the sub-models.  This effectively becomes a form of ensemble learning: each mapper learns its own sequential model, emits it, the reducers (one for each topic) sum up these models into one, which is fed out to all the models in the next iteration.\nIn its current form, this LDA implementation can churn through about two M/R iterations per hour on the same cluster/data set mentioned above (which makes it at least 15x faster on larger data sets).\nIt probably requires a fair amount of documentation / cleanup, but it comes with a nice end-to-end unit test (same as the one added to MAHOUT-399), and also comes with an \"in-memory\" version of the same algorithm, for smaller datasets (i.e. those which can fit in memory).",
        "Issue Links": []
    },
    "MAHOUT-898": {
        "Key": "MAHOUT-898",
        "Summary": "Error in formula for preference estimation in GenericItemBasedRecommender",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Paulo Villegas",
        "Created": "27/Nov/11 21:11",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "14/May/12 16:34",
        "Description": "The formula to estimate the preference for an item in the Taste item-based recommender normalizes by the sum of similarities for items used in estimation. But the terms in the sum taken to normalize should be in absolute value, since they can be negative (e.g. when using Pearson correlation, similarity is in [-1,1]). Now they are not, and as a result when there are negative and positive values they cancel out, giving a small denominator and incorrectly boosting the preference for the item (symptom: it is easy for a predicted preference to take the maximum value, since the quotient becomes large and it is capped afterwards)\nThe patch is rather trivial (a one-liner, actually) for src/main/java/org/apache/mahout/cf/taste/impl/recommender/GenericItemBasedRecommender.java\nNote: the same error & suggested fix happens in GenericUserBasedRecommender",
        "Issue Links": []
    },
    "MAHOUT-899": {
        "Key": "MAHOUT-899",
        "Summary": "Add Point Sampling, Color coding to ClusterDumper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering,                                            Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "27/Nov/11 21:59",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "08/Jan/12 20:39",
        "Description": "When running the cluster dumper, or outputting values to a file for display purposes, it is useful to not have to deal with all the points per cluster.  This issue will add the ability to specify a maximum number of points to output per cluster in the cluster dumper.",
        "Issue Links": []
    },
    "MAHOUT-900": {
        "Key": "MAHOUT-900",
        "Summary": "RandomSeedGenerator samples / output k texts incorrectly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "28/Nov/11 10:40",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "28/Nov/11 18:54",
        "Description": "int currentSize = chosenTexts.size();\n          if (currentSize < k) {\n            chosenTexts.add(newText);\n            chosenClusters.add(newCluster);\n          } else if (random.nextInt(currentSize + 1) == 0) { // with chance 1/(currentSize+1) pick new element\n            int indexToRemove = random.nextInt(currentSize); // evict one chosen randomly\n            chosenTexts.remove(indexToRemove);\n            chosenClusters.remove(indexToRemove);\n            chosenTexts.add(newText);\n            chosenClusters.add(newCluster);\n          }\n\n\nThe second \"if\" condition ought to be \"!= 0\", right? Only if it is 0 do we skip the body, which removes an existing element, since the new element itself is evicted.\nSecond, this code:\n\n        for (int i = 0; i < k; i++) {\n          writer.append(chosenTexts.get(i), chosenClusters.get(i));\n        }\n\n\n... assumes that at least k elements existed in the input, and fails otherwise. Probably need to cap this.\nPatch attached.",
        "Issue Links": []
    },
    "MAHOUT-901": {
        "Key": "MAHOUT-901",
        "Summary": "KnnItemBasedRecommender is not working properly",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Georgi Stanev",
        "Created": "28/Nov/11 11:00",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "29/Nov/11 10:56",
        "Description": "For more information see the mailing list archive (KnnItemBasedRecommender question)",
        "Issue Links": []
    },
    "MAHOUT-902": {
        "Key": "MAHOUT-902",
        "Summary": "TanimotoCoefficientSimilarity should return Double.NaN for two items that have zero overlap",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "29/Nov/11 13:12",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "05/Dec/11 17:54",
        "Description": "org.apache.mahout.cf.taste.impl.similarity.TanimotoCoefficientSimilarity should return Double.NaN for two items that have zero overlap. Please also provide a unit-test for this.",
        "Issue Links": []
    },
    "MAHOUT-903": {
        "Key": "MAHOUT-903",
        "Summary": "Slope one doesn't write, read diff counts resulting in no recs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "29/Nov/11 20:04",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "02/Dec/11 00:12",
        "Description": "An astute MiA reader points out that the FileDiffStorage class does not have a way to read diff counts, or the number of item-item pairs that resulted in the average diff. It is assumed to be 1, which causes it to be pruned quickly. The SlopeOneAverageDiffsJob needs to likewise output counts.",
        "Issue Links": []
    },
    "MAHOUT-904": {
        "Key": "MAHOUT-904",
        "Summary": "SplitInput should support randomizing the input",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Raphael Cendrillon",
        "Reporter": "Grant Ingersoll",
        "Created": "30/Nov/11 17:04",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "02/Jan/12 19:12",
        "Description": "For some learning tasks, we need the input to be randomized (SGD) instead of blocks of labels all at once.  SplitInput is a useful tool for setting up train/test files but it currently doesn't support randomizing the input.",
        "Issue Links": []
    },
    "MAHOUT-905": {
        "Key": "MAHOUT-905",
        "Summary": "CachingUserSimilarity and CachingItemSimilarity have wrong (far to small) default maxSizes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Manuel Blechschmidt",
        "Created": "30/Nov/11 22:12",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "30/Nov/11 22:19",
        "Description": "I am currently tuning my recommender discussed here: http://thread.gmane.org/gmane.comp.apache.mahout.user/10433.\nAs a first step I wrapped my LogLikelihoodSimilarity with an CachingUserSimilarity. I used Java Visual VM to profile the calls. I recognized that I didn't get any performance benefits. So I had a look into the code.\nActually line 47 this(similarity, dataModel.getNumItems()); in CachingUserSimilarity.java is wrong. If we want to cache all item similarities we need a cache with (dataModel.getNumItems()*(dataModel.getNumItems()-1))/2 possible entries.\nI am now doing this in the constructor. I attached a patch to adjust this in the trunk build.",
        "Issue Links": []
    },
    "MAHOUT-906": {
        "Key": "MAHOUT-906",
        "Summary": "Allow collaborative filtering evaluators to use custom logic in splitting data set",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Anatoliy Kats",
        "Created": "02/Dec/11 11:27",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "29/Dec/11 20:56",
        "Description": "I want to start a discussion about factoring out the logic used in splitting the data set into training and testing.  Here is how things stand:  There are two independent evaluator based classes:  AbstractDifferenceRecommenderEvaluator, splits all the preferences randomly into a training and testing set.  GenericRecommenderIRStatsEvaluator takes one user at a time, removes their top AT preferences, and counts how many of them the system recommends back.\nI have two use cases that both deal with temporal dynamics.  In one case, there may be expired items that can be used for building a training model, but not a test model.  In the other, I may want to simulate the behavior of a real system by building a preference matrix on days 1-k, and testing on the ratings the user generated on the day k+1.  In this case, it's not items, but preferences(user, item, rating triplets) which may belong only to the training set.  Before we discuss appropriate design, are there any other use cases we need to keep in mind?",
        "Issue Links": []
    },
    "MAHOUT-907": {
        "Key": "MAHOUT-907",
        "Summary": "Several Watchmaker Examples tests fail when there is a space in the path",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "02/Dec/11 13:08",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "02/Dec/11 13:42",
        "Description": "Three of the watchmaker tests in examples fail when there is a space in the path.",
        "Issue Links": []
    },
    "MAHOUT-908": {
        "Key": "MAHOUT-908",
        "Summary": "Example shell scripts don't run properly on Ubuntu",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "02/Dec/11 13:48",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "02/Dec/11 13:52",
        "Description": "There is some funkiness on Ubuntu when running example/bin/ shell scripts such as classify-newsgroups.sh.  It has to do w/ the use of /bin/sh at the top you get odd syntax errors:\n\n./classify-20newsgroups.sh: 35: Syntax error: \"(\" unexpected (expecting \"fi\")\nIf you switch it to /bin/bash or invoke it as \"bash classify-newsgroups.sh\" then it all works.\nI assume we can make the assumption that bash exists everywhere this is going to run?",
        "Issue Links": []
    },
    "MAHOUT-909": {
        "Key": "MAHOUT-909",
        "Summary": "Make it so you can pass in all the answers to the questions asked in the example shell scripts",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Raphael Cendrillon",
        "Reporter": "Grant Ingersoll",
        "Created": "02/Dec/11 14:00",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "06/Dec/11 15:07",
        "Description": "For shell scripts like asf-email-examples, make it so that one can pass in the answers on the command line.  This will facilitate automatically testing these.",
        "Issue Links": []
    },
    "MAHOUT-910": {
        "Key": "MAHOUT-910",
        "Summary": "Improve sampling in SamplingCandidateItemStrategy, optimize intersection computations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "02/Dec/11 16:46",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "07/Dec/11 10:24",
        "Description": "Per the lengthy discussion on the mailing list about optimizing SamplingCandidateItemStrategy and related code, I'm opening this placeholder issue.",
        "Issue Links": []
    },
    "MAHOUT-911": {
        "Key": "MAHOUT-911",
        "Summary": "Naive Bayes trains models that are too large to apply",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Tom Pierce",
        "Created": "03/Dec/11 01:41",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "04/Jun/12 14:12",
        "Description": "I'm seeing the same issue that Lyall Morrison mentioned on the user list not too long ago; I can train a model that apparently has too many classes (or is otherwise too large) to read back in and apply to new documents.  \nI was able to duplicate this issue using the Wikipedia classification example.  I used an expanded set of categories (125, which is well over the 30-some that caused trouble for Lyall).\nI'll attach the list of categories I used.",
        "Issue Links": []
    },
    "MAHOUT-912": {
        "Key": "MAHOUT-912",
        "Summary": "InMemoryCollapsedVariationalBayes0 should ignore _SUCCESS files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "03/Dec/11 09:43",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "04/Dec/11 07:36",
        "Description": "InMemoryCollapsedVariationalBayes0 should ignore _SUCCESS files when reading in the tf-vectors",
        "Issue Links": []
    },
    "MAHOUT-913": {
        "Key": "MAHOUT-913",
        "Summary": "Style changes / discussion",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "03/Dec/11 21:14",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "05/Dec/11 12:33",
        "Description": "Guys I've still been seeing code committed that doesn't match standard Java style or a reasonable policy I can imagine. I wanted to talk about it since I've just been silently changing it and that is not ideal.\nThis should be easy to get right, as automated tools exist to check and fix this. I recommend IntelliJ's free Community edition. Flip on even basic inspections. A hundred things will jump out (that are already jumping out at me). Most are automatically fixable. \nI think that standardized, readable code invites attention, work and care: it feels like something you want to improve, and don't want to hack up.\nI think it helps attract committers. Strong engineering organizations wouldn't let basic style problems in the codebase, just by using automated checks. Code reviews don't begin otherwise, and then reviews focus on real issues like design. We can make a basic effort to approach that level of quality. Otherwise, people who are used to a higher standard won't be inclined to participate in the project, and will just fork.\nI think it's a prerequisite to fixing real design issues, TODOs, correctness problems (cloning for instance), and refactorings. This code is not near that point, and won't get there at this rate. \nPersonally it makes we want to only support anything I've written, and write any \"next generation\" recommender system in a new and separate venture. And I'm a friendly, and maybe not the only one! So would be great to keep some focus on quality and design.\nHere's a patch showing all the changes I've picked up and made with the IDE \u2013 just basic style issues, and just since the last 2 weeks. The issues are, among others:\n\t\u2043\tEmpty javadoc\n\t\u2043\tRedundant javadoc (\"@param foo the foo\")\n\t\u2043\tMissing copyright headers\n\t\u2043\tCopyright headers not at top of file (sometimes after imports!)\n\t\u2043\tVery long lines (>> 120 chars)\n\t\u2043\t\"throws Exception\" not on main() or test method\n\t\u2043\t\"transient\" fields \u2013 should never be used for us\n\t\u2043\tMissing @Override\n\t\u2043\tUsing new Random()\n\t\u2043\tRedundant boolean expressions like \"foo == true\"\n\t\u2043\tUnused variables and parameters\n\t\u2043\tUnused imports\n\t\u2043\tLoops and conditionals without braces\n\t\u2043\tWeird literals (\"1d\")",
        "Issue Links": []
    },
    "MAHOUT-914": {
        "Key": "MAHOUT-914",
        "Summary": "Provide a non-distributed counterpart of the sampling which is applied in the distributed item similarity computation",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "04/Dec/11 08:56",
        "Updated": "09/Feb/12 14:03",
        "Resolved": "07/Dec/11 12:55",
        "Description": "The distributed item similarity computation applies a so-called 'interaction-cut': it selectively down samples 'power users' in org.apache.mahout.cf.taste.hadoop.preparation.ToItemVectorsMapper. This is done because the users with the most interactions usually dominate the runtime without providing much benefit to the quality, as users with an enormous amount of interactions are very often crawlers or people sharing an account.\nMahout should have an exact counterpart of this strategy for the non-distributed code.\nI also attach a figure that shows experiments with this strategy for the movielens 1M dataset. The dataset was split into 90% training and 10% test set. An interaction cut of size k was applied and the prediction quality (using mean average error) was measured. The prediction in the unsampled dataset corresponds to using k = 1000 as this is the maximum number of interactions per user. We see that with k > 300 the error seems to converge and we get a quality that sufficiently replicates the unsampled quality.",
        "Issue Links": []
    },
    "MAHOUT-915": {
        "Key": "MAHOUT-915",
        "Summary": "OutOfMemoryError in EigenVerificationJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "06/Dec/11 14:27",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/Feb/12 14:06",
        "Description": "I'm getting an OutOfMemoryError in EigenVerificationJob that seems to come from the following lines (200 - 204) that try to print the complete eigenvector to the console.\n\n \nEigenVector ev = ...\nlog.info(\"appending {} to {}\", ev, path);\n\n\nAm I doing something wrong here? Or should we handle this somehow else? I don't want to use another loglevel because of this.",
        "Issue Links": []
    },
    "MAHOUT-916": {
        "Key": "MAHOUT-916",
        "Summary": "Make Mahout's tests run in parallel",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Grant Ingersoll",
        "Created": "06/Dec/11 15:18",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 23:41",
        "Description": "Maven now supports parallel execution of tests.  We should hook this in to Mahout.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1200",
            "/jira/browse/MAHOUT-917"
        ]
    },
    "MAHOUT-917": {
        "Key": "MAHOUT-917",
        "Summary": "Build takes too long",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Frank Scholten",
        "Created": "06/Dec/11 16:19",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 08:32",
        "Description": "On my machine a full mvn clean install takes 55 minutes.\nAs an experiment I put all MapReduce job tests for all clustering algorithms on ignore. This reduces the build to 45 minutes. There are a lot of these long running tests in the project.\nWhat about creating a separate maven profile for the nightly build that run all MapReduce job tests? For this we have to move these MapReduce tests\nto separate classes with a naming convention such as *JobTest or *IntegrationTest and add some maven configuration.",
        "Issue Links": [
            "/jira/browse/MAHOUT-916"
        ]
    },
    "MAHOUT-918": {
        "Key": "MAHOUT-918",
        "Summary": "Implement SGD based classifiers using MapReduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "issei yoshida",
        "Created": "07/Dec/11 10:07",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "02/Mar/14 20:01",
        "Description": "Implement SGD based classifiers (Logistic Regression, Adaptive Logistic regression and Passive-Aggressive) using MapReduce.\nThey are implemented using Iterative Parameter Mixtures algorithm which is referred to in the following papers.\nhttp://research.google.com/pubs/pub36948.html\nhttp://aclweb.org/anthology-new/N/N10/N10-1069.pdf\nhttp://books.nips.cc/papers/files/nips22/NIPS2009_0345.pdf",
        "Issue Links": []
    },
    "MAHOUT-919": {
        "Key": "MAHOUT-919",
        "Summary": "WeightedRunningAverage does not initialize correctly.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Anatoliy Kats",
        "Created": "09/Dec/11 08:47",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "09/Dec/11 10:09",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-920": {
        "Key": "MAHOUT-920",
        "Summary": "Remove a mapreduce job from parallel FPGrowth workflow",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Tom Pierce",
        "Created": "10/Dec/11 20:55",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "15/Jan/12 16:19",
        "Description": "The transaction sorting job could have been made map-only, and another mapreduce job follows, so it made sense to combine them.  It would have been possible to use a ChainMapper approach, but it seemed simpler to fold the functionality into the following mapper (ParallelFPGrowthMapper).  The attached patch does just that.",
        "Issue Links": [
            "/jira/browse/MAHOUT-890",
            "/jira/browse/MAHOUT-921",
            "/jira/browse/MAHOUT-927"
        ]
    },
    "MAHOUT-921": {
        "Key": "MAHOUT-921",
        "Summary": "FPG uses a lot of boxed primitives - this patch eliminates a bunch of List<Integer>",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Tom Pierce",
        "Created": "11/Dec/11 19:59",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "15/Jan/12 16:19",
        "Description": "TransactionTree uses List<Integer> internally and as part of its API; this patch changes that and pushes the use of List<Integer> down to FPGrowth.java.  This can be converted if the perf bug can be fixed (MAHOUT-890).  I will also provide an alternate base FPGrowth implementation.\nPlease note that this patch assumes the patch attached to MAHOUT-920 has already been applied.",
        "Issue Links": [
            "/jira/browse/MAHOUT-920",
            "/jira/browse/MAHOUT-927"
        ]
    },
    "MAHOUT-922": {
        "Key": "MAHOUT-922",
        "Summary": "SSVD: ABt Job tweaks for extra sparse inputs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "11/Dec/11 20:44",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "21/Dec/11 19:19",
        "Description": "Per tests on Sebastian's extremely sparse large inputs (4.5m x 4.5 m). \nAB' performance is still a bottleneck if one uses power iterations. For sufficiently sparse inputs it may turn out that mappers cannot form the entire blocked product in memory for Y_i. the Y_i block is going to be of size s x (k+p) where s is number of A rows read in a given split. in cases when A is extra sparse, such blocks may actually take more space than the A input. When this happens, s is constrained by -oh parameter and combiners and reducers get flooded by partial oh x (k+p) outer products and seem to have hard time to sort and shuffle them (especially high pressure on combiners has been seen). \nSo, several improvements in this patch: \n\u2013 present Y_i blocks as dense (they are beleived to be dense anyway, so keeping them as sparse just eats up RAM by sparse encoding, so at least twice as high blocks can actually be formed);\n\u2013 eliminate combining completely. instead of persisting and sorting and summing up partial product in combiner, sum up map-side. if block height is still insufficient and cannot be extended due RAM constraints (unlikely for Sebastien's 4.5 x 4.5 mln case) just perform additional passes over B'. Since computation is cpu bound, I/O overhead from additional passes over B' should not register. Besides, distributed cache option is now provided to efficiently address that. However, elimination of combiner phase for high load cases is probably going to have a dramatic effect.\n\u2013 set max block height for Q'A and AB' separately instead of single -oh option. Their scaling seems to be quite different in terms of OOM danger. in my experiments Q'A blocking enters red zone at ~150,000 already whereas AB' block height can freely roam over a million easily for the same RAM. I provide 200,000 (~160Mb for k+p=100) as a default for AB' blocks which should be enough for Sebastien's 4.5 x 4.5 mln sparse case without causing more than one block. \n\u2013 provided broadcast option (--broadcast, -br) to enable/disable using DistributedCache for broadcasting B' in AB' job and R-hat during QR step.\n\u2013 forcing --reduceTasks (-t) option as non-optional. I had at least two cases when people did not set it and it is wildly important for parallelism of these jobs.\nMiscellanea: \n\u2013 Test run time: removed redundant tests and checks for SSVD. reduced test input size.\n\u2013 Per Nathan's suggestion, p parameter is now optional, default is 15 (single task running time is proportional to (k+p), so I want to be careful not to run it too high by default).\nCurrent patch branch work is here: https://github.com/dlyubimov/mahout-commits/tree/MAHOUT-922",
        "Issue Links": []
    },
    "MAHOUT-923": {
        "Key": "MAHOUT-923",
        "Summary": "Row mean job for PCA",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Raphael Cendrillon",
        "Reporter": "Raphael Cendrillon",
        "Created": "12/Dec/11 00:21",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "26/Dec/11 20:26",
        "Description": "Add map reduce job for calculating mean row (column-wise mean) of a Distributed Row Matrix for use in PCA.",
        "Issue Links": []
    },
    "MAHOUT-924": {
        "Key": "MAHOUT-924",
        "Summary": "Allow creation of symmetric adjacency matrices",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "12/Dec/11 13:06",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "07/Dec/12 08:54",
        "Description": "org.apache.mahout.graph.AdjacencyMatrixJob should allow the creation of symmetric adjacency matrices (which correspond to an undirected graph)",
        "Issue Links": []
    },
    "MAHOUT-925": {
        "Key": "MAHOUT-925",
        "Summary": "Evaluate the reach of recommender algorithms",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Anatoliy Kats",
        "Created": "12/Dec/11 15:26",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "13/Dec/11 21:09",
        "Description": "The evaluation of a CF algorithm should include reach, the proportion of users for whom a recommendation could be made.  An algorithm usually has a cutoff value on the confidence of the recommender, and if it is not high enough, no recommendation is made.  The number of requested recommendations, or this parameter could be varied as part of the evaluation.  The proposed patch adds this.\nMy build with this patch breaks testMapper(org.apache.mahout.classifier.df.mapreduce.partial.Step1MapperTest): org.apache.mahout.classifier.df.node.Leaf.<init>(I)V .  The test seems unrelated to the patch, so I am assuming this is broken in the trunk head as well.  Unfortunately I am under a deadline, and I do not have time to write tests for the patch.",
        "Issue Links": []
    },
    "MAHOUT-926": {
        "Key": "MAHOUT-926",
        "Summary": "Add the Tree/Forest Visualizer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.6",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Ikumasa Mukai",
        "Created": "13/Dec/11 03:56",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "26/Dec/11 16:40",
        "Description": "TreePrinter and ForestPrinter are made for checking the model on MAHOUT-840.\nI think they are useful for checking the model and making unit testing.",
        "Issue Links": [
            "/jira/browse/MAHOUT-840",
            "/jira/browse/MAHOUT-961"
        ]
    },
    "MAHOUT-927": {
        "Key": "MAHOUT-927",
        "Summary": "FPG saves a mapping from from feature to mining group, when this can be calculated on the fly",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Tom Pierce",
        "Created": "15/Dec/11 13:37",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "15/Jan/12 16:19",
        "Description": "The group membership list can easily be computed on the fly, rather than written out and reread, then saved in memory.",
        "Issue Links": [
            "/jira/browse/MAHOUT-920",
            "/jira/browse/MAHOUT-921"
        ]
    },
    "MAHOUT-928": {
        "Key": "MAHOUT-928",
        "Summary": "Add the ARFF data loader/converter on DF",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Ikumasa Mukai",
        "Reporter": "Ikumasa Mukai",
        "Created": "16/Dec/11 08:38",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "08/Mar/14 12:03",
        "Description": "ArffDataLoader, ArffData, ArffInvalidFormatException are made for checking the model on MAHOUT-840.\nI think this function improves usability. (now we have to remove headers from arff data before using mahout)",
        "Issue Links": [
            "/jira/browse/MAHOUT-840",
            "/jira/browse/MAHOUT-155"
        ]
    },
    "MAHOUT-929": {
        "Key": "MAHOUT-929",
        "Summary": "Refactor Clustering (Vector Classification) into a Separate Postprocess with Outlier Pruning",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Classification,                                            Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Jeff Eastman",
        "Created": "16/Dec/11 19:41",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/May/12 22:07",
        "Description": "The current clustering drivers have a -cp option to produce clusteredPoints directory containing the input vectors classified by the final clusters produced by the algorithm. These options are redundantly implemented in those drivers.\n\nFactor out & implement an independent post processor to perform the classification step independently of the various clustering implementations.\n\n\nImplement a pluggable outlier removal capability for this classifier.\n\n\nConsider building off of the ClusterClassifier & ClusterIterator ideas.",
        "Issue Links": [
            "/jira/browse/MAHOUT-930",
            "/jira/browse/MAHOUT-931",
            "/jira/browse/MAHOUT-933"
        ]
    },
    "MAHOUT-930": {
        "Key": "MAHOUT-930",
        "Summary": "Refactor Vector Classifaction out of Clustering - Make Classification abstract",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Classification,                                            Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "18/Dec/11 04:34",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "01/Apr/12 05:38",
        "Description": "Right now, each clustering algorithm has its own runClustering ( -cp ) implementation which produces clusteredPoints. The current design lacks :\n1) Extensibility - No place to plugin new features like outlier removal while classification\n2) Uniformity in design - as new algorithms don't have a pattern to follow.\n3) Abstraction - the clusterData should only bother about classifying vectors i.e. assigning different vectors to clusters. Currently it lacks a bit of abstraction. It should not care about how to classify. That should be the work of a separate entity, which can have features like outlier removal.\nThe new implementation factor out & implement an independent entity to perform the classification step independently of the various clustering implementations. The new design would start with ClusterClassifier, ClusteringPolicy and ClusterIterator whose experimental versions are available and committed. The currently committed version seems to work for all the iterative clustering algorithms.\nThe ClusterClassifier provides probability of any vector belonging to the different clusters available. These probabilities are converted into weights by different ClusteringPolicy implementations, which are for respective clustering algorithms. This is the place where the outlier removal implementation can be plugged in. In future, different implementations of ClusteringPolicy can be provided (configured) for different type of classification.\nThe ClusteringPolicy can be initialized with the ClusterConfig objects. These ClusterConfig objects would hold the Clustering Algorithm parameters which will help in classifying the Clusters.\nThe ClusterClassifier also gives the capability to train the existing classifiers (clusters), by the input. This is the place where clustering/classification will converge.\nThe execution is done by a ClusterIterator for now, which runs a clustering policy on the input and tries to classify the vectors to different clusters. It can simultaneously train the classifiers, as it can run for given number of iterations and each iteration would improve the quality of the classifiers.",
        "Issue Links": [
            "/jira/browse/MAHOUT-931",
            "/jira/browse/MAHOUT-929"
        ]
    },
    "MAHOUT-931": {
        "Key": "MAHOUT-931",
        "Summary": "Implement a pluggable outlier removal capability for cluster classifiers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Classification,                                            Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "18/Dec/11 04:41",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "01/Apr/12 05:39",
        "Description": "A pluggable outlier removal capability while classifying the clusters is needed. The classification and outlier removal implementations, both should be completely separate entities for better abstraction.",
        "Issue Links": [
            "/jira/browse/MAHOUT-930",
            "/jira/browse/MAHOUT-929"
        ]
    },
    "MAHOUT-932": {
        "Key": "MAHOUT-932",
        "Summary": "RandomForest quits with ArrayIndexOutOfBoundsException while running sample",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Berttenfall M.",
        "Created": "18/Dec/11 13:50",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Mar/14 21:51",
        "Description": "Hello,\nwhen running the example under https://cwiki.apache.org/MAHOUT/partial-implementation.html with the recommended data sets several issues occur.\nFirst: ARFF files seem no longer to be supported, I've been using the UCI format as recommended here (https://cwiki.apache.org/MAHOUT/breiman-example.html). Using ARFF files, Mahout quits when creating the description file (wrong number of attributes in the string), using UCI format it works.\nThe main error happends during the BuildForest step (I could not test TestForest, due to missing tree).\nRunning:\n$MAHOUT_HOME/bin/mahout org.apache.mahout.classifier.df.mapreduce.BuildForest -Dmapred.max.split.size=1874231 -d convertedData/data.data -ds KDDTrain+.info -sl 5 -p -t 100 -o nsl-forest.\nI tested different split.size values. 1874231, 187423, 18742 give the following error. 1874 does not finish on my machine (Dual Core MacBook Pro 2009, 8 Gb, SSD).\nIt quits after a while (map is almost done) with the following message:\n11/12/17 16:23:24 INFO mapred.Task: Task 'attempt_local_0001_m_000998_0' done.\n11/12/17 16:23:24 INFO mapred.Task: Task:attempt_local_0001_m_000999_0 is done. And is in the process of commiting\n11/12/17 16:23:24 INFO mapred.LocalJobRunner: \n11/12/17 16:23:24 INFO mapred.Task: Task attempt_local_0001_m_000999_0 is allowed to commit now\n11/12/17 16:23:24 INFO output.FileOutputCommitter: Saved output of task 'attempt_local_0001_m_000999_0' to file:/Users/martin/Documents/Studium/Master/LargeScaleProcessing/Repository/mahout_algorithms_evaluation/testingRandomForests/nsl-forest\n11/12/17 16:23:27 INFO mapred.LocalJobRunner: \n11/12/17 16:23:27 INFO mapred.Task: Task 'attempt_local_0001_m_000999_0' done.\n11/12/17 16:23:28 INFO mapred.JobClient:  map 100% reduce 0%\n11/12/17 16:23:28 INFO mapred.JobClient: Job complete: job_local_0001\n11/12/17 16:23:28 INFO mapred.JobClient: Counters: 8\n11/12/17 16:23:28 INFO mapred.JobClient:   File Output Format Counters \n11/12/17 16:23:28 INFO mapred.JobClient:     Bytes Written=41869032\n11/12/17 16:23:28 INFO mapred.JobClient:   FileSystemCounters\n11/12/17 16:23:28 INFO mapred.JobClient:     FILE_BYTES_READ=37443033225\n11/12/17 16:23:28 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=44946910704\n11/12/17 16:23:28 INFO mapred.JobClient:   File Input Format Counters \n11/12/17 16:23:28 INFO mapred.JobClient:     Bytes Read=20478569\n11/12/17 16:23:28 INFO mapred.JobClient:   Map-Reduce Framework\n11/12/17 16:23:28 INFO mapred.JobClient:     Map input records=125973\n11/12/17 16:23:28 INFO mapred.JobClient:     Spilled Records=0\n11/12/17 16:23:28 INFO mapred.JobClient:     Map output records=100000\n11/12/17 16:23:28 INFO mapred.JobClient:     SPLIT_RAW_BYTES=215000\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 100\n\tat org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilder.processOutput(PartialBuilder.java:126)\n\tat org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilder.parseOutput(PartialBuilder.java:89)\n\tat org.apache.mahout.classifier.df.mapreduce.Builder.build(Builder.java:303)\n\tat org.apache.mahout.classifier.df.mapreduce.BuildForest.buildForest(BuildForest.java:201)\n\tat org.apache.mahout.classifier.df.mapreduce.BuildForest.run(BuildForest.java:163)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.classifier.df.mapreduce.BuildForest.main(BuildForest.java:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nPS: I adjusted the class to .classifier.df. and removed -oop",
        "Issue Links": [
            "/jira/browse/MAHOUT-1419"
        ]
    },
    "MAHOUT-933": {
        "Key": "MAHOUT-933",
        "Summary": "Implement mapreduce version of ClusterIterator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Classification,                                            Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Paritosh Ranjan",
        "Created": "20/Dec/11 04:32",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/May/12 22:05",
        "Description": "Right now, ClusterIterator consumes vectors only from in-memory and sequential hdfs. A mapreduce version to consume vectors needs to be implemented.",
        "Issue Links": [
            "/jira/browse/MAHOUT-929"
        ]
    },
    "MAHOUT-934": {
        "Key": "MAHOUT-934",
        "Summary": "Deploy sgd classifier trained model in an application",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Classification,                                            Integration",
        "Assignee": null,
        "Reporter": "Selvakumar Arumugam",
        "Created": "21/Dec/11 15:43",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Mar/13 19:29",
        "Description": "I have generated trained model by modifying mahout sgd classifier for my own train data.\nBut now i want to use that model in my web application which need to give response as classified category for user query.\nI dont know how to use that model. So please tell me\nHow to deploy a sgd classifier trained MODEL in an web application or any sample application? \nOr\nHow to use that model in Jsp/Servlet Web application to classify the query keyword which given by user?",
        "Issue Links": []
    },
    "MAHOUT-935": {
        "Key": "MAHOUT-935",
        "Summary": "Improve artifact package to supply configuration files and example shell scripts",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "22/Dec/11 06:19",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "10/Nov/12 02:28",
        "Description": "The release artifacts only include Java binaries. The project would be more accessible to end users if the artifacts included the ability to run bin/mahout and the examples/bin and integration/bin shell scripts.",
        "Issue Links": []
    },
    "MAHOUT-936": {
        "Key": "MAHOUT-936",
        "Summary": "Mismatch in case for arg handling in org.apache.mahout.vectorizer.collocations.llr.CollocDriver",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Mat Kelcey",
        "Created": "23/Dec/11 06:32",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "23/Dec/11 14:11",
        "Description": "the option minSupport is added with a capital S\n   addOption(\"minSupport\", \"s\", \"(Optional) Minimum Support. Default Value: \"\nbut the actual value retrieved from the arg map has a lower case s\n   if (argMap.get(\"--minsupport\") != null) {\nWas tempted to attach a patch and then thought... well.... it might not be quite worth it",
        "Issue Links": []
    },
    "MAHOUT-937": {
        "Key": "MAHOUT-937",
        "Summary": "Collocations Job Partitioner not being configured properly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.6",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Mat Kelcey",
        "Created": "28/Dec/11 06:29",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "29/Dec/11 03:21",
        "Description": "The first pass of the collocations discovery job (as described by CollocDriver.generateCollocations) uses the org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitioner partitioner. \nThis partitoner has an instance variable offset that is supposed to be set by a call to setOffsets() but this call is never made (not sure why? is this method expected to be called by the Hadoop framework itself?) \nThe offset not being set results in getPartition always returning 0 and so all intermediate data is sent to the one reducer. \nI couldn't quite understand what this partitioning was meant to be doing, but simply hashing the Grams primary string representation (ie without the leading 'type' byte) does what is required...\n\npublic class GramKeyPartitioner extends Partitioner<GramKey, Gram> {\n\n  @Override\n  public int getPartition(GramKey key, Gram value, int numPartitions) {\n    // exclude first byte which is the key type \n    byte[] keyBytesWithoutTypeByte = new byte[key.getPrimaryLength()-1]; \n    System.arraycopy(key.getBytes(), 1, keyBytesWithoutTypeByte, 0, keyBytesWithoutTypeByte.length); \n    int hash = WritableComparator.hashBytes(keyBytesWithoutTypeByte, keyBytesWithoutTypeByte.length);\n    return (hash & Integer.MAX_VALUE) % numPartitions;    \n  }\n  \n}",
        "Issue Links": []
    },
    "MAHOUT-938": {
        "Key": "MAHOUT-938",
        "Summary": "add javadoc for code under integration subfold",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "build",
        "Assignee": "Sean R. Owen",
        "Reporter": "Yue Guan",
        "Created": "29/Dec/11 00:51",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "29/Dec/11 03:20",
        "Description": "No javadoc for code under integration subfold. \nFix:\nAdd following lines at line 80 in mahout_trunk/integration/pom.xml:\n<plugin>\n  <artifactId>maven-javadoc-plugin</artifactId>\n</plugin>",
        "Issue Links": []
    },
    "MAHOUT-939": {
        "Key": "MAHOUT-939",
        "Summary": "ASF Email Classification Examples don't always produce good results",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Grant Ingersoll",
        "Created": "03/Jan/12 13:11",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "08/Jun/12 22:07",
        "Description": "The classification examples for the ASF email don't work all that well currently in terms of quality when it comes to more than a few labels.  Also, need to determine how much memory is required for vectors of cardinality size 100K.",
        "Issue Links": [
            "/jira/browse/MAHOUT-941"
        ]
    },
    "MAHOUT-940": {
        "Key": "MAHOUT-940",
        "Summary": "Clusterdumper - Get rid of map based implementation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "04/Jan/12 17:13",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jun/12 08:22",
        "Description": "Current implementation of ClusterDumper puts clusters and related vectors in map. This generally results in OOM.\nSince ClusterOutputProcessor is availabale now. The ClusterDumper will at first process the clusteredPoints, and then write down the clusters to a local file. \nThe inability to properly read the clustering output due to ClusterDumper facing OOM is seen too often in the mailing list. This improvement will fix that problem.",
        "Issue Links": []
    },
    "MAHOUT-941": {
        "Key": "MAHOUT-941",
        "Summary": "Improve ConfusionMatrix statistics",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Lance Norskog",
        "Created": "05/Jan/12 06:46",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 20:44",
        "Description": "This patch adds more statistics to the ConfusionMatrix and RequestAnalyzer.\n\nAdd Kappa measure - a standard measure comparing a sample v.s. random assignment.\nAdd mean & standard deviation of \"Reliability\" (User Accuracy) - assist in identifying consistent mal-assignment against \"good\" and \"bad\" labels.",
        "Issue Links": [
            "/jira/browse/MAHOUT-939"
        ]
    },
    "MAHOUT-942": {
        "Key": "MAHOUT-942",
        "Summary": "Improbe the way to process the missing value for DF.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Ikumasa Mukai",
        "Created": "05/Jan/12 16:26",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 16:40",
        "Description": "If we process the data which contains the missing value(\"?\"),\nthe tree cannot be created because DataConverter.convert inserts the null value\nto the list of Instances.\nOf cause we can fix this issue with prohibiting DataConverter.convert insert\nthe null value, but I notice that there is a potentiality that the rows\nwhich have missing value(\"?\") can be also used to make the tree.\nWe can use them for making all stems on the edge where we use the missing value.",
        "Issue Links": []
    },
    "MAHOUT-943": {
        "Key": "MAHOUT-943",
        "Summary": "Improbe the way to make the split point on DF.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Ikumasa Mukai",
        "Created": "05/Jan/12 17:00",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "02/Mar/14 20:04",
        "Description": "The numericalSplit() on OptIgSplit adopts the way to regard the attribute value having the best IG as the split point.\nBut I think this is a little too strict and think it is better on some situation to  use the average value which is calced with the best IG value and the 2nd value.",
        "Issue Links": [
            "/jira/browse/MAHOUT-945",
            "/jira/browse/MAHOUT-1419"
        ]
    },
    "MAHOUT-944": {
        "Key": "MAHOUT-944",
        "Summary": "LuceneIndexToSequenceFiles (lucene2seq) utility",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Frank Scholten",
        "Created": "11/Jan/12 09:33",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "06/Jun/13 15:51",
        "Description": "Here is a lucene2seq tool I used in a project. It creates sequence files based on the stored fields of a lucene index.\nThe output from this tool can be then fed into seq2sparse and from there you can do text clustering.\nComes with Java bean configuration.\nLet me know what you think. Some CLI code can be added later on. I used this for a small-scale project +- 100.000 docs. Is a MR version useful or is that overkill?\nSee https://github.com/frankscholten/mahout/tree/lucene2seq for commits and review comments from Simon Willnauer (Thanks Simon!)\nor the attached patch.",
        "Issue Links": []
    },
    "MAHOUT-945": {
        "Key": "MAHOUT-945",
        "Summary": "The variance calculation of Random forest regression tree",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Wang Yue",
        "Created": "11/Jan/12 12:00",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Mar/13 23:14",
        "Description": "Hi, Mukai\n  Thanks for your efforts in expand the RF to regression. However, I have a doubt about your implementation regarding to Regressionsplit.java. The variance method \n\"\n private static double variance(double[] s, double[] ss, double[] dataSize) {\n    double var = 0;\n    for (int i = 0; i < s.length; i++) {\n      if (dataSize[i] > 0) \n{\n        var += ss[i] - ((s[i] * s[i]) / dataSize[i]);\n      }\n    }\n    return var;\n  }\n\"\nWhile the variance in my mind should be something like \nvar += ss[i]/dataSize[i] - ((s[i] * s[i]) / (dataSize[i]*dataSize[i]));\nPlease help correct me if I am wrong. Thanks",
        "Issue Links": [
            "/jira/browse/MAHOUT-943"
        ]
    },
    "MAHOUT-946": {
        "Key": "MAHOUT-946",
        "Summary": "Map-reduce job status often left unchecked",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Tom Pierce",
        "Created": "15/Jan/12 22:40",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "08/Feb/12 17:17",
        "Description": "I've run into a few places in Mahout where mapreduce jobs can fail and their status won't be checked, so processing will continue on.  This sometimes obscures the root problem.  I've tracked down a bunch of places where this problem exists, and tried to fix the code to do something reasonable (return -1 status code, throw exception, etc.) when jobs fail.",
        "Issue Links": []
    },
    "MAHOUT-947": {
        "Key": "MAHOUT-947",
        "Summary": "Improvements to seqdumper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Tom Pierce",
        "Created": "15/Jan/12 22:52",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "14/Feb/12 21:50",
        "Description": "I've put together a few handy additions to seqdumper:\n\nAbility to dump all sequence files in a directory.\nA quiet flag to attenuate the non-data output.\nA flag to toggle name-only printing for NamedVector values.\nAn option to only print the N highest-valued elements in WeightedVector values\n\nSeems like others will probably find some of these to be helpful.",
        "Issue Links": []
    },
    "MAHOUT-948": {
        "Key": "MAHOUT-948",
        "Summary": "Improved error reporting when ARFF index does not exist in arff.vector [fix provided]",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Stuart Smith",
        "Created": "18/Jan/12 02:57",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/Feb/12 14:09",
        "Description": "MapBackedARFFModel throws an NPE when getValue is passed an idx for an attribute that does not exist.\nIn short, this:\n<code>\n  public double getValue(String data, int idx) {\n    ARFFType type = typeMap.get(idx);\n    data = QUOTE_PATTERN.matcher(data).replaceAll(\"\");\n    data = data.trim();\n    double result;\n    if( type == null ) \n{\n\tthrow new IllegalStateException( \"Attribute type cannot be NULL, attribute index was: \" + idx );\n    }\n \n    switch (type) {\n      case NUMERIC:\n        result = processNumeric(data);\n</code>\nIs better than this:\n<code>\n  public double getValue(String data, int idx) {\n    ARFFType type = typeMap.get(idx);\n    data = QUOTE_PATTERN.matcher(data).replaceAll(\"\");\n    data = data.trim();\n    double result;\n   switch (type) {\n      case NUMERIC:\n        result = processNumeric(data);\n </code>",
        "Issue Links": []
    },
    "MAHOUT-949": {
        "Key": "MAHOUT-949",
        "Summary": "ClusterDumper option to skip vector terms and weights",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Ioan Eugen Stan",
        "Created": "18/Jan/12 15:53",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 08:46",
        "Description": "ClusterDumper dumps a lot of output when displaying cluster points. Sometimes people just want to see what points are in a cluster without displaying them (terms and weights). It would be nice to provide a skipTerms option that will display the center point and points in a cluster without all the associated terms.",
        "Issue Links": []
    },
    "MAHOUT-950": {
        "Key": "MAHOUT-950",
        "Summary": "Change BtJob to use new MultipleOutputs API",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Thomas White",
        "Created": "18/Jan/12 18:00",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/Apr/14 14:40",
        "Description": "BtJob uses a mixture of the old and new MapReduce API to allow it to use MultipleOutputs (which isn't available in Hadoop 0.20/1.0). This fails when run against 0.23 (see MAHOUT-822), so we should change BtJob to use the new MultipleOutputs API. (Hopefully the new MultipleOutputs API will be made available in a 1.x release - see MAPREDUCE-3607.)",
        "Issue Links": [
            "/jira/browse/MAHOUT-822",
            "/jira/browse/MAHOUT-1427"
        ]
    },
    "MAHOUT-951": {
        "Key": "MAHOUT-951",
        "Summary": "StackOverflow Error when using mahout lucene.vector",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Paul Rudin",
        "Created": "19/Jan/12 16:15",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "07/Feb/12 21:52",
        "Description": "Running mahout lucene.vector can result in a java StackOverFlowError.\nI think this is probably because the current implementation of LuceneIterator.computeNext() is recursive and with appropriate data the stack becomes too large. The recursion only occurs when you hit a document that doesn't have termvectors in the specified field - so you need a lucene.index with lots of documents lacking such in order to hit this problem.\nI've made minimal changes to convert to a loop rather than recurse and I'll attach a patch to this ticket.",
        "Issue Links": []
    },
    "MAHOUT-952": {
        "Key": "MAHOUT-952",
        "Summary": "ARFFVectorIterable/MapBackedArffModel doesn't handle question mark '?', other ARFF issues",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Stuart Smith",
        "Created": "19/Jan/12 20:16",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 21:54",
        "Description": "Whatever is parsing the ARFF file for the ARFFVectorIterable (As far as I can tell, it's the class itself) doesn't handle '?' as a marker for unknown value. See: http://www.cs.waikato.ac.nz/~ml/weka/arff.html \nI just started looking at Mahout classifiers this week, so I'm not sure how to handle this yet. If I figure it out, I'll post a patch, but until then, guidance would be helpful!",
        "Issue Links": []
    },
    "MAHOUT-953": {
        "Key": "MAHOUT-953",
        "Summary": "ArffVectorIterable does not gracefully handle duplicate attribute name",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.10.0",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Stuart Smith",
        "Created": "19/Jan/12 23:52",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "08/Mar/14 12:05",
        "Description": "If you have duplicate attribute names in your ARFF file, and you have non-sparse arff vectors, ARFFVectorIterable.computeNext will throw a ArrayIndexOutOfBoundsExceptions, as it allocates a DenseVector with the size of your attribute labels (duplicates removed), but your arff vectors could have more values (if they reference the attribute at both indexes). This is a somewhat pathological ARFF file.\nNot sure if I should note the error (throw an exception) in computeNext() when it's out of bounds, or when someone tries to add duplicate label to the MapBackedArffModel.\nMy first impulse would be to check in computeNext(), but addLabel() in MapBackedArffModel will do something rather pathological in the case of duplicate attributes: it overwrites the Label map with the new index, but the idxLabel map will hold a mapping from both indexes to the attribute name, so it's out of sync.. so it may be best to disallow duplicate attribute names \"IllegalArgumentException\" altogether.\nFor example\n@attribute my_attribute NUMERIC\n@attribute my_attribute NUMERIC\naddLabel()\naddLabel()\nlabelBindings -> ('my_attribute', 1)\nidxLabel -> (0, 'my_attribute), (1, 'my_attribute')\nI'll happily submit a patch, just wondering if it should be in computeNext() or addLabel()",
        "Issue Links": []
    },
    "MAHOUT-954": {
        "Key": "MAHOUT-954",
        "Summary": "\"Unpredictable\" have to be represented by NaN on DF.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Ikumasa Mukai",
        "Created": "24/Jan/12 13:00",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 10:42",
        "Description": "On our RandomForest src, we uses \"-1\" for representing \"Unpredictable\",\nbut for regression problem, there is a case which outputs \"-1\" for the correct result.\nFor resolving this problem, we should change the unpredictable flag to \"NaN\" from \"-1\".",
        "Issue Links": [
            "/jira/browse/MAHOUT-840"
        ]
    },
    "MAHOUT-955": {
        "Key": "MAHOUT-955",
        "Summary": "Bayes classification result are unstable after classifying non-existing features",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Micha\u0142 B",
        "Created": "24/Jan/12 18:47",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "04/Jun/12 14:08",
        "Description": "Bayes classification results are unstable, and change during runtime!\nSample test:\nMyClassifier classifier = new MyClassifier(new BayesAlgorithm(),params); //Custom simple wrapper for classifier\nClassifierResult category = classifier.classify(\"existing\");\ndouble resultA = category.getScore();\ncategory = classifier.classify(\"nonexisting\");\ncategory = classifier.classify(\"existing\");\ndouble resultB = category.getScore();\nAssert.assertEquals(resultA,resultB,0.0); // FAIL!!!\nTest like the one above will fail. Because nonexisting tokens are added to InMemoryBayesDatastore->featureDictionary therefore datastore.getWeight(\"sumWeight\", \"vocabCount\") change after classification of unknown feature. Moreover, the featureDictionary fills with not wanted strings using heapspace.\nMore on this here\nhttp://www.lucidimagination.com/search/document/7dabe3efec8d136d/issues_with_memory_use_and_inconsistent_or_state_influenced_results_when_using_cbayesalgorit#8853165db260bf75",
        "Issue Links": []
    },
    "MAHOUT-956": {
        "Key": "MAHOUT-956",
        "Summary": "RowIdJob fails if logsCRCfilter is not set.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Suneel Marthi",
        "Created": "24/Jan/12 22:09",
        "Updated": "09/Feb/12 14:00",
        "Resolved": "27/Jan/12 21:42",
        "Description": "RowIdJob when executed doesn't ignore '_SUCCESS' or '*crc' files as PathFilters.logsCRCFilter is not set.",
        "Issue Links": []
    },
    "MAHOUT-957": {
        "Key": "MAHOUT-957",
        "Summary": "term vectors not created in SparseVectorsFromSequenceFiles using tf weighting and maxDFSigma filtering",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.6",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "John Conwell",
        "Created": "25/Jan/12 00:14",
        "Updated": "09/Feb/12 14:01",
        "Resolved": "29/Jan/12 00:56",
        "Description": "The SparseVectorsFromSequenceFiles throws an exception when you want term frequency vectors output, with the maxDFSigma filtering option.\nBasically the if / else if section shown below, will skip calling DictionaryVectorizer.createTermFrequencyVectors when have that combination.  The condition will create vectors when you want tf vectors without maxDFSigma filtering, or tfidf vectors with maxDFSigma filtering, but if you want tf vectors with maxDFSigma filtering, it totally skips over the call to createTermFrequencyVectors, and later on throws an exception because the vector input path doesn't exist.\nFor example, the following cmd line will reproduce this situation:\nbin/mahout seq2sparse -i /Users/me/Documents/workspace/mahoutStuff/seq -o /Users/me/Documents/workspace/mahoutStuff/termvecs -wt tf --minSupport 2 --minDF 2 --maxDFSigma 3 -seq\n//the suspect code at line ~267 in DictionaryVectorizer.createTermFrequencyVectors\nif (!processIdf && !shouldPrune) {\n        DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath, outputDir, tfDirName, conf, minSupport, maxNGramSize,\n          minLLRValue, norm, logNormalize, reduceTasks, chunkSize, sequentialAccessOutput, namedVectors);\n} else if (processIdf) {\n        DictionaryVectorizer.createTermFrequencyVectors(tokenizedPath, outputDir, tfDirName, conf, minSupport, maxNGramSize,\n          minLLRValue, -1.0f, false, reduceTasks, chunkSize, sequentialAccessOutput, namedVectors);\n}",
        "Issue Links": []
    },
    "MAHOUT-958": {
        "Key": "MAHOUT-958",
        "Summary": "NullPointerException in RepresentativePointsMapper when running cluster-reuters.sh example with kmeans",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Examples",
        "Assignee": "Dan Filimon",
        "Reporter": "Rares Vernica",
        "Created": "25/Jan/12 00:25",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "07/Jun/13 19:39",
        "Description": "> svn info\nPath: .\nURL: http://svn.apache.org/repos/asf/mahout/trunk\nRepository Root: http://svn.apache.org/repos/asf\nRepository UUID: 13f79535-47bb-0310-9956-ffa450edef68\nRevision: 1235544\nNode Kind: directory\nSchedule: normal\nLast Changed Author: tdunning\nLast Changed Rev: 1231800\nLast Changed Date: 2012-01-15 16:01:38 -0800 (Sun, 15 Jan 2012)\n\n\n\n> ./examples/bin/cluster-reuters.sh\n...\n1. kmeans clustering\n...\nInter-Cluster Density: NaN\nIntra-Cluster Density: 0.0\nCDbw Inter-Cluster Density: 0.0\nCDbw Intra-Cluster Density: NaN\nCDbw Separation: 0.0\n12/01/24 16:08:47 INFO clustering.ClusterDumper: Wrote 20 clusters\n12/01/24 16:08:47 INFO driver.MahoutDriver: Program took 126749 ms (Minutes: 2.1124833333333335)\n\n\nAll five \"Representative Points Driver\" jobs fail.\n\n2012-01-24 16:07:11,555 INFO org.apache.hadoop.util.NativeCodeLoader: Loaded the native-hadoop library\n2012-01-24 16:07:11,881 INFO org.apache.hadoop.mapred.MapTask: io.sort.mb = 100\n2012-01-24 16:07:11,896 INFO org.apache.hadoop.mapred.MapTask: data buffer = 79691776/99614720\n2012-01-24 16:07:11,896 INFO org.apache.hadoop.mapred.MapTask: record buffer = 262144/327680\n2012-01-24 16:07:11,956 INFO org.apache.hadoop.mapred.TaskLogsTruncater: Initializing logs' truncater with mapRetainSize=-1 and reduceRetainSize=-1\n2012-01-24 16:07:11,979 INFO org.apache.hadoop.io.nativeio.NativeIO: Initialized cache for UID to User mapping with a cache timeout of 14400 seconds.\n2012-01-24 16:07:11,979 INFO org.apache.hadoop.io.nativeio.NativeIO: Got UserName vernica for UID 1000 from the native implementation\n2012-01-24 16:07:11,981 WARN org.apache.hadoop.mapred.Child: Error running child\njava.lang.NullPointerException\n\tat org.apache.mahout.clustering.evaluation.RepresentativePointsMapper.mapPoint(RepresentativePointsMapper.java:73)\n\tat org.apache.mahout.clustering.evaluation.RepresentativePointsMapper.map(RepresentativePointsMapper.java:60)\n\tat org.apache.mahout.clustering.evaluation.RepresentativePointsMapper.map(RepresentativePointsMapper.java:40)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:259)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:253)",
        "Issue Links": []
    },
    "MAHOUT-959": {
        "Key": "MAHOUT-959",
        "Summary": "VectorWritable does not preserve the laxPrecision flag",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Rares Vernica",
        "Created": "25/Jan/12 20:18",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "25/Jan/12 21:55",
        "Description": "When reading a vector, VectorWritable uses the laxPrecision flag to read with the right precision, but it does not preserve the flag in case the vector is written back.",
        "Issue Links": []
    },
    "MAHOUT-960": {
        "Key": "MAHOUT-960",
        "Summary": "Reduce memory usage of ImplicitFeedbackAlternatingLeastSquaresSolver",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Doug Mittendorf",
        "Created": "25/Jan/12 21:04",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 08:19",
        "Description": "One of the main limiting factors of the implicit ALS algorithm when processing large datasets is the fact that it must fit the entire U or M matrix in memory.  This is further compounded by the fact that the current implementation represents the matrix in memory 3 times:\n1. As an OpenIntObjectHashMap read in from disk\n2. A sorted DenseMatrix representation of #1 to prepare for computing Y'Y\n3. The transpose of #2 (another DenseMatrix)\nThe #3 copy of the matrix can be eliminated by computing Y'Y directly from Y without first computing the transpose of Y as an intermediate step.  This should also be more efficient in terms of CPU usage.\nNote that the #1 copy of the matrix could also be eliminated if it's assumed that the user and item IDs are sequentially assigned and ordered.  This would allow the DenseMatrix to be populated directly from disk instead of reading into an intermediate OpenIntObjectHashMap.",
        "Issue Links": []
    },
    "MAHOUT-961": {
        "Key": "MAHOUT-961",
        "Summary": "Modify the Tree/Forest Visualizer on DF.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Ikumasa Mukai",
        "Created": "26/Jan/12 08:29",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "06/Jun/13 09:11",
        "Description": "The Tree/Forest visualizer (MAHOUT-926) has problems.\n1) a un-complemented stem which has no leaf or node is shown.\n2) all stems are not shown when the data doesn't have all categories.",
        "Issue Links": [
            "/jira/browse/MAHOUT-926"
        ]
    },
    "MAHOUT-962": {
        "Key": "MAHOUT-962",
        "Summary": "minDF and maxDFPercent filtering doesnt get applied when output weight is tf in SpareVecorsFromSequenceFile",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "John Conwell",
        "Created": "27/Jan/12 19:05",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "02/Jun/13 14:57",
        "Description": "This is similar to the same reasoning behind the fix for MAHOUT-957.  The desired output is term frequency vectors, but I want terms filtered by their min and max DF values. This might be valid in LDA, where tf vectors is desired for input, but filtering out the maxDFPercent is also useful.\nCurrently minDF and maxDFPercent are only used when calculating tfidf, and the original tv vectors are not updated to represent the term filtering.",
        "Issue Links": []
    },
    "MAHOUT-963": {
        "Key": "MAHOUT-963",
        "Summary": "GenericUserPreferenceArray and GenericItemPreferenceArray use selection sorts",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Bryce Nyeggen",
        "Created": "27/Jan/12 20:35",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "07/Feb/12 18:46",
        "Description": "Both PreferenceArray implementations use selection sorts with poor performance.  These sorts are invoked during construction of GenericDataModels, causing excessive construction time.",
        "Issue Links": []
    },
    "MAHOUT-964": {
        "Key": "MAHOUT-964",
        "Summary": "RowSimilarityJob should exit immediately if an invalid similarity measure specified and it would be nice to have an --overwrite option for the RowSimilarityJob CLI",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "28/Jan/12 13:58",
        "Updated": "09/Feb/12 14:02",
        "Resolved": "30/Jan/12 13:24",
        "Description": "1. If an invalid Similarity Measure has been specified as input to the RowSimilarityJob, it presently throws a ClassNotFoundException but still proceeds with executing all of the subsequent tasks - VectorNormalizer, Cooccurrences Mapper and UnSymmetrify Mapper. We should exit the process early without having to invoke all of the subsequent tasks (all of them fail anyways).\n2. It would be nice to have an --overwrite option for the Command line interface which would delete the temp and output paths at the beginning of RowSimilarityJob execution, similar to what's being done in seq2sparse, seqdirectory. If I run RowSimilarityJob over and over again with different similarity measures, I should not be forced to delete my temp and output paths first prior to invoking the job.",
        "Issue Links": []
    },
    "MAHOUT-965": {
        "Key": "MAHOUT-965",
        "Summary": "Added possibility to configure map collection name of MongoDBDataModel",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Danny R\u00f6schke",
        "Created": "30/Jan/12 08:12",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "07/Feb/12 20:25",
        "Description": "Added field for map collection name, two costructors with mapping collection name as parameter and a cleanup method to delete the mapping collection with the configured name.",
        "Issue Links": []
    },
    "MAHOUT-966": {
        "Key": "MAHOUT-966",
        "Summary": "Mismatch in the number of points given by the clusterDumper and ClusterOutputPostProcessor",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Gaurav Redkar",
        "Created": "30/Jan/12 09:56",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "02/Jun/13 19:22",
        "Description": "After running the post processor the number of points that each cluster contains is not matching the number of points each cluster should contain as stated by clusterdumper.\nMSV-287\n{ n=90 c=[0.05195, 0.05675, 0.07151, 0.05713, 0.06946,...}\nMSV-145\n{ n=90 c=[0.93685, 0.93071, 0.93641, 0.94629, 0.94409,..}\nthe n mentioned in clusters-n-final against each cluster is different from the number of points actually contained in d directory for each cluster. Any idea why is this happening ...?",
        "Issue Links": []
    },
    "MAHOUT-967": {
        "Key": "MAHOUT-967",
        "Summary": "SequenceFileFromMailArchive missing from driver.classes.props",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Examples",
        "Assignee": "Sean R. Owen",
        "Reporter": "Ioan Eugen Stan",
        "Created": "31/Jan/12 08:20",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "07/Feb/12 20:20",
        "Description": "I've noticed the above class is not mentioned in driver.classes.props so I've created a patch for this, in case it was not left out intentionally.",
        "Issue Links": []
    },
    "MAHOUT-968": {
        "Key": "MAHOUT-968",
        "Summary": "Classifier based on restricted boltzmann machines",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Dirk Wei\u00dfenborn",
        "Created": "01/Feb/12 11:31",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "08/Mar/14 12:04",
        "Description": "This is a proposal for a new classifier based on restricted boltzmann machines. The development of this feature follows the paper on \"Deep Boltzmann Machines\" (DBM) [1] from 2009. The proposed model (DBM) got an error rate of 0.95% on the mnist dataset [2], which is really good. Main parts of the implementation should also be applicable to other scenarios than classification where restricted boltzmann machines are used (ref. MAHOUT-375).\nI am working on this feature right now, and the results are promising. The only problem with the training algorithm is, that it is still mostly sequential (if training batches are small, what they should be), which makes Map/Reduce until now, not really beneficial. However, since the algorithm itself is fast (for a training algorithm), training can be done on a single machine in managable time.\nTesting of the algorithm is currently done on the mnist dataset itself to reproduce results of [1]. As soon as results indicate, that everything is working fine, I will upload the patch.\n[1] http://www.cs.toronto.edu/~hinton/absps/dbm.pdf\n[2] http://yann.lecun.com/exdb/mnist/",
        "Issue Links": []
    },
    "MAHOUT-969": {
        "Key": "MAHOUT-969",
        "Summary": "testclassifier should check if the model file actually exists",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stuart Smith",
        "Created": "02/Feb/12 01:35",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "05/May/13 09:47",
        "Description": "The ./mahout testclassifier command for naive bayes does not check if the -model argument actually exists on the hdfs file system.\nIf it does not exist, it appears to run successfully, but generates all zeroes in the results.",
        "Issue Links": []
    },
    "MAHOUT-970": {
        "Key": "MAHOUT-970",
        "Summary": "Make hadoop version overridable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Bruno Mah\u00e9",
        "Created": "03/Feb/12 03:11",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "07/Feb/12 20:19",
        "Description": "Right now, one cannot change the version of Apache Hadoop on which Apache Mahout depend without editing the pom.xml.\nThis ticket is about making this possible.\nThis would help ensuring Apache Mahout work well with more recent versions of Apache Hadoop without disruption.",
        "Issue Links": []
    },
    "MAHOUT-971": {
        "Key": "MAHOUT-971",
        "Summary": "kmeans does not work in S3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Antonio Molins",
        "Created": "03/Feb/12 22:36",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "07/Feb/12 21:52",
        "Description": "S3n:// URIs will not work in kmeans because of a couple of calls to FileSystem.get(conf) with no URI.",
        "Issue Links": []
    },
    "MAHOUT-972": {
        "Key": "MAHOUT-972",
        "Summary": "Implement Taste DynamoDBDataModel",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Nick Jordan",
        "Created": "05/Feb/12 19:56",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Oct/12 09:00",
        "Description": "Implement Amazon's DynamoDB as a data model to be used for collaborative filtering Taste models.\nI've actually begun work on this, but have never submitted to an ASF project before.  I'll submit the patch when I've done enough testing that I think it is ready.  If anyone has any hints/tips that will make the patch/submission process easier I'd be happy to hear them.",
        "Issue Links": []
    },
    "MAHOUT-973": {
        "Key": "MAHOUT-973",
        "Summary": "SparseVectorsFromSequenceFiles will not create a proper TFIDF (bug in TFIDFPartialVectorReducer)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Viktor Gal",
        "Created": "06/Feb/12 16:45",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "06/Apr/12 12:41",
        "Description": "Although I'm using a little bit different the TFIDFConverter, but the problem will occur the same way with SparseVectorsFromSequenceFiles when somebody wants to create a TFIDF vectors for their documents.\nBasically if maxDFSigma is not set then because of SparseVectorsFromSequenceFiles.java:281\nlong maxDF = maxDFPercent;\nmaxDF will be 99. which is then passed to TFIDFConvert.processTfIdf function as an argument, where it is interpreted as \"The max percentage of vectors for the DF.\" Partial vectors will be created with TFIDFPartialVectorReducer.class and because of TFIDFPartialVectorReducer.java:81 as maxDF = 99 if (df > maxDF) the term will be ignored.\nthe problem here is that two different quantities are compared. df value is the number of documents which contains the given term, and it's not normalized by the document number, i.e. it's not a percentage! see TermDocumentCountReducer.java for details. while maxDF is interpreted as a percentage, see above. Thus, as soon as the df count gets higher than 99, or in the best case 100, meaning the given term occurs in more than 99 or 100 different documents, it'll be ignored... and this is not what we would like it to do.\nI.e. there's a bug in TFIDFPartialVectorReducer.java at line 81.\nI've attached a possible fix for this problem.\nthe bug was introduced a61e5ff8 commit (git) or rev 1210994 in svn:\n@@ -78,7 +78,7 @@ public class TFIDFPartialVectorReducer extends\n         continue;\n       }\n       long df = dictionary.get(e.index());\n\nif (df * 100.0 / vectorCount > maxDfPercent) {\n+      if (maxDf > -1 && df > maxDf) \n{\n         continue;\n       }\n       if (df < minDf) {",
        "Issue Links": []
    },
    "MAHOUT-974": {
        "Key": "MAHOUT-974",
        "Summary": "org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob  use integer as userId and itemId",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Han Hui Wen",
        "Created": "07/Feb/12 07:41",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "08/Jun/13 07:29",
        "Description": "org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob  uses integer as userId and itemId,but org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob  and  org.apache.mahout.cf.taste.hadoop.item.RecommenderJob .use Long as userId and ItemId.\nIt's best that ParallelALSFactorizationJob   also uses Long as userId and itemId ,so that same dataset can use all the recommendation arithrmetic",
        "Issue Links": []
    },
    "MAHOUT-975": {
        "Key": "MAHOUT-975",
        "Summary": "Bug in Gradient Machine  - Computation of the gradient",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Ted Dunning",
        "Reporter": "Christian Herta",
        "Created": "07/Feb/12 11:54",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "02/Mar/14 21:13",
        "Description": "The initialisation to compute the gradient descent weight updates for the output units should be wrong:\nIn the comment: \"dy / dw is just w since  y = x' * w + b.\"\nThis is wrong. dy/dw is x (ignoring the indices). The same initialisation is done in the code.\nCheck by using neural network terminology:\nThe gradient machine is a specialized version of a multi layer perceptron (MLP).\nIn a MLP the gradient for computing the \"weight change\" for the output units is:\ndE / dw_ij = dE / dz_i * dz_i / d_ij with z_i = sum_j (w_ij * a_j)\nhere: i index of the output layer; j index of the hidden layer\n(d stands for the partial derivatives)\nhere: z_i = a_i (no squashing in the output layer)\nwith the special loss (cost function) is  E = 1 - a_g + a_b = 1 - z_g + z_b\nwith\ng index of output unit with target value: +1 (positive class)\nb: random output unit with target value: 0\n=>\ndE / dw_gj = -dE/dz_g * dz_g/dw_gj = -1 * a_j (a_j: activity of the hidden unit\nj)\ndE / dw_bj = -dE/dz_b * dz_b/dw_bj = +1 * a_j (a_j: activity of the hidden unit\nj)\nThat's the same if the comment would be correct:\ndy /dw = x (x is here the activation of the hidden unit) * (-1) for weights to\nthe output unit with target value +1.\n------------\nIn neural network implementations it's common to compute the gradient\nnumerically for a test of the implementation. This can be done by:\ndE/dw_ij = (E(w_ij + epsilon) -E(w_ij - epsilon) ) / (2* (epsilon))",
        "Issue Links": [
            "/jira/browse/MAHOUT-1265"
        ]
    },
    "MAHOUT-976": {
        "Key": "MAHOUT-976",
        "Summary": "Implement Multilayer Perceptron",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Christian Herta",
        "Created": "07/Feb/12 20:39",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "20/Dec/13 21:17",
        "Description": "Implement a multi layer perceptron\n\nvia Matrix Multiplication\nLearning by Backpropagation; implementing tricks by Yann LeCun et al.: \"Efficent Backprop\"\narbitrary number of hidden layers (also 0  - just the linear model)\nconnection between proximate layers only\ndifferent cost and activation functions (different activation function in each layer)\ntest of backprop by gradient checking\nnormalization of the inputs (storeable) as part of the model\n\nFirst:\n\nimplementation \"stocastic gradient descent\" like gradient machine\nsimple gradient descent incl. momentum\n\nLater (new jira issues):  \n\nDistributed Batch learning (see below)\n\"Stacked (Denoising) Autoencoder\" - Feature Learning\nadvanced cost minimazation like 2nd order methods, conjugate gradient etc.\n\nDistribution of learning can be done by (batch learning):\n 1 Partioning of the data in x chunks \n 2 Learning the weight changes as matrices in each chunk\n 3 Combining the matrixes and update of the weights - back to 2\nMaybe this procedure can be done with random parts of the chunks (distributed quasi online learning). \nBatch learning with delta-bar-delta heuristics for adapting the learning rates.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1265"
        ]
    },
    "MAHOUT-977": {
        "Key": "MAHOUT-977",
        "Summary": "Thread-safe version of PlusAnonymousUserDataModel with multiple concurrent users",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Aleksei Udat\u0161n\u00f5i",
        "Created": "15/Feb/12 09:45",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "17/Feb/12 15:33",
        "Description": "This is a suggested improvement to PlusAnonymousUserDataModel. \nCurrently the ID of the anonymous user is fixed to Long.MIN_VALUE. Only one set\nof temp data can be inserted into the model and used at one time.\nI propose the change to enable multiple users to insert into this\nmodel concurrently in a thread-safe manner.\nThe idea is to define a pool of available anonymous user IDs. Every\ntime a new anonymous user makes a request, the next available\nTEMP_USER_ID is pulled from the queue. After recommendations are\nretrieved, the TEMP_USER_ID can be returned to the pool.\nThe discussion about this topic can be found in:\nhttp://mail-archives.apache.org/mod_mbox/mahout-dev/201202.mbox/%3CCAEccTywC7QJuAJj-u3C1C=WW0yEeBcScUE6Kmj=bedBrxOpsSg@mail.gmail.com%3E",
        "Issue Links": []
    },
    "MAHOUT-978": {
        "Key": "MAHOUT-978",
        "Summary": "spectralkmeans utility fails when input filename begins with leading underscore",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Dan Brickley",
        "Created": "19/Feb/12 13:57",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 17:17",
        "Description": "The commandline 'bin/mahout spectralkmeans' utility fails with NoSuchElementException after \"Loading vector from: spectral/output/results2/calculations/diagonal/part-r-00000\"  when input data in hdfs has filename beginning with a leading underscore.\nThis was partially reported in comments for MAHOUT-524 but I believe identified now as a distinct issue (thanks to Shannon for help diagnosing). I have not investigated if there is an equivalent problem for API-based use of this piece of Mahout.\nSteps to reproduce: \n1. put affinity file into hdfs, following https://cwiki.apache.org/MAHOUT/spectral-clustering.html - note that node IDs count from zero etc. Name your file with a leading underscore. For example, try http://danbri.org/2012/spectral/dbpedia/_topic_skm.csv and store it in spectral/input/_topic_skm.csv\n(I'll leave that example input file in place unchanged for others to try. It is built from dbpedia data, encoding associations from Wikipedia pages to categories. Whether it is a good use of spectral clustering I'm not sure, but I'd at least hope the job would run to completion.)\n2. Run 'mahout spectralkmeans -k 20 -d 4192499 -x 7 -i spectral/input/ -o spectral/output/results1'\n3. Wait for it to fail just after printing \"Loading vector from: spectral/output/results1/calculations/diagonal/part-r-00000\", with java.util.NoSuchElementException at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:152).\n4. Rename the file in hdfs to eliminate the leading underscore. Re-run the command (give a different results dir or cleanup from the first run, to avoid mixing the tests). This attempt should succeed and you'll see it proceed deeper into the job, i.e. something like \n12/02/19 14:38:32 INFO common.VectorCache: Loading vector from: spectral/output/results2/calculations/diagonal/part-r-00000\n12/02/19 14:38:41 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n12/02/19 14:38:43 INFO input.FileInputFormat: Total input paths to process : 1\n12/02/19 14:38:44 INFO mapred.JobClient: Running job: job_201202191410_0005\n12/02/19 14:38:45 INFO mapred.JobClient:  map 0% reduce 0%\n12/02/19 14:39:31 INFO mapred.JobClient:  map 1% reduce 0%\n(5. You might get a memory-based failure some time later; that is a separate problem.)\nI'll attach a more detailed transcript. I've made no attempt to diagnose internals yet, but did make some other tests and can confirm that it does not seem to matter whether the commandline invocation names the file explicitly, or by directory name only. Also trailing slash does not seem to be an issue. Finally, a related 'gotcha': make sure the results directory is not inside the input directory when testing.",
        "Issue Links": []
    },
    "MAHOUT-979": {
        "Key": "MAHOUT-979",
        "Summary": "RowSimilarityJob should be able to infer the number of columns from the input matrix if not specified",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "20/Feb/12 06:02",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "08/May/12 20:07",
        "Description": "Presently RowSimilarityJob expects to be provided the 'numberOfColumns' (-r) by the user and this is not an optional argument. If this is not specified explicitly RowSimilarityJob should be able to get the number of columns by the vector size.",
        "Issue Links": []
    },
    "MAHOUT-980": {
        "Key": "MAHOUT-980",
        "Summary": "Patch to make PFPGrowth run on Amazon MapReduce (also shows possible pattern to make other algorithms work in Amazon MapReduce)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5,                                            0.6,                                            0.7",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Tom Pierce",
        "Reporter": "Matteo Riondato",
        "Created": "22/Feb/12 05:18",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "12/Mar/12 19:21",
        "Description": "The patch at http://www.cs.brown.edu/~matteo/PFPGrowth.java.diff (against trunk as of Wed Feb 22 00:07:35 EST 2012, revision 1292127) makes it possible to run PFPGrowth on Elastic MapReduce. \nThe problem was in the way the fList stored in the DistributedCache was accessed. DistributedCache.getCacheFiles(conf) should be reserved for internal use according to the Hadoop API Documentation. The suggested way to access the files in the DistributedCache is through DistributedCache.getLocalCacheFiles(conf) and then through a LocalFilesystem. This is what the patch does. Note that there is a fallback case if we are running PFPGrowth with \"-method mapreduce\" but locally (e.g. when HADOOP_HOME is not set or MAHOUT_LOCAL is set). In this case, we use DistributedCache.getCacheFiles() as it is done in the unpatched version.\nA quick grep in the source tree shows that there are other places where DistributedCache.getCacheFiles(conf) is used. It may be worth checking whether the corresponding algorithms can be made to work in Amazon MapReduce by fixing them in a similar fashion.\nThe patch was tested also outside Amazon MapReduce and does not change any other functionality.",
        "Issue Links": []
    },
    "MAHOUT-981": {
        "Key": "MAHOUT-929 Refactor Clustering (Vector Classification) into a Separate Postprocess with Outlier Pruning",
        "Summary": "Refactor KMeans Clustering into a separate post process with outlier pruning",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Classification,                                            Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "23/Feb/12 07:45",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "17/Mar/12 15:58",
        "Description": "Use ClusterClassificationDriver to refactor clustering out of KMeansDriver with outlier pruning support.",
        "Issue Links": []
    },
    "MAHOUT-982": {
        "Key": "MAHOUT-929 Refactor Clustering (Vector Classification) into a Separate Postprocess with Outlier Pruning",
        "Summary": "Refactor Canopy Clustering into a separate post process with outlier pruning",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "23/Feb/12 07:50",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "13/Mar/12 01:42",
        "Description": "Use ClusterClassificationDriver to refactor clustering out of CanopyDriver with outlier pruning support.",
        "Issue Links": []
    },
    "MAHOUT-983": {
        "Key": "MAHOUT-929 Refactor Clustering (Vector Classification) into a Separate Postprocess with Outlier Pruning",
        "Summary": "Refactor Dirichlet Clustering into a separate post process with outlier pruning",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "23/Feb/12 07:52",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "17/Mar/12 04:25",
        "Description": "Use ClusterClassificationDriver to refactor clustering out of DirichletDriver with outlier pruning support.",
        "Issue Links": []
    },
    "MAHOUT-984": {
        "Key": "MAHOUT-929 Refactor Clustering (Vector Classification) into a Separate Postprocess with Outlier Pruning",
        "Summary": "Refactor Fuzzy K Means Clustering into a separate post process with outlier pruning",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "23/Feb/12 07:56",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "01/Apr/12 05:34",
        "Description": "Use ClusterClassificationDriver to refactor clustering out of FuzzyKMeansDriver with outlier pruning support.",
        "Issue Links": []
    },
    "MAHOUT-985": {
        "Key": "MAHOUT-985",
        "Summary": "MapBackedArffModel Unable To Parse ARFF Files Containing Instance Weights",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Dave Kor",
        "Created": "25/Feb/12 02:37",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 11:58",
        "Description": "When parsing an Arff file that contain instance-specific weights, MapBackedArffModel throws the following NPE exception. While I have only tested this in 0.5, I suspect this bug also occur in 0.6\nException in thread \"main\" java.lang.NullPointerException\n        at org.apache.mahout.utils.vectors.arff.MapBackedARFFModel.getValue(MapBackedARFFModel.java:87)\n        at org.apache.mahout.utils.vectors.arff.ARFFIterator.computeNext(ARFFIterator.java:75)\n        at org.apache.mahout.utils.vectors.arff.ARFFIterator.computeNext(ARFFIterator.java:30)\n        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)\n        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)\n        at org.apache.mahout.utils.vectors.io.SequenceFileVectorWriter.write(SequenceFileVectorWriter.java:43)\n        at org.apache.mahout.utils.vectors.arff.Driver.writeFile(Driver.java:159)\n        at org.apache.mahout.utils.vectors.arff.Driver.main(Driver.java:127)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:187)\nThe code works properly when all instance weights are set to the default value of 1. However when any instance has a non-default weight, such as in the sample Arff file below, the NPE occurs when MapBackedArffModel attempts to parse line 8. \n\n@relation 'Test Mahout'\n@attribute Attr0 numeric\n@attribute Label \n{True,False}\n\n@data\n0,False\n1,True,\n{2}\n\nThe reason is that in Weka, all data instances are assumed to have a default weight of 1 and this default weight is not saved in the Arff file. However when a data instance DOES NOT have the default weight of 1, the non-default instance weight is appended at the end of the line surrounded by curly braces. When MapBackedArffModel.getValue method tries to parse this weight as an attribute, typeMap.get(idx) returns a null ARFFtype as there is no such attribute, which results in an NPE.",
        "Issue Links": []
    },
    "MAHOUT-986": {
        "Key": "MAHOUT-986",
        "Summary": "OutOfMemoryError in LanczosState by way of SpectralKMeans",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "01/Mar/12 20:35",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "04/Jun/12 14:21",
        "Description": "Dan Brickley and I have been testing SpectralKMeans with a dbpedia dataset ( http://danbri.org/2012/spectral/dbpedia/ ); effectively, a graph with 4,192,499 nodes. Not surprisingly, the LanczosSolver throws an OutOfMemoryError when it attempts to instantiate a DenseMatrix of dimensions 4192499-by-4192499 (~17.5 trillion double-precision floating point values). Here's the full stack trace:\n\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.mahout.math.DenseMatrix.<init>(DenseMatrix.java:50)\n\tat org.apache.mahout.math.decomposer.lanczos.LanczosState.<init>(LanczosState.java:45)\n\tat org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:146)\n\tat org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:86)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.main(SpectralKMeansDriver.java:53)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:616)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:616)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nObviously SKM needs a more sustainable and memory-efficient way of performing an eigen-decomposition of the graph laplacian. For those who are more knowledgeable in the linear systems solvers of Mahout than I, can the Lanczos parameters be tweaked to negate the requirement of a full DenseMatrix? Or should SKM move to SSVD instead?",
        "Issue Links": []
    },
    "MAHOUT-987": {
        "Key": "MAHOUT-987",
        "Summary": "Our build is unstable - this should reduce our style warnings by >200",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Tom Pierce",
        "Reporter": "Tom Pierce",
        "Created": "08/Mar/12 03:08",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "15/May/12 20:35",
        "Description": "If we're going to keep these Jenkins style rules, let's get our build stable!\nHere's about 200 small fixes created by:\nfind . -name *java | xargs perl -pi -e 's/(if|while|for)(/$1 (/'\nAny objections?",
        "Issue Links": []
    },
    "MAHOUT-988": {
        "Key": "MAHOUT-933 Implement mapreduce version of ClusterIterator",
        "Summary": "Convert K-means buildClusters to use new ClusterIterator",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Jeff Eastman",
        "Created": "09/Mar/12 20:03",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "01/Apr/12 15:51",
        "Description": "Refactor the current K-means implementation to use the ClusterIterator/Classifier implementation. This will replace the mapper, combiner, reducer, clusterer and many unit tests but will not modify the other driver APIs, thus retaining compatibility with existing CLI.",
        "Issue Links": []
    },
    "MAHOUT-989": {
        "Key": "MAHOUT-933 Implement mapreduce version of ClusterIterator",
        "Summary": "Convert fuzzy-K-means buildClusters to use new ClusterIterator",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Jeff Eastman",
        "Created": "09/Mar/12 20:05",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "01/Apr/12 15:50",
        "Description": "Refactor the current fuzzy-K-means implementation to use the ClusterIterator/Classifier implementation. This will replace the mapper, combiner, reducer, clusterer and many unit tests but will not modify the other driver APIs, thus retaining compatibility with existing CLI.",
        "Issue Links": []
    },
    "MAHOUT-990": {
        "Key": "MAHOUT-933 Implement mapreduce version of ClusterIterator",
        "Summary": "Convert Dirichlet buildClusters to use new ClusterIterator",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Jeff Eastman",
        "Created": "09/Mar/12 20:07",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/May/12 22:04",
        "Description": "Refactor the current Dirichlet implementation to use the ClusterIterator/Classifier implementation. This will replace the mapper, combiner, reducer, clusterer and many unit tests but will not modify the other driver APIs, thus retaining compatibility with existing CLI.",
        "Issue Links": []
    },
    "MAHOUT-991": {
        "Key": "MAHOUT-933 Implement mapreduce version of ClusterIterator",
        "Summary": "Convert Canopy, MeanShift, K-means, Dirichlet, Fuzzy KMeans and Other Tools to emit ClusterWritable",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Jeff Eastman",
        "Created": "09/Mar/12 20:10",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "24/Mar/12 03:40",
        "Description": "Adjust the Canopy, MeanShift, K-means, Dirichlet and Fuzzy KMeans implementations to emit ClusterWritables instead of Clusters. Adjust the other clustering tools (ClusterDumper and ClusterEvaluators) to accept ClusterWritables produced by these algorithms.\nThe new ClusterIterator and ClusterClassifier uses an expanded sequence file representation that stores Clusters as self-describing ClusterWritable objects. So, once all of these algorithms will start emitting ClusterWritables, then KMeans, Dirichlet and FuzzyK will be able to use ClusterIterator and ClusterClassifier for buildClusters phase.",
        "Issue Links": []
    },
    "MAHOUT-992": {
        "Key": "MAHOUT-992",
        "Summary": "Audit DistributedCache use to support EMR",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Tom Pierce",
        "Created": "12/Mar/12 19:27",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 20:52",
        "Description": "Apparently some of our DistributedCache use is not EMR-safe.  It would be great if someone could audit our uses of DC, and fix up this problem where it exists.\nFor an example of problematic usage (and the fix), see MAHOUT-980.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1247"
        ]
    },
    "MAHOUT-993": {
        "Key": "MAHOUT-993",
        "Summary": "Some vector dumper flags are expecting arguments.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Tom Pierce",
        "Created": "14/Mar/12 22:49",
        "Updated": "14/Apr/15 22:36",
        "Resolved": "02/Jun/13 13:25",
        "Description": "I ran VectorDumper from the command line like this:\n$MAHOUT_HOME/bin/mahout vectordump -i ${HDFS_VECTS} --csv -p > ${LOCAL_VECTS}\nI've used this command before to dump similar vectors, but I'm now getting missing argument errors:\nERROR common.AbstractJob: Missing value(s) --printKey\nYou can add an argument after each of these flags to work around this problem for now:\n$MAHOUT_HOME/bin/mahout vectordump -i ${INPATH} --csv x -p x > ${LOCAL_OUT}\nI encountered this on the trunk, but I think it's likely this problem went out in 0.6; I can check in the next fews days if nobody does it first.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1690"
        ]
    },
    "MAHOUT-994": {
        "Key": "MAHOUT-994",
        "Summary": "mahout script shouldn't rely on HADOOP_HOME since that was deprecated in all major Hadoop branches",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Integration",
        "Assignee": "Tom Pierce",
        "Reporter": "Roman Shaposhnik",
        "Created": "21/Mar/12 16:56",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "02/May/12 18:57",
        "Description": "Mahout should follow the Pig and Hive example and not rely explicitly on HADOOP_HOME and HADOOP_CONF_DIR",
        "Issue Links": []
    },
    "MAHOUT-995": {
        "Key": "MAHOUT-995",
        "Summary": "Make synthetic control examples work with Hadoop 0.23",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Bilung Lee",
        "Created": "28/Mar/12 00:52",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "03/Jul/12 22:35",
        "Description": "Current synthetic control examples do not really print out the final cluster due to the missing suffix in the cluster name.\nThis causes exception under Hadoop 0.23.",
        "Issue Links": []
    },
    "MAHOUT-996": {
        "Key": "MAHOUT-996",
        "Summary": "Support NamedVectors in arff.vector job by convention",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Harbick",
        "Created": "29/Mar/12 22:21",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "27/Jan/14 01:58",
        "Description": "If you do something like:\nMAHOUT_LOCAL=1 $MAHOUT_HOME/bin/mahout arff.vector --input $PWD/file.arff --dictOut file.bindings --output $PWD\nMAHOUT_LOCAL=1 $MAHOUT_HOME/bin/mahout kmeans --input $PWD/file.arff.mvc --clusters $PWD/output/file.clusters --output $PWD/output --numClusters 3 --maxIter 1000 --clustering\nMAHOUT_LOCAL=1 $MAHOUT_HOME/bin/mahout clusterdump -seqFileDir $PWD/output/clusters*-final --pointsDir $PWD/output/clusteredPoints --output $PWD/output/clusteranalyze.txt\nCurrently you don't get any information out of clusterdump that helps you identify which element from your source data is in which cluster.\nI did an patch for illustration of using an attribute (by convention) from the ARFF file as the name for a NamedVector.  The result of clusterdump is much easier to use:\nVL-18589\n{n=6165 c=[1.376, 879.144, 3.947, 10.691, 0.874, 1.266, 16.644, 9.689, 2.207, 1.855] r=[0.484, 160.571, 1.959, 6.176, 0.551, 0.442, 34.125, 7.953, 1.988, 0.352]}\n        Weight : [props - optional]:  Point:\n        1.0: 4ee342afd04516354c000140 = [1.000, 597.000, 7.000, 7.000, 1.000, 1.000, 11.000, 12.000, 6.000, 2.000]\n        1.0: 4ee49257eb8b3e28c60025a2 = [1.000, 597.000, 1.000, 7.000, 1.000, 1.000, 8.000, 17.000, 6.000, 2.000]\n        1.0: 4ee60430ab2c714006000937 = [1.000, 597.000, 2.000, 9.000, 1.000, 1.000, 21.000, 21.000, 2.000, 2.000]\n        1.0: 4ef2d580ab2c71231b0019ae = [0:1.000, 1:598.000, 2:5.000, 3:3.000, 5:1.000, 6:4.000, 9:1.000]\n        1.0: 4eda14a30b5d3e655b0043e9 = [1.000, 599.000, 7.000, 8.000, 2.000, 1.000, 15.000, 7.000, 3.000, 2.000]\n        1.0: 4edba62deb8b3e27e6000614 = [0:1.000, 1:599.000, 2:1.000, 3:12.000, 4:1.000, 5:1.000, 6:3.000, 8:3.000, 9:2.000]\n        1.0: 4ede1ea6eb8b3e1f330050f4 = [0:1.000, 1:599.000, 2:3.000, 3:9.000, 4:1.000, 5:1.000, 6:14.000, 7:20.000, 9:2.000]\n...\nI haven't done serious Java in 15 years so the attached patch is just for idea sake...\nThanks,\nAndy",
        "Issue Links": []
    },
    "MAHOUT-997": {
        "Key": "MAHOUT-997",
        "Summary": "Make splitData smart enough to not consider a CSV header to be part of the data",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Andrew Harbick",
        "Created": "30/Mar/12 17:57",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jun/12 07:09",
        "Description": "If you do something like:\nMAHOUT_LOCAL=1 $MAHOUT_HOME/bin/mahout splitDataset  --input all.csv --output split --trainingPercentage 0.9 --probePercentage 0.1\nThe header row from your CSV will end up with 90% chance in your training data and 10% chance in your evaluation data.  To use a tool like trainlogistic or runlogistic the header file is needed in both.\nPerhaps add an argument to splitData to duplicate the header line?",
        "Issue Links": []
    },
    "MAHOUT-998": {
        "Key": "MAHOUT-998",
        "Summary": "Fix up the cluster-reuters.sh script",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Grant Ingersoll",
        "Created": "08/Apr/12 12:21",
        "Updated": "31/May/13 12:30",
        "Resolved": "31/May/13 12:30",
        "Description": "ClusterDumper is being called incorrectly in cluster-reuters.sh",
        "Issue Links": [
            "/jira/browse/MAHOUT-1024"
        ]
    },
    "MAHOUT-999": {
        "Key": "MAHOUT-999",
        "Summary": "KMeans failing to create correct Clustering Policy",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Grant Ingersoll",
        "Created": "09/Apr/12 12:42",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "11/Apr/12 07:33",
        "Description": "Jenkins has been complaining about the cluster-reuters.sh example for a while and the issue appears to be due to trying to write a file to a directory that doesn't exist due to the fact that it is a file, not a directory.\n\n\nException in thread \"main\" java.io.IOException: Mkdirs failed to create /tmp/mahout-work-grantingersoll/reuters-kmeans-clusters/part-randomSeed/clusters-0\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:366)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:528)\n\tat org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:843)\n\tat org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:831)\n\tat org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:823)\n\tat org.apache.mahout.clustering.classify.ClusterClassifier.writePolicy(ClusterClassifier.java:232)\n\tat org.apache.mahout.clustering.classify.ClusterClassifier.writeToSeqFiles(ClusterClassifier.java:185)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.buildClusters(KMeansDriver.java:254)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:154)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:104)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.main(KMeansDriver.java:48)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\n\n\nRun examples/bin/cluster-reuters.sh and choose 1 (KMeans) to see the error.",
        "Issue Links": []
    },
    "MAHOUT-1000": {
        "Key": "MAHOUT-1000",
        "Summary": "Implementation of Single Sample T-Test using Map Reduce/Mahout",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Dev Lakhani",
        "Created": "20/Apr/12 18:25",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "11/Mar/13 15:39",
        "Description": "Implement a map/reduce version of the single sample t test to test whether a sample of n subjects comes from a population in which the mean equals a particular value.\nFor a large dataset, say n millions of rows, one can test whether the sample (large as it is) comes from the population mean.\nInput:\n1) specified population mean to be tested against\n2) hypothesis direction : i.e. \"two.sided\", \"less\", \"greater\".\n3) confidence level or alpha\n4) flag to indicate paired or not paired\nThe procedure is as follows:\n1. Use Map/Reduce to calculate the mean of the sample.\n2. Use Map/Reduce to calculate standard error of the population mean.\n3. Use Map/Reduce to calculate the t statistic\n4. Estimate the degrees of freedom depending on equal sample variances \nOutput\n1) The value of the t-statistic.\n2) The p-value for the test.\n3) Flag that is true if the null hypothesis can be rejected with confidence 1 - alpha; false otherwise.\nReferences\nhttp://www.basic.nwu.edu/statguidefiles/ttest_unpaired_ass_viol.html",
        "Issue Links": []
    },
    "MAHOUT-1001": {
        "Key": "MAHOUT-1001",
        "Summary": "Performance improvement in recommenditembased",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Bhaskar Devireddy",
        "Created": "25/Apr/12 15:32",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "25/Apr/12 16:48",
        "Description": "While running the recommendations with ASFEMail dataset using the example script provided with mahout, we noticed that execution time for unsymmetrify mapper is very long.  While profiling the task we noticed a hotspot consuming high CPU cycles.  Please find the attached patch addressing issue and optimizes the unsymmetrify mapper class.  This patch while retaining functionality(verified the output with and without patch) speeds up the unsymmetrify mapper by more then 5X on x86 architectures.",
        "Issue Links": []
    },
    "MAHOUT-1002": {
        "Key": "MAHOUT-1002",
        "Summary": "Update documentation to reflect minimum Maven version required to build",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Sean R. Owen",
        "Reporter": "Nick Grimshaw",
        "Created": "27/Apr/12 14:12",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 08:55",
        "Description": "The wiki page for building Mahout suggests that Maven 2.0.11 is required, but attempting to build with that version fails with the error:\nError resolving version for 'org.apache.maven.plugins:maven-site-plugin': Plugin requires Maven version 2.2.0",
        "Issue Links": []
    },
    "MAHOUT-1003": {
        "Key": "MAHOUT-1003",
        "Summary": "Vectordumper --help output breaks on --seqFile line",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Pawe\u0142 Lesiecki",
        "Created": "27/Apr/12 17:39",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 09:38",
        "Description": "mahout/bin/mahout vectordump --help output looks well till --seqFile, then it goes like below\n...\nOptions                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                \n  --seqFile (-s) seqFile                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              T\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      h\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      e\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      S\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      e",
        "Issue Links": []
    },
    "MAHOUT-1004": {
        "Key": "MAHOUT-1004",
        "Summary": "Distributed User-based Collaborative Filtering",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Kris Jack",
        "Created": "01/May/12 10:23",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "02/Mar/14 20:05",
        "Description": "I'd like to contribute code that implements a distributed user-based collaborative filtering algorithm.\nIn brief, so far I've taken the code for the existing org.apache.mahout.cf.taste.hadoop.item.RecommenderJob and created a new org.apache.mahout.cf.taste.hadoop.user.RecommenderJob.  With help from Sean Owen, I followed a similar approach to the item-based implementation, but multiplied a user-user matrix with a user-item vector rather than an item-item matrix with an item-user vector.  The result of the multiplication then needs to be transposed in order to output recommendations by user id.",
        "Issue Links": []
    },
    "MAHOUT-1005": {
        "Key": "MAHOUT-1005",
        "Summary": "Nuke Colt math functions that remain untested - The Matrix edition",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5,                                            0.6",
        "Fix Version/s": "0.7",
        "Component/s": "Math",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "01/May/12 23:38",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "05/May/12 23:47",
        "Description": "We are down to very few references to Colt's matrix capabilities.\nI have implemented a new eigen-solver (with tests) and updated the Lanczos code to use it.  I also have nuked all of the DoubleMatrix*d and related classes and consolidated several others.",
        "Issue Links": []
    },
    "MAHOUT-1006": {
        "Key": "MAHOUT-1006",
        "Summary": "Example from book no longer works - prepare20newsgroups broken with Lucene upgrade",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Ted Dunning",
        "Created": "03/May/12 18:10",
        "Updated": "07/Aug/12 17:28",
        "Resolved": "04/Jun/12 16:39",
        "Description": "The StandardAnalyzer from Lucene no longer has a no-args constructor.  Our code uses reflection to create this class, but looks for a no-args constructor and that causes this:\n\n./bin/mahout prepare20newsgroups -p 20news-bydate-train/ -o 20news-train/ -a org.apache.lucene.analysis.standard.StandardAnalyzer -c UTF-8  \nMAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\nno HADOOP_HOME set, running locally\nUnable to find a $JAVA_HOME at \"/usr\", continuing with system-provided Java...\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/Users/hadoop/mahout/examples/target/mahout-examples-0.7-SNAPSHOT-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/hadoop/mahout/examples/target/dependency/slf4j-jcl-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/hadoop/mahout/examples/target/dependency/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nException in thread \"main\" java.lang.IllegalStateException: java.lang.NoSuchMethodException: org.apache.lucene.analysis.standard.StandardAnalyzer.<init>()\n\tat org.apache.mahout.common.ClassUtils.instantiateAs(ClassUtils.java:68)\n\tat org.apache.mahout.common.ClassUtils.instantiateAs(ClassUtils.java:28)\n\tat org.apache.mahout.classifier.bayes.PrepareTwentyNewsgroups.main(PrepareTwentyNewsgroups.java:89)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:188)\nCaused by: java.lang.NoSuchMethodException: org.apache.lucene.analysis.standard.StandardAnalyzer.<init>()\n\tat java.lang.Class.getConstructor0(Class.java:2706)\n\tat java.lang.Class.getConstructor(Class.java:1657)\n\tat org.apache.mahout.common.ClassUtils.instantiateAs(ClassUtils.java:62)\n\t... 9 more\n\n\nThis is really bad.",
        "Issue Links": []
    },
    "MAHOUT-1007": {
        "Key": "MAHOUT-1007",
        "Summary": "Performance improvement in recommenditembased by splitting long records",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Bhaskar Devireddy",
        "Created": "04/May/12 19:47",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "14/May/12 16:35",
        "Description": "While running the recommendations with ASFEMail dataset using the example script provided with mahout, we are noticing that one of the map task in unsymmetrify mapper job has a very long execution time than others.  While profiling, the problem seems to be with the number of elements in each record.  The attached patch address this issue by splitting longer records into smaller once, so the data distributed evenly among the unsymmetrify map tasks.\nThere is a new command line option maxSimilarityReducerVectorSize is introduced for RecommanderJob.  Tested with maxSimilarityReducerVectorSize=5000 and with same functionality speeds up unsymmetrify mapper job by several X on x86 architectures and increases CPU utilization.  By default the records are not split and setting the command line option maxSimilarityReducerVectorSize to a value greater than 0 will increase performance.",
        "Issue Links": []
    },
    "MAHOUT-1008": {
        "Key": "MAHOUT-1008",
        "Summary": "Remove link analysis package",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "08/May/12 06:38",
        "Updated": "31/Mar/15 22:48",
        "Resolved": "10/May/12 21:24",
        "Description": "As a first step to removing code that is barely used in Mahout, I suggest to remove the linkanalysis code (PageRank).",
        "Issue Links": []
    },
    "MAHOUT-1009": {
        "Key": "MAHOUT-1009",
        "Summary": "Remove old LDA implementation from codebase",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Jake Mannix",
        "Created": "08/May/12 17:40",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "03/Jun/12 18:43",
        "Description": "The old LDA is unmaintained and unsupported.  We already (since 0.6) have a newer, faster version in the o.a.m.clustering.lda.cvb package, which I'm actively working on and using in production at Twitter.  We should delete the old o.a.m.clustering.lda codebase.\nNormally, I'd say that we should at the same time promote o.a.m.clustering.lda.cvb up a package-level, but that would cause some serious merge conflicts on my GitHub branch (with updates/improvements/new features targetted for 0.8), so we can get users on this new code by simply changing the driver.classes.props to have \"lda\" point to CVB0Driver as the main().\nOne thing which goes away entirely, is the LDAPrintTopics class, but it's replaced by simply doing VectorDumper with the -sort option on the model files, which is more standard anyways.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1024",
            "/jira/browse/MAHOUT-399"
        ]
    },
    "MAHOUT-1010": {
        "Key": "MAHOUT-1010",
        "Summary": "Remove the old naive bayes implementation (org.apache.mahout.classifier.bayes) from the codebase",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "08/May/12 18:30",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "11/May/12 09:15",
        "Description": "Remove the old naive bayes implementation. We have already have a reworked implementation of NB at org.apache.mahout.classifier.naivebayes.",
        "Issue Links": []
    },
    "MAHOUT-1011": {
        "Key": "MAHOUT-1011",
        "Summary": "RecommenderJob is ignoring the command line threshold parameter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Bhaskar Devireddy",
        "Created": "10/May/12 19:03",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "10/May/12 19:44",
        "Description": "RecommenderJob is ignoring the command line value for threshold parameter.  The attached small patch fixes the issue.",
        "Issue Links": []
    },
    "MAHOUT-1012": {
        "Key": "MAHOUT-1012",
        "Summary": "Remove watchmaker from codebase",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "11/May/12 07:22",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "12/May/12 21:50",
        "Description": "As suggested on the mailinglist, we aim to remove org.apache.mahout.ga.watchmaker to make Mahout leaner. I will remove this from the code base in two days if there are no objections.",
        "Issue Links": []
    },
    "MAHOUT-1013": {
        "Key": "MAHOUT-1013",
        "Summary": "The tests for org.apache.mahout.math.stats.entropy should not write to the project home but to a temp directory",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "11/May/12 09:22",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "11/May/12 11:13",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1014": {
        "Key": "MAHOUT-1010 Remove the old naive bayes implementation (org.apache.mahout.classifier.bayes) from the codebase",
        "Summary": "Recreate Newsgroups example using naivebayes package",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "13/May/12 18:31",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "04/Jun/12 14:09",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1015": {
        "Key": "MAHOUT-1015",
        "Summary": "Precondition check in IRStatisticsImpl broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.7",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Robert J\u00e4schke",
        "Created": "16/May/12 09:58",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "16/May/12 10:36",
        "Description": "The precondition check in the constructor of org.apache.mahout.cf.taste.impl.eval.IRStatisticsImpl includes Copy&Paste errors:\nPreconditions.checkArgument(fallOut >= 0.0 && fallOut <= 1.0, \"Illegal fallOut: \" + fallOut);\nPreconditions.checkArgument(fallOut >= 0.0 && fallOut <= 1.0, \"Illegal nDCG: \" + ndcg);\nPreconditions.checkArgument(reach >= 0.0 && reach <= 1.0, \"Illegal reach: \" + ndcg);\nThe second line shows a message for nDCG but checks the fallOut, the third line checks the reach but prints out the nDCG.\nThis error is also in the current version from the SVN (http://svn.apache.org/viewvc/mahout/trunk/core/src/main/java/org/apache/mahout/cf/taste/impl/eval/IRStatisticsImpl.java?revision=1213930&view=markup)",
        "Issue Links": []
    },
    "MAHOUT-1016": {
        "Key": "MAHOUT-1016",
        "Summary": "running clusterControlDataWithMeanShift against Hadoop 2.0.0 RC resulted in ClassCastException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Examples",
        "Assignee": "Jeff Eastman",
        "Reporter": "Roman Shaposhnik",
        "Created": "18/May/12 00:02",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "13/Jun/12 13:57",
        "Description": "$ mahout org.apache.mahout.clustering.syntheticcontrol.meanshift.Job\n.....\n12/05/17 13:07:06 INFO mapreduce.Job: Task Id : attempt_1337269859451_0048_m_000000_0, Status : FAILED\nError: java.lang.ClassCastException: org.apache.mahout.clustering.meanshift.MeanShiftCanopy cannot be cast to org.apache.mahout.clustering.iterator.ClusterWritable\n\tat org.apache.mahout.clustering.meanshift.MeanShiftCanopyMapper.map(MeanShiftCanopyMapper.java:31)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:725)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:152)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:147)\n\n\nHere's a failed Bigtop test report for more details (make sure to scroll to where the shell trace is given):\nhttp://bigtop01.cloudera.org:8080/view/Test/job/SmokeCluster/lastCompletedBuild/testReport/org.apache.bigtop.itest.mahout.smoke/TestMahoutExamples/clusterControlDataWithMeanShift/",
        "Issue Links": []
    },
    "MAHOUT-1017": {
        "Key": "MAHOUT-1017",
        "Summary": "clusterControlDataWithCanopy, clusterControlDataWithFuzzyKMeans, clusterControlDataWithDirichle examples are looking for output in the wrong place",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Examples",
        "Assignee": "Jeff Eastman",
        "Reporter": "Roman Shaposhnik",
        "Created": "18/May/12 00:06",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "12/Jun/12 15:32",
        "Description": "When executed against Hadoop 2.0.0 RC the tests fails like this:\n\n$ mahout org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job\n...\nCaused by: java.io.FileNotFoundException: File output/clusters-0 does not exist.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:365)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1279)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1319)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:70)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)\n\n\n\n$ mahout org.apache.mahout.clustering.syntheticcontrol.dirichlet.Job\n.....\nCaused by: java.io.FileNotFoundException: File output/clusters-0 does not exist.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:365)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1279)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1319)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:70)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)\n\t... 17 more\n\n\n\n$ mahout org.apache.mahout.clustering.syntheticcontrol.canopy.Job\n....\nCaused by: java.io.FileNotFoundException: File output/clusters-0 does not exist.\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:365)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1279)\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1356)\n\tat org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:1486)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1441)\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1419)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:68)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)\n\t... 17 more\n\n\nFor more details please look at the Bigtop test failures:\nhttp://bigtop01.cloudera.org:8080/view/Test/job/SmokeCluster/lastCompletedBuild/testReport/org.apache.bigtop.itest.mahout.smoke/",
        "Issue Links": []
    },
    "MAHOUT-1018": {
        "Key": "MAHOUT-1018",
        "Summary": "LDADriver fails with -overwrite option under Hadoop 0.23",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Bilung Lee",
        "Created": "18/May/12 18:53",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 15:19",
        "Description": "Because of the change in HADOOP-8131, Hadoop will throw exception for nonexistent directory.  As a result, the output directory needs to be created as needed when \"mahout lda\" remove it with \"-overwrite\" option.",
        "Issue Links": []
    },
    "MAHOUT-1019": {
        "Key": "MAHOUT-1019",
        "Summary": "VectorDistanceSimilarityJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Timothy Potter",
        "Created": "21/May/12 20:18",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 11:15",
        "Description": "The VectorDistanceSimilarityJob is a fantastic tool, but poses the risk of creating terabytes of output of dubious value. For example, I have ~10K seed vectors and millions of vectors to compute the similarity between so I would like to add an optional parameter to this job to specify a maximum distance threshold that prevents any distances above this value from being written to the output. The default would be 1.0d so no filtering is applied which ensures backwards compatibility, but if supplied, only rows where the distance is less than the threshold would be output from the mapper. This can help reduce the storage requirements of the output immensely. Probably name the parameter something like: noOutputIfDistanceGreaterThan",
        "Issue Links": []
    },
    "MAHOUT-1020": {
        "Key": "MAHOUT-1020",
        "Summary": "The Cluster Evaluator is returning bad results",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Jeff Eastman",
        "Reporter": "Pat Ferrel",
        "Created": "23/May/12 23:15",
        "Updated": "11/Jul/12 20:18",
        "Resolved": "03/Jun/12 17:04",
        "Description": "Conversation with between Pat Ferrel and Jeff Eastman on the user list\nHi Pat,\nI don't have a good answer here. Evidently, something in CDbw has become broken and you are the first to notice. When I run TestCDbwEvaluator, the values for k-means and fuzzy-k are clearly incorrect. The values for Canopy, MeanShift and Dirichlet are not so obviously incorrect but I remain suspicious. Something must have become broken in the recent clustering refactoring.\nFrom the method CDbwEvaluator.invalidCluster comment (used to enable pruning):\n\nReturn if the cluster is valid. Valid clusters must have more than 2 representative points,\nand at least one of them must be different than the cluster center. This is because the\nrepresentative points extraction will duplicate the cluster center if it is empty.\n\nOddly enough, inspection of the test log indicates that only k-means and fuzzy-k are not pruning clusters. Clearly some more investigation is needed. I will take a look at it tomorrow. In the mean time if you develop any additional insight please do share it with us.\nThanks,\nJeff\nOn 5/17/12 3:53 PM, Pat Ferrel wrote:\n> I built a tool that iterates through a list of values for k on the same data and spits out the CDbw and ClusterEvaluator results each time.\n>\n> When the evaluator or CDbw prunes a cluster, how do I interpret that? They seem to throw out the same clusters on a given run. Also CDbw always returns an inter-cluster density of 0?",
        "Issue Links": []
    },
    "MAHOUT-1021": {
        "Key": "MAHOUT-1021",
        "Summary": "Blank csv input file given to Canopy/Kmeans clustering",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Nabarun Sengupta",
        "Created": "30/May/12 04:57",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "28/Jan/14 23:25",
        "Description": "Hi,\nThis is regarding a bug that we observed in Canopy clustering. We could reflect the same in Kmeans too. Given a blank csv input file, we observe the algorithm executes two jobs, during the third job execution, it throws an error. When I tried to execute a malformed csv file with decimal or characters, I received an error during the first job itself. Therefore, I feel the same validation should be done if the input file is blank and exception should be thrown during the first job execution.\nFollowing is the job execution details:\nApps\\dist\\mahout\\examples\\bin>build-cluster-syntheticcontrol.cmd\nease select a number to choose the corresponding clustering algorithm\"\n canopy clustering\"\n kmeans clustering\"\n fuzzykmeans clustering\"\n dirichlet clustering\"\n meanshift clustering\"\ner your choice:1\n. You chose 1 and we'll use canopy Clustering\"\nS is healthy... \"\nloading Synthetic control data to HDFS\"\neted hdfs://10.114.251.23:9000/user/milind/testdata\nccessfully Uploaded Synthetic control data to HDFS \"\nnning on hadoop, using HADOOP_HOME=c:\\Apps\\dist\"\nApps\\dist\\bin\\hadoop jar c:\\Apps\\dist\\mahout\\mahout-examples-0.5-job.jar org.apache.mahout.driver.MahoutDriver org.apache.mah\nontrol.canopy.Job\n05/17 10:46:11 WARN driver.MahoutDriver: No org.apache.mahout.clustering.syntheticcontrol.canopy.Job.props found on classpath\nrguments only\n05/17 10:46:11 INFO canopy.Job: Running with default arguments\n05/17 10:46:12 INFO common.HadoopUtil: Deleting output\n05/17 10:46:12 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool\n05/17 10:46:13 INFO input.FileInputFormat: Total input paths to process : 1\n05/17 10:46:14 INFO mapred.JobClient: Running job: job_201205170655_0017\n05/17 10:46:15 INFO mapred.JobClient:  map 0% reduce 0%\n05/17 10:46:48 INFO mapred.JobClient:  map 100% reduce 0%\n05/17 10:46:59 INFO mapred.JobClient: Job complete: job_201205170655_0017\n05/17 10:46:59 INFO mapred.JobClient: Counters: 15\n05/17 10:46:59 INFO mapred.JobClient:   Job Counters\n05/17 10:46:59 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=29672\n05/17 10:46:59 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0\n05/17 10:46:59 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0\n05/17 10:46:59 INFO mapred.JobClient:     Launched map tasks=1\n05/17 10:46:59 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0\n05/17 10:46:59 INFO mapred.JobClient:   File Output Format Counters\n05/17 10:46:59 INFO mapred.JobClient:     Bytes Written=90\n05/17 10:46:59 INFO mapred.JobClient:   FileSystemCounters\n05/17 10:46:59 INFO mapred.JobClient:     FILE_BYTES_READ=130\n05/17 10:46:59 INFO mapred.JobClient:     HDFS_BYTES_READ=134\n05/17 10:46:59 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=21557\n05/17 10:46:59 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=90\n05/17 10:46:59 INFO mapred.JobClient:   File Input Format Counters\n05/17 10:46:59 INFO mapred.JobClient:     Bytes Read=0\n05/17 10:46:59 INFO mapred.JobClient:   Map-Reduce Framework\n05/17 10:46:59 INFO mapred.JobClient:     Map input records=0\n05/17 10:46:59 INFO mapred.JobClient:     Spilled Records=0\n05/17 10:46:59 INFO mapred.JobClient:     Map output records=0\n05/17 10:46:59 INFO mapred.JobClient:     SPLIT_RAW_BYTES=134\n05/17 10:46:59 INFO canopy.CanopyDriver: Build Clusters Input: output/data Out: output Measure: org.apache.mahout.common.dist\nsure@6eedf759 t1: 80.0 t2: 55.0\n05/17 10:46:59 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool\n05/17 10:46:59 INFO input.FileInputFormat: Total input paths to process : 1\n05/17 10:47:00 INFO mapred.JobClient: Running job: job_201205170655_0018\n05/17 10:47:01 INFO mapred.JobClient:  map 0% reduce 0%\n05/17 10:47:33 INFO mapred.JobClient:  map 100% reduce 0%\n05/17 10:47:51 INFO mapred.JobClient:  map 100% reduce 100%\n05/17 10:48:02 INFO mapred.JobClient: Job complete: job_201205170655_0018\n05/17 10:48:02 INFO mapred.JobClient: Counters: 25\n05/17 10:48:02 INFO mapred.JobClient:   Job Counters\n05/17 10:48:02 INFO mapred.JobClient:     Launched reduce tasks=1\n05/17 10:48:02 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=30327\n05/17 10:48:02 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0\n05/17 10:48:02 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0\n05/17 10:48:02 INFO mapred.JobClient:     Launched map tasks=1\n05/17 10:48:02 INFO mapred.JobClient:     Data-local map tasks=1\n05/17 10:48:02 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=16031\n05/17 10:48:02 INFO mapred.JobClient:   File Output Format Counters\n05/17 10:48:02 INFO mapred.JobClient:     Bytes Written=95\n05/17 10:48:02 INFO mapred.JobClient:   FileSystemCounters\n05/17 10:48:02 INFO mapred.JobClient:     FILE_BYTES_READ=396\n05/17 10:48:02 INFO mapred.JobClient:     HDFS_BYTES_READ=217\n05/17 10:48:02 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=45263\n05/17 10:48:02 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=95\n05/17 10:48:02 INFO mapred.JobClient:   File Input Format Counters\n05/17 10:48:02 INFO mapred.JobClient:     Bytes Read=90\n05/17 10:48:02 INFO mapred.JobClient:   Map-Reduce Framework\n05/17 10:48:02 INFO mapred.JobClient:     Reduce input groups=0\n05/17 10:48:02 INFO mapred.JobClient:     Map output materialized bytes=6\n05/17 10:48:02 INFO mapred.JobClient:     Combine output records=0\n05/17 10:48:02 INFO mapred.JobClient:     Map input records=0\n05/17 10:48:02 INFO mapred.JobClient:     Reduce shuffle bytes=0\n05/17 10:48:02 INFO mapred.JobClient:     Reduce output records=0\n05/17 10:48:02 INFO mapred.JobClient:     Spilled Records=0\n05/17 10:48:02 INFO mapred.JobClient:     Map output bytes=0\n05/17 10:48:02 INFO mapred.JobClient:     Combine input records=0\n05/17 10:48:02 INFO mapred.JobClient:     Map output records=0\n05/17 10:48:02 INFO mapred.JobClient:     SPLIT_RAW_BYTES=127\n05/17 10:48:02 INFO mapred.JobClient:     Reduce input records=0\n05/17 10:48:02 WARN mapred.JobClient: Use GenericOptionsParser for parsing the arguments. Applications should implement Tool\n05/17 10:48:03 INFO input.FileInputFormat: Total input paths to process : 1\n05/17 10:48:03 INFO mapred.JobClient: Running job: job_201205170655_0019\n05/17 10:48:04 INFO mapred.JobClient:  map 0% reduce 0%\n05/17 10:48:35 INFO mapred.JobClient: Task Id : attempt_201205170655_0019_m_000000_0, Status : FAILED\na.lang.IllegalStateException: Canopies are empty!\n     at org.apache.mahout.clustering.canopy.ClusterMapper.setup(ClusterMapper.java:81)\n     at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n     at org.Aapache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)\n     at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)\n     at org.apache.hadoop.mapred.Child$4.run(Child.java:266)\n     at java.security.AccessController.doPrivileged(Native Method)\n     at javax.security.auth.Subject.doAs(Subject.java:415)\n     at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n     at org.apache.hadoop.mapred.Child.main(Child.java:260)\nempt_201205170655_0019_m_000000_0: log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdfs.DFSClient).\nempt_201205170655_0019_m_000000_0: log4j:WARN Please initialize the log4j system properly.\n05/17 10:48:53 INFO mapred.JobClient: Task Id : attempt_201205170655_0019_m_000000_1, Status : FAILED\na.lang.IllegalStateException: Canopies are empty!\n     at org.apache.mahout.clustering.canopy.ClusterMapper.setup(ClusterMapper.java:81)\n     at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n     at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)\n     at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)\n     at org.apache.hadoop.mapred.Child$4.run(Child.java:266)\n     at java.security.AccessController.doPrivileged(Native Method)\n     at javax.security.auth.Subject.doAs(Subject.java:415)\n     at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n     at org.apache.hadoop.mapred.Child.main(Child.java:260)\nempt_201205170655_0019_m_000000_1: log4j:WARN No appenders could be found for logger (org.apache.hadoop.hdfs.DFSClient).\nempt_201205170655_0019_m_000000_1: log4j:WARN Please initialize the log4j system properly.\nminate batch job (Y/N)? ^V\nPlease let me know if this issue can be resolved.",
        "Issue Links": []
    },
    "MAHOUT-1022": {
        "Key": "MAHOUT-1022",
        "Summary": "Process Mining Algorithm Example in Mahout",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "John Leach",
        "Created": "31/May/12 03:32",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "02/Mar/14 20:06",
        "Description": "We currently have been implementing process mining algorithms in Hadoop for generating business processes based on event logs.  Here is an example white paper with some of the algorithms defined.\nhttp://cms.ieis.tue.nl/Beta/Files/WorkingPapers/Beta_wp166.pdf\nOur goal is to add a process mining example to the Mahout implementation for others that might be interested in reverse engineering processes from event logs.",
        "Issue Links": []
    },
    "MAHOUT-1023": {
        "Key": "MAHOUT-1023",
        "Summary": "TestFuzzyKmeans is throwing NPE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "03/Jun/12 17:32",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "03/Jun/12 17:55",
        "Description": "<error type=\"java.lang.NullPointerException\">java.lang.NullPointerException\n        at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:117)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:214)\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.&lt;init&gt;(SequenceFileDirValueIterator.java:66)\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)\n        at org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansUtil.configureWithClusterInfo(FuzzyKMeansUtil.java:48)\n        at org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver.buildClusters(FuzzyKMeansDriver.java:270)\n        at org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver.run(FuzzyKMeansDriver.java:221)\n        at org.apache.mahout.clustering.fuzzykmeans.FuzzyKMeansDriver.run(FuzzyKMeansDriver.java:110)\n        at org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering.testFuzzyKMeansSeqJob(TestFuzzyKmeansClustering.java:142)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\n        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\n        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\n        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\n        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\n        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\n        at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)\n        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)\n        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\n        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\n        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\n        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\n        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)",
        "Issue Links": []
    },
    "MAHOUT-1024": {
        "Key": "MAHOUT-1024",
        "Summary": "cluster_reuters.sh still relies on old (now removed) lda implementation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "03/Jun/12 20:35",
        "Updated": "31/May/13 12:30",
        "Resolved": "05/Jun/12 04:31",
        "Description": "Reproduce by running cluster_reuters.sh in examples.\nSeems like the example relies on lda and lda_topics in the mahout driver. The script needs to be changed to rely on the new implementation only.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1009",
            "/jira/browse/MAHOUT-998"
        ]
    },
    "MAHOUT-1025": {
        "Key": "MAHOUT-1025",
        "Summary": "Update documentation for LDA before the release.",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Robin Anil",
        "Created": "04/Jun/12 14:01",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 17:27",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1026": {
        "Key": "MAHOUT-1026",
        "Summary": "Add LDA (CVB implementation) to the cluster_reuters.sh example script",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "05/Jun/12 04:33",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "08/Jun/13 16:27",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1027": {
        "Key": "MAHOUT-1027",
        "Summary": "Change to latest lucene version",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Robin Anil",
        "Created": "05/Jun/12 11:42",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "05/Jun/12 12:15",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1028": {
        "Key": "MAHOUT-1028",
        "Summary": "seq2sparse n-gram weighting creates malformed vectors which crashes kmeans",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Pat Ferrel",
        "Created": "05/Jun/12 19:15",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "09/Jun/12 00:47",
        "Description": "I think I found the root but not sure what needs fixing.\nI took out n-gram generation and the vector now looks like this:\nKey: https://farfetchers.com/category/collections/source/brice-berard:\nValue: https://farfetchers.com/category/collections/source/brice-berard:\n{701:0.5484552974788475,1876:0.6020428878306935,3620:0.5802940184767269}\n\nThis works in clustering.\nIt doesn't seem like a malformed vector should crash clustering (it apparently doesn't in mahout 0.6) but it looks like something in seq2sparse's n-gram weighting does cause a malformed vector.\nI'll file a JIRA\nOn 6/5/12 11:48 AM, Pat Ferrel wrote:\n> Using seqdumper on the TFIDF vectors, that vector is indeed in the list\n> Key: https://farfetchers.com/category/collections/source/brice-berard:\n> Value: https://farfetchers.com/category/collections/source/brice-berard:{\n>\n> Looking in the seqfiles we find the document in part-00005 of 10 in no particular part of the file.\n> Key: https://farfetchers.com/category/collections/source/brice-berard:\n> Value: ::Title::\n> Brice Berard | FarFetchers.com\n> Blog Posts\n>\n> On the chance that this originates in seq2sparse I'll try changing options until the vector looks different. and try clustering again.\n>\n> On 6/5/12 10:43 AM, Pat Ferrel wrote:\n>> I'm not completely sure what I'm looking at but...\n>>\n>> In iterateSeq on iteration #1  of processing vectors/tfidf-vectors it reads\n>> vector = \"https://farfetchers.com/category/collections/source/brice-berard:{\"\n>>\n>> it's a named vector where the  url is the name, the value is \"{\", which looks wrong and when that is classified to get a probability it gets\n>>\n>> probabilities = \"\n{0:NaN,1:NaN,2:NaN,3:NaN,4:NaN,5:NaN,6:NaN,7:NaN,8:NaN,9:NaN,10:NaN,11:NaN,12:NaN,13:NaN,14:NaN,15:NaN,16:NaN,17:NaN,18:NaN,19:NaN}\n\"\n>>\n>> That causes the probabilities.maxValueIndex() = -1 and everything dies.\n>>\n>> vector looks wrong, doesn't it? Truncated?\n>>\n>> I went back to try the same on mahout 0.6 but iterateSeq does not get called though I used -xm sequential on both runs. I can't see kmeans-clusters/clusters-0 being created on mahout 0.6 either. Is that part of the refactoring?\n>>\n>> On 6/4/12 3:07 PM, Pat Ferrel wrote:\n>>> Some things to try:\n>>> - Have you verified the contents of your input vectors actually have data in them?\n>>> * YES, from the other email you know that the data works fine in 0.6\n>>> - Can you run the cluster dumper on the b3/kmeans-clusters/clusters-0 contents?\n>>> * YES, It is attached from trunk's clusterdump after the failure of kmeans, of course. A simple data set fortunately.\n>>> - Is it possible to run the sequential version (-xm sequential)? If it is you could run it in a debugger to gain more insight.\n>>> * YES, will report back.\n>>>\n>>> On 6/4/12 2:19 PM, Jeff Eastman wrote:\n>>>> It looks like the probabilities vector returned by AbstractClusteringPolicy.classify() has no non-zero elements. In this case, AbstractClusteringPolicy.select()'s call to AbstractVector.maxValueIndex() is returning -1 and that is causing the exception.\n>>>>\n>>>> How could this happen? I'm not exactly sure, but consider that the probabilities vector is calculated in AbstractClusteringPolicy.classify() by calling DistanceMeasureCluster.pdf() on each of the prior clusters in b3/kmeans-clusters/clusters-0. With a CosineDistanceMeasure I don't see how this could ever return zero. Certainly, some of your vectors will match the prior cluster centers exactly (they were sampled from the input) and those values would return pdf==1. Even if the cosine distance was 1 the pdf would be 0.5.\n>>>>\n>>>> Some things to try:\n>>>> - Have you verified the contents of your input vectors actually have data in them?\n>>>> - Can you run the cluster dumper on the b3/kmeans-clusters/clusters-0 contents?\n>>>> - Is it possible to run the sequential version (-xm sequential)? If it is you could run it in a debugger to gain more insight.\n>>>>\n>>>> Jeff\n>>>>\n>>>> On 6/4/12 12:05 PM, Pat Ferrel wrote:\n>>>>> Using the CLI to kmeans from several trunk versions I get an error I don't understand.  When the job died the b3/canopy-centroids/clusters-0-final contained the random-seeds file generated by the kmeans driver and the b3/kmeans-clusters/clusters-0 had several part files but b3/kmeans-clusters/clusters-1 was empty. When I look through the code from the trace it doesn't make much sense.\n>>>>>\n>>>>> Command line:\n>>>>> mahout kmeans\n>>>>>   -i b3/vectors/tfidf-vectors/\n>>>>>   -k 20\n>>>>>   -c b3/canopy-centroids/clusters-0-final\n>>>>>   -cl\n>>>>>   -o b3/kmeans-clusters\n>>>>>   -ow\n>>>>>   -cd 0.01\n>>>>>   -x 30\n>>>>>   -dm org.apache.mahout.common.distance.CosineDistanceMeasure\n>>>>>\n>>>>> Error:\n>>>>> 12/06/04 07:55:03 INFO common.AbstractJob: Command line arguments: {--clustering=null, --clusters=[b3/canopy-centroids/clusters-0-final], --convergenceDelta=[0.01], --distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[b3/vectors/tfidf-vectors/], --maxIter=[30], --method=[mapreduce], --numClusters=[20], --output=[b3/kmeans-clusters], --overwrite=null, --startPhase=[0], --tempDir=[temp]}\n>>>>> 2012-06-04 07:55:03.752 java[67308:1903] Unable to load realm info from SCDynamicStore\n>>>>> 12/06/04 07:55:03 INFO common.HadoopUtil: Deleting b3/canopy-centroids/clusters-0-final\n>>>>> 12/06/04 07:55:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n>>>>> 12/06/04 07:55:04 INFO compress.CodecPool: Got brand-new compressor\n>>>>> 12/06/04 07:55:04 INFO kmeans.RandomSeedGenerator: Wrote 20 vectors to b3/canopy-centroids/clusters-0-final/part-randomSeed\n>>>>> 12/06/04 07:55:04 INFO kmeans.KMeansDriver: Input: b3/vectors/tfidf-vectors Clusters In: b3/canopy-centroids/clusters-0-final/part-randomSeed Out: b3/kmeans-clusters Distance: org.apache.mahout.common.distance.CosineDistanceMeasure\n>>>>> 12/06/04 07:55:04 INFO kmeans.KMeansDriver: convergence: 0.01 max Iterations: 30 num Reduce Tasks: org.apache.mahout.math.VectorWritable Input Vectors: {}\n>>>>> 12/06/04 07:55:04 INFO compress.CodecPool: Got brand-new decompressor\n>>>>> Cluster Iterator running iteration 1 over priorPath: b3/kmeans-clusters/clusters-0\n>>>>> 12/06/04 07:55:05 INFO input.FileInputFormat: Total input paths to process : 1\n>>>>> 12/06/04 07:55:05 INFO mapred.JobClient: Running job: job_local_0001\n>>>>> 12/06/04 07:55:06 INFO mapred.MapTask: io.sort.mb = 100\n>>>>> 12/06/04 07:55:08 INFO mapred.MapTask: data buffer = 79691776/99614720\n>>>>> 12/06/04 07:55:08 INFO mapred.MapTask: record buffer = 262144/327680\n>>>>> 12/06/04 07:55:08 INFO mapred.JobClient:  map 0% reduce 0%\n>>>>> 12/06/04 07:55:09 WARN mapred.LocalJobRunner: job_local_0001\n>>>>> org.apache.mahout.math.IndexException: Index -1 is outside allowable range of [0,20)\n>>>>>     at org.apache.mahout.math.AbstractVector.set(AbstractVector.java:439)\n>>>>>     at org.apache.mahout.clustering.iterator.AbstractClusteringPolicy.select(AbstractClusteringPolicy.java:44)\n>>>>>     at org.apache.mahout.clustering.iterator.CIMapper.map(CIMapper.java:52)\n>>>>>     at org.apache.mahout.clustering.iterator.CIMapper.map(CIMapper.java:18)\n>>>>>     at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n>>>>>     at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n>>>>>     at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n>>>>>     at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)\n>>>>> 12/06/04 07:55:09 INFO mapred.JobClient: Job complete: job_local_0001\n>>>>> 12/06/04 07:55:09 INFO mapred.JobClient: Counters: 0\n>>>>> Exception in thread \"main\" java.lang.InterruptedException: Cluster Iteration 1 failed processing b3/kmeans-clusters/clusters-1\n>>>>>     at org.apache.mahout.clustering.iterator.ClusterIterator.iterateMR(ClusterIterator.java:186)\n>>>>>     at org.apache.mahout.clustering.kmeans.KMeansDriver.buildClusters(KMeansDriver.java:229)\n>>>>>     at org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:149)\n>>>>>     at org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:108)\n>>>>>     at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n>>>>>     at org.apache.mahout.clustering.kmeans.KMeansDriver.main(KMeansDriver.java:49)\n>>>>>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n>>>>>     at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n>>>>>     at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n>>>>>     at java.lang.reflect.Method.invoke(Method.java:597)\n>>>>>     at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n>>>>>     at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n>>>>>     at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>>\n>>>>",
        "Issue Links": []
    },
    "MAHOUT-1029": {
        "Key": "MAHOUT-1029",
        "Summary": "User mailing list \"subscribe\" link points to wrong mailing list",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Documentation",
        "Assignee": "Sean R. Owen",
        "Reporter": "Filippo Diotalevi",
        "Created": "06/Jun/12 09:16",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "22/Jun/12 11:40",
        "Description": "In \nhttp://mahout.apache.org/mailinglists.html#Mahout User List\nThe Mahout User List subscribe and unsubscribe links points to\nmahout-user-subscribe@lucene.apache.org\nHowever, when a user tries to subscribe that mailing list, he receives an error email:\n--------------------------------------------------------\nHi. This is the qmail-send program at apache.org.\nI'm afraid I wasn't able to deliver your message to the following addresses.\nThis is a permanent error; I've given up. Sorry it didn't work out.\n<mahout-user-subscribe@lucene.apache.org>:\nThis mailing list has moved to user at mahout.apache.org.\n\u2014 Below this line is a copy of the message.\n......\n--------------------------------------------------------\nThe correct email addresses are\nuser-subscribe@mahout.apache.org\nand\nuser-unsubscribe@mahout.apache.org",
        "Issue Links": []
    },
    "MAHOUT-1030": {
        "Key": "MAHOUT-1030",
        "Summary": "Regression: Clustered Points Should be WeightedPropertyVectorWritable not WeightedVectorWritable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering,                                            Integration",
        "Assignee": "Andrew Musselman",
        "Reporter": "Jeff Eastman",
        "Created": "09/Jun/12 16:08",
        "Updated": "28/Jan/14 16:41",
        "Resolved": "08/Dec/13 04:08",
        "Description": "Looks like this won't make it into this build. Pretty widespread impact on code and tests and I don't know which properties were implemented in the old version. I will create a JIRA and post my interim results.\nOn 6/8/12 12:21 PM, Jeff Eastman wrote:\n> That's a reversion that evidently got in when the new ClusterClassificationDriver was introduced. It should be a pretty easy fix and I will see if I can make the change before Paritosh cuts the release bits tonight.\n>\n> On 6/7/12 1:00 PM, Pat Ferrel wrote:\n>> It appears that in kmeans the clusteredPoints are now written as WeightedVectorWritable where in mahout 0.6 they were WeightedPropertyVectorWritable? This means that the distance from the centroid is no longer stored here? Why? I hope I'm wrong because that is not a welcome change. How is one to order clustered docs by distance from cluster centroid?\n>>\n>> I'm sure I could calculate the distance but that would mean looking up the centroid for the cluster id given in the above WeightedVectorWritable, which means iterating through all the clusters for each clustered doc. In my case the number of clusters could be fairly large.\n>>\n>> Am I missing something?\n>>\n>>\n>",
        "Issue Links": []
    },
    "MAHOUT-1031": {
        "Key": "MAHOUT-1031",
        "Summary": "Drop empty vectors in encoding pipeline",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "09/Jun/12 23:21",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 15:28",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1032": {
        "Key": "MAHOUT-1032",
        "Summary": "AggregateAndRecommendReducer gets OOM in setup() method",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.5,                                            0.6,                                            0.7,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "CodyInnowhere",
        "Created": "15/Jun/12 08:54",
        "Updated": "16/Jun/12 09:35",
        "Resolved": "15/Jun/12 10:53",
        "Description": "This bug is actually caused by the very first job: itemIDIndex. This job transfers itemID to an integer index, and in the later AggregateAndRecommendReducer, tries to read all items to the OpenIntLongHashMap indexItemIDMap. However, for large data sets, e.g., my test data set covers 100million+ items(not too many items for a large e-commerce website), tasks get out of memory in setup() method. I don't think the itemIDIndex is necessary, without this job, the final AggregateAndRecommend step doesn't have to read all items to the memory to do the reverse index mapping.",
        "Issue Links": []
    },
    "MAHOUT-1033": {
        "Key": "MAHOUT-1033",
        "Summary": "Solution for MAHOUT-848 doesn't check for the Configuration object being null before using it.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Timothy Potter",
        "Created": "19/Jun/12 13:56",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jun/12 10:47",
        "Description": "A patch was supplied for MAHOUT-848 but it didn't check for the Configuration object being null before using it, so now Mahout jobs will throw an NPE when run in oozie (which is why I marked this ticket as major).",
        "Issue Links": []
    },
    "MAHOUT-1034": {
        "Key": "MAHOUT-1034",
        "Summary": "ERROR in Navie Bayes Training(update: seqdirectory does not give output)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": "Robin Anil",
        "Reporter": "Leting Wu",
        "Created": "28/Jun/12 23:52",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Jul/12 00:34",
        "Description": "When run either examples/classify-20newsgrouops.sh or ash-email-examples.sh, trainnb always fails:\n\nINFO mapred.JobClient: Task Id : attempt_201206281546_0003_m_000000_0, Status : FAILED\njava.lang.IllegalArgumentException\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:72)\n\tat org.apache.mahout.classifier.naivebayes.training.WeightsMapper.setup(WeightsMapper.java:42)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:647)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:270)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:264)",
        "Issue Links": []
    },
    "MAHOUT-1035": {
        "Key": "MAHOUT-1035",
        "Summary": "Hotspot in recommenditembased \u2013 UnsymmetrifyMapper job",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Bhaskar Devireddy",
        "Created": "29/Jun/12 02:45",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "29/Jun/12 21:51",
        "Description": "While profiling the unsymmetrify mapper job in recommendations we noticed an hotspot consuming 90% of the CPU runtime in org.apache.mahout.math.map.OpenIntDoubleHashMap.keys method for the first map task.  We used the script provided in mahout examples for running ASF Email recommendations for profiling.  The attached patch addresses  the hotspot by reducing the number of for loop iterations in OpenIntDoubleHashMap.keys method by changing the initialization of transposedPartial.  This patch while retaining functionality(verified the output with and without patch) speeds up the unsymmetrify mapper task by more than 4X on x86 architectures.",
        "Issue Links": []
    },
    "MAHOUT-1036": {
        "Key": "MAHOUT-1036",
        "Summary": "I can't run the code in \u300aMahout In Action\u300b under Mahout 0.7,what's happening ?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Roy Guo",
        "Created": "03/Jul/12 02:56",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "03/Jul/12 10:17",
        "Description": "I'm reading the book \u300aMahout In Action\u300b and i'd like to try the code myself.\nBut i found lots of the code can't run(Most of the issues are method can't be found).\n1\u3001RandomSeedGenerator.chooseRandomPoints(pointVectors, k); chooseRandomPoints can't be found.\n2\u3001new Cluster(v, clusterId++, measure); the constructor can't be found.\nthere are many of these kind of issues.\nCan anyone tell me how to find those methods ?",
        "Issue Links": []
    },
    "MAHOUT-1037": {
        "Key": "MAHOUT-1037",
        "Summary": "I can't run the code in \u300aMahout In Action\u300b under Mahout 0.7,what's happening ?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Roy Guo",
        "Created": "03/Jul/12 02:57",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "03/Jul/12 10:18",
        "Description": "I'm reading the book \u300aMahout In Action\u300b and i'd like to try the code myself.\nBut i found lots of the code can't run(Most of the issues are method can't be found).\n1\u3001RandomSeedGenerator.chooseRandomPoints(pointVectors, k); chooseRandomPoints can't be found.\n2\u3001new Cluster(v, clusterId++, measure); the constructor can't be found.\nthere are many of these kind of issues.\nCan anyone tell me how to find those methods ?",
        "Issue Links": []
    },
    "MAHOUT-1038": {
        "Key": "MAHOUT-1038",
        "Summary": "is it a bug of getPreferenceSQL?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jingsi Liu",
        "Created": "04/Jul/12 02:58",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "04/Jul/12 07:49",
        "Description": "@AbstractJDBCDataModel.java\n@public Long getPreferenceTime(long userID, long itemID) throws TasteException \nthere is a prepareStatement of getPreferenceSQL, should it be getPreferenceTimeSQL? because i found you check the getPreferenceTimeSQL null and log for getPreferenceTimeSQL, and this is an function of getPreferenceTime...\ni didn't check relative code, so i am not sure it's a bug or just what you want.",
        "Issue Links": []
    },
    "MAHOUT-1039": {
        "Key": "MAHOUT-1039",
        "Summary": "org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob throws IllegalArgumentException when no usersFile is supplied",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Dan Cartoon",
        "Created": "04/Jul/12 04:55",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "04/Jul/12 07:52",
        "Description": "When trying to run the pseudo.RecommenderJob without a usersFile, there is an IllegalArgumentException:\nException in thread \"main\" java.lang.IllegalArgumentException: Can not create a Path from a null string\n        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:78)\n        at org.apache.hadoop.fs.Path.<init>(Path.java:90)\n        at org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob.run(RecommenderJob.java:122)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.mahout.cf.taste.hadoop.pseudo.RecommenderJob.main(RecommenderJob.java:148)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:601)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nIn addition, providing a usersFile does not appear to cause any filtering to occur.  This can be reproduced running the pseudo.RecommenderJob without a userFile and some basic preferences(I'm happy to provide more specifics here if necessary)\nIt looks like we're taking the wrong action based on this check at RecommenderJob.java:122\nPath usersFile = hasOption(\"usersFile\") ? inputFile : new Path(getOption(\"usersFile\"));\nIf the usersFile option has been specified, it seems like we should be using that, rather than the inputFile, and vice versa.\nI have attached attached a patch, runnable from trunk that swaps the action on line 122.  This appears to fix the issue.",
        "Issue Links": []
    },
    "MAHOUT-1040": {
        "Key": "MAHOUT-1040",
        "Summary": "Simplify RunningAverage suite",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "05/Jul/12 00:40",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "05/Jul/12 08:00",
        "Description": "The RunningAverage suite of classes is not used in the Mahout source base, except for FullRunningAverage.\nWould it be acceptable to the Mahout user base to collapse this feature into one class?",
        "Issue Links": []
    },
    "MAHOUT-1041": {
        "Key": "MAHOUT-1041",
        "Summary": "Support for PMML",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Duraimurugan",
        "Created": "05/Jul/12 20:45",
        "Updated": "16/Jul/15 02:47",
        "Resolved": "01/Jun/13 17:31",
        "Description": "Would like to request a support for PMML. With that once the predictive models are built and provided in PMML format, we should be able to import into hadoop cluster for scoring. This way models built in external (non-mahout) systems can be imported to Hadoop/Mahout for scalable environment.",
        "Issue Links": []
    },
    "MAHOUT-1042": {
        "Key": "MAHOUT-1042",
        "Summary": "Hotspot in RecommenderJob-PartialMultiplyMapper-Reducer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Bhaskar Devireddy",
        "Created": "09/Jul/12 21:31",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "10/Jul/12 15:20",
        "Description": "While profiling PartialMultiplyMapper-Reducer job we noticed a hotspot consuming more than 40% of the CPU time in org.apache.mahout.math.RandomAccessSparseVector.assign method for the reducer task.  We used the script provided in mahout examples for running ASF Email recommendations for profiling. The hotspot is coming from the use of Vector.plus(Vector x) method in AggregateAndRecommendReducerc class.  The pattern used is VectorA = VectorA.plus(VectorB).  In this case VectorA doesn't have to be cloned using assign method.  The attached patch addresses the hotspot by eliminating cloning in the above case for plus and times methods.  This patch while retaining functionality (verified the output with and without patch), speeds up execution time of PartialMultiplyMapper-Reducer job by more than 10X on x86 architectures.",
        "Issue Links": []
    },
    "MAHOUT-1043": {
        "Key": "MAHOUT-1043",
        "Summary": "Support building project with m2e plugin in eclipse",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Sean R. Owen",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "10/Jul/12 00:28",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "10/Jul/12 14:49",
        "Description": "See this and this and this for examples on various issues developers have when trying to import and build Apache Mahout using eclipse and m2e plugin. Build scripts in sources seem to favour Apache Ant and maven-eclipse-plugin, while Apache Maven 3 and m2e are more popular build tool choices nowadays.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1136"
        ]
    },
    "MAHOUT-1044": {
        "Key": "MAHOUT-1044",
        "Summary": "bin/mahout throws NoClassDefFoundError: org/apache/hadoop/util/ProgramDriver",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "CLI",
        "Assignee": "Sean R. Owen",
        "Reporter": "Yann Moisan",
        "Created": "11/Jul/12 14:32",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Jul/12 19:22",
        "Description": "bin/mahout throws NoClassDefFoundError: org/apache/hadoop/util/ProgramDriver\nI think this is due to line 224\n    CLASSPATH=\"${CLASSPATH}:${MAHOUT_HOME/lib/hadoop/*}\"\nI fix the issue by moving the closing brace\n    CLASSPATH=\"${CLASSPATH}:${MAHOUT_HOME}/lib/hadoop/*\"",
        "Issue Links": []
    },
    "MAHOUT-1045": {
        "Key": "MAHOUT-1045",
        "Summary": "Cluster evaluators returning bad results",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Pat Ferrel",
        "Created": "11/Jul/12 20:17",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 17:36",
        "Description": "With real world crawl data the Intra-cluster density from ClusterEvaluator is almost always NaN. The CDbw inter-cluster density is almost always 0. I have also seen several cases where CDbw fails to return any results but have not tracked down why yet.\nI have sent a link to an 8G data set that reproduces these errors to Jeff Eastman.",
        "Issue Links": []
    },
    "MAHOUT-1046": {
        "Key": "MAHOUT-1046",
        "Summary": "Views of matrix views are created incorrectly.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "14/Jul/12 20:02",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "14/Jul/12 20:18",
        "Description": "There are two issues, one benign and one huge.  The benign issue is that all references in MatrixView.viewPart to the constant 0 got incorrectly replaced by ROW.\nThe big issue is that origin.clone() is used instead of this.origin.clone().  This causes double incrementing and an index error for some args.  I have added a test and will commit shortly.",
        "Issue Links": []
    },
    "MAHOUT-1047": {
        "Key": "MAHOUT-1047",
        "Summary": "CVB hangs after completion",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Seth Hendrickson",
        "Created": "20/Jul/12 00:10",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 18:21",
        "Description": "After running the new LDA CVB implementation, it hangs and does not terminate the process like every other time I run Mahout\nTerminal output:\n12/07/19 11:38:49 INFO mapred.LocalJobRunner: \n12/07/19 11:38:49 INFO mapred.Task: Task 'attempt_local_0022_m_000000_0' done.\n12/07/19 11:38:49 INFO mapred.JobClient:  map 100% reduce 0%\n12/07/19 11:38:49 INFO mapred.JobClient: Job complete: job_local_0022\n12/07/19 11:38:49 INFO mapred.JobClient: Counters: 8\n12/07/19 11:38:49 INFO mapred.JobClient:   File Output Format Counters \n12/07/19 11:38:49 INFO mapred.JobClient:     Bytes Written=2247793\n12/07/19 11:38:49 INFO mapred.JobClient:   File Input Format Counters \n12/07/19 11:38:49 INFO mapred.JobClient:     Bytes Read=1920337\n12/07/19 11:38:49 INFO mapred.JobClient:   FileSystemCounters\n12/07/19 11:38:49 INFO mapred.JobClient:     FILE_BYTES_READ=1342812616\n12/07/19 11:38:49 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=1326092302\n12/07/19 11:38:49 INFO mapred.JobClient:   Map-Reduce Framework\n12/07/19 11:38:49 INFO mapred.JobClient:     Map input records=2772\n12/07/19 11:38:49 INFO mapred.JobClient:     Spilled Records=0\n12/07/19 11:38:49 INFO mapred.JobClient:     SPLIT_RAW_BYTES=140\n12/07/19 11:38:49 INFO mapred.JobClient:     Map output records=2772\n12/07/19 11:38:49 INFO driver.MahoutDriver: Program took 4089950 ms (Minutes: 68.16583333333334)\n$MAHOUT_HOME/mahout cvb -i /home/seth/Scripted/mahout_data/vectors/vectors/vectors-for-cvb/ -o /home/seth/Scripted/mahout_data/clusters/ -ow -k 90 -dt /home/seth/Scripted/mahout_data/distributions -dict /home/seth/Scripted/mahout_data/vectors/vectors/dictionary.file-0 -mt /home/seth/Scripted/mahout_data/temp/ -x 20 -cd 0.05",
        "Issue Links": []
    },
    "MAHOUT-1048": {
        "Key": "MAHOUT-1048",
        "Summary": "most examples not working on hadoop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.5,                                            0.6,                                            0.7",
        "Fix Version/s": "None",
        "Component/s": "Examples",
        "Assignee": null,
        "Reporter": "jmm",
        "Created": "29/Jul/12 14:55",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "29/Jul/12 16:12",
        "Description": "Most examples don't work when you try to run them on Hadoop. This is probably not an easy fix, but I would really help if this was atleast mentioned somewhere (website, readme, etc.). If this was mentioned, it could have saved me over 10 hours of fiddling with the examples, thinking my setup was wrong, when actually the examples are just failing \nPlease include a notice to prevent people from wasting a lot of time on trying to run Mahout's examples on Hadoop.",
        "Issue Links": []
    },
    "MAHOUT-1049": {
        "Key": "MAHOUT-1049",
        "Summary": "out of memory error when running PageRank",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yan Liu",
        "Created": "03/Aug/12 08:26",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "03/Aug/12 08:47",
        "Description": "We always met a 'out of memory' error when running large-scale data for PageRank. Since we have to run large-scale data, is there any way for improvement?",
        "Issue Links": []
    },
    "MAHOUT-1050": {
        "Key": "MAHOUT-1050",
        "Summary": "mutable in-memory datamodel",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Renaud Richardet",
        "Created": "05/Aug/12 09:39",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "10/Apr/13 13:31",
        "Description": "I could not find an in-memory DataModel that supports setPreference / removePreference / refresh. Instead, they all \"recreate\" a new GenericDataModel anytime one adds or remove preferences.",
        "Issue Links": []
    },
    "MAHOUT-1051": {
        "Key": "MAHOUT-1051",
        "Summary": "InMemoryCollapsedVariationalBayes0 to load input vectors with docIDs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Jake Mannix",
        "Reporter": "Gokhan Capan",
        "Created": "07/Aug/12 11:15",
        "Updated": "09/Aug/12 20:20",
        "Resolved": "09/Aug/12 18:28",
        "Description": "Based upon our conversation with Jake in the user-list, I have modified the o.a.m.clustering.lda.cvb.InMemoryCollapsedVariationalBayes0.loadVectors so that it does not ignore document ids in input. To preserve backwards compatibility, it behaves as it did earlier if a ClassCastException is thrown; which occurs when ids are not integers, and/or the document vector (or getDelegate() if it is a NamedVector) cannot be cast to a RandomAccessSparseVector.",
        "Issue Links": []
    },
    "MAHOUT-1052": {
        "Key": "MAHOUT-1052",
        "Summary": "Add an option to MinHashDriver that specifies the dimension of vector to hash (indexes or values)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Elena Smirnova",
        "Created": "12/Aug/12 18:59",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "04/Jun/13 03:41",
        "Description": "Add a parameter to MinHash clustering that specifies the dimension of vector to hash (indexes or values). Current version of MinHash clustering only hashed values of vectors. Based on discussion on dev-mahout list, both of the use-cases are possible and frequently met in practice. \nPreserve backward compatibility with default dimension set to values. Add new unit tests.",
        "Issue Links": []
    },
    "MAHOUT-1053": {
        "Key": "MAHOUT-1053",
        "Summary": "Use KMeans++ for cluster Initialization",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Paritosh Ranjan",
        "Created": "13/Aug/12 02:10",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 17:41",
        "Description": "Use KMeans++ for cluster intialization.\nTed has already implemented a similar version. http://github.com/tdunning/knn",
        "Issue Links": [
            "/jira/browse/MAHOUT-1054"
        ]
    },
    "MAHOUT-1054": {
        "Key": "MAHOUT-1054",
        "Summary": "Use ball KMeans for clustering",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Paritosh Ranjan",
        "Created": "13/Aug/12 02:13",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 17:41",
        "Description": "Use ball KMeans for clustering.\nTed has already implemented a similar version. http://github.com/tdunning/knn",
        "Issue Links": [
            "/jira/browse/MAHOUT-1053"
        ]
    },
    "MAHOUT-1055": {
        "Key": "MAHOUT-1055",
        "Summary": "Change id fields to use LongWritable instead of IntWritable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Markus Paaso",
        "Created": "13/Aug/12 05:53",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Oct/12 08:57",
        "Description": "Why is IntWritable used as id field type in Mahout CVB? (org.apache.mahout.clustering.lda.cvb.CachingCVB0Mapper)\nDoes Long have that significant impact on performance?\nLong is much more usable as id type and int causes compatibility issues like the one below.\nIn method org.apache.mahout.utils.vectors.lucene.Driver.getSeqFileWriter() LongWritable is used correctly as id field type.\nI suggest that every IntWritable id should be changed to LongWritable.\nSequencefile produced by command 'mahout lucene.vector' cannot be handled by command 'mahout cvb' due to this id type incompatibility issue.\nsee http://mahout.markmail.org/thread/r3m6ojkpbzlxxizy",
        "Issue Links": []
    },
    "MAHOUT-1056": {
        "Key": "MAHOUT-1056",
        "Summary": "ALSWRFactorizer should also handle implicit feedback data",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "13/Aug/12 09:05",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "13/Aug/12 09:08",
        "Description": "In order to make ALSWRFactorizer the non-distributed counterpart of ParallelALSFactorizationJob, it also needs to support implicit feedback data.",
        "Issue Links": []
    },
    "MAHOUT-1057": {
        "Key": "MAHOUT-1057",
        "Summary": "CandidateItemsStrategies should be refreshable",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "13/Aug/12 22:06",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "13/Aug/12 23:06",
        "Description": "Our users might want to create customized strategies to provide candidate items for recommenders. These should also be refreshable to enable caching.",
        "Issue Links": []
    },
    "MAHOUT-1058": {
        "Key": "MAHOUT-1058",
        "Summary": "IDRescorers should also be able to filter candidate items",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "14/Aug/12 10:42",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "15/Aug/12 11:15",
        "Description": "Currently when a recommendation for a particular user is computed, at first the candidate items are determined via a CandidateItemsStrategy, then predicitions for all these items are computed and afterwards the IDRescorer can filter the items that might be recommended.\nFor performance reasons, it should also be possible to apply custom filtering before the estimation is done. This cannot be done in the CandidateItemsStrategy, as the filtering logic might depend on some request/or user-specific criteria that are not known in the recommender application.\nIt would be possible to implement such a logic in IDRescorer.isFiltered(), this is however bad for performance reasons, as the very costly estimation has already happen then.\nI propose to add a method to IDRescorer that can filter the candidate items right before estimation starts.",
        "Issue Links": []
    },
    "MAHOUT-1059": {
        "Key": "MAHOUT-1059",
        "Summary": "New matrix extensions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "17/Aug/12 20:23",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "04/Sep/12 02:25",
        "Description": "The upcoming clustering needs several capabilities to support different operations.  These include some matrix extensions for adding behaviors to different kinds of matrices.  Also there is a file based matrix that uses mmap to access a file as if it were a matrix in shared memory.  Since this is off-heap and shared between processes, it can seriously help some programs.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1060"
        ]
    },
    "MAHOUT-1060": {
        "Key": "MAHOUT-1060",
        "Summary": "Search for nearest neighbor",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "18/Aug/12 20:54",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 17:43",
        "Description": "This will contain a patch for sequential nearest neighbor search routines that underpin new clustering algorithms.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1059"
        ]
    },
    "MAHOUT-1061": {
        "Key": "MAHOUT-1061",
        "Summary": "mapreduce split causes ClassNotFound exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Sebastian Schelter",
        "Reporter": "David Engel",
        "Created": "28/Aug/12 20:49",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 12:35",
        "Description": "Running the split program in mapreduce mode, e.g. \"mahout split -xm mapreduce ...\" results in a ClassNotFound exception because the job jar is not set.  The following patch fixes the problem for me.\ndiff -ur mahout-distribution-0.7.orig/integration/src/main/java/org/apache/mahout/utils/SplitInputJob.java mahout-distribution-0.7/integration/src/main/java/org/apache/mahout/utils/SplitInputJob.java\n\u2014 mahout-distribution-0.7.orig/integration/src/main/java/org/apache/mahout/utils/SplitInputJob.java\t2012-06-12 03:30:39.000000000 -0500\n+++ mahout-distribution-0.7/integration/src/main/java/org/apache/mahout/utils/SplitInputJob.java\t2012-08-20 17:28:18.000000000 -0500\n@@ -114,6 +114,6 @@\n     // Setup job with new API\n     Job job = new Job(oldApiJob);\n+    job.setJarByClass(SplitInputJob.class);\n     FileInputFormat.addInputPath(job, inputPath);\n     FileOutputFormat.setOutputPath(job, outputPath);\n     job.setNumReduceTasks(1);",
        "Issue Links": []
    },
    "MAHOUT-1062": {
        "Key": "MAHOUT-1062",
        "Summary": "alphaI is not correctly saved in NaiveBayesModel",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Sebastian Schelter",
        "Reporter": "David Engel",
        "Created": "28/Aug/12 20:56",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "18/Jan/13 07:56",
        "Description": "alphaI is passed in for training via the thetaSummer job configuration.  When the model is saved, however, the wrong configuration is used causing the saved alphaI to always be 1.0.  The following patch fixes the problem for me.\ndiff -ur mahout-distribution-0.7.orig/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java mahout-distribution-0.7/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java\n\u2014 mahout-distribution-0.7.orig/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java\t2012-06-12 03:32:17.000000000 -0500\n+++ mahout-distribution-0.7/core/src/main/java/org/apache/mahout/classifier/naivebayes/training/TrainNaiveBayesJob.java\t2012-08-28 13:06:11.000000000 -0500\n@@ -134,7 +134,7 @@\n     }*/\n     //validate our model and then write it out to the official output\n\nNaiveBayesModel naiveBayesModel = BayesUtils.readModelFromDir(getTempPath(), getConf());\n+    NaiveBayesModel naiveBayesModel = BayesUtils.readModelFromDir(getTempPath(), thetaSummer.getConfiguration());\n     naiveBayesModel.validate();\n     naiveBayesModel.serialize(getOutputPath(), getConf());",
        "Issue Links": []
    },
    "MAHOUT-1063": {
        "Key": "MAHOUT-1063",
        "Summary": "ARFF parser does not support 'integer' and 'real' attribute types",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Santiago M. Mola",
        "Created": "01/Sep/12 13:48",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Sep/12 22:32",
        "Description": "ARFF format supports attributes specified as 'integer' and 'real'. They are treated just like 'numeric':\nhttp://weka.wikispaces.com/ARFF+%28stable+version%29\nLatest Weka development versions come with examples using these types.",
        "Issue Links": []
    },
    "MAHOUT-1064": {
        "Key": "MAHOUT-1064",
        "Summary": "Weird behavior of vector dumper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Gokhan Capan",
        "Created": "03/Sep/12 08:41",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Mar/13 21:52",
        "Description": "When vectordump utility is executed with sort flag true, I expect the resulting vector that is sorted by values. If that is the case, sometimes VectorHelper.vectorToJson method returns unexpected results.",
        "Issue Links": []
    },
    "MAHOUT-1065": {
        "Key": "MAHOUT-1065",
        "Summary": "Add CassandraDataModelTest",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering,                                            Integration",
        "Assignee": null,
        "Reporter": "Eduardo Gurgel Pinho",
        "Created": "03/Sep/12 17:09",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Mar/14 18:23",
        "Description": "The test class for the CassandraDataModel class.",
        "Issue Links": []
    },
    "MAHOUT-1066": {
        "Key": "MAHOUT-1066",
        "Summary": "How to generate sparsed Vectors from the specified dictionary.",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Hiroaki Kubota",
        "Created": "12/Sep/12 05:12",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 16:39",
        "Description": "I'd like to do clustering our natural language data.\nThe first, I used the 'seq2sparse' command to vectorize our data.\nI got sparsed vectors and a dictionary.\nAnd we could do k-means and got suitable clusters.\nIt was OK.\nThe next, I'd like to add some data to previous calculated clusters.\nSo I want to get additional vectors from new additional data based on previous dictionary.\nProbably I think,\nIt is impossible to get really accurate vectors by using only additional data.\nHowever,I'd like to reduce processing time so It's OK if I get the vector that is useful for decision tree.\nPlease give me advice !\nRegard,",
        "Issue Links": []
    },
    "MAHOUT-1067": {
        "Key": "MAHOUT-1067",
        "Summary": "SSVD enhancements: +named vector propagation to U, +USigma output",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "12/Sep/12 16:52",
        "Updated": "26/Jul/13 23:01",
        "Resolved": "09/Jun/13 22:10",
        "Description": "1) PCA will benefit from outputting U*Sigma product.\n2) Dimensionality reduction pipelines need NamedVector propagation to U, U*Sigma or U*Sigma^0.5.",
        "Issue Links": []
    },
    "MAHOUT-1068": {
        "Key": "MAHOUT-1068",
        "Summary": "FileDataModel should ignore directories when reloading data",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Yann Moisan",
        "Created": "17/Sep/12 17:20",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "17/Sep/12 17:25",
        "Description": "I work with a directory that contains :\n\na file test.csv (my data for recommendation)\na directory test (for other purpose ...)\n\nAnd surprinsigly i encountered the following error.java.io.FileNotFoundException: .../test (Is a directory)\n\tat java.io.FileInputStream.open(Native Method) ~[na:1.7.0_03]\n\tat java.io.FileInputStream.<init>(FileInputStream.java:138) ~[na:1.7.0_03]\n\tat org.apache.mahout.common.iterator.FileLineIterator.getFileInputStream(FileLineIterator.java:98) ~[mahout-core-0.7.jar:0.7]\n\tat org.apache.mahout.common.iterator.FileLineIterator.<init>(FileLineIterator.java:79) ~[mahout-core-0.7.jar:0.7]\n\tat org.apache.mahout.common.iterator.FileLineIterator.<init>(FileLineIterator.java:67) ~[mahout-core-0.7.jar:0.7]\n\tat org.apache.mahout.cf.taste.impl.model.file.FileDataModel.buildModel(FileDataModel.java:238) [mahout-core-0.7.jar:0.7]\n\tat org.apache.mahout.cf.taste.impl.model.file.FileDataModel.reload(FileDataModel.java:207) [mahout-core-0.7.jar:0.7]\n\tat org.apache.mahout.cf.taste.impl.model.file.FileDataModel.<init>(FileDataModel.java:193) [mahout-core-0.7.jar:0.7]\n\tat org.apache.mahout.cf.taste.impl.model.file.FileDataModel.<init>(FileDataModel.java:148) [mahout-core-0.7.jar:0.7]\nAfter looking at the code, i saw that the method findUpdateFilesAfter doesn't filter directories. \nI proposed to add a test in the method :\n    ...\n    for (File updateFile : parentDir.listFiles()) {\n+     if (!updateFile.isDirectory()) { \n      String updateFileName = updateFile.getName();",
        "Issue Links": []
    },
    "MAHOUT-1069": {
        "Key": "MAHOUT-1069",
        "Summary": "Multi-target, side-info aware, SGD-based recommender algorithms, examples, and tools to run",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.1",
        "Fix Version/s": "None",
        "Component/s": "CLI,                                            Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Gokhan Capan",
        "Created": "18/Sep/12 12:19",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "11/Mar/13 16:28",
        "Description": "Upon our conversations on dev-list, I would like to state that I have completed the merge of the recommender algorithms that is mentioned in http://goo.gl/fh4d9 to mahout. \nThese are a set of learning algorithms for matrix factorization based recommendation, which are capable of:\n\nRecommending multiple targets:\n\t\nNumerical Recommendation with OLS Regression\nBinary Recommendation with Logistic Regression\nMultinomial Recommendation with Softmax Regression\nOrdinal Recommendation with Proportional Odds Model\n\n\n\n\nLeveraging side info in mahout vector format where available\n\t\nUser side information\nItem side information\nDynamic side information (side info at feedback moment, such as proximity, day of week etc.)\n\n\n\n\nOnline learning\n\nSome command-line tools are provided as mahout jobs, for pre-experiment utilities and running experiments.\nEvaluation tools for numerical and categorical recommenders are added.\nA simple example for Movielens-1M data is provided, and it achieved pretty good results (0.851 RMSE in a randomly generated test data after some validation to determine learning and regularization rates on a separate validation data)\nThere is no modification in the existing Mahout code, except the added lines in driver.class.props for command-line tools. However, that became a huge patch with dozens of new source files.\nThese algorithms are highly inspired from various influential Recommender System papers, especially Yehuda Koren's. For example, the Ordinal model is from Koren's OrdRec paper, except the cuts are not user-specific but global.\nLeft for future:\n\nThe core algorithms are tested, but there probably exists some parts those tests do not cover. I saw many of those in action without problem, but I am going to add new tests regularly.\nNot all algorithms have been tried on appropriate datasets, and they may need some improvement. However, I use the algorithms also for my M.Sc. thesis, which means I will eventually submit more experiments. As the experimenting infrastructure exists, I believe community may provide more experiments, too.",
        "Issue Links": []
    },
    "MAHOUT-1070": {
        "Key": "MAHOUT-1070",
        "Summary": "DisplayKMeans example has transposed/mislabelled arguments",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Examples",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Gabriel Reid",
        "Created": "19/Sep/12 20:06",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 17:57",
        "Description": "The org.apache.mahout.clustering.display.DisplayKMeans example class uses a value for k (numClusters) and maximum number of iterations to come to convergence, but their use is transposed (i.e. the numClusters is used as max iterations, and max iterations is used for numClusters). Furthermore, a second hard-coded version of the value is used. The end result is that it's not directly possible to experiment with different values of numClusters and maxIterations.",
        "Issue Links": []
    },
    "MAHOUT-1071": {
        "Key": "MAHOUT-1071",
        "Summary": "Mahout-0.7 is not running on Hadoop 0.23 CDH4-MR1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dror Mizrachi",
        "Created": "20/Sep/12 08:16",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "20/Sep/12 08:32",
        "Description": "Hi \nI'm using Mahout-0.7-job.jar on Hadoop-0.23(CDH4-MR1). I got the below exception.\nNote: org.apache.hadoop.mapreduce.JobContext is an Interface on 0.23 while it was class on 0.20.x\nIt seems that Mahout-0.7 is still incompatible with Hadoop-0.23 related to this class/interface and maybe for other classes that have been changed in 0.23 vs. 0.20.x\nPlease advice / fix is ASAP.\nThanks in advance \nDror\n-------------------\nException in thread \"main\" java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected\n        at org.apache.mahout.common.HadoopUtil.getCustomJobName(HadoopUtil.java:166)\n        at org.apache.mahout.common.AbstractJob.prepareJob(AbstractJob.java:579)\n        at org.apache.mahout.cf.taste.hadoop.preparation.PreparePreferenceMatrixJob.run(PreparePreferenceMatrixJob.java:75)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.mahout.cf.taste.hadoop.item.RecommenderJob.run(RecommenderJob.java:154)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n       ...",
        "Issue Links": []
    },
    "MAHOUT-1072": {
        "Key": "MAHOUT-1072",
        "Summary": "FuzzyKmeansDriver emitMostLikely has no effect",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Dave Byrne",
        "Created": "21/Sep/12 17:01",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "08/Oct/12 12:06",
        "Description": "FuzzyKmeansDriver.clusterData() has emitMostLikely hardcoded to true and ignores argument passed to function",
        "Issue Links": []
    },
    "MAHOUT-1073": {
        "Key": "MAHOUT-1073",
        "Summary": "Map reduce cluster classification does not add weights to vectors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Dave Byrne",
        "Created": "21/Sep/12 17:32",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "08/Oct/12 12:07",
        "Description": "Map reduce version of cluster classification gives every vector a weight of 1.0 while sequential version uses pdf",
        "Issue Links": []
    },
    "MAHOUT-1074": {
        "Key": "MAHOUT-1074",
        "Summary": "FPGrowthDriver only supports input from local file",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Examples",
        "Assignee": null,
        "Reporter": "Pavel Tcholakov",
        "Created": "24/Sep/12 13:21",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "13/Mar/13 08:16",
        "Description": "FPGrowthDriver currently reads its input using a normal File. It would be better if it supported the Hadoop FileSystem abstraction so that the input can be located on any supported cluster file system.",
        "Issue Links": []
    },
    "MAHOUT-1075": {
        "Key": "MAHOUT-1075",
        "Summary": "ClusterDumper output file should be optional",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dave Byrne",
        "Created": "24/Sep/12 17:45",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Mar/13 19:07",
        "Description": "ClusterDumper output option should be optional, defaults to System.out if -o is not specified",
        "Issue Links": []
    },
    "MAHOUT-1076": {
        "Key": "MAHOUT-1076",
        "Summary": "Matrix Multiplication output to user specified directory",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Smita Wadhwa",
        "Created": "25/Sep/12 08:33",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 12:18",
        "Description": "Matrix multiplication output directory is being formed with system nano time, which is really difficult for user to figure.\n Instead user can give output path as arg only.",
        "Issue Links": []
    },
    "MAHOUT-1077": {
        "Key": "MAHOUT-1077",
        "Summary": "apparent spectral kmeans bug",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Paul Hubenig",
        "Created": "25/Sep/12 21:54",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "18/Jan/13 07:45",
        "Description": "Using example data at:  https://cwiki.apache.org/MAHOUT/spectral-clustering.html\n0,0,0\n0,1,0.8\n0,2,0.5\n1,0,0.8\n1,1,0\n1,2,0.9\n2,0,0.5\n2,1,0.9\n2,2,0\nUsing 0.7 distribution.\nmahout spectralkmeans -i file:///Users/phubenig/affExGraph.txt -o file:///Users/phubenig/spectralEx -k 2 -d 3 -x 30 -cd 0.01   -ow\n12/09/05 16:14:00 INFO mapred.JobClient:     Combine output records=1\n12/09/05 16:14:00 INFO mapred.JobClient:     Reduce output records=1\n12/09/05 16:14:00 INFO mapred.JobClient:     Map output records=1\n12/09/05 16:14:00 INFO lanczos.LanczosSolver: 2 passes through the corpus so far...\nException in thread \"main\" org.apache.mahout.math.IndexException: Index 2 is outside allowable range of [0,2)\n    at org.apache.mahout.math.AbstractMatrix.set(AbstractMatrix.java:479)\n    at org.apache.mahout.math.decomposer.lanczos.LanczosSolver.solve(LanczosSolver.java:132)\n    at org.apache.mahout.math.hadoop.decomposer.DistributedLanczosSolver.runJob(DistributedLanczosSolver.java:73)\n    at org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:148)\n    at org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:86)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n    at org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.main(SpectralKMeansDriver.java:53)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n    at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n    at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
        "Issue Links": []
    },
    "MAHOUT-1078": {
        "Key": "MAHOUT-1078",
        "Summary": "matrixmult gives wrong answer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Paul Hubenig",
        "Created": "26/Sep/12 05:51",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "26/Sep/12 06:05",
        "Description": "Create a simple matrix: \n1 2\n3 4\npackage exp\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.Path\nimport org.apache.hadoop.io.\n{IOUtils, IntWritable, SequenceFile}\nimport org.apache.mahout.math.\n{SequentialAccessSparseVector, VectorWritable, RandomAccessSparseVector}\n\nobject MakeMatrix extends App with CommonMethods {\n  val matrixFileOutput = \"/Users/phubenig/matrixFile.sf\"\n  val conf = new Configuration()\n  val fs = getFileSystem(true, conf)\n  val path = new Path(matrixFileOutput)\n  val arr = Array(Array(1, 2), Array(3, 4))\n  val rank = 2\n  val key = new org.apache.hadoop.io.IntWritable()\n  val writer = SequenceFile.createWriter(fs, conf, path, classOf[IntWritable], classOf[VectorWritable])\n  for (i <- 0 until rank) {\n    key.set\n    val v = new SequentialAccessSparseVector(rank)\n    for (j <- 0 until rank) \n{\n      v.setQuick(j, arr(i)(j))\n    }\n    writer.append(key, new VectorWritable(v))\n  }\n  IOUtils.closeStream(writer)\n}\nEngage mahout to square this matrix.\n$ mahout matrixmult --numRowsA 2 --numColsA 2 --numRowsB 2 --numColsB 2 --inputPathA matrixFile.sf --inputPathB matrixFile.sf\nRead back the result:\nI get the answer:??\n(0, 0): 10.0\n(0, 1): 14.0\n(1, 0): 14.0\n(1, 1): 20.0\ni.e.,\n10 14\n14 20\nCorrect answer??\n1 2    1 2         7 10\n      x         =\n3 4     3 4        15 22\nI tried mahout transpose and that seems to work.\nAm I specifying the matrix wrong somehow?\n10 =|(1, 3)| * |(1,3)|\n20 =|(2,4)| * |(2,4)|\n14 = (1,3) , (2,4)  \nbut why is it doing this?\nInstead of two vectors in the matrix creation program, I also tried one vector consisting of:  (1, 2, 3, 4) - matrixmult also give the wrong answer.\nRegardless of convention used (???) that I don't understand, here is no documentation and no error checking.",
        "Issue Links": []
    },
    "MAHOUT-1079": {
        "Key": "MAHOUT-1079",
        "Summary": "Kmeans clustered-points distance not getting updated in MR job run",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Smita Wadhwa",
        "Created": "26/Sep/12 07:00",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "26/Sep/12 07:28",
        "Description": "K Means output clustered points - distance from cluster centre is not getting updated , when you run as MR job, whereas it is getting updated in sequential run.",
        "Issue Links": []
    },
    "MAHOUT-1080": {
        "Key": "MAHOUT-1080",
        "Summary": "Kmeans clustered output losses vectorId given in the input",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Smita Wadhwa",
        "Created": "26/Sep/12 08:09",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 18:24",
        "Description": "The input to the Kmeans is Intwritable and vectorWritable \nand the output of clustered points is clusterId WeightedVectorWitable(vector,distance-from-the-centre)\nThe information the id of the vector is lost in this processing .",
        "Issue Links": []
    },
    "MAHOUT-1081": {
        "Key": "MAHOUT-1081",
        "Summary": "Precision of float to 3 digits after decimal - loss of information",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Invalid",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Smita Wadhwa",
        "Created": "26/Sep/12 09:51",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Mar/13 07:05",
        "Description": "Precision of vectors containing data like 0.00167328372 and 0.000829739 , 0.001783978978 will end up in output as 0.001,0.000, 0.001 when specially vector id has also been lost in clustering after kmeans , its really difficult to get back the original data.",
        "Issue Links": []
    },
    "MAHOUT-1082": {
        "Key": "MAHOUT-1082",
        "Summary": "driver seqdirectory fails with param -filter set",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Johannes Rauber",
        "Created": "26/Sep/12 22:10",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Mar/13 21:45",
        "Description": "The following error is thrown when an own implementation of PrefixAdditionFilter is specified with parameter -filter for seqdirectory.\nException in thread \"main\" java.lang.IllegalArgumentException: wrong number of arguments\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat org.apache.mahout.text.SequenceFilesFromDirectory.run(SequenceFilesFromDirectory.java:96)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.text.SequenceFilesFromDirectory.main(SequenceFilesFromDirectory.java:53)\nIn class org.apache.mahout.text.SequenceFilesFromDirectory line 96 the following additional parameter should be inserted into the reflection call of the ctor: \"charset\"\nRaises Error:\npathFilter = constructor.newInstance(conf, keyPrefix, options, writer, fs);\nFix:\npathFilter = constructor.newInstance(conf, keyPrefix, options, writer, charset, fs);",
        "Issue Links": []
    },
    "MAHOUT-1083": {
        "Key": "MAHOUT-1083",
        "Summary": "CIReducer in kmeans doesn't work well",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "liutengfei",
        "Created": "27/Sep/12 01:19",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "28/Sep/12 09:38",
        "Description": "the function \"reduce\" in mahout-0.7-kmeans-CIReducer.java doesn't work well as it looks like.\n  protected void reduce(IntWritable key, Iterable<ClusterWritable> values, Context context) throws IOException,\n      InterruptedException {\n    Iterator<ClusterWritable> iter = values.iterator();\n    ClusterWritable first = null;\n    while (iter.hasNext()) {\n      ClusterWritable cw = iter.next();\n      if (first == null) \n{\n        first = cw;\n      }\n else \n{\n        first.getValue().observe(cw.getValue());\n      }\n    }\n    List<Cluster> models = new ArrayList<Cluster>();\n    models.add(first.getValue());\n    classifier = new ClusterClassifier(models, policy);\n    classifier.close();\n    context.write(key, first);\n  }\nApparently\uff0c the variable \"first\" will collect all output data of maps. Actually but, the value of \"first\" will change after the code \"ClusterWritable cw = iter.next();\", same with this new variable \"cw\"! I don't why but running result shows that the code runs looks like this:\"ClusterWritable cw = first = iter.next();\".\nis \"cw\" a reference a to \"iter\"?\nis \"iter.next\" just change the value of \"iter\" itself to the next?",
        "Issue Links": []
    },
    "MAHOUT-1084": {
        "Key": "MAHOUT-1084",
        "Summary": "Kmeans for synthetic control example--there are 12 cluster during iterations.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "liutengfei",
        "Created": "29/Sep/12 03:19",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "08/Jun/13 12:01",
        "Description": "In Mahout-Kmeans for syntheticcontrol example, using the default parameters means to compute 6 clusters at last. But why there are 12 clusters during Kmeans iterations. According to my observation, the former 6 clusters and the latter 6 clusters are the same before the first iteration,those 6 clusters are generatored by RandomSeedGenerator.java. Then the CIMapper will assign its own points to this 12 clusters. Is here existing logical errors?\n       The 12 clusters are created by the function \"setup\" in CIMapper.java, more specifically, is the line \"classifier.readFromSeqFiles(conf, new Path(priorClustersPath));\", here the \"priorClustersPath\" means hdfs direction \"output/clusters-0/\", there are 8 files in this direction: \"_policy\",\"part-randomSeed\"(one file record six cluster),\"part-00000\" to \"part-00005\"(total six files,every one record a cluster), while reading this direction, \"_policy\" will be filtered out, so program will read \"part-00000\" to \"part-00005\" to create six clusters, then read \"part-randomSeed\" to create the other six clusters, this is the reason why there will be 12 clusters before first iteration.\n      Solution: delete associated code to avoid duplicately creating clusters in \"output/clusters-0/\", here i delete codes where create files: \"part-00000\" to \"part-00005\" in ClusterClassfier.java:\n  public void writeToSeqFiles(Path path) throws IOException {\n    writePolicy(policy, path);\n    /*\n    Configuration config = new Configuration();\n    FileSystem fs = FileSystem.get(path.toUri(), config);\n    SequenceFile.Writer writer = null;\n    ClusterWritable cw = new ClusterWritable();\n    for (int i = 0; i < models.size(); i++) {\n      try \n{\n        Cluster cluster = models.get(i);\n        cw.setValue(cluster);\n        writer = new SequenceFile.Writer(fs, config,\n            new Path(path, \"part-\" + String.format(Locale.ENGLISH, \"%05d\", i)), IntWritable.class,\n            ClusterWritable.class);\n        Writable key = new IntWritable(i);\n        writer.append(key, cw);\n      }\n finally \n{\n        Closeables.closeQuietly(writer);\n      }\n    }\n    */\n  }\n    I don't know if it is still okay for other progams who using this file, but for KMeans in Syntheticcontrol example, program will create 6 clusters during every iterations as i expected.",
        "Issue Links": []
    },
    "MAHOUT-1085": {
        "Key": "MAHOUT-1085",
        "Summary": "mahout-kmeans, the chance to pick new element",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "liutengfei",
        "Created": "29/Sep/12 03:27",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 17:54",
        "Description": "In RandomSeedGenerator.java:\n           .............\nelse if (random.nextInt(currentSize + 1) != 0)\n          { // with chance 1/(currentSize+1) pick new element\n             ...........\n           }\n\ni think \"!=\"  should be \"==\"\n         ................\nelse if (random.nextInt(currentSize + 1) == 0)\n          { // with chance 1/(currentSize+1) pick new element\n        ..................",
        "Issue Links": []
    },
    "MAHOUT-1086": {
        "Key": "MAHOUT-1086",
        "Summary": "Mean Shift Test Now Produces 4 Clusters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Jeff Eastman",
        "Created": "29/Sep/12 05:10",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Oct/12 01:18",
        "Description": "Something changed in Mahout around 9/6/12 that caused TestMeanShift.testCanopyEuclideanMRJobNoClustering to return 4 clusters rather than 3. All of the other tests using the same data still return 3 clusters. No changes were made to any of the MeanShiftCanopy classes other than 1 formatting change to the driver so I'm at a loss to the cause.",
        "Issue Links": []
    },
    "MAHOUT-1087": {
        "Key": "MAHOUT-1087",
        "Summary": "ExpectationMaximizationSVDFactorizer doesn't do expectation maximization",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "30/Sep/12 17:41",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "30/Sep/12 21:23",
        "Description": "This factorizer simply learns the user and item features via SGD as described in Simon Funk's famous blogpost, which is not expectation maximization, so I suggest we rename it to FunkSVD.",
        "Issue Links": []
    },
    "MAHOUT-1088": {
        "Key": "MAHOUT-1088",
        "Summary": "biased item-based recommender",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "30/Sep/12 19:44",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "30/Sep/12 21:23",
        "Description": "user-item-baseline estimation offers a simple yet very effective to improve the rating prediction of recommenders (see http://dl.acm.org/citation.cfm?id=1644874 for details).\nWe should offer an item-based recommender that incorporates this technique",
        "Issue Links": []
    },
    "MAHOUT-1089": {
        "Key": "MAHOUT-1089",
        "Summary": "SGD matrix factorization for rating prediction with user and item biases",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Zeno Gantner",
        "Created": "30/Sep/12 21:01",
        "Updated": "19/Feb/15 03:55",
        "Resolved": "29/Oct/12 20:52",
        "Description": "A matrix factorization that is trained with standard SGD on all features at the same time, in contrast to ExpectationMaximizationFactorizer, which learns feature by feature.\nAdditionally to the free features it models a rating bias for each user and item.",
        "Issue Links": []
    },
    "MAHOUT-1090": {
        "Key": "MAHOUT-1090",
        "Summary": "Add a similarity implementation that computes cosine over all entries",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Yann Moisan",
        "Created": "03/Oct/12 08:41",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 16:21",
        "Description": "The aim of this feature is to use a recommender to compute similarities as the hadoop RowSimilarityJob. It will be faster for small dataset because in-memory. So we need an in-memory implementation of the Cosine Similarity which computes cosine over all entries (UncenteredCosineSimilarity use only entries that are in both vectors).\nHere is my implementation (doesn't support refresh for the moment):\nimport java.util.Collection;\nimport java.util.HashMap;\nimport java.util.Map;\nimport org.apache.mahout.cf.taste.common.Refreshable;\nimport org.apache.mahout.cf.taste.common.TasteException;\nimport org.apache.mahout.cf.taste.impl.similarity.AbstractItemSimilarity;\nimport org.apache.mahout.cf.taste.model.DataModel;\nimport org.apache.mahout.cf.taste.model.PreferenceArray;\npublic class CosineSimilarity extends AbstractItemSimilarity {\n    protected CosineSimilarity(DataModel dataModel) \n{\n        super(dataModel);\n    }\n\n    @Override\n    public void refresh(Collection<Refreshable> alreadyRefreshed) \n{\n        throw new UnsupportedOperationException();\n    }\n\n    @Override\n    public double itemSimilarity(long itemID1, long itemID2) throws TasteException {\n        DataModel model = getDataModel();\n        PreferenceArray xPrefs = model.getPreferencesForItem(itemID1);\n        PreferenceArray yPrefs = model.getPreferencesForItem(itemID2);\n        double sumXY = 0;\n        double sumX2 = 0;\n        double sumY2 = 0;\n        Map<Long, Float> mX = new HashMap<Long, Float>();\n        for (int xPrefIndex = 0; xPrefIndex < xPrefs.length(); xPrefIndex++) \n{\n            float x = xPrefs.get(xPrefIndex).getValue();\n            mX.put(xPrefs.get(xPrefIndex).getUserID(), x);\n            sumX2 += x * x;\n        }\n\n        for (int yPrefIndex = 0; yPrefIndex < yPrefs.length(); yPrefIndex++) {\n            float y = yPrefs.get(yPrefIndex).getValue();\n            Float x = mX.get(yPrefs.get(yPrefIndex).getUserID());\n            if (x != null) \n{\n                sumXY += x * y;\n            }\n            sumY2 += y * y;\n        }\n        return sumXY / (Math.sqrt(sumX2) * Math.sqrt(sumY2));\n    }\n    @Override\n    public double[] itemSimilarities(long itemID1, long[] itemID2s) throws TasteException {\n        int length = itemID2s.length;\n        double[] result = new double[length];\n        for (int i = 0; i < length; i++) \n{\n          result[i] = itemSimilarity(itemID1, itemID2s[i]);\n        }\n        return result;\n    }\n}",
        "Issue Links": []
    },
    "MAHOUT-1091": {
        "Key": "MAHOUT-1091",
        "Summary": "Bug in SequentialAccessSparseVector full iteration",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "08/Oct/12 20:51",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Oct/12 06:25",
        "Description": "The iterator for the SequentialAccessSparseVector doesn't return any items beyond the last non-zero.  This breaks some stuff pretty massively, but hopefully doesn't break much user code since iterating through all elements of a sparse vector is a relatively rare thing to do.",
        "Issue Links": []
    },
    "MAHOUT-1092": {
        "Key": "MAHOUT-1092",
        "Summary": "MultiNormal is slow in common case",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "08/Oct/12 21:06",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 19:22",
        "Description": "The multinormal generator unnecessarily uses matrix arithmetic for some simple cases.",
        "Issue Links": []
    },
    "MAHOUT-1093": {
        "Key": "MAHOUT-1093",
        "Summary": "CrossFoldLearner trains in all folds if trackign key is negative",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Eric Springer",
        "Created": "09/Oct/12 01:34",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 16:55",
        "Description": "See: https://github.com/apache/mahout/pull/7",
        "Issue Links": []
    },
    "MAHOUT-1094": {
        "Key": "MAHOUT-1094",
        "Summary": "when i am giving the testing data from the new set of data without using split ..it is giving the completely wrong confusion matrix",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Priyadarshan raj",
        "Created": "09/Oct/12 06:00",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 21:56",
        "Description": "hi,\ni am able to successfully create  a model by the command below:-\nbin/mahout trainnb -i /user/cloudera/MahoutWeighted/1_FactWt-train-vectors -el -o /user/cloudera/MahoutWeighted/model -li /user/cloudera/MahoutWeighted/labelindex -ow\n....\nbut  i am unable to use that model...when i am feeding the model with test data in the same way,i trained it..i am not able to get the correct confusion matrix.and the number of map output records coming is equal to the number of files i am feeding .can anyone tell me why it is not coming to be number of lines ??",
        "Issue Links": []
    },
    "MAHOUT-1095": {
        "Key": "MAHOUT-1095",
        "Summary": "using mahout(0.7) model",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Priyadarshan raj",
        "Created": "10/Oct/12 08:53",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 15:40",
        "Description": "hi,\ni am a newbie in mahout(0.7).i am able to successfully create a model by the command below:-\nbin/mahout trainnb -i /user/cloudera/MahoutWeighted/1_FactWt-train-vectors -el -o /user/cloudera/MahoutWeighted/model -li /user/cloudera/MahoutWeighted/labelindex -ow\n....\nbut i am unable to use that model...when i am feeding the model with test data in the same way,i trained it..i am not able to get the correct confusion matrix..can anyone please tell me how to use the model created..it would be a great help to me",
        "Issue Links": []
    },
    "MAHOUT-1096": {
        "Key": "MAHOUT-1096",
        "Summary": "Lots of style warnings in generated code are easy to fix",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "11/Oct/12 05:03",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Oct/12 03:43",
        "Description": "We have a method that is protected in our primitive maps and sets.  It is never over-ridden, however, so it might as well be final.  This will nuke between 70-80 high priority style warning.\nWe also have some cases of new String(\"foo\") which are easy to fix.",
        "Issue Links": []
    },
    "MAHOUT-1097": {
        "Key": "MAHOUT-1097",
        "Summary": "SSVd does not writing uSigma after uSigma true",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Smita Wadhwa",
        "Created": "11/Oct/12 10:52",
        "Updated": "26/Jul/13 23:01",
        "Resolved": "11/Oct/12 19:35",
        "Description": "SSVd does not writing uSigma file  after uSigma variable true",
        "Issue Links": []
    },
    "MAHOUT-1098": {
        "Key": "MAHOUT-1098",
        "Summary": "ColumnMeansJob broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "11/Oct/12 21:39",
        "Updated": "26/Jul/13 23:01",
        "Resolved": "09/Jun/13 10:07",
        "Description": "getting various errors, e.g.\njava.lang.IllegalStateException: java.lang.ClassNotFoundException: DistributedRowMatrix.columnMeans.vector.class\n\tat org.apache.mahout.common.ClassUtils.instantiateAs(ClassUtils.java:38)\n\tat org.apache.mahout.math.hadoop.MatrixColumnMeansJob$MatrixColumnMeansMapper.map(MatrixColumnMeansJob.java:159)\n\tat org.apache.mahout.math.hadoop.MatrixColumnMeansJob$MatrixColumnMeansMapper.map(MatrixColumnMeansJob.java:134)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:647)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:270)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1157)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:264)",
        "Issue Links": []
    },
    "MAHOUT-1099": {
        "Key": "MAHOUT-1099",
        "Summary": "Multiple slf4j bindings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering,                                            Integration",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "11/Oct/12 22:34",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "19/Nov/12 13:36",
        "Description": "Faced exception due to multiple slf4j bindings of different versions while seqdirectory of cluster-reuters was being executed. Excluding the slf4j-log4j12 from hbase dependency of integration.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1118"
        ]
    },
    "MAHOUT-1100": {
        "Key": "MAHOUT-1100",
        "Summary": "[PATCH] ArrayIndexOutOfBoundsException in TreeClusteringRecommender2.java",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Julien Aym\u00e9",
        "Created": "12/Oct/12 08:14",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Oct/12 08:54",
        "Description": "Hi, I found an ArrayIndexOutOfBoundsException when trying the TreeClusteringRecommender2, in method mergeClosestClusters (stacktrace attached).\nSo here is a patch to fix it.\nThe patch is trivial.",
        "Issue Links": []
    },
    "MAHOUT-1101": {
        "Key": "MAHOUT-1101",
        "Summary": "[PATCH] RecommenderServlet: Wrong output when using JSON format",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering,                                            Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Julien Aym\u00e9",
        "Created": "12/Oct/12 09:30",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Oct/12 09:37",
        "Description": "When using \"json\" as format for the RecommenderServlet, the JSON returned is not valid: there is a comma ',' just before the closing bracket of the \"item\" array.\nThe fix is trivial: do not add a ',' if the item is the last one.\nPatch is attached.",
        "Issue Links": []
    },
    "MAHOUT-1102": {
        "Key": "MAHOUT-1102",
        "Summary": "Mahout build fails for default profile if hadoop.version is passed as argument",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7,                                            0.8,                                            1.0.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Ashish Singh",
        "Created": "18/Oct/12 00:37",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 15:20",
        "Description": "Default hadoop-0.20 profile is used for mahout-core. Profile hadoop-0.23 is driven by hadoop.version variable defined at command line.\nThe build fails for hadoop-common and hadoop-maprreduce dependency as these jars are not the part of hadoop-1.1.x. \nhadoop.version change to 1.1.x at command line, makes hadoop-0.23 profile to be activated, and we don't want to activate hadoop-0.23 profile for hadoop-1.1.x. \nIf hadoop.version is overridden and hadoop-20 profile is activated and hadoop-23 profile is deactivated using command\nmvn -P hadoop-0.20,'!hadoop-0.23' -Dhadoop.version=1.1.0 help:active-profiles clean install -DskipTests.\nThis results in mahout-integration project failure for the dependency of hadoop-common and hadoop-maprreduce.\nMoving the profile from core/pom.xml to parent pom.xml resolve the Issue.",
        "Issue Links": []
    },
    "MAHOUT-1103": {
        "Key": "MAHOUT-1103",
        "Summary": "clusterpp is not writing directories for all clusters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Matt Molek",
        "Created": "22/Oct/12 14:16",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 12:41",
        "Description": "After running kmeans clustering on a set of ~3M points, clusterpp fails to populate directories for some clusters, no matter what k is.\nI've tested this on my data with k = 300, 250, 150, 100, 50, 25, 10, 5, 2\nEven with k=2 only one cluster directory was created. For each reducer that fails to produce directories there is an empty part-r-* file in the output directory.\nHere is my command sequence for the k=2 run:\n\nbin/mahout kmeans -i ssvd2/USigma -c 2clusters/init-clusters -o 2clusters/pca-clusters -dm org.apache.mahout.common.distance.TanimotoDistanceMeasure -cd 0.05 -k 2 -x 15 -cl\n\nbin/mahout clusterdump -i 2clusters/pca-clusters/clusters-*-final -o 2clusters.txt\n\nbin/mahout clusterpp -i 2clusters/pca-clusters -o 2clusters/bottom\n\nThe output of clusterdump shows two clusters: VL-3742464 and VL-3742466 containing 2585843 and 1156624 points respectively.\nDiscussion on the user mailing list suggested that this might be caused by the default hadoop hash partitioner. The hashes of these two clusters aren't identical, but they are close. Putting both cluster names into a Text and caling hashCode() gives:\nVL-3742464 -> -685560454\nVL-3742466 -> -685560452\nFinally, when running with \"-xm sequential\", everything performs as expected.",
        "Issue Links": []
    },
    "MAHOUT-1104": {
        "Key": "MAHOUT-1104",
        "Summary": "Improve Javadoc for AbstractVectorClassifier",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Timothy Mann",
        "Created": "23/Oct/12 17:32",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Mar/13 07:22",
        "Description": "Modify javadocs for AbstractVectorClassifier to clarify what classify and classifyFull methods do.\nOverride javadoc for classify and classifyScalar methods in AbstractNaiveBayesClassifier to reflect the fact that they are not supported.",
        "Issue Links": []
    },
    "MAHOUT-1105": {
        "Key": "MAHOUT-1105",
        "Summary": "AbstractNaiveBayesClassifier.classifyFull(Vector,Vector) not reusing provided result vector",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Timothy Mann",
        "Created": "25/Oct/12 21:22",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "17/Jan/13 23:13",
        "Description": "AbstractNaiveBayesClassifier.classifyFull(Vector,Vector) is not using provided result vector to store the result of classification.",
        "Issue Links": []
    },
    "MAHOUT-1106": {
        "Key": "MAHOUT-1106",
        "Summary": "SVD++",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Zeno Gantner",
        "Created": "29/Oct/12 20:48",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "29/Oct/12 21:06",
        "Description": "Initial shot at SVD++.\nRelies on the RatingsSGDFactorizer class introduced in MAHOUT-1089.\nOne could also think about several enhancements, e.g. having separate regularization constants for user and item factors.\nI am also the author of the SVDPlusPlus class in MyMediaLite, so if there are any similarities, no need to worry \u2013 I am okay with relicensing this to the Apache 2.0 license.\nhttps://github.com/zenogantner/MyMediaLite/blob/master/src/MyMediaLite/RatingPrediction/SVDPlusPlus.cs",
        "Issue Links": []
    },
    "MAHOUT-1107": {
        "Key": "MAHOUT-1107",
        "Summary": "OnlineLogisticRegression doesn't seem to work for some people/problems",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "30/Oct/12 23:54",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "14/Nov/12 01:13",
        "Description": "Rajesh reports that using the classic Iris dataset gives random guess results.\nThis may be a bug in OLR or a bug in his code.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1113"
        ]
    },
    "MAHOUT-1108": {
        "Key": "MAHOUT-1108",
        "Summary": "cluster-reuters.sh executes seqdirectory with MAHOUT_LOCAL=true",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Elmer Garduno",
        "Created": "31/Oct/12 05:43",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "02/Jun/13 17:22",
        "Description": "Got the following exception when running the command with HADOOP_CONF and  HADOOP_CONF_DIR\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/hadoop/util/ProgramDriver\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:96)\nCaused by: java.lang.ClassNotFoundException: org.apache.hadoop.util.ProgramDriver\n\tat java.net.URLClassLoader$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(Unknown Source)\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source)\n\tat java.lang.ClassLoader.loadClass(Unknown Source)\n\t... 1 more",
        "Issue Links": []
    },
    "MAHOUT-1109": {
        "Key": "MAHOUT-1109",
        "Summary": "ClusterDumper fails to write output",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering,                                            Integration",
        "Assignee": "Paritosh Ranjan",
        "Reporter": "Paritosh Ranjan",
        "Created": "01/Nov/12 12:39",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Nov/12 13:03",
        "Description": "ClusterDumper fails to write output if the parent files are not created.",
        "Issue Links": []
    },
    "MAHOUT-1110": {
        "Key": "MAHOUT-1110",
        "Summary": "bin/mahout in released package only works with external Hadoop cluster, not with packaged Hadoop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lance Norskog",
        "Created": "10/Nov/12 02:33",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 20:32",
        "Description": "The bin/mahout in the mahout-0.7 and current trunk does not support running without a Hadoop cluster. The problem is that the hadoop-core library is not stored in the top-level lib, but in /lib/hadoop. bin/mahout only searches the top-level lib.\nThis problem does not show in running development builds, only the released builds.\nPatch is attached.",
        "Issue Links": []
    },
    "MAHOUT-1111": {
        "Key": "MAHOUT-1111",
        "Summary": "Logging bindings not working in current trunk as of github 2012-November-9 18:41",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build,                                            Examples",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Lance Norskog",
        "Created": "10/Nov/12 02:46",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 15:07",
        "Description": "Current commit is 1743c1521679daab600a982be6e537517111130e\nOn trunk, running examples/bin/classify-20newsgroups.sh gives this error:\n\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: slf4j-api 1.6.x (or later) is incompatible with this binding.\nSLF4J: Your binding is version 1.5.5 or earlier.\nSLF4J: Upgrade your binding to version 1.6.x.\nException in thread \"main\" java.lang.NoSuchMethodError: org.slf4j.impl.StaticLoggerBinder.getSingleton()Lorg/slf4j/impl/StaticLoggerBinder;\n\tat org.slf4j.LoggerFactory.bind(LoggerFactory.java:128)\n\tat org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:107)\n\tat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:295)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:269)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281)\n\tat org.apache.mahout.driver.MahoutDriver.<clinit>(MahoutDriver.java:89)\n\n\nMarked Blocker since script just plain does not run.\nHere is the complete trace from running the script under shell's -x option:\n\n\n@mac bin [trunk] $ sh -x classify-20newsgroups.sh \n+ '[' '' = --help ']'\n+ '[' '' = '--?' ']'\n+ SCRIPT_PATH=classify-20newsgroups.sh\n+ '[' classify-20newsgroups.sh '!=' classify-20newsgroups.sh ']'\n++ pwd\n+ START_PATH=/Users/lancenorskog/Documents/open/mahout/examples/bin\n+ WORK_DIR=/tmp/mahout-work-lancenorskog\n+ algorithm=(cnaivebayes naivebayes sgd clean)\n+ '[' -n '' ']'\n+ echo 'Please select a number to choose the corresponding task to run'\nPlease select a number to choose the corresponding task to run\n+ echo '1. cnaivebayes'\n1. cnaivebayes\n+ echo '2. naivebayes'\n2. naivebayes\n+ echo '3. sgd'\n3. sgd\n+ echo '4. clean -- cleans up the work area in /tmp/mahout-work-lancenorskog'\n4. clean -- cleans up the work area in /tmp/mahout-work-lancenorskog\n+ read -p 'Enter your choice : ' choice\nEnter your choice : 1\n+ echo 'ok. You chose 1 and we'\\''ll use cnaivebayes'\nok. You chose 1 and we'll use cnaivebayes\n+ alg=cnaivebayes\n+ echo 'creating work directory at /tmp/mahout-work-lancenorskog'\ncreating work directory at /tmp/mahout-work-lancenorskog\n+ mkdir -p /tmp/mahout-work-lancenorskog\n+ '[' '!' -e /tmp/mahout-work-lancenorskog/20news-bayesinput ']'\n+ '[' '!' -e /tmp/mahout-work-lancenorskog/20news-bydate ']'\n+ cd /Users/lancenorskog/Documents/open/mahout/examples/bin\n+ cd ../..\n+ set -e\n+ '[' xcnaivebayes == xnaivebayes -o xcnaivebayes == xcnaivebayes ']'\n+ c=\n+ '[' xcnaivebayes == xcnaivebayes ']'\n+ c=' -c'\n+ set -x\n+ echo 'Preparing 20newsgroups data'\nPreparing 20newsgroups data\n+ rm -rf /tmp/mahout-work-lancenorskog/20news-all\n+ mkdir /tmp/mahout-work-lancenorskog/20news-all\n+ cp -R /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/alt.atheism /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/comp.graphics /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/comp.os.ms-windows.misc /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/comp.sys.ibm.pc.hardware /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/comp.sys.mac.hardware /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/comp.windows.x /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/misc.forsale /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/rec.autos /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/rec.motorcycles /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/rec.sport.baseball /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/rec.sport.hockey /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/sci.crypt /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/sci.electronics /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/sci.med /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/sci.space /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/soc.religion.christian /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/talk.politics.guns /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/talk.politics.mideast /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/talk.politics.misc /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-test/talk.religion.misc /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/alt.atheism /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/comp.graphics /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/comp.os.ms-windows.misc /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/comp.sys.ibm.pc.hardware /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/comp.sys.mac.hardware /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/comp.windows.x /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/misc.forsale /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/rec.autos /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/rec.motorcycles /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/rec.sport.baseball /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/rec.sport.hockey /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/sci.crypt /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/sci.electronics /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/sci.med /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/sci.space /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/soc.religion.christian /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/talk.politics.guns /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/talk.politics.mideast /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/talk.politics.misc /tmp/mahout-work-lancenorskog/20news-bydate/20news-bydate-train/talk.religion.misc /tmp/mahout-work-lancenorskog/20news-all\n+ echo 'Creating sequence files from 20newsgroups data'\nCreating sequence files from 20newsgroups data\n+ ./bin/mahout seqdirectory -i /tmp/mahout-work-lancenorskog/20news-all -o /tmp/mahout-work-lancenorskog/20news-seq\nMAHOUT_LOCAL is set, so we don't add HADOOP_CONF_DIR to classpath.\nhadoop binary is not in PATH,HADOOP_HOME/bin,HADOOP_PREFIX/bin, running locally\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/Users/lancenorskog/Documents/open/mahout/examples/target/mahout-examples-0.8-SNAPSHOT-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/lancenorskog/Documents/open/mahout/examples/target/dependency/slf4j-jcl-1.7.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/Users/lancenorskog/Documents/open/mahout/examples/target/dependency/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: slf4j-api 1.6.x (or later) is incompatible with this binding.\nSLF4J: Your binding is version 1.5.5 or earlier.\nSLF4J: Upgrade your binding to version 1.6.x.\nException in thread \"main\" java.lang.NoSuchMethodError: org.slf4j.impl.StaticLoggerBinder.getSingleton()Lorg/slf4j/impl/StaticLoggerBinder;\n\tat org.slf4j.LoggerFactory.bind(LoggerFactory.java:128)\n\tat org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:107)\n\tat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:295)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:269)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281)\n\tat org.apache.mahout.driver.MahoutDriver.<clinit>(MahoutDriver.java:89)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1118",
            "/jira/browse/MAHOUT-1399"
        ]
    },
    "MAHOUT-1112": {
        "Key": "MAHOUT-1112",
        "Summary": "Migrate code from Lucene / Solr 3.6 to 4.0.0",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Andrew Janowczyk",
        "Created": "12/Nov/12 11:37",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "30/Jan/13 10:29",
        "Description": "Lucene/Solr 4.0.0 was released on October 12, 2012 \nMany of the functionalities have changed, so it wasn't an easy migration.\nI'm including a patch, which passes all junit tests, for review.",
        "Issue Links": []
    },
    "MAHOUT-1113": {
        "Key": "MAHOUT-1113",
        "Summary": "Need test case to demonstrate simple use of SGD",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "14/Nov/12 01:06",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "14/Nov/12 01:12",
        "Description": "Need a test case that shows how to use SGD on a well known data set like the Iris data.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1107"
        ]
    },
    "MAHOUT-1114": {
        "Key": "MAHOUT-1114",
        "Summary": "Some delegating vectors have subtle clone bug",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "14/Nov/12 01:14",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "14/Nov/12 01:56",
        "Description": "Cloning a Centroid returns a WeightedVector.",
        "Issue Links": []
    },
    "MAHOUT-1115": {
        "Key": "MAHOUT-1115",
        "Summary": "[PATCH] Add values() method to FastByIDMap",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "collections",
        "Assignee": "Sean R. Owen",
        "Reporter": "Julien Aym\u00e9",
        "Created": "15/Nov/12 16:25",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "15/Nov/12 19:19",
        "Description": "The values() view of FastByIDMap is missing, where as it is implemented in FastMap. It should also be provided in FastByIDMap. Patch will follow.",
        "Issue Links": []
    },
    "MAHOUT-1116": {
        "Key": "MAHOUT-1116",
        "Summary": "WeightedVectors do not implement equals()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": "Sean R. Owen",
        "Reporter": "Dan Filimon",
        "Created": "16/Nov/12 10:51",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "16/Nov/12 18:06",
        "Description": "WeightedVectors [1] implement compareTo but not equals so in cases where just the weights or the indices differ, despire compareTo returning 0, equals returns false.\n[1] https://github.com/apache/mahout/blob/trunk/math/src/main/java/org/apache/mahout/math/WeightedVector.java",
        "Issue Links": []
    },
    "MAHOUT-1117": {
        "Key": "MAHOUT-1117",
        "Summary": "Vectors are not hashable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "16/Nov/12 10:56",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 17:42",
        "Description": "No *Vector classes (DenseVector, WeightedVector, etc.) implement hashCode().\nIn working on improving clustering in Mahout, Ted Dunning wrote prototype code for Streaming KMeans and Ball KMeans, that I'm working with him on. These need to be used together in the MapReduce version.\nHowever, in Ball KMeans, we initialize the clusters using a probabilistic approach similar to k-means++. This however requires a Multinomial<WeightedVector> distribution of the points we want to cluster to pick the centroids.\nInternally, the Multinomial<T> uses a HashMap to keep track of the values it can sample from.\nSince Vectors don't override Object's hashCode(), it is possible to get the same value multiple times in the map (as long as the references differ).\nThis is less of an issue because of how we're adding the vectors to the multinomial (we can guarantee that the references will be unique) and once MAHOUT-1116 is resolved the hashing will work okay for our needs.\nIt still seems that it would be useful to have hashable vectors.\nWhat do you think? And what would a hash function look like?",
        "Issue Links": []
    },
    "MAHOUT-1118": {
        "Key": "MAHOUT-1118",
        "Summary": "SLF4J Log4j bindings are messed up causing examples to fail",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "18/Nov/12 13:13",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "19/Nov/12 13:34",
        "Description": "We are routinely seeing the following failures when running the examples on Jenkins and they are due to old SLF4j bindings on Cassandra and HBase:\n\nTraining on /tmp/mahout-work-jenkins/20news-bydate/20news-bydate-train/\nhadoop binary is not in PATH,HADOOP_HOME/bin,HADOOP_PREFIX/bin, running locally\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/x1/jenkins/jenkins-slave/workspace/Mahout-Examples-Classify-20News/trunk/examples/target/mahout-examples-0.8-SNAPSHOT-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/x1/jenkins/jenkins-slave/workspace/Mahout-Examples-Classify-20News/trunk/examples/target/dependency/slf4j-jcl-1.7.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/x1/jenkins/jenkins-slave/workspace/Mahout-Examples-Classify-20News/trunk/examples/target/dependency/slf4j-log4j12-1.4.3.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: slf4j-api 1.6.x (or later) is incompatible with this binding.\nSLF4J: Your binding is version 1.5.5 or earlier.\nSLF4J: Upgrade your binding to version 1.6.x.\nException in thread \"main\" java.lang.NoSuchMethodError: org.slf4j.impl.StaticLoggerBinder.getSingleton()Lorg/slf4j/impl/StaticLoggerBinder;\n\tat org.slf4j.LoggerFactory.bind(LoggerFactory.java:128)\n\tat org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:107)\n\tat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:295)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:269)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281)\n\tat org.apache.mahout.driver.MahoutDriver.<clinit>(MahoutDriver.java:89)\nCould not find the main class: org.apache.mahout.driver.MahoutDriver.  Program will exit.\nBuild step 'Execute shell' marked build as failure\nSending e-mails to: dev@mahout.apache.org ssc.open@googlemail.com p",
        "Issue Links": [
            "/jira/browse/MAHOUT-1099",
            "/jira/browse/MAHOUT-1111",
            "/jira/browse/MAHOUT-1399"
        ]
    },
    "MAHOUT-1119": {
        "Key": "MAHOUT-1119",
        "Summary": "code bug in org.apache.mahout.text.SequenceFilesFromDirectory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Sebastian Schelter",
        "Reporter": "\u5f90\u5bb6",
        "Created": "22/Nov/12 03:21",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 15:43",
        "Description": "in  org.apache.mahout.text.SequenceFilesFromDirectory from line 89 to 96 the code is \n  pathFilterClass.getConstructor(Configuration.class,\n                                           String.class,\n                                           Map.class,\n                                           ChunkedWriter.class,\n                                           Charset.class,\n                                           FileSystem.class);\n        pathFilter = constructor.newInstance(conf, keyPrefix, options, writer, charset,fs);\nobviously,the method  \"constructor.newInstance\" lacks a parameter \"charset\",if i implements a subclass of SequenceFilesFromDirectoryFilter,there will be a runtime error.",
        "Issue Links": []
    },
    "MAHOUT-1120": {
        "Key": "MAHOUT-1120",
        "Summary": "Mahout examples script execution fails as part of rpm install.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7,                                            0.8",
        "Component/s": "Examples",
        "Assignee": null,
        "Reporter": "Ashish Singh",
        "Created": "29/Nov/12 05:48",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 20:42",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1121": {
        "Key": "MAHOUT-1121",
        "Summary": "Centroid clone() not implemented",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Incomplete",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "29/Nov/12 21:34",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "06/Mar/13 15:43",
        "Description": "Mahout's Centroid class doesn't implement clone(). The returned type is not Centroid so it needs an annoying cast every time.",
        "Issue Links": []
    },
    "MAHOUT-1122": {
        "Key": "MAHOUT-1122",
        "Summary": "Mahout prints usage statement AND executes the request class which is confusing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "CLI",
        "Assignee": null,
        "Reporter": "Clint Heath",
        "Created": "05/Dec/12 16:00",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 20:52",
        "Description": "We have seen an issue when running mahout on the CLI where if you do not include required arguments, the specified class still gets executed to completion, but Mahout prints a usage statement too, which is confusing.  If a required argument is missed, print usage and exit.\nHere is the log:\n[server~]$ mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job -Dpool.name=analytics-pool \nMAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath. \nRunning on hadoop, using /usr/lib/hadoop/bin/hadoop and HADOOP_CONF_DIR=/etc/hadoop/conf \nMAHOUT-JOB: /usr/lib/mahout/mahout-examples-0.7-cdh4.1.1-job.jar \n12/11/29 00:01:37 WARN driver.MahoutDriver: No org.apache.mahout.clustering.syntheticcontrol.kmeans.Job.props found on classpath, will use command-line arguments only \n12/11/29 00:01:37 INFO kmeans.Job: Running with only user-supplied arguments \n12/11/29 00:01:38 ERROR common.AbstractJob: Missing required option --t1 \nusage: <command> [Generic Options] [Job-Specific Options] \nGeneric Options: \n-archives <paths> comma separated archives to be unarchived \non the compute machines. \n-conf <configuration file> specify an application configuration file \n-D <property=value> use value for given property \n-files <paths> comma separated files to be copied to the \nmap reduce cluster \n-fs <local|namenode:port> specify a namenode \n-jt <local|jobtracker:port> specify a job tracker \n-libjars <paths> comma separated jar files to include in \nthe classpath. \n-tokenCacheFile <tokensFile> name of the file with the tokens \nMissing required option --t1 \nUsage: \n[--input <input> --output <output> --distanceMeasure <distanceMeasure> \n--numClusters <k> --t1 <t1> --t2 <t2> --convergenceDelta <convergenceDelta> \n--maxIter <maxIter> --overwrite --help --tempDir <tempDir> --startPhase \n<startPhase> --endPhase <endPhase>] \n--t1 (-t1) t1 T1 threshold value \n12/11/29 00:01:38 INFO driver.MahoutDriver: Program took 192 ms (Minutes: 0.0032)",
        "Issue Links": []
    },
    "MAHOUT-1123": {
        "Key": "MAHOUT-1123",
        "Summary": "Support Lucene 3.6 analyzers for vectorization",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chris Birchall",
        "Created": "07/Dec/12 00:26",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Mar/13 21:37",
        "Description": "Passing a Lucene analyzer class name to Mahout (e.g. seq2sparse --analyzerName) results in failure with the error shown below.\nThis is caused by Mahout trying to instantiate the analyzer using a zero-argument constructor. The zero-arg constructors were removed from the standard Lucene analyzers in Lucene 3.6 (if I recall correctly).\nThis patch adds support for the new one-arg constructors, as well as keeping support for the legacy analyzers.\n=====\nException in thread \"main\" java.lang.IllegalStateException: java.lang.NoSuchMethodException: org.apache.lucene.analysis.standard.StandardAnalyzer.<init>()\n        at org.apache.mahout.common.ClassUtils.instantiateAs(ClassUtils.java:68)\n        at org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.run(SparseVectorsFromSequenceFiles.java:204)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.main(SparseVectorsFromSequenceFiles.java:55)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\nCaused by: java.lang.NoSuchMethodException: org.apache.lucene.analysis.standard.StandardAnalyzer.<init>()\n        at java.lang.Class.getConstructor0(Class.java:2706)\n        at java.lang.Class.getConstructor(Class.java:1657)\n        at org.apache.mahout.common.ClassUtils.instantiateAs(ClassUtils.java:62)\n        ... 11 more",
        "Issue Links": []
    },
    "MAHOUT-1124": {
        "Key": "MAHOUT-1124",
        "Summary": "Deprecate ununsed recommenders",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sebastian Schelter",
        "Created": "07/Dec/12 08:56",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "07/Dec/12 09:03",
        "Description": "Deprecating some recommenders for the following reasons:\norg.apache.mahout.cf.taste.impl.recommender.svd.FunkSVDFactorizer\nRatingSGDFactorizer should be learning faster and has a nicer model as\nit includes user/item biases\norg.apache.mahout.cf.taste.impl.recommender.svd.ImplicitLinearRegressionFactorizer\nSeems to be using the same model as ALSWRFactorizer, however there are\nno tests and ALSWR can handle more explicit and implicit feedback\norg.apache.mahout.cf.taste.impl.recommender.TreeClusteringRecommender\norg.apache.mahout.cf.taste.impl.recommender.TreeClusteringRecommender2\norg.apache.mahout.cf.taste.impl.recommender.knn\nI don't recall anybody using those or asking about them the last years.",
        "Issue Links": []
    },
    "MAHOUT-1125": {
        "Key": "MAHOUT-1125",
        "Summary": "DatasetSplitter.run doesn't parseArguments before getOption so throws and exception always",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Pat Ferrel",
        "Created": "08/Dec/12 17:32",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "17/Jan/13 19:02",
        "Description": "In DatasetSplitter.run it looks like getOption is being called before the parseArguments. When I do this \nMap<String,List<String>> parsedArgs = parseArguments(args);\nif (parsedArgs == null) {\n return -1;\n}\nbefore any call to getOption in DatasetSplitter.run it completes correctly. Not exactly sure how this is supposed to be done, it doesn't look like the options get parsed in the super class automatically.\nThis will cause any invocation of splitDataset or DatasetSplitter to crash running the current trunk. \nOn Dec 5, 2012, at 1:58 PM, Pat Ferrel <pat.ferrel@gmail.com> wrote:\ndoes anyone know if mahout/examples/bin/factorize-movielens-1M.sh is still working? CLI version of splitDataset is crashing in my build (latest trunk). Even as in \"mahout splitDataset\" to get the params. Wouldn't be the first time I mucked up a build though.",
        "Issue Links": []
    },
    "MAHOUT-1126": {
        "Key": "MAHOUT-1126",
        "Summary": "Mac builds won't unjar",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Pat Ferrel",
        "Created": "10/Dec/12 04:47",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 12:40",
        "Description": "On the Mac you have to remove the licenses in the mahout jar or hadoop can't unjar mahout. The Mac has a case insensitive file system and so can't tell the difference between LICENSE and license. This was fixed at one point https://issues.apache.org/jira/browse/MAHOUT-780\nzip -d mahout/examples/target/mahout-examples-0.8-SNAPSHOT-job.jar META-INF/license/\nzip -d mahout/examples/target/mahout-examples-0.8-SNAPSHOT-job.jar META-INF/LICENSE/\nLooks like as is mentioned in https://issues.apache.org/jira/browse/MAHOUT-780 \nmv target/maven-shared-archive-resources/META-INF/LICENSE target/maven-shared-archive-resources/META-INF/LICENSES\nworks too.\nCan this get a permanent fix?",
        "Issue Links": []
    },
    "MAHOUT-1127": {
        "Key": "MAHOUT-1127",
        "Summary": "OnlineLogisticRegression test is flaky (and wrong)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "15/Dec/12 00:33",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "15/Dec/12 00:36",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1128": {
        "Key": "MAHOUT-1128",
        "Summary": "MAHOUT-999 issue still actual",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Andrey Davydov",
        "Created": "17/Dec/12 14:59",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 20:58",
        "Description": "I'm sorry my english is not well and I'm newbie with Mahout. But it seems that MAHOUT-999 issue still actual.\nI use mahout-core 0.7 loaded from maven-central and I've got the same fail. \nI've investigate sources and found following in the org.apache.mahout.clustering.classify.ClusterClassifier class:\n  public void writeToSeqFiles(Path path) throws IOException {\n    writePolicy(policy, path);\n    Configuration config = new Configuration();\n    FileSystem fs = FileSystem.get(path.toUri(), config);\n    SequenceFile.Writer writer = null;\n    ClusterWritable cw = new ClusterWritable();\n    for (int i = 0; i < models.size(); i++) \n{\n...\n      }\n finally \n{\n        Closeables.closeQuietly(writer);\n      }\n    }\n  }\n  public void readFromSeqFiles(Configuration conf, Path path) throws IOException {\n    Configuration config = new Configuration();\n    List<Cluster> clusters = Lists.newArrayList();\n    for (ClusterWritable cw : new SequenceFileDirValueIterable<ClusterWritable>(path, PathType.LIST,\n        PathFilters.logsCRCFilter(), config)) \n{\n...\n    }\n    this.models = clusters;\n    modelClass = models.get(0).getClass().getName();\n    this.policy = readPolicy(path);\n  }\nBoth methods use new default Configuration and they try to work with local file system. I.e. KMeansDriver wrote initial clusters to local file system of the \"client\" system and CIMapper try to read it from cluster node local file system.\nIt seems that current implementation can work only pseudo-distributed hadoop system. I think that ClusterClassifier should store intermediate results in the HDFS using Configuration passed by api from user.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1201"
        ]
    },
    "MAHOUT-1129": {
        "Key": "MAHOUT-1129",
        "Summary": "Mahout Java Heap Out of Memory in TrainNewsGroup class",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Classification,                                            Examples",
        "Assignee": null,
        "Reporter": "Huned Lokhandwala",
        "Created": "18/Dec/12 20:13",
        "Updated": "02/Jan/13 19:17",
        "Resolved": "02/Jan/13 19:16",
        "Description": "On Mahout on Linux RHEL 5.6 (Single node, no cluster), a Java Heap Space Out of Memory Exception occurs when calling the TrainNewsGroup class from the mahout-examples-0.7.0.17-job.jar file. I downloaded the 20news-bydate.tar.gz (from http://people.csail.mit.edu/jrennie/20Newsgroups/20news-bydate.tar.gz ) to use as classification data and added it in the folder as below, and called the org.apache.mahout.classifier.sgd.TrainNewsGroups on the folder and found the Java Heap Space Out of Memory Exception as show below.\nOutput from run:\n> /usr/lib/mahout/bin/mahout org.apache.mahout.classifier.sgd.TrainNewsGroups /artifacts/mahout_sgd_classifier/20news-bydate-train\nRunning on hadoop, using /usr/bin/hadoop and HADOOP_CONF_DIR=\nMAHOUT-JOB: /usr/lib/mahout/mahout-examples-0.7.0.17-job.jar\n12/12/17 20:35:06 WARN driver.MahoutDriver: No org.apache.mahout.classifier.sgd.TrainNewsGroups.props found on classpath, will use command-line arguments only\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n\tat org.apache.mahout.math.DenseMatrix.<init>(DenseMatrix.java:50)\n\tat org.apache.mahout.classifier.sgd.OnlineLogisticRegression.<init>(OnlineLogisticRegression.java:60)\n\tat org.apache.mahout.classifier.sgd.CrossFoldLearner.<init>(CrossFoldLearner.java:67)\n\tat org.apache.mahout.classifier.sgd.CrossFoldLearner.copy(CrossFoldLearner.java:217)\n\tat org.apache.mahout.classifier.sgd.AdaptiveLogisticRegression$Wrapper.copy(AdaptiveLogisticRegression.java:399)\n\tat org.apache.mahout.classifier.sgd.AdaptiveLogisticRegression$Wrapper.copy(AdaptiveLogisticRegression.java:386)\n\tat org.apache.mahout.ep.State.copy(State.java:94)\n\tat org.apache.mahout.ep.State.mutate(State.java:114)\n\tat org.apache.mahout.ep.EvolutionaryProcess.initializePopulation(EvolutionaryProcess.java:103)\n\tat org.apache.mahout.ep.EvolutionaryProcess.<init>(EvolutionaryProcess.java:97)\n\tat org.apache.mahout.classifier.sgd.AdaptiveLogisticRegression.setupOptimizer(AdaptiveLogisticRegression.java:279)\n\tat org.apache.mahout.classifier.sgd.AdaptiveLogisticRegression.setPoolSize(AdaptiveLogisticRegression.java:265)\n\tat org.apache.mahout.classifier.sgd.AdaptiveLogisticRegression.<init>(AdaptiveLogisticRegression.java:121)\n\tat org.apache.mahout.classifier.sgd.AdaptiveLogisticRegression.<init>(AdaptiveLogisticRegression.java:100)\n\tat org.apache.mahout.classifier.sgd.TrainNewsGroups.main(TrainNewsGroups.java:100)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
        "Issue Links": []
    },
    "MAHOUT-1130": {
        "Key": "MAHOUT-1130",
        "Summary": "Wrong logic in org.apache.mahout.clustering.kmeans.RandomSeedGenerator",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Andrey Davydov",
        "Created": "20/Dec/12 13:45",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Mar/13 07:53",
        "Description": "There is following code in line 101:\n              } else if (random.nextInt(currentSize + 1) != 0) { // with chance 1/(currentSize+1) pick new element\nbut it actually means pick new element with chance currentSize/(currentSize+1)\nso generator takes initial centers from the end of source data file.\nIt seems that chance of replace vector in output set should decrease with number of processed input vectors",
        "Issue Links": []
    },
    "MAHOUT-1131": {
        "Key": "MAHOUT-1131",
        "Summary": "Can't execute alternative FPG implementation from command line",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kirill A. Korinskiy",
        "Created": "30/Dec/12 09:24",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Mar/13 18:55",
        "Description": "When I execute: ./bin/mahout fpg -i input -o output -2 option -2 \u2014 execute alternative FPG implementation didn't work.\nFollow patch fix it.",
        "Issue Links": []
    },
    "MAHOUT-1132": {
        "Key": "MAHOUT-1132",
        "Summary": "fpgrowth2 crash when have not unique items in one line",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Kirill A. Korinskiy",
        "Created": "30/Dec/12 09:32",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 21:46",
        "Description": "I create follow file as input for fpgrowth2:\n0, 0, 0\n0, 0, 0\n0, 0, 0\nand when I run ./bin/mahout -i kv -o output -2 --mathod mapreduct I take a crash:\njava.lang.IllegalStateException: mismatched counts for targetAttr=0, (3 != 9); thisTree=[FPTree\n  -\n{attr:-1, cnt:0}\n1>-\n{attr:0, cnt:3}\n]\n\tat org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPTree.createMoreFreqConditionalTree(FPTree.java:259)\n\tat org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthIds.growth(FPGrowthIds.java:238)\n\tat org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthIds.fpGrowth(FPGrowthIds.java:163)\n\tat org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthIds.generateTopKFrequentPatterns(FPGrowthIds.java:220)\n\tat org.apache.mahout.fpm.pfpgrowth.fpgrowth2.FPGrowthIds.generateTopKFrequentPatterns(FPGrowthIds.java:115)\n\tat org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthReducer.reduce(ParallelFPGrowthReducer.java:99)\n\tat org.apache.mahout.fpm.pfpgrowth.ParallelFPGrowthReducer.reduce(ParallelFPGrowthReducer.java:48)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:417)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\nFollow patch fix it.",
        "Issue Links": []
    },
    "MAHOUT-1133": {
        "Key": "MAHOUT-1133",
        "Summary": "mahout-examples-0.7-job.jar loses hadoop libary",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Examples",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "31/Dec/12 10:48",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "31/Dec/12 11:33",
        "Description": "mahout-examples-0.7-job.jar loses hadoop libary , the classification example can not run .",
        "Issue Links": []
    },
    "MAHOUT-1134": {
        "Key": "MAHOUT-1134",
        "Summary": "mahout script does not convert path MAHOUT-JOB when run on Cygwin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "CLI",
        "Assignee": "Sean R. Owen",
        "Reporter": "Han Hui Wen",
        "Created": "31/Dec/12 10:57",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "31/Dec/12 11:34",
        "Description": "script :mahout\nelse\n  echo \"Running on hadoop, using $HADOOP_BINARY and HADOOP_CONF_DIR=$HADOOP_CONF_DIR\"\n  if [ \"$MAHOUT_JOB\" = \"\" ] ; then\n    echo \"ERROR: Could not find mahout-examples-*.job in $MAHOUT_HOME or $MAHOUT_HOME/examples/target, please run 'mvn install' to create the .job file\"\n    exit 1\n  else\n    case \"$1\" in\n    (hadoop)\n      shift\n      export HADOOP_CLASSPATH=$MAHOUT_CONF_DIR:${HADOOP_CLASSPATH}:$CLASSPATH\n      exec \"$HADOOP_BINARY\" \"$@\"\n      ;;\n    (classpath)\n      echo $CLASSPATH\n      ;;\n\n      echo \"MAHOUT-JOB: $MAHOUT_JOB\"\n      export HADOOP_CLASSPATH=$MAHOUT_CONF_DIR:${HADOOP_CLASSPATH}\n\nwe need convert $MAHOUT_JOB before it been passed to hadoop .\n      exec \"$HADOOP_BINARY\" jar $MAHOUT_JOB $CLASS \"$@\"\n    esac\n  fi\nfi\n\nerror message :\nException in thread \"main\" java.io.IOException: Error opening job jar: /usr/local/mahout-distribution-0.7/mahout-example\ns-0.7-job.jar\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:130)\nCaused by: java.io.FileNotFoundException: \\usr\\local\\mahout-distribution-0.7\\mahout-examples-0.7-job.jar (The system can\nnot find the path specified)\n        at java.util.zip.ZipFile.open(Native Method)\n        at java.util.zip.ZipFile.<init>(ZipFile.java:214)\n        at java.util.zip.ZipFile.<init>(ZipFile.java:144)\n        at java.util.jar.JarFile.<init>(JarFile.java:152)\n        at java.util.jar.JarFile.<init>(JarFile.java:89)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:128)",
        "Issue Links": []
    },
    "MAHOUT-1135": {
        "Key": "MAHOUT-1135",
        "Summary": "Unify decorated vectors in DecoratedVector<T>",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "01/Jan/13 20:03",
        "Updated": "03/May/13 20:32",
        "Resolved": "03/May/13 15:12",
        "Description": "I'm finding the current Vector classes in Mahout a bit confusing.\nThe vector implementation are just fine, I'm talking more about the decorated vectors:\nWeightedVector\nMatrixSlice\nNamedVector\nI propose using a single DecoratedVector<T> type that can easily be extended.\nFor example, right now MatrixSlice doesn't even implement the Vector interface.\nSo,\nWeightedVector -> DecoratedVector<Pair<Integer, Double>>\nMatrixSlice -> DecoratedVector<Integer>\nNamedVector -> DecoratedVector<String>\nWe could even keep the names (maybe changing MatrixSlice to something like IndexedVector though?) by extending DecoratedVector<T>.\nI'd be willing to fix this if people think it's a good idea.\nWhat about it?",
        "Issue Links": []
    },
    "MAHOUT-1136": {
        "Key": "MAHOUT-1136",
        "Summary": "Cannot import project into eclipse with m2e 1.2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Stevo Slavi\u0107",
        "Created": "05/Jan/13 20:36",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "09/Jan/13 23:18",
        "Description": "Seems fix for MAHOUT-1043 wasn't good, in pluginExecutionFilter instead of version, versionRange should be used.\nRelated SO entry: http://stackoverflow.com/a/6701595/381140",
        "Issue Links": [
            "/jira/browse/MAHOUT-1043"
        ]
    },
    "MAHOUT-1137": {
        "Key": "MAHOUT-1137",
        "Summary": "Same to MAHOUT-1061: ClassNotFoundException in mahout split -xm mapreduce: org.apache.mahout.utils.SplitInputJob$SplitInputMapper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Shengchao Ding",
        "Created": "10/Jan/13 21:11",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 08:41",
        "Description": "I'm running the 20 newsgroups examples on virtual machine of CDH4.1.2.\nIt ran smoothly but failed if I modify the split command to\nmahout split \\\n    -i newsgroup/vectors \\\n    --trainingOutput newsgroup/train-vectors \\\n    --testOutput newsgroup/test-vectors  \\\n    --randomSelectionPct 40 --overwrite --sequenceFiles -xm mapreduce\n-mro newsgroup/mro\nThe only different to original command is that the method is modified\nto mapreduce while the original example is sequential.\nI got the following exception.\nError: java.lang.RuntimeException: java.lang.ClassNotFoundException:\nClass org.apache.mahout.utils.SplitInputJob$SplitInputMapper not found\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1571)\n        at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:685)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:152)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1332)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:147)\nCaused by: java.lang.ClassNotFoundException: Class\norg.apache.mahout.utils.SplitInputJob$SplitInputMapper not found\n        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1477)\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:1569)\n        ... 8 more\nI checked the mahout package on the distribution as follows.\n[cloudera@localhost ~]$ jar tf\n/usr/lib/mahout/mahout-examples-0.7-cdh4.1.2-job.jar | grep SplitInput\norg/apache/mahout/utils/SplitInputJob$SplitInputReducer.class\norg/apache/mahout/utils/SplitInputJob$SplitInputMapper.class\norg/apache/mahout/utils/SplitInputJob$SplitInputComparator.class\norg/apache/mahout/utils/SplitInputJob.class\norg/apache/mahout/utils/SplitInput.class\norg/apache/mahout/utils/SplitInput$SplitCallback.class",
        "Issue Links": []
    },
    "MAHOUT-1138": {
        "Key": "MAHOUT-1138",
        "Summary": "Clean up some findbugs warnings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "11/Jan/13 00:53",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Jan/13 01:00",
        "Description": "Several of the high priority warnings are due to classes such as IntPairWritable that advertise Serializable, but which are coded incorrectly (and never used that way).\nSimple fixes.  Tests runs.",
        "Issue Links": []
    },
    "MAHOUT-1139": {
        "Key": "MAHOUT-1139",
        "Summary": "Off by one error in LSMR",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "11/Jan/13 02:09",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 21:52",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1140": {
        "Key": "MAHOUT-1140",
        "Summary": "Uniform random sampling problem in RandomSeedGenerator.java",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "sam wu",
        "Created": "13/Jan/13 22:48",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/Mar/13 21:41",
        "Description": "Uniform random sampling in RandomSeedGenerator.java is buggy. The selected samplings will be highly toward ending samples, eg: sample size=1000, k=3, almost all selected samples will be >=980th sample. \nThis problem also exists in MiA code, ch09 at RandomPointUtils.java, chooseRandomPoints() method.",
        "Issue Links": []
    },
    "MAHOUT-1141": {
        "Key": "MAHOUT-1141",
        "Summary": "Driver for cvb0_local does not warn about missing maxIterations command line parameter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Samar Lotia",
        "Created": "16/Jan/13 17:08",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Mar/13 19:46",
        "Description": "The driver for cvb0_local does not seem to verify whether the caller has specified the required maxIterations command line parameter. This results in an exception much further down which pretty much requires looking at the source to discover the source of the error.\nException in thread \"main\" java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String\n\tat org.apache.mahout.clustering.lda.cvb.InMemoryCollapsedVariationalBayes0.main2(InMemoryCollapsedVariationalBayes0.java:374)\n\tat org.apache.mahout.clustering.lda.cvb.InMemoryCollapsedVariationalBayes0.run(InMemoryCollapsedVariationalBayes0.java:521)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.clustering.lda.cvb.InMemoryCollapsedVariationalBayes0.main(InMemoryCollapsedVariationalBayes0.java:525)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)",
        "Issue Links": []
    },
    "MAHOUT-1142": {
        "Key": "MAHOUT-1142",
        "Summary": "TestDistributedLanczosSolverCLI.testDistributedLanczosSolverEVJCLI is broken with Hadoop 0.23.5",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Jimmy Xiang",
        "Created": "18/Jan/13 04:04",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jan/13 11:41",
        "Description": "Stacktrace\njava.lang.AssertionError: number of clean eigenvectors expected:<3> but was:<0>\nat org.junit.Assert.fail(Assert.java:91)\nat org.junit.Assert.failNotEquals(Assert.java:645)\nat org.junit.Assert.assertEquals(Assert.java:126)\nat org.junit.Assert.assertEquals(Assert.java:470)\nat org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI.testDistributedLanczosSolverEVJCLI(TestDistributedLanczosSolverCLI.java:143)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)\nat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)\nat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)\nat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)\nat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)\nat org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)\nat org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)\nat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)\nat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)\nat org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)\nat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)\nat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)\nat org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)\nat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)\nat org.junit.runners.ParentRunner.run(ParentRunner.java:236)\nat org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:236)\nat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:134)\nat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:113)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\nat java.lang.reflect.Method.invoke(Method.java:597)\nat org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189)\nat org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165)\nat org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85)\nat org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:103)\nat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:74)",
        "Issue Links": [
            "/jira/browse/HADOOP-8906"
        ]
    },
    "MAHOUT-1143": {
        "Key": "MAHOUT-1143",
        "Summary": "DecisionForest classifier should output label string instead of code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Abdel Hakim Deneche",
        "Reporter": "Abdel Hakim Deneche",
        "Created": "18/Jan/13 15:22",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "22/Jan/13 07:24",
        "Description": "when calling TestForest with a classification problem, output labels are numerical values corresponding to the label's internal code. TestForest should instead output the string label instead of the code.",
        "Issue Links": []
    },
    "MAHOUT-1144": {
        "Key": "MAHOUT-1144",
        "Summary": "Wrong normalization in SVD++",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "31/Jan/13 11:15",
        "Updated": "11/Mar/13 17:21",
        "Resolved": "11/Mar/13 16:18",
        "Description": "Reported by Agnonchik:\nSeems that I have found a discrepancy of this implementation from the original Yehuda Koren's SVD++ algorithm.\nline 140:\ndouble denominator = Math.sqrt(itemsByUser.size());\nshould be\ndouble denominator = Math.sqrt(itemsByUser.get(u).size());\nline 164:\ndouble denominator = Math.sqrt(itemsByUser.size());\nshould be\ndouble denominator = Math.sqrt(itemsByUser.get(u).size());\nThe sum of y parameters should be normalized by square root of number of items for which user u provided implicit feedback. Am I right?\nCurrently, it is normalized by square root of number of users not items.",
        "Issue Links": []
    },
    "MAHOUT-1145": {
        "Key": "MAHOUT-1145",
        "Summary": "Build Warnings after Lucene 4.1 upgrade",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "31/Jan/13 23:47",
        "Updated": "07/Apr/13 14:52",
        "Resolved": "01/Feb/13 11:26",
        "Description": "The Lucene upgrade changes seem to have added several warnings.\nhttps://builds.apache.org/job/Mahout-Quality/1845/pmdResult/new/",
        "Issue Links": []
    },
    "MAHOUT-1146": {
        "Key": "MAHOUT-1146",
        "Summary": "Cardinality exception bug in 'cross' method of AbstractVector class.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Ceyhun Can \u00dclker",
        "Created": "03/Feb/13 20:43",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 12:02",
        "Description": "This bug is probably about the viewColumn method, or matrixLike method or MatrixVectorView constructor, I didn't really looked into it. But it is easy to reproduce.\nI have a dense matrix U of size 10x670. When I try to do this I get a cardinality exception, which doesn't make sense:\nVector uI = U.viewColumn;\nMatrix outerProduct = uI.cross(uI);\nException is thrown from AbstractMatrix class' cross method, when a row is tried to be assigned in the result matrix.\n  public Matrix cross(Vector other) {\n    Matrix result = matrixLike(size, other.size());\n    for (int row = 0; row < size; row++) \n{\n      result.assignRow(row, other.times(getQuick(row)));\n    }\n    return result;\n  }\nThe problem here is matrixLike method does not return a 10x10 matrix as it should, instead it returns 1x1 matrix. \nHope this helps.",
        "Issue Links": []
    },
    "MAHOUT-1147": {
        "Key": "MAHOUT-1147",
        "Summary": "CVB Bug in CVB0Driver causes doc/topic distributions to be trained on random matrix",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Jake Mannix",
        "Reporter": "Jack Pay",
        "Created": "03/Feb/13 21:17",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Jun/13 05:44",
        "Description": "Problem:\nWhen training doc/topic model no paths for the term/topic model found (outputs null).\nThese paths are set using setModelPaths in CVB0Driver.\nReason for Problem:\nVariety of Job instances call this method. \nThe Job is passed to the method instead of the Configuration object given to the Job.\nThe configuration is retrieved from the Job instance itself.\nI believe that this Configuration instance is a clone of the original.\nThis is a problem as the variable MODEL_PATHS is set on the clone which is then discarded when the given Job is complete.\nThe original Configuration has no MODEL_PATHS String set and therefore returns null.\nThe code stipulates that if it cannot find a model to use a new random matrix. This happens every time as MODEL_PATHS is not set for the Configuration instance used.\nSolution:\nDo not pass the Job to the setModels method, but pass the Configuration instance passed into the method which created the Job.\ni.e.\nchange from:\nsetModelPaths(Job job, Path modelPath)\nto:\nsetModelPaths(Configuration conf, Path modelPath)\nAnd change all calling methods accordingly (obviously).\nSo far what little testing I have done appears to solve this problem.",
        "Issue Links": []
    },
    "MAHOUT-1148": {
        "Key": "MAHOUT-1148",
        "Summary": "QR Decomposition is too slow",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "04/Feb/13 00:40",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "05/Feb/13 18:39",
        "Description": "A user reported that QR decomposition is too slow.  I coded up a replacement that can be 10x faster under certain cases and the new version is also tested.",
        "Issue Links": []
    },
    "MAHOUT-1149": {
        "Key": "MAHOUT-1149",
        "Summary": "Partial Implementation wiki page is out of date",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Marty Kube",
        "Created": "10/Feb/13 01:04",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Mar/13 02:22",
        "Description": "The example on the wiki could use some minor updates.  I ran through the examples and the errors caused me a lot of noob confusion.\n1) The packages are incorrect.\n2) The training example uses the -oob parameter which is obsolete on conflicts with the -o parameter leading to noob confusion.\nI'd be happy to do the updates if I could get access to the wiki or could generate a patch.",
        "Issue Links": []
    },
    "MAHOUT-1150": {
        "Key": "MAHOUT-1150",
        "Summary": "ARFF Integration does not support quoted identifiers",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Marty Kube",
        "Created": "27/Feb/13 21:33",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 17:52",
        "Description": "I ran the NSL-KDD data set (http://nsl.cs.unb.ca/NSL-KDD/) through the ARFF integration.  The process failed to parse the arff formatted file.  The file has quoted identifiers:\n@relation 'KDDTrain-20Percent'\n@attribute 'duration' real\n@attribute 'protocol_type' \n{'tcp','udp', 'icmp'}\n \nThe quotes caused the problem.  The \"official\" arff BNF shows that quotes should be supported:\nhttps://list.scms.waikato.ac.nz/mailman/htdig/wekalist/2008-January/012153.html",
        "Issue Links": []
    },
    "MAHOUT-1151": {
        "Key": "MAHOUT-1151",
        "Summary": "Object reuse in distributed ALS",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "06/Mar/13 11:40",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 08:39",
        "Description": "In order to improve the performance our distributed ALS code, we should try to avoid object instantiation as much as possible, especially when it is done per input tuple.",
        "Issue Links": []
    },
    "MAHOUT-1152": {
        "Key": "MAHOUT-1152",
        "Summary": "mRMR feature selection algorithm",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Claudio Reggiani",
        "Created": "06/Mar/13 12:51",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "01/Jun/13 21:32",
        "Description": "Proposal Title: mRMR Feature Selection Algorithm on Map-Reduce.\nStudent Name: Claudio Reggiani\nStudent E-mail: nophiq@gmail.com\nProposal Abstract:\nThe mRMR algorithm, described in [1], is a feature selection algorithm that leverages mutual information evaluation to select features. At each iteration, mRMR selects a new feature based on both how much it's strongly correlated to the target output and how much it's less correlated to the features already selected. The correlation is measured by means of mutual information. The project proposes to provide the mRMR algorithm in MapReduce programming framework.\nAdditional information:\n1. The code is already available with some tests, because I'm working on my master thesis an initial milestone of my research was to implement mRMR algorithm in MapReduce.\n2. I'm figuring out if it's possible for me to apply at Google Summer of Code 2013.\nReferences: \n[1] Hanchuan Peng, Fuhui Long, and Chris Ding\nIEEE Transactions on Pattern Analysis and Machine Intelligence,\nVol. 27, No. 8, pp.1226-1238, 2005.\nLink: http://penglab.janelia.org/papersall/docpdf/2005_TPAMI_FeaSel.pdf",
        "Issue Links": []
    },
    "MAHOUT-1153": {
        "Key": "MAHOUT-1153",
        "Summary": "Implement streaming random forests",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Andy Twigg",
        "Created": "06/Mar/13 14:22",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "18/May/14 06:21",
        "Description": "The current random forest implementations are in-core and not scalable. This issue is to add an out-of-core, scalable, streaming implementation. Initially it could be based on [1], and using mappers in a master-worker style.\n[1] http://jmlr.csail.mit.edu/papers/volume11/ben-haim10a/ben-haim10a.pdf",
        "Issue Links": []
    },
    "MAHOUT-1154": {
        "Key": "MAHOUT-1154",
        "Summary": "Implementing Streaming KMeans",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Dan Filimon",
        "Reporter": "Dan Filimon",
        "Created": "06/Mar/13 15:19",
        "Updated": "02/Dec/13 01:58",
        "Resolved": "02/Jun/13 19:42",
        "Description": "An implementation of Streaming KMeans as mentioned in [1] is available here [2].\n[1]http://mail-archives.apache.org/mod_mbox/mahout-dev/201303.mbox/%3CCAOwb3gOyf9zufrgXHsucpkJXk6cW0Nnr8GwG__JSey+kVABeyg@mail.gmail.com%3E\n[2] https://github.com/dfilimon/mahout\nSince there will be more than one patches, there will be specific JIRA issues that address each one.\nThe description of the code being added is:\nThe main classes are in o.a.m.clustering.streaming [1], under the\ncore/ project. These are subdivided into 2 packages:\n\ncluster: contains the BallKMeans and StreamingKMeans classes that\ncan be used standalone.\n  BallKMeans is exactly what it sounds like (uses k-means++ for the\ninitialization, then does a normal k-means pass and ignoring\noutilers).\n  StreamingKMeans implements the online clustering that doesn't return\nexactly k clusters, (it returns an estimate). This is used to\napproximate the data.\n\n\nmapreduce: contains the CentroidWritable, StreamingKMeansDriver,\nStreamingKMeansMapper and StreamingKMeansReducer classes.\n  CentroidWritable serializes Centroids (sort of like AbstractCluster).\n  StreamingKMeansDriver provides the driver for the job.\n  StreamingKMeansMapper runs StreamingKMeans in the mappers to produce\nsketches of the data for the reducer.\n  StreamingKMeansReducer collects the centroids produced by the\nmappers into one set of weighted points and runs BallKMeans on them\nproducing the final results.\n\nAdditionally the searchers are in o.a.m.math.neighborhood\n\nneighborhood: various searcher classes that implement nearest-neighbor\nsearch using different strategies.\n  Searcher, UpdatableSearcher: abstract classes that define how to\nsearch through collections of vectors.\n  BruteSearch: does a brute search (looks at every point...)\n  ProjectionSearch: uses random projections for searching.\n  FastProjectionSearch: also uses random projections (but not binary\nsearch trees as in ProjectionSearch).\n  HashedVector, LocalitySensitiveHashSearch: implement locality\nsensitive hash search.\n\nAll the tools that I used are in o.a.m.clustering.streaming [2], under\nthe examples/ project.\nThere are a bunch of classes here, covering everything from\nvectorizing 20 newsgroups data to various IO utils. The more important\nones are:\n  utils.ExperimentUtils: convenience methods.\n  tools.ClusterQuality20NewsGroups: actual experiment, with hardcoded paths.\n[3] https://github.com/dfilimon/mahout/tree/skm/core/src/main/java/org/apache/mahout/clustering/streaming\n[4] https://github.com/dfilimon/mahout/tree/skm/examples/src/main/java/org/apache/mahout/clustering/streaming\nThe relevant issues are:\n\nMAHOUT-1155 (Centroid, WeightedVector)\nMAHOUT-1156 (searchers)\nMAHOUT-1162 (clustering, non map-reduce)\nMAHOUT-1181 (map-reduce, command-line changes, pom.xml)",
        "Issue Links": []
    },
    "MAHOUT-1155": {
        "Key": "MAHOUT-1155",
        "Summary": "Make MatrixSlice a Vector (and fix Centroid cloning; MAHOUT-1202)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "06/Mar/13 15:42",
        "Updated": "07/May/13 03:42",
        "Resolved": "03/May/13 15:22",
        "Description": "There are two changes in this issue:\n\nmaking MatrixSlice a Vector by extending DelegatingVector;\nmaking a few changes to the vector cloning code so that when cloning a Centroid, the result is also a Centroid.\n\nThis is part of the changes in https://issues.apache.org/jira/browse/MAHOUT-1154\nThe Centroid changes will now be part of the larger changes to Vectors:\nhttps://issues.apache.org/jira/browse/MAHOUT-1202",
        "Issue Links": []
    },
    "MAHOUT-1156": {
        "Key": "MAHOUT-1156",
        "Summary": "Adding nearest neighbor Searchers",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "06/Mar/13 15:53",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "05/May/13 12:24",
        "Description": "Adding the Searcher, UpdatableSearcher abstract classes defining what a nearest-neighbor searcher does.\nThe following implementations are available in the o.a.m.math.neighborhood package:\n\nBruteSearch\nProjectionSearch\nFastProjectionSearch\nLocalitySensityHashSearch [oddly broken, NOT included here]\n\nAdditionally there are 2 new abstract classes available:\n\nSearcher\nUpdatableSearcher\n\nThis is part of https://issues.apache.org/jira/browse/MAHOUT-1154\nThere are no more test issues.\nCommitted revision 1479307.",
        "Issue Links": []
    },
    "MAHOUT-1157": {
        "Key": "MAHOUT-1157",
        "Summary": "AbstractCluster.formatVector iteration bug.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Adam Bozanich",
        "Created": "08/Mar/13 06:20",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Mar/13 09:48",
        "Description": "AbstractCluster.formatVector's use of the size field of the given vector causes problems when the vector is sparse.\nI clustered a handful of vectors which had been initialized with a cardinality of Integer.MAX_VALUE. Running seqdump on the resulting clusteredPoints took over four minutes.  This is because formatVector() was iterating over the entire integer space for every vector.",
        "Issue Links": []
    },
    "MAHOUT-1158": {
        "Key": "MAHOUT-1158",
        "Summary": "Migrate/transform IDs from alphanumeric to Long",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Adam Bozanich",
        "Created": "08/Mar/13 23:57",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 16:20",
        "Description": "Generating item similarities from user interactions or preferences doesn't necessarily require user IDs to remain consistent or even recoverable.\nItemSimilarityJob could have a flag that transforms the IDs automatically.\nUse case:  Input data is a collection of (cookie,product_id); cookie is alphanumeric.  ItemSimilarityJob should be able to consume this data without needing the user to preprocess it.",
        "Issue Links": []
    },
    "MAHOUT-1159": {
        "Key": "MAHOUT-1159",
        "Summary": "Add SSVD option to SpectralKMeans",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "11/Mar/13 18:47",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "11/Mar/13 19:07",
        "Description": "This adds SSVD as an option for eigensolver, in addition to the [default] Lanczos solver. Testing indicated it yielded similar clustering accuracy with a possible performance boost.\nThis patch includes other small fixes, such as using the default \"tempDir\" for intermediate calculations.\nThe initialization of the SSVD solver is a bit awkward, with specifying the number of reducers. I hard-coded this at 10; is there a better solution? Perhaps making it an optional parameter to the SSVD constructor?\n[Thanks to University of Pittsburgh CS undergraduates Andrew King, Pawan Solanki, and Philip Schinis for working on this.]",
        "Issue Links": []
    },
    "MAHOUT-1160": {
        "Key": "MAHOUT-1160",
        "Summary": "Add performant iterators to primitive collections",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "collections",
        "Assignee": "Jake Mannix",
        "Reporter": "Jake Mannix",
        "Created": "12/Mar/13 04:20",
        "Updated": "22/Apr/13 17:27",
        "Resolved": "22/Apr/13 02:52",
        "Description": "Iterating on RandomAccessSparseVector currently requires dumping all keys from the underlying OpenIntDoubleHashMap into a separate IntArrayList, then iterating over that and accessing the values.\nWe should just make sure that OpenIntDoubleHashMap properly does iteration (instead of just allowing iteration callbacks a la forEachPair/forEachKey).",
        "Issue Links": []
    },
    "MAHOUT-1161": {
        "Key": "MAHOUT-1161",
        "Summary": "Unable to run CJKAnalyzer for conversion of a sequence file to sparse vector due to instantiation exception.",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.5,                                            0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Sebastian Schelter",
        "Reporter": "ajit kumar",
        "Created": "12/Mar/13 13:41",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "09/Apr/13 05:27",
        "Description": "Unable to run CJKAnalyzer while runnig the mahout command seq2sparse along with option -a \"org.apache.lucene.analysis.cjk.CJKAnalyzer\".The problem is with instantiation of CJKAnanlyzer class.\nExecuted Command :\nmahout seq2sparse -i inpuDir -o OutputDir -ow \n-a org.apache.lucene.analysis.cjk.CJKAnalyzer -chunk 200 -wt tfidf -s 5 -md 3 -x 90 -ng 2 -ml 50 -seq\nError Stack trace :\nMAHOUT-JOB: /home/ajit/mahout-0.5-cdh3u5/mahout-examples-0.5-cdh3u5-job.jar\n13/03/12 15:56:15 INFO vectorizer.SparseVectorsFromSequenceFiles: Maximum n-gram size is: 2\n13/03/12 15:56:16 INFO vectorizer.SparseVectorsFromSequenceFiles: Minimum LLR value: 50.0\n13/03/12 15:56:16 INFO vectorizer.SparseVectorsFromSequenceFiles: Number of reduce tasks: 1\nException in thread \"main\" java.lang.InstantiationException: org.apache.lucene.analysis.cjk.CJKAnalyzer\n\tat java.lang.Class.newInstance0(Class.java:340)\n\tat java.lang.Class.newInstance(Class.java:308)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.run(SparseVectorsFromSequenceFiles.java:198)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.main(SparseVectorsFromSequenceFiles.java:52)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:187)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:197)",
        "Issue Links": []
    },
    "MAHOUT-1162": {
        "Key": "MAHOUT-1162",
        "Summary": "Adding BallKMeans and StreamingKMeans classes",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "12/Mar/13 16:57",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 20:12",
        "Description": "Adding BallKMeans and StreamingKMeans clustering algorithms.\nThese both implement Iterable<Centroid> and thus return the resulting centroids after clustering.\nBallKMeans implements:\n\nkmeans++ initialization;\na normal k-means pass;\na trimming threshold so that points that are too far from the cluster they were assigned to are not used in the new centroid computation.\n\nStreamingKMeans implements http://books.nips.cc/papers/files/nips24/NIPS2011_1271.pdf:\n\nan online clustering algorithm that takes each point into account one by one\nfor each point, it computes the distance to the nearest existing cluster\nif the distance is greater than a set distanceCutoff, it will create a new cluster, otherwise it might be added to the cluster it's closest to (proportional to the value of the distance / distanceCutoff)\nif there are too many clusters, the clusters will be collapsed (the same method gets called, but the number of clusters is re-adjusted)\nfinally, about as many clusters as requested are returned (not precise!); this represents a sketch of the original points.",
        "Issue Links": []
    },
    "MAHOUT-1163": {
        "Key": "MAHOUT-1163",
        "Summary": "Make random forest classifier meta-data file human readable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Marty Kube",
        "Created": "14/Mar/13 02:13",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 10:59",
        "Description": "The RF classifier has as a Describe utility which figures out a description of a data set (how many attributes, types, and enumerated values, etc...) and writes this meta-data to file for later use during training or testing.\nThe file format is binary.  That means the only way to generate it is with the Describe utility and it is hard to modify.  If the format was human readable it is then possible to modify/generate the meta-data by hand.\nThis will also make it easier to support standard formats such as ARFF.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1164"
        ]
    },
    "MAHOUT-1164": {
        "Key": "MAHOUT-1164",
        "Summary": "Make ARFF integration generate meta-data in JSON format",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification,                                            Integration",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Marty Kube",
        "Created": "15/Mar/13 04:49",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 11:52",
        "Description": "Add a command line option to generate meta-data in a JSON format.\nThis ticket supports the larger goal of making RF classifiers consume the meta-data and sequence files generated by the integration components.  \nMAHOUT-1163 makes RF classifiers read JSON meta-data.  This ticket is to get the ARFF integration to generate the same JSON formatted meta-data.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1163"
        ]
    },
    "MAHOUT-1165": {
        "Key": "MAHOUT-1165",
        "Summary": "TreeVisualizer does not show info of CategoricalNode correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "ey-chih chow",
        "Created": "15/Mar/13 21:38",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "16/Mar/13 07:26",
        "Description": "In a CategoricalNode, array entries of the field 'values' are actually indexes to the array attrValue[attr] of the corresponding dataset.  Given such indexes, we should get their values from indexing to the array attrValues[attr].",
        "Issue Links": []
    },
    "MAHOUT-1166": {
        "Key": "MAHOUT-1166",
        "Summary": "Multithreaded version of distributed ALS",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "16/Mar/13 07:48",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "16/Mar/13 11:54",
        "Description": "Our implementation of ALS broadcasts the feature matrices in each iteration. Therefore, it makes sense to run the mappers in multithreaded mode to not have to load one copy of the feature matrix per core, but share the read-only in-memory copy.",
        "Issue Links": []
    },
    "MAHOUT-1167": {
        "Key": "MAHOUT-1167",
        "Summary": "Parallel item similarity precomputation on a single machine",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "18/Mar/13 10:42",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "20/Mar/13 11:18",
        "Description": "We need some code for item-based CF usecases with an intermediate data size (e.g., a few million interactions). In such cases, the data might be too big to allow online computation of similarities and recommendations, but at the same time, going to Hadoop might still not be necessary and desired.\nIn such a case, it makes sense to precompute item similarities on a single machine.",
        "Issue Links": []
    },
    "MAHOUT-1168": {
        "Key": "MAHOUT-1168",
        "Summary": "Allow access to current Lucene document from LuceneIterator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Benson Margulies",
        "Reporter": "Benson Margulies",
        "Created": "20/Mar/13 14:14",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "21/Mar/13 13:59",
        "Description": "I propose to add a getCurrentDocument() API to the LuceneIterator to allow a vectorizer to produce more information.",
        "Issue Links": []
    },
    "MAHOUT-1169": {
        "Key": "MAHOUT-1169",
        "Summary": "Multithreaded recommendation computation from a factorization",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "20/Mar/13 18:14",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "21/Mar/13 15:46",
        "Description": "Similar to the latest changes to the ALS code, the code to compute recommendations from a factorization should also take advantage of sharing read-only data and multithreading.",
        "Issue Links": []
    },
    "MAHOUT-1170": {
        "Key": "MAHOUT-1170",
        "Summary": "missing java files from mahout-distribution-0.7",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "collections,                                            Examples,                                            Math",
        "Assignee": "Benson Margulies",
        "Reporter": "saeed iqbal",
        "Created": "23/Mar/13 19:19",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "24/Mar/13 16:28",
        "Description": "I download and configured correctly (http://saeedkhattak.wordpress.com/2013/03/23/mahout-development-environment-with-maven-and-eclipse/), but when i import mahout-distribution-0.7 projects into eclipse, and after building it shows me some errors that these files(OpenIntLongHashMap.java, math.function.IntObjectProcedure.java, math.set.OpenIntHashSet.java, etc) are missing. There are many other files that are missed. \nhow can i resolve this problem, please help me,\nSorry for poor English.\nThanks in Advance.",
        "Issue Links": []
    },
    "MAHOUT-1171": {
        "Key": "MAHOUT-1171",
        "Summary": "PMD regression",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "24/Mar/13 11:09",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "24/Mar/13 12:37",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1172": {
        "Key": "MAHOUT-1172",
        "Summary": "Replace org.apache.mahout.cf.taste.common.TopK with Lucene's PriorityQueue",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "25/Mar/13 08:11",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "25/Mar/13 08:16",
        "Description": "Using Lucene's PriorityQueue allows for faster and more memory-efficient top-k selection.",
        "Issue Links": []
    },
    "MAHOUT-1173": {
        "Key": "MAHOUT-1173",
        "Summary": "Reactivate checkstyle",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "25/Mar/13 21:06",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "26/Mar/13 13:27",
        "Description": "I would like to reactivate checkstyle in our build. IMHO we should not make it fail on checkstyle errors at the moment (anyone disagree on this?).",
        "Issue Links": []
    },
    "MAHOUT-1174": {
        "Key": "MAHOUT-1174",
        "Summary": "Lanczos code and javadocs should refer users to the SSVD stuff",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "25/Mar/13 22:03",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "26/Mar/13 01:15",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1175": {
        "Key": "MAHOUT-1175",
        "Summary": "IllegalStateException and FileNotFoundException occures when running mahout inbuilt mapreduce implementation of frequent pattern mining.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Afsal Thaj",
        "Created": "26/Mar/13 03:59",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "31/Jul/13 14:09",
        "Description": "We cannot integrate the code for parallel frequent pattern mining to a project which is supposed to be run in an external server that connects to cluster.Program works fine only inside the cluster (from command line to be specific).IllegalStateException and FileNotFoundException can occur otherwise.",
        "Issue Links": []
    },
    "MAHOUT-1176": {
        "Key": "MAHOUT-1176",
        "Summary": "Introduce an Changelog file to raise contributors attribution",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "27/Mar/13 06:07",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "27/Mar/13 06:12",
        "Description": "In response to the current discussion about raising attribution for\ncontributors, I suggest we introduce a CHANGELOG file similar to the one\nused in GIRAPH. For every commit, we add a single line with the id\nand name of the jira, the name of the committer and potentially the name\nof the contributor, e.g.\nMAHOUT-1234: Fixed bug in some class (john-doe via ssc)",
        "Issue Links": []
    },
    "MAHOUT-1177": {
        "Key": "MAHOUT-1177",
        "Summary": "GSOC 2013: Reform and simplify the clustering APIs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "28/Mar/13 15:55",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Mar/14 20:12",
        "Description": "Clustering is one of the most used features in Mahout and has many applications http://en.wikipedia.org/wiki/Cluster_analysis#Applications.\nWe have of lots clustering algorithms. There's:\n\nbasic k-means\ncanopy clustering\nDirichlet clustering\nFuzzy k-means\nSpectral k-means\nStreaming k-means [coming soon]\n\nWe want to make them easier to use by updating the APIs and make sure they all work in the same way have consistent inputs, outputs, diagnostics and documentation.\nThis is a great way to gain an in-depth understanding of clustering algorithms, familiarize yourself with Hadoop, Mahout clustering and good software engineering principles.",
        "Issue Links": []
    },
    "MAHOUT-1178": {
        "Key": "MAHOUT-1178",
        "Summary": "GSOC 2013: Improve Lucene support in Mahout",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Gokhan Capan",
        "Reporter": "Dan Filimon",
        "Created": "29/Mar/13 11:09",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "14/Apr/14 10:52",
        "Description": "[via Ted Dunning]\nIt should be possible to view a Lucene index as a matrix.  This would\nrequire that we standardize on a way to convert documents to rows.  There\nare many choices, the discussion of which should be deferred to the actual\nwork on the project, but there are a few obvious constraints:\na) it should be possible to get the same result as dumping the term vectors\nfor each document each to a line and converting that result using standard\nMahout methods.\nb) numeric fields ought to work somehow.\nc) if there are multiple text fields that ought to work sensibly as well.\n Two options include dumping multiple matrices or to convert the fields\ninto a single row of a single matrix.\nd) it should be possible to refer back from a row of the matrix to find the\ncorrect document.  THis might be because we remember the Lucene doc number\nor because a field is named as holding a unique id.\ne) named vectors and matrices should be used if plausible.",
        "Issue Links": []
    },
    "MAHOUT-1179": {
        "Key": "MAHOUT-1179",
        "Summary": "GSOC 2013: Refactor and improve the classification APIs",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "29/Mar/13 11:12",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "02/Mar/14 20:13",
        "Description": "[via Andy Twigg]\nImprove and unify the Mahout classification API. Also related to the refactoring of the clustering APIs MAHOUT-1177.\nThe two APIs should be roughly the same, at least in\nterms of input/output so that pipelining etc is easier. (cf\nscikit-learn clustering/classifier/regression API)\nCurrently Mahout support:\n\nlogistic regression\nNaive Bayes\nRandom Forests",
        "Issue Links": []
    },
    "MAHOUT-1180": {
        "Key": "MAHOUT-1180",
        "Summary": "Multinomial<T> throws ConcurrentModificationException when iterating and setting probabilities",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "29/Mar/13 11:19",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "03/May/13 11:08",
        "Description": "Here's [1] an example of the problem (from BallKMeans, lines 225-232, [2]).\nWhen iterating through the elements in a Multinomial and updating the probabilities, sometimes newWeight becomes 0 (because of using CosineDistances).\nWhen setting a weight to 0 in Multinomial, the element is removed from the items hash map while using the hash map for iteration.\nThis causes a ConcurrentModificationException.\n[1] https://gist.github.com/dfilimon/5270234\n[2] https://github.com/dfilimon/mahout/blob/skm/core/src/main/java/org/apache/mahout/clustering/streaming/cluster/BallKMeans.java#L225",
        "Issue Links": []
    },
    "MAHOUT-1181": {
        "Key": "MAHOUT-1181",
        "Summary": "Adding StreamingKMeans MapReduce classes",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "29/Mar/13 11:40",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "15/May/13 15:36",
        "Description": "This patch implements the MapReduce version of StreamingKMeans for MAHOUT-1154.\nIt adds 5 new classes:\n\nCentroidWritable: class representing a centroid that can be written to a SeqFile\nStreamingKMeansDriver: class implementing AbstractJob that is the entry point to the mapreduction\nStreamingKMeansMapper: mapper, running StreamingKMeans (see MAHOUT-1162) clustering the points one by one\nStreamingKMeansReducer: reducer, running BallKMeans (see MAHOUT-1162) a number of times and picking the clustering with the lowest total clustering cost.\nThe cost is determined by randomly splitting the incoming centroids into a \"training\" and \"test\" set, computing the centroids on the training set and the cost on the test set. The intent is to see whether the centroids actually describe the distribution of the points or not.\nStreamingKMeansUtilMR: helper class with a method to instantiate a searcher from a Configuration.\n\nAdditionally, there is a test class StreamingKMeansTestMR that tests the mapper, reducer and mapper and reducer together using MRUnit.\n!!!\nSince MRUnit is now a dependency, the core pom.xml file adds MRUnit as a dependency. We depend on snapshot 1.0 which is not yet released (it will be very soon), hence the updated pom.xml is not provided for now.\n!!!",
        "Issue Links": []
    },
    "MAHOUT-1182": {
        "Key": "MAHOUT-1182",
        "Summary": "remove useless append",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "David Brosius",
        "Created": "31/Mar/13 01:50",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "31/Mar/13 05:40",
        "Description": ".append(\"\") removed",
        "Issue Links": []
    },
    "MAHOUT-1183": {
        "Key": "MAHOUT-1183",
        "Summary": "remove duplicate (masked) unused field",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "David Brosius",
        "Created": "31/Mar/13 02:00",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "07/May/13 15:27",
        "Description": "outer/inner classes have a index variable... the outer not used. remove.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1184"
        ]
    },
    "MAHOUT-1184": {
        "Key": "MAHOUT-1184",
        "Summary": "Another take at pmd, findbugs and checkstyle",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "01/Apr/13 08:06",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Apr/13 09:19",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1183"
        ]
    },
    "MAHOUT-1185": {
        "Key": "MAHOUT-1185",
        "Summary": "MemoryDiffStorage.class has a bug for slope one algorithm which could cause incorrect recommendation results",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Cunlu Zou",
        "Created": "02/Apr/13 06:39",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "02/Apr/13 09:36",
        "Description": "The function processOneUser(long averageCount, long userID) in the MemoryDiffStorage.class file contains a bug for calculating the itemAverage. Since the function tried to calculate the average difference among items (in a nested loop) and also the average individual item preference value in the same loop (the loop only from 0 to length-2, for (int i = 0; i < length - 1; i++)), the itemAverage variable does not count the last item's preference value for every users which could lead to an incorrect recommendation results.",
        "Issue Links": []
    },
    "MAHOUT-1186": {
        "Key": "MAHOUT-1186",
        "Summary": "OpenKeyTypeObjectHashMap#clear() has been broken forever.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.3,                                            collections-1.0",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Andy Schlaikjer",
        "Created": "05/Apr/13 18:00",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "05/Apr/13 22:41",
        "Description": "Arrays.clear(array, 0, array.length - 1, FREE);\nMeans that the last entry in the array is not FREE, and hence arrays.clear only sometimes empties the array, and hence only sometimes clears out all of the entries in the map.  Other times, it leaves one element behind.",
        "Issue Links": []
    },
    "MAHOUT-1187": {
        "Key": "MAHOUT-1187",
        "Summary": "Upgrade Commons Lang to Commons Lang3.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "07/Apr/13 13:27",
        "Updated": "09/Apr/13 16:19",
        "Resolved": "07/Apr/13 17:02",
        "Description": "Upgrading Mahout codebase to using Commons Lang3.",
        "Issue Links": []
    },
    "MAHOUT-1188": {
        "Key": "MAHOUT-1188",
        "Summary": "Inconsistency in  Lucene versions between codebase and POM",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "09/Apr/13 13:32",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Apr/13 16:45",
        "Description": "The lucene version specified in pom.xml is 4.2.0\nThe Lucene version as referenced in the codebase = Version.LUCENE_41.  It should be Version.LUCENE_42 throughout.",
        "Issue Links": []
    },
    "MAHOUT-1189": {
        "Key": "MAHOUT-1189",
        "Summary": "CosineDistanceMeasure doesn't return 0 for two 0 vectors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "09/Apr/13 15:08",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "03/May/13 11:25",
        "Description": "CosineDistanceMeasure for two equal vectors should always return 0 like for any other distance measure, however it returns 1.\nThis patch fixes this issue.\nAlso, note that it's not necessarily obvious what the return value should be since the cosine of two 0-length vectors isn't defined.",
        "Issue Links": []
    },
    "MAHOUT-1190": {
        "Key": "MAHOUT-1190",
        "Summary": "SequentialAccessSparseVector function assignment is very slow and other iterator woes",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "10/Apr/13 21:15",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "03/May/13 12:05",
        "Description": "Currently when calling .assign() on a SASV with another vector and a custom function, it will iterate through it and assign every single entry while also referring it by index.\nThis makes the process hugely expensive. (on a run of BallKMeans on the 20 newsgroups data set, profiling reveals that 92% of the runtime was spent updating assigning the vectors).\nHere's a prototype patch:\nhttps://github.com/dfilimon/mahout/commit/63998d82bb750150a6ae09052dadf6c326c62d3d",
        "Issue Links": []
    },
    "MAHOUT-1191": {
        "Key": "MAHOUT-1191",
        "Summary": "Cleanup Vector Benchmarks make it less variable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "15/Apr/13 22:12",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "18/Apr/13 19:07",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1192": {
        "Key": "MAHOUT-1192",
        "Summary": "Speed up Vector Operations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "16/Apr/13 05:21",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "18/Apr/13 19:39",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1193": {
        "Key": "MAHOUT-1193",
        "Summary": "We may want a BlockSparseMatrix",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "18/Apr/13 15:39",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "08/Mar/14 12:02",
        "Description": "Here is an implementation.\nIs it good enough to commit?\nIs it useful?\nIs it redundant?",
        "Issue Links": []
    },
    "MAHOUT-1194": {
        "Key": "MAHOUT-1194",
        "Summary": "Allow to change java target version during the build",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jarek Jarcec Cecho",
        "Reporter": "Jarek Jarcec Cecho",
        "Created": "22/Apr/13 02:29",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "02/Jun/13 13:50",
        "Description": "It seems that current build have hard coded java target for JDK6. I think that it would be useful to parametrise that, so that it can be easily overridden on the command line.",
        "Issue Links": [
            "https://reviews.apache.org/r/10697/"
        ]
    },
    "MAHOUT-1195": {
        "Key": "MAHOUT-1195",
        "Summary": "Unable to instantiate EnglishAnalyzer when passed as a CLI parameter to seq2sparse.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "CLI",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "22/Apr/13 19:10",
        "Updated": "22/Apr/13 19:18",
        "Resolved": "22/Apr/13 19:18",
        "Description": "Invoking seq2sparse by specifying \nmahout seq2sparse -i ./contentDataDir/sequenced -o ./contentDataDir/sparseVectors --namedVector -wt tf -a org.apache.lucene.analysis.EnglishAnalyzer\nthrows a ClassNotFoundException.\nException in thread \"main\" java.lang.ClassNotFoundException: org.apache.lucene.analysis.EnglishAnalyzer\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:202)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:248)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:169)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.run(SparseVectorsFromSequenceFiles.java:203)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.main(SparseVectorsFromSequenceFiles.java:56)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
        "Issue Links": []
    },
    "MAHOUT-1196": {
        "Key": "MAHOUT-1196",
        "Summary": "LogisticModelParameters uses csv.getTargetCategories() even if csv is not used.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Vineet Krishnan",
        "Reporter": "Vineet Krishnan",
        "Created": "22/Apr/13 19:14",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "02/Jun/13 19:37",
        "Description": "saveTo(OutputStream out) tries to get csv.getTargetCategories() even when it has already been set. In a case when CsvRecordFactory is not used, this gives a NullPointerException when saveTo() is called.\nIMHO a simple null check for targetCategories is sufficient.",
        "Issue Links": []
    },
    "MAHOUT-1197": {
        "Key": "MAHOUT-1197",
        "Summary": "AbstractVector#cross is only appropriately efficient for dense vectors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Jake Mannix",
        "Created": "24/Apr/13 22:05",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "26/Apr/13 07:48",
        "Description": "Nobody overrides this implementation:\n[code]\n  @Override\n  public Matrix cross(Vector other) {\n    Matrix result = matrixLike(size, other.size());\n    for (int row = 0; row < size; row++) \n{\n      result.assignRow(row, other.times(getQuick(row)));\n    }\n    return result;\n  }\n[code]\nI think you can imagine what kind of performance this has on sparse vectors (k non-zeroes) with high cardinality (N) - scales as O(N^2) instead of O(k^2).\nI think the right approach is to not implement this in AbstractVector at all, and force concrete implementations to properly implement it performantly.\nAlternatively, killing this method entirely might be appropriate.  If anyone was using it (and uses sparse vectors), they'd have complained about this by now.",
        "Issue Links": []
    },
    "MAHOUT-1198": {
        "Key": "MAHOUT-1198",
        "Summary": "Allow Latex in javadox",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "29/Apr/13 19:40",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 21:55",
        "Description": "We are headed into a release (hopefully) and now would be a nice time to add the capability to generate javadocs with embedded latex.\nFollowing a hint from commons math, I tested a way to inject mathjax into the header of the resulting web-site and got good results (see http://tdunning.github.io/bandit-ranking/ especially docs for GammaNormalDistribution and BetaBinomialDistribution.\nThe basic idea is that we need to add the following config to the javadocs plugin:\n\n<configuration>\n    <additionalparam>-header '<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML&quot;&gt; </script>'</additionalparam>\n</configuration>\nHaving done this, [ ] and ( ) can be used to embed latex equations in the javadocs.",
        "Issue Links": []
    },
    "MAHOUT-1199": {
        "Key": "MAHOUT-1199",
        "Summary": "Improve javadoc comments of mahout-integration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Angel Martinez Gonzalez",
        "Created": "29/Apr/13 22:17",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/May/13 05:27",
        "Description": "Most classes in the following packages have no Javadoc comments:\norg.apache.mahout.utils.io\norg.apache.mahout.utils.vectors\norg.apache.mahout.utils.email\nOther packages have the same problem. This is just to start with something.",
        "Issue Links": []
    },
    "MAHOUT-1200": {
        "Key": "MAHOUT-1200",
        "Summary": "Mahout tests depend on writing to /tmp/hadoop-$user",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "30/Apr/13 20:04",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 22:31",
        "Description": "Running the Mahout test suite creates the temp directory /tmp/hadoop-$user which is used by all Hadoop related tests that pull up a local cluster. The directory is not removed after running the tests. In particular when running multiple tests in parallel on the same machine as the same user this can lead to problems.\nTo re-produce issue the following commands prior to running the full test suite:\nmkdir /tmp/hadoop-$USER\nchmod 000 /tmp/hadoop-$USER\nmvn test",
        "Issue Links": [
            "/jira/browse/MAHOUT-1201",
            "/jira/browse/MAHOUT-916",
            "/jira/browse/MAHOUT-1325"
        ]
    },
    "MAHOUT-1201": {
        "Key": "MAHOUT-1201",
        "Summary": "Some Mahout jobs do not pass user supplied Configuration object to sub jobs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering,                                            Math",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "30/Apr/13 20:07",
        "Updated": "31/Mar/15 22:49",
        "Resolved": "01/Jun/13 22:30",
        "Description": "Some (see patch) of our Hadoop jobs do not pass a user supplied configuration object down to sub jobs created. As a result some Hadoop related settings may not be honored.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1128",
            "/jira/browse/MAHOUT-1200"
        ]
    },
    "MAHOUT-1202": {
        "Key": "MAHOUT-1202",
        "Summary": "Speed up Vector operations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "03/May/13 12:04",
        "Updated": "10/Sep/14 19:32",
        "Resolved": "03/May/13 20:31",
        "Description": "Vector assign() and aggregate() can be significantly improved in some conditions taking into account the different properties of the vectors we're working with.\nThis issue relates to the design document at https://docs.google.com/document/d/1g1PjUuvjyh2LBdq2_rKLIcUiDbeOORA1sCJiSsz-JVU/edit#heading=h.koi571fvwha3jj\nand the patch at\nhttps://reviews.apache.org/r/10669\nThe benchmarks are at\nhttps://docs.google.com/spreadsheet/ccc?key=0AochdzPoBmWodG9RTms1UG40YlNQd3ByUFpQY0FLWmc&pli=1#gid=10\nand while there are a few regressions (which will be fixed later regarding RandomAccessSparseVectors), it improves a lot of benchmarks as well as cleans up the code significantly.\nPart 1, the new function interfaces is merged. [Committed revision 1478853.]",
        "Issue Links": []
    },
    "MAHOUT-1203": {
        "Key": "MAHOUT-1203",
        "Summary": "Problem in PhD Topic",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.4,                                            0.5,                                            0.6,                                            0.7",
        "Fix Version/s": "None",
        "Component/s": "Classification,                                            Clustering,                                            Integration",
        "Assignee": null,
        "Reporter": "saeed iqbal",
        "Created": "03/May/13 16:00",
        "Updated": "03/May/13 16:12",
        "Resolved": "03/May/13 16:07",
        "Description": "Recently, i study literature review about cloud computing, hadoop, mahout and machine learning algorithms. Actually i am working on hadoop in my PhD study. But now i confuse about my topic, i can't identify my PhD topic, please guide me (hadoop and mahout).",
        "Issue Links": []
    },
    "MAHOUT-1204": {
        "Key": "MAHOUT-1204",
        "Summary": "Rewrite Benchmarks using Caliper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "04/May/13 17:52",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "08/Mar/14 12:02",
        "Description": "https://code.google.com/p/caliper/",
        "Issue Links": []
    },
    "MAHOUT-1205": {
        "Key": "MAHOUT-1205",
        "Summary": "ParallelALSFactorizationJob should leverage the distributed cache",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "05/May/13 09:54",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "07/May/13 08:05",
        "Description": "ParallelALSFactorizationJob should use the DistributedCache to broadcast the feature matrices only once per re-computation.",
        "Issue Links": []
    },
    "MAHOUT-1206": {
        "Key": "MAHOUT-1206",
        "Summary": "Add density-based clustering algorithms to mahout",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yexi Jiang",
        "Created": "07/May/13 18:47",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Mar/14 18:44",
        "Description": "The clustering algorithms (kmeans, fuzzy kmeans, dirichlet clustering, and spectral cluster) clustering data by assuming that the data can be clustered into the regular hyper sphere or ellipsoid. However, in practical, not all the data can be clustered in this way. \nTo enable the data to be clustered in arbitrary shapes, clustering algorithms like DBSCAN, BIRCH, CLARANCE (http://en.wikipedia.org/wiki/Cluster_analysis#Density-based_clustering) are proposed.\nIt is better that we can implement one or some of these clustering algorithm to enrich the clustering library.",
        "Issue Links": []
    },
    "MAHOUT-1207": {
        "Key": "MAHOUT-1207",
        "Summary": "Fix typos in description in parent pom",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "09/May/13 20:10",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/May/13 13:51",
        "Description": "Description element in mahout parent pom contains few typos.",
        "Issue Links": []
    },
    "MAHOUT-1208": {
        "Key": "MAHOUT-1208",
        "Summary": "Not able to get the distance from the cluster.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Sameer Sebastian",
        "Created": "10/May/13 05:53",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 19:21",
        "Description": "Hello,\nAfter clustering, when I am running the clusterdump mahout command, the result doesn't have the distance.\nIs https://issues.apache.org/jira/browse/MAHOUT-1073, the only reason why it is happening.\nIf there is a work around without a patch, please tell.\nThanks,",
        "Issue Links": []
    },
    "MAHOUT-1209": {
        "Key": "MAHOUT-1209",
        "Summary": "DRY out maven-compiler-plugin configuration",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Stevo Slavi\u0107",
        "Created": "11/May/13 09:30",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "12/May/13 14:12",
        "Description": "maven-compiler-plugin configuration is unnecessarily repeated in POMs of several modules, even though mahout parent POM defines same maven-compiler-plugin configuration which is inherited by all of the modules.\nThis unnecessarily clutters module POMs making them harder to read and maintain.",
        "Issue Links": []
    },
    "MAHOUT-1210": {
        "Key": "MAHOUT-1210",
        "Summary": "Fix URLs in mahout-collection-codegen-plugin pom",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "collections-1.0",
        "Fix Version/s": "0.8",
        "Component/s": "build,                                            collections,                                            Math",
        "Assignee": "Benson Margulies",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "11/May/13 13:13",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 20:24",
        "Description": "URLs in mahout-collection-codegen-plugin trunk POM still point to Lucene project and Lucene SVN repository.",
        "Issue Links": []
    },
    "MAHOUT-1211": {
        "Key": "MAHOUT-1211",
        "Summary": "Replace deprecated Closables.closeQuietly calls",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "12/May/13 10:58",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 20:53",
        "Description": "Deprecated Guava Closables.closeQuietly API has to be replaced, it's usage is a code smell, and that method is scheduled to be removed from Guava 16.0.\nSee this discussion for more info.",
        "Issue Links": []
    },
    "MAHOUT-1212": {
        "Key": "MAHOUT-1212",
        "Summary": "Incorrect classify-20newsgroups.sh file description",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Julian Ortega",
        "Created": "13/May/13 13:07",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "13/May/13 13:31",
        "Description": "Looking at the classify-20newsgroups.sh file it says:\n#To run:  change into the mahout directory and type:\n#examples/bin/build-20news.sh\nI think it was not updated to say classify-20newsgroups.sh instead of build-20news.sh when the MAHOUT-857 changes were made.",
        "Issue Links": [
            "/jira/browse/MAHOUT-857"
        ]
    },
    "MAHOUT-1213": {
        "Key": "MAHOUT-1213",
        "Summary": "SSVD job doesn't clean it's temp dir, and fails when seeing it again",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "CLI",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "15/May/13 04:00",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "26/May/13 03:26",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1214": {
        "Key": "MAHOUT-1214",
        "Summary": "Improve the accuracy of the Spectral KMeans Method",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Robin Anil",
        "Reporter": "Yiqun Hu",
        "Created": "16/May/13 07:54",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "27/Jun/13 22:32",
        "Description": "The current implementation of the spectral KMeans algorithm (Andrew Ng. etc. NIPS 2002) in version 0.7 has two serious issues. These two incorrect implementations make it fail even for a very obvious trivial dataset. We have implemented a solution to resolve these two issues and hope to contribute back to the community.\n\nIssue 1:\nThe EigenVerificationJob in version 0.7 does not check the orthogonality of eigenvectors, which is necessary to obtain the correct clustering results for the case of K>1; We have an idea and implementation to select based on cosAngle/orthogonality;\n\n\nIssue 2:\nThe random seed initialization of KMeans algorithm is not optimal and sometimes a bad initialization will generate wrong clustering result. In this case, the selected K eigenvector actually provides a better way to initalize cluster centroids because each selected eigenvector is a relaxed indicator of the memberships of one cluster. For every selected eigenvector, we use the data point whose eigen component achieves the maximum absolute value. \n\nWe have already verified our improvement on synthetic dataset and it shows that the improved version get the optimal clustering result while the current 0.7 version obtains the wrong result.",
        "Issue Links": []
    },
    "MAHOUT-1215": {
        "Key": "MAHOUT-1215",
        "Summary": "NullPointerException in ItemSimilarityJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "LEE YOONJAE",
        "Created": "16/May/13 08:06",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "16/May/13 08:31",
        "Description": "<--command-->\nhadoop --config /engn001/sbp/scm/role/namenode/conf jar \n/home/hdfs/mahout_test/mahout-core-0.7-job.jar \\org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob \n--input /user/hdfs/mahout_test/input/mahout_input_final3_0.csv \n--output /user/hdfs/mahout_test/output/final3/ItemSimilarity\n<--error message-->\nException in thread \"main\" java.lang.NullPointerException\n        at java.lang.String.<init>(String.java:147)\n        at org.apache.commons.cli2.commandline.Parser.parse(Parser.java:66)\n        at org.apache.mahout.common.AbstractJob.parseArguments(AbstractJob.java:347)\n        at org.apache.mahout.common.AbstractJob.parseArguments(AbstractJob.java:317)\n        at org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJob.run(RowSimilarityJob.java:91)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.run(ItemSimilarityJob.java:145)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJob.main(ItemSimilarityJob.java:91)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nwhen I first did, it was successful, but in second time, \nException in thread \"main\" java.lang.NullPointerException error occured\nso I removed that temp directory, \nand then in third time, NullPointerException occured.\nWhy is this error ocurred? and How can I fix it??",
        "Issue Links": []
    },
    "MAHOUT-1216": {
        "Key": "MAHOUT-1216",
        "Summary": "Add locality sensitive hashing and a LocalitySensitiveHash searcher",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "16/May/13 08:22",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "17/May/13 09:05",
        "Description": "This issue tackles the LocalitySensitiveHashSearch, that was initially supposed to be part of MAHOUT-1156.\nIt adds HashedVector, the class that adds the LSH to vectors, a new searcher (although a better implementation is possible) and adds support in the existing tests and new StreamingKMeans infrastructure.",
        "Issue Links": []
    },
    "MAHOUT-1217": {
        "Key": "MAHOUT-1217",
        "Summary": "Nearest neighbor searchers sometimes fail to remove points",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "16/May/13 17:27",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "20/May/13 08:35",
        "Description": "When updating a Centroid in StreamingKMeans, the Centroid needs to be removed and its updated version added.\nWhen removing points in a searcher that are already there, sometimes the searcher fails to return the closest point (the one being searched for) causing a RuntimeException.\nThis has been observed for TF-IDF vectors with SquaredEuclideanDistance and CosineDistance and FastProjectionSearch.",
        "Issue Links": []
    },
    "MAHOUT-1218": {
        "Key": "MAHOUT-1218",
        "Summary": "Streamimg k-means fails when the number of clusters specified is <= estimated map clusters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "16/May/13 18:41",
        "Updated": "27/Jul/13 04:19",
        "Resolved": "16/May/13 22:49",
        "Description": "Running Streaming k-means with CosineDistanceMeasure, Fast Projection Search, number of clusters k= 60, number of estimated map clsuters -km = 60.\n\nException in thread \"main\" java.lang.IllegalArgumentException: Invalid number of estimated map clusters; There must be more than the final number of clusters (k log n vs k)\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.configureOptionsForWorkers(StreamingKMeansDriver.java:327)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.configureOptionsForWorkers(StreamingKMeansDriver.java:280)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.run(StreamingKMeansDriver.java:227)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.main(StreamingKMeansDriver.java:472)",
        "Issue Links": []
    },
    "MAHOUT-1219": {
        "Key": "MAHOUT-1219",
        "Summary": "LSHSearcher not always faster than BruteSearcher",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "17/May/13 13:40",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "20/May/13 08:34",
        "Description": "This is a known issue and the performance of LocalitySensitiveHashSearch needs to be further investigated.\nCurrently, the one \"benchmark\" that does this, SearchQualityTest is too variable to be informative.\nSo, I'm removing LSHSearcher from SearchQualityTest.",
        "Issue Links": []
    },
    "MAHOUT-1220": {
        "Key": "MAHOUT-1220",
        "Summary": "seqdirectory brings empty files out",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Summer Lee",
        "Created": "20/May/13 03:31",
        "Updated": "08/Mar/14 12:06",
        "Resolved": "03/Jun/13 03:25",
        "Description": "I put the input file on \"mahout seqdirectory\"  \n--> command\nmahout seqdirectory --input user/hdfs/mahout_test/input2/mahout_input_final3_0.csv --output /user/hdfs/mahout_test/output/final3/seqdirectory/\nbut the result file, \"chunk-0\" contains like this.\n--> chunk-0\nSEQ\u0006\u0019org.apache.hadoop.io.Text\u0019org.apache.hadoop.io.Text\nI heard that chunk-0 files should have number like SEQ\u0006\u0019org.apache.hadoop.io.Text\u0019org.apache.hadoop.io.Text 1.0 2.0 ...\nI think my input file is something wrong, so I tried with other different input files but results are same.\nHow can I fix this?",
        "Issue Links": []
    },
    "MAHOUT-1221": {
        "Key": "MAHOUT-1221",
        "Summary": "SparseMatrix.viewRow is sometimes readonly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7,                                            0.8",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Maysam Yabandeh",
        "Created": "20/May/13 04:05",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "20/May/13 06:05",
        "Description": "The implementation returns a new vector if it already does not exist. But it does not add the new vector to the matrix. So, the later changes will not be reflected in the matrix.\n\nif (res == null) {\nres = newRandomAccessSparceVector(columnSize());\n//now the row must be added by assignRow(row, res);\n}\nreturn res;\n\n\nAn example in which this bug manifests is the following:\nQRDecomposition.java\nx.viewRow(k).assign(y.viewRow(k), Functions.plusMult(1 / r.get(k, k)));\n\n\nwhere Matrix x is not updated if it is an instance of SparseMatrix.",
        "Issue Links": []
    },
    "MAHOUT-1222": {
        "Key": "MAHOUT-1222",
        "Summary": "Fix total weight in FastProjectionSearch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "20/May/13 15:16",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "21/May/13 08:39",
        "Description": "Sometimes when removing a Vector that's in pendingAdditions, the wrong Vector gets removed.\nThis happens because the closest Vector is removed rather than the one that's equal.",
        "Issue Links": []
    },
    "MAHOUT-1223": {
        "Key": "MAHOUT-1223",
        "Summary": "Point skipped in StreamingKMeans when iterating through centroids from a reducer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Dan Filimon",
        "Created": "20/May/13 15:28",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "21/May/13 10:53",
        "Description": "When calling StreamingKMeans in the reducer (to collapse the number of clusters to they can fit into memory), the clustering is done on the Hadoop reducer iterable.\nCurrently, the first Centroid is added directly as a special case and then is skipped when iterating through the main loop.\nHowever, Hadoop reducer iterables cannot be rewound therefore causing SKM to skip one point.",
        "Issue Links": []
    },
    "MAHOUT-1224": {
        "Key": "MAHOUT-1224",
        "Summary": "Add the option of running a StreamingKMeans pass in the Reducer before BallKMeans",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Dan Filimon",
        "Reporter": "Dan Filimon",
        "Created": "20/May/13 15:35",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "02/Jun/13 18:59",
        "Description": "Sometimes, the number of points passed to the reducer from the mappers in the StreamingKMeansDriver job is too large to fit into memory.\nIn that case, applying another StreamingKMeans pass can collapse the mapper intermediate clusters to a more manageable size to be clustered.",
        "Issue Links": []
    },
    "MAHOUT-1225": {
        "Key": "MAHOUT-1225",
        "Summary": "Sets and maps incorrectly clear() their state arrays (potential endless loops)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Math",
        "Assignee": "Dawid Weiss",
        "Reporter": "Sophie Sperner",
        "Created": "21/May/13 17:46",
        "Updated": "03/Jun/13 13:21",
        "Resolved": "01/Jun/13 21:28",
        "Description": "The code I attached hangs on forever, Eclipse does not print me its stack trace because it does not terminate the program. So I decided to make a small test.java file that you can easily run.\nThis code has the main function that simply runs getItemList() method which successfully executes getDataset() method (here please download mushroom.dat dataset and set the full path into filePath string variable) and the hangs on (the problem happens on a fourth columnValues.add() call). After the dataset was taken into X array, the code simply goes through X column by column and searches for different items in it.\nIf you uncomment IntSet columnValues = new IntOpenHashSet(); and corresponding import headers then everything will work just fine (you will also need to include hppc jar file found here http://labs.carrotsearch.com/hppc.html or below in the attachment).",
        "Issue Links": []
    },
    "MAHOUT-1226": {
        "Key": "MAHOUT-1226",
        "Summary": "mahout ssvd Bt-job bug",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jakub",
        "Created": "22/May/13 13:29",
        "Updated": "23/May/13 10:25",
        "Resolved": "23/May/13 10:24",
        "Description": "when using mahout ssvd job, Bt-job creates lots of spills to disk.\nThose might be minimized by tuning hadoop io.sort.mb parameter.\nHowever, when io.sort.mb is bigger than ~ 1100 , ie. 1500 I'm getting that exception:\njava.io.IOException: Spill failed\n    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1029)\n    at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:691)\n    at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n    at org.apache.mahout.math.hadoop.stochasticsvd.BtJob$BtMapper$1.collect(BtJob.java:261)\n    at org.apache.mahout.math.hadoop.stochasticsvd.BtJob$BtMapper$1.collect(BtJob.java:255)\n    at org.apache.mahout.math.hadoop.stochasticsvd.SparseRowBlockAccumulator.flushBlock(SparseRowBlockAccumulator.java:65)\n    at org.apache.mahout.math.hadoop.stochasticsvd.SparseRowBlockAccumulator.collect(SparseRowBlockAccumulator.java:75)\n    at org.apache.mahout.math.hadoop.stochasticsvd.BtJob$BtMapper.map(BtJob.java:158)\n    at org.apache.mahout.math.hadoop.stochasticsvd.BtJob$BtMapper.map(BtJob.java:102)\n    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:415)\n    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)\n    at org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.lang.RuntimeException: next value iterator failed\n    at org.apache.hadoop.mapreduce.ReduceContext$ValueIterator.next(ReduceContext.java:166)\n    at org.apache.mahout.math.hadoop.stochasticsvd.BtJob$OuterProductCombiner.reduce(BtJob.java:322)\n    at org.apache.mahout.math.hadoop.stochasticsvd.BtJob$OuterProductCombiner.reduce(BtJob.java:302)\n    at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)\n    at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1502)\n    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)\n    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:853)\n    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1344)\nCaused by: java.io.EOFException\n    at java.io.DataInputStream.readByte(DataInputStream.java:267)\n    at org.apache.mahout.math.Varint.readUnsignedVarInt(Varint.java:159)\n    at org.apache.mahout.math.hadoop.stochasticsvd.SparseRowBlockWritable.readFields(SparseRowBlockWritable.java:60)\n    at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)\n    at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)\n    at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)\n    at org.apache.hadoop.mapreduce.ReduceContext$ValueIterator.next(ReduceContext.java:163)\n    ... 7 more\nby changing this value I've already managed to reduce spills from 100 (for default io.sort.mb value) to 10, disk usage dropped from around 7 gigabytes for my small data set to around 900 mb. repairing this issue might bring big performance improvements.\nI've got lots of free ram, that's not some lack of memory issue.",
        "Issue Links": []
    },
    "MAHOUT-1227": {
        "Key": "MAHOUT-1227",
        "Summary": "Vector.iterateNonZero() is super-clumsy to use: add Iterable<Element> allNonZero()",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jake Mannix",
        "Reporter": "Andy Schlaikjer",
        "Created": "23/May/13 18:53",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "24/May/13 17:25",
        "Description": "Currently, our codebase is littered with the following:\n\nIterator<Element> it = vector.iterateNonZero();\nwhile (it.hasNext()) {\n  Element e = it.next();\n  ...\n\n\nwouldn't it be nice to be able to do:\n\nfor (Element e : vector.allNonZero()) {\n  ...\n\n\ninstead?\nI propose adding an Iterable<Element> allNonZero() which allow this syntactic sugar.  To make it symmetric with iterateAll, let's also add Iterable<Element> all(), and implement the simply in AbstractVector.\nThe first diff adding this is very non-invasive - new methods added to interface, implemented in the three classes from which all Vector implementations derive (AbstractVector, NamedVector, and DelegatingVector).  User code should just work, unless they've implemented their own vector without subclassing one of these three (yikes).\nNext diff, which is more invasive, would remove \"extends Iterable<Element>\" from Vector, because using the foreach of a Vector itself is very rarely what the caller really means to do (it's the all-iterator, very bad for the more common sparse use case).  To achieve the same effect, the caller chooses between vector.all() and vector.allNonZero(), and then they're being crystal clear what they mean.\nLastly, I'd propose we make iterateAll() and iterateAllNonZero() protected methods on AbstractVector, so that we are forced to remove all the clumsy places where we do Iterator<Element> it = ... all throughout the codebase.  I suspect there will be very few places left that really want the raw iterator, but if there are any, it can be gotten by calling vector.(all/allNonZero).iterator()\n(feature-request/api fix suggestion idea courtesy of Andy Schlaikjer, formalized as a proposal and posted up here by me, Jake Mannix)",
        "Issue Links": []
    },
    "MAHOUT-1228": {
        "Key": "MAHOUT-1228",
        "Summary": "Cleanup .gitignore",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "26/May/13 23:50",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "02/Jun/13 09:16",
        "Description": ".gitignore unnecessarily has duplicate entries for ignoring eclipse IDE specific files and directories, as well as Maven build output directory. For distribution module Maven build output directory is not ignored.",
        "Issue Links": []
    },
    "MAHOUT-1229": {
        "Key": "MAHOUT-1229",
        "Summary": "Conf directory content from Mahout distribution archives cannot be unpacked",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "27/May/13 01:12",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "27/May/13 07:04",
        "Description": "Conf directory permissions are cleared in the archive, so permissions-preserving archives (all but zip) when unpacked create conf directory but cannot unpack files into it (one gets permission denied errors).",
        "Issue Links": []
    },
    "MAHOUT-1230": {
        "Key": "MAHOUT-1230",
        "Summary": "SparceMatrix.clone() is not deep copy",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Maysam Yabandeh",
        "Created": "27/May/13 15:22",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "29/May/13 10:26",
        "Description": "After changing the matrix created by clone(), the original matrix changes as well. I am attaching a patch with the unit test.",
        "Issue Links": []
    },
    "MAHOUT-1231": {
        "Key": "MAHOUT-1231",
        "Summary": "\"No input clusters found in \" error in kmeans",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Summer Lee",
        "Created": "28/May/13 06:59",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "17/Jun/13 01:00",
        "Description": "1.seqdirectory\n> mahout seqdirectory --input /user/hdfs/input/new1.csv --output\n> /user/hdfs/new1/seqdirectory --tempDir\n> /user/hdfs/new1/seqdirectory/tempDir\n2.seq2sparse \n> mahout seq2sparse --input /user/hdfs/new1/seqdirectory --output\n> /user/hdfs/new1/seq2sparse -wt tfidf\n3.kmeans \n> mahout kmeans --input /user/hdfs/new1/seq2sparse/tfidf-vectors\n> --output /user/hdfs/new1/kmeans -c /user/hdfs/new1/clusters/kmeans -x 3 -k 3 --tempDir /user/hdfs/new1/kmeans/tempDir\nand then error is occured\nFailing Oozie Launcher, Main class [org.apache.mahout.driver.MahoutDriver], main() threw exception, No input clusters found in /user/oozie/mahout/z3/kmeansCopy/clusters/part-randomSeed. Check your -c argument.\njava.lang.IllegalStateException: No input clusters found in /user/oozie/mahout/z3/kmeansCopy/clusters/part-randomSeed. Check your -c argument.\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.buildClusters(KMeansDriver.java:217)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:148)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:107)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.main(KMeansDriver.java:48)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.oozie.action.hadoop.LauncherMapper.map(LauncherMapper.java:467)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nOozie Launcher failed, finishing Hadoop job gracefully\nOozie Launcher ends\n=======================================================================================================\nWhy kmeans driver can't make clusters in Hadoop with oozie system?\nIn hadoop with not oozie system, it worked.",
        "Issue Links": []
    },
    "MAHOUT-1232": {
        "Key": "MAHOUT-1232",
        "Summary": "VectorHelper.topEntries() throws a NPE when number of NonZero elements in vector < maxEntries",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "29/May/13 02:00",
        "Updated": "27/Jul/13 04:15",
        "Resolved": "29/May/13 05:45",
        "Description": "Vectordump throws a NullPointerException when sort is specified and the number of NonZero elements in the input vector is less than the specified vector size (-vs).\n\n\nmahout vectordump -i reuters-vectors/tfidf-vectors -dt sequencefile -d reuters-vectors/dictionary.file-* -vs 15 -ni 30 -o vectordump -p true -sort reuters-vectors/tfidf-vectors\n\nINFO: Sort? true\nException in thread \"main\" java.lang.NullPointerException\n\tat org.apache.mahout.utils.vectors.VectorHelper.topEntries(VectorHelper.java:89)\n\tat org.apache.mahout.utils.vectors.VectorHelper.vectorToJson(VectorHelper.java:135)\n\tat org.apache.mahout.utils.vectors.VectorDumper.run(VectorDumper.java:242)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.utils.vectors.VectorDumper.main(VectorDumper.java:262)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\n\n\nThe issue is in the following block of code that is invoked when sort=true in VectorHelper.java \n\n\n    for (Element e : vector.nonZeroes()) {\n      queue.insertWithOverflow(Pair.of(e.index(), e.get()));\n    }\n    List<Pair<Integer, Double>> entries = Lists.newArrayList();\n    Pair<Integer, Double> pair;\n    while ((pair = queue.pop()) != null) {\n      if (pair.getFirst() > -1) {\n        entries.add(pair);\n      }\n    }",
        "Issue Links": []
    },
    "MAHOUT-1233": {
        "Key": "MAHOUT-1233",
        "Summary": "Problem in processing datasets as a single chunk vs many chunks in HADOOP mode in mostly all the clustering algos",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Incomplete",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "yannis ats",
        "Reporter": "yannis ats",
        "Created": "30/May/13 09:28",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Jun/13 14:11",
        "Description": "I am trying to process a dataset and i do it in two ways.\nFirstly i give it as a single chunk(all the dataset) and secondly as many smaller chunks in order to increase the throughput of my machine.\nThe problem is that when i perform the single chunk computation the results are fine \nand by fine i mean that if i have in the input 1000 vectors i get in the output 1000 vectorids with their cluster_ids (i have tried in canopy,kmeans and fuzzy kmeans).\nHowever when i split the dataset in order to speed up the computations then strange phenomena occur.\nFor instance the same dataset that contains 1000 vectors and is split in  for example 10 files then in the output i will obtain more vector ids(w.g 1100 vectorids with their corresponding clusterids).\nThe question is, am i doing something wrong in the process?\nIs there a problem in clusterdump and seqdumper when the input is in many files?\nI have observed when mahout is performing the computations that in the screen says that processed the correct number of vectors.\nAm i missing something?\nI use as input the transformed to mvc weka vectors.\nI have tried this in v0.7 and the v0.8 snapshot.\nThank you in advance for your time.",
        "Issue Links": []
    },
    "MAHOUT-1234": {
        "Key": "MAHOUT-1234",
        "Summary": "Canopy Clustering",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Sameer Sebastian",
        "Created": "30/May/13 14:44",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "01/Jun/13 17:24",
        "Description": "Hello,\nI'm trying out Canopy clustering.\nI want to know, how to determine the optimum value for the distance thresholds t1 and t2.\nThanks.",
        "Issue Links": []
    },
    "MAHOUT-1235": {
        "Key": "MAHOUT-1235",
        "Summary": "ParallelALSFactorizationJob does not use VectorSumCombiner",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "01/Jun/13 15:31",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "01/Jun/13 16:08",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1236": {
        "Key": "MAHOUT-1236",
        "Summary": "Need a cleaned up serialized format for Vectors to handle names and all other kinds of things",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "01/Jun/13 18:07",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "28/Apr/14 01:32",
        "Description": "Our current serialization is subject several ills\na) it breaks alignment by having a 1 byte flag field (evil, generic)\nb) it doesn't handle any kind of extensible format like protobufs so it isn't future-proof\nc) it doesn't handle named vectors very well\nd) it totally breaks with any other kind of decoration as with Centroids or WeightedVector or ... (see b)\nI propose that we use the current tag byte on the current serialization with a new flag bit that indicates that the vector will use a protobuf encoding.  Then 3 bytes will be skipped to restore alignment.  Then there will be a protobuf encoding for the vector.",
        "Issue Links": []
    },
    "MAHOUT-1237": {
        "Key": "MAHOUT-1237",
        "Summary": "Total cost isn't computed properly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Dan Filimon",
        "Reporter": "Dan Filimon",
        "Created": "02/Jun/13 19:41",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "02/Jun/13 19:48",
        "Description": "The problem is that it adds up cluster weights instead of computing the sum of all the distances.",
        "Issue Links": []
    },
    "MAHOUT-1238": {
        "Key": "MAHOUT-1238",
        "Summary": "VectorWritable's bug with VectorView of sparse vectors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.7,                                            0.8",
        "Component/s": "Math",
        "Assignee": "Robin Anil",
        "Reporter": "Maysam Yabandeh",
        "Created": "03/Jun/13 10:46",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "03/Jun/13 14:18",
        "Description": "VectorWritable raises an exception if it is used on a VectorView of a sparse vector. The reason is that the sparse vector writes only the non-zero elements, while VectorView's implementation of getNumNondefaultElements() returns the size of the entire data. Later when reading the vector, VectorWritable expects reading more items that was written.",
        "Issue Links": []
    },
    "MAHOUT-1239": {
        "Key": "MAHOUT-1239",
        "Summary": "Standardize form of log-likelihood computation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "03/Jun/13 20:40",
        "Updated": "02/Mar/14 20:17",
        "Resolved": "04/Jun/13 00:19",
        "Description": "qzchenwl@gmail.com reported that LogLikelihood.logLikelihoodRatio() looked like its formula was incorrect, at least with respect to http://tdunning.blogspot.mx/2008/03/surprise-and-coincidence.html\nIt appears that the calculation is correct but in a different form, that is not immediately recognizable as correct. The proposal here is to change the code to match the blog post and avoid confusion (and ends up avoiding 2 method calls).\n(Along the way, I think this fixes a tiny other problem in a related test. We have a test case that detects when round-off would produce a negative LLR and should be clamped to 0, but the test asserts that the result is >0 not >=0.)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1257"
        ]
    },
    "MAHOUT-1240": {
        "Key": "MAHOUT-1240",
        "Summary": "Randomized testing and Serialization of NonZeros",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "04/Jun/13 13:48",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "04/Jun/13 14:42",
        "Description": "Currently the nonZero iterator does not guarantee nonZero iteration for certain vectors (RASV, SASV) for performance reason. However vector view iterator adds a zero check.. To be correct we have to either remove the check or do correct non zero serialization everywhere. However this means going over the vectors in two passes. Given that is pretty fast already, I am fixing the logic bug. We can tackle the speed up for the next release.\nThis also adds a randomized test for serialization that catches all such bugs.",
        "Issue Links": []
    },
    "MAHOUT-1241": {
        "Key": "MAHOUT-1241",
        "Summary": "Mailing list archives not available",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Documentation",
        "Assignee": "Robin Anil",
        "Reporter": "Zeno Gantner",
        "Created": "04/Jun/13 14:55",
        "Updated": "10/Jun/13 08:27",
        "Resolved": "08/Jun/13 15:29",
        "Description": "http://mail-archives.apache.org/mod_mbox/lucene-mahout-user/\nhttp://mail-archives.apache.org/mod_mbox/lucene-mahout-dev/\ngive me a 404 error.\nThese are the mailing lists archives are linked from here:\nhttp://mahout.apache.org/mailinglists.html",
        "Issue Links": []
    },
    "MAHOUT-1242": {
        "Key": "MAHOUT-1242",
        "Summary": "No key redistribution function for associative maps",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.9",
        "Component/s": "collections,                                            Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dawid Weiss",
        "Created": "05/Jun/13 13:08",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "04/Dec/13 20:05",
        "Description": "All integer-based maps currently use HashFunctions.hash(int) which just returns the key value:\n\n  /**\n   * Returns a hashcode for the specified value.\n   *\n   * @return a hash code value for the specified value.\n   */\n  public static int hash(int value) {\n    return value;\n\n    //return value * 0x278DDE6D; // see org.apache.mahout.math.jet.random.engine.DRand\n\n    /*\n    value &= 0x7FFFFFFF; // make it >=0\n    int hashCode = 0;\n    do hashCode = 31*hashCode + value%10;\n    while ((value /= 10) > 0);\n\n    return 28629151*hashCode; // spread even further; h*31^5\n    */\n  }\n \n\nThis easily leads to very degenerate behavior on keys that have constant lower bits (long collision chains). A simple (and strong) hash function like the final step of murmurhash3 goes a long way at ensuring the keys distribution is more uniform regardless of the input distribution.",
        "Issue Links": []
    },
    "MAHOUT-1243": {
        "Key": "MAHOUT-1243",
        "Summary": "Dictionary file format in Lucene-Mahout integration is not in SequenceFileFormat",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "06/Jun/13 02:40",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "08/Jun/13 08:50",
        "Description": "Dictionary file format generated from lucene.vectors is not in SequenceFileFormat and hence not acceptable as input to CVB clustering.\nThe problem code from Driver.java\n\n\n    File dictOutFile = new File(dictOut);\n    log.info(\"Dictionary Output file: {}\", dictOutFile);\n    Writer writer = Files.newWriter(dictOutFile, Charsets.UTF_8);\n    DelimitedTermInfoWriter tiWriter = new DelimitedTermInfoWriter(writer, delimiter, field);\n    try {\n      tiWriter.write(termInfo);\n    } finally {\n      Closeables.close(tiWriter, false);\n    }",
        "Issue Links": []
    },
    "MAHOUT-1244": {
        "Key": "MAHOUT-1244",
        "Summary": "Upgrade Mahout to Lucene 4.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "07/Jun/13 02:53",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "07/Jun/13 04:11",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1245": {
        "Key": "MAHOUT-1245",
        "Summary": "Move Website(s) to ASF CMS",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Grant Ingersoll",
        "Created": "08/Jun/13 11:07",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "14/Dec/13 16:29",
        "Description": "The ASF CMS makes editing sites a whole lot easier using pub-sub and Markdown.\nWe should move to it.  We will be much happier.  I'd even propose we move most of our wiki to it and let users comment instead of edit.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1305"
        ]
    },
    "MAHOUT-1246": {
        "Key": "MAHOUT-1246",
        "Summary": "Bring Mahout website from the 2000s to 2010s.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Robin Anil",
        "Reporter": "Robin Anil",
        "Created": "08/Jun/13 17:11",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "08/Jun/13 17:56",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1247": {
        "Key": "MAHOUT-1247",
        "Summary": "cluster-reuters doesn't work on Hadoop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Grant Ingersoll",
        "Created": "09/Jun/13 11:50",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 20:51",
        "Description": "At least two issues:\n1. MAHOUT-992 messed up the Distributed Cache stuff somehow\n2. The ExtractReuters data is not being moved to HDFS.",
        "Issue Links": [
            "/jira/browse/MAHOUT-992"
        ]
    },
    "MAHOUT-1248": {
        "Key": "MAHOUT-1248",
        "Summary": "Build tools around mahout to use grid search with cross validation to tune hyperparameter lambda",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Saikat Kanjilal",
        "Created": "09/Jun/13 15:09",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "21/Mar/14 22:57",
        "Description": "The goal of this task is to create tools around mahout to tune the hyperparameter lambda by using grid search with cross validation data.  It'd be good to create a subcomponent of ALS that is strictly geared towards convergence.",
        "Issue Links": []
    },
    "MAHOUT-1249": {
        "Key": "MAHOUT-1249",
        "Summary": "Build tools around mahout to check the training error of factorization and automatically detect convergence",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Saikat Kanjilal",
        "Created": "09/Jun/13 15:13",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "18/Apr/14 07:15",
        "Description": "The goal of this task is to check the training error of the factorization during the computation to make it automatically detect convergence.  The goal is not to have to specify the number of iterations as a parameter needed for convergence.",
        "Issue Links": []
    },
    "MAHOUT-1250": {
        "Key": "MAHOUT-1250",
        "Summary": "Deprecate unused algorithms",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Jun/13 18:09",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 22:03",
        "Description": "As discussed on the mailinglist, we will deprecate a lot of unused algorithms to tackle the 1.0 release with a more slim codebase. \nSo far we propose to deprecate these algorithms (and schedule them for removal):\n\nClustering:\n    Dirichlet\n    MeanShift\n    MinHash\n\t\tEigencuts in o.a.m.clustering.spectral.eigencuts\nClassification:\n\t\tWinnow\n\t\tPerceptron\nFrequent Pattern Mining\nCollaborative Filtering:\n\t\tall recommenders in o.a.m.cf.taste.impl.recommender.knn\n\t\tTreeClusteringRecommender in o.a.m.cf.taste.impl.recommender\n\t\tSlopeOne implementations in o.a.m..cf.taste.hadoop.slopeone and o.a.m.cf.taste.impl.recommender.slopeone\n\t\tdistributed pseudo recommender in o.a.m.cf.taste.hadoop.pseudo\n\n-Math\n\t\tHadoop entropy stuff in o.a.m.math.stats.entropy\n                Lanczos in favor of SSVD",
        "Issue Links": []
    },
    "MAHOUT-1251": {
        "Key": "MAHOUT-1251",
        "Summary": "Optimize MinHashMapper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Jun/13 19:36",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "09/Jun/13 20:20",
        "Description": "Remove unnecessary actions from MinHashMapper:\nIt does a unnecessary string comparison per non-zero element of each input vector and it unnecessarily clones the input vector for each keyGroup",
        "Issue Links": []
    },
    "MAHOUT-1252": {
        "Key": "MAHOUT-1252",
        "Summary": "Add support for Finite State Transducers (FST) as a DictionaryType.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.11.0",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "09/Jun/13 19:48",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "11/Aug/15 23:32",
        "Description": "Add support for Finite State Transducers (FST) as a DictionaryType, this should result in an order of magnitude speedup of seq2sparse.",
        "Issue Links": []
    },
    "MAHOUT-1253": {
        "Key": "MAHOUT-1253",
        "Summary": "Add experiment tools for StreamingKMeans",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Dan Filimon",
        "Reporter": "Dan Filimon",
        "Created": "11/Jun/13 20:29",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "24/Nov/13 23:32",
        "Description": "Merge in this patch https://reviews.apache.org/r/11302/",
        "Issue Links": []
    },
    "MAHOUT-1254": {
        "Key": "MAHOUT-1254",
        "Summary": "Final round of cleanup for StreamingKMeans",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Dan Filimon",
        "Reporter": "Dan Filimon",
        "Created": "11/Jun/13 20:30",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "16/Jun/13 17:00",
        "Description": "Did a bit of tweaking on StreamingKMeans, driver, mapper and reducer to share more code and make it nicer.\nNeed to put this in.",
        "Issue Links": []
    },
    "MAHOUT-1255": {
        "Key": "MAHOUT-1255",
        "Summary": "Change BallKMeans weighting to use log(weight)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Dan Filimon",
        "Reporter": "Dan Filimon",
        "Created": "11/Jun/13 20:31",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "16/Jun/13 17:04",
        "Description": "Weirdness is happening in the reducer when doing k-means++ sampling by weights.\nChange from multiplying the probability with the weight to multiplying by 2 * log(weight).",
        "Issue Links": []
    },
    "MAHOUT-1256": {
        "Key": "MAHOUT-1256",
        "Summary": "Improve the CSV handling code to get vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dan Filimon",
        "Created": "11/Jun/13 20:33",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "03/Dec/13 23:22",
        "Description": "Minor additions to iterate through a CSV file directly (as long as it's only numbers).",
        "Issue Links": []
    },
    "MAHOUT-1257": {
        "Key": "MAHOUT-1257",
        "Summary": "performance improvement to LogLikehood",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Michael Sokolov",
        "Created": "12/Jun/13 17:39",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Mar/14 20:17",
        "Description": "This patch reduces the amount of computation required for LLR. It simplifies the math by canceling terms.  In a microbenchmark we saw an 18% run time improvement.\nI couldn't seem to find the UI for uploading a patch file.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1239"
        ]
    },
    "MAHOUT-1258": {
        "Key": "MAHOUT-1258",
        "Summary": "Another shot at findbugs and checkstyle",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "12/Jun/13 19:41",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "12/Jun/13 20:42",
        "Description": "We should have a shot at findbugs and checkstyle before the upcoming 0.8 release.",
        "Issue Links": []
    },
    "MAHOUT-1259": {
        "Key": "MAHOUT-1259",
        "Summary": "toString() method of SequentialAccessSparseVector has closing brace missing for empty vector",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Math",
        "Assignee": "Robin Anil",
        "Reporter": "Abhinav M Kulkarni",
        "Created": "13/Jun/13 02:39",
        "Updated": "13/Jun/13 05:13",
        "Resolved": "13/Jun/13 04:13",
        "Description": "toString() method of SequentialAccessSparseVector.java in Math module did not have closing brace for empty vectors. If the sparse vector is empty (or newly created), toString() method should return '{}'. Currently it returns '{'.",
        "Issue Links": []
    },
    "MAHOUT-1260": {
        "Key": "MAHOUT-1260",
        "Summary": "Online javadocs has documentation for classes only from core module",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Abhinav M Kulkarni",
        "Created": "13/Jun/13 13:36",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "23/Mar/14 08:40",
        "Description": "The online javadoc has no documentation for the classes in other modules such as math. Given that there are very few textbooks and other resources to learn about Mahout, most new users are going to familiarize themselves with the codebase using online javadocs. Hence this needs to be fixed.",
        "Issue Links": []
    },
    "MAHOUT-1261": {
        "Key": "MAHOUT-1261",
        "Summary": "TasteHadoopUtils.idToIndex can return an int that has size Integer.MAX_VALUE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dan Filimon",
        "Created": "13/Jun/13 15:09",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "28/Nov/13 14:47",
        "Description": "I'm running ItemSimilarityJob on a very large (~600M by 4B) matrix that's very sparse (total set of associations is 630MB).\nThe job fails because of an IndexException in ToUserVectorsReducer.\nTasteHadoopUtils.idToIndex(long id) hashes a long with:\n0x7fffffff & Longs.hashCode(id) (line o.a.m.cf.taste.hadoop.TasteHadoopUtils:57).\nFor some id (I don't know what value), the result returned is Integer.MAX_VALUE.\nThis cannot be set in the userVector because the cardinality of that is also Integer.MAX_VALUE and it throws an exception.\nSo, the issue is that values from 0 to INT_MAX are returned by idToIndex but the vector only has 0 to INT_MAX - 1 possible entries.\nIt's a nasty little off-by-one bug.\nI'm thinking of just % size when setting.\nssc & everyone else, thoughts?",
        "Issue Links": []
    },
    "MAHOUT-1262": {
        "Key": "MAHOUT-1262",
        "Summary": "Cleanup LDA code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "13/Jun/13 22:31",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "17/Jun/13 05:59",
        "Description": "A few cleanups in the LDA code. Removed a lot of unused methods, made the driver use AbstractJob's prepareJob() methods, avoided a few unnecessary copies in vector instantiations etc.",
        "Issue Links": []
    },
    "MAHOUT-1263": {
        "Key": "MAHOUT-1263",
        "Summary": "Serialise/Deserialise Lambda value for OnlineLogisticRegression",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mike Davy",
        "Created": "14/Jun/13 13:18",
        "Updated": "27/Jul/13 04:23",
        "Resolved": "15/Jun/13 04:40",
        "Description": "The value for Lambda in OnlineLogisticRegression seems not the be serialised/deserialised correctly. \nIf I train a model with a specific lambda value, serialise it, then read it back in, the value of Lambda goes back to the default value (1.0e-5). \nI've created a patch that adds the lambda value into the write/readFields (org.apache.hadoop.io.Writable). Patch includes a unit test that checks the values after serialising/deserialising to/from a ByteArray. \nI think this is correct, unless I'm missing something obvious?\nNote: this patch is not backwards compatible - can easily adapt to be more backwards compatible if required.",
        "Issue Links": []
    },
    "MAHOUT-1264": {
        "Key": "MAHOUT-1264",
        "Summary": "Performance optimizations in RecommenderJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "17/Jun/13 07:05",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "17/Jun/13 07:10",
        "Description": "Rework RecommenderJob to minimize object instantiations and use MultipleInputs where possible.",
        "Issue Links": []
    },
    "MAHOUT-1265": {
        "Key": "MAHOUT-1265",
        "Summary": "Add Multilayer Perceptron",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Yexi Jiang",
        "Created": "18/Jun/13 12:28",
        "Updated": "15/Apr/14 20:13",
        "Resolved": "19/Dec/13 19:31",
        "Description": "Design of multilayer perceptron\n1. Motivation\nA multilayer perceptron (MLP) is a kind of feed forward artificial neural network, which is a mathematical model inspired by the biological neural network. The multilayer perceptron can be used for various machine learning tasks such as classification and regression. It is helpful if it can be included in mahout.\n2. API\nThe design goal of API is to facilitate the usage of MLP for user, and make the implementation detail user transparent.\nThe following is an example code of how user uses the MLP.\n-------------------------------------\n//  set the parameters\ndouble learningRate = 0.5;\ndouble momentum = 0.1;\nint[] layerSizeArray = new int[] \n{2, 5, 1}\n;\nString costFuncName = \u201cSquaredError\u201d;\nString squashingFuncName = \u201cSigmoid\u201d;\n//  the location to store the model, if there is already an existing model at the specified location, MLP will throw exception\nURI modelLocation = ...\nMultilayerPerceptron mlp = new MultiLayerPerceptron(layerSizeArray, modelLocation);\nmlp.setLearningRate(learningRate).setMomentum(momentum).setRegularization(...).setCostFunction(...).setSquashingFunction(...);\n//  the user can also load an existing model with given URI and update the model with new training data, if there is no existing model at the specified location, an exception will be thrown\n/*\nMultilayerPerceptron mlp = new MultiLayerPerceptron(learningRate, regularization, momentum, squashingFuncName, costFuncName, modelLocation);\n*/\nURI trainingDataLocation = \u2026\n//  the detail of training is transparent to the user, it may running in a single machine or in a distributed environment\nmlp.train(trainingDataLocation);\n//  user can also train the model with one training instance in stochastic gradient descent way\nVector trainingInstance = ...\nmlp.train(trainingInstance);\n//  prepare the input feature\nVector inputFeature \u2026\n//  the semantic meaning of the output result is defined by the user\n//  in general case, the dimension of output vector is 1 for regression and two-class classification\n//  the dimension of output vector is n for n-class classification (n > 2)\nVector outputVector = mlp.output(inputFeature); \n-------------------------------------\n3. Methodology\nThe output calculation can be easily implemented with feed-forward approach. Also, the single machine training is straightforward. The following will describe how to train MLP in distributed way with batch gradient descent. The workflow is illustrated as the below figure.\nhttps://docs.google.com/drawings/d/1s8hiYKpdrP3epe1BzkrddIfShkxPrqSuQBH0NAawEM4/pub?w=960&h=720\nFor the distributed training, each training iteration is divided into two steps, the weight update calculation step and the weight update step. The distributed MLP can only be trained in batch-update approach.\n3.1 The partial weight update calculation step:\nThis step trains the MLP distributedly. Each task will get a copy of the MLP model, and calculate the weight update with a partition of data.\nSuppose the training error is E(w) = \u00bd \\sigma_\n{d \\in D}\n cost(t_d, y_d), where D denotes the training set, d denotes a training instance, t_d denotes the class label and y_d denotes the output of the MLP. Also, suppose sigmoid function is used as the squashing function, \nsquared error is used as the cost function, \nt_i denotes the target value for the ith dimension of the output layer, \no_i denotes the actual output for the ith dimension of the output layer, \nl denotes the learning rate,\nw_\n{ij} denotes the weight between the jth neuron in previous layer and the ith neuron in the next layer. \n\nThe weight of each edge is updated as \n\n\\Delta w_{ij}\n = l * 1 / m * \\delta_j * o_i, \nwhere \\delta_j = - \\sigma_\n{m} * o_j^{(m)} * (1 - o_j^{(m)}) * (t_j^{(m)} - o_j^{(m)}) for output layer, \\delta = - \\sigma_{m}\n * o_j^\n{(m)} * (1 - o_j^{(m)}\n) * \\sigma_k \\delta_k * w_\n{jk}\n for hidden layer. \nIt is easy to know that \\delta_j can be rewritten as \n\\delta_j = - \\sigma_\n{i = 1}\n^k \\sigma_\n{m_i}\n * o_j^\n{(m_i)} * (1 - o_j^{(m_i)}\n) * (t_j^\n{(m_i)} - o_j^{(m_i)}\n)\nThe above equation indicates that the \\delta_j can be divided into k parts.\nSo for the implementation, each mapper can calculate part of \\delta_j with given partition of data, and then store the result into a specified location.\n3.2 The model update step:\nAfter k parts of \\delta_j been calculated, a separate program can be used to merge the k parts of \\delta_j into one to update the weight matrices.\nThis program can load the results calculated in the weight update calculation step and update the weight matrices.",
        "Issue Links": [
            "/jira/browse/MAHOUT-975",
            "/jira/browse/MAHOUT-976"
        ]
    },
    "MAHOUT-1266": {
        "Key": "MAHOUT-1266",
        "Summary": "Two minor problems in DistributedRowMatrix using MatrixMultiplication",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Martin Illecker",
        "Created": "18/Jun/13 21:06",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "19/Jun/13 10:51",
        "Description": "Hello,\nI think I have found two minor problems in DistributedRowMatrix.\nIn [1] the condition is wrong, because (l x m) * (m x n) = (l x n).\nThe condition should be like in [2]. \nAnd in times[3] the this.transpose() seems to be missing? (See [4])\nDo you have any benchmark results for Mahout MatrixMultiplication?\nThanks!\nMartin\n[1] https://github.com/apache/mahout/blob/trunk/core/src/main/java/org/apache/mahout/math/hadoop/DistributedRowMatrix.java#L191-193\n[2] https://github.com/millecker/applications/blob/master/hadoop/rootbeer/matrixmultiplication/src/at/illecker/hadoop/rootbeer/examples/matrixmultiplication/DistributedRowMatrix.java#L221-225\n[3] https://github.com/apache/mahout/blob/trunk/core/src/main/java/org/apache/mahout/math/hadoop/DistributedRowMatrix.java#L190-206\n[4] https://github.com/millecker/applications/blob/master/hadoop/rootbeer/matrixmultiplication/src/at/illecker/hadoop/rootbeer/examples/matrixmultiplication/DistributedRowMatrix.java#L230-231",
        "Issue Links": []
    },
    "MAHOUT-1267": {
        "Key": "MAHOUT-1267",
        "Summary": "Remove object instantiations from RowSimilarityJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "19/Jun/13 06:00",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "18/Oct/13 07:47",
        "Description": "remove object instantiations from RowSimilarityJob, use primitive arrays instead of a Vector.Element[] array in the cooccurrence enumeration.",
        "Issue Links": []
    },
    "MAHOUT-1268": {
        "Key": "MAHOUT-1268",
        "Summary": "Wrong output directory for CVB",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "25/Jun/13 04:21",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "25/Jun/13 12:42",
        "Description": "I think that I introduced a bug in MAHOUT-1262 by accidentally writing to the wrong output dir (as reported by Mark Wicks on the mailinglist).",
        "Issue Links": []
    },
    "MAHOUT-1269": {
        "Key": "MAHOUT-1269",
        "Summary": "Cleanup deprecated Lucene 3.x API calls from lucene2seq utility unit tests",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "25/Jun/13 18:33",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "25/Jun/13 18:57",
        "Description": "Cleanup deprecated Lucene 3.x API calls from lucene2seq utility from UnstoredFieldsDocument.java and MultipleFieldsDocument.java",
        "Issue Links": []
    },
    "MAHOUT-1270": {
        "Key": "MAHOUT-1270",
        "Summary": "Broken link on Developer Resources page",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.8",
        "Component/s": "Documentation",
        "Assignee": "Robin Anil",
        "Reporter": "Erhan Bagdemir",
        "Created": "27/Jun/13 08:25",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "27/Jun/13 21:36",
        "Description": "The link \"How to contribute\" on the page\nhttps://cwiki.apache.org/confluence/display/MAHOUT/Developer+Resources\nis broken :-| \nhttps://cwiki.apache.org/MAHOUT/how-to-contribute.html returns 404.",
        "Issue Links": []
    },
    "MAHOUT-1271": {
        "Key": "MAHOUT-1271",
        "Summary": "classify-20newsgroups.sh fails during the seqdirectory step",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "28/Jun/13 03:16",
        "Updated": "27/Jul/13 02:09",
        "Resolved": "28/Jun/13 03:58",
        "Description": "classify-20newsgroups.sh fails during the seqdirectory step as it fails to overwrite the output folder.\n\n\nException in thread \"main\" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory /tmp/mahout-work-jenkins/20news-seq already exists",
        "Issue Links": []
    },
    "MAHOUT-1272": {
        "Key": "MAHOUT-1272",
        "Summary": "Parallel SGD matrix factorizer for SVDrecommender",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sean R. Owen",
        "Reporter": "Peng Cheng",
        "Created": "28/Jun/13 20:53",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "07/Jul/13 23:51",
        "Description": "a parallel factorizer based on MAHOUT-1089 may achieve better performance on multicore processor.\nexisting code is single-thread and perhaps may still be outperformed by the default ALS-WR.\nIn addition, its hardcoded online-to-batch-conversion prevents it to be used by an online recommender. An online SGD implementation may help build high-performance online recommender as a replacement of the outdated slope-one.\nThe new factorizer can implement either DSGD (http://www.mpi-inf.mpg.de/~rgemulla/publications/gemulla11dsgd.pdf) or hogwild! (www.cs.wisc.edu/~brecht/papers/hogwildTR.pdf).\nRelated discussion has been carried on for a while but remain inconclusive:\nhttp://web.archiveorange.com/archive/v/z6zxQUSahofuPKEzZkzl",
        "Issue Links": []
    },
    "MAHOUT-1273": {
        "Key": "MAHOUT-1273",
        "Summary": "Single Pass Algorithm for Penalized Linear Regression with Cross Validation on MapReduce",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kun Yang",
        "Created": "03/Jul/13 18:47",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "09/Mar/14 22:27",
        "Description": "Penalized linear regression such as Lasso, Elastic-net are widely used in machine learning, but there are no very efficient scalable implementations on MapReduce.\nThe published distributed algorithms for solving this problem is either iterative (which is not good for MapReduce, see Steven Boyd's paper) or approximate (what if we need exact solutions, see Paralleled stochastic gradient descent); another disadvantage of these algorithms is that they can not do cross validation in the training phase, which requires a user-specified penalty parameter in advance. \nMy ideas can train the model with cross validation in a single pass. They are based on some simple observations.\nThe core algorithm is a modified version of coordinate descent (see J. Freedman's paper). They implemented a very efficient R package \"glmnet\", which is the de facto standard of penalized regression.\nI have implemented the primitive version of this algorithm in Alpine Data Labs.",
        "Issue Links": []
    },
    "MAHOUT-1274": {
        "Key": "MAHOUT-1274",
        "Summary": "SGD-based Online SVD recommender",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Peng Cheng",
        "Created": "06/Jul/13 17:16",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Mar/14 19:17",
        "Description": "an online SVD recommender is otherwise similar to an offline SVD recommender except that, upon receiving one or several new recommendations, it can add them into the training dataModel and update the result accordingly in real time.\nan online SVD recommender should override setPreference(...) and removePreference(...) in AbstractRecommender such that the factorization result is updated in O(1) time and without retraining.\nRight now the slopeOneRecommender is the only component possessing such capability.\nSince SGD is intrinsically an online algorithm and its CF implementation is available in core-0.8 (See MAHOUT-1089, MAHOUT-1272), I presume it would be a good time to convert it. Such feature could come in handy for some websites.\nImplementation: Adding new users, items, or increasing rating matrix rank are just increasing size of user and item matrices. Reducing rating matrix rank involves just one svd. The real challenge here is that sgd is NO ONE-PASS algorithm, multiple passes are required to achieve an acceptable optimality and even more so if hyperparameters are bad. But here are two possible circumvents:\n1. Use one-pass algorithms like averaged-SGD, not sure if it can ever work as applying stochastic convex-opt algorithm to non-convex problem is anarchy. But it may be a long shot.\n2. Run incomplete passes in each online update using ratings randomly sampled (but not uniformly sampled) from latest dataModel. I don't know how exactly this should be done but new rating should be sampled more frequently. Uniform sampling will results in old ratings being used more than new ratings in total. If somebody has worked on this batch-to-online conversion before and share his insight that would be awesome. This seems to be the most viable option, if I get the non-uniform pseudorandom generator that maintains a cumulative uniform distribution I want.\nI found a very old ticket (MAHOUT-572) mentioning online SVD recommender but it didn't pay off. Hopefully its not a bad idea to submit a new ticket here.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1286"
        ]
    },
    "MAHOUT-1275": {
        "Key": "MAHOUT-1275",
        "Summary": "Drop some of the Release Artifact File Types",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Grant Ingersoll",
        "Created": "08/Jul/13 10:39",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "27/Oct/13 21:22",
        "Description": "There really is no reason why we need so many release artifacts for the distribution.  We run on *NIX machines.  Zip and Gzip are standard tools, let's save a few bits, along with Release Manager upload times, and drop the BZ2 format.",
        "Issue Links": []
    },
    "MAHOUT-1276": {
        "Key": "MAHOUT-1276",
        "Summary": "job name for ParallelALSFactorizationJob is confusing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Do Yung Yoon",
        "Created": "09/Jul/13 08:39",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "20/Nov/13 21:54",
        "Description": "ParallelALSFactorizationJob set job name per each iteration in different with logger`s info. job name start from 2 and numIterations is 0 based. therefore if I run job with iteration 5, job names start from \"Recompute M, iteration (2/5)\". It would be better if we set job name same as logger info message since current job name could be confusing.",
        "Issue Links": []
    },
    "MAHOUT-1277": {
        "Key": "MAHOUT-1277",
        "Summary": "Lose dependency on custom commons-cli",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "build,                                            CLI",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "10/Jul/13 11:45",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 17:06",
        "Description": "In 0.8 we have dependency on custom commons-cli fork org.apache.mahout.commons:commons-cli. There are no sources for this under Mahout version control. It's a risk keeping this as dependency.\nWe should either use officially released and maintained commons-cli version, or if it's not sufficient for Mahout project needs, replace it completely with something else (e.g. like JCommander).",
        "Issue Links": [
            "/jira/browse/MAHOUT-622"
        ]
    },
    "MAHOUT-1278": {
        "Key": "MAHOUT-1278",
        "Summary": "Upgrade to Apache parent pom v16",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "10/Jul/13 12:09",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "01/Apr/15 22:48",
        "Description": "We should update dependency on Apache parent pom (currently we depend on version 9, while 13 is already released).\nWith the upgrade we should make the most of inherited settings and plugin versions from Apache parent pom, so we override only what is necessary, to make Mahout POMs smaller and easier to maintain.\nHopefully by the time this issue gets worked on, maven-remote-resources-plugin with MRRESOURCES-53 fix will be released (since we're affected by it - test jars are being resolved from remote repository instead from the current build / rector repository), and updated Apache parent pom released.\nImplementation note: Mahout parent module and mahout-buildtools module both use Apache parent pom as parent, so both need to be updated. mahout-buildtools module had to be separate from the mahout parent pom (not inheriting it), so that buildtools module can be referenced as dependency of various source quality check plugins.",
        "Issue Links": []
    },
    "MAHOUT-1279": {
        "Key": "MAHOUT-1279",
        "Summary": "The test \"TestMinHashClustering\" fails in Mahout core",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Erhan Bagdemir",
        "Created": "10/Jul/13 13:27",
        "Updated": "03/Feb/14 07:49",
        "Resolved": "29/Jul/13 12:23",
        "Description": "Hey folks, \ni tried building the core component, but unfortunately the test TestMinHashClustering fails. \nFailed tests: \n  TestMinHashClustering.testMurmurMinHashMRJobHashIndex:218->Assert.assertEquals:555->Assert.assertEquals:118->Assert.failNotEquals:743->Assert.fail:88 MinHash MR Job Hash Index failed for MURMUR expected:<0> but was:<-1>\nTests run: 834, Failures: 1, Errors: 0, Skipped: 0",
        "Issue Links": []
    },
    "MAHOUT-1280": {
        "Key": "MAHOUT-1280",
        "Summary": "Move out UpperTriangular matrix out of ssvd into mahout-math. Add SymmetricMatrix.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "10/Jul/13 17:47",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "31/Jul/13 05:56",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1281"
        ]
    },
    "MAHOUT-1281": {
        "Key": "MAHOUT-1281",
        "Summary": "Wean distributed SSVD from apache-math dependency.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "10/Jul/13 17:54",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "31/Jul/13 05:56",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1280"
        ]
    },
    "MAHOUT-1282": {
        "Key": "MAHOUT-1282",
        "Summary": "Optimize Diagonal matrix multiplication. add left/right multiply versions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "11/Jul/13 01:10",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "31/Jul/13 05:56",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1297",
            "/jira/browse/MAHOUT-1299"
        ]
    },
    "MAHOUT-1283": {
        "Key": "MAHOUT-1283",
        "Summary": "Matrix multiplication problem in mahout-math-0.6.jar. Details can be seen in the Carrot2's forum discussion whose link is given in the description.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Seyfullah Demir",
        "Created": "11/Jul/13 20:15",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "29/Jul/13 05:51",
        "Description": "http://carrot2-users-and-developers-forum.607571.n2.nabble.com/A-question-about-Apache-Mahout-API-td7578181.html",
        "Issue Links": []
    },
    "MAHOUT-1284": {
        "Key": "MAHOUT-1284",
        "Summary": "DummyRecordWriter's bug with reused Writables",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Maysam Yabandeh",
        "Created": "12/Jul/13 07:32",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "29/Jul/13 07:03",
        "Description": "It is a recommended practice to reuse the Writable objects. DummyRecordWriter, which is used for testing in Mahout, however keeps the same Writable instance in a map: next time that the user reuses the Writable object, the internal map of DummyRecordWriter changes as well. This makes DummyRecordWriter fail for testing the MapReduce jobs that reuse the Writables.",
        "Issue Links": []
    },
    "MAHOUT-1285": {
        "Key": "MAHOUT-1285",
        "Summary": "Arff loader can misparse string data as double",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Neil Walkinshaw",
        "Created": "15/Jul/13 21:31",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "29/Nov/13 19:06",
        "Description": "Have successfully loaded numerous ARFF files with Mahout (originally generated via WEKA). The files contain randomly generated data. For a specific random seed, the following exception is thrown:\njava.lang.NumberFormatException: For input string: \"b1shkt70694difsmmmdv0ikmoh\"\n\tat sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:1241)\n\tat java.lang.Double.parseDouble(Double.java:540)\n\tat org.apache.mahout.utils.vectors.arff.MapBackedARFFModel.processNumeric(MapBackedARFFModel.java:146)\n\tat org.apache.mahout.utils.vectors.arff.MapBackedARFFModel.getValue(MapBackedARFFModel.java:97)\n\tat org.apache.mahout.utils.vectors.arff.ARFFIterator.computeNext(ARFFIterator.java:77)\n\tat org.apache.mahout.utils.vectors.arff.ARFFIterator.computeNext(ARFFIterator.java:30)\n\tat com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)\n\tat com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)\n\tat org.apache.mahout.utils.vectors.io.SequenceFileVectorWriter.write(SequenceFileVectorWriter.java:44)\n\tat org.apache.mahout.utils.vectors.arff.Driver.writeFile(Driver.java:251)\n\tat org.apache.mahout.utils.vectors.arff.Driver.main(Driver.java:145)\n\tat libInterfaces.MahoutTraceBuilder.generateMahoutFile(MahoutTraceBuilder.java:38)\n\tat libInterfaces.MahoutTraceBuilder.generateMahoutReader(MahoutTraceBuilder.java:42)\n\tat tests.InputTester.testMahoutMeansShift(InputTester.java:111)",
        "Issue Links": []
    },
    "MAHOUT-1286": {
        "Key": "MAHOUT-1286",
        "Summary": "Memory-efficient DataModel, supporting fast online updates and element-wise iteration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Peng Cheng",
        "Created": "17/Jul/13 21:57",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "03/Dec/13 03:44",
        "Description": "Most DataModel implementation in current CF component use hash map to enable fast 2d indexing and update. This is not memory-efficient for big data set. e.g. Netflix prize dataset takes 11G heap space as a FileDataModel.\nImproved implementation of DataModel should use more compact data structure (like arrays), this can trade a little of time complexity in 2d indexing for vast improvement in memory efficiency. In addition, any online recommender or online-to-batch converted recommender will not be affected by this in training process.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1274"
        ]
    },
    "MAHOUT-1287": {
        "Key": "MAHOUT-1287",
        "Summary": "classifier.sgd.CsvRecordFactory incorrectly parses CSV format",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.9",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alex Franchuk",
        "Created": "18/Jul/13 17:39",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "27/Jul/13 05:09",
        "Description": "CsvRecordFactory uses very simplistic CSV parsing, and incorrectly parses CSV strings when there are double-quoted fields with commas present.\nThis problem also affects the command-line demo programs which use CsvRecordFactory (mostly the sgd-related programs).\nAttached is a patch to fix the problem.",
        "Issue Links": []
    },
    "MAHOUT-1288": {
        "Key": "MAHOUT-1288",
        "Summary": "Create recommendation as search demo",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "21/Jul/13 03:54",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "18/Dec/13 20:30",
        "Description": "The basic idea is that a recommendation engine can be deployed by doing off-line analysis to find anomalously cooccurring items and then indexing these items in a search engine.  These anomalous items are considered indicators which are indexed using an ordinary text search engine.  Recommendations are generated by querying the search engine using recent behavior as a query.  Recommendations can be combined with geo filtering and text queries.\nSee http://bit.ly/18vbbaT for a living design document.",
        "Issue Links": []
    },
    "MAHOUT-1289": {
        "Key": "MAHOUT-1289",
        "Summary": "Move downsampling code into RowSimilarityJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "21/Jul/13 18:15",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "30/Jul/13 05:15",
        "Description": "When computing similarities with RowSimilarityJob, downsampling highly frequent things is crucial for performance. At the moment, this is done by the data preparation code for collaborative filtering.\nWe should move the downsampling directly into RowSimilarityJob as we've seen a lot of cases where users want to directly use it.\nFurthermore, it should be possible to fix the random seed for the sampling to be able to conduct repeatable experiments.",
        "Issue Links": []
    },
    "MAHOUT-1290": {
        "Key": "MAHOUT-1290",
        "Summary": "Issue when running Mahout Recommender Demo",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "25/Jul/13 05:31",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "26/Jul/13 20:13",
        "Description": "When running jetty:run under mahout-integration, seeing a ClassNotFoundException:\n org.apache.mahout.cf.taste.*example.grouplens.*GroupLensRecommender.\nThe problem is happening because the webapp\nfolder wasn't moved to the examples dir and the Jetty dependency wasn't added asa Maven plugin when the GroupLens example moved to the examples submodule.",
        "Issue Links": []
    },
    "MAHOUT-1291": {
        "Key": "MAHOUT-1291",
        "Summary": "MahoutDriver yields cosmetically suboptimal exception when bin/mahout runs without args, on some Hadoop versions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sean R. Owen",
        "Created": "25/Jul/13 14:40",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "26/Jul/13 18:59",
        "Description": "If you run bin/mahout without arguments, an error is correctly displayed about lack of an argument. The part that displays the error is actually within Hadoop code. In some versions of Hadoop, in the error case, it will quit the JVM with System.exit(). In others, it does not.\nIn the calling code in MahoutDriver, in this error case, the main() method does not actually return. So, for versions where Hadoop code doesn't immediately exit the JVM, execution continues. This yields another exception. It's pretty harmless but ugly.\nAttached is a one-line fix, to return from main() in the error case, which is more correct to begin with.",
        "Issue Links": []
    },
    "MAHOUT-1292": {
        "Key": "MAHOUT-1292",
        "Summary": "lucene2seq should validate the 'id' field",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Liz Merkhofer",
        "Created": "25/Jul/13 17:48",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "16/Nov/13 19:11",
        "Description": "Lucene2seq creates only one sequencefile, rather than a file for each document in the index.\nRunning lucene2seq on my Solr (4.3) index produces a file with a header and, it seems, the field I specified from the index, concatenated for all the documents. After running this through seq2sparse and rowid (to prepare for cvb), the resulting matrix has only one row, though it should create one row per document.\nThis issue prevents, at least, data from a lucene index from being easily used as input for cvb. Lucene.vector is also currently inadequate: the keys to its sequence files are LongWriteable, and rowid will not convert only Text to IntWriteable, as is necessary for the keys in cvb.",
        "Issue Links": []
    },
    "MAHOUT-1293": {
        "Key": "MAHOUT-1293",
        "Summary": "mahout-distribution-0.8-src.tar.gz cannot be unpacked on Linux",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "26/Jul/13 17:38",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "21/Nov/13 21:21",
        "Description": "Just like binary distribution couldn't be unpacked (see MAHOUT-1229), mahout-distribution-0.8-src.tar.gz also cannot be unpacked (mahout executable cannot be unpacked to bin directory, bin directory permissions are not set). Zip distribution src archive can be unpacked.",
        "Issue Links": []
    },
    "MAHOUT-1294": {
        "Key": "MAHOUT-1294",
        "Summary": "Cleanup previously installed artifacts from CI server local repository",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "26/Jul/13 20:24",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "28/Oct/13 13:41",
        "Description": "Mahout builds produce multiple and relatively large artifacts.\nBefore being uploaded to Apache snapshots repository they get installed into Jenkins node local repository.\nMahout-Examples-Cluster-Reuters-II job often runs on Apache Jenkins nodes with little free disk space, and fails in package phase when trying to create new artifacts.\nWe can be little bit better users of Apache infrastructure, and in build jobs on Apache Jenkins nodes, before producing new artifacts, cleanup previously built and installed Apache Mahout artifacts from CI server node local repository.",
        "Issue Links": []
    },
    "MAHOUT-1295": {
        "Key": "MAHOUT-1295",
        "Summary": "Exclude all Maven's target directories from distribution archives",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "26/Jul/13 23:01",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "28/Oct/13 13:57",
        "Description": "Current assembly descriptors for binary and source distribution archives include *.properties and other files from any directory looking from root of the project, but exclude only target directory of the parent/aggregator module.\nThis results in Maven's target directories from submodules being included in distribution archives with e.g. pom.properties files in them.",
        "Issue Links": []
    },
    "MAHOUT-1296": {
        "Key": "MAHOUT-1296",
        "Summary": "Remove deprecated algorithms",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "29/Jul/13 04:47",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "31/Jul/13 05:08",
        "Description": "Remove the algorithms we chose to deprecate in MAHOUT-1250",
        "Issue Links": []
    },
    "MAHOUT-1297": {
        "Key": "MAHOUT-1297",
        "Summary": "New module for linear algebra scala DSL (in-core operators support only to start with)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "29/Jul/13 19:54",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "21/Nov/13 21:37",
        "Description": "See initial set of in-core R-like operations here http://weatheringthrutechdays.blogspot.com/2013/07/scala-dsl-for-mahout-in-core-linear.html.\nA separate DSL for matlab-like syntax is being developed. The differences here are about replacing R-like %*% with * and finding another way to express elementwise * and /.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1300",
            "/jira/browse/MAHOUT-1282",
            "/jira/browse/MAHOUT-1346"
        ]
    },
    "MAHOUT-1298": {
        "Key": "MAHOUT-1298",
        "Summary": "SparseRowMatrix,SparseColMatrix: optimize transpose()",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "29/Jul/13 22:36",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "17/Oct/13 23:14",
        "Description": "these matrices lack optimized transpose and rely onto AbstractMatrix's O(mn) implementation which is not cool for very sparse subblocks. \nproposal is to implement a custom transpose with two things in mind: \n1) transpose result to row sparse matrix should be col sparse matrix, and vice versa (and not from default like() as default implementation would take);\n2) obviously, iterate only thru non-zero elements only of all rows(columns).",
        "Issue Links": []
    },
    "MAHOUT-1299": {
        "Key": "MAHOUT-1299",
        "Summary": "Add optimized versions of timesLeft(), timesRight() to SparseRow~,SparseColMatrices and binary times() operation in general",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "29/Jul/13 23:07",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "05/Nov/13 21:53",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1282"
        ]
    },
    "MAHOUT-1300": {
        "Key": "MAHOUT-1300",
        "Summary": "Support for easy functional matrix views and some of their derivatives",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "30/Jul/13 19:40",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "08/Oct/13 19:10",
        "Description": "Support for easy matrix views based on (Int,Int)=>Double function. \nCurrent derived views: \n(1) general functional view\n(2) transposed matrix view\n(3) uniform matrix view (based on function composition over symmetric uniform)\n(4) symmetric uniform matrix view (based on murmur64)\n(5) random gaussian matrix view.\nI know that there's a trinary random matrix as well which could be scripted out as a view as well (methinks), as well as Omega thing in distributed SSVD which also perhaps could be replaced by a symmetric uniform view.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1297"
        ]
    },
    "MAHOUT-1301": {
        "Key": "MAHOUT-1301",
        "Summary": "toString() method of SequentialAccessSparseVector has excess comma at the end",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexander Senov",
        "Created": "01/Aug/13 14:31",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "01/Aug/13 21:48",
        "Description": "Realization of SequentialAccessSparseVector toString() method had changed in MAHOUT-1259 patch. Unfortunately, that patch introduced new bug: output of the toString() method had been changed - extra comma added at the end of the string\nExample: \nConsider following sparse vector\n\nVector v = new SequentialAccessSparseVector(capacity);\nv.set(1, 0.1);\nv.set(3, 0.3);\n\n\nIn 0.7 v.toString() returns following string:\n\n{1:0.1,3:0.3}\n\n\nbut in 0.8 it returns\n\n{1:0.1,3:0.3,}\n\n\nAs you can see, there is extra comma at the end of the string.",
        "Issue Links": []
    },
    "MAHOUT-1302": {
        "Key": "MAHOUT-1302",
        "Summary": "SequenceFilesFromMailArchivesTest.testSequential failing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "05/Aug/13 08:34",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "06/Sep/13 14:50",
        "Description": "SequenceFilesFromMailArchivesTest.testSequential is failing only on ubuntu3 and ubuntu6 Jenkins nodes. Because of that, MahoutQuality and integration job builds either fail or are successful depending on where they get run.\nTest fails because it expects entries in chunk-0 SequenceFile to be in specific order, but that order is not guaranteed because of the way the chunk-0 is created/filled - SequenceFilesFromMailArchives traverses input using Java's\nFile[] java.io.File.listFiles(FileFilter filter)\nwhich does not guarantee order of files/directories.\nUnless we want in SequenceFileIterator to guarantee order by sorting, test needs to be changed to verify presence of given files and their content, but not their exact order.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1334"
        ]
    },
    "MAHOUT-1303": {
        "Key": "MAHOUT-1303",
        "Summary": "GenericItemBasedRecommender.estimate(id) method  confusing (line 347-349)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "chsz wu",
        "Created": "06/Aug/13 23:08",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "18/Oct/13 08:04",
        "Description": "--------\n if (excludeItemIfNotSimilarToAll || !Double.isNaN(estimate)) \n{  -- line 347\n\n          average.addDatum(estimate);\n\n        }\n\n\nline 347 seems logic confusing, \nif excludeItemIfNotSimilarToAll == true, -> always add data into average, doesn't matter what the data is , Nan ???",
        "Issue Links": []
    },
    "MAHOUT-1304": {
        "Key": "MAHOUT-1304",
        "Summary": "Website doesn't fit on 1280 px",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Documentation",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Sebastian Schelter",
        "Created": "07/Aug/13 14:22",
        "Updated": "03/Feb/14 07:49",
        "Resolved": "15/Jan/14 06:40",
        "Description": "Hi,\nsince the latest changes, our website doesn't fit onto 1280 anymore it seems.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1307"
        ]
    },
    "MAHOUT-1305": {
        "Key": "MAHOUT-1305",
        "Summary": "Rework the wiki",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "07/Aug/13 14:27",
        "Updated": "03/Feb/14 13:25",
        "Resolved": "03/Feb/14 13:25",
        "Description": "We should think about completely redoing our wiki. At the moment, we're listing lots of algorithms that we either never implemented or already removed. I also have the impression that a lot of stuff is outdated.\nIt would be awesome if we had an up-to-date documentation of the code with instructions on how to get into using mahout quickly.\nWe should also have examples for all our 3 C's.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1245",
            "/jira/browse/MAHOUT-1381",
            "/jira/browse/MAHOUT-1307"
        ]
    },
    "MAHOUT-1306": {
        "Key": "MAHOUT-1306",
        "Summary": "SSVD-PCA results mangled if -ow (overwrite) is requested",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "08/Aug/13 05:49",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "16/Sep/13 18:07",
        "Description": "Seems some PCA related vectors are wiped by incorrect application of directory cleanup when -ow is given. \nI will also take this opportunity to do some architectural and test cleanup.",
        "Issue Links": []
    },
    "MAHOUT-1307": {
        "Key": "MAHOUT-1307",
        "Summary": "Distinguish implemented algorithms from algorithms which may be implemented in the future in algorithms page",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "yamakatu",
        "Created": "08/Aug/13 17:45",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "16/Jan/14 16:45",
        "Description": "In case of the description of the Mahout algorithms web page,\n(https://cwiki.apache.org/confluence/display/MAHOUT/Algorithms)\nthe algorithms which may be implemented in the future are easy to be confused with the already implemented algorithms,\nand I think that it is difficult to recognize both intuitively.\nI think that both algorithms should be distinguished more clearly.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1305",
            "/jira/browse/MAHOUT-1304"
        ]
    },
    "MAHOUT-1308": {
        "Key": "MAHOUT-1308",
        "Summary": "Cannot extend CandidateItemsStrategy due to restricted visibility",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "David Geiger",
        "Created": "09/Aug/13 10:12",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "22/Nov/13 00:16",
        "Description": "In order to implement a custom CandidateItemsStrategy, I'd like to extend AbstractCandidateItemsStrategy. This is not possible as the visibility modifier of doGetCandidateItems() is package default. Can it be extended to protected?",
        "Issue Links": []
    },
    "MAHOUT-1309": {
        "Key": "MAHOUT-1309",
        "Summary": "Install mahout on windows",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sergey Svinarchuk",
        "Created": "12/Aug/13 15:09",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "28/Feb/14 11:53",
        "Description": "Need create installation script for mahout on Windows and install it.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1310"
        ]
    },
    "MAHOUT-1310": {
        "Key": "MAHOUT-1310",
        "Summary": "Mahout support windows",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Sergey Svinarchuk",
        "Reporter": "Sergey Svinarchuk",
        "Created": "12/Aug/13 15:15",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "28/Apr/14 15:47",
        "Description": "Update mahout for build it on Windows with hadoop 2 and add bin/mahout.cmd",
        "Issue Links": [
            "/jira/browse/MAHOUT-1316",
            "/jira/browse/MAHOUT-1309"
        ]
    },
    "MAHOUT-1311": {
        "Key": "MAHOUT-1311",
        "Summary": "100% system tests pass for mahout on Windows",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sergey Svinarchuk",
        "Created": "12/Aug/13 15:19",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "28/Feb/14 11:54",
        "Description": "Mahout system tests must be 100 passed. And mahout must be work without exception on windows.",
        "Issue Links": []
    },
    "MAHOUT-1312": {
        "Key": "MAHOUT-1312",
        "Summary": "LocalitySensitiveHashSearch.search does not respect search result limit",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "12/Aug/13 22:12",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "30/Nov/13 19:03",
        "Description": "According to documented org.apache.mahout.math.neighborhood.Searcher, public abstract List<WeightedThing<Vector>> search(Vector query, int limit) contract, limit should be the number of results to return.\nLocalitySensitiveHashSearch implements Searcher but does not respect that contract, as it can return more results than the given limit.\nThis issue was encountered while debugging MAHOUT-1302.",
        "Issue Links": []
    },
    "MAHOUT-1313": {
        "Key": "MAHOUT-1313",
        "Summary": "RowSimilarityJob, sampleDown method problem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "sam wu",
        "Created": "13/Aug/13 19:18",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "13/Aug/13 21:32",
        "Description": "RowSimilarityJob.java , sampleDown method \n----line 291 or 300\n double rowSampleRate = Math.min(maxObservationsPerRow, observationsPerRow) / observationsPerRow;\ncurrently always return either 0.0 or 1.0.\nShould return true double, eg: 0.34,...",
        "Issue Links": []
    },
    "MAHOUT-1314": {
        "Key": "MAHOUT-1314",
        "Summary": "StreamingKMeansReducer throws NullPointerException when REDUCE_STREAMING_KMEANS is set to true",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Adi Haviv",
        "Created": "18/Aug/13 14:07",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "18/Nov/13 07:53",
        "Description": "when REDUCE_STREAMING_KMEANS option is set to true (-rskm) the reducer fails with NullPointerException.\nthe problem is in the reduce method itself: on line 60 ( return input.getCentroid(); )\nit should be input.getCentroid().clone();\nsimilar to line 81.\nfull stack trace: \njava.lang.NullPointerException\n\tat com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)\n\tat org.apache.mahout.math.random.WeightedThing.<init>(WeightedThing.java:31)\n\tat org.apache.mahout.math.neighborhood.BruteSearch.searchFirst(BruteSearch.java:133)\n\tat org.apache.mahout.clustering.ClusteringUtils.estimateDistanceCutoff(ClusteringUtils.java:100)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:64)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:66)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:1)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:650)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\nit happens every time the REDUCE_STREAMING_KMEANS is set to true.",
        "Issue Links": []
    },
    "MAHOUT-1315": {
        "Key": "MAHOUT-1315",
        "Summary": "ClusteringUtils is missing Apache license header",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "18/Aug/13 16:29",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "18/Aug/13 17:43",
        "Description": "org.apache.mahout.clustering.ClusteringUtils added for MAHOUT-1162 in Mahout 0.8 is missing Apache license header.",
        "Issue Links": []
    },
    "MAHOUT-1316": {
        "Key": "MAHOUT-1316",
        "Summary": "Create tmp directory in TrainNewsGroups on Windows",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": null,
        "Reporter": "Sergey Svinarchuk",
        "Created": "20/Aug/13 13:35",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "28/Feb/14 11:55",
        "Description": "If mahout on Windows and use TrainNewsGroups need create tmp directory for news-group.model",
        "Issue Links": [
            "/jira/browse/MAHOUT-1310"
        ]
    },
    "MAHOUT-1317": {
        "Key": "MAHOUT-1317",
        "Summary": "Clarify some of the messages in Preconditions.checkArgument",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "BFL",
        "Created": "23/Aug/13 18:05",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "22/Nov/13 20:56",
        "Description": "In experimenting with things, I was getting some errors from RowSimilarityJob, that in looking at the source I realized were a little incomplete as to what the true issue was.  In this case, they were of the form:\nPreconditions.checkArgument(maxSimilaritiesPerRow > 0, \"Incorrect maximum number of similarities per row!\");\nHere, it is known that the actual issue is that the parameter must be zero (or negative), not just that it's \"incorrect\", and a (trivial) change to the error message might save some folks some time... especially newbies like myself.\nA quick grep of the code showed a few more cases like that across the code base that would be (apparently) easy to fix and maybe save folks time when they get the relevant error.",
        "Issue Links": []
    },
    "MAHOUT-1318": {
        "Key": "MAHOUT-1318",
        "Summary": "Make ToItemVectorsReducer public",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Pat Ferrel",
        "Created": "25/Aug/13 17:55",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "27/Aug/13 04:34",
        "Description": "The cross-action and solr recommender need to pass in the class of ToItemVectorsReducer and so it needs to be a public class. I work around by changing it to public in a local build, which I'd rather not maintain.\nSebastian said he'd fix this.",
        "Issue Links": []
    },
    "MAHOUT-1319": {
        "Key": "MAHOUT-1319",
        "Summary": "seqdirectory -filter argument silently ignored when run as MR",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Liz Merkhofer",
        "Created": "26/Aug/13 18:28",
        "Updated": "14/Jul/14 00:56",
        "Resolved": "20/Dec/13 17:19",
        "Description": "Running \"seqdirectory\" (Sequence Files from Input Directory) from the command line and specifying a custom filter using the -filter parameter, the argument is ignored and the default \"PrefixAdditionFilter\" is used on the input. No exception is thrown.\nWhen the same command is run with \"-xm sequential\", the filter is found and works as expected.",
        "Issue Links": []
    },
    "MAHOUT-1320": {
        "Key": "MAHOUT-1320",
        "Summary": "BallKMeansTest.testClustering is unstable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "29/Aug/13 09:07",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "17/Sep/13 18:36",
        "Description": "From time to time this test fails with following in build log:\n\nTests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 48.134 sec <<< FAILURE! - in org.apache.mahout.clustering.streaming.cluster.BallKMeansTest\ntestClustering(org.apache.mahout.clustering.streaming.cluster.BallKMeansTest)  Time elapsed: 2.051 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<625.0> but was:<796.0>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:494)\n\tat org.junit.Assert.assertEquals(Assert.java:592)\n\tat org.apache.mahout.clustering.streaming.cluster.BallKMeansTest.testClustering(BallKMeansTest.java:119)\n\n\nHere is a bit more of build log output, which also shows other tests were running in parallel with this one:\n\n[INFO] --- maven-surefire-plugin:2.15:test (default-test) @ mahout-core ---\n[INFO] Surefire report directory: /home/jenkins/jenkins-slave/workspace/Mahout-Quality/trunk/core/target/surefire-reports\n[INFO] parallel='classes', perCoreThreadCount=false, threadCount=1, useUnlimitedThreads=false\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.common.distance.TestChebyshevMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.043 sec - in org.apache.mahout.common.distance.TestChebyshevMeasure\nRunning org.apache.mahout.common.distance.TestMinkowskiMeasure\nRunning org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure\nRunning org.apache.mahout.common.distance.TestManhattanDistanceMeasure\nRunning org.apache.mahout.common.distance.CosineDistanceMeasureTest\nRunning org.apache.mahout.common.distance.TestTanimotoDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.143 sec - in org.apache.mahout.common.distance.TestMinkowskiMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.078 sec - in org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure\nRunning org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.099 sec - in org.apache.mahout.common.distance.TestManhattanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.075 sec - in org.apache.mahout.common.distance.CosineDistanceMeasureTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.094 sec - in org.apache.mahout.common.distance.TestTanimotoDistanceMeasure\nRunning org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest\nRunning org.apache.mahout.common.distance.TestEuclideanDistanceMeasure\nRunning org.apache.mahout.common.iterator.TestFixedSizeSampler\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.135 sec - in org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure\nRunning org.apache.mahout.common.iterator.CountingIteratorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.009 sec - in org.apache.mahout.common.iterator.CountingIteratorTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.073 sec - in org.apache.mahout.common.iterator.TestFixedSizeSampler\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.111 sec - in org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.121 sec - in org.apache.mahout.common.distance.TestEuclideanDistanceMeasure\nRunning org.apache.mahout.common.iterator.TestSamplingIterator\nRunning org.apache.mahout.common.iterator.TestStableFixedSizeSampler\nRunning org.apache.mahout.common.DummyRecordWriterTest\nRunning org.apache.mahout.common.StringUtilsTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.093 sec - in org.apache.mahout.common.iterator.TestStableFixedSizeSampler\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.113 sec - in org.apache.mahout.common.DummyRecordWriterTest\nRunning org.apache.mahout.common.AbstractJobTest\nRunning org.apache.mahout.common.IntPairWritableTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.02 sec - in org.apache.mahout.common.IntPairWritableTest\nRunning org.apache.mahout.common.lucene.AnalyzerUtilsTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.07 sec - in org.apache.mahout.common.lucene.AnalyzerUtilsTest\nRunning org.apache.mahout.clustering.topdown.PathDirectoryTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in org.apache.mahout.clustering.topdown.PathDirectoryTest\nRunning org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.505 sec - in org.apache.mahout.common.StringUtilsTest\nRunning org.apache.mahout.clustering.classify.ClusterClassificationDriverTest\nRunning org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest\nRunning org.apache.mahout.clustering.spectral.TestVectorCache\nRunning org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob\nRunning org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob\nRunning org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer\nRunning org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob\nRunning org.apache.mahout.clustering.spectral.TestUnitVectorizerJob\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.356 sec - in org.apache.mahout.common.AbstractJobTest\nRunning org.apache.mahout.clustering.canopy.TestCanopyCreation\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.046 sec - in org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.288 sec - in org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.624 sec - in org.apache.mahout.clustering.spectral.TestVectorCache\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.713 sec - in org.apache.mahout.common.iterator.TestSamplingIterator\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.054 sec - in org.apache.mahout.clustering.spectral.TestUnitVectorizerJob\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.518 sec - in org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob\nRunning org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.609 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest\nRunning org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator\nRunning org.apache.mahout.clustering.kmeans.TestKmeansClustering\nRunning org.apache.mahout.clustering.TestGaussianAccumulators\nRunning org.apache.mahout.clustering.iterator.TestClusterClassifier\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.065 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest\nRunning org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering\nRunning org.apache.mahout.clustering.TestClusterInterface\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.203 sec - in org.apache.mahout.clustering.TestClusterInterface\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.746 sec - in org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.897 sec - in org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator\nRunning org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest\nRunning org.apache.mahout.clustering.streaming.cluster.BallKMeansTest\nRunning org.apache.mahout.math.stats.OnlineAucTest\nRunning org.apache.mahout.math.stats.SamplerTest\nRunning org.apache.mahout.math.VarintTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.031 sec - in org.apache.mahout.math.VarintTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.294 sec - in org.apache.mahout.math.stats.SamplerTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.821 sec - in org.apache.mahout.clustering.classify.ClusterClassificationDriverTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest\nRunning org.apache.mahout.math.hadoop.stats.BasicStatsTest\nRunning org.apache.mahout.math.hadoop.TestDistributedRowMatrix\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.175 sec - in org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.366 sec - in org.apache.mahout.clustering.TestGaussianAccumulators\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.578 sec - in org.apache.mahout.math.stats.OnlineAucTest\nTests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.893 sec - in org.apache.mahout.clustering.iterator.TestClusterClassifier\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest\nRunning org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.419 sec - in org.apache.mahout.clustering.canopy.TestCanopyCreation\nRunning org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.33 sec - in org.apache.mahout.math.hadoop.stats.BasicStatsTest\nRunning org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.475 sec - in org.apache.mahout.clustering.kmeans.TestKmeansClustering\nRunning org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.076 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest\nRunning org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest\nTests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.855 sec - in org.apache.mahout.math.hadoop.TestDistributedRowMatrix\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.63 sec - in org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob\nRunning org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI\nRunning org.apache.mahout.math.VectorWritableTest\nTests run: 100, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.046 sec - in org.apache.mahout.math.VectorWritableTest\nRunning org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.541 sec - in org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.658 sec - in org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver\nRunning org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.045 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest\nRunning org.apache.mahout.math.neighborhood.SearchSanityTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.918 sec - in org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering\nRunning org.apache.mahout.math.MatrixWritableTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.944 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.08 sec - in org.apache.mahout.math.MatrixWritableTest\nRunning org.apache.mahout.vectorizer.DocumentProcessorTest\nRunning org.apache.mahout.vectorizer.HighDFWordsPrunerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.631 sec - in org.apache.mahout.vectorizer.HighDFWordsPrunerTest\nRunning org.apache.mahout.math.neighborhood.SearchQualityTest\nRunning org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.062 sec - in org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.581 sec - in org.apache.mahout.vectorizer.DocumentProcessorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.304 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest\nRunning org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.113 sec - in org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.137 sec - in org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.174 sec - in org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.TextValueEncoderTest\nRunning org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.082 sec - in org.apache.mahout.vectorizer.collocations.llr.GramTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.544 sec - in org.apache.mahout.vectorizer.encoders.TextValueEncoderTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.069 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.061 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.552 sec - in org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.369 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.081 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.693 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest\nRunning org.apache.mahout.vectorizer.DictionaryVectorizerTest\nRunning org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest\nTests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 48.134 sec <<< FAILURE! - in org.apache.mahout.clustering.streaming.cluster.BallKMeansTest\ntestClustering(org.apache.mahout.clustering.streaming.cluster.BallKMeansTest)  Time elapsed: 2.051 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<625.0> but was:<796.0>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:494)\n\tat org.junit.Assert.assertEquals(Assert.java:592)\n\tat org.apache.mahout.clustering.streaming.cluster.BallKMeansTest.testClustering(BallKMeansTest.java:119)\n\n\nLast time test failed it was on ubuntu-1 node, but it's also randomly successful on same node so it doesn't seem to be caused by something node specific.",
        "Issue Links": []
    },
    "MAHOUT-1321": {
        "Key": "MAHOUT-1321",
        "Summary": "TestSequenceFilesFromDirectory.testSequenceFileFromDirectoryMapReduce is unstable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "29/Aug/13 22:52",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "18/Sep/13 15:22",
        "Description": "Relevant Mahout-Quality job execution #2216 build output:\n\n[INFO] --- maven-surefire-plugin:2.15:test (default-test) @ mahout-integration ---\n[INFO] Surefire report directory: /home/jenkins/jenkins-slave/workspace/Mahout-Quality/trunk/integration/target/surefire-reports\n[INFO] parallel='classes', perCoreThreadCount=false, threadCount=1, useUnlimitedThreads=false\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.text.LuceneStorageConfigurationTest\nRunning org.apache.mahout.text.LuceneSegmentInputSplitTest\nRunning org.apache.mahout.text.LuceneSegmentInputFormatTest\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest\nRunning org.apache.mahout.text.MailArchivesClusteringAnalyzerTest\nRunning org.apache.mahout.text.LuceneSegmentRecordReaderTest\nRunning org.apache.mahout.text.SequenceFilesFromMailArchivesTest\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.153 sec - in org.apache.mahout.text.MailArchivesClusteringAnalyzerTest\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageTest\nRunning org.apache.mahout.utils.vectors.VectorHelperTest\nRunning org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest\nRunning org.apache.mahout.text.TestSequenceFilesFromDirectory\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.04 sec - in org.apache.mahout.utils.vectors.VectorHelperTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.455 sec - in org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.905 sec - in org.apache.mahout.text.LuceneStorageConfigurationTest\nRunning org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.029 sec - in org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest\nRunning org.apache.mahout.utils.vectors.arff.DriverTest\nRunning org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.194 sec - in org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest\nRunning org.apache.mahout.utils.vectors.arff.ARFFTypeTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.791 sec - in org.apache.mahout.utils.vectors.arff.DriverTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.034 sec - in org.apache.mahout.utils.vectors.arff.ARFFTypeTest\nRunning org.apache.mahout.utils.vectors.lucene.DriverTest\nRunning org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest\nRunning org.apache.mahout.utils.vectors.lucene.LuceneIterableTest\nRunning org.apache.mahout.utils.vectors.io.VectorWriterTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.421 sec - in org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.161 sec - in org.apache.mahout.text.SequenceFilesFromMailArchivesTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.704 sec - in org.apache.mahout.utils.vectors.lucene.LuceneIterableTest\nTests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 3.302 sec <<< FAILURE! - in org.apache.mahout.text.TestSequenceFilesFromDirectory\ntestSequenceFileFromDirectoryMapReduce(org.apache.mahout.text.TestSequenceFilesFromDirectory)  Time elapsed: 3.133 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<1> but was:<0>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.junit.Assert.assertEquals(Assert.java:555)\n\tat org.junit.Assert.assertEquals(Assert.java:542)\n\tat org.apache.mahout.text.TestSequenceFilesFromDirectory.checkMRResultFiles(TestSequenceFilesFromDirectory.java:282)\n\tat org.apache.mahout.text.TestSequenceFilesFromDirectory.testSequenceFileFromDirectoryMapReduce(TestSequenceFilesFromDirectory.java:135)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1325"
        ]
    },
    "MAHOUT-1322": {
        "Key": "MAHOUT-1322",
        "Summary": "TestDistributedRowMatrix.testTranspose is unstable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "30/Aug/13 12:39",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "21/Nov/13 20:49",
        "Description": "Mahout-Quality build job execution #2217 failed:\n\n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ mahout-core ---\n[INFO] Surefire report directory: /x1/jenkins/jenkins-slave/workspace/Mahout-Quality/trunk/core/target/surefire-reports\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.cf.taste.impl.model.GenericItemPreferenceArrayTest\nRunning org.apache.mahout.math.VectorWritableTest\nRunning org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest\nRunning org.apache.mahout.cf.taste.impl.model.BooleanUserPreferenceArrayTest\nRunning org.apache.mahout.cf.taste.impl.model.file.FileIDMigratorTest\nRunning org.apache.mahout.math.MatrixWritableTest\nRunning org.apache.mahout.cf.taste.impl.model.GenericDataModelTest\nRunning org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob\nRunning org.apache.mahout.cf.taste.impl.model.MemoryIDMigratorTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest\nRunning org.apache.mahout.ep.EvolutionaryProcessTest\nRunning org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest\nRunning org.apache.mahout.math.hadoop.TestDistributedRowMatrix\nRunning org.apache.mahout.math.stats.SamplerTest\nRunning org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver\nRunning org.apache.mahout.math.neighborhood.SearchSanityTest\nRunning org.apache.mahout.cf.taste.impl.model.PlusAnonymousConcurrentUserDataModelTest\nRunning org.apache.mahout.math.hadoop.stats.BasicStatsTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.245 sec - in org.apache.mahout.cf.taste.impl.model.GenericItemPreferenceArrayTest\nRunning org.apache.mahout.cf.taste.hadoop.TopItemsQueueTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest\nRunning org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest\nRunning org.apache.mahout.cf.taste.hadoop.item.ToUserVectorsReducerTest\nRunning org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest\nRunning org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI\nRunning org.apache.mahout.math.VarintTest\nRunning org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest\nRunning org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI\nRunning org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest\nRunning org.apache.mahout.math.neighborhood.SearchQualityTest\nRunning org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest\nRunning org.apache.mahout.cf.taste.impl.model.file.FileDataModelTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.33 sec - in org.apache.mahout.cf.taste.impl.model.BooleanUserPreferenceArrayTest\nRunning org.apache.mahout.cf.taste.impl.model.GenericUserPreferenceArrayTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.307 sec - in org.apache.mahout.cf.taste.impl.model.MemoryIDMigratorTest\nRunning org.apache.mahout.cf.taste.impl.model.BooleanItemPreferenceArrayTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.383 sec - in org.apache.mahout.math.MatrixWritableTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.383 sec - in org.apache.mahout.cf.taste.impl.model.GenericDataModelTest\nRunning org.apache.mahout.math.stats.OnlineAucTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.244 sec - in org.apache.mahout.math.VarintTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.373 sec - in org.apache.mahout.math.stats.SamplerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.308 sec - in org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.215 sec - in org.apache.mahout.cf.taste.impl.model.GenericUserPreferenceArrayTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.4 sec - in org.apache.mahout.cf.taste.hadoop.TopItemsQueueTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.21 sec - in org.apache.mahout.cf.taste.impl.model.BooleanItemPreferenceArrayTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.356 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest\nTests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.529 sec - in org.apache.mahout.cf.taste.impl.model.PlusAnonymousConcurrentUserDataModelTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.68 sec - in org.apache.mahout.cf.taste.hadoop.item.ToUserVectorsReducerTest\nRunning org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.CachingRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.FilePersistenceStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.RandomRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.159 sec - in org.apache.mahout.ep.EvolutionaryProcessTest\nRunning org.apache.mahout.cf.taste.impl.recommender.ItemAverageRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.ParallelSGDFactorizerTest\nRunning org.apache.mahout.cf.taste.impl.recommender.ItemUserAverageRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.AllUnknownItemsCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.309 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.FilePersistenceStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.PreferredItemsNeighborhoodCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.558 sec - in org.apache.mahout.cf.taste.impl.recommender.CachingRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.305 sec - in org.apache.mahout.cf.taste.impl.recommender.AllUnknownItemsCandidateItemsStrategyTest\nTests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.872 sec - in org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.554 sec - in org.apache.mahout.cf.taste.impl.recommender.RandomRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.5 sec - in org.apache.mahout.cf.taste.impl.recommender.ItemAverageRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.TopItemsTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.273 sec - in org.apache.mahout.cf.taste.impl.recommender.PreferredItemsNeighborhoodCandidateItemsStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.498 sec - in org.apache.mahout.cf.taste.impl.recommender.ItemUserAverageRecommenderTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.623 sec - in org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommenderTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.425 sec - in org.apache.mahout.cf.taste.impl.recommender.TopItemsTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.399 sec - in org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategyTest\nRunning org.apache.mahout.cf.taste.impl.similarity.file.FileItemSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.recommender.NullRescorerTest\nTests run: 100, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.489 sec - in org.apache.mahout.math.VectorWritableTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.729 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.TanimotoCoefficientSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.SpearmanCorrelationSimilarityTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.251 sec - in org.apache.mahout.cf.taste.impl.recommender.NullRescorerTest\nRunning org.apache.mahout.cf.taste.impl.similarity.AveragingPreferenceInferrerTest\nRunning org.apache.mahout.cf.taste.impl.neighborhood.NearestNNeighborhoodTest\nRunning org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarityTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.498 sec - in org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.neighborhood.ThresholdNeighborhoodTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.377 sec - in org.apache.mahout.cf.taste.impl.similarity.SpearmanCorrelationSimilarityTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.468 sec - in org.apache.mahout.cf.taste.impl.similarity.TanimotoCoefficientSimilarityTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.597 sec - in org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarityTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.489 sec - in org.apache.mahout.cf.taste.impl.neighborhood.NearestNNeighborhoodTest\nRunning org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluatorImplTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.337 sec - in org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarityTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.57 sec - in org.apache.mahout.cf.taste.impl.similarity.AveragingPreferenceInferrerTest\nRunning org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIteratorTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.766 sec - in org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.common.WeightedRunningAverageTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.454 sec - in org.apache.mahout.cf.taste.impl.neighborhood.ThresholdNeighborhoodTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.118 sec - in org.apache.mahout.cf.taste.impl.common.WeightedRunningAverageTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.405 sec - in org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluatorImplTest\nRunning org.apache.mahout.cf.taste.impl.common.FastIDSetTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.33 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizerTest\nRunning org.apache.mahout.cf.taste.impl.common.BitSetTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.148 sec - in org.apache.mahout.cf.taste.impl.common.BitSetTest\nRunning org.apache.mahout.cf.taste.impl.common.RefreshHelperTest\nRunning org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDevTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.662 sec - in org.apache.mahout.cf.taste.impl.model.file.FileDataModelTest\nRunning org.apache.mahout.cf.taste.impl.common.CacheTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.205 sec - in org.apache.mahout.cf.taste.impl.common.RefreshHelperTest\nRunning org.apache.mahout.cf.taste.impl.common.FastByIDMapTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.694 sec - in org.apache.mahout.cf.taste.impl.common.FastIDSetTest\nRunning org.apache.mahout.cf.taste.impl.common.RunningAverageTest\nRunning org.apache.mahout.cf.taste.impl.common.LongPrimitiveArrayIteratorTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.441 sec - in org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDevTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMModelTest\nRunning org.apache.mahout.cf.taste.impl.common.InvertedRunningAverageTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.118 sec - in org.apache.mahout.cf.taste.impl.common.LongPrimitiveArrayIteratorTest\nRunning org.apache.mahout.cf.taste.common.CommonTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.197 sec - in org.apache.mahout.cf.taste.impl.common.RunningAverageTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.611 sec - in org.apache.mahout.cf.taste.impl.model.file.FileIDMigratorTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMUtilsTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.163 sec - in org.apache.mahout.cf.taste.impl.common.InvertedRunningAverageTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.223 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMModelTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMAlgorithmsTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.222 sec - in org.apache.mahout.cf.taste.common.CommonTest\nRunning org.apache.mahout.cf.taste.impl.common.FastMapTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.172 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMAlgorithmsTest\nRunning org.apache.mahout.classifier.sgd.PassiveAggressiveTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.291 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMUtilsTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMTrainerTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.811 sec - in org.apache.mahout.cf.taste.impl.common.FastByIDMapTest\nRunning org.apache.mahout.classifier.sgd.ModelSerializerTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMEvaluatorTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.205 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMTrainerTest\nRunning org.apache.mahout.classifier.sgd.OnlineLogisticRegressionTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.992 sec - in org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIteratorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.301 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMEvaluatorTest\nRunning org.apache.mahout.classifier.sgd.GradientMachineTest\nRunning org.apache.mahout.classifier.sgd.AdaptiveLogisticRegressionTest\nRunning org.apache.mahout.classifier.df.split.RegressionSplitTest\nRunning org.apache.mahout.classifier.sgd.CsvRecordFactoryTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.637 sec - in org.apache.mahout.classifier.sgd.PassiveAggressiveTest\nRunning org.apache.mahout.classifier.df.split.OptIgSplitTest\nRunning org.apache.mahout.classifier.df.split.DefaultIgSplitTest\nRunning org.apache.mahout.classifier.df.data.DatasetTest\nTests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.084 sec - in org.apache.mahout.cf.taste.impl.common.FastMapTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.444 sec - in org.apache.mahout.math.stats.OnlineAucTest\nRunning org.apache.mahout.classifier.df.data.DescriptorUtilsTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.471 sec - in org.apache.mahout.classifier.sgd.CsvRecordFactoryTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.643 sec - in org.apache.mahout.classifier.sgd.GradientMachineTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.936 sec - in org.apache.mahout.cf.taste.impl.common.CacheTest\nRunning org.apache.mahout.classifier.df.data.DataTest\nRunning org.apache.mahout.classifier.df.data.DataLoaderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.24 sec - in org.apache.mahout.classifier.df.data.DescriptorUtilsTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.933 sec - in org.apache.mahout.classifier.df.split.RegressionSplitTest\nRunning org.apache.mahout.classifier.df.data.DataConverterTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.26 sec - in org.apache.mahout.cf.taste.impl.similarity.file.FileItemSimilarityTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.Step1MapperTest\nRunning org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputSplitTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.TreeIDTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.053 sec - in org.apache.mahout.classifier.df.split.DefaultIgSplitTest\nRunning org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputFormatTest\nRunning org.apache.mahout.classifier.df.builder.DefaultTreeBuilderTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest\nRunning org.apache.mahout.classifier.df.builder.DecisionTreeBuilderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.036 sec - in org.apache.mahout.classifier.df.data.DatasetTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.331 sec - in org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputSplitTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.41 sec - in org.apache.mahout.classifier.df.split.OptIgSplitTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.033 sec - in org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.295 sec - in org.apache.mahout.classifier.df.builder.DefaultTreeBuilderTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.124 sec - in org.apache.mahout.classifier.sgd.ModelSerializerTest\nRunning org.apache.mahout.classifier.df.builder.InfiniteRecursionTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.362 sec - in org.apache.mahout.classifier.df.builder.DecisionTreeBuilderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.933 sec - in org.apache.mahout.classifier.df.data.DataConverterTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.511 sec - in org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputFormatTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.643 sec - in org.apache.mahout.classifier.df.mapreduce.partial.TreeIDTest\nRunning org.apache.mahout.classifier.df.DecisionForestTest\nRunning org.apache.mahout.classifier.df.node.NodeTest\nRunning org.apache.mahout.classifier.RegressionResultAnalyzerTest\nRunning org.apache.mahout.classifier.df.tools.VisualizerTest\nRunning org.apache.mahout.classifier.naivebayes.NaiveBayesModelTest\nRunning org.apache.mahout.classifier.ConfusionMatrixTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.931 sec - in org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest\nRunning org.apache.mahout.classifier.naivebayes.NaiveBayesTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.261 sec - in org.apache.mahout.classifier.df.node.NodeTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.668 sec - in org.apache.mahout.classifier.df.builder.InfiniteRecursionTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.216 sec - in org.apache.mahout.classifier.RegressionResultAnalyzerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.226 sec - in org.apache.mahout.classifier.df.mapreduce.partial.Step1MapperTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.283 sec - in org.apache.mahout.classifier.naivebayes.NaiveBayesModelTest\nRunning org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifierTest\nRunning org.apache.mahout.classifier.naivebayes.training.ThetaMapperTest\nRunning org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifierTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.293 sec - in org.apache.mahout.classifier.ConfusionMatrixTest\nRunning org.apache.mahout.classifier.naivebayes.training.WeightsMapperTest\nRunning org.apache.mahout.classifier.naivebayes.training.IndexInstancesMapperTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.936 sec - in org.apache.mahout.classifier.df.data.DataTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.213 sec - in org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifierTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.243 sec - in org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifierTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.88 sec - in org.apache.mahout.classifier.df.DecisionForestTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.657 sec - in org.apache.mahout.classifier.df.tools.VisualizerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.382 sec - in org.apache.mahout.classifier.naivebayes.training.ThetaMapperTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.283 sec - in org.apache.mahout.classifier.df.data.DataLoaderTest\nRunning org.apache.mahout.clustering.streaming.cluster.BallKMeansTest\nRunning org.apache.mahout.clustering.kmeans.TestKmeansClustering\nRunning org.apache.mahout.classifier.evaluation.AucTest\nRunning org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator\nRunning org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest\nRunning org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.614 sec - in org.apache.mahout.classifier.naivebayes.training.IndexInstancesMapperTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.679 sec - in org.apache.mahout.classifier.naivebayes.training.WeightsMapperTest\nRunning org.apache.mahout.clustering.spectral.TestUnitVectorizerJob\nRunning org.apache.mahout.clustering.TestGaussianAccumulators\nRunning org.apache.mahout.clustering.spectral.TestVectorCache\nRunning org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer\nRunning org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob\nRunning org.apache.mahout.clustering.TestClusterInterface\nRunning org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.328 sec - in org.apache.mahout.clustering.TestClusterInterface\nRunning org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob\nRunning org.apache.mahout.clustering.topdown.PathDirectoryTest\nRunning org.apache.mahout.clustering.iterator.TestClusterClassifier\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.972 sec - in org.apache.mahout.clustering.spectral.TestUnitVectorizerJob\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.17 sec - in org.apache.mahout.classifier.evaluation.AucTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.288 sec - in org.apache.mahout.clustering.topdown.PathDirectoryTest\nRunning org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.73 sec - in org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.721 sec - in org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.463 sec - in org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob\nRunning org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest\nRunning org.apache.mahout.clustering.classify.ClusterClassificationDriverTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.808 sec - in org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob\nRunning org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.392 sec - in org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.076 sec - in org.apache.mahout.clustering.spectral.TestVectorCache\nRunning org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest\nRunning org.apache.mahout.clustering.canopy.TestCanopyCreation\nRunning org.apache.mahout.vectorizer.encoders.TextValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.394 sec - in org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.277 sec - in org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.293 sec - in org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.497 sec - in org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.304 sec - in org.apache.mahout.vectorizer.encoders.TextValueEncoderTest\nRunning org.apache.mahout.vectorizer.DocumentProcessorTest\nRunning org.apache.mahout.vectorizer.HighDFWordsPrunerTest\nRunning org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.023 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.798 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest\nRunning org.apache.mahout.vectorizer.DictionaryVectorizerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.316 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.397 sec - in org.apache.mahout.vectorizer.collocations.llr.GramTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.921 sec - in org.apache.mahout.vectorizer.HighDFWordsPrunerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.268 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyTest\nRunning org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.306 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.815 sec - in org.apache.mahout.clustering.TestGaussianAccumulators\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.199 sec - in org.apache.mahout.classifier.sgd.AdaptiveLogisticRegressionTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.144 sec - in org.apache.mahout.clustering.classify.ClusterClassificationDriverTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.187 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.786 sec - in org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.132 sec - in org.apache.mahout.vectorizer.DocumentProcessorTest\nRunning org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest\nRunning org.apache.mahout.common.StringUtilsTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.086 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest\nRunning org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure\nRunning org.apache.mahout.common.distance.TestChebyshevMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.702 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.385 sec - in org.apache.mahout.math.hadoop.stats.BasicStatsTest\nRunning org.apache.mahout.common.distance.TestManhattanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.218 sec - in org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.211 sec - in org.apache.mahout.common.distance.TestChebyshevMeasure\nRunning org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest\nRunning org.apache.mahout.common.distance.CosineDistanceMeasureTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.466 sec - in org.apache.mahout.common.StringUtilsTest\nRunning org.apache.mahout.common.distance.TestEuclideanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.162 sec - in org.apache.mahout.common.distance.CosineDistanceMeasureTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.215 sec - in org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.295 sec - in org.apache.mahout.common.distance.TestManhattanDistanceMeasure\nRunning org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure\nRunning org.apache.mahout.common.distance.TestMinkowskiMeasure\nRunning org.apache.mahout.common.distance.TestTanimotoDistanceMeasure\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.221 sec - in org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.301 sec - in org.apache.mahout.common.distance.TestEuclideanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.194 sec - in org.apache.mahout.common.distance.TestMinkowskiMeasure\nRunning org.apache.mahout.common.lucene.AnalyzerUtilsTest\nRunning org.apache.mahout.common.DummyRecordWriterTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.277 sec - in org.apache.mahout.common.distance.TestTanimotoDistanceMeasure\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.614 sec - in org.apache.mahout.classifier.naivebayes.NaiveBayesTest\nRunning org.apache.mahout.common.IntPairWritableTest\nRunning org.apache.mahout.common.iterator.TestSamplingIterator\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.234 sec - in org.apache.mahout.common.lucene.AnalyzerUtilsTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.247 sec - in org.apache.mahout.common.DummyRecordWriterTest\nRunning org.apache.mahout.common.iterator.TestStableFixedSizeSampler\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.217 sec - in org.apache.mahout.common.IntPairWritableTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.163 sec - in org.apache.mahout.common.iterator.TestStableFixedSizeSampler\nRunning org.apache.mahout.common.iterator.TestFixedSizeSampler\nRunning org.apache.mahout.common.iterator.CountingIteratorTest\nRunning org.apache.mahout.common.AbstractJobTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.157 sec - in org.apache.mahout.common.iterator.CountingIteratorTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.19 sec - in org.apache.mahout.common.iterator.TestFixedSizeSampler\nRunning org.apache.mahout.driver.MahoutDriverTest\nTests run: 11, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 18.217 sec <<< FAILURE! - in org.apache.mahout.math.hadoop.TestDistributedRowMatrix\ntestTranspose(org.apache.mahout.math.hadoop.TestDistributedRowMatrix)  Time elapsed: 1.37 sec  <<< ERROR!\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/mahout-TestDistributedRowMatrix-4563572172465515520/testdata/transpose-44 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)\n\tat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:975)\n\tat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:910)\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1353)\n\tat org.apache.mahout.math.hadoop.DistributedRowMatrix.transpose(DistributedRowMatrix.java:238)\n\tat org.apache.mahout.math.hadoop.TestDistributedRowMatrix.testTranspose(TestDistributedRowMatrix.java:87)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1325"
        ]
    },
    "MAHOUT-1323": {
        "Key": "MAHOUT-1323",
        "Summary": "ParallelALSFactorizationJobTest.completeJobToyExample is unstable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "02/Sep/13 22:22",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "21/Nov/13 20:46",
        "Description": "Mahout-Quality build #2224 failed because of this test.\nRelevant build log:\n\n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ mahout-core ---\n[INFO] Surefire report directory: /x1/jenkins/jenkins-slave/workspace/Mahout-Quality/trunk/core/target/surefire-reports\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.math.VarintTest\nRunning org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest\nRunning org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver\nRunning org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI\nRunning org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest\nRunning org.apache.mahout.math.hadoop.TestDistributedRowMatrix\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest\nRunning org.apache.mahout.math.VectorWritableTest\nRunning org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest\nRunning org.apache.mahout.cf.taste.impl.model.file.FileIDMigratorTest\nRunning org.apache.mahout.math.neighborhood.SearchQualityTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest\nRunning org.apache.mahout.cf.taste.impl.model.MemoryIDMigratorTest\nRunning org.apache.mahout.math.hadoop.stats.BasicStatsTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.09 sec - in org.apache.mahout.math.VarintTest\nRunning org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest\nRunning org.apache.mahout.ep.EvolutionaryProcessTest\nRunning org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest\nRunning org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob\nRunning org.apache.mahout.math.neighborhood.SearchSanityTest\nRunning org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI\nRunning org.apache.mahout.math.stats.SamplerTest\nRunning org.apache.mahout.math.MatrixWritableTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest\nRunning org.apache.mahout.cf.taste.hadoop.item.ToUserVectorsReducerTest\nRunning org.apache.mahout.cf.taste.impl.model.PlusAnonymousConcurrentUserDataModelTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.211 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest\nRunning org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest\nRunning org.apache.mahout.cf.taste.impl.model.GenericItemPreferenceArrayTest\nRunning org.apache.mahout.cf.taste.impl.model.BooleanItemPreferenceArrayTest\nRunning org.apache.mahout.math.stats.OnlineAucTest\nRunning org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.317 sec - in org.apache.mahout.cf.taste.impl.model.MemoryIDMigratorTest\nRunning org.apache.mahout.cf.taste.impl.model.GenericDataModelTest\nRunning org.apache.mahout.cf.taste.impl.model.GenericUserPreferenceArrayTest\nRunning org.apache.mahout.cf.taste.impl.model.file.FileDataModelTest\nTests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.297 sec - in org.apache.mahout.cf.taste.impl.model.PlusAnonymousConcurrentUserDataModelTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.429 sec - in org.apache.mahout.math.stats.SamplerTest\nRunning org.apache.mahout.cf.taste.hadoop.TopItemsQueueTest\nRunning org.apache.mahout.cf.taste.impl.model.BooleanUserPreferenceArrayTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.315 sec - in org.apache.mahout.cf.taste.impl.model.BooleanItemPreferenceArrayTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.469 sec - in org.apache.mahout.math.MatrixWritableTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.337 sec - in org.apache.mahout.cf.taste.impl.model.GenericItemPreferenceArrayTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.469 sec - in org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.341 sec - in org.apache.mahout.cf.taste.impl.model.GenericUserPreferenceArrayTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.201 sec - in org.apache.mahout.cf.taste.impl.model.BooleanUserPreferenceArrayTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.244 sec - in org.apache.mahout.cf.taste.hadoop.TopItemsQueueTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.559 sec - in org.apache.mahout.cf.taste.impl.model.GenericDataModelTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.81 sec - in org.apache.mahout.cf.taste.hadoop.item.ToUserVectorsReducerTest\nRunning org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.ItemUserAverageRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.FilePersistenceStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.ParallelSGDFactorizerTest\nRunning org.apache.mahout.cf.taste.impl.recommender.CachingRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.346 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.FilePersistenceStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.ItemAverageRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.AllUnknownItemsCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.539 sec - in org.apache.mahout.cf.taste.impl.recommender.ItemUserAverageRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.652 sec - in org.apache.mahout.ep.EvolutionaryProcessTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.327 sec - in org.apache.mahout.cf.taste.impl.recommender.AllUnknownItemsCandidateItemsStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.43 sec - in org.apache.mahout.cf.taste.impl.recommender.ItemAverageRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.RandomRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.PreferredItemsNeighborhoodCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.853 sec - in org.apache.mahout.cf.taste.impl.recommender.CachingRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.TopItemsTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.477 sec - in org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommenderTest\nTests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.268 sec - in org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.286 sec - in org.apache.mahout.cf.taste.impl.recommender.PreferredItemsNeighborhoodCandidateItemsStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.NullRescorerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.621 sec - in org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.487 sec - in org.apache.mahout.cf.taste.impl.recommender.RandomRecommenderTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.452 sec - in org.apache.mahout.cf.taste.impl.recommender.TopItemsTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.854 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommenderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.203 sec - in org.apache.mahout.cf.taste.impl.recommender.NullRescorerTest\nRunning org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.file.FileItemSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarityTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.179 sec - in org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.AveragingPreferenceInferrerTest\nTests run: 100, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.897 sec - in org.apache.mahout.math.VectorWritableTest\nRunning org.apache.mahout.cf.taste.impl.similarity.TanimotoCoefficientSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.SpearmanCorrelationSimilarityTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.455 sec - in org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarityTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.481 sec - in org.apache.mahout.cf.taste.impl.similarity.AveragingPreferenceInferrerTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.324 sec - in org.apache.mahout.cf.taste.impl.similarity.SpearmanCorrelationSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.common.WeightedRunningAverageTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.559 sec - in org.apache.mahout.cf.taste.impl.similarity.TanimotoCoefficientSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.neighborhood.NearestNNeighborhoodTest\nRunning org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluatorImplTest\nRunning org.apache.mahout.cf.taste.impl.neighborhood.ThresholdNeighborhoodTest\nRunning org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIteratorTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.426 sec - in org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarityTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.265 sec - in org.apache.mahout.cf.taste.impl.common.WeightedRunningAverageTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.264 sec - in org.apache.mahout.cf.taste.impl.neighborhood.ThresholdNeighborhoodTest\nRunning org.apache.mahout.cf.taste.impl.common.BitSetTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.793 sec - in org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarityTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.312 sec - in org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluatorImplTest\nRunning org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDevTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.086 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.399 sec - in org.apache.mahout.cf.taste.impl.neighborhood.NearestNNeighborhoodTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.188 sec - in org.apache.mahout.cf.taste.impl.common.BitSetTest\nRunning org.apache.mahout.cf.taste.impl.common.FastIDSetTest\nRunning org.apache.mahout.cf.taste.impl.common.CacheTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.299 sec - in org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDevTest\nRunning org.apache.mahout.cf.taste.impl.common.RefreshHelperTest\nRunning org.apache.mahout.cf.taste.impl.common.RunningAverageTest\nRunning org.apache.mahout.cf.taste.impl.common.FastByIDMapTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.9 sec - in org.apache.mahout.cf.taste.impl.model.file.FileDataModelTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMModelTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.441 sec - in org.apache.mahout.cf.taste.impl.model.file.FileIDMigratorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.146 sec - in org.apache.mahout.cf.taste.impl.common.RunningAverageTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.351 sec - in org.apache.mahout.cf.taste.impl.common.RefreshHelperTest\nRunning org.apache.mahout.cf.taste.impl.common.LongPrimitiveArrayIteratorTest\nRunning org.apache.mahout.cf.taste.impl.common.FastMapTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.125 sec - in org.apache.mahout.cf.taste.impl.common.LongPrimitiveArrayIteratorTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMAlgorithmsTest\nRunning org.apache.mahout.cf.taste.impl.common.InvertedRunningAverageTest\nRunning org.apache.mahout.cf.taste.common.CommonTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMTrainerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.483 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMModelTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.172 sec - in org.apache.mahout.cf.taste.impl.common.InvertedRunningAverageTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMUtilsTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.218 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMAlgorithmsTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.992 sec - in org.apache.mahout.cf.taste.impl.common.FastIDSetTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.289 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMTrainerTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.222 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMUtilsTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.365 sec - in org.apache.mahout.cf.taste.common.CommonTest\nRunning org.apache.mahout.classifier.sgd.OnlineLogisticRegressionTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMEvaluatorTest\nRunning org.apache.mahout.classifier.sgd.AdaptiveLogisticRegressionTest\nRunning org.apache.mahout.classifier.sgd.PassiveAggressiveTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.781 sec - in org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIteratorTest\nRunning org.apache.mahout.classifier.sgd.ModelSerializerTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.072 sec - in org.apache.mahout.cf.taste.impl.common.FastByIDMapTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.173 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMEvaluatorTest\nRunning org.apache.mahout.classifier.sgd.CsvRecordFactoryTest\nTests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.072 sec - in org.apache.mahout.cf.taste.impl.common.FastMapTest\nRunning org.apache.mahout.classifier.sgd.GradientMachineTest\nRunning org.apache.mahout.classifier.df.split.DefaultIgSplitTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.502 sec - in org.apache.mahout.classifier.sgd.PassiveAggressiveTest\nRunning org.apache.mahout.classifier.df.data.DatasetTest\nRunning org.apache.mahout.classifier.df.split.RegressionSplitTest\nRunning org.apache.mahout.classifier.df.split.OptIgSplitTest\nRunning org.apache.mahout.classifier.df.data.DescriptorUtilsTest\nRunning org.apache.mahout.classifier.df.data.DataLoaderTest\nRunning org.apache.mahout.classifier.df.data.DataConverterTest\nRunning org.apache.mahout.classifier.df.data.DataTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.237 sec - in org.apache.mahout.classifier.df.data.DescriptorUtilsTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.532 sec - in org.apache.mahout.classifier.sgd.GradientMachineTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.667 sec - in org.apache.mahout.classifier.sgd.CsvRecordFactoryTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.383 sec - in org.apache.mahout.cf.taste.impl.common.CacheTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.Step1MapperTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.TreeIDTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.872 sec - in org.apache.mahout.classifier.df.split.RegressionSplitTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.916 sec - in org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob\nRunning org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputSplitTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.248 sec - in org.apache.mahout.classifier.df.split.DefaultIgSplitTest\nRunning org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputFormatTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.2 sec - in org.apache.mahout.classifier.df.data.DatasetTest\nRunning org.apache.mahout.classifier.df.builder.DecisionTreeBuilderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.039 sec - in org.apache.mahout.classifier.df.data.DataConverterTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.208 sec - in org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputSplitTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.292 sec - in org.apache.mahout.classifier.df.split.OptIgSplitTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.648 sec - in org.apache.mahout.classifier.df.mapreduce.partial.TreeIDTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.047 sec - in org.apache.mahout.math.stats.OnlineAucTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.639 sec - in org.apache.mahout.cf.taste.impl.similarity.file.FileItemSimilarityTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.215 sec - in org.apache.mahout.classifier.df.builder.DecisionTreeBuilderTest\nRunning org.apache.mahout.classifier.df.builder.DefaultTreeBuilderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.536 sec - in org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputFormatTest\nRunning org.apache.mahout.classifier.df.node.NodeTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.259 sec - in org.apache.mahout.classifier.sgd.ModelSerializerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.223 sec - in org.apache.mahout.classifier.df.builder.DefaultTreeBuilderTest\nRunning org.apache.mahout.classifier.df.builder.InfiniteRecursionTest\nRunning org.apache.mahout.classifier.df.tools.VisualizerTest\nRunning org.apache.mahout.classifier.RegressionResultAnalyzerTest\nRunning org.apache.mahout.classifier.df.DecisionForestTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.403 sec - in org.apache.mahout.classifier.df.mapreduce.partial.Step1MapperTest\nRunning org.apache.mahout.classifier.ConfusionMatrixTest\nRunning org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifierTest\nRunning org.apache.mahout.classifier.naivebayes.NaiveBayesTest\nRunning org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifierTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.329 sec - in org.apache.mahout.classifier.df.node.NodeTest\nRunning org.apache.mahout.classifier.naivebayes.NaiveBayesModelTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.226 sec - in org.apache.mahout.classifier.RegressionResultAnalyzerTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.938 sec - in org.apache.mahout.classifier.df.data.DataTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.295 sec - in org.apache.mahout.classifier.ConfusionMatrixTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.286 sec - in org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifierTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.264 sec - in org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifierTest\nRunning org.apache.mahout.classifier.naivebayes.training.WeightsMapperTest\nRunning org.apache.mahout.classifier.naivebayes.training.IndexInstancesMapperTest\nRunning org.apache.mahout.classifier.naivebayes.training.ThetaMapperTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.344 sec - in org.apache.mahout.classifier.naivebayes.NaiveBayesModelTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.639 sec - in org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest\nRunning org.apache.mahout.clustering.kmeans.TestKmeansClustering\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.8 sec - in org.apache.mahout.classifier.df.DecisionForestTest\nRunning org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator\nRunning org.apache.mahout.clustering.TestClusterInterface\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.009 sec - in org.apache.mahout.classifier.df.builder.InfiniteRecursionTest\nRunning org.apache.mahout.classifier.evaluation.AucTest\nRunning org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.769 sec - in org.apache.mahout.classifier.df.data.DataLoaderTest\nRunning org.apache.mahout.clustering.streaming.cluster.BallKMeansTest\nRunning org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.708 sec - in org.apache.mahout.classifier.naivebayes.training.WeightsMapperTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.678 sec - in org.apache.mahout.classifier.naivebayes.training.IndexInstancesMapperTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.283 sec - in org.apache.mahout.clustering.TestClusterInterface\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.264 sec - in org.apache.mahout.classifier.df.tools.VisualizerTest\nRunning org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer\nRunning org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.957 sec - in org.apache.mahout.classifier.naivebayes.training.ThetaMapperTest\nRunning org.apache.mahout.clustering.spectral.TestVectorCache\nRunning org.apache.mahout.clustering.TestGaussianAccumulators\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.854 sec - in org.apache.mahout.classifier.evaluation.AucTest\nRunning org.apache.mahout.clustering.spectral.TestUnitVectorizerJob\nRunning org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob\nRunning org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob\nRunning org.apache.mahout.clustering.iterator.TestClusterClassifier\nRunning org.apache.mahout.clustering.topdown.PathDirectoryTest\nRunning org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.502 sec - in org.apache.mahout.clustering.topdown.PathDirectoryTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.955 sec - in org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.579 sec - in org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.162 sec - in org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.237 sec - in org.apache.mahout.clustering.spectral.TestUnitVectorizerJob\nRunning org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.236 sec - in org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.559 sec - in org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.05 sec - in org.apache.mahout.clustering.spectral.TestVectorCache\nRunning org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering\nRunning org.apache.mahout.clustering.classify.ClusterClassificationDriverTest\nRunning org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest\nRunning org.apache.mahout.clustering.canopy.TestCanopyCreation\nRunning org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.TextValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.355 sec - in org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.565 sec - in org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.733 sec - in org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest\nRunning org.apache.mahout.vectorizer.HighDFWordsPrunerTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.964 sec - in org.apache.mahout.vectorizer.encoders.TextValueEncoderTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.225 sec - in org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest\nRunning org.apache.mahout.vectorizer.DocumentProcessorTest\nRunning org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.903 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest\nRunning org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest\nRunning org.apache.mahout.vectorizer.DictionaryVectorizerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.716 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.341 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.255 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.462 sec - in org.apache.mahout.vectorizer.HighDFWordsPrunerTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.523 sec - in org.apache.mahout.vectorizer.collocations.llr.GramTest\nRunning org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.285 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.789 sec - in org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.259 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.443 sec - in org.apache.mahout.classifier.sgd.AdaptiveLogisticRegressionTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.796 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.491 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest\nRunning org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.474 sec - in org.apache.mahout.vectorizer.DocumentProcessorTest\nRunning org.apache.mahout.common.StringUtilsTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.035 sec - in org.apache.mahout.math.hadoop.stats.BasicStatsTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.665 sec - in org.apache.mahout.clustering.TestGaussianAccumulators\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.392 sec - in org.apache.mahout.clustering.classify.ClusterClassificationDriverTest\nRunning org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure\nRunning org.apache.mahout.common.distance.TestManhattanDistanceMeasure\nRunning org.apache.mahout.common.distance.TestChebyshevMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.262 sec - in org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure\nRunning org.apache.mahout.common.distance.CosineDistanceMeasureTest\nRunning org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest\nRunning org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.272 sec - in org.apache.mahout.common.distance.TestManhattanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.274 sec - in org.apache.mahout.common.distance.TestChebyshevMeasure\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.682 sec - in org.apache.mahout.classifier.naivebayes.NaiveBayesTest\nRunning org.apache.mahout.common.distance.TestMinkowskiMeasure\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.561 sec - in org.apache.mahout.common.StringUtilsTest\nRunning org.apache.mahout.common.distance.TestEuclideanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.203 sec - in org.apache.mahout.common.distance.CosineDistanceMeasureTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.261 sec - in org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.205 sec - in org.apache.mahout.common.distance.TestMinkowskiMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.314 sec - in org.apache.mahout.common.distance.TestEuclideanDistanceMeasure\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.387 sec - in org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure\nRunning org.apache.mahout.common.distance.TestTanimotoDistanceMeasure\nRunning org.apache.mahout.common.DummyRecordWriterTest\nRunning org.apache.mahout.common.lucene.AnalyzerUtilsTest\nRunning org.apache.mahout.common.IntPairWritableTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.263 sec - in org.apache.mahout.common.distance.TestTanimotoDistanceMeasure\nRunning org.apache.mahout.common.iterator.TestStableFixedSizeSampler\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.204 sec - in org.apache.mahout.common.DummyRecordWriterTest\nRunning org.apache.mahout.common.iterator.TestFixedSizeSampler\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.135 sec - in org.apache.mahout.common.IntPairWritableTest\nRunning org.apache.mahout.common.iterator.CountingIteratorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.277 sec - in org.apache.mahout.common.lucene.AnalyzerUtilsTest\nRunning org.apache.mahout.common.iterator.TestSamplingIterator\nRunning org.apache.mahout.common.AbstractJobTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.27 sec - in org.apache.mahout.common.iterator.TestFixedSizeSampler\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.293 sec - in org.apache.mahout.common.iterator.TestStableFixedSizeSampler\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.259 sec - in org.apache.mahout.common.iterator.CountingIteratorTest\nRunning org.apache.mahout.driver.MahoutDriverTest\nTests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.239 sec - in org.apache.mahout.clustering.iterator.TestClusterClassifier\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.54 sec - in org.apache.mahout.driver.MahoutDriverTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.926 sec - in org.apache.mahout.common.AbstractJobTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.321 sec - in org.apache.mahout.common.iterator.TestSamplingIterator\nTests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.886 sec - in org.apache.mahout.math.hadoop.TestDistributedRowMatrix\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.864 sec - in org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 20.716 sec - in org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.076 sec - in org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.249 sec - in org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.486 sec - in org.apache.mahout.clustering.canopy.TestCanopyCreation\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.179 sec - in org.apache.mahout.classifier.sgd.OnlineLogisticRegressionTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 26.884 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 21.12 sec - in org.apache.mahout.clustering.kmeans.TestKmeansClustering\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.232 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest\nTests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.159 sec - in org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 37.022 sec - in org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.79 sec - in org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 40.81 sec - in org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.451 sec - in org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 47.251 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 33.506 sec - in org.apache.mahout.vectorizer.DictionaryVectorizerTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 42.832 sec - in org.apache.mahout.clustering.streaming.cluster.BallKMeansTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 56.141 sec - in org.apache.mahout.math.neighborhood.SearchSanityTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 55.249 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.ParallelSGDFactorizerTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 58.046 sec - in org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 60.057 sec - in org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 73.216 sec - in org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer\nTests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 96.559 sec <<< FAILURE! - in org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest\ncompleteJobToyExample(org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest)  Time elapsed: 22.164 sec  <<< ERROR!\njava.lang.IllegalStateException: Job failed!\n\tat org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.runSolver(ParallelALSFactorizationJob.java:351)\n\tat org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.run(ParallelALSFactorizationJob.java:214)\n\tat org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.explicitExample(ParallelALSFactorizationJobTest.java:111)\n\tat org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.completeJobToyExample(ParallelALSFactorizationJobTest.java:70)\n\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 101.111 sec - in org.apache.mahout.math.neighborhood.SearchQualityTest\n\nResults :\n\nTests in error: \n  ParallelALSFactorizationJobTest.completeJobToyExample:70->explicitExample:111 ? IllegalState\n\nTests run: 734, Failures: 0, Errors: 1, Skipped: 0\n\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Mahout Build Tools ................................ SUCCESS [5.637s]\n[INFO] Apache Mahout ..................................... SUCCESS [1.875s]\n[INFO] Mahout Math ....................................... SUCCESS [1:18.085s]\n[INFO] Mahout Core ....................................... FAILURE [1:47.266s]\n[INFO] Mahout Integration ................................ SKIPPED\n[INFO] Mahout Examples ................................... SKIPPED\n[INFO] Mahout Release Package ............................ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 3:14.686s\n[INFO] Finished at: Mon Sep 02 22:07:54 UTC 2013\n[INFO] Final Memory: 58M/454M\n[INFO] ------------------------------------------------------------------------",
        "Issue Links": [
            "/jira/browse/MAHOUT-1325"
        ]
    },
    "MAHOUT-1324": {
        "Key": "MAHOUT-1324",
        "Summary": "SequenceFilesFromLuceneStorageMRJobTest.testRun is unstable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "02/Sep/13 22:29",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "18/Sep/13 15:20",
        "Description": "Mahout-Quality build job execution #2223 failed because of this test.\nRelevant build log output:\n\n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ mahout-integration ---\n[INFO] Surefire report directory: /home/hudson/jenkins-slave/workspace/Mahout-Quality/trunk/integration/target/surefire-reports\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest\nRunning org.apache.mahout.utils.vectors.arff.DriverTest\nRunning org.apache.mahout.utils.SplitInputTest\nRunning org.apache.mahout.utils.vectors.lucene.DriverTest\nRunning org.apache.mahout.utils.regex.RegexUtilsTest\nRunning org.apache.mahout.utils.regex.RegexMapperTest\nRunning org.apache.mahout.utils.email.MailProcessorTest\nRunning org.apache.mahout.utils.Bump125Test\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.164 sec - in org.apache.mahout.utils.regex.RegexUtilsTest\nRunning org.apache.mahout.utils.TestConcatenateVectorsJob\nRunning org.apache.mahout.utils.vectors.lucene.LuceneIterableTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.374 sec - in org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.219 sec - in org.apache.mahout.utils.Bump125Test\nRunning org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest\nRunning org.apache.mahout.utils.vectors.VectorHelperTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.437 sec - in org.apache.mahout.utils.email.MailProcessorTest\nRunning org.apache.mahout.utils.vectors.arff.ARFFTypeTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.364 sec - in org.apache.mahout.utils.vectors.VectorHelperTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.827 sec - in org.apache.mahout.utils.vectors.arff.DriverTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.211 sec - in org.apache.mahout.utils.vectors.arff.ARFFTypeTest\nRunning org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest\nRunning org.apache.mahout.utils.vectors.io.VectorWriterTest\nRunning org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.182 sec - in org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.296 sec - in org.apache.mahout.utils.regex.RegexMapperTest\nRunning org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.317 sec - in org.apache.mahout.utils.TestConcatenateVectorsJob\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.191 sec - in org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.492 sec - in org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest\nRunning org.apache.mahout.text.LuceneSegmentInputSplitTest\nRunning org.apache.mahout.text.LuceneSegmentRecordReaderTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.635 sec - in org.apache.mahout.utils.vectors.lucene.LuceneIterableTest\nRunning org.apache.mahout.text.MailArchivesClusteringAnalyzerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.649 sec - in org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.342 sec - in org.apache.mahout.text.MailArchivesClusteringAnalyzerTest\nRunning org.apache.mahout.text.LuceneSegmentInputFormatTest\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest\nRunning org.apache.mahout.text.TestSequenceFilesFromDirectory\nRunning org.apache.mahout.text.LuceneStorageConfigurationTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.64 sec - in org.apache.mahout.utils.vectors.io.VectorWriterTest\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest\nRunning org.apache.mahout.text.SequenceFilesFromMailArchivesTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.031 sec - in org.apache.mahout.text.LuceneStorageConfigurationTest\nRunning org.apache.mahout.cf.taste.impl.similarity.jdbc.MySQLJDBCInMemoryItemSimilarityTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.94 sec - in org.apache.mahout.utils.vectors.lucene.DriverTest\nRunning org.apache.mahout.clustering.cdbw.TestCDbwEvaluator\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.778 sec - in org.apache.mahout.cf.taste.impl.similarity.jdbc.MySQLJDBCInMemoryItemSimilarityTest\nRunning org.apache.mahout.clustering.TestClusterDumper\nRunning org.apache.mahout.clustering.TestClusterEvaluator\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.835 sec - in org.apache.mahout.text.LuceneSegmentRecordReaderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.545 sec - in org.apache.mahout.text.LuceneSegmentInputFormatTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.5 sec - in org.apache.mahout.text.LuceneSegmentInputSplitTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.611 sec - in org.apache.mahout.text.SequenceFilesFromMailArchivesTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.229 sec - in org.apache.mahout.text.TestSequenceFilesFromDirectory\nTests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.593 sec - in org.apache.mahout.utils.SplitInputTest\nTests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 6.437 sec <<< FAILURE! - in org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest\ntestRun(org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest)  Time elapsed: 6.257 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<2002> but was:<0>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.junit.Assert.assertEquals(Assert.java:555)\n\tat org.junit.Assert.assertEquals(Assert.java:542)\n\tat org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest.testRun(SequenceFilesFromLuceneStorageMRJobTest.java:73)\n\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.941 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.049 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.736 sec - in org.apache.mahout.clustering.TestClusterDumper\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.254 sec - in org.apache.mahout.clustering.cdbw.TestCDbwEvaluator\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 16.973 sec - in org.apache.mahout.clustering.TestClusterEvaluator\n\nResults :\n\nFailed tests: \n  SequenceFilesFromLuceneStorageMRJobTest.testRun:73->Assert.assertEquals:542->Assert.assertEquals:555->Assert.assertEquals:118->Assert.failNotEquals:743->Assert.fail:88 expected:<2002> but was:<0>\n\nTests run: 95, Failures: 1, Errors: 0, Skipped: 0\n\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Mahout Build Tools ................................ SUCCESS [14.112s]\n[INFO] Apache Mahout ..................................... SUCCESS [6.729s]\n[INFO] Mahout Math ....................................... SUCCESS [1:43.719s]\n[INFO] Mahout Core ....................................... SUCCESS [4:22.548s]\n[INFO] Mahout Integration ................................ FAILURE [27.568s]\n[INFO] Mahout Examples ................................... SKIPPED\n[INFO] Mahout Release Package ............................ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 6:59.636s\n[INFO] Finished at: Sun Sep 01 22:13:07 UTC 2013\n[INFO] Final Memory: 55M/303M\n[INFO] ------------------------------------------------------------------------",
        "Issue Links": [
            "/jira/browse/MAHOUT-1325"
        ]
    },
    "MAHOUT-1325": {
        "Key": "MAHOUT-1325",
        "Summary": "MapReduce unit tests cannot be run in parallel",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "04/Sep/13 18:25",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "13/Nov/13 21:43",
        "Description": "Mahout-Quality build job execution #2225 failed because of this test.\nRelevant build log:\n\n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ mahout-integration ---\n[INFO] Surefire report directory: /home/jenkins/jenkins-slave/workspace/Mahout-Quality/trunk/integration/target/surefire-reports\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.utils.vectors.lucene.LuceneIterableTest\nRunning org.apache.mahout.utils.email.MailProcessorTest\nRunning org.apache.mahout.clustering.cdbw.TestCDbwEvaluator\nRunning org.apache.mahout.utils.regex.RegexMapperTest\nRunning org.apache.mahout.utils.vectors.io.VectorWriterTest\nRunning org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest\nRunning org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest\nRunning org.apache.mahout.clustering.TestClusterEvaluator\nRunning org.apache.mahout.utils.regex.RegexUtilsTest\nRunning org.apache.mahout.utils.vectors.lucene.DriverTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.17 sec - in org.apache.mahout.utils.regex.RegexUtilsTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.412 sec - in org.apache.mahout.utils.email.MailProcessorTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.344 sec - in org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest\nRunning org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest\nRunning org.apache.mahout.clustering.TestClusterDumper\nRunning org.apache.mahout.utils.vectors.VectorHelperTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.601 sec - in org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest\nRunning org.apache.mahout.utils.vectors.arff.ARFFTypeTest\nRunning org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.383 sec - in org.apache.mahout.utils.vectors.VectorHelperTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.332 sec - in org.apache.mahout.utils.vectors.lucene.LuceneIterableTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.252 sec - in org.apache.mahout.utils.vectors.arff.ARFFTypeTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.369 sec - in org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.415 sec - in org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.5 sec - in org.apache.mahout.utils.regex.RegexMapperTest\nRunning org.apache.mahout.utils.vectors.arff.DriverTest\nRunning org.apache.mahout.utils.Bump125Test\nRunning org.apache.mahout.utils.SplitInputTest\nRunning org.apache.mahout.utils.TestConcatenateVectorsJob\nRunning org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.043 sec - in org.apache.mahout.utils.vectors.io.VectorWriterTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.362 sec - in org.apache.mahout.utils.Bump125Test\nRunning org.apache.mahout.cf.taste.impl.similarity.jdbc.MySQLJDBCInMemoryItemSimilarityTest\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.304 sec - in org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.887 sec - in org.apache.mahout.utils.vectors.arff.DriverTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.486 sec - in org.apache.mahout.cf.taste.impl.similarity.jdbc.MySQLJDBCInMemoryItemSimilarityTest\nRunning org.apache.mahout.text.TestSequenceFilesFromDirectory\nRunning org.apache.mahout.text.LuceneStorageConfigurationTest\nRunning org.apache.mahout.text.LuceneSegmentInputFormatTest\nRunning org.apache.mahout.text.SequenceFilesFromMailArchivesTest\nRunning org.apache.mahout.text.MailArchivesClusteringAnalyzerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.265 sec - in org.apache.mahout.text.MailArchivesClusteringAnalyzerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.8 sec - in org.apache.mahout.utils.TestConcatenateVectorsJob\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.097 sec - in org.apache.mahout.text.LuceneStorageConfigurationTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.902 sec - in org.apache.mahout.utils.vectors.lucene.DriverTest\nRunning org.apache.mahout.text.LuceneSegmentRecordReaderTest\nRunning org.apache.mahout.text.LuceneSegmentInputSplitTest\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.237 sec - in org.apache.mahout.text.LuceneSegmentInputFormatTest\nTests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.122 sec <<< FAILURE! - in org.apache.mahout.text.SequenceFilesFromMailArchivesTest\ntestMapReduce(org.apache.mahout.text.SequenceFilesFromMailArchivesTest)  Time elapsed: 2.333 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<1> but was:<0>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.junit.Assert.assertEquals(Assert.java:555)\n\tat org.junit.Assert.assertEquals(Assert.java:542)\n\tat org.apache.mahout.text.SequenceFilesFromMailArchivesTest.testMapReduce(SequenceFilesFromMailArchivesTest.java:153)\n\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.74 sec - in org.apache.mahout.text.TestSequenceFilesFromDirectory\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.172 sec - in org.apache.mahout.text.LuceneSegmentInputSplitTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.85 sec - in org.apache.mahout.text.LuceneSegmentRecordReaderTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.571 sec - in org.apache.mahout.clustering.TestClusterDumper\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.021 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.121 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.613 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest\nTests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.246 sec - in org.apache.mahout.utils.SplitInputTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 13.07 sec - in org.apache.mahout.clustering.cdbw.TestCDbwEvaluator\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.224 sec - in org.apache.mahout.clustering.TestClusterEvaluator\n\nResults :\n\nFailed tests: \n  SequenceFilesFromMailArchivesTest.testMapReduce:153->Assert.assertEquals:542->Assert.assertEquals:555->Assert.assertEquals:118->Assert.failNotEquals:743->Assert.fail:88 expected:<1> but was:<0>\n\nTests run: 95, Failures: 1, Errors: 0, Skipped: 0\n\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Mahout Build Tools ................................ SUCCESS [8.296s]\n[INFO] Apache Mahout ..................................... SUCCESS [2.731s]\n[INFO] Mahout Math ....................................... SUCCESS [1:38.594s]\n[INFO] Mahout Core ....................................... SUCCESS [4:44.809s]\n[INFO] Mahout Integration ................................ FAILURE [26.458s]\n[INFO] Mahout Examples ................................... SKIPPED\n[INFO] Mahout Release Package ............................ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 7:03.463s\n[INFO] Finished at: Tue Sep 03 22:40:41 UTC 2013\n[INFO] Final Memory: 62M/447M\n[INFO] ------------------------------------------------------------------------",
        "Issue Links": [
            "/jira/browse/MAPREDUCE-5367",
            "/jira/browse/MAHOUT-1321",
            "/jira/browse/MAHOUT-1322",
            "/jira/browse/MAHOUT-1323",
            "/jira/browse/MAHOUT-1324",
            "/jira/browse/MAHOUT-1200"
        ]
    },
    "MAHOUT-1326": {
        "Key": "MAHOUT-1326",
        "Summary": "Fix broken links to quickstart tutorials",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Ravi Mummulla",
        "Created": "04/Sep/13 19:38",
        "Updated": "03/Feb/14 07:50",
        "Resolved": "23/Jan/14 21:39",
        "Description": "All links are broken in https://cwiki.apache.org/MAHOUT/quickstart.html.",
        "Issue Links": []
    },
    "MAHOUT-1327": {
        "Key": "MAHOUT-1327",
        "Summary": "org.apache.mahout.clustering.classify.ClusterClassifier.readFromSeqFiles is using a new instance of the Configuration object to read the file form the Path instead of using the Configuration object passed to the method",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "alan krumholz",
        "Created": "05/Sep/13 19:36",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "06/Sep/13 00:01",
        "Description": "When you use KmeansDriver.run with a Configuration object pointing to HDFS:\n Configuration conf = new Configuration();\n        conf.addResource(new Path(\"C:\\\\hdp-win\\\\hadoop\\\\hadoop-1.1.0-SNAPSHOT\\\\confcore-site.xml\"));\n        conf.addResource(new Path(\"C:\\\\hdp-win\\\\hadoop\\\\hadoop-1.1.0-SNAPSHOT\\\\confhdfs-site.xml\"))\nIt calls org.apache.mahout.clustering.classify.ClusterClassifier.readFromSeqFiles\nat some point and I get an exception (there is no problem if you run it with a conf object pointing to the local file system):\njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n    at java.util.ArrayList.RangeCheck(ArrayList.java:547)\n    at java.util.ArrayList.get(ArrayList.java:322)\n    at org.apache.mahout.clustering.classify.ClusterClassifier.readFromSeqFiles(ClusterClassifier.java:215)\nI think this is happening because that method is using a new instance of the Configuration object to read the file form the Path instead of using the Configuration object passed to the method.",
        "Issue Links": []
    },
    "MAHOUT-1328": {
        "Key": "MAHOUT-1328",
        "Summary": "CLI-invoked K-means final step (Cluster Classification Driver) ignores job-specific -D MR parameters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stewart Whiting",
        "Created": "09/Sep/13 15:50",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "03/Dec/13 13:29",
        "Description": "I believe this is an issue - someone please correct me if not!\nI am running a large k-means clustering task. Our default cluster map/reduce slots per node and JVM memory parameters etc are not appropriate for the memory requirements of this.\nSo, I invoke K-means clustering from the CLI using, for example:\nmahout kmeans -i /mahout-input -o /mahout-output -c clusters -dm org.apache.mahout.common.distance.CosineDistanceMeasure -x 12 -ow -k 50 -cl -Dmapred.child.java.opts=-Xmx7096m -Dmapred.tasktracker.reduce.tasks.maximum=1 -Dmapred.tasktracker.map.tasks.maximum=1 -Dmapred.job.map.memory.mb=7000 -Dmapred.cluster.max.map.memory.mb=7000 -Dmapred.cluster.reduce.memory.mb=7000 -Dmapred.cluster.max.reduce.memory.mb=7000\nThe initial MR tasks for each clustering iteration run successfully. Inspecting the Hadoop config for each task after completion show that the job runs with the explicitly provided MR configuration from the -D parameters.\nHowever, when the final cluster classification task is run (i.e. to generate the clusteredPoints/ directory), it usually fails due to outOfMemory errors. Inspecting the MR task logs for it shows that it ran with the default cluster settings, not those provided by my -D CLI parameters.",
        "Issue Links": []
    },
    "MAHOUT-1329": {
        "Key": "MAHOUT-1329",
        "Summary": "Mahout for hadoop 2",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Gokhan Capan",
        "Reporter": "Sergey Svinarchuk",
        "Created": "09/Sep/13 16:18",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "25/Feb/14 10:02",
        "Description": "Update mahout for work with hadoop 2.X, targeting this for Mahout 1.0.",
        "Issue Links": [
            "/jira/browse/BIGTOP-1470",
            "/jira/browse/BIGTOP-1470",
            "/jira/browse/MAHOUT-1354"
        ]
    },
    "MAHOUT-1330": {
        "Key": "MAHOUT-1330",
        "Summary": "Unable to do K-means clustering on Reuters dataset",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Karthik Prakhya",
        "Created": "10/Sep/13 18:54",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "25/Nov/13 15:02",
        "Description": "The attached code uses the Mahout API to do k-means clustering on the Reuters dataset and generates the initial centroids using the canopy algorithm. The parameters are exactly the same as the ones in the Scala example presented in the following link:\nhttp://sujitpal.blogspot.com/2012/09/learning-mahout-clustering.html\nThe code compiles without an error, but the K-means algorithm cannot initiate because the initial centroids are not being generated. This in turn is due to the fact that the TF-IDF vectors are not being generated.\nConsidering that this code compiles and is based on earlier Scala code that worked, it is suggestive that there is a bug in the Mahout source code that may need fixing. I thought I should bring it to your attention.\nI have attached the source code, the included JAR files and the shell script (called test-kmeans-clustering-reuters-java-api.sh) to compile and run the code. The output of the shell script is located in NewsKMeansClustering-output.txt. Please note that you may need to change the path (see environmental variable JARPATH) to the JAR files in the shell script based on where you put the JARs. I also attached the output of clusterdump utility in the form of .txt files for the intermediate outputs of my code such as the TF vectors and TF-IDF vectors (see tf-vectors.txt, tfidf-vectors.txt, df-count.txt and frequency-file.txt).",
        "Issue Links": []
    },
    "MAHOUT-1331": {
        "Key": "MAHOUT-1331",
        "Summary": "Class AbstractVector.java index is an int, increase to long",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Max Weule",
        "Created": "11/Sep/13 15:10",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "13/Apr/14 11:19",
        "Description": "While using mahout the following problem appears: \norg.apache.mahout.math.IndexException: Index -1469322379 is outside allowable range of [0,2147483647) at org.apache.mahout.math.AbstractVector.set(AbstractVector.java:395).\nWe have indexes above 2147483647 and they are exceeding the limit of integers. Plz consider increasing index from int to long.",
        "Issue Links": []
    },
    "MAHOUT-1332": {
        "Key": "MAHOUT-1332",
        "Summary": "Javadoc typos in cf",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Carl Clark",
        "Created": "13/Sep/13 15:07",
        "Updated": "14/Sep/13 16:54",
        "Resolved": "13/Sep/13 21:06",
        "Description": "There are a few typos in the o.a.m.cf.taste package.\nmodel.Preference\n22  * A  encapsulates an item and a preference value, which indicates the strength of the\n23  * preference for it. s are associated to users.\nrecommender.Rescorer\n22  * A  simply assigns a new \"score\" to a thing like an ID of an item or user which a\n29  * A  can also exclude a thing from consideration entirely by returning \n{@code true}\n from\nBoth of these arose after removing a self link.\n\"A \n{@link Preference}\n encapsulates an item ...\"\nmodel.DataModel\n178    * @return true iff this implementation actually stores and returns distinct preference values;\niff\nimpl.model.PlusAnonymousConcurrentUserDataModel\n47  * a user has to be taken from the pool and returned back immediately afterwars.\nafterwars\neval.RelevantItemsDataSplitter\n54    * @param otherUserID     for whom we are adding preferences to the trianing model\ntrianing\neval.RecommenderEvaluator\n47    * recommendatinos, and for each user, the remaining preferences are compared against the user's real\nrecommendatinos\nhadoop.als.FactorizationEvaluator\n53  * <p>Measures the root-mean-squared error of a ratring matrix factorization against a test set.</p>\nratring",
        "Issue Links": []
    },
    "MAHOUT-1333": {
        "Key": "MAHOUT-1333",
        "Summary": "Build artifacts set mode 000 on examples/bin directory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mike Percy",
        "Created": "16/Sep/13 21:04",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "17/Sep/13 15:33",
        "Description": "The source amd binary artifacts contain bad permissions for the examples/bin directory.\nProblem:\n\n~/src/mahout/distribution$ tar tzvf target/mahout-distribution-0.9-SNAPSHOT-src.tar.gz | grep -- ----\nd--------- 0/0                 0 2013-09-16 13:22 mahout-distribution-0.9-SNAPSHOT/examples/bin/\n~/src/mahout/distribution$ tar tzvf target/mahout-distribution-0.9-SNAPSHOT.tar.gz | grep -- ----\nd--------- 0/0                  0 2013-09-16 13:22 mahout-distribution-0.9-SNAPSHOT/examples/bin/",
        "Issue Links": []
    },
    "MAHOUT-1334": {
        "Key": "MAHOUT-1334",
        "Summary": "SequenceFilesFromMailArchivesTest fails on Oracle JDK7 on Linux",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mike Percy",
        "Created": "16/Sep/13 21:17",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "16/Sep/13 21:30",
        "Description": "I get test failures when running SequenceFilesFromMailArchivesTest in Oracle JDK7 on Linux:\n\n~/src/mahout$ mvn clean install -DfailIfNoTests=false -Dtest=SequenceFilesFromMailArchivesTest\n...\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.text.SequenceFilesFromMailArchivesTest\nTests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 1.934 sec <<< FAILURE! - in org.apache.mahout.text.SequenceFilesFromMailArchivesTest\ntestSequential(org.apache.mahout.text.SequenceFilesFromMailArchivesTest)  Time elapsed: 0.57 sec  <<< FAILURE!\norg.junit.ComparisonFailure: expected:<TEST/subdir/[mail-messages].gz/user@example.com> but was:<TEST/subdir/[subsubdir/mail-messages-2].gz/user@example.com>\n\tat org.junit.Assert.assertEquals(Assert.java:115)\n\tat org.junit.Assert.assertEquals(Assert.java:144)\n\tat org.apache.mahout.text.SequenceFilesFromMailArchivesTest.testSequential(SequenceFilesFromMailArchivesTest.java:106)\n\n\nResults :\n\nFailed tests: \n  SequenceFilesFromMailArchivesTest.testSequential:106->Assert.assertEquals:144->Assert.assertEquals:115 expected:<TEST/subdir/[mail-messages].gz/user@example.com> but was:<TEST/subdir/[subsubdir/mail-messages-2].gz/user@example.com>\n\nTests run: 2, Failures: 1, Errors: 0, Skipped: 0",
        "Issue Links": [
            "/jira/browse/MAHOUT-1302"
        ]
    },
    "MAHOUT-1335": {
        "Key": "MAHOUT-1335",
        "Summary": "MultithreadedSharingMapper fails on Hadoop 2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mike Percy",
        "Created": "17/Sep/13 02:35",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "17/Sep/13 05:45",
        "Description": "Between Hadoop 1 and 2, the string value of the Configuration option that stores the mapper classname in MultithreadedMapper changed from \"mapred.map.multithreadedrunner.class\" to \"mapreduce.mapper.multithreadedmapper.mapclass\".\nSince there is a stable API (getMapperClass()) that abstracts these differences away, we should use that instead.",
        "Issue Links": []
    },
    "MAHOUT-1336": {
        "Key": "MAHOUT-1336",
        "Summary": "HighDFWordsPrunerTest is failing silently",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mike Percy",
        "Created": "17/Sep/13 04:35",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "17/Sep/13 05:48",
        "Description": "Apparently ToolRunner does not allow the --mapred option. The validation is not very foolproof, so there is a resulting silent failure in HighDFWordsPrunerTest.\nError message:\n\norg.apache.commons.cli2.OptionException: Unexpected --mapred while processing Options\n\tat org.apache.commons.cli2.commandline.Parser.parse(Parser.java:99)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.run(SparseVectorsFromSequenceFiles.java:154)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.vectorizer.HighDFWordsPrunerTest.runTest(HighDFWordsPrunerTest.java:111)\n\tat org.apache.mahout.vectorizer.HighDFWordsPrunerTest.testHighDFWordsPruning(HighDFWordsPrunerTest.java:85)\n...\nUsage:                                                                          \n [--minSupport <minSupport> --analyzerName <analyzerName> --chunkSize           \n<chunkSize> --output <output> --input <input> --minDF <minDF> --maxDFSigma      \n<maxDFSigma> --maxDFPercent <maxDFPercent> --weight <weight> --norm <norm>      \n--minLLR <minLLR> --numReducers <numReducers> --maxNGramSize <ngramSize>        \n--overwrite --help --sequentialAccessVector --namedVector --logNormalize]       \nO",
        "Issue Links": []
    },
    "MAHOUT-1337": {
        "Key": "MAHOUT-1337",
        "Summary": "SearchSanityTest is unstable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "17/Sep/13 18:43",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "17/Sep/13 19:32",
        "Description": "From time to time SearchSanityTest fails and causes Mahout build to be unstable.\nHere is relevant build output from one such recent build failure (MahoutQuality #2248 on ubuntu4 node):\n\n[INFO] --- maven-surefire-plugin:2.16:test (default-test) @ mahout-core ---\n[INFO] Surefire report directory: /home/jenkins/jenkins-slave/workspace/Mahout-Quality/trunk/core/target/surefire-reports\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\n\n-------------------------------------------------------\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramTest\nRunning org.apache.mahout.ep.EvolutionaryProcessTest\nRunning org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest\nRunning org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest\nRunning org.apache.mahout.driver.MahoutDriverTest\nRunning org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest\nRunning org.apache.mahout.vectorizer.encoders.TextValueEncoderTest\nRunning org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.354 sec - in org.apache.mahout.vectorizer.collocations.llr.GramTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.458 sec - in org.apache.mahout.vectorizer.encoders.InteractionValueEncoderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.387 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyPartitionerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.259 sec - in org.apache.mahout.vectorizer.encoders.ConstantValueEncoderTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.406 sec - in org.apache.mahout.vectorizer.encoders.WordLikeValueEncoderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.514 sec - in org.apache.mahout.vectorizer.encoders.ContinuousValueEncoderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.31 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyGroupComparatorTest\nRunning org.apache.mahout.vectorizer.DictionaryVectorizerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.693 sec - in org.apache.mahout.vectorizer.collocations.llr.LLRReducerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.969 sec - in org.apache.mahout.ep.EvolutionaryProcessTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.817 sec - in org.apache.mahout.vectorizer.encoders.TextValueEncoderTest\nRunning org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.792 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocReducerTest\nRunning org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.155 sec - in org.apache.mahout.driver.MahoutDriverTest\nRunning org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest\nRunning org.apache.mahout.vectorizer.HighDFWordsPrunerTest\nRunning org.apache.mahout.vectorizer.DocumentProcessorTest\nRunning org.apache.mahout.vectorizer.collocations.llr.GramKeyTest\nRunning org.apache.mahout.math.MatrixWritableTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.245 sec - in org.apache.mahout.math.MatrixWritableTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.373 sec - in org.apache.mahout.vectorizer.collocations.llr.GramKeyTest\nRunning org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.53 sec - in org.apache.mahout.math.hadoop.stochasticsvd.SSVDCommonTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.288 sec - in org.apache.mahout.vectorizer.collocations.llr.CollocMapperTest\nRunning org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI\nRunning org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI\nRunning org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob\nRunning org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.348 sec - in org.apache.mahout.vectorizer.DocumentProcessorTest\nRunning org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.71 sec - in org.apache.mahout.math.hadoop.similarity.TestVectorDistanceSimilarityJob\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.791 sec - in org.apache.mahout.vectorizer.EncodedVectorsFromSequenceFilesTest\nRunning org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.207 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.measures.VectorSimilarityMeasuresTest\nRunning org.apache.mahout.math.hadoop.TestDistributedRowMatrix\nRunning org.apache.mahout.math.hadoop.stats.BasicStatsTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 22.183 sec - in org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolverCLI\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.611 sec - in org.apache.mahout.math.hadoop.stats.BasicStatsTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 18.145 sec - in org.apache.mahout.math.hadoop.similarity.cooccurrence.RowSimilarityJobTest\nRunning org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest\nRunning org.apache.mahout.math.VarintTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.077 sec - in org.apache.mahout.math.VarintTest\nRunning org.apache.mahout.math.stats.OnlineAucTest\nRunning org.apache.mahout.math.stats.SamplerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.26 sec - in org.apache.mahout.math.stats.SamplerTest\nRunning org.apache.mahout.math.VectorWritableTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.6 sec - in org.apache.mahout.math.hadoop.solver.TestDistributedConjugateGradientSolver\nRunning org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest\nTests run: 100, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.745 sec - in org.apache.mahout.math.VectorWritableTest\nRunning org.apache.mahout.math.neighborhood.SearchQualityTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.42 sec - in org.apache.mahout.math.stats.OnlineAucTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 26.927 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverSparseSequentialTest\nRunning org.apache.mahout.math.neighborhood.SearchSanityTest\nRunning org.apache.mahout.cf.taste.hadoop.TopItemsQueueTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.134 sec - in org.apache.mahout.cf.taste.hadoop.TopItemsQueueTest\nTests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.593 sec - in org.apache.mahout.math.hadoop.TestDistributedRowMatrix\nRunning org.apache.mahout.cf.taste.hadoop.item.ToUserVectorsReducerTest\nRunning org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 28.697 sec - in org.apache.mahout.vectorizer.HighDFWordsPrunerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.377 sec - in org.apache.mahout.cf.taste.hadoop.item.ToUserVectorsReducerTest\nRunning org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest\nRunning org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.498 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDPCASparseTest\nRunning org.apache.mahout.cf.taste.common.CommonTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.138 sec - in org.apache.mahout.cf.taste.common.CommonTest\nRunning org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluatorImplTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.219 sec - in org.apache.mahout.cf.taste.impl.eval.GenericRecommenderIRStatsEvaluatorImplTest\nRunning org.apache.mahout.cf.taste.impl.recommender.RandomRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.222 sec - in org.apache.mahout.cf.taste.impl.recommender.RandomRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.ItemUserAverageRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.242 sec - in org.apache.mahout.cf.taste.impl.recommender.ItemUserAverageRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.NullRescorerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.099 sec - in org.apache.mahout.cf.taste.impl.recommender.NullRescorerTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.FilePersistenceStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.212 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.FilePersistenceStrategyTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.485 sec - in org.apache.mahout.vectorizer.DictionaryVectorizerTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommenderTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.403 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.SVDRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizerTest\nRunning org.apache.mahout.cf.taste.impl.recommender.svd.ParallelSGDFactorizerTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.248 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.ALSWRFactorizerTest\nRunning org.apache.mahout.cf.taste.impl.recommender.AllUnknownItemsCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.152 sec - in org.apache.mahout.cf.taste.impl.recommender.AllUnknownItemsCandidateItemsStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommenderTest\nTests run: 11, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.274 sec - in org.apache.mahout.cf.taste.impl.recommender.GenericItemBasedRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.ItemAverageRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.346 sec - in org.apache.mahout.cf.taste.impl.recommender.ItemAverageRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommenderTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.283 sec - in org.apache.mahout.cf.taste.impl.recommender.GenericUserBasedRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.PreferredItemsNeighborhoodCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.227 sec - in org.apache.mahout.cf.taste.impl.recommender.PreferredItemsNeighborhoodCandidateItemsStrategyTest\nRunning org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategyTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.198 sec - in org.apache.mahout.cf.taste.impl.recommender.SamplingCandidateItemsStrategyTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 44.107 sec - in org.apache.mahout.math.hadoop.decomposer.TestDistributedLanczosSolverCLI\nRunning org.apache.mahout.cf.taste.impl.recommender.CachingRecommenderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.262 sec - in org.apache.mahout.cf.taste.impl.recommender.CachingRecommenderTest\nRunning org.apache.mahout.cf.taste.impl.recommender.TopItemsTest\nRunning org.apache.mahout.cf.taste.impl.common.InvertedRunningAverageTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.175 sec - in org.apache.mahout.cf.taste.impl.recommender.TopItemsTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.154 sec - in org.apache.mahout.cf.taste.impl.common.InvertedRunningAverageTest\nRunning org.apache.mahout.cf.taste.impl.common.FastByIDMapTest\nRunning org.apache.mahout.cf.taste.impl.common.RunningAverageTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.119 sec - in org.apache.mahout.cf.taste.impl.common.RunningAverageTest\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.47 sec - in org.apache.mahout.cf.taste.impl.common.FastByIDMapTest\nRunning org.apache.mahout.cf.taste.impl.common.RefreshHelperTest\nRunning org.apache.mahout.cf.taste.impl.common.FastIDSetTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.231 sec - in org.apache.mahout.cf.taste.impl.common.RefreshHelperTest\nRunning org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDevTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.45 sec - in org.apache.mahout.cf.taste.impl.common.FastIDSetTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.233 sec - in org.apache.mahout.cf.taste.impl.common.RunningAverageAndStdDevTest\nRunning org.apache.mahout.cf.taste.impl.common.CacheTest\nRunning org.apache.mahout.cf.taste.impl.common.BitSetTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.147 sec - in org.apache.mahout.cf.taste.impl.common.BitSetTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 19.359 sec - in org.apache.mahout.cf.taste.hadoop.similarity.item.ItemSimilarityJobTest\nRunning org.apache.mahout.cf.taste.impl.common.LongPrimitiveArrayIteratorTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.097 sec - in org.apache.mahout.cf.taste.impl.common.LongPrimitiveArrayIteratorTest\nRunning org.apache.mahout.cf.taste.impl.common.WeightedRunningAverageTest\nRunning org.apache.mahout.cf.taste.impl.common.FastMapTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.247 sec - in org.apache.mahout.cf.taste.impl.common.WeightedRunningAverageTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.642 sec - in org.apache.mahout.cf.taste.impl.common.CacheTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 49.493 sec - in org.apache.mahout.math.hadoop.stochasticsvd.LocalSSVDSolverDenseTest\nRunning org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIteratorTest\nRunning org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarityTest\nTests run: 14, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.812 sec - in org.apache.mahout.cf.taste.impl.common.FastMapTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.447 sec - in org.apache.mahout.cf.taste.impl.similarity.GenericItemSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.TanimotoCoefficientSimilarityTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.449 sec - in org.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarityTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.335 sec - in org.apache.mahout.cf.taste.impl.similarity.TanimotoCoefficientSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.AveragingPreferenceInferrerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.278 sec - in org.apache.mahout.cf.taste.impl.similarity.AveragingPreferenceInferrerTest\nRunning org.apache.mahout.cf.taste.impl.similarity.file.FileItemSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.SpearmanCorrelationSimilarityTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.164 sec - in org.apache.mahout.cf.taste.impl.common.SamplingLongPrimitiveIteratorTest\nRunning org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarityTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.601 sec - in org.apache.mahout.cf.taste.impl.similarity.SpearmanCorrelationSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarityTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.628 sec - in org.apache.mahout.cf.taste.impl.similarity.EuclideanDistanceSimilarityTest\nTests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.538 sec - in org.apache.mahout.cf.taste.impl.similarity.PearsonCorrelationSimilarityTest\nRunning org.apache.mahout.cf.taste.impl.model.MemoryIDMigratorTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.308 sec - in org.apache.mahout.cf.taste.impl.model.MemoryIDMigratorTest\nRunning org.apache.mahout.cf.taste.impl.model.GenericDataModelTest\nRunning org.apache.mahout.cf.taste.impl.model.BooleanUserPreferenceArrayTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.257 sec - in org.apache.mahout.cf.taste.impl.model.BooleanUserPreferenceArrayTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.633 sec - in org.apache.mahout.cf.taste.impl.model.GenericDataModelTest\nRunning org.apache.mahout.cf.taste.impl.model.BooleanItemPreferenceArrayTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.25 sec - in org.apache.mahout.cf.taste.impl.model.BooleanItemPreferenceArrayTest\nRunning org.apache.mahout.cf.taste.impl.model.file.FileIDMigratorTest\nRunning org.apache.mahout.cf.taste.impl.model.file.FileDataModelTest\nRunning org.apache.mahout.cf.taste.impl.model.PlusAnonymousConcurrentUserDataModelTest\nTests run: 15, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.391 sec - in org.apache.mahout.cf.taste.impl.model.PlusAnonymousConcurrentUserDataModelTest\nRunning org.apache.mahout.cf.taste.impl.model.GenericUserPreferenceArrayTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.483 sec - in org.apache.mahout.cf.taste.impl.similarity.file.FileItemSimilarityTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.198 sec - in org.apache.mahout.cf.taste.impl.model.GenericUserPreferenceArrayTest\nRunning org.apache.mahout.cf.taste.impl.model.GenericItemPreferenceArrayTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.275 sec - in org.apache.mahout.cf.taste.impl.model.GenericItemPreferenceArrayTest\nRunning org.apache.mahout.cf.taste.impl.neighborhood.ThresholdNeighborhoodTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.343 sec - in org.apache.mahout.cf.taste.impl.neighborhood.ThresholdNeighborhoodTest\nRunning org.apache.mahout.cf.taste.impl.neighborhood.NearestNNeighborhoodTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.365 sec - in org.apache.mahout.cf.taste.impl.neighborhood.NearestNNeighborhoodTest\nRunning org.apache.mahout.common.iterator.CountingIteratorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.145 sec - in org.apache.mahout.common.iterator.CountingIteratorTest\nRunning org.apache.mahout.common.iterator.TestSamplingIterator\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.377 sec - in org.apache.mahout.cf.taste.impl.model.file.FileDataModelTest\nRunning org.apache.mahout.common.iterator.TestFixedSizeSampler\nRunning org.apache.mahout.common.iterator.TestStableFixedSizeSampler\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.331 sec - in org.apache.mahout.common.iterator.TestFixedSizeSampler\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 4.37 sec - in org.apache.mahout.cf.taste.impl.model.file.FileIDMigratorTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.313 sec - in org.apache.mahout.common.iterator.TestStableFixedSizeSampler\nRunning org.apache.mahout.common.AbstractJobTest\nRunning org.apache.mahout.common.IntPairWritableTest\nRunning org.apache.mahout.common.lucene.AnalyzerUtilsTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.134 sec - in org.apache.mahout.common.IntPairWritableTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.156 sec - in org.apache.mahout.common.lucene.AnalyzerUtilsTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.29 sec - in org.apache.mahout.common.iterator.TestSamplingIterator\nRunning org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest\nRunning org.apache.mahout.common.distance.TestChebyshevMeasure\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.504 sec - in org.apache.mahout.common.AbstractJobTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.397 sec - in org.apache.mahout.common.distance.TestWeightedEuclideanDistanceMeasureTest\nRunning org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.312 sec - in org.apache.mahout.common.distance.TestChebyshevMeasure\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.293 sec - in org.apache.mahout.common.distance.TestWeightedManhattanDistanceMeasure\nRunning org.apache.mahout.common.distance.CosineDistanceMeasureTest\nRunning org.apache.mahout.common.distance.TestEuclideanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.229 sec - in org.apache.mahout.common.distance.CosineDistanceMeasureTest\nRunning org.apache.mahout.common.distance.TestTanimotoDistanceMeasure\nRunning org.apache.mahout.common.distance.TestMinkowskiMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.507 sec - in org.apache.mahout.common.distance.TestEuclideanDistanceMeasure\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.362 sec - in org.apache.mahout.common.distance.TestTanimotoDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.304 sec - in org.apache.mahout.common.distance.TestMinkowskiMeasure\nRunning org.apache.mahout.common.distance.TestManhattanDistanceMeasure\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.296 sec - in org.apache.mahout.common.distance.TestManhattanDistanceMeasure\nRunning org.apache.mahout.common.StringUtilsTest\nRunning org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 39.367 sec - in org.apache.mahout.math.ssvd.SequentialOutOfCoreSvdTest\nRunning org.apache.mahout.common.DummyRecordWriterTest\nTests run: 21, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 34.852 sec - in org.apache.mahout.cf.taste.hadoop.item.RecommenderJobTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.291 sec - in org.apache.mahout.common.distance.TestMahalanobisDistanceMeasure\nRunning org.apache.mahout.clustering.iterator.TestClusterClassifier\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.229 sec - in org.apache.mahout.common.DummyRecordWriterTest\nRunning org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer\nRunning org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.919 sec - in org.apache.mahout.common.StringUtilsTest\nRunning org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob\nRunning org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob\nRunning org.apache.mahout.clustering.spectral.TestVectorCache\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.399 sec - in org.apache.mahout.clustering.spectral.TestAffinityMatrixInputJob\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.186 sec - in org.apache.mahout.clustering.spectral.TestVectorMatrixMultiplicationJob\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.697 sec - in org.apache.mahout.clustering.spectral.TestMatrixDiagonalizeJob\nRunning org.apache.mahout.clustering.spectral.TestUnitVectorizerJob\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.497 sec - in org.apache.mahout.clustering.spectral.TestVectorCache\nRunning org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest\nRunning org.apache.mahout.clustering.streaming.cluster.BallKMeansTest\nRunning org.apache.mahout.clustering.TestClusterInterface\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.248 sec - in org.apache.mahout.clustering.TestClusterInterface\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 67.708 sec - in org.apache.mahout.vectorizer.SparseVectorsFromSequenceFilesTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.54 sec - in org.apache.mahout.clustering.spectral.TestUnitVectorizerJob\nRunning org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering\nRunning org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator\nRunning org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.068 sec - in org.apache.mahout.clustering.kmeans.TestEigenSeedGenerator\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.205 sec - in org.apache.mahout.clustering.kmeans.TestRandomSeedGenerator\nRunning org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest\nRunning org.apache.mahout.clustering.kmeans.TestKmeansClustering\nTests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.247 sec - in org.apache.mahout.clustering.iterator.TestClusterClassifier\nRunning org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.255 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterCountReaderTest\nRunning org.apache.mahout.clustering.topdown.PathDirectoryTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.28 sec - in org.apache.mahout.clustering.topdown.PathDirectoryTest\nRunning org.apache.mahout.clustering.classify.ClusterClassificationDriverTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.875 sec - in org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorTest\nRunning org.apache.mahout.clustering.canopy.TestCanopyCreation\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.983 sec - in org.apache.mahout.clustering.classify.ClusterClassificationDriverTest\nRunning org.apache.mahout.clustering.TestGaussianAccumulators\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 5.793 sec - in org.apache.mahout.clustering.TestGaussianAccumulators\nRunning org.apache.mahout.classifier.ConfusionMatrixTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.213 sec - in org.apache.mahout.classifier.ConfusionMatrixTest\nRunning org.apache.mahout.classifier.naivebayes.training.IndexInstancesMapperTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.833 sec - in org.apache.mahout.classifier.naivebayes.training.IndexInstancesMapperTest\nRunning org.apache.mahout.classifier.naivebayes.training.ThetaMapperTest\nTests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 15.39 sec - in org.apache.mahout.clustering.canopy.TestCanopyCreation\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.684 sec - in org.apache.mahout.classifier.naivebayes.training.ThetaMapperTest\nRunning org.apache.mahout.classifier.naivebayes.training.WeightsMapperTest\nRunning org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifierTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.239 sec - in org.apache.mahout.classifier.naivebayes.ComplementaryNaiveBayesClassifierTest\nRunning org.apache.mahout.classifier.naivebayes.NaiveBayesModelTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.881 sec - in org.apache.mahout.classifier.naivebayes.training.WeightsMapperTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.21 sec - in org.apache.mahout.classifier.naivebayes.NaiveBayesModelTest\nRunning org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifierTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.215 sec - in org.apache.mahout.classifier.naivebayes.StandardNaiveBayesClassifierTest\nRunning org.apache.mahout.classifier.naivebayes.NaiveBayesTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 25.106 sec - in org.apache.mahout.clustering.kmeans.TestKmeansClustering\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMModelTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.505 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMModelTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMEvaluatorTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMUtilsTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.194 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMEvaluatorTest\nTests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.159 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMUtilsTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMTrainerTest\nRunning org.apache.mahout.classifier.sequencelearning.hmm.HMMAlgorithmsTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.257 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMAlgorithmsTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.369 sec - in org.apache.mahout.classifier.sequencelearning.hmm.HMMTrainerTest\nRunning org.apache.mahout.classifier.evaluation.AucTest\nRunning org.apache.mahout.classifier.df.data.DescriptorUtilsTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.224 sec - in org.apache.mahout.classifier.df.data.DescriptorUtilsTest\nRunning org.apache.mahout.classifier.df.data.DataConverterTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.251 sec - in org.apache.mahout.classifier.evaluation.AucTest\nRunning org.apache.mahout.classifier.df.data.DataLoaderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.105 sec - in org.apache.mahout.classifier.df.data.DataConverterTest\nRunning org.apache.mahout.classifier.df.data.DatasetTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.143 sec - in org.apache.mahout.classifier.df.data.DatasetTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.615 sec - in org.apache.mahout.classifier.df.data.DataLoaderTest\nRunning org.apache.mahout.classifier.df.data.DataTest\nRunning org.apache.mahout.classifier.df.tools.VisualizerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 37.53 sec - in org.apache.mahout.clustering.fuzzykmeans.TestFuzzyKmeansClustering\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.357 sec - in org.apache.mahout.classifier.naivebayes.NaiveBayesTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.054 sec - in org.apache.mahout.classifier.df.tools.VisualizerTest\nRunning org.apache.mahout.classifier.df.DecisionForestTest\nRunning org.apache.mahout.classifier.df.node.NodeTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.088 sec - in org.apache.mahout.classifier.df.data.DataTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.205 sec - in org.apache.mahout.classifier.df.node.NodeTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.Step1MapperTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.TreeIDTest\nRunning org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.078 sec - in org.apache.mahout.classifier.df.DecisionForestTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.83 sec - in org.apache.mahout.classifier.df.mapreduce.partial.TreeIDTest\nRunning org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputFormatTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.536 sec - in org.apache.mahout.classifier.df.mapreduce.partial.Step1MapperTest\nRunning org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputSplitTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.152 sec - in org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputSplitTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.719 sec - in org.apache.mahout.classifier.df.mapreduce.inmem.InMemInputFormatTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.641 sec - in org.apache.mahout.classifier.df.mapreduce.partial.PartialBuilderTest\nRunning org.apache.mahout.classifier.df.builder.DecisionTreeBuilderTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.217 sec - in org.apache.mahout.classifier.df.builder.DecisionTreeBuilderTest\nRunning org.apache.mahout.classifier.df.builder.DefaultTreeBuilderTest\nRunning org.apache.mahout.classifier.df.builder.InfiniteRecursionTest\nRunning org.apache.mahout.classifier.df.split.RegressionSplitTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.409 sec - in org.apache.mahout.classifier.df.builder.DefaultTreeBuilderTest\nRunning org.apache.mahout.classifier.df.split.OptIgSplitTest\nRunning org.apache.mahout.classifier.df.split.DefaultIgSplitTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.032 sec - in org.apache.mahout.classifier.df.builder.InfiniteRecursionTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.908 sec - in org.apache.mahout.classifier.df.split.RegressionSplitTest\nRunning org.apache.mahout.classifier.sgd.OnlineLogisticRegressionTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.583 sec - in org.apache.mahout.classifier.df.split.OptIgSplitTest\nRunning org.apache.mahout.classifier.sgd.CsvRecordFactoryTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.454 sec - in org.apache.mahout.classifier.df.split.DefaultIgSplitTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.495 sec - in org.apache.mahout.classifier.sgd.CsvRecordFactoryTest\nRunning org.apache.mahout.classifier.sgd.AdaptiveLogisticRegressionTest\nRunning org.apache.mahout.classifier.sgd.GradientMachineTest\nRunning org.apache.mahout.classifier.sgd.ModelSerializerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.484 sec - in org.apache.mahout.classifier.sgd.GradientMachineTest\nRunning org.apache.mahout.classifier.sgd.PassiveAggressiveTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.752 sec - in org.apache.mahout.classifier.sgd.PassiveAggressiveTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.418 sec - in org.apache.mahout.classifier.sgd.ModelSerializerTest\nRunning org.apache.mahout.classifier.RegressionResultAnalyzerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.417 sec - in org.apache.mahout.classifier.RegressionResultAnalyzerTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 92.983 sec - in org.apache.mahout.math.neighborhood.LocalitySensitiveHashSearchTest\nTests run: 18, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 92.341 sec <<< FAILURE! - in org.apache.mahout.math.neighborhood.SearchSanityTest\ntestRemoval[2](org.apache.mahout.math.neighborhood.SearchSanityTest)  Time elapsed: 0.185 sec  <<< FAILURE!\njava.lang.AssertionError: Previous second neighbor should be first expected:<0.0> but was:<12.079332354796572>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:494)\n\tat org.apache.mahout.math.neighborhood.SearchSanityTest.testRemoval(SearchSanityTest.java:166)\n\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.528 sec - in org.apache.mahout.classifier.sgd.AdaptiveLogisticRegressionTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 68.469 sec - in org.apache.mahout.clustering.lda.cvb.TestCVBModelTrainer\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 71.343 sec - in org.apache.mahout.clustering.streaming.cluster.BallKMeansTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 35.635 sec - in org.apache.mahout.classifier.sgd.OnlineLogisticRegressionTest\nTests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 117.681 sec - in org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 92.578 sec - in org.apache.mahout.clustering.streaming.cluster.StreamingKMeansTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 129.494 sec - in org.apache.mahout.cf.taste.impl.recommender.svd.ParallelSGDFactorizerTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 193.786 sec - in org.apache.mahout.math.neighborhood.SearchQualityTest\n\nResults :\n\nFailed tests: \n  SearchSanityTest.testRemoval:166->Assert.assertEquals:494->Assert.failNotEquals:743->Assert.fail:88 Previous second neighbor should be first expected:<0.0> but was:<12.079332354796572>\n\nTests run: 734, Failures: 1, Errors: 0, Skipped: 0\n\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Mahout Build Tools ................................ SUCCESS [15.971s]\n[INFO] Apache Mahout ..................................... SUCCESS [3.131s]\n[INFO] Mahout Math ....................................... SUCCESS [1:42.383s]\n[INFO] Mahout Core ....................................... FAILURE [3:47.758s]\n[INFO] Mahout Integration ................................ SKIPPED\n[INFO] Mahout Examples ................................... SKIPPED\n[INFO] Mahout Release Package ............................ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 5:53.176s\n[INFO] Finished at: Tue Sep 17 18:04:27 UTC 2013\n[INFO] Final Memory: 48M/325M\n[INFO] ------------------------------------------------------------------------",
        "Issue Links": []
    },
    "MAHOUT-1338": {
        "Key": "MAHOUT-1338",
        "Summary": "Reduce mahout-integration transitive dependencies to avoid JAR hell, version conflicts",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sean R. Owen",
        "Created": "18/Sep/13 08:09",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "20/Sep/13 10:56",
        "Description": "mahout-integration contains bits of client and connector code for a lot of projects, like Lucene, Cassandra, MongoDB, etc. As such, its transitive dependencies in Maven pull in quite a lot. \nMost of these are unnecessary for any particular user, since probably at most one client/package is of interest. In fact, mahout-integration is not used by most users at all. \nIn the worst case, it causes actual version problems when trying to package up the transitive dependencies of something depending on Mahout.\nI suggest several changes along these lines, all of which are represented in the attached patch:\n1. Remove direct lucene-core and cassandra-all dependencies, as they are not necessary\n2. Mark all dependencies like hector, mongodb, etc as optional in Maven\n3. In fact, mark mahout-examples, mahout-buildtools and mahout-integration as optional with respect to the overall project.\n4. Bonus: update Cassandra client version to pull in slightly newer deps",
        "Issue Links": []
    },
    "MAHOUT-1339": {
        "Key": "MAHOUT-1339",
        "Summary": "Add RAT check; fix copyright headers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sean R. Owen",
        "Created": "20/Sep/13 10:57",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "24/Sep/13 12:04",
        "Description": "Many files in the project don't carry the standard license header. There is a plugin called RAT that automates these checks. The attached patch adds a suitable RAT config, and fixes the errors it identifies. (I'm not yet proposing making the check part of the build.)",
        "Issue Links": []
    },
    "MAHOUT-1340": {
        "Key": "MAHOUT-1340",
        "Summary": "Make LuceneSegment* tests work with both Hadoop 1 and 2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mike Percy",
        "Created": "23/Sep/13 22:21",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "24/Sep/13 12:05",
        "Description": "Hadoop changed a few classes to interfaces between Hadoop 1 and Hadoop 2. A couple of these classes are used in the LuceneSegment unit tests. A little bit of simple reflection abstracts these differences away and makes these tests pass under both Hadoop 1 and Hadoop 2.",
        "Issue Links": []
    },
    "MAHOUT-1341": {
        "Key": "MAHOUT-1341",
        "Summary": "Skip working directory initialization in classification 20newsgroups examples script clean task",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Implemented",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "24/Sep/13 01:38",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "24/Sep/13 15:48",
        "Description": "When running examples/bin/classify-20newsgroups.sh and choosing clean option, working directory gets initialized first (so 20newsgroups dataset gets downloaded if not already in the working directory) just to have working directory deleted. So cleaning already clean working directory will take unnecessarily long.\nFor clean task we should skip working directory initialization and just clean things up.",
        "Issue Links": []
    },
    "MAHOUT-1342": {
        "Key": "MAHOUT-1342",
        "Summary": "Exclude .git/**, Eclipse, and .patch files from Rat check",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mike Percy",
        "Created": "24/Sep/13 22:23",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "25/Sep/13 20:41",
        "Description": "Simple patch to exclude the .git directory, along with Eclipse working directory files and patch files from the Rat check so that it can be run in a developer environment with those impurities present.",
        "Issue Links": [
            "/jira/browse/RAT-107"
        ]
    },
    "MAHOUT-1343": {
        "Key": "MAHOUT-1343",
        "Summary": "JSON output format for clusterdumper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering,                                            Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Telvis Calhoun",
        "Created": "26/Sep/13 02:04",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "03/Nov/13 19:10",
        "Description": "This patch adds JSON output format to the clusterdump utility. Each cluster is represented as a JSON-encoded line. The command is something like:\n>> mahout clusterdump -d dictionary -dt text -i clusters/clusters-2-final -p clusters/clusteredPoints -n 10 -o clusterdump.json -of JSON",
        "Issue Links": []
    },
    "MAHOUT-1344": {
        "Key": "MAHOUT-1344",
        "Summary": "Self-Organizing Map algorithm (batch version)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "\u00c1lvaro P\u00e9rez Alarc\u00f3n",
        "Created": "30/Sep/13 14:29",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "09/Apr/14 09:38",
        "Description": "Good morning.\nAs part of my final year project, I have implemented a new module for Apache Mahout, implementing Kohonen's self-organizing map algorithm, in its batch version.\nThe work is already done, and I will proceed to submit a patch ASAP. It was developed over Mahout 0.8.\nThe patch includes unit tests and the algorithm was successfully used in a Hadoop cluster to cluster two big datasets. Results can be seen in this image gallery.\nThe implementation uses the generic clustering algorithms implemented in the ClusterIterator class. Minor changes were made to this and other related classes to support some of the features, without affecting the execution of other algorithms.\nThe algorithm supports convergence and the ability to resume a work at a given iteration (mainly, in order to initialize KohonenBatchClusteringPolicy with a given iteration number, althought it also affects the names of the output directories).",
        "Issue Links": []
    },
    "MAHOUT-1345": {
        "Key": "MAHOUT-1345",
        "Summary": "Enable randomised testing for all Mahout modules",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "04/Oct/13 18:34",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "01/Dec/13 17:49",
        "Description": "When enabling randomised testing for all modules I found a few tests became unstable or even fail deterministically due to lingering threads. The attached patch:\n\ndefines the randomised testing dependency in our parent pom\nre-uses said dependencies in all depending modules (makes upgrading easier as the version number needs to be changed in just one place)\nadds several code changes that fixed the failures due to lingering threads for me on my machine. I'd greatly appreciate input a) from those who wrote the respective code and b) others who ran the tests with these changes to make sure there are no other tests that suffer from the same issues.\n\nWarning: I touched quite a few bits and pieces I'm not intimately familiar with over the last few weeks  (whenever I had a few spare minutes) - second pair of eyes needed.",
        "Issue Links": []
    },
    "MAHOUT-1346": {
        "Key": "MAHOUT-1346",
        "Summary": "Spark Bindings (DRM)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "07/Oct/13 20:50",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "29/Apr/14 19:56",
        "Description": "Spark bindings for Mahout DRM. \nDRM DSL. \nDisclaimer. This will all be experimental at this point.\nThe idea is to wrap DRM by Spark RDD with support of some basic functionality, perhaps some humble beginning of Cost-based optimizer \n(0) Spark serialization support for Vector, Matrix \n(1) Bagel transposition \n(2) slim X'X\n(2a) not-so-slim X'X\n(3) blockify() (compose RDD containing vertical blocks of original input)\n(4) read/write Mahout DRM off HDFS\n(5) A'B\n...",
        "Issue Links": [
            "/jira/browse/MAHOUT-1297",
            "/jira/browse/MAHOUT-1365"
        ]
    },
    "MAHOUT-1347": {
        "Key": "MAHOUT-1347",
        "Summary": "Add Streaming K-Means clustering algorithm to examples/bin/cluster-reuters.sh",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "17/Oct/13 01:45",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "29/Nov/13 20:18",
        "Description": "Add Streaming K-Means Clustering to examples/bin/cluster_reuters.sh",
        "Issue Links": []
    },
    "MAHOUT-1348": {
        "Key": "MAHOUT-1348",
        "Summary": "Why does Mahout RecommenderJob does not support User Based Recommendation, It only Item based similarity recommendation",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Anusha R",
        "Created": "30/Oct/13 10:51",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/Apr/14 11:18",
        "Description": "Why does Mahout RecommenderJob does not support User Based similarity Recommendation, It only Item based similarity recommendation. \nIs it not possible to do in hadoop set up?\nor it reflects in any upcoming version?",
        "Issue Links": []
    },
    "MAHOUT-1349": {
        "Key": "MAHOUT-1349",
        "Summary": "Clusterdumper/loadTermDictionary crashes when highest index in (sparse) dictionary vector is larger than dictionary vector size?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alex Piggott",
        "Created": "01/Nov/13 12:59",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "08/Dec/13 23:35",
        "Description": "I'm not sure if I'm doing something wrong here, or if ClusterDumper does\nnot support my (fairly simple) use case\nI had a repository of 500K documents, for which I generated the input\nvectors and a dictionary using some custom code (not seq2sparse etc).\nI hashed the features with max size 5M (because I didn't know how many\nfeatures were in the dataset and wanted to minimize collisions).\nThe kmeans ran fine and generate sensible looking results, but when I tried\nto run ClusterDumper I got the following error:\n#bash> bin/mahout clusterdump -dt sequencefile -d\n completed/5159bba4e4b0718d03c8cf79_/EmailContentAnalytics_dict_5159bba4e4b0718d03c8cf79/part-*\n-i test-kmeans/clusters-19 -b 10 -n 10 -sp 10 -o ~/test-kmeans-out\nRunning on hadoop, using /usr/bin/hadoop and HADOOP_CONF_DIR=\nMAHOUT-JOB: /opt/mahout-distribution-0.7/mahout-examples-0.7-job.jar\n13/05/17 08:26:41 INFO common.AbstractJob: Command line arguments:\n{--dictionary=[completed/5159bba4e4b0718d03c8cf79_/EmailContentAnalytics_dict_5159bba4e4b0718d03c8cf79/part-*],\n--dictionaryType=[sequencefile],\n--distanceMeasure=[org.apache.mahout.common.distance.SquaredEuclideanDistanceMeasure],\n--endPhase=[2147483647], --input=[test-kmeans/clusters-19],\n--numWords=[10], --output=[/usr/share/tomcat6/test-kmeans-out],\n--outputFormat=[TEXT], --samplePoints=[10], --startPhase=[0],\n--substring=[10], --tempDir=[temp]}\nException in thread \"main\" java.lang.ArrayIndexOutOfBoundsException: 698948\n        at\norg.apache.mahout.clustering.AbstractCluster.formatVector(AbstractCluster.java:350)\n        at\norg.apache.mahout.clustering.AbstractCluster.asFormatString(AbstractCluster.java:306)\n        at\norg.apache.mahout.utils.clustering.ClusterDumperWriter.write(ClusterDumperWriter.java:54)\n        at\norg.apache.mahout.utils.clustering.AbstractClusterWriter.write(AbstractClusterWriter.java:169)\n        at\norg.apache.mahout.utils.clustering.AbstractClusterWriter.write(AbstractClusterWriter.java:156)\n        at\norg.apache.mahout.utils.clustering.ClusterDumper.printClusters(ClusterDumper.java:187)\n        at\norg.apache.mahout.utils.clustering.ClusterDumper.run(ClusterDumper.java:153)\n(...)\nThe error is when it tries to access the dictionary for the feature with\nindex 698948\nLooking at the dictionary loading code (\nhttp://grepcode.com/file/repo1.maven.org/maven2/org.apache.mahout/mahout-integration/0.7/org/apache/mahout/utils/vectors/VectorHelper.java#VectorHelper.loadTermDictionary%28java.io.File%29\n\nchecked 0.8 and it hasn't changed)\n\nIt looks like the dictionary array is sized for the number of unique\nkeywords, not the highest index:\n  OpenObjectIntHashMap dict = new OpenObjectIntHashMap();\n//...\n  String [] dictionary = new String[dict.size()];\nAfter I ran my custom dictionary/feature generation code I discovered I\nonly had 517,327 unique features, therefore it is not surprising it would\ndie on an index >= 517327 (though I don't understand why it didn't die when trying to load the dictionary file)\nIs there any reason why the VectorHelper code should not create a\ndictionary array that has size the highest index read from the dictionary\nsequence file (which can be easily calculated during the preceding loop)?\nOr am I misunderstanding something?\nIt worked fine when I reduced the hash size to be <= than the total number\nof features, but this is not desirable in general (for me) since I don't\nknow the number of features before I run the job (and if I guess too high\nthen ClusterDumper crashes)\nAlex Piggott\nIKANOW",
        "Issue Links": []
    },
    "MAHOUT-1350": {
        "Key": "MAHOUT-1350",
        "Summary": "Bean Utils JarClassLoader Warnings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "David Williams",
        "Created": "01/Nov/13 20:11",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "28/Nov/13 21:00",
        "Description": "Hi all,\nI am trying to embed a user based recommender in a web service using embedded jetty, and spring 3.  However, including the mahout libraries leads to this collision.  It means I CANNOT use Mahout in its current implementation.\n\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/ArrayStack.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$Values.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$CollectionView$CollectionViewIterator.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$1.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/BufferUnderflowException.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$KeySet.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$CollectionView.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$EntrySet.class in lib/commons-beanutils-1.7.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/BasicDynaBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/BasicDynaClass.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/BeanAccessLanguageException.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/BeanUtils.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/BeanUtilsBean$1.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/BeanUtilsBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/ConstructorUtils.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/ContextClassLoaderLocal.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/ConversionException.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/ConvertUtils.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/ConvertUtilsBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/Converter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/ConvertingWrapDynaBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/DynaBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/DynaClass.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/DynaProperty.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/JDBCDynaClass.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/LazyDynaBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/LazyDynaClass.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/LazyDynaMap.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/MappedPropertyDescriptor.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/MethodUtils$MethodDescriptor.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/MethodUtils.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/MutableDynaClass.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/NestedNullException.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/PropertyUtils.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/PropertyUtilsBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/ResultSetDynaClass.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/ResultSetIterator.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/RowSetDynaClass.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/WrapDynaBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/WrapDynaClass.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/AbstractArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/BigDecimalConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/BigIntegerConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/BooleanArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/BooleanConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/ByteArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/ByteConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/CharacterArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/CharacterConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/ClassConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/DoubleArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/DoubleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/FileConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/FloatArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/FloatConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/IntegerArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/IntegerConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/LongArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/LongConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/ShortArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/ShortConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/SqlDateConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/SqlTimeConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/SqlTimestampConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/StringArrayConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/StringConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/converters/URLConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/BaseLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/LocaleBeanUtils$Descriptor.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/LocaleBeanUtils.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/LocaleBeanUtilsBean$1.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/LocaleBeanUtilsBean$Descriptor.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/LocaleBeanUtilsBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/LocaleConvertUtils.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/LocaleConvertUtilsBean.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/LocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/BigDecimalLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/BigIntegerLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/ByteLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/DateLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/DecimalLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/DoubleLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/FloatLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/IntegerLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/LongLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/ShortLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/SqlDateLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/SqlTimeLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/SqlTimestampLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/beanutils/locale/converters/StringLocaleConverter.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-beanutils-1.7.0.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/ArrayStack.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/Buffer.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/BufferUnderflowException.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$1.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$CollectionView$CollectionViewIterator.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$CollectionView.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$EntrySet.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$KeySet.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap$Values.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)\nJarClassLoader: Warning: org/apache/commons/collections/FastHashMap.class in lib/commons-beanutils-core-1.8.0.jar is hidden by lib/commons-collections-3.2.1.jar (with different bytecode)",
        "Issue Links": []
    },
    "MAHOUT-1351": {
        "Key": "MAHOUT-1351",
        "Summary": "Adding DenseVector support to AbstractCluster",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dave DeBarr",
        "Created": "05/Nov/13 22:54",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "17/Nov/13 05:45",
        "Description": "This improvement reduces runtime by 80% when performing k-means clustering of Scale Invariant Feature Transform (SIFT) descriptors to derive visual words for computer vision.  Unlike sparse document vectors, SIFT descriptors are dense.  This improvement involves updating the org.apache.mahout.clustering.AbstractCluster(Vector point, int id2) constructor to use \"point.clone()\" instead of \"new RandomAccessSparseVector(point)\" for creating the centroid.  Also added testKMeansSeqJobDenseVector() test for DenseVector processing.",
        "Issue Links": []
    },
    "MAHOUT-1352": {
        "Key": "MAHOUT-1352",
        "Summary": "Option to change RecommenderJob output format",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Pat Ferrel",
        "Created": "06/Nov/13 23:11",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "08/Nov/13 13:32",
        "Description": "Can we add an option to output a SequenceFile? I hard coded it to allow the Solr-recommender to work with the output file but it is probably best put in an option.\n      //extract out the recommendations\n     Job aggregateAndRecommend = prepareJob(\n             new Path(aggregateAndRecommendInput), outputPath, SequenceFileInputFormat.class,\n             PartialMultiplyMapper.class, VarLongWritable.class, PrefAndSimilarityColumnWritable.class,\n             AggregateAndRecommendReducer.class, VarLongWritable.class, RecommendedItemsWritable.class,\n             SequenceFileOutputFormat.class); <-- text format in Mahout trunk\nShould be able to choose format class from the params.",
        "Issue Links": []
    },
    "MAHOUT-1353": {
        "Key": "MAHOUT-1353",
        "Summary": "Visibility of preparePreferenceMatrix directory location",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Pat Ferrel",
        "Created": "06/Nov/13 23:13",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "07/Nov/13 14:59",
        "Description": "The Solr-recommender needs to find where the RecommenderJob is putting it\u2019s output. \nMahout 0.8 RecommenderJob code was:\n   public static final String DEFAULT_PREPARE_DIR = \"preparePreferenceMatrix\u201d;\nMahout 0.9 RecommenderJob code just puts \u201cpreparePreferenceMatrix\u201d inline in the code:\n   Path prepPath = getTempPath(\"preparePreferenceMatrix\");\nThis change to Mahout 0.9 works:\n   public static final String DEFAULT_PREPARE_DIR = \"preparePreferenceMatrix\u201d;\nand\n   Path prepPath = getTempPath(DEFAULT_PREPARE_DIR);\nYou could also make this a getter method on the RecommenderJob Class instead of using a public constant.",
        "Issue Links": []
    },
    "MAHOUT-1354": {
        "Key": "MAHOUT-1354",
        "Summary": "Mahout Support for Hadoop 2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "07/Nov/13 19:23",
        "Updated": "18/Apr/14 19:20",
        "Resolved": "18/Dec/13 03:47",
        "Description": "Mahout support for Hadoop , now that Hadoop 2 is official.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1329"
        ]
    },
    "MAHOUT-1355": {
        "Key": "MAHOUT-1355",
        "Summary": "Frequent Pattern Mining algorithms for Mahout",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sandy Moens",
        "Created": "13/Nov/13 16:57",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/Apr/14 11:30",
        "Description": "We implemented frequent pattern mining algorithms for Hadoop and adapted them to Mahout. We used \"PFP\" (now deprecated) as a benchmark and these implementations perform better in terms of speed and memory footprint. The details of the implementations can be found in the paper Frequent Pattern Mining for BigData ( http://adrem.ua.ac.be/bigfim )\nWe have been maintaining the project for a while in GitLab ( https://gitlab.com/adrem/bigfim ). Documentation for adaptation ( Readme-Mahout.md ) and usage in mahout ( Mahout-wiki.md ) can be found there.\nWe are open to any modification and/or improvement requests to make it more worthwhile for the Mahout project. We, as the research group, volunteer to maintain FPM algorithms as well.",
        "Issue Links": []
    },
    "MAHOUT-1356": {
        "Key": "MAHOUT-1356",
        "Summary": "Ensure unit tests fail fast when writing outside mvn target directory",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "13/Nov/13 21:43",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "22/Mar/14 03:17",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1357": {
        "Key": "MAHOUT-1357",
        "Summary": "InteractionValueEncoder produces wrong traceDictionary entries",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Johannes Schulte",
        "Created": "15/Nov/13 14:23",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "15/Nov/13 14:48",
        "Description": "In the trace code the byte values of the terms being hashed are not converted back to string but just concatenated in their raw form with Arrays.asString()\nThis makes the reverse engineering even harder!\nFix is to just create new string, see patch attached.",
        "Issue Links": []
    },
    "MAHOUT-1358": {
        "Key": "MAHOUT-1358",
        "Summary": "StreamingKMeansThread throws IllegalArgumentException when REDUCE_STREAMING_KMEANS is set to true",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "18/Nov/13 07:58",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "18/Nov/13 17:55",
        "Description": "Running StreamingKMeans Clustering with REDUCE_STREAMING_KMEANS = true and when no estimatedDistanceCutoff is specified, throws the following error\n\n\njava.lang.IllegalArgumentException: Must have nonzero number of training and test vectors. Asked for %.1f %% of %d vectors for test [10.000000149011612, 0]\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:120)\n\tat org.apache.mahout.clustering.streaming.cluster.BallKMeans.splitTrainTest(BallKMeans.java:176)\n\tat org.apache.mahout.clustering.streaming.cluster.BallKMeans.cluster(BallKMeans.java:192)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.getBestCentroids(StreamingKMeansReducer.java:107)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:73)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:37)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:177)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n\n\n\nThe issue is caused by the following code in StreamingKMeansThread.call()\n\n    Iterator<Centroid> datapointsIterator = datapoints.iterator();\n    if (estimateDistanceCutoff == StreamingKMeansDriver.INVALID_DISTANCE_CUTOFF) {\n      List<Centroid> estimatePoints = Lists.newArrayListWithExpectedSize(NUM_ESTIMATE_POINTS);\n      while (datapointsIterator.hasNext() && estimatePoints.size() < NUM_ESTIMATE_POINTS) {\n        estimatePoints.add(datapointsIterator.next());\n      }\n      estimateDistanceCutoff = ClusteringUtils.estimateDistanceCutoff(estimatePoints, searcher.getDistanceMeasure());\n    }\n\n    StreamingKMeans clusterer = new StreamingKMeans(searcher, numClusters, estimateDistanceCutoff);\n    while (datapointsIterator.hasNext()) {\n      clusterer.cluster(datapointsIterator.next());\n    }\n\n\nThe code is using the same iterator twice, and it fails on the second use for obvious reasons.",
        "Issue Links": []
    },
    "MAHOUT-1359": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1360": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1361": {
        "Key": "MAHOUT-1361",
        "Summary": "Online algorithm for computing accurate Quantiles using 1-D clustering",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "18/Nov/13 08:36",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "27/Nov/13 21:02",
        "Description": "Implementation of Ted Dunning's paper and initial work on this subject. See https://github.com/tdunning/t-digest/blob/master/docs/theory/t-digest-paper/histo.pdf for the paper.\nAn on-line algorithm for computing approximations of rank-based statistics that allows controllable accuracy. This algorithm can also be used to compute hybrid statistics such as trimmed means in addition to computing arbitrary quantiles.",
        "Issue Links": [
            "/jira/browse/SOLR-5302"
        ]
    },
    "MAHOUT-1362": {
        "Key": "MAHOUT-1362",
        "Summary": "Remove examples/bin/build-reuters.sh from 0.9",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "18/Nov/13 19:21",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "18/Nov/13 19:24",
        "Description": "Remove examples/bin/build-reuters.sh as its been replaced by cluster-reuters.sh",
        "Issue Links": []
    },
    "MAHOUT-1363": {
        "Key": "MAHOUT-1363",
        "Summary": "Rebase packages in mahout-scala",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "25/Nov/13 20:45",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "25/Nov/13 21:22",
        "Description": "It has occurred to me that in my commit of mahout-scala stuff, i haven't rebased packages onto o.a.m... as has been discussed. \nit also has occurred to me that putting that stuff into o.a.m.math in this case may create unwelcome interference between java and scala stuff. \nSo I am moving scala math DSL stuff into 0.a.m.math.scalabindings package. It is awfully awkward compared to just \"mahout.math\" scala style package it bears now, but i guess modern IDE tools make it no problem to import.",
        "Issue Links": []
    },
    "MAHOUT-1364": {
        "Key": "MAHOUT-1364",
        "Summary": "Upgrade Mahout codebase to Lucene 4.6",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Classification,                                            CLI,                                            Clustering,                                            Examples,                                            Integration",
        "Assignee": "Frank Scholten",
        "Reporter": "Suneel Marthi",
        "Created": "25/Nov/13 22:04",
        "Updated": "03/Feb/14 07:47",
        "Resolved": "18/Dec/13 18:00",
        "Description": "Parallel Randomized tests (using Carrot RandomizedRunner) fail on Mac OS for code that invokes Lucene API, see the discussion in M-1345.  The fix is to upgrade to a Lucene version > 4.3.1 (which is the present Lucene version in Mahout trunk).",
        "Issue Links": []
    },
    "MAHOUT-1365": {
        "Key": "MAHOUT-1365",
        "Summary": "Weighted ALS-WR iterator for Spark",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "26/Nov/13 00:03",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "30/Jun/14 21:53",
        "Description": "Given preference P and confidence C distributed sparse matrices, compute ALS-WR solution for implicit feedback (Spark Bagel version).\nFollowing Hu-Koren-Volynsky method (stripping off any concrete methodology to build C matrix), with parameterized test for convergence.\nThe computational scheme is following ALS-WR method (which should be slightly more efficient for sparser inputs). \nThe best performance will be achieved if non-sparse anomalies prefilitered (eliminated) (such as an anomalously active user which doesn't represent typical user anyway).\nthe work is going here https://github.com/dlyubimov/mahout-commits/tree/dev-0.9.x-scala. I am porting away our (A1) implementation so there are a few issues associated with that.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1346",
            "/jira/browse/MAHOUT-1583",
            "/jira/browse/MAHOUT-1566"
        ]
    },
    "MAHOUT-1366": {
        "Key": "MAHOUT-1366",
        "Summary": "Please delete old releases from mirroring system",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.4,                                            0.5,                                            0.6,                                            0.7",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebb",
        "Created": "26/Nov/13 17:57",
        "Updated": "16/Feb/14 22:40",
        "Resolved": "03/Feb/14 06:46",
        "Description": "To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\nPlease can you remove all non-current releases?\nThanks!\n[Note that older releases are always available from the ASF archive server]\n[1] http://www.apache.org/dev/release.html#when-to-archive",
        "Issue Links": []
    },
    "MAHOUT-1367": {
        "Key": "MAHOUT-1367",
        "Summary": "WikipediaXmlSplitter --> Exception in thread \"main\" java.lang.NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "ollagnier",
        "Created": "29/Nov/13 14:20",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "01/Dec/13 12:23",
        "Description": "Hi !\nWhen I run this command : $MAHOUT_HOME/bin/mahout org.apache.mahout.text.wikipedia.WikipediaXmlSplitter -d frwiki-latest-pages-articles.xml.bz2 -o wikipedia-xml-chunks -c 100\nI have this error :\nMAHOUT-JOB: /home/ollagnier/Documents/tools/mahout-distribution-0.8/bin/trunk/examples/target/mahout-examples-0.9-SNAPSHOT-job.jar\n13/11/29 14:51:08 WARN driver.MahoutDriver: No org.apache.mahout.text.wikipedia.WikipediaXmlSplitter.props found on classpath, will use command-line arguments only\n13/11/29 14:51:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nException in thread \"main\" java.lang.NullPointerException\n\tat org.apache.hadoop.io.compress.bzip2.Bzip2Factory.isNativeBzip2Loaded(Bzip2Factory.java:54)\n\tat org.apache.hadoop.io.compress.bzip2.Bzip2Factory.getBzip2Decompressor(Bzip2Factory.java:131)\n\tat org.apache.hadoop.io.compress.BZip2Codec.createDecompressor(BZip2Codec.java:250)\n\tat org.apache.hadoop.io.compress.BZip2Codec.createInputStream(BZip2Codec.java:156)\n\tat org.apache.mahout.text.wikipedia.WikipediaXmlSplitter.main(WikipediaXmlSplitter.java:190)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nAnd I have a little trouble seeing where it comes from\nThank you for your help",
        "Issue Links": []
    },
    "MAHOUT-1368": {
        "Key": "MAHOUT-1368",
        "Summary": "Convert OnlineSummarizer to use the new TDigest",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "02/Dec/13 06:51",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "04/Dec/13 21:03",
        "Description": "The new TDigest provides better accuracy for quartile estimation as well as producing any other quantile you might like.  The current quartile estimation of the OnlineSummarizer fails for highly skewed distributions and can't really be extended to provide other quantiles.  The TDigest handles all of this.",
        "Issue Links": []
    },
    "MAHOUT-1369": {
        "Key": "MAHOUT-1369",
        "Summary": "Why does theta normalization for naive bayes classification commented out?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "utku yaman",
        "Created": "02/Dec/13 12:27",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "15/Apr/14 03:16",
        "Description": "TrainNaiveBayesJob line 155:158\nand\nBayesUtils line 86:93\nare commented out and these lines are for theta normalization for bayes.\nwhat is the problem with the code and is there a plan for correcting these methods.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1504"
        ]
    },
    "MAHOUT-1370": {
        "Key": "MAHOUT-1370",
        "Summary": "Vectordump doesn't write to output file in MapReduce Mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "04/Dec/13 00:31",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "04/Dec/13 00:37",
        "Description": "When trying to run to run Vectordump in MR mode, get a \nFileNotFoundException: No such File or Directory.\n\n\n13/12/03 19:29:22 INFO vectors.VectorDumper: Output file: /tmp/mahout-work-user/reuters-lda/vectordump\nException in thread \"main\" java.io.FileNotFoundException: /tmp/mahout-work-user/reuters-lda/vectordump (No such file or directory)\n\tat java.io.FileOutputStream.open(Native Method)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:194)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:145)\n\tat com.google.common.io.Files.newWriter(Files.java:101)\n\tat org.apache.mahout.utils.vectors.VectorDumper.run(VectorDumper.java:153)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.mahout.utils.vectors.VectorDumper.main(VectorDumper.java:262)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:160)",
        "Issue Links": []
    },
    "MAHOUT-1371": {
        "Key": "MAHOUT-1371",
        "Summary": "Arff loader can misinterprete nominals with integer, real or string",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mansur Iqbal",
        "Created": "05/Dec/13 00:15",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "15/Dec/13 18:31",
        "Description": "If the nominal values contain a value like integer, real or string it will be misinterpreted as such instead of nominal.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1372"
        ]
    },
    "MAHOUT-1372": {
        "Key": "MAHOUT-1371 Arff loader can misinterprete nominals with integer, real or string",
        "Summary": "Arff loader treats commas inside quotes and escaped quotes wrong",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mansur Iqbal",
        "Created": "05/Dec/13 00:20",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "05/Dec/13 09:53",
        "Description": "If you have commas in nominal values or escaped quotes, the arff loader parses the files wrong.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1371"
        ]
    },
    "MAHOUT-1373": {
        "Key": "MAHOUT-1373",
        "Summary": "Ability to handle non numeric itemid's",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Romit Singhai",
        "Created": "07/Dec/13 05:47",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "13/Apr/14 11:17",
        "Description": "Current command line option for recommendation  does not handle non numeric itemid's.",
        "Issue Links": []
    },
    "MAHOUT-1374": {
        "Key": "MAHOUT-1374",
        "Summary": "Ability to provide input file with userid, itemid pair",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Romit Singhai",
        "Created": "07/Dec/13 06:03",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "05/Apr/14 11:23",
        "Description": "Currently while running the itembased recommender from the command line, user can get recommendation for a particular set of users by using the userFile option or for particular set of itemId's by using the itemFile option. Using these option together seems to provide recommendations for the cross product for the two inputs. We need an option say userItemFile where a coma separated userid,itemid pair is provide on each line as input and output will be userid,itemid,recommendation. This functionality will be very useful for lot users.",
        "Issue Links": []
    },
    "MAHOUT-1375": {
        "Key": "MAHOUT-1375",
        "Summary": "Apache Mahout",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "kaan can",
        "Created": "09/Dec/13 22:38",
        "Updated": "03/Feb/14 07:48",
        "Resolved": "10/Dec/13 19:09",
        "Description": "Hello,\n Firstly, thank you for spending time in read my letter! \n well,my question is :\n1) Which tools are used in Carrot2?\n2) Carrot2 is provide suitable for supervised learning or unsupervised?\n3) Which preprocessing methods tools in Carrot2?\nKind regards",
        "Issue Links": []
    },
    "MAHOUT-1376": {
        "Key": "MAHOUT-1376",
        "Summary": "when mahout train data, there is Task Id : attempt_201312031842_0751_m_000000_0, Status : FAILED java.lang.IllegalArgumentException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "wangqiaoshi",
        "Created": "10/Dec/13 02:20",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "11/Dec/13 11:20",
        "Description": "vm001:/usr/local/hadoop/mahout-distribution-0.8 # ./bin/mahout trainnb -i /tmp/mahout-work-root/20news-train-vectors -el -o /tmp/mahout-work-root/model -li /tmp/mahout-work-root/labelindex -ow -c\nRunning on hadoop, using /usr/local/hadoop/hadoop-0.20.2/bin/hadoop and HADOOP_CONF_DIR=\nMAHOUT-JOB: /usr/local/hadoop/mahout-distribution-0.8/mahout-examples-0.8-job.jar\n13/12/10 10:29:56 WARN driver.MahoutDriver: No trainnb.props found on classpath, will use command-line arguments only\n13/12/10 10:29:56 INFO common.AbstractJob: Command line arguments: {--alphaI=[1.0], --endPhase=[2147483647], --extractLabels=null, --input=[/tmp/mahout-work-root/20news-train-vectors], --labelIndex=[/tmp/mahout-work-root/labelindex], --output=[/tmp/mahout-work-root/model], --overwrite=null, --startPhase=[0], --tempDir=[temp], --trainComplementary=null}\n13/12/10 10:29:56 INFO common.HadoopUtil: Deleting temp\n13/12/10 10:29:57 INFO util.NativeCodeLoader: Loaded the native-hadoop library\n13/12/10 10:29:57 INFO zlib.ZlibFactory: Successfully loaded & initialized native-zlib library\n13/12/10 10:29:57 INFO compress.CodecPool: Got brand-new decompressor\n13/12/10 10:30:00 INFO input.FileInputFormat: Total input paths to process : 1\n13/12/10 10:30:01 INFO mapred.JobClient: Running job: job_201312031842_0750\n13/12/10 10:30:02 INFO mapred.JobClient:  map 0% reduce 0%\n13/12/10 10:30:18 INFO mapred.JobClient:  map 100% reduce 0%\n13/12/10 10:30:30 INFO mapred.JobClient:  map 100% reduce 100%\n13/12/10 10:30:35 INFO mapred.JobClient: Job complete: job_201312031842_0750\n13/12/10 10:30:35 INFO mapred.JobClient: Counters: 29\n13/12/10 10:30:35 INFO mapred.JobClient:   Job Counters \n13/12/10 10:30:35 INFO mapred.JobClient:     Launched reduce tasks=1\n13/12/10 10:30:35 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=12445\n13/12/10 10:30:35 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Rack-local map tasks=1\n13/12/10 10:30:35 INFO mapred.JobClient:     Launched map tasks=1\n13/12/10 10:30:35 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=10355\n13/12/10 10:30:35 INFO mapred.JobClient:   File Output Format Counters \n13/12/10 10:30:35 INFO mapred.JobClient:     Bytes Written=97\n13/12/10 10:30:35 INFO mapred.JobClient:   FileSystemCounters\n13/12/10 10:30:35 INFO mapred.JobClient:     FILE_BYTES_READ=119\n13/12/10 10:30:35 INFO mapred.JobClient:     HDFS_BYTES_READ=270\n13/12/10 10:30:35 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=45827\n13/12/10 10:30:35 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=97\n13/12/10 10:30:35 INFO mapred.JobClient:   File Input Format Counters \n13/12/10 10:30:35 INFO mapred.JobClient:     Bytes Read=133\n13/12/10 10:30:35 INFO mapred.JobClient:   Map-Reduce Framework\n13/12/10 10:30:35 INFO mapred.JobClient:     Map output materialized bytes=14\n13/12/10 10:30:35 INFO mapred.JobClient:     Map input records=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Reduce shuffle bytes=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Spilled Records=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Map output bytes=0\n13/12/10 10:30:35 INFO mapred.JobClient:     CPU time spent (ms)=2080\n13/12/10 10:30:35 INFO mapred.JobClient:     Total committed heap usage (bytes)=1016594432\n13/12/10 10:30:35 INFO mapred.JobClient:     Combine input records=0\n13/12/10 10:30:35 INFO mapred.JobClient:     SPLIT_RAW_BYTES=137\n13/12/10 10:30:35 INFO mapred.JobClient:     Reduce input records=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Reduce input groups=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Combine output records=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Physical memory (bytes) snapshot=313008128\n13/12/10 10:30:35 INFO mapred.JobClient:     Reduce output records=0\n13/12/10 10:30:35 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=2980098048\n13/12/10 10:30:35 INFO mapred.JobClient:     Map output records=0\n13/12/10 10:30:38 INFO input.FileInputFormat: Total input paths to process : 1\n13/12/10 10:30:38 INFO mapred.JobClient: Running job: job_201312031842_0751\n13/12/10 10:30:39 INFO mapred.JobClient:  map 0% reduce 0%\n13/12/10 10:30:55 INFO mapred.JobClient: Task Id : attempt_201312031842_0751_m_000000_0, Status : FAILED\njava.lang.IllegalArgumentException\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76)\n        at org.apache.mahout.classifier.naivebayes.training.WeightsMapper.setup(WeightsMapper.java:44)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n13/12/10 10:31:04 INFO mapred.JobClient: Task Id : attempt_201312031842_0751_m_000000_1, Status : FAILED\njava.lang.IllegalArgumentException\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76)\n        at org.apache.mahout.classifier.naivebayes.training.WeightsMapper.setup(WeightsMapper.java:44)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n13/12/10 10:31:13 INFO mapred.JobClient: Task Id : attempt_201312031842_0751_m_000000_2, Status : FAILED\njava.lang.IllegalArgumentException\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:76)\n        at org.apache.mahout.classifier.naivebayes.training.WeightsMapper.setup(WeightsMapper.java:44)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n13/12/10 10:31:28 INFO mapred.JobClient: Job complete: job_201312031842_0751\n13/12/10 10:31:28 INFO mapred.JobClient: Counters: 8\n13/12/10 10:31:28 INFO mapred.JobClient:   Job Counters \n13/12/10 10:31:28 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=26279\n13/12/10 10:31:28 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0\n13/12/10 10:31:28 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0\n13/12/10 10:31:28 INFO mapred.JobClient:     Rack-local map tasks=3\n13/12/10 10:31:28 INFO mapred.JobClient:     Launched map tasks=4\n13/12/10 10:31:28 INFO mapred.JobClient:     Data-local map tasks=1\n13/12/10 10:31:28 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0\n13/12/10 10:31:28 INFO mapred.JobClient:     Failed map tasks=1\n13/12/10 10:31:28 INFO driver.MahoutDriver: Program took 92707 ms (Minutes: 1.5451166666666667)",
        "Issue Links": []
    },
    "MAHOUT-1377": {
        "Key": "MAHOUT-1377",
        "Summary": "Exclude JUnit.jar from tarball",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sergey Svinarchuk",
        "Created": "13/Dec/13 12:13",
        "Updated": "03/Feb/14 07:50",
        "Resolved": "15/Dec/13 13:17",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1378": {
        "Key": "MAHOUT-1378",
        "Summary": "Running Random Forest with Ignored features fails when loading feature descriptor from JSON file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "sam wu",
        "Created": "15/Dec/13 04:52",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "15/Dec/13 12:56",
        "Description": "Running Random Forest with Ignored features fails when loading feature descriptor from JSON file.\nin Dataset.java , fromJSON(String json) function\nline 400\noriginal: ------  nominalValues[i] = array; \nshould be change to \n nominalValues[i - ignored.size()] = array;\n// put array in ignore-feature filtered index",
        "Issue Links": []
    },
    "MAHOUT-1379": {
        "Key": "MAHOUT-1379",
        "Summary": "ClusterQualitySummarizer fails with the new T-Digest for clusters with 1 data point",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "15/Dec/13 06:15",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "15/Dec/13 07:47",
        "Description": "ClusterQualitySummarizer (with the new t-digest) fails if a cluster has only a single data point. The issue is the call to OnlineSummarizer.getQuartile() expects > 1 point to be available in the cluster.",
        "Issue Links": []
    },
    "MAHOUT-1380": {
        "Key": "MAHOUT-1380",
        "Summary": "Streaming KMeans fails when executed in Sequential Mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "15/Dec/13 09:17",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "15/Dec/13 09:23",
        "Description": "Streaming KMeans fails when executed in Sequential mode because it presently doesn't ignore 'logsCRCFilter' (in sequential execution).\n\nINFO: Starting StreamingKMeans clustering for vectors in /tmp/mahout-work/reuters-out-seqdir-sparse-streamingkmeans/tfidf-vectors; results are output to /tmp/mahout-work/reuters-streamingkmeans\nDec 15, 2013 4:11:27 AM org.slf4j.impl.JCLLoggerAdapter info\nINFO: Finished running Mappers\nException in thread \"main\" java.util.concurrent.ExecutionException: java.lang.IllegalStateException: file:/tmp/mahout-work/reuters-out-seqdir-sparse-streamingkmeans/tfidf-vectors/_SUCCESS\n\tat java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:83)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.runSequentially(StreamingKMeansDriver.java:436)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.run(StreamingKMeansDriver.java:417)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.run(StreamingKMeansDriver.java:239)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.main(StreamingKMeansDriver.java:492)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\nCaused by: java.lang.IllegalStateException: file:/tmp/mahout-work/reuters-out-seqdir-sparse-streamingkmeans/tfidf-vectors/_SUCCESS\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable.iterator(SequenceFileValueIterable.java:62)\n\tat com.google.common.collect.Iterables$8.iterator(Iterables.java:713)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:62)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:37)\n\tat java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:138)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n\tat java.lang.Thread.run(Thread.java:695)\nCaused by: java.io.EOFException\n\tat java.io.DataInputStream.readFully(DataInputStream.java:180)\n\tat java.io.DataInputStream.readFully(DataInputStream.java:152)\n\tat org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1512)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1490)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1479)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1474)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator.<init>(SequenceFileValueIterator.java:56)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterable.iterator(SequenceFileValueIterable.java:60)\n\t... 8 more",
        "Issue Links": []
    },
    "MAHOUT-1381": {
        "Key": "MAHOUT-1381",
        "Summary": "Download page must not link to snapshots / nightly builds",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sebb",
        "Created": "16/Dec/13 16:45",
        "Updated": "14/Jan/14 17:38",
        "Resolved": "18/Dec/13 20:56",
        "Description": "Nightly builds / snapshots which are not formal releases should not be linked from the main download page.\nSuch builds have not been voted on and should only be used by developers who should be made aware that the code is without any guarantees,\nNightly builds are not formal ASF releases, and must not be promoted to the general public.\nSee [1] second para. The second sentence states:\n\"Do not include any links on the project website that might encourage non-developers to download and use nightly builds, snapshots, release candidates, or any other similar package.\"\n[1] http://www.apache.org/dev/release.html#what",
        "Issue Links": [
            "/jira/browse/MAHOUT-1305"
        ]
    },
    "MAHOUT-1382": {
        "Key": "MAHOUT-1382",
        "Summary": "Upgrade Mahout third party jars for 0.9 Release",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "18/Dec/13 03:56",
        "Updated": "19/Apr/14 04:16",
        "Resolved": "18/Dec/13 04:09",
        "Description": "a) Upgrade Guava to Guava 16.0\nb) Upgrade Carrot Randomized-runner to 2.0.15",
        "Issue Links": []
    },
    "MAHOUT-1383": {
        "Key": "MAHOUT-1383",
        "Summary": "Download link on Mahout main page still points to Confluence",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "19/Dec/13 15:04",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "20/Dec/13 07:18",
        "Description": "The download button on the Mahout main page (and all sub-pages as it is part of the template) still points to the Confluence Wiki page. Need to re-direct this to the new, fixed CMS page.\nCode lives in svn under site/mahout-cms/templates/standard.html",
        "Issue Links": []
    },
    "MAHOUT-1384": {
        "Key": "MAHOUT-1384",
        "Summary": "Executing the MR version of Naive Bayes/CNB of classify_20newgroups.sh fails in seqdirectory step.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "20/Dec/13 08:31",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "20/Dec/13 08:45",
        "Description": "Executing the MR version of Naive Bayes/CNB of classify_20newgroups.sh fails.  This is because the example files are not copied to HDFS for the MR version (like what's presently being done in cluster-reuters.sh).",
        "Issue Links": []
    },
    "MAHOUT-1385": {
        "Key": "MAHOUT-1385",
        "Summary": "Caching Encoders don't cache",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Johannes Schulte",
        "Created": "20/Dec/13 14:23",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/May/14 16:52",
        "Description": "The Caching... line of encoders contains code of caching the hash code terms added to the vector. However, the method \"hashForProbe\" inside this classes is never called as the signature has String for the parameter original form (instead of byte[] like other encoders).\nChanging this to byte[] however would lose the java String internal caching of the Strings hash code , that is used as a key in the cache map, triggering another hash code calculation.",
        "Issue Links": []
    },
    "MAHOUT-1386": {
        "Key": "MAHOUT-1386",
        "Summary": "Create a list of all past board reports in CMS site (or link to them)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Isabel Drost-Fromm",
        "Created": "22/Dec/13 19:01",
        "Updated": "03/Feb/14 08:05",
        "Resolved": "06/Jan/14 15:15",
        "Description": "Some of our past reports are in the mailing list archives, some in Confluence, some I don't know where. For future reference and in order for newcomers to know where we are coming from there should be one canonical go-to page for Mahout board reports.",
        "Issue Links": []
    },
    "MAHOUT-1387": {
        "Key": "MAHOUT-1387",
        "Summary": "Create page for release notes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "23/Dec/13 17:45",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/Apr/14 09:40",
        "Description": "Starting 0.6 our release notes are published on our main web page - interleaved with other news items.\nFor reference it would be good to have one canonical go-to page for past release notes on our main Apache CMS powered web page.",
        "Issue Links": []
    },
    "MAHOUT-1388": {
        "Key": "MAHOUT-1388",
        "Summary": "Add command line support and logging for MLP",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Yexi Jiang",
        "Created": "24/Dec/13 03:52",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "18/May/14 21:04",
        "Description": "The user should have the ability to run the Perceptron from the command line.\nThere are two programs to execute MLP, the training and labeling. The first one takes the data as input and outputs the model, the second one takes the model and unlabeled data as input and outputs the results.\nThe parameters for training are as follows:\n------------------------------------------------\n--input -i (input data)\n--skipHeader -sk // whether to skip the first row, this parameter is optional\n--labels -labels // the labels of the instances, separated by whitespace. Take the iris dataset for example, the labels are 'setosa versicolor virginica'.\n--model -mo  // in training mode, this is the location to store the model (if the specified location has an existing model, it will update the model through incremental learning), in labeling mode, this is the location to store the result\n--update -u // whether to incremental update the model, if this parameter is not given, train the model from scratch\n--output -o           // this is only useful in labeling mode\n--layersize -ls (no. of units per hidden layer) // use whitespace separated number to indicate the number of neurons in each layer (including input layer and output layer), e.g. '5 3 2'.\n--squashingFunction -sf // currently only supports Sigmoid\n--momentum -m \n--learningrate -l\n--regularizationweight -r\n--costfunction -cf   // the type of cost function,\n------------------------------------------------\nFor example, train a 3-layer (including input, hidden, and output) MLP with 0.1 learning rate, 0.1 momentum rate, and 0.01 regularization weight, the parameter would be:\nmlp -i /tmp/training-data.csv -labels setosa versicolor virginica -o /tmp/model.model -ls 5,3,1 -l 0.1 -m 0.1 -r 0.01\nThis command would read the training data from /tmp/training-data.csv and write the trained model to /tmp/model.model.\nThe parameters for labeling is as follows:\n-------------------------------------------------------------\n--input -i // input file path\n--columnRange -cr // the range of column used for feature, start from 0 and separated by whitespace, e.g. 0 5\n--format -f // the format of input file, currently only supports csv\n--model -mo // the file path of the model\n--output -o // the output path for the results\n-------------------------------------------------------------\nIf a user need to use an existing model, it will use the following command:\nmlp -i /tmp/unlabel-data.csv -m /tmp/model.model -o /tmp/label-result\nMoreover, we should be providing default values if the user does not specify any.",
        "Issue Links": []
    },
    "MAHOUT-1389": {
        "Key": "MAHOUT-1389",
        "Summary": "Complementary Naive Bayes Classifier not getting called when \"-c\" option is activated",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Gouri Sankar Majumder",
        "Created": "30/Dec/13 08:30",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "30/Dec/13 10:57",
        "Description": "When run trainnb and testnb with \"-c\" option, Complementary Naive bayes Classifier not getting called.",
        "Issue Links": []
    },
    "MAHOUT-1390": {
        "Key": "MAHOUT-1390",
        "Summary": "SVD hangs for certain inputs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "02/Jan/14 06:03",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "03/Jan/14 04:21",
        "Description": "For certain inputs, the SingularValueDecomposition implementation that we have doesn't detect that it has effectively converged and runs into an infinite loop.\nLuckily, there is a fix that has been added to the Jama implementation that our SVD is ultimately based on and that fix works for our problem.",
        "Issue Links": []
    },
    "MAHOUT-1391": {
        "Key": "MAHOUT-1391",
        "Summary": "Possibility to disable confusion matrix in naive bayes",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Mansur Iqbal",
        "Created": "08/Jan/14 09:34",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/Apr/14 14:49",
        "Description": "Sometimes confusion matrix is to big and not really necessary.\nAnd there is another case for the possibility:\nIf you split a dataset with many labels with random selection percent to testdataset and trainingdataset, it could happen, that there are classes/labels in testdata, which do not appear in the trainingdataset. By creating a model with the trainingdata the created labelindex does not include some labels from testdata. Therefore if you test on this model with the testdata, mahout tries to create a confusion matrix with the labels from testdata which are not included in the labelindex and throws an exception.",
        "Issue Links": []
    },
    "MAHOUT-1392": {
        "Key": "MAHOUT-1392",
        "Summary": "Streaming KMeans should write centroid output to a 'part-r-xxxx' file when executed in sequential mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "09/Jan/14 16:59",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "09/Jan/14 17:02",
        "Description": "Streaming KMeans presently creates the centroid output in the output file specified when executed in sequential mode. This is inconsistent with the behavior from the MapReduce version.",
        "Issue Links": []
    },
    "MAHOUT-1393": {
        "Key": "MAHOUT-1393",
        "Summary": "Removed duplicated code from getTopTerms and getTopFeatures in AbstractClusterWriter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Diego Carrion",
        "Created": "09/Jan/14 20:44",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "09/Jan/14 21:38",
        "Description": "The class org.apache.mahout.utils.clusteringAbstractClusterWriterTest has duplicated code in the methods getTopFeatures and getTopTerms. The duplicated code was extracted to a third private method called getTopTermsPairs.",
        "Issue Links": []
    },
    "MAHOUT-1394": {
        "Key": "MAHOUT-1394",
        "Summary": "Undeprecate Lanczos",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "14/Jan/14 07:28",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "16/Jan/14 07:50",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1395": {
        "Key": "MAHOUT-1395",
        "Summary": "Mahout CMS 404 Pages",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sotiris Salloumis",
        "Created": "14/Jan/14 08:34",
        "Updated": "03/Feb/14 07:46",
        "Resolved": "20/Jan/14 16:21",
        "Description": "Following pages currently are 404, please provide me the correct link/content to update it in CMS\n1) Developes -> Code Quality reports: Broken Link: https://builds.apache.org/hudson/job/Mahout-Quality/clover/\n2) Classification -> Design complimentary bayes : http://mahout.apache.org/users/classification/complementary-naive-bayes.html",
        "Issue Links": []
    },
    "MAHOUT-1396": {
        "Key": "MAHOUT-1396",
        "Summary": "Accidental use of commons-math won't work with next Hadoop 2 release",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sean R. Owen",
        "Created": "15/Jan/14 08:02",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "15/Jan/14 17:30",
        "Description": "The project uses commons-math3, since about a year ago. However there is a use of old commons-math (2.2) lurking:\ncore/src/main/java/org/apache/mahout/classifier/sgd/TPrior.java:\nimport org.apache.commons.math.special.Gamma;\nThis happens to have worked since commons-math has been pulled in by hadoop-common. But it no longer is in HEAD:\nhttp://svn.apache.org/viewvc/hadoop/common/trunk/hadoop-common-project/hadoop-common/pom.xml?view=markup\nSo this will no longer compile against the latest Hadoop. I believe it will also not actually run again the latest Hadoop, even if one were to use a version compiled versus older Hadoop 2, since the class that uses it is used in the context of Writables \u2013 that is, outside the client environment that might happen to have packaged commons-math \u2013 and so would fail on the cluster.\nThe change is trivial, to import the commons-math3 class. I've verified that tests pass and a patch is attached.\nQuestion is how much of a 'blocker' this should be for the pending release. It would cause it to stop working with the next Hadoop 2 release, so would be useful to get in, IMHO.",
        "Issue Links": []
    },
    "MAHOUT-1397": {
        "Key": "MAHOUT-1397",
        "Summary": "mahaout-math-scala/pom.xml not readable",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Maruf Aytekin",
        "Created": "17/Jan/14 09:43",
        "Updated": "03/Feb/14 08:06",
        "Resolved": "20/Jan/14 04:56",
        "Description": "maven-scala-plugin in mahaout-math-scala/pom.xml gives an error.\n\n\t\t\t<plugin>\n\t\t\t\t<groupId>org.scala-tools</groupId>\n\t\t\t\t<artifactId>maven-scala-plugin</artifactId>\n\t\t\t\t\t<executions>\n\t\t\t\t\t\t<execution>\n\t\t\t\t\t\t\t<goals>\n\t\t\t\t\t\t\t\t<goal>compile</goal>\n\t\t\t\t\t\t\t\t<goal>testCompile</goal>\n\t\t\t\t\t\t\t</goals>\n\t\t\t\t\t\t</execution>\n\t\t\t\t\t</executions>\n\t\t\t\t<configuration>\n\t\t\t\t\t<sourceDir>src/main/scala</sourceDir>\n\t\t\t\t\t<jvmArgs>\n\t\t\t\t\t\t<jvmArg>-Xms64m</jvmArg>\n\t\t\t\t\t\t<jvmArg>-Xmx1024m</jvmArg>\n\t\t\t\t\t</jvmArgs>\n\t\t\t\t</configuration>\n\t\t\t</plugin>\n\n\nError displayed:\n\nMultiple annotations found at this line:\n\nPlugin execution not covered by lifecycle configuration: org.scala-tools:maven-scala-plugin:2.15.2:compile (execution: default, phase: compile)\nPlugin execution not covered by lifecycle configuration: org.scala-tools:maven-scala-plugin:2.15.2:testCompile (execution: default, phase: test-\n\t compile)",
        "Issue Links": []
    },
    "MAHOUT-1398": {
        "Key": "MAHOUT-1398",
        "Summary": "FileDataModel should provide a constructor with a delimiterPattern",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Roy Guo",
        "Created": "19/Jan/14 15:40",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "21/Jan/14 21:38",
        "Description": "For now we only have ',' and '\\t' as delimiters, this is really not enough for users.\nOf course users can overwritten processLine etc. to archive their goal(e.g. use four spaces as delimiter pattern), but as a well designed framework, Mahout should consider vary demands of most users and make it very easy to use.\nAlso, it will not cost much time to implement, can I push a patch on this ?",
        "Issue Links": []
    },
    "MAHOUT-1399": {
        "Key": "MAHOUT-1399",
        "Summary": "Silence logging noise from Mahout Jobs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "CLI",
        "Assignee": "Suneel Marthi",
        "Reporter": "Frank Scholten",
        "Created": "19/Jan/14 21:15",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "27/Jan/14 01:49",
        "Description": "Mahout jobs produce a lot of output on the command line and log messages like these below are distracting:\nattempt_201311181700_0002_m_000000_0: SLF4J: Class path contains multiple SLF4J bindings.\nattempt_201311181700_0002_m_000000_0: SLF4J: Found binding in [jar:file:/home/hadoop/lib/slf4j-log4j12-1.7.4.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nattempt_201311181700_0002_m_000000_0: SLF4J: Found binding in [jar:file:/mnt/var/lib/hadoop/mapred/taskTracker/hadoop/jobcache/job_201311181700_0002/jars/job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nattempt_201311181700_0002_m_000000_0: SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nattempt_201311181700_0002_m_000000_0: SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nLet's silence log messages like these that do not provide information about the Mahout job itself. If you have more instances of superfluous logging add this to the issue.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1111",
            "/jira/browse/MAHOUT-1118"
        ]
    },
    "MAHOUT-1400": {
        "Key": "MAHOUT-1400",
        "Summary": "Remove references to deprecated and removed algorithms from examples scripts",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "21/Jan/14 20:31",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "21/Jan/14 20:57",
        "Description": "Still see references to old clustering algorithms like Minhash, Dirichlet in asf-email-examples.sh and cluster-syntheticcontrol.sh.\nAlso remove build-asf-email.sh and build-cluster-syntheticcontrol.sh from examples/bin.",
        "Issue Links": []
    },
    "MAHOUT-1401": {
        "Key": "MAHOUT-1401",
        "Summary": "Resurrect Frequent Pattern mining",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "22/Jan/14 04:53",
        "Updated": "13/Jan/15 19:47",
        "Resolved": "22/Jan/14 06:02",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1402": {
        "Key": "MAHOUT-1402",
        "Summary": "Zero clusters using streaming k-means option in cluster-reuters.sh",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Musselman",
        "Created": "22/Jan/14 04:57",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "22/Jan/14 09:13",
        "Description": "Running cluster-reuters.sh in examples/bin results in this:\n[snip]\nINFO: Number of Centroids: 0\nJan 22, 2014 1:52:22 AM org.apache.hadoop.mapred.LocalJobRunner$Job run\nWARNING: job_local23982482_0001\njava.lang.IllegalArgumentException: Must have nonzero number of training and test vectors. Asked for %.1f %% of %d vectors for test [10.000000149011612, 0]\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:120)\n        at org.apache.mahout.clustering.streaming.cluster.BallKMeans.splitTrainTest(BallKMeans.java:176)\n        at org.apache.mahout.clustering.streaming.cluster.BallKMeans.cluster(BallKMeans.java:192)\n        at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.getBestCentroids(StreamingKMeansReducer.java:107)\n        at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:73)\n        at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:37)\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:177)\n        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n[snip]\nWARNING: No qualcluster.props found on classpath, will use command-line arguments only\nNum clusters: 0; maxDistance: 0.000000\n[Dunn Index] First: Infinity\n[Davies-Bouldin Index] First: NaN\nJan 22, 2014 1:52:24 AM org.slf4j.impl.JCLLoggerAdapter info\nINFO: Program took 535 ms (Minutes: 0.008916666666666666)\ncluster,distance.mean,distance.sd,distance.q0,distance.q1,distance.q2,distance.q3,distance.q4,count,is.train",
        "Issue Links": []
    },
    "MAHOUT-1403": {
        "Key": "MAHOUT-1403",
        "Summary": "MatrixVectorView allows out of bounds index",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "23/Jan/14 03:16",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "24/Jan/14 04:02",
        "Description": "The MatrixVectorView has a > where it should have a >= in a test for index out of bounds.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1409"
        ]
    },
    "MAHOUT-1404": {
        "Key": "MAHOUT-1404",
        "Summary": "MatrixVectorView allows out of bounds index",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "23/Jan/14 03:16",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "24/Jan/14 04:03",
        "Description": "The MatrixVectorView has a > where it should have a >= in a test for index out of bounds.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1409"
        ]
    },
    "MAHOUT-1405": {
        "Key": "MAHOUT-1405",
        "Summary": "MatrixVectorView allows out of bounds index",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "23/Jan/14 03:17",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "24/Jan/14 04:04",
        "Description": "The MatrixVectorView has a > where it should have a >= in a test for index out of bounds.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1409"
        ]
    },
    "MAHOUT-1406": {
        "Key": "MAHOUT-1406",
        "Summary": "MatrixVectorView allows out of bounds index",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "23/Jan/14 03:17",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "24/Jan/14 04:02",
        "Description": "The MatrixVectorView has a > where it should have a >= in a test for index out of bounds.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1409"
        ]
    },
    "MAHOUT-1407": {
        "Key": "MAHOUT-1407",
        "Summary": "MatrixVectorView allows out of bounds index",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "23/Jan/14 03:18",
        "Updated": "18/Apr/14 22:17",
        "Resolved": "24/Jan/14 04:05",
        "Description": "The MatrixVectorView has a > where it should have a >= in a test for index out of bounds.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1409"
        ]
    },
    "MAHOUT-1408": {
        "Key": "MAHOUT-1408",
        "Summary": "Distributed cache file matching bug while running SSVD in broadcast mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Angad Singh",
        "Created": "23/Jan/14 11:49",
        "Updated": "08/Sep/15 15:22",
        "Resolved": "20/Feb/14 06:55",
        "Description": "The error is:\njava.lang.IllegalArgumentException: Unexpected file name, unable to deduce partition #:file:/data/d1/mapred/local/taskTracker/distcache/434503979705629827_-1822139941_1047712745/nn.red.ua2.inmobi.com/user/rmcuser/oozie-oozi/0034272-140120102756143-oozie-oozi-W/inmobi-ssvd_mahout--java/java-launcher.jar\n\tat org.apache.mahout.math.hadoop.stochasticsvd.SSVDHelper$1.compare(SSVDHelper.java:154)\n\tat org.apache.mahout.math.hadoop.stochasticsvd.SSVDHelper$1.compare(SSVDHelper.java:1)\n\tat java.util.Arrays.mergeSort(Arrays.java:1270)\n\tat java.util.Arrays.mergeSort(Arrays.java:1281)\n\tat java.util.Arrays.mergeSort(Arrays.java:1281)\n\tat java.util.Arrays.sort(Arrays.java:1210)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.init(SequenceFileDirValueIterator.java:112)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:94)\n\tat org.apache.mahout.math.hadoop.stochasticsvd.BtJob$BtMapper.setup(BtJob.java:220)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:647)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:266)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:260)\nThe bug is @ https://github.com/apache/mahout/blob/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/BtJob.java, near line 220.\nand  @ https://github.com/apache/mahout/blob/trunk/core/src/main/java/org/apache/mahout/math/hadoop/stochasticsvd/SSVDHelper.java near line 144.\nSSVDHelper's PARTITION_COMPARATOR assumes all files in the distributed cache will have a particular pattern whereas we have jar files in our distributed cache which causes the above exception.",
        "Issue Links": []
    },
    "MAHOUT-1409": {
        "Key": "MAHOUT-1409",
        "Summary": "MatrixVectorView has index check error",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "24/Jan/14 02:16",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "24/Jan/14 20:05",
        "Description": "There is a > in the test for the correct index where there should be a >=",
        "Issue Links": [
            "/jira/browse/MAHOUT-1403",
            "/jira/browse/MAHOUT-1404",
            "/jira/browse/MAHOUT-1405",
            "/jira/browse/MAHOUT-1406",
            "/jira/browse/MAHOUT-1407"
        ]
    },
    "MAHOUT-1410": {
        "Key": "MAHOUT-1410",
        "Summary": "clusteredPoints do not contain a vector id",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Pat Ferrel",
        "Created": "24/Jan/14 18:37",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "26/Jan/14 03:47",
        "Description": "When clustering non-named vectors there are no vector ids in clusteredPoints so the other values there, cluster id, vector values, distance-squared, pdf, cannot be tied to any known vector.",
        "Issue Links": []
    },
    "MAHOUT-1411": {
        "Key": "MAHOUT-1411",
        "Summary": "Random test failures from TDigestTest",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "28/Jan/14 18:35",
        "Updated": "03/Feb/14 07:57",
        "Resolved": "28/Jan/14 18:42",
        "Description": "Seeing random test failures like below from TDigestTest. These errors are not repeatable. \n\n\ntestUniform(org.apache.mahout.math.stats.TDigestTest)  Time elapsed: 0.356 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<0.5> but was:<0.50578>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:494)\n\tat org.junit.Assert.assertEquals(Assert.java:592)\n\tat org.apache.mahout.math.stats.TDigestTest.runTest(TDigestTest.java:373)\n\tat org.apache.mahout.math.stats.TDigestTest.testUniform(TDigestTest.java:79)\n\nResults :\n\nFailed tests: \n  TDigestTest.testUniform:79->runTest:373 expected:<0.5> but was:<0.50578>\n\n\n\n\nTests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 40.063 sec\n<<< FAILURE! - in org.apache.mahout.math.stats.TDigestTest\ntestSequentialPoints(org.apache.mahout.math.stats.TDigestTest)  Time\nelapsed: 4.674 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<0.5> but was:<0.49489>",
        "Issue Links": []
    },
    "MAHOUT-1412": {
        "Key": "MAHOUT-1412",
        "Summary": "Build warning due to multiple Scala versions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Frank Scholten",
        "Created": "03/Feb/14 21:42",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "13/Apr/14 11:30",
        "Description": "I see the following build warning:\n22:42:07 [WARNING]  Expected all dependencies to require Scala version: 2.9.3\n22:42:07 [WARNING]  org.apache.mahout:mahout-math-scala:1.0-SNAPSHOT requires scala version: 2.9.3\n22:42:07 [WARNING]  org.scalatest:scalatest_2.9.2:1.9.1 requires scala version: 2.9.2\n22:42:07 [WARNING] Multiple versions of scala libraries detected!\nWhich version should we use?",
        "Issue Links": []
    },
    "MAHOUT-1413": {
        "Key": "MAHOUT-1413",
        "Summary": "Rework algorithms page",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "05/Feb/14 10:49",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "08/Mar/14 23:51",
        "Description": "It's crucial that we update our algorithms page to reflect the current state of algorithms in Mahout 0.9!!!\nhttps://mahout.apache.org/users/basics/algorithms.html",
        "Issue Links": []
    },
    "MAHOUT-1414": {
        "Key": "MAHOUT-1414",
        "Summary": "Mahout Stream Analysis Package",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Amir Rahnama",
        "Created": "11/Feb/14 11:55",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Mar/14 18:20",
        "Description": "I am working with Stream Analysis in Java and unfortunately as I see it, if someone chooses to work with Java in Machine Learning he needs to implement all the stuff.\nSuggestion is that I contribute to the stream analysis package of Mahout and make it reusable.\nGuide me on this plz.",
        "Issue Links": []
    },
    "MAHOUT-1415": {
        "Key": "MAHOUT-1415",
        "Summary": "Clone method on sparse matrices fails if there is an empty row which has not been set explicitly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Till Rohrmann",
        "Created": "11/Feb/14 11:58",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "11/Feb/14 14:58",
        "Description": "The clone method of the SparseMatrix class fails with a NullPointerException if there exists an empty row in the matrix which has not been explicitly set. The reason for this problem is that the clone operation iterates over all rows and clones them whether there exists a Vector instance for this row or not. The problem should be easily fixed by iterating only over the existing matrix slices.",
        "Issue Links": []
    },
    "MAHOUT-1416": {
        "Key": "MAHOUT-1416",
        "Summary": "Make access of DecisionForest read() less restricted",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Manoj Awasthi",
        "Created": "12/Feb/14 10:12",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "12/Feb/14 17:35",
        "Description": "There is a use case I encounter in which a random forest model is passed to remote server over a REST API and is used for scoring the incoming requests in there. \nIn such a scenario it is highly inefficient to write the byte buffer received at the remote server on the filesystem and then use Mahout APIs to load the forest.  \nWe should be able to load the forest from the byte buffer received. This is useful in non-map-reduce usecases of classification like that mentioned above.",
        "Issue Links": []
    },
    "MAHOUT-1417": {
        "Key": "MAHOUT-1417",
        "Summary": "Random decision forest implementation fails in Hadoop 2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Sean R. Owen",
        "Created": "13/Feb/14 08:37",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "16/Feb/14 05:58",
        "Description": "We've observed two errors in the RDF implementation, one of which stops it from working on Hadoop 2 (at least I think it is Hadoop 2 only), and one of which just makes the workload quite imbalanced.\nA key piece of logic in PartialBuilder.java queries mapred.map.tasks to know the total number of mappers. However this has never been guaranteed to be set to the number of mappers; it is how a caller sets a default number of mappers, which may be overridden by Hadoop, and which defaults to 1. \nI suspect that this may have actually been set, in some or all cases, to the number of mappers in Hadoop 1, but I am not sure. Certainly, sometimes it will happen to be set to a value that equals the number of mappers used.\nBut when it doesn't it causes the distribution of trees to mappers to be quite wrong. For example, with 20 trees and 8 mappers in one example, I find that mapred.map.tasks=1. Logging messages indicate that mapper 0 handles all trees (0-19), mapper 1 handles non-existent 20-39, etc.\nThe result is that most mappers do nothing and one does everything. This results in empty part-m-xxxxx files. And, that in turn fails the job. (This part I also suspect is new, or situation-specific, behavior in Hadoop 2. In any event, this code should never have idle mappers and fixing that avoids whatever is going on there.)\nThere's a second less serious issue in how trees are assigned to mappers. When the number of trees is not a multiple of the number of mappers, the remainer is assigned entirely to mapper 0. So with 20 trees and 8 mappers, all mappers build 2 trees, but mapper 0 builds 6. This is unnecessarily imbalanced.\nPatch coming once I can verify the fix, but current proposal is to:\n\nCompute the number of maps ahead of time using TextInputFormat and set mapred.map.tasks\nFix the method that computes trees per mapper to spread as evenly as possible (i.e. all mappers build either N or N+1 trees)",
        "Issue Links": []
    },
    "MAHOUT-1418": {
        "Key": "MAHOUT-1418",
        "Summary": "Removal of write access to anything but CMS for username isabel",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Isabel Drost-Fromm",
        "Reporter": "Isabel Drost-Fromm",
        "Created": "18/Feb/14 09:03",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "19/Feb/14 15:16",
        "Description": "Hi,\nPlease remove write access to user name \"isabel\" - effective mid March. For background check the Mahout board report of October last year*.\nDon't worry - I'm not planning to go completely silent and offline by then. However I know from several years of doing Berlin Buzzwords that being completely sleep deprived is not a good state to commit to subversion - except that when sleep deprived I usually don't remember this insight. So this is my security net forcing me to go through the regular submit patch in jira, get it reviewed and committed cycle (except for documentation changes).\n\n<http://www.apache.org/foundation/records/minutes/2013/board_minutes_2013_10_16.txt>",
        "Issue Links": []
    },
    "MAHOUT-1419": {
        "Key": "MAHOUT-1419",
        "Summary": "Random decision forest is excessively slow on numeric features",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "18/Feb/14 19:42",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "25/Feb/14 14:15",
        "Description": "Follow-up to MAHOUT-1417. There's a customer running this and observing it take an unreasonably long time on about 2GB of data \u2013 like, >24 hours when other RDF M/R implementations take 9 minutes. The difference is big enough to probably be considered a defect. MAHOUT-1417 got that down to about 5 hours. I am trying to further improve it.\nOne key issue seems to be how splits are evaluated over numeric features. A split is tried for every distinct numeric value of the feature in the whole data set. Since these are floating point values, they could (and in the customer's case are) all distinct. 200K rows means 200K splits to evaluate every time a node is built on the feature.\nA better approach is to sample percentiles out of the feature and evaluate only those as splits. Really doing that efficiently would require a lot of rewrite. However, there are some modest changes possible which get some of the benefit, and appear to make it run about 3x faster. That is --on a data set that exhibits this problem \u2013 meaning one using numeric features which are generally distinct. Which is not exotic.\nThere are comparable but different problems with handling of categorical features, but that's for a different patch.\nI have a patch, but it changes behavior to some extent since it is evaluating only a sample of splits instead of every single possible one. In particular it makes the output of \"OptIgSplit\" no longer match the \"DefaultIgSplit\". Although I think the point is that \"optimized\" may mean giving different choices of split here, which could yield differing trees. So that test probably has to go.\n(Along the way I found a number of micro-optimizations in this part of the code that added up to maybe a 3% speedup. And fixed an NPE too.)\nI will propose a patch shortly with all of this for thoughts.",
        "Issue Links": [
            "/jira/browse/MAHOUT-932",
            "/jira/browse/MAHOUT-943"
        ]
    },
    "MAHOUT-1420": {
        "Key": "MAHOUT-1420",
        "Summary": "Add solr-recommender to examples",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "19/Feb/14 18:40",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "13/Apr/14 04:27",
        "Description": "Write a new example that builds a solr-recommender based on Pat's code at https://github.com/pferrel/solr-recommender and which has the glue scripts needed to pipe all the way from start(raw data) to finish(running web service and UI page).",
        "Issue Links": []
    },
    "MAHOUT-1421": {
        "Key": "MAHOUT-1421",
        "Summary": "Adapter package for all mahout tools",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jay vyas",
        "Created": "20/Feb/14 13:06",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "13/Apr/14 14:35",
        "Description": "Hi mahout.  I'd like to create an umbrella JIRA for allowing more runtime flexibility for reading different types of input formats for all mahout tasks. \nSpecifically, I'd like to start with the FreeTextRecommenderAdapeter, which typically requires:\n1) Hashing text entries into numbers\n2) Saving the large transformed file on disk\n3) Feeding it into classifieer \nInstead, we could build adapters into the classifier itself, so that the user\n1) Specifies input file to recommender\n2) Specifies transformation class which converts each record of input to 3 column recommender format\n3) Runs internal mahout recommender directly against the data\nAnd thus the user could easily run mahout against existing data without having to munge it to much.\nThis package might be called something like \"org.apache.mahout.adapters\", and would over time provide flexible adapters to the core mahout algorithm implementations, so that folks wouldnt have to worry so much about vectors/csv transformers/etc... \nAny thoughts on this?  If positive feedback I can submit an initial patch to get things started.",
        "Issue Links": []
    },
    "MAHOUT-1422": {
        "Key": "MAHOUT-1422",
        "Summary": "Make a version of RSJ that uses two inputs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Pat Ferrel",
        "Created": "20/Feb/14 16:48",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "10/Apr/14 19:05",
        "Description": "Currently the RowSimiairtyJob uses a similarity measure to pairwise compare all rows in a DistributedRowMatrix.\nFor many applications including a cross-action recommender we need something like RSJ that takes two DRMs and compares matching rows of each.  The output would be the same form as RSJ, and ideally would allow the use of any similarity type already defined--especially LLR.\nThere are two implementations of a Cross-Recommender one based on the Mahout RecommenderJob, and another based on Solr, that can immediately benefit from a Cross-RSJ. \nA modification of the matrix multiply job may be a place to start since the current RSJ seems to rely heavily if self-similarity.",
        "Issue Links": []
    },
    "MAHOUT-1423": {
        "Key": "MAHOUT-1423",
        "Summary": "Add time series anomaly detection example",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "Examples",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "20/Feb/14 17:02",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 17:15",
        "Description": "Add an example of anomaly detection in a time series along the lines of Ted's EKG example starting about slide 20 here:  http://www.slideshare.net/tdunning/strata-2014-anomaly-detection",
        "Issue Links": []
    },
    "MAHOUT-1424": {
        "Key": "MAHOUT-1424",
        "Summary": "VFDT Very fast decision trees implementation",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Amir Rahnama",
        "Created": "23/Feb/14 12:15",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "23/Mar/14 05:18",
        "Description": "I would like to use VFDT (or equivalently called as hoeffding tree classifier) and I did not find the algorithm implemented in Mahout.\nDo you think I can contribute on that? If so, any hint or code pointers?\nIf not, could someone come up with a reason why not to have this very important algorithm which is great in processing high speed data?",
        "Issue Links": []
    },
    "MAHOUT-1425": {
        "Key": "MAHOUT-1425",
        "Summary": "SGD classifier example with bank marketing dataset",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Frank Scholten",
        "Reporter": "Frank Scholten",
        "Created": "23/Feb/14 18:50",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "25/Mar/15 03:56",
        "Description": "As discussed on the mailing list a few weeks back I started working on an SGD classifier example with the bank marketing dataset from UCI: http://archive.ics.uci.edu/ml/datasets/Bank+Marketing\nSee https://github.com/frankscholten/mahout-sgd-bank-marketing\nTed has also made further changes that were very useful so I suggest to add this example to Mahout\nTed: can you tell a bit more about the log transforms? Some of them are just Math.log while others are more complex expressions. \nWhat else is needed to contribute it to Mahout? Anything that could improve the example?",
        "Issue Links": []
    },
    "MAHOUT-1426": {
        "Key": "MAHOUT-1426",
        "Summary": "GSOC 2014 Neural network algorithms",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Maciej Mazur",
        "Created": "25/Feb/14 15:33",
        "Updated": "19/Mar/14 22:27",
        "Resolved": "19/Mar/14 22:26",
        "Description": "I would like to ask about possibilites of implementing neural network algorithms in mahout during GSOC.\nThere is a classifier.mlp package with neural network.\nI can't see neighter RBM  nor Autoencoder in these classes.\nThere is only one word about Autoencoders in NeuralNetwork class.\nAs far as I know Mahout doesn't support convolutional networks.\nIs it a good idea to implement one of these algorithms?\nIs it a reasonable amount of work?\nHow hard is it to get GSOC in Mahout?\nDid anyone succeed last year?",
        "Issue Links": []
    },
    "MAHOUT-1427": {
        "Key": "MAHOUT-1427",
        "Summary": "Convert old .mapred API to new .mapreduce",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering,                                            Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "01/Mar/14 06:13",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "15/Jul/14 22:30",
        "Description": "Migrate code still using the old .mapred to .mapreduce API",
        "Issue Links": [
            "/jira/browse/MAHOUT-950"
        ]
    },
    "MAHOUT-1428": {
        "Key": "MAHOUT-1428",
        "Summary": "Recommending already consumed items",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Mario Levitin",
        "Created": "02/Mar/14 20:47",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "06/May/14 05:02",
        "Description": "Mahout does not recommend items which are already consumed by the user.\nFor example,\nIn the getAllOtherItems method of GenericUserBasedRecommender class there is the following line\npossibleItemIDs.removeAll(dataModel.getItemIDsFromUser(theUserID));  \nwhich removes user's items from the possibleItemIDs to prevent these items from being recommended to the user. This is ok for many recommendation cases but for many other cases it is not. \nThe Recommender classes  (I mean all of them, NN-based and SVD-based as well as hadoop and non-hadoop versions) might have a parameter for this for excluding or not excluding user items in the returned recommendations.",
        "Issue Links": []
    },
    "MAHOUT-1429": {
        "Key": "MAHOUT-1429",
        "Summary": "Parallelize YtransposeY in ImplicitFeedbackAlternatingLeastSquaresSolver",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Adam Ilardi",
        "Created": "03/Mar/14 15:58",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "03/Mar/14 21:55",
        "Description": "I wrote a simple patch to do the calculation in parallel. It could be better but it gets the job done. Is there some other (Matrix transpose dot matrix) code in mahout that is already parallel? I will reuse that if it exists.",
        "Issue Links": []
    },
    "MAHOUT-1430": {
        "Key": "MAHOUT-1430",
        "Summary": "GSOC 2014 Proposal of implementing a new recommender",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Mihai Pitu",
        "Created": "03/Mar/14 17:36",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/Apr/14 11:24",
        "Description": "I would like to ask about possibilities of implementing Sparse Linear Methods (SLIM) recommender in Mahout during GSOC 2014.\nThe SLIM algorithm generates efficient recommendations and its performance is shown in the original paper (http://glaros.dtc.umn.edu/gkhome/fetch/papers/SLIM2011icdm.pdf). The study demonstrates that SLIM outperforms traditional algorithms (such as itemkNN, userkNN, SVD or Matrix Factorization approaches) on various data-sets in terms of run-time and recommendation quality. The algorithm can be paralellized and Map-Reduce can help us achieve that.\nI am aware of real world systems that are using SLIM as a recommendation engine (e.g. Mendeley: http://www.slideshare.net/MarkLevy/efficient-slides) and I think it represents the state-of-the-art in collaborative filtering right now.\nWould this be an interesting addition to Mahout and is somebody interested in mentoring this at Google Summer of Code 2014?",
        "Issue Links": []
    },
    "MAHOUT-1431": {
        "Key": "MAHOUT-1431",
        "Summary": "Comparison of Mahout 0.8 vs mahout 0.9 in EMR",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "yannis ats",
        "Created": "04/Mar/14 11:01",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "20/Apr/14 09:38",
        "Description": "Hi all,\ni tested mahout 0.8 and 0.9 in mahout emr with a large dataset as input and \ni performed kmeans experiments with both versions in amazon EMR.\nWhat i found is that mahout 0.8 is faster than mahout 0.9\nin particular i observed that mahout 0.8 is performing less iterations and every iteration of kmeans is faster than mahout 0.9.Every iteration in mahout 0.8 is twice as fast as that of 0.9\nthe hadoop version was 1.0.x and the input of the data was roughly 2 million datapoints with dimensionality of 1800.\nThe input parameters in both experiments were exactly the same,modulo the initialization which was random in both cases and i can understand that this may affect the convergence(the amount of iterations),but i am baffled by the fact that every iteration takes almost twice the time in 0.9 vs 0.8\nIs this normal?is this  expected?\nthank you in advance for your time.",
        "Issue Links": []
    },
    "MAHOUT-1432": {
        "Key": "MAHOUT-1432",
        "Summary": "Reduce Hadoop dependencies - NaiveBayesModel",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Frank Scholten",
        "Reporter": "Frank Scholten",
        "Created": "05/Mar/14 18:17",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "17/Apr/14 18:37",
        "Description": "After the discussion on the mailinglist I decided to clean up some Hadoop dependencies and started with the NaiveBayesModel. \nI like to commit this soon and move on to the next algorithm or model to clean it up.",
        "Issue Links": []
    },
    "MAHOUT-1433": {
        "Key": "MAHOUT-1433",
        "Summary": "Make SVDRecommender look at all unknown items of a user per default",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "05/Mar/14 23:06",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "05/Mar/14 23:27",
        "Description": "We had a discussion on the mailinglist that (correctly) stated that SVDRecommender should consider all items the user has not interacted with as candidates to recommend per default. This patch changes the default CandidateItemsStrategy for SVDRecommender to realize that.",
        "Issue Links": []
    },
    "MAHOUT-1434": {
        "Key": "MAHOUT-1434",
        "Summary": "Dead links on the website",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "K\u00e9vin Moulart",
        "Created": "06/Mar/14 14:20",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "08/Mar/14 12:33",
        "Description": "There are several links on the website that point to absent pages. I just\nclicked on all the link present on page :\nhttp://mahout.apache.org/users/basics/quickstart.html\nAnd those links are broken :\nhttp://mahout.apache.org/users/basics/recommender-documentation.html\nhttp://mahout.apache.org/users/classification/partial-implementation.html\nhttp://mahout.apache.org/users/basics/TasteCommandLine\nhttp://mahout.apache.org/users/recommender/recommendationexamples.html\nhttp://mahout.apache.org/users/basics/parallel-frequent-pattern-mining.html\nhttp://mahout.apache.org/users/basics/mahout.ga.tutorial.html\nhttp://hadoop.apache.org.html/\nThat's just the ones I found in 2 minutes on the quickstart page, there are maybe other dead links elsewhere.",
        "Issue Links": []
    },
    "MAHOUT-1435": {
        "Key": "MAHOUT-1435",
        "Summary": "Website Redesign",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "08/Mar/14 11:18",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/Apr/14 11:23",
        "Description": "We decided to redesign the website to make it look nicer and easier to read.",
        "Issue Links": []
    },
    "MAHOUT-1436": {
        "Key": "MAHOUT-1436",
        "Summary": "Missing pages need to be migrated over from old CMS site",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Suneel Marthi",
        "Created": "08/Mar/14 12:35",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "17/Mar/14 06:12",
        "Description": "Following missing pages need to be migrated over from the old CMS site:\nhttps://cwiki.apache.org/confluence/display/MAHOUT/RecommendationExamples\nhttps://cwiki.apache.org/confluence/display/MAHOUT/TasteCommandLine\nhttps://cwiki.apache.org/confluence/display/MAHOUT/Quick+tour+of+text+analysis+using+the+Mahout+command+line",
        "Issue Links": []
    },
    "MAHOUT-1437": {
        "Key": "MAHOUT-1437",
        "Summary": "Remove all links to wiki pages from the website",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "08/Mar/14 12:59",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "16/Mar/14 07:43",
        "Description": "A lot of links still point to the outdated and broken wiki. I'll walk through the site and remove/fix em.",
        "Issue Links": []
    },
    "MAHOUT-1438": {
        "Key": "MAHOUT-1438",
        "Summary": "\"quickstart\" tutorial for building a simple recommender",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": null,
        "Reporter": "Maciej Mazur",
        "Created": "08/Mar/14 14:00",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "09/Mar/14 16:20",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1439": {
        "Key": "MAHOUT-1439",
        "Summary": "Update talks on Mahout",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "08/Mar/14 16:43",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "27/Jun/14 11:59",
        "Description": "The talks listed on our homepage seem to end somewhere in 2012.\nI know that there have been tons of other talks on Mahout since then, I've added mine already. It would be great if everybody who knows of additional talks would paste them here, so I can add them to the website.",
        "Issue Links": []
    },
    "MAHOUT-1440": {
        "Key": "MAHOUT-1440",
        "Summary": "Add option to set the RNG seed for inital cluster generation in Kmeans/fKmeans",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI,                                            Clustering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Andrew Palumbo",
        "Created": "08/Mar/14 22:31",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "18/Apr/14 11:15",
        "Description": "It was noted recently that there should be a way to set a static seed for the the initial clusters of Kmeans. In the interests of reproducibility and benchmarking, this patch adds an option to set the seed in the RNG used in the RandomSeedGenerator.buildRandom() method called from the KmeansDriver and FuzzyKMeansDriver.  \nI've added in a CLI option -setRandomSeed that when set to the same value (with the -k option set) will produce reproducible results from kmeans and fkmeans.\nThis patch allows the user to set a value.  It may make more sense to just have an option to set a flag to use the STANDARD_SEED from RandomWrapper.\nI am still feeling my way around the codebase so if this will be useful and there need to be any changes let me know.",
        "Issue Links": []
    },
    "MAHOUT-1441": {
        "Key": "MAHOUT-1441",
        "Summary": "Add documentation for Spectral KMeans to Mahout Website",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Shannon Quinn",
        "Reporter": "Suneel Marthi",
        "Created": "09/Mar/14 15:56",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "07/May/14 17:21",
        "Description": "Need to update the Website with Design, user guide and any relevant documentation for Spectral KMeans clustering.",
        "Issue Links": []
    },
    "MAHOUT-1442": {
        "Key": "MAHOUT-1442",
        "Summary": "Force Mahout to be build with guava 11.0.2 as Hadoop uses 11.0.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build,                                            Integration,                                            Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Bikash Gupta",
        "Created": "09/Mar/14 18:11",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "18/Apr/14 11:29",
        "Description": "Currently Hadoop supports guava 11.02, hence Mahout needs to be compatible with guava 11.0.2",
        "Issue Links": []
    },
    "MAHOUT-1443": {
        "Key": "MAHOUT-1443",
        "Summary": "Update \"How to release page\"",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "09/Mar/14 19:08",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "06/Nov/15 01:33",
        "Description": "I have a favor to ask. Could you have a look at the \"How To Release\" \npage and tell if the information there is still correct? I'm asking you \nthis because you have done the latest release. After your OK, I'll go \nand improve formatting and readability of that page.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1668"
        ]
    },
    "MAHOUT-1444": {
        "Key": "MAHOUT-1444",
        "Summary": "Check whether the examples on the website still work",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Maciej Mazur",
        "Created": "10/Mar/14 19:41",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "13/Apr/14 11:23",
        "Description": "https://mahout.apache.org/users/clustering/clustering-of-synthetic-control-data.html\nhttps://mahout.apache.org/users/classification/wikipedia-bayes-example.html\nhttps://mahout.apache.org/users/clustering/twenty-newsgroups.html\nhttps://mahout.apache.org/users/classification/breiman-example.html",
        "Issue Links": []
    },
    "MAHOUT-1445": {
        "Key": "MAHOUT-1445",
        "Summary": "Create an intro for item based recommender",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Maciej Mazur",
        "Created": "10/Mar/14 19:43",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "15/Apr/14 04:38",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1446": {
        "Key": "MAHOUT-1446",
        "Summary": "Create an intro for matrix factorization",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Maciej Mazur",
        "Created": "10/Mar/14 19:46",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "25/May/14 14:56",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1447": {
        "Key": "MAHOUT-1447",
        "Summary": "ImplicitFeedbackAlternatingLeastSquaresSolver tests and features",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Adam Ilardi",
        "Created": "11/Mar/14 15:37",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "11/Mar/14 18:32",
        "Description": "I added a test case for the YtY calculation code\nI removed the indexes.quickSort() in the YtY calculation  because I don't think it's necessary and the test cases passed without it. The order shouldn't matter since you're adding the scalers together. Correct me if i'm wrong.\nIn Factorization.java I added methods to access the iterator of item ids and user ids directly. This saves memory when using classes like TopItems.java when you don't have the DataModel class in memory as well.",
        "Issue Links": []
    },
    "MAHOUT-1448": {
        "Key": "MAHOUT-1448",
        "Summary": "In Random Forest, the training does not support multiple input files. The input dataset must be one single file.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Manoj Awasthi",
        "Created": "11/Mar/14 16:59",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "12/Mar/14 06:21",
        "Description": "In Random Forest, the training does not support multiple input files. The input dataset must be one single file. \nRef: https://mahout.apache.org/users/stuff/partial-implementation.html (Known issues)",
        "Issue Links": []
    },
    "MAHOUT-1449": {
        "Key": "MAHOUT-1449",
        "Summary": "Update the Known Issues in Random Forests Page",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Manoj Awasthi",
        "Created": "12/Mar/14 07:02",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "23/Mar/14 04:47",
        "Description": "Ref: https://mahout.apache.org/users/stuff/partial-implementation.html\nOn this page at the bottom there is a section for Known Issues. It mentions:\n==== \nFor now, the training does not support multiple input files. The input dataset must be one single file. \n==== \nThis sentence should be augmented as following: \n==== \nFor now, the training does not support multiple input files. The input dataset must be one single file. (with upcoming 1.0 release this support will be available.)\n==== \nCHANGE Ref: http://svn.apache.org/viewvc/mahout/trunk/core/src/main/java/org/apache/mahout/classifier/df/data/DataLoader.java?view=diff&r1=1576589&r2=1576590&pathrev=1576590",
        "Issue Links": []
    },
    "MAHOUT-1450": {
        "Key": "MAHOUT-1450",
        "Summary": "Cleaning up clustering documentation on mahout website",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Pavan Kumar N",
        "Created": "12/Mar/14 10:29",
        "Updated": "14/Apr/14 09:13",
        "Resolved": "13/Apr/14 14:28",
        "Description": "In canopy clustering, the strategy for parallelization seems to have some dead links. Need to clean them and replace with new links (if there are any). Here is the link:\nhttp://mahout.apache.org/users/clustering/canopy-clustering.html\nHere are some details of the dead links for kmeans clustering page:\nOn the k-Means clustering - basics page, \nfirst line of the Quickstart part of the documentation, the hyperlink \"Here\"\nhttp://mahout.apache.org/users/clustering/k-means-clustering%5Equickstart-kmeans.sh.html\nfirst sentence of Strategy for parallelization part of documentation, the hyperlink \"Cluster computing and MapReduce\", second second sentence the hyperlink \"here\" and last sentence the hyperlink \"http://www2.chass.ncsu.edu/garson/PA765/cluster.htm\" are dead.\nhttp://code.google.com/edu/content/submissions/mapreduce-minilecture/listing.html\nhttp://code.google.com/edu/content/submissions/mapreduce-minilecture/lec4-clustering.ppt\nhttp://www2.chass.ncsu.edu/garson/PA765/cluster.htm\nUnder the page: http://mahout.apache.org/users/clustering/visualizing-sample-clusters.html\nin the second sentence of Pre-prep part of this page, the hyperlink \"setup mahout\" is dead.\nhttp://mahout.apache.org/users/clustering/users/basics/quickstart.html\nThe existing documentation is too ambiguous and I recommend to make the following changes so the new users can use it as tutorial.\nThe Quickstart should be replaced with the following:\nGet the data from:\nwget http://www.daviddlewis.com/resources/testcollections/reuters21578/reuters21578.tar.gz\nPlace it within the example folder from mahout home director:\nmahout-0.7/examples/reuters\nmkdir reuters\ncd reuters\nmkdir reuters-out\nmv reuters21578.tar.gz reuters-out\ncd reuters-out\ntar -xzvf reuters21578.tar.gz\ncd ..\nMahout specific Commands\n#1 run the org.apache.lucene.benchmark .utils.ExtractReuters class\n${MAHOUT_HOME}/bin/mahout\norg.apache.lucene.benchmark.utils.ExtractReuters reuters-out\nreuters-text\n#2 copy the file to your HDFS\nbin/hadoop fs -copyFromLocal\n/home/bigdata/mahout-distribution-0.7/examples/reuters-text\nhdfs://localhost:54310/user/bigdata/\n#3 generate sequence-file\nmahout seqdirectory -i hdfs://localhost:54310/user/bigdata/reuters-text\n-o hdfs://localhost:54310/user/bigdata/reuters-seqfiles -c UTF-8 -chunk 5\n-chunk \u2192 specifying the number of data blocks\nUTF-8 \u2192 specifying the appropriate input format\n#4 Check the generated sequence-file\nmahout-0.7$ ./bin/mahout seqdumper -i\n/your-hdfs-path-to/reuters-seqfiles/chunk-0 | less\n#5 From sequence-file generate vector file\nmahout seq2sparse -i\nhdfs://localhost:54310/user/bigdata/reuters-seqfiles -o\nhdfs://localhost:54310/user/bigdata/reuters-vectors -ow\n-ow \u2192 overwrite\n#6 take a look at it should have 7 items by using this command\nbin/hadoop fs -ls\nreuters-vectors/df-count\nreuters-vectors/dictionary.file-0\nreuters-vectors/frequency.file-0\nreuters-vectors/tf-vectors\nreuters-vectors/tfidf-vectors\nreuters-vectors/tokenized-documents\nreuters-vectors/wordcount\nbin/hadoop fs -ls reuters-vectors\n#7 check the vector: reuters-vectors/tf-vectors/part-r-00000\nmahout-0.7$ hadoop fs -ls reuters-vectors/tf-vectors\n#8 Run canopy clustering to get optimal initial centroids for k-means\nmahout canopy -i\nhdfs://localhost:54310/user/bigdata/reuters-vectors/tf-vectors -o\nhdfs://localhost:54310/user/bigdata/reuters-canopy-centroids -dm\norg.apache.mahout.common.distance.CosineDistanceMeasure -t1 1500 -t2 2000\n-dm \u2192 specifying the distance measure to be used while clustering (here it is cosine distance measure)\n#9 Run k-means clustering algorithm\nmahout kmeans -i\nhdfs://localhost:54310/user/bigdata/reuters-vectors/tfidf-vectors -c\nhdfs://localhost:54310/user/bigdata/reuters-canopy-centroids -o\nhdfs://localhost:54310/user/bigdata/reuters-kmeans-clusters -cd 0.1 -ow\n-x 20 -k 10\n-i \u2192 input\n-o \u2192 output\n-c \u2192 initial centroids for k-means (not defining this parameter will\ntrigger k-means to generate random initial centroids)\n-cd \u2192 convergence delta parameter\n-ow \u2192 overwrite\n-x \u2192 specifying number of k-means iterations\n-k \u2192 specifying number of clusters\n#10 Export k-means output using Cluster Dumper tool\nmahout clusterdump dt sequencefile -d hdfs://localhost:54310/user/bigdata/reuters-vectors/dictionary.file*\ni hdfs://localhost:54310/user/bigdata/reuters-kmeans-clusters/clusters-8\nfinal -o clusters.txt -b 15\n-dt \u2192 dictionary type\n-b \u2192 specifying length of each word\nMahout 0.7 version did have some problems using the DisplayKmeans module which should ideally display the clusters in a 2d graph. But it gave me the same output for different input datasets. I was using dataset of recent news items that was crawled from various websites.",
        "Issue Links": []
    },
    "MAHOUT-1451": {
        "Key": "MAHOUT-1451",
        "Summary": "Cleaning up the examples for clustering on the website",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Gaurav Misra",
        "Created": "12/Mar/14 11:21",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "13/Mar/14 12:04",
        "Description": "Cleaning up the following clustering examples:\n=====================================\nhttps://mahout.apache.org/users/clustering/clustering-of-synthetic-control-data.html\nIntroduction\nThis example will demonstrate clustering of time series data, specifically control charts. [Control charts : http://en.wikipedia.org/wiki/Control_chart] are tools used to determine whether a manufacturing or business process is in a state of statistical control. Such control charts are generated / simulated repeatedly at equal time intervals. A simulated dataset is available for use in UCI machine learning repository. The data is described [here : http://archive.ics.uci.edu/ml/databases/synthetic_control/synthetic_control.data.html].\nProblem Description\nA time series of control charts needs to be clustered into their close knit groups. The data set we use is synthetic and is meant to resemble real world information in an anonymized format. It contains six different classes: Normal, Cyclic, Increasing trend, Decreasing trend, Upward shift, Downward shift. In this example we will use Mahout to cluster the data into corresponding class buckets. \nAt the end of this example\n\nYou will have clustered data using mahout.\nYou will see how to analyse the clusters produced by mahout.\nYou will have a starting point for incorporating clustering into your own software.\n\nSetup\nWe need to do some initial setup before we are able to run the example. \n  1. Start out by downloading the input dataset (to be clustered) from the UCI Machine Learning Repository: http://archive.ics.uci.edu/ml/databases/synthetic_control/synthetic_control.data\n  2. Make sure the data consists of 600 rows and 60 columns. The first 100 rows contains Normal data followed by 100 rows of Cyclic data and so on with a total of 6 classes.\n  3. This example assumes that you have already set up Mahout/Hadoop. If you have not done so yet:\n  4. \n\nHadoop: Follow the instructions on http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleNodeSetup.html to set up Hadoop.\nMahout: Follow the instructions on the [Quickstart: https://mahout.apache.org/users/basics/quickstart.html] page.\n  5. Make sure the Hadoop daemons are running if you are running Hadoop in distributed mode. \n  6. Create a directory on your local machine called \u00ab testdata \u00bb and place the input dataset in this directory.\n  7. Run the following command to copy the input data into HDFS:\n\n\nCreate a directory called \u00ab testdata \u00bb  on HDFS:\n\n                         $HADOOP_HOME/bin/hadoop fs -mkdir testdata\n\nCopy the directory named \u00ab testdata \u00bb from your local filesystem to HDFS:\n                         $HADOOP_HOME/bin/hadoop fs -put testdata\n  8. The final setup step is to build Mahout by going to the $MAHOUT_HOME directory and running one of the following commands: \n  9. \nFor a full build: mvn clean install\nFor a build without unit tests: mvn -DskipTests clean install\n  10. You should see a build successful message once the build script has completed.\n  11. Finally make sure that the examples have compiled successfully. You should find the compiled jar in the /examples/target directory under the name mahout-examples-\n{version}\n.job.jar\n  12. This concludes all the setup required to run the examples.\n\nClustering Examples\nThere are examples available for three clustering algorithms:\n\nCanopy Clustering: https://mahout.apache.org/users/clustering/canopy-clustering.html\nk-Means Clustering: https://mahout.apache.org/users/clustering/k-means-clustering.html\nFuzzy k-Means Clustering: https://mahout.apache.org/users/clustering/fuzzy-k-means.html\n\nDepending on the example you want to run the following command can be used:\n\nCanopy Clustering: $MAHOUT_HOME/bin/mahout org.apache.mahout.clustering.syntheticcontrol.canopy.Job\nk-Means Clustering: $MAHOUT_HOME/bin/mahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job\nFuzzy k-Means Clustering: $MAHOUT_HOME/bin/mahout org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job\n\nThe clustering output will be produced in the \u00ab output \u00bb directory on HDFS. The output should be copied to your local filesystem since it is overwritten on each run.\nUse the following command to copy out the data to your local filesystem:\n$HADOOP_HOME/bin/hadoop fs -get output $MAHOUT_HOME/examples\nThis creates an output folder inside examples directory. The output data points are in vector format. In order to read/analyze the output, you can use [clusterdump: https://mahout.apache.org/users/clustering/cluster-dumper.html] utility provided by Mahout.\nThe source code for these examples is located under the examples project.\n=====================================\nhttps://mahout.apache.org/users/clustering/clustering-seinfeld-episodes.html",
        "Issue Links": []
    },
    "MAHOUT-1452": {
        "Key": "MAHOUT-1452",
        "Summary": "Kmeans unexpected behaviour after removal of file scheme in output path for method mapreduce",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Bikash Gupta",
        "Created": "12/Mar/14 20:03",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "18/Apr/14 07:30",
        "Description": "Remove the hdfs scheme from output path, it will create clusters-0 in local file system and clusters-1 in HDFS and after that it spits an error as it expects clusters-0 to be in HDFS. Please check below stacktrace\n2014-03-11 14:52:15 o.a.m.c.AbstractJob [INFO] Command line arguments: {--clustering=null, --clusters=[/3/clusters-0-final], --convergenceDelta=[0.1], --distanceMeasure=[org.apache.mahout.common.distance.EuclideanDistanceMeasure], --endPhase=[2147483647], --input=[/2/sequence], --maxIter=[100], --method=[mapreduce], --output=[/5], --overwrite=null, --startPhase=[0], --tempDir=[temp]}\n2014-03-11 14:52:15 o.a.h.u.NativeCodeLoader [WARN] Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2014-03-11 14:52:15 o.a.m.c.k.KMeansDriver [INFO] Input: /2/sequence Clusters In: /3/clusters-0-final Out: /5\n2014-03-11 14:52:15 o.a.m.c.k.KMeansDriver [INFO] convergence: 0.1 max Iterations: 100\n2014-03-11 14:52:16 o.a.h.m.JobClient [WARN] Use GenericOptionsParser for parsing the arguments. Applications should implement Tool for the same.\n2014-03-11 14:52:17 o.a.h.m.l.i.FileInputFormat [INFO] Total input paths to process : 3\n2014-03-11 14:52:19 o.a.h.m.JobClient [INFO] Running job: job_201403111332_0011\n2014-03-11 14:52:20 o.a.h.m.JobClient [INFO]  map 0% reduce 0%\n2014-03-11 14:52:28 o.a.h.m.JobClient [INFO] Task Id : attempt_201403111332_0011_m_000000_0, Status : FAILED\n2014-03-11 14:52:28 STDIO [ERROR] java.lang.IllegalStateException: /5/clusters-0\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:78)\n        at org.apache.mahout.clustering.classify.ClusterClassifier.readFromSeqFiles(ClusterClassifier.java:208)\n        at org.apache.mahout.clustering.iterator.CIMapper.setup(CIMapper.java:44)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:138)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:672)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:330)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:268)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)\n        at org.apache.hadoop.mapred.Child.main(Child.java:262)\nCaused by: java.io.FileNotFoundException: File /5/clusters-0\nIf you provide HDFS uri in output then it works like a charm.",
        "Issue Links": []
    },
    "MAHOUT-1453": {
        "Key": "MAHOUT-1453",
        "Summary": "ImplicitFeedbackAlternatingLeastSquaresSolver add support for user supplied confidence functions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Adam Ilardi",
        "Created": "13/Mar/14 14:44",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "18/May/14 06:45",
        "Description": "double confidence(double rating) \n{\n    return 1 + alpha * rating;\n  }\n\nThe original paper mentions other functions that could be used as well. @ the moment It's not easy for a user to change this without compiling the source.",
        "Issue Links": []
    },
    "MAHOUT-1454": {
        "Key": "MAHOUT-1454",
        "Summary": "Remove the seinfeld clustering example from the website",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gaurav Misra",
        "Created": "13/Mar/14 15:08",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "23/Mar/14 06:24",
        "Description": "I wanted to move this example to the mahout website but it seems like the example itself is not working at this point. Unless the original author is able to fix it, we should remove this link as it might confuse newcomers. \nhttps://mahout.apache.org/users/clustering/clustering-seinfeld-episodes.html",
        "Issue Links": [
            "/jira/browse/MAHOUT-1474"
        ]
    },
    "MAHOUT-1455": {
        "Key": "MAHOUT-1455",
        "Summary": "Forkcount config causes JVM crashes during build",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Frank Scholten",
        "Reporter": "Frank Scholten",
        "Created": "13/Mar/14 19:14",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/Mar/14 19:14",
        "Description": "When building Mahout the JVM crashes occasionally because it cannot allocate enough memory. \nI changed the forkcount property to 2 instead of 1.5C, which is 1.5 the number of cores.\nSee commit: #1574644",
        "Issue Links": []
    },
    "MAHOUT-1456": {
        "Key": "MAHOUT-1456",
        "Summary": "The wikipediaXMLSplitter example fails with \"heap size\" error",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": null,
        "Reporter": "mahmood",
        "Created": "13/Mar/14 19:14",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "27/Apr/14 11:37",
        "Description": "1- The XML file is http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n2- When I run \"mahout wikipediaXMLSplitter -d enwiki-latest-pages-articles.xml -o wikipedia/chunks -c 64\", it stuck at chunk #571 and after 30 minutes it fails to continue with the java heap size error. Previous chunks are created rapidly (10 chunks per second).\n3- Increasing the heap size via \"-Xmx4096m\" option doesn't work.\n4- No matter what is the configuration, it seems that there is a memory leak that eat all space.",
        "Issue Links": []
    },
    "MAHOUT-1457": {
        "Key": "MAHOUT-1457",
        "Summary": "Move EigenSeedGenerator into spectral kmeans package",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Frank Scholten",
        "Reporter": "Frank Scholten",
        "Created": "13/Mar/14 19:16",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "13/Mar/14 19:18",
        "Description": "Code was not used by code in original package.\nSee commit #1574645",
        "Issue Links": []
    },
    "MAHOUT-1458": {
        "Key": "MAHOUT-1458",
        "Summary": "Remove KMeansConfigKeys and FuzzyKMeansConfigKeys",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Frank Scholten",
        "Reporter": "Frank Scholten",
        "Created": "13/Mar/14 19:18",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "13/Mar/14 19:18",
        "Description": "These classes are not used so they can be removed.\nSee commit #1574647",
        "Issue Links": []
    },
    "MAHOUT-1459": {
        "Key": "MAHOUT-1459",
        "Summary": "Move Hadoop related code out of CanopyClusterer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Frank Scholten",
        "Reporter": "Frank Scholten",
        "Created": "13/Mar/14 19:21",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/Mar/14 19:21",
        "Description": "Canopy clusterer no longer has Hadoop dependencies.\nHadoop configuration of canopy configuration now happens in CanopyConfigKeys.\nSee commit #1574684",
        "Issue Links": []
    },
    "MAHOUT-1460": {
        "Key": "MAHOUT-1460",
        "Summary": "Remove reference to Dirichlet in ClusterIterator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Frank Scholten",
        "Reporter": "Frank Scholten",
        "Created": "13/Mar/14 19:22",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/Mar/14 19:22",
        "Description": "Dirichlet clustering is no longer supported so remove the Javadoc.\nSee commit #1574687",
        "Issue Links": []
    },
    "MAHOUT-1461": {
        "Key": "MAHOUT-1461",
        "Summary": "The tour",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Scott",
        "Created": "13/Mar/14 19:58",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "27/Apr/14 07:53",
        "Description": "The documentation at https://cwiki.apache.org/confluence/display/MAHOUT/Quick+tour+of+text+analysis+using+the+Mahout+command+line is out of date.",
        "Issue Links": []
    },
    "MAHOUT-1462": {
        "Key": "MAHOUT-1462",
        "Summary": "Cleaning up Random Forests documentation on Mahout website",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Musselman",
        "Reporter": "Manoj Awasthi",
        "Created": "14/Mar/14 09:28",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "11/Aug/15 23:49",
        "Description": "Following are the items which need be added or changed. \nI think this page can be broken into two segments. First can be following: \n<snip>\nIntroduction to Random Forests\nRandom Forests are an ensemble machine learning technique originally proposed by Leo Breiman (UCB) which uses classification and regression trees as underlying classification mechanism. Trademark to Random Forest is maintained by Leo Breiman and Adele Cutler. \nOfficial website for Random Forests: http://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm\nOriginal paper published: http://oz.berkeley.edu/~breiman/randomforest2001.pdf\n</snip>\nSecond section can following: \n<snip>\nClassifying with random forests with Mahout\n</snip>\nThis section can be what it is right now on the website.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1478"
        ]
    },
    "MAHOUT-1463": {
        "Key": "MAHOUT-1463",
        "Summary": "Modify OnlineSummarizers to use the TDigest dependency from Maven Central.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "16/Mar/14 06:03",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "16/Mar/14 23:22",
        "Description": "TDigest is now being used across several other projects and not being specific to Mahout, need to :-\na) modify code to include TDigest as a maven dependency \nb) purge existing TDigest.java and GroupTree.java from codebase",
        "Issue Links": []
    },
    "MAHOUT-1464": {
        "Key": "MAHOUT-1464",
        "Summary": "Cooccurrence Analysis on Spark",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "16/Mar/14 19:43",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "18/Mar/15 13:51",
        "Description": "Create a version of Cooccurrence Analysis (RowSimilarityJob with LLR) that runs on Spark. This should be compatible with Mahout Spark DRM DSL so a DRM can be used as input. \nIdeally this would extend to cover MAHOUT-1422. This cross-cooccurrence has several applications including cross-action recommendations.",
        "Issue Links": []
    },
    "MAHOUT-1465": {
        "Key": "MAHOUT-1465",
        "Summary": "Clean up README",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "16/Mar/14 21:24",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "17/Mar/14 02:55",
        "Description": "Clean up URIs to documentation, tidy up bullets, pull out the paragraph of text and just point people to the web page.",
        "Issue Links": []
    },
    "MAHOUT-1466": {
        "Key": "MAHOUT-1466",
        "Summary": "Cluster visualization fails to execute",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "17/Mar/14 06:33",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "17/Mar/14 20:12",
        "Description": "When executing the cluster visualization as described in https://mahout.apache.org/users/clustering/visualizing-sample-clusters.html , the following error occurs:\n\nException in thread \"main\" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory\n\tat org.apache.mahout.clustering.display.DisplayClustering.<clinit>(DisplayClustering.java:66)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:190)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:113)\nCaused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\t... 4 more",
        "Issue Links": []
    },
    "MAHOUT-1467": {
        "Key": "MAHOUT-1467",
        "Summary": "ClusterClassifier readPolicy leaks file handles",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Avi Shinnar",
        "Created": "17/Mar/14 19:12",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "18/Mar/14 00:14",
        "Description": "The org.apache.mahout.clustering.classify.ClusterClassifier.readPolicy method leaks file handles.\nThis leak causes a serious problems in setups that reuse JVMs for multiple tasks (as with \"mapred.job.reuse.jvm.num.tasks\").\nIn more detail:\nThe org.apache.mahout.clustering.classify.ClusterClassifier class has  static methods to read policies: readPolicy.\nIt opens a SequenceFile.Reader and reads from it, and then neglects to close it.\nsuggested fix:\nadd a call to reader.close() before the return statement in readPolicy",
        "Issue Links": []
    },
    "MAHOUT-1468": {
        "Key": "MAHOUT-1468",
        "Summary": "Creating a new page for StreamingKMeans documentation on mahout website",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Musselman",
        "Reporter": "Pavan Kumar N",
        "Created": "19/Mar/14 16:40",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "25/Apr/14 15:18",
        "Description": "Separate page required on Streaming K Means algorithm description and overview, explaining the various parameters can be used in streamingkmeans, strategy for parallelization, link to this paper: http://papers.nips.cc/paper/3812-streaming-k-means-approximation.pdf",
        "Issue Links": []
    },
    "MAHOUT-1469": {
        "Key": "MAHOUT-1469",
        "Summary": "Streaming KMeans fails when executed in MapReduce mode and REDUCE_STREAMING_KMEANS is set to true",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "1.0.0",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "20/Mar/14 06:47",
        "Updated": "22/May/16 00:07",
        "Resolved": "30/Mar/16 02:06",
        "Description": "Centroids are not being generated when executed in MR mode with -rskm flag set. \n\n14/03/20 02:42:12 INFO mapreduce.StreamingKMeansThread: Estimated Points: 282\n14/03/20 02:42:12 INFO mapred.JobClient:  map 100% reduce 0%\n14/03/20 02:42:14 INFO mapreduce.StreamingKMeansReducer: Number of Centroids: 0\n14/03/20 02:42:14 WARN mapred.LocalJobRunner: job_local1374896815_0001\njava.lang.IllegalArgumentException: Must have nonzero number of training and test vectors. Asked for %.1f %% of %d vectors for test [10.000000149011612, 0]\n\tat com.google.common.base.Preconditions.checkArgument(Preconditions.java:148)\n\tat org.apache.mahout.clustering.streaming.cluster.BallKMeans.splitTrainTest(BallKMeans.java:176)\n\tat org.apache.mahout.clustering.streaming.cluster.BallKMeans.cluster(BallKMeans.java:192)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.getBestCentroids(StreamingKMeansReducer.java:107)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:73)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:37)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:177)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\n14/03/20 02:42:14 INFO mapred.JobClient: Job complete: job_local1374896815_0001\n14/03/20 02:42:14 INFO mapred.JobClient: Counters: 16\n14/03/20 02:42:14 INFO mapred.JobClient:   File Input Format Counters \n14/03/20 02:42:14 INFO mapred.JobClient:     Bytes Read=17156391\n14/03/20 02:42:14 INFO mapred.JobClient:   FileSystemCounters\n14/03/20 02:42:14 INFO mapred.JobClient:     FILE_BYTES_READ=41925624\n14/03/20 02:42:14 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=25974741\n14/03/20 02:42:14 INFO mapred.JobClient:   Map-Reduce Framework\n14/03/20 02:42:14 INFO mapred.JobClient:     Map output materialized bytes=956293\n14/03/20 02:42:14 INFO mapred.JobClient:     Map input records=21578\n14/03/20 02:42:14 INFO mapred.JobClient:     Reduce shuffle bytes=0\n14/03/20 02:42:14 INFO mapred.JobClient:     Spilled Records=282\n14/03/20 02:42:14 INFO mapred.JobClient:     Map output bytes=1788012\n14/03/20 02:42:14 INFO mapred.JobClient:     Total committed heap usage (bytes)=217214976\n14/03/20 02:42:14 INFO mapred.JobClient:     Combine input records=0\n14/03/20 02:42:14 INFO mapred.JobClient:     SPLIT_RAW_BYTES=163\n14/03/20 02:42:14 INFO mapred.JobClient:     Reduce input records=0\n14/03/20 02:42:14 INFO mapred.JobClient:     Reduce input groups=0\n14/03/20 02:42:14 INFO mapred.JobClient:     Combine output records=0\n14/03/20 02:42:14 INFO mapred.JobClient:     Reduce output records=0\n14/03/20 02:42:14 INFO mapred.JobClient:     Map output records=282\n14/03/20 02:42:14 INFO driver.MahoutDriver: Program took 506269 ms (Minutes: 8.437816666666667)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1486"
        ]
    },
    "MAHOUT-1470": {
        "Key": "MAHOUT-1470",
        "Summary": "Topic dump",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "Clustering",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "20/Mar/14 22:00",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "05/Nov/15 23:27",
        "Description": "Per http://mail-archives.apache.org/mod_mbox/mahout-user/201403.mbox/%3CCAMc_qaL2DCgbVbam2miNsLpa4qvaA9sMy1-arccF9Nz6ApcsvQ%40mail.gmail.com%3E\n> The script needs to be corrected to not call vectordump for LDA as\n> vectordump utility (or even clusterdump) are presently not capable of\n> displaying topics and relevant documents. I recall this issue was\n> previously reported by Peyman Faratin post 0.9 release.\n>\n> Mahout's missing a clusterdump utility that reads in LDA\n> topics, Document - DocumentId mapping and displays a report of the topics\n> and the documents that belong to a topic.",
        "Issue Links": []
    },
    "MAHOUT-1471": {
        "Key": "MAHOUT-1471",
        "Summary": "Cleanup website on Canopy Clustering",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:33",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "23/Mar/14 14:36",
        "Description": "The websites on Canopy clustering needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/clustering/canopy-clustering.html\nhttps://mahout.apache.org/users/clustering/canopy-commandline.html",
        "Issue Links": []
    },
    "MAHOUT-1472": {
        "Key": "MAHOUT-1472",
        "Summary": "Cleanup website on Fuzzy k-Means",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:34",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "23/Mar/14 05:14",
        "Description": "The websites on fuzzy k-Means needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/clustering/fuzzy-k-means.html\nhttps://mahout.apache.org/users/clustering/fuzzy-k-means-commandline.html",
        "Issue Links": []
    },
    "MAHOUT-1473": {
        "Key": "MAHOUT-1473",
        "Summary": "Cleanup website on Spectral Clustering",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Shannon Quinn",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:35",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "23/Mar/14 08:37",
        "Description": "The website on spectral clustering needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/clustering/spectral-clustering.html",
        "Issue Links": []
    },
    "MAHOUT-1474": {
        "Key": "MAHOUT-1474",
        "Summary": "Remove Seinfeld clustering example",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Frank Scholten",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:39",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "24/Mar/14 19:44",
        "Description": "The example on clustering seinfeld episodes need to be added to the website.\nhttps://mahout.apache.org/users/clustering/clustering-seinfeld-episodes.html",
        "Issue Links": [
            "/jira/browse/MAHOUT-1454"
        ]
    },
    "MAHOUT-1475": {
        "Key": "MAHOUT-1475",
        "Summary": "Clean up website on Naive Bayes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:44",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "23/Mar/14 05:46",
        "Description": "The website on Naive Bayes needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/classification/bayesian.html",
        "Issue Links": []
    },
    "MAHOUT-1476": {
        "Key": "MAHOUT-1476",
        "Summary": "Cleanup website on Hidden Markov Models",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Musselman",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:45",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "28/Mar/14 15:17",
        "Description": "The website on Hidden Markov Models needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/stuff/hidden-markov-models.html",
        "Issue Links": []
    },
    "MAHOUT-1477": {
        "Key": "MAHOUT-1477",
        "Summary": "Clean up website on Logistic Regression",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:48",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "30/Mar/15 17:05",
        "Description": "The website on Logistic regression needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code. We should also link to the example created in MAHOUT-1425 \nhttps://mahout.apache.org/users/classification/logistic-regression.html",
        "Issue Links": []
    },
    "MAHOUT-1478": {
        "Key": "MAHOUT-1478",
        "Summary": "Clean up website on Random Forests",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:51",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "23/Mar/14 04:52",
        "Description": "The website on Random Forest needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/stuff/partial-implementation.html",
        "Issue Links": [
            "/jira/browse/MAHOUT-1462"
        ]
    },
    "MAHOUT-1479": {
        "Key": "MAHOUT-1479",
        "Summary": "Cleanup website on wikipedia example",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:53",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "27/Apr/14 20:51",
        "Description": "The website on the wikipedia example needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/classification/wikipedia-bayes-example.html",
        "Issue Links": []
    },
    "MAHOUT-1480": {
        "Key": "MAHOUT-1480",
        "Summary": "Clean up website on 20 newsgroups",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:55",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "25/May/14 11:50",
        "Description": "The website on the twenty newsgroups example needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/clustering/twenty-newsgroups.html",
        "Issue Links": []
    },
    "MAHOUT-1481": {
        "Key": "MAHOUT-1481",
        "Summary": "Clean up website on breiman example",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 14:56",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "27/Apr/14 07:57",
        "Description": "The website on the breiman example needs clean up. We need to go through the text, remove dead links and check whether the information is still consistent with the current code.\nhttps://mahout.apache.org/users/classification/breiman-example.html",
        "Issue Links": []
    },
    "MAHOUT-1482": {
        "Key": "MAHOUT-1482",
        "Summary": "Rework quickstart website",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "22/Mar/14 15:00",
        "Updated": "12/Apr/15 17:24",
        "Resolved": "05/Apr/14 09:10",
        "Description": "Our quickstart website must be much more helpful.\nWe should describe how to add Mahout as maven dependency to a Java project and how to start working on the source code.\nhttps://mahout.apache.org/users/basics/quickstart.html",
        "Issue Links": []
    },
    "MAHOUT-1483": {
        "Key": "MAHOUT-1483",
        "Summary": "Organize links in web site navigation bar",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "22/Mar/14 16:32",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "13/Apr/14 19:34",
        "Description": "Some links in the drop-down menus in the navigation bar are inconsistent.\nUnder \"Basics\", there are some links whose path starts with '/users/basics', one that starts with '/users/dim-reduction', one that starts with '/users/clustering', and one that starts with '/users/sparkbindings'.  These inconsistencies aren't terrible but are a little confusing for maintaining the site.\nUnder \"Classification\", there are some that start with '/users/classification', some that start with '/users/stuff', and there's a clustering example (20 newsgroups example).",
        "Issue Links": []
    },
    "MAHOUT-1484": {
        "Key": "MAHOUT-1484",
        "Summary": "Spectral algorithm for HMMs",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Emaad Manzoor",
        "Created": "23/Mar/14 09:19",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "18/May/14 06:42",
        "Description": "Following up with this comment by isabel on the sequential HMM proposal, is there any interest in a spectral algorithm as described in: \"A spectral algorithm for learning hidden Markov models (D. Hsu, S. Kakade, T. Zhang)\"?\nI would like to take up this effort.\nThis will enable learning the parameters of and making predictions with a HMM in a single step. At its core, the algorithm involves computing estimates from triples of observations, performing an SVD and then some matrix multiplications.\nThis could also form the base for an implementation of \"Hilbert Space Embeddings of Hidden Markov Models (L. Song, B. Boots, S. Saddiqi, G. Gordon, A. Smola)\".",
        "Issue Links": []
    },
    "MAHOUT-1485": {
        "Key": "MAHOUT-1485",
        "Summary": "Clean up Recommender Overview page",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "24/Mar/14 06:10",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "27/Mar/15 02:19",
        "Description": "Clean up the recommender overview page, remove outdated content and make sure the examples work.\nhttps://mahout.apache.org/users/recommender/recommender-documentation.html",
        "Issue Links": []
    },
    "MAHOUT-1486": {
        "Key": "MAHOUT-1486",
        "Summary": "Streaming KMeans NPE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Reinis Vicups",
        "Created": "24/Mar/14 12:21",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "24/Mar/14 12:32",
        "Description": "I am assuming that this occurs because of  --reduceStreamingKMeans (-rskm) option set. Will try and test it without reduce and report if the NPE goes away.\nError: java.lang.NullPointerException\n        at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:187)\n        at org.apache.mahout.math.random.WeightedThing.<init>(WeightedThing.java:31)\n        at org.apache.mahout.math.neighborhood.BruteSearch.searchFirst(BruteSearch.java:127)\n        at org.apache.mahout.clustering.ClusteringUtils.estimateDistanceCutoff(ClusteringUtils.java:116)\n        at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansThread.call(StreamingKMeansThread.java:63)\n        at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:55)\n        at org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansReducer.reduce(StreamingKMeansReducer.java:35)\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)\n        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:165)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:160)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1469"
        ]
    },
    "MAHOUT-1487": {
        "Key": "MAHOUT-1487",
        "Summary": "More understandable error message when attempt to use wrong FileSystem",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Konstantin",
        "Created": "24/Mar/14 18:59",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/May/14 06:48",
        "Description": "RandomSeedGenerator has following code:\nFileSystem fs = FileSystem.get(output.toUri(), conf);\n...\nfs.getFileStatus(input).isDir() \nIf specify output path correctly and input path not correctly, Mahout throws not well understandable error message. \"Exception in thread \"main\" java.lang.IllegalArgumentException: This file system object (hdfs://172.31.41.65:9000) does not support access to the request path 's3://by.kslisenko.bigdata/stackovweflow-small/out_new/sparse/tfidf-vectors' You possibly called FileSystem.get(conf) when you should have called FileSystem.get(uri, conf) to obtain a file system supporting your path\"\nThis happens because FileSystem object was created from output path, and getFileStatus has parameter for input path. This caused misunderstanding when try to understand what error message means.\nTo prevent this misunderstanding, I propose to improve error message adding following details:\n1. Specify which filesystem type used (DistributedFileSystem, NativeS3FileSystem, etc. using fs.getClass().getName())\n2. Then specify which path can not be processed correctly.\nThis can be done by validation utility which can be applied to many places in Mahout. When we use Mahout we need to specify many paths and we also can use many types of file systems: local for debugging, distributed on Hadoop, and s3 on Amazon. In this case better error messages can save much time.",
        "Issue Links": []
    },
    "MAHOUT-1488": {
        "Key": "MAHOUT-1488",
        "Summary": "DisplaySpectralKMeans fails: examples/output/clusteredPoints/part-m-00000 does not exist.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Saleem Ansari",
        "Created": "26/Mar/14 08:08",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "26/Mar/14 20:06",
        "Description": "Running the DisplaySpectralKMeans class fails with following error, at the GUI display step:\nException in thread \"AWT-EventQueue-0\" java.lang.IllegalStateException: output/clusteredPoints/part-m-00000\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable.iterator(SequenceFileIterable.java:63)\n\tat org.apache.mahout.clustering.display.DisplayClustering.plotClusteredSampleData(DisplayClustering.java:203)\n\tat org.apache.mahout.clustering.display.DisplaySpectralKMeans.paint(DisplaySpectralKMeans.java:86)\n\tat sun.awt.RepaintArea.paintComponent(RepaintArea.java:264)\n\tat sun.awt.X11.XRepaintArea.paintComponent(XRepaintArea.java:73)\n\tat sun.awt.RepaintArea.paint(RepaintArea.java:240)\n\tat sun.awt.X11.XComponentPeer.handleEvent(XComponentPeer.java:591)\n\tat java.awt.Component.dispatchEventImpl(Component.java:4937)\n\tat java.awt.Container.dispatchEventImpl(Container.java:2287)\n\tat java.awt.Window.dispatchEventImpl(Window.java:2719)\n\tat java.awt.Component.dispatchEvent(Component.java:4687)\n\tat java.awt.EventQueue.dispatchEventImpl(EventQueue.java:735)\n\tat java.awt.EventQueue.access$200(EventQueue.java:103)\n\tat java.awt.EventQueue$3.run(EventQueue.java:694)\n\tat java.awt.EventQueue$3.run(EventQueue.java:692)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)\n\tat java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:87)\n\tat java.awt.EventQueue$4.run(EventQueue.java:708)\n\tat java.awt.EventQueue$4.run(EventQueue.java:706)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.security.ProtectionDomain$1.doIntersectionPrivilege(ProtectionDomain.java:76)\n\tat java.awt.EventQueue.dispatchEvent(EventQueue.java:705)\n\tat java.awt.EventDispatchThread.pumpOneEventForFilters(EventDispatchThread.java:242)\n\tat java.awt.EventDispatchThread.pumpEventsForFilter(EventDispatchThread.java:161)\n\tat java.awt.EventDispatchThread.pumpEventsForHierarchy(EventDispatchThread.java:150)\n\tat java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:146)\n\tat java.awt.EventDispatchThread.pumpEvents(EventDispatchThread.java:138)\n\tat java.awt.EventDispatchThread.run(EventDispatchThread.java:91)\nCaused by: java.io.FileNotFoundException: File file:/home/saleem/work/learn/external/mahout/examples/output/clusteredPoints/part-m-00000 does not exist.\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:402)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:255)\n\tat org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:816)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1479)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1474)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileIterator.<init>(SequenceFileIterator.java:63)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileIterable.iterator(SequenceFileIterable.java:61)\n\t... 28 more\nThis is because SpectralKMeansDriver writes clustered points to \noutput/kmeans_out/clusteredPoints/part-m-00000\nBut DisplaySpectralKMeans is looking at:\noutput/clusteredPoints/part-m-00000",
        "Issue Links": []
    },
    "MAHOUT-1489": {
        "Key": "MAHOUT-1489",
        "Summary": "Interactive Scala & Spark Bindings Shell & Script processor",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Saikat Kanjilal",
        "Created": "26/Mar/14 17:49",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "06/May/14 19:19",
        "Description": "Build an interactive shell /scripting (just like spark shell). Something very similar in R interactive/script runner mode.",
        "Issue Links": []
    },
    "MAHOUT-1490": {
        "Key": "MAHOUT-1490",
        "Summary": "Data frame R-like bindings",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Grant Ingersoll",
        "Reporter": "Saikat Kanjilal",
        "Created": "26/Mar/14 17:50",
        "Updated": "27/Mar/15 06:19",
        "Resolved": "27/Mar/15 06:19",
        "Description": "Create Data frame R-like bindings for spark",
        "Issue Links": []
    },
    "MAHOUT-1491": {
        "Key": "MAHOUT-1491",
        "Summary": "Spectral KMeans Clustering doesn't clean its /tmp dir and fails when seeing it again",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "26/Mar/14 23:56",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "27/Mar/14 02:54",
        "Description": "Spectral KMeans clustering presently doesn't clean its /tmp directory on exit and fails when seeing it again.",
        "Issue Links": []
    },
    "MAHOUT-1492": {
        "Key": "MAHOUT-1492",
        "Summary": "Doap file has references to cwiki still",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "27/Mar/14 17:31",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "27/Mar/14 19:11",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1493": {
        "Key": "MAHOUT-1493",
        "Summary": "Port Naive Bayes to the Spark DSL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Sebastian Schelter",
        "Created": "27/Mar/14 21:15",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "06/Apr/15 03:25",
        "Description": "Port our Naive Bayes implementation to the new spark dsl. Shouldn't require more than a few lines of code.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1564"
        ]
    },
    "MAHOUT-1494": {
        "Key": "MAHOUT-1494",
        "Summary": "README.txt is examples/clustering needs to be updated",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Andrew Musselman",
        "Reporter": "Suneel Marthi",
        "Created": "28/Mar/14 16:48",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "09/Apr/14 09:40",
        "Description": "List of clustering algorithms referenced in examples/src/main/java/org/apache/mahout/clustering/display/Readme.txt needs to be updated.\n1. Remove Mean shift (and associated code if any)\n2. Add 'DisplaySpectralKMeans - use Spectral KMeans clustering'",
        "Issue Links": []
    },
    "MAHOUT-1495": {
        "Key": "MAHOUT-1495",
        "Summary": "Create a website describing the distributed item-based recommender",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering,                                            Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "28/Mar/14 16:57",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "28/Mar/15 17:16",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1496": {
        "Key": "MAHOUT-1496",
        "Summary": "Create a website describing the distributed ALS recommender",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering,                                            Documentation",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "28/Mar/14 16:57",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "20/Apr/14 13:23",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1497": {
        "Key": "MAHOUT-1497",
        "Summary": "mahout resplit not producing splited files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Reinis Vicups",
        "Created": "28/Mar/14 21:18",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/Apr/14 13:12",
        "Description": "when I run \"mahout resplit\", I get the output below but no split files are being produced.\n\nsupport@hadoop1:~$ mahout resplit --input .../final/clusteredPoints/part-m-* --output .../final/split --numSplits 4\nMAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\nRunning on hadoop, using /opt/cloudera/parcels/CDH-5.0.0-0.cdh5b2.p0.27/bin/../lib/hadoop/bin/hadoop and HADOOP_CONF_DIR=/etc/hadoop/conf\nMAHOUT-JOB: /opt/cloudera/parcels/CDH-5.0.0-0.cdh5b2.p0.27/lib/mahout/mahout-examples-0.8-cdh5.0.0-beta-2-job.jar\n14/03/28 16:22:50 WARN driver.MahoutDriver: No resplit.props found on classpath, will use command-line arguments only\nWriting 4 splits\nWriting split 0\nWriting split 1\nWriting split 2\nWriting split 3\n14/03/28 16:22:52 INFO driver.MahoutDriver: Program took 2077 ms (Minutes: 0.034616666666666664)\n\n\nThe folder \"cluteredPoints\" passed to --input of resplit contains clustered points generated by k-means algorithm from mahout.",
        "Issue Links": []
    },
    "MAHOUT-1498": {
        "Key": "MAHOUT-1498",
        "Summary": "DistributedCache.setCacheFiles in DictionaryVectorizer overwrites jars pushed using oozie",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sergey",
        "Created": "30/Mar/14 15:02",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "18/May/14 17:35",
        "Description": "Hi, I get exception \n\n<<< Invocation of Main class completed <<<\n\nFailing Oozie Launcher, Main class [org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles], main() threw exception, Job failed!\njava.lang.IllegalStateException: Job failed!\nat org.apache.mahout.vectorizer.DictionaryVectorizer.makePartialVectors(DictionaryVectorizer.java:329)\nat org.apache.mahout.vectorizer.DictionaryVectorizer.createTermFrequencyVectors(DictionaryVectorizer.java:199)\nat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.run(SparseVectorsFromSequenceFiles.java:271)\n\n\nThe root cause is:\n\nError: java.lang.ClassNotFoundException: org.apache.mahout.math.Vector\nat java.net.URLClassLoader$1.run(URLClassLoader.java:202)\nat java.security.AccessController.doPrivileged(Native Method)\nat java.net.URLClassLoader.findClass(URLClassLoader.java:190)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:306)\nat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\nat java.lang.ClassLoader.loadClass(ClassLoader.java:247)\nat java.lang.Class.forName0(Native Method)\nat java.lang.Class.forName(Class.java:247\n\n\nLooks like it happens because of \nDictionaryVectorizer.makePartialVectors method.\nIt has code:\n\nDistributedCache.setCacheFiles(new URI[] {dictionaryFilePath.toUri()}, conf);\n\n\nwhich overrides jars pushed with job by oozie:\n\npublic static void More ...setCacheFiles(URI[] files, Configuration conf) {\n         String sfiles = StringUtils.uriToString(files);\n         conf.set(\"mapred.cache.files\", sfiles);\n}",
        "Issue Links": []
    },
    "MAHOUT-1499": {
        "Key": "MAHOUT-1499",
        "Summary": "Build error: org.eclipse.paho:mqtt-client",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Ken Williams",
        "Created": "31/Mar/14 17:52",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "31/Mar/14 19:39",
        "Description": "Using Maven, I'm unable to build the 0.9.0 distribution I just downloaded. I attempt like so:\n\nmvn -U -Pyarn -Dhadoop.version=2.2.0 -Dyarn.version=2.2.0 -DskipTests package\n\n\nThe Maven error is:\n\n[ERROR] Failed to execute goal on project spark-examples_2.10: Could not resolve dependencies for project org.apache.spark:spark-examples_2.10:jar:0.9.0-incubating: Could not find artifact org.eclipse.paho:mqtt-client:jar:0.4.0 in nexus\n\n\nMy Maven version is 3.2.1, running on Java 1.7.0, using Scala 2.10.4.\nIs there an additional Maven repository I should add or something?\nIf I go into the pom.xml and comment out the external/mqtt and examples modules, the build succeeds. I'm fine without the MQTT stuff, but I would really like to get the examples working because I haven't played with Spark before.",
        "Issue Links": []
    },
    "MAHOUT-1500": {
        "Key": "MAHOUT-1500",
        "Summary": "H2O integration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Anand Avati",
        "Created": "01/Apr/14 07:02",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "05/Sep/14 23:33",
        "Description": "Provide H2O backend for the Mahout DSL",
        "Issue Links": []
    },
    "MAHOUT-1501": {
        "Key": "MAHOUT-1501",
        "Summary": "ClusterOutputPostProcessorDriver has private default constructor",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Reinis Vicups",
        "Created": "01/Apr/14 15:39",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "05/Apr/14 09:53",
        "Description": "Some guy just decided to make default constructor of org.apache.mahout.clustering.topdown.postprocessor.ClusterOutputPostProcessorDriver private.\nThis is not really bug, it's just inconsistent since all the other Drivers I found in mahout-core do not have private constructor.\nThe private constructor inhibits me from doing this:\n\nreturn ToolRunner.run(new ClusterOutputPostProcessorDriver(), clusterPPArgsStrings);\n\n\nthe very same way I am doing this:\n\nreturn ToolRunner.run(new KMeansDriver(), kmeansArgsStrings);\n\n\nFix would be to just remove private constructor.",
        "Issue Links": []
    },
    "MAHOUT-1502": {
        "Key": "MAHOUT-1502",
        "Summary": "Update Naive Bayes Webpage to Current Implementation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "02/Apr/14 02:24",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "01/May/14 18:49",
        "Description": "Current Naive Bayes page is for pre .7 NB implementation:\nhttps://mahout.apache.org/users/classification/bayesian.html\npost .7, TF-IDF calculations are preformed outside of NB.",
        "Issue Links": []
    },
    "MAHOUT-1503": {
        "Key": "MAHOUT-1503",
        "Summary": "TestNaiveBayesDriver fails in sequential mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification,                                            Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "02/Apr/14 02:44",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "15/Apr/14 02:36",
        "Description": "As reported by Chandler Burgess, testnb fails in sequential mode with exception:\nException in thread \"main\" java.io.FileNotFoundException: /tmp/mahout-work-andy/20news-train-vectors (Is a directory)\n\tat java.io.FileInputStream.open(Native Method)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:120)\n\tat org.apache.hadoop.fs.RawLocalFileSystem$TrackingFileInputStream.<init>(RawLocalFileSystem.java:71)\n{...}\tat org.apache.mahout.classifier.naivebayes.test.TestNaiveBayesDriver.run(TestNaiveBayesDriver.java:99){...}",
        "Issue Links": []
    },
    "MAHOUT-1504": {
        "Key": "MAHOUT-1504",
        "Summary": "Enable/fix thetaSummer job in TrainNaiveBayesJob",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification,                                            Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "02/Apr/14 02:58",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "15/Apr/14 02:57",
        "Description": "A new implementation of Naive Bayes was introduced in .7.  The weight (theta) normalization job was at least partially carried over but not fully implemented or enabled.  Weight normalization does not effect simple NB or CNB however enabling it will allow for all NB implementations in the Rennie et al. paper.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1369"
        ]
    },
    "MAHOUT-1505": {
        "Key": "MAHOUT-1505",
        "Summary": "structure of clusterdump's JSON output",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Andrew Musselman",
        "Reporter": "Terry Blankers",
        "Created": "02/Apr/14 21:23",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "31/May/14 17:26",
        "Description": "Hi all, I'm working on some automated analysis of the clusterdump output using '-of = JSON'. While digging into the structure of the representation of the data I've noticed something that seems a little odd to me.\nIn order to access the data for a particular cluster, the 'cluster', 'n', 'c' & 'r' values are all in one continuous string. For example:\n\n{\"cluster\":\"VL-10515{n=5924 c=[action:0.023, adherence:0.223, administration:0.011 r=[action:0.446, adherence:1.501, administration:0.306]}\"}\n\n\nThis is also the case for the \"point\":\n\n{\"point\":\"013FFD34580BA31AECE5D75DE65478B3D691D138 = [body:6.904, harm:10.101]\",\"vector_name\":\"013FFD34580BA31AECE5D75DE65478B3D691D138\",\"weight\":\"1.0\"}\n\n\nThis leads me to believe that the only way I can get to the individual data in these items is by string parsing. For JSON deserialization I would have expected to see something along the lines of:\n\n{\n    \"cluster\":\"VL-10515\",\n    \"n\":5924,\n    \"c\":\n    [\n        {\"action\":0.023},\n        {\"adherence\":0.223},\n        {\"administration\":0.011}\n    ],\n    \"r\":\n    [\n        {\"action\":0.446},\n        {\"adherence\":1.501},\n        {\"administration\":0.306}\n    ]\n}\n\n\nand:\n\n{\n    \"point\": {\n        \"body\": 6.904,\n        \"harm\": 10.101\n    },\n    \"vector_name\": \"013FFD34580BA31AECE5D75DE65478B3D691D138\",\n    \"weight\": 1.0\n} \n\n\nAndrew Musselman replied:\n\nLooks like a bug to me as well; I would have expected something similar to\nwhat you were expecting except maybe something like this which puts the \"c\"\nand \"r\" values in objects rather than arrays of single-element objects:\n\n{\n    \"cluster\":\"VL-10515\",\n    \"n\":5924,\n    \"c\":\n    {\n        \"action\":0.023,\n        \"adherence\":0.223,\n        \"administration\":0.011\n    },\n    \"r\":\n    {\n       \"action\":0.446,\n       \"adherence\":1.501,\n       \"administration\":0.306\n    }\n}",
        "Issue Links": []
    },
    "MAHOUT-1506": {
        "Key": "MAHOUT-1506",
        "Summary": "Creation of affinity matrix for spectral clustering",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "03/Apr/14 17:29",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/Apr/14 07:24",
        "Description": "I wanted to get this discussion going, since I think this is a critical blocker for any kind of documentation update on spectral clustering (I can't update the documentation until the algorithm is useful, and it won't be useful until there's a built-in method for converting raw data to an affinity matrix).\nNamely, I'm wondering what kind of \"raw\" data should this algorithm be expecting (anything that k-means expects, basically?), and what are the data structures associated with this? I've created a proof-of-concept for how pairwise affinity generation could work.\nhttps://github.com/magsol/Hadoop-Affinity\nIt's a two-step job, but if the data structures in the input data format provides 1) the total number of data points, and 2) for each data point to know its index in the overall set, then the first job can be scrapped entirely and affinity generation will consist of 1 MR task.\n(discussions on Spark / h20 pending, of course)\nMainly this is an engineering problem at this point. Let me know your thoughts and I'll get this done (I'm out of town the next 10 days for my wedding/honeymoon, will get to this on my return).",
        "Issue Links": []
    },
    "MAHOUT-1507": {
        "Key": "MAHOUT-1507",
        "Summary": "Support input and output using user defined ID wherever possible",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "03/Apr/14 18:23",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "01/Apr/15 18:14",
        "Description": "All users of Mahout have data which is addressed by keys or IDs of their own devise. In order to use much of Mahout they must translate these IDs into Mahout IDs, then run their jobs and translate back again when retrieving the output. If the ID space is very large this is a difficult problem for users to solve at scale.\nFor many Mahout operations this would not be necessary if these external keys could be maintained for vectors and dimensions, or for rows and columns of a DRM.\nThe reason I bring this up now is that much groundwork is being laid for Mahout's future on Spark so getting this notion in early could be fundamentally important and used to build on.\nIf external IDs for rows and columns were maintained then RSJ, DRM Transpose (and other DRM ops), vector extraction, clustering, and recommenders would need no ID translation steps, a big user win.\nA partial solution might be to support external row IDs alone somewhat like the NamedVector and PropertyVector in the Mahout hadoop code.\nOn Apr 3, 2014, at 11:00 AM, Pat Ferrel <pat@occamsmachete.com> wrote:\nPerhaps this is best phrased as a feature request.\nOn Apr 2, 2014, at 2:55 PM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:\nPS.\nsequence file keys have also special meaning if they are Ints. .E.g. A'\nphysical operator requires keys to be ints, in which case it interprets\nthem as row indexes that become column indexes. This of course isn't always\nthe case, e.g. (Aexpr).t %*% Aexpr doesn't require int indices because in\nreality optimizer will never choose actual transposition as a physical step\nin such pipeline. This interpretation is consistent with interpretation of\nlong-existing Hadoop-side DistributedRowMatrix#transpose.\nOn Wed, Apr 2, 2014 at 2:45 PM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:\nOn Wed, Apr 2, 2014 at 1:56 PM, Pat Ferrel <pat@occamsmachete.com> wrote:\nOn Apr 2, 2014, at 1:39 PM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:\nI think this duality, names and keys, is not very healthy really, and\njust\ncreates addtutiinal hassle. Spark drm takes care of keys automatically\nthoughout, but propagating names from name vectors is solely algorithm\nconcern as it stands.\nNot sure what you mean.\nNot what you think, it looks like.\nI mean that Mahout DRM structure is a bag of (key -> Vector) pairs. When\npersisted, key goes to the key of a sequence file. In particular, it means\nthat there is a case of Bag[ key -> NamedVector]. Which means, external\nanchor could be saved to either key or name of a row. In practice it causes\ncompatibility mess, e.g. we saw those numerous cases where e.g. seq2sparse\nsaves external keys (file paths) into  key, whereas e.g. clustering\nalgorithms are not seeing them because they expect them to be the name part\nof the vector. I am just saying we have two ways to name the rows, and it\nis generally not a healthy choice for the aforementioned reason.\nIn my experience Names and Properties are primarily used to store\nexternal keys, which are quite healthy.\nUsers never have data with Mahout keys, they must constantly go back and\nforth. This is exactly what the R data frame does, no? I'm not so concerned\nwith being able to address an element by the external key\ndrmB[\"pat\"][\"iPad'] like a HashMap. But it would sure be nice to have the\nexternal ids follow the data through any calculation that makes sense.\nI am with you on this.\nThis would mean clustering, recommendations, transpose, RSJ would require\nno id transforming steps. This would make dealing with Mahout much easier.\nData frames is a little bit a different thing, right now we work just with\nmatrices. Although, yes, our in-core matrices support row and column names\n(just like in R) and distributed matrices support row keys only.  what i\nmean is that algebraic expression e.g.\nAexpr %*% Bexpr will automatically propagate keys from Aexpr as implied\nabove, but not necessarily named vectors, because internally algorithms\nblockify things into matrix blocks, and i am far from sure that Mahout\nin-core stuff works correctly with named vectors as part of a matrix block\nin all situations. I may be wrong. I always relied on sequence file keys to\nidentify data points.\nNote that sequence file keys are bigger than just a name, it is anything\nWritable. I.e. you could save a data structure there, as long as you have a\nWritable for it.\nOn Apr 2, 2014 1:08 PM, \"Pat Ferrel\" <pat@occamsmachete.com> wrote:\nAre the Spark efforts supporting all Mahout Vector types? Named,\nProperty\nVectors? It occurred to me that data frames in R is a related but more\ngeneral solution. If all rows and columns of a DRM and their\ncoresponding\nVectors (row or column vectors) were to support arbitrary properties\nattached to them in such a way that they are preserved during\ntranspose,\nVector extraction, and any other operations that make sense there\nwould be\na huge benefit for users.\nOne of the constant problems with input to Mahout is translation of\nIDs.\nExternal to Mahout going in, Mahout to external coming out. Most of\nthis\nwould be unneeded if Mahout supported data frames, some would be\navoided by\nsupporting named or property vectors universally.",
        "Issue Links": []
    },
    "MAHOUT-1508": {
        "Key": "MAHOUT-1508",
        "Summary": "Performance problems with sparse matrices",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "11/Apr/14 06:01",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "12/Apr/14 11:53",
        "Description": "I'm currently working with the new Scala DSL and running into a problem with SparseMatrix and SparseRowMatrix.\nThe problem is that they don't implement a specialized assign(Matrix other, DoubleDoubleFunction f) function, but use the implementation from AbstractMatrix which loops through all  possible entries. \nWe have to fix this asap.",
        "Issue Links": []
    },
    "MAHOUT-1509": {
        "Key": "MAHOUT-1509",
        "Summary": "Invalid URL in link from \"quick start/basics\" page",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Nick Martin",
        "Created": "15/Apr/14 01:29",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "15/Apr/14 01:47",
        "Description": "From https://mahout.apache.org/users/basics/quickstart.html the \"Dos and Don'ts\" link under \"Recommendations\" goes to nowhere (URL typo - \"ecommender\") https://mahout.apache.org/users/recommender/ecommender-first-timer-faq.html \nCan't remember who's running point on the URL updates or I'd [at] them...",
        "Issue Links": []
    },
    "MAHOUT-1510": {
        "Key": "MAHOUT-1510",
        "Summary": "Goodbye MapReduce",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "15/Apr/14 05:04",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "25/Apr/14 14:54",
        "Description": "We should prominently state on the website that we reject any future MR algorithm contributions (but still maintain and bugfix what we have so far).",
        "Issue Links": []
    },
    "MAHOUT-1511": {
        "Key": "MAHOUT-1511",
        "Summary": "Renaming core to mrlegacy",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Frank Scholten",
        "Reporter": "Sebastian Schelter",
        "Created": "15/Apr/14 05:07",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "19/Apr/14 14:33",
        "Description": "Rename the core module to mrlegacy to reflect that we still maintain this code but do not add new MR algorithms. We should aim to gradually pull out items that are really needed from this module.",
        "Issue Links": []
    },
    "MAHOUT-1512": {
        "Key": "MAHOUT-1512",
        "Summary": "Hadoop 2 compatibility",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastian Schelter",
        "Created": "15/Apr/14 05:08",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "05/Apr/15 03:52",
        "Description": "We must ensure that all our MR code also runs on Hadoop 2.",
        "Issue Links": []
    },
    "MAHOUT-1513": {
        "Key": "MAHOUT-1513",
        "Summary": "Deprecate Canopy Clustering",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "15/Apr/14 05:09",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "18/Apr/14 11:23",
        "Description": "citing smarthi \"I meant to deprecate first (and eventually remove) Canopy clustering. This is in line with the conversation I had with Ted and Frank at AMS about weaning users away from the old style Canopy->KMeans clustering to start using Streaming KMeans. No point in keeping Canopy once users switch to using Streaming KMeans.\"",
        "Issue Links": []
    },
    "MAHOUT-1514": {
        "Key": "MAHOUT-1514",
        "Summary": "Contact the original Random Forest author",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "15/Apr/14 05:12",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/May/14 06:25",
        "Description": "We should contact the original Random Forest author to ask about maintenance of the implementation. Otherwise, this becomes a candidate for removal.",
        "Issue Links": []
    },
    "MAHOUT-1515": {
        "Key": "MAHOUT-1515",
        "Summary": "Contact the original Frequent Pattern Mining author",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "15/Apr/14 05:12",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "01/May/14 07:17",
        "Description": "We should contact the original FPM author to ask about maintenance of the algorithm. Otherwise this becomes a candidate for removal.",
        "Issue Links": []
    },
    "MAHOUT-1516": {
        "Key": "MAHOUT-1516",
        "Summary": "run classify-20newsgroups.sh failed cause by /tmp/mahout-work-jpan/20news-all does not exists in hdfs.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Jian Pan",
        "Created": "17/Apr/14 02:58",
        "Updated": "13/Apr/15 09:56",
        "Resolved": "30/Mar/15 20:36",
        "Description": "+ echo 'Copying 20newsgroups data to HDFS'\nCopying 20newsgroups data to HDFS\n+ set +e\n+ /home/jpan/Software/hadoop-2.2.0/bin/hadoop dfs -rmr /tmp/mahout-work-jpan/20news-all\nDEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\nrmr: DEPRECATED: Please use 'rm -r' instead.\n14/04/17 10:26:25 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nrmr: `/tmp/mahout-work-jpan/20news-all': No such file or directory\n+ set -e\n+ /home/jpan/Software/hadoop-2.2.0/bin/hadoop dfs -put /tmp/mahout-work-jpan/20news-all /tmp/mahout-work-jpan/20news-all\nDEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n14/04/17 10:26:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\nput: `/tmp/mahout-work-jpan/20news-all': No such file or directory",
        "Issue Links": []
    },
    "MAHOUT-1517": {
        "Key": "MAHOUT-1517",
        "Summary": "Remove casts to int in ALSWRFactorizer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "18/Apr/14 23:02",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "19/Apr/14 14:31",
        "Description": "ALSWRFactorizer casts the long ids to int which can lead to exception for long ids that are outside of the int range. Change this to use a mapping for the ids.",
        "Issue Links": []
    },
    "MAHOUT-1518": {
        "Key": "MAHOUT-1518",
        "Summary": "Preprocessing for collaborative filtering with the Scala DSL",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "20/Apr/14 07:24",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "03/May/14 17:58",
        "Description": "The aim here is to provide some easy-to-use machinery to enable the usage of the new Cooccurrence Analysis code from MAHOUT-1464 with datasets represented as follows in a CSV file with the schema timestamp, userId, itemId, action, e.g.\n\ntimestamp1, userIdString1, itemIdString1, \u201cview\"\ntimestamp2, userIdString2, itemIdString1, \u201clike\"",
        "Issue Links": []
    },
    "MAHOUT-1519": {
        "Key": "MAHOUT-1519",
        "Summary": "Remove StandardThetaTrainer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "20/Apr/14 11:21",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "22/Apr/14 06:36",
        "Description": "Andrew_Palumbo if I understand your work in MAHOUT-1504 correctly, the theta training is only necessary for complementary naive bayes, right?\nThen, we should remove the StandardthetaTrainer and make the TrainNaiveBayesJob only do the theta training in the complementary case.\nCorrect me if I miss something here.",
        "Issue Links": []
    },
    "MAHOUT-1520": {
        "Key": "MAHOUT-1520",
        "Summary": "Fix links in Mahout website documentation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Saleem Ansari",
        "Created": "21/Apr/14 12:31",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "27/Apr/14 09:33",
        "Description": "Following pages have hyphen in some urls which cause a 404 not found error:\n\nhttps://mahout.apache.org/users/clustering/latent-dirichlet-allocation.html\nhttps://mahout.apache.org/developers/how-to-release.html\n\nThis can be verified using W3C link check as below:\n\nhttp://validator.w3.org/checklink?uri=https%3A%2F%2Fmahout.apache.org%2Fusers%2Fclustering%2Flatent-dirichlet-allocation.html&summary=on&hide_type=all&depth=&check=Check#d1code_404",
        "Issue Links": []
    },
    "MAHOUT-1521": {
        "Key": "MAHOUT-1521",
        "Summary": "lucene2seq - Error trying to  load data from stored field (when non-indexed)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Frank Scholten",
        "Reporter": "Terry Blankers",
        "Created": "21/Apr/14 16:28",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "15/Jul/14 22:19",
        "Description": "When using lucene2seq to load data from a field that is stored but not indexed I receive the following error:\n\nIllegalArgumentException: Field 'body' does not exist in the index\n\nField is described in schema.xml as:\n\n<fieldname=\"body\"type=\"string\" stored=\"true\" indexed=\"false\"/>\n\nBTW,  field is copied to 'content' field for searching, schema.xml snippet:\n\n<copyField source=\"body\" dest=\"content\" />\n\nCopy field is described in schema.xml as:\n\n<fieldname=\"content\" type=\"text\" stored=\"false\" indexed=\"true\" multiValued=\"true\"/>\n\nIf I try to load data from the copy field, lucene2seq runs with no errors but I receive empty data for each key/doc:\n\nKey class: class org.apache.hadoop.io.Text Value Class: class org.apache.hadoop.io.Text\nKey: 96C4C76CF9D7449C724CA77CB8F650EAFD33E31C: Value:\nKey: D6842B81B8D09733B50BEDB4767C2A5C49E43B20: Value:",
        "Issue Links": []
    },
    "MAHOUT-1522": {
        "Key": "MAHOUT-1522",
        "Summary": "Handle logging levels via log4j.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "22/Apr/14 03:19",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "31/Mar/15 21:54",
        "Description": "We don't have a properties file to tell log4j what to do, so we inherit other frameworks' settings.\nSuggestion is to add a log4j.xml file in a canonical place and set up logging levels, maybe separating out components for ease of setting levels during debugging.",
        "Issue Links": []
    },
    "MAHOUT-1523": {
        "Key": "MAHOUT-1523",
        "Summary": "Remove @author tags in sparkbindings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "22/Apr/14 06:40",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "27/Apr/14 09:22",
        "Description": "A bunch of classes in the sparkbindings module has @author tags, which are discouraged by ASF conventions. We should remove them.",
        "Issue Links": []
    },
    "MAHOUT-1524": {
        "Key": "MAHOUT-1524",
        "Summary": "Script to auto-generate and view the Mahout website on a local machine",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Saleem Ansari",
        "Created": "23/Apr/14 19:08",
        "Updated": "13/Apr/15 09:56",
        "Resolved": "20/Mar/15 17:15",
        "Description": "Attached with this ticket is a script that creates a simple setup for editing Mahout Website on a local machine.\nIt is useful in the sense that, we can edit the source and the changes are automatically reflected in the generated site. All we need to do is refresh the browser. No further steps required.\nSo now one can review the website changes ( the complete website ), on a developer's machine.",
        "Issue Links": []
    },
    "MAHOUT-1525": {
        "Key": "MAHOUT-1525",
        "Summary": "train/validateAdaptiveLogistic",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Richard Scharrer",
        "Created": "24/Apr/14 18:23",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "26/Apr/14 19:58",
        "Description": "Hi,\nI tried to use train- and validateAdaptiveLogistic on my data which is like:\ncategory, id, var1, var2, ...var72 (all numeric)\nI used the following settings:\nmahout trainAdaptiveLogistic --input resource/trainingData \\\n--output ./model \\\n--target category --categories 9 \\\n--predictors a0 a1 a2 a3 a4 a5 a6 a7 a8 a9 a10 a11 .....\n--types numeric \\\n--passes 100 \\\n--showperf \\\nmahout validateAdaptiveLogistic --input resource/testData --model model --confusion --defaultCategory none\nThe output of validateAdaptiveLogistic is:\nLog-likelihood:Min=-5.54, Max=-0.04, Mean=-1.58, Median=-1.33\n=======================================================\nConfusion Matrix\n-------------------------------------------------------\na    \tb    \td    \te    \tf    \tg    \th    \ti    \t<--Classified as\n14   \t0    \t0    \t0    \t0    \t0    \t0    \t0    \t |  14    \ta     = projekt\n0    \t18   \t0    \t0    \t0    \t0    \t0    \t0    \t |  18    \tb     = news/aktuelles/presse\n0    \t0    \t24   \t0    \t0    \t0    \t0    \t0    \t |  24    \td     = lehrveranstaltung\n0    \t0    \t0    \t19   \t0    \t0    \t0    \t0    \t |  19    \te     = publikation\n0    \t0    \t0    \t0    \t20   \t0    \t0    \t0    \t |  20    \tf     = event\n0    \t0    \t0    \t0    \t0    \t14   \t0    \t0    \t |  14    \tg     = mitarbeiter/person\n0    \t0    \t0    \t0    \t0    \t0    \t44   \t0    \t |  44    \th     = \u00fcbersicht\n0    \t0    \t0    \t0    \t0    \t0    \t0    \t13   \t |  13    \ti     = institut\n(in case you were wondering, the categories a in german)\nMy problem is that this is impossible. I always get a perfect classification even with just a little amount of training data. It doesnt even matter how many features I use I tried it with all 72 and with only one. Am I missing something?\nRegards,\nRichard",
        "Issue Links": []
    },
    "MAHOUT-1526": {
        "Key": "MAHOUT-1526",
        "Summary": "Ant file in examples",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "27/Apr/14 20:30",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "01/May/14 07:39",
        "Description": "There is a ant file (build.xml) in the examples module with some commands to download a few datasets, but these commands don't seem to be used or documented somewhere. If nobody shouts in the next days, I will remove that file.",
        "Issue Links": []
    },
    "MAHOUT-1527": {
        "Key": "MAHOUT-1527",
        "Summary": "Fix wikipedia classifier example",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification,                                            Documentation,                                            Examples",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "27/Apr/14 20:58",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/May/14 15:30",
        "Description": "The examples package has a classification showcase for prediciting the labels of wikipedia  pages. Unfortunately, the example is totally broken:\nIt relies on the old NB implementation which has been removed, suggests to use the whole wikipedia as input, which will not work well on a single machine and the documentation uses commands that have long been removed from bin/mahout. \nThe example needs to be updated to use the current naive bayes implementation and documentation on the website needs to be written.",
        "Issue Links": []
    },
    "MAHOUT-1528": {
        "Key": "MAHOUT-1528",
        "Summary": "Source tag and source release tarball for 0.9 don't exactly match",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Mark Grover",
        "Created": "28/Apr/14 04:51",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "18/May/14 06:30",
        "Description": "If you download the source tarball for the Apache Mahout 0.9 release, you'd notice that it doesn't contain CHANGELOG or .gitignore file. However, if you look at the tag for the release in the github repo (https://github.com/apache/mahout/tree/mahout-0.9), you'd notice both the files there.\nI think, both as a best practice and to make life of downstream integrators less miserable, it would be fantastic if we could have the release tag in the source match one to one with the source code in the released source tarball. \nA test to do this in particular, would be awesome!\nThanks in advance!",
        "Issue Links": []
    },
    "MAHOUT-1529": {
        "Key": "MAHOUT-1529",
        "Summary": "Finalize abstraction of distributed logical plans from backend operations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "29/Apr/14 05:50",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "27/May/14 23:40",
        "Description": "We have a few situations when algorithm-facing API has Spark dependencies creeping in. \nIn particular, we know of the following cases:\n(1) checkpoint() accepts Spark constant StorageLevel directly;\n(2) certain things in CheckpointedDRM;\n(3) drmParallelize etc. routines in the \"drm\" and \"sparkbindings\" package.\n(5) drmBroadcast returns a Spark-specific Broadcast object\n(6) Stratosphere/Flink conceptual api changes.\nCurrent tracker: PR #1 https://github.com/apache/mahout/pull/1 - closed, need new PR for remaining things once ready.\nPull requests are welcome.",
        "Issue Links": []
    },
    "MAHOUT-1530": {
        "Key": "MAHOUT-1530",
        "Summary": "Custom prompt and welcome message for the Spark Shell",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "29/Apr/14 11:51",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "29/Apr/14 19:52",
        "Description": "We should have a custom prompt and welcome message in our shell",
        "Issue Links": []
    },
    "MAHOUT-1531": {
        "Key": "MAHOUT-1531",
        "Summary": "Remove ALPHA_I CLI option in NB",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification,                                            CLI",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "29/Apr/14 20:32",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "30/Apr/14 14:35",
        "Description": "The APLHA_I CLI option accepts a float for what should (according to the paper) be a vector of floats (or more correctly a path to a SequenceFile containing word, alpha_i pairs for each word in the respective dictionary).   The current MapReduce NB implementation treats APLHA_I as a constant float where it should be a vector (ALPHA) of floats (smoothing parameter per word).  \nHowever as (Rennie et al.) sec 2.2 uses a constant ALPHA_I = 1 and the NB implementation defaults to APLHA_I = 1 and is an implementation of Rennie, there is not really a problem.\nTo avoid confusion, I think its a good idea to remove the option to set ALPHA_I and to keep ALPHA_I a constant 1.0 in NB.",
        "Issue Links": []
    },
    "MAHOUT-1532": {
        "Key": "MAHOUT-1532",
        "Summary": "Add solve() function to the Scala DSL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "30/Apr/14 05:23",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/May/14 06:38",
        "Description": "We should add a solve() function to the Scala DSL with helps with solving Ax = b for in-core matrices and vectors.",
        "Issue Links": []
    },
    "MAHOUT-1533": {
        "Key": "MAHOUT-1533",
        "Summary": "Remove Frequent Pattern Mining",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "01/May/14 08:31",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "01/May/14 16:57",
        "Description": "As per (second discussion) on the mailinglist, we'll remove Frequent Pattern Mining due to lack of usage and maintenance. This was already intended for the 0.9 release, but a user stepped up promising to take care of the algorithm (but never cam back to work on it).\nThe source code for the latest version of Frequent Pattern Mining will always be available in our svn: https://svn.apache.org/viewvc/mahout/tags/mahout-0.9/core/src/main/java/org/apache/mahout/fpm/",
        "Issue Links": []
    },
    "MAHOUT-1534": {
        "Key": "MAHOUT-1534",
        "Summary": "Add documentation for using Mahout with Hadoop2 to the website",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Gokhan Capan",
        "Reporter": "Sebastian Schelter",
        "Created": "01/May/14 08:43",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "22/May/14 06:40",
        "Description": "MAHOUT-1329 describes how to build the current trunk for usage with Hadoop 2. We should have a page on the website describing this for our users.",
        "Issue Links": []
    },
    "MAHOUT-1535": {
        "Key": "MAHOUT-1535",
        "Summary": "Fix mathjax on website",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Sebastian Schelter",
        "Created": "01/May/14 10:39",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "01/May/14 11:06",
        "Description": "There are problem with displaying mathjax formulas on the website that should be fixed asap",
        "Issue Links": []
    },
    "MAHOUT-1536": {
        "Key": "MAHOUT-1536",
        "Summary": "Update \"Creating vectors from text\" page",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "01/May/14 18:47",
        "Updated": "24/Oct/15 06:13",
        "Resolved": "22/Jun/14 20:47",
        "Description": "At least the seq2sparse section of the \"Creating vectors from text\" page is out of date.  \nhttps://mahout.apache.org/users/basics/creating-vectors-from-text.html",
        "Issue Links": []
    },
    "MAHOUT-1537": {
        "Key": "MAHOUT-1537",
        "Summary": "minor fixes to spark-shell",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Anand Avati",
        "Created": "01/May/14 22:17",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/May/14 18:45",
        "Description": "Terminal clutters up after exiting spark shell (as terminal settings are changed within spark repl).\nSave and restore system terminal settings to avoid clutter.\nAlso minor fix of prompt styling",
        "Issue Links": []
    },
    "MAHOUT-1538": {
        "Key": "MAHOUT-1538",
        "Summary": "Port spectral clustering to Mahout DSL",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.0",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "02/May/14 17:17",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "11/Aug/15 23:56",
        "Description": "Move spectral clustering logic to Mahout DSL. Dependencies include SSVD (already ported) and K-means (currently in progress, or can use Spark MLlib implementation as a temporary fix).",
        "Issue Links": []
    },
    "MAHOUT-1539": {
        "Key": "MAHOUT-1539",
        "Summary": "Implement affinity matrix computation in Mahout DSL",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.0",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "02/May/14 17:21",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "11/Aug/15 23:56",
        "Description": "This has the same goal as MAHOUT-1506, but rather than code the pairwise computations in MapReduce, this will be done in the Mahout DSL.\nAn orthogonal issue is the format of the raw input (vectors, text, images, SequenceFiles), and how the user specifies the distance equation and any associated parameters.",
        "Issue Links": []
    },
    "MAHOUT-1540": {
        "Key": "MAHOUT-1540",
        "Summary": "Reuters example for spectral clustering",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "Examples",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "02/May/14 17:23",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 17:04",
        "Description": "Once MAHOUT-1538 and MAHOUT-1539 are complete, create a working example of spectral clustering using the Reuters dataset.",
        "Issue Links": []
    },
    "MAHOUT-1541": {
        "Key": "MAHOUT-1541",
        "Summary": "Create CLI Driver for Spark Cooccurrence Analysis",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "03/May/14 00:26",
        "Updated": "13/Apr/15 09:56",
        "Resolved": "18/Mar/15 13:56",
        "Description": "Create a CLI driver to import data in a flexible manner, create an IndexedDataset with BiMap ID translation dictionaries, call the Spark CooccurrenceAnalysis with the appropriate params, then write output with external IDs optionally reattached.\nUltimately it should be able to read input as the legacy mr does but will support reading externally defined IDs and flexible formats. Output will be of the legacy format or text files of the user's specification with reattached Item IDs. \nSupport for legacy formats is a question, users can always use the legacy code if they want this. Internal to the IndexedDataset is a Spark DRM so pipelining can be accomplished without any writing to an actual file so the legacy sequence file output may not be needed.\nOpinions?",
        "Issue Links": []
    },
    "MAHOUT-1542": {
        "Key": "MAHOUT-1542",
        "Summary": "Tutorial for playing with Mahout's Spark shell",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation,                                            Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "03/May/14 08:26",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/May/14 13:50",
        "Description": "I have a created a tutorial for setting up the spark shell and implementing a simple linear regression algorithm. I'd love to make this part of the website, could someone give it a review?\nhttps://github.com/sscdotopen/krams/blob/master/linear-regression-cereals.md\nPS: If you wanna try out the code, you have to add the patch from MAHOUT-1532 to your sources.",
        "Issue Links": []
    },
    "MAHOUT-1543": {
        "Key": "MAHOUT-1543",
        "Summary": "JSON output format for classifying with random forests",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7,                                            0.8,                                            0.9",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "larryhu",
        "Created": "04/May/14 07:24",
        "Updated": "27/Mar/15 01:26",
        "Resolved": "27/Mar/15 01:26",
        "Description": "This patch adds JSON output format to build random forests,",
        "Issue Links": []
    },
    "MAHOUT-1544": {
        "Key": "MAHOUT-1544",
        "Summary": "make Mahout DSL shell depend dynamically on Spark",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Anand Avati",
        "Created": "04/May/14 22:50",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "20/May/14 00:42",
        "Description": "Today the Mahout's scala shell depends on spark.\nCreate a cleaner separation between the shell and Spark. For e.g, the in core scalabindings and operators do not need Spark. So make Spark a runtime \"addon\" to the shell. Similarly in the future new distributed backend engines can transparently (dynamically) be available through the DSL shell.\nThe new shell works, looks and feels exactly like the shell before, but has a cleaner modular architecture.",
        "Issue Links": []
    },
    "MAHOUT-1545": {
        "Key": "MAHOUT-1545",
        "Summary": "Creating holdout sets with seq2sparse and split",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification,                                            CLI,                                            Examples",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "05/May/14 01:05",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/May/14 06:35",
        "Description": "The current method for vectorizing data using seq2sparse and then \"split\" allows for a large amount of information to spill over from the training sets to the test sets- especially in the case of TF-IDF transformations.  The IDF transform provides alot of information on the holdout set to the training set if calculated previous to splitting them up.  \nI'm not sure if given the current seq2sparse implementation's status as Legacy and the relatively minor advantages that it might give whether or not its worth adding something like a \"split\" option to SparseVectorsFromSequenceFiles.java.  But i know that i saw a new implementation being discussed and and think that it would be worth it to have an option like this built in.    \nI think that this issue may have been raised before, but i wanted to bring it up again in light of the current move away from MapReduce and the new implementations of Mahout tools that will be coming along.",
        "Issue Links": []
    },
    "MAHOUT-1546": {
        "Key": "MAHOUT-1546",
        "Summary": "building spark context fails due to incorrect classpath query",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Pat Ferrel",
        "Created": "05/May/14 16:37",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "05/May/14 18:37",
        "Description": "The classpath retrieval is using a \"-spark\" flag that returns nothing, using the default \"mahout classpath\" seems to get all needed jar paths so commenting out the \"-spark\" makes it work for me. Not sure this is the best fix though.\nThis is in def mahoutSparkContext(...)\n\n        //val p = Runtime.getRuntime.exec(Array(exec.getAbsolutePath, \"-spark\", \"classpath\"))\n        val p = Runtime.getRuntime.exec(Array(exec.getAbsolutePath, \"classpath\"))",
        "Issue Links": []
    },
    "MAHOUT-1547": {
        "Key": "MAHOUT-1547",
        "Summary": "Implementation of Support Vector Machines (Sequential Minimal Optimazation technique) on Hadoop",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Adarsh Khalique",
        "Created": "06/May/14 06:46",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "06/May/14 06:50",
        "Description": "Cloudera Virtual Machine 4.3.0 can be used where hadoop is already configured. The code for SMO can be found by visiting https://sites.google.com/site/postechdm/research/implementation/svm-java. All is required to implement the technique of Sequential Minimal Optimization on Hadoop so that we can optimize the existing technique.",
        "Issue Links": []
    },
    "MAHOUT-1548": {
        "Key": "MAHOUT-1548",
        "Summary": "Fix broken links in quickstart webpage",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "06/May/14 19:19",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "06/May/14 21:07",
        "Description": "http://mahout.apache.org/users/basics/quickstart.html\nFix a few broken links, remove Naive Bayes Wikipedia reference",
        "Issue Links": []
    },
    "MAHOUT-1549": {
        "Key": "MAHOUT-1549",
        "Summary": "Extracting tfidf-vectors by key",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "0.7,                                            0.8,                                            0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Richard Scharrer",
        "Created": "07/May/14 05:55",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "18/May/14 21:40",
        "Description": "Hi,\nI have about 200000 tfidf-vectors and I need to extract 500 of them of which I have the keys. Is there some kind of magical option which allows me something like taking the output of mahout seqdumper and transform it back into a sequencefile that I can use for trainnb /testnb? The sequencefiles of tfidf use the Text class for the keys and the VectorWritable class for the values. I tried \nhttps://github.com/kevinweil/elephant-bird/blob/master/pig/src/main/java/com/twitter/elephantbird/pig/store/SequenceFileStorage.java\nwith different settings but the output always gives me the Text class for both, key and value which can't be used in trainnb and testnb.\nI posted this question on:\nhttp://stackoverflow.com/questions/23502362/extracting-tfidf-vectors-by-key-without-destroying-the-fileformat\nI ask this question in here because I've seen similar questions on stackoverflow that where asked last year and still didn't get an answer\nI really need this information so in case you know anything please tell me.\nRegards,\nRichard",
        "Issue Links": []
    },
    "MAHOUT-1550": {
        "Key": "MAHOUT-1550",
        "Summary": "Naive Bayes training fails with Hadoop 2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Paul Marret",
        "Created": "13/May/14 12:54",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "13/May/14 15:56",
        "Description": "When using the trainnb option of the program, we get the following error:\nException in thread \"main\" java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected\n        at org.apache.mahout.common.HadoopUtil.getCustomJobName(HadoopUtil.java:174)\n        at org.apache.mahout.common.AbstractJob.prepareJob(AbstractJob.java:614)\n        at org.apache.mahout.classifier.naivebayes.training.TrainNaiveBayesJob.run(TrainNaiveBayesJob.java:100)\n[...]\nIt is possible to correct this by modifying the file mrlegacy/src/main/java/org/apache/mahout/common/HadoopUtil.java and converting the instance job (line 174) to a Job object (it is a JobContext in the current version).",
        "Issue Links": []
    },
    "MAHOUT-1551": {
        "Key": "MAHOUT-1551",
        "Summary": "Add document to describe how to use mlp with command line",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification,                                            CLI,                                            Documentation",
        "Assignee": null,
        "Reporter": "Yexi Jiang",
        "Created": "19/May/14 00:44",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "05/Apr/15 17:25",
        "Description": "Add documentation about the usage of multi-layer perceptron in command line.",
        "Issue Links": []
    },
    "MAHOUT-1552": {
        "Key": "MAHOUT-1552",
        "Summary": "Avoid new Configuration() instantiation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sergey",
        "Created": "19/May/14 20:32",
        "Updated": "27/Mar/15 01:37",
        "Resolved": "27/Mar/15 01:36",
        "Description": "Hi, it's related to MAHOUT-1498\nYou get troubles when run mahout stuff from oozie java action.\n\nava.lang.InterruptedException: Cluster Classification Driver Job failed processing /tmp/sku/tfidf/90453\n\tat org.apache.mahout.clustering.classify.ClusterClassificationDriver.classifyClusterMR(ClusterClassificationDriver.java:276)\n\tat org.apache.mahout.clustering.classify.ClusterClassificationDriver.run(ClusterClassificationDriver.java:135)\n\tat org.apache.mahout.clustering.canopy.CanopyDriver.clusterData(CanopyDriver.java:372)\n\tat org.apache.mahout.clustering.canopy.CanopyDriver.run(CanopyDriver.java:158)\n\tat org.apache.mahout.clustering.canopy.CanopyDriver.run(CanopyDriver.java:117)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.mahout.clustering.canopy.CanopyDriver.main(CanopyDriver.java:64)",
        "Issue Links": []
    },
    "MAHOUT-1553": {
        "Key": "MAHOUT-1553",
        "Summary": "Fix for run Mahout stuff as oozie java action",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Sergey",
        "Created": "19/May/14 20:40",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "23/May/14 18:25",
        "Description": "Related to MAHOUT-1498, the problem is the same. mapred.job.classpath.files property is not correctly pushed down to Mahout MR stuff because of new Configuration usage\nat org.apache.mahout.clustering.classify.ClusterClassificationDriver.classifyClusterMR(ClusterClassificationDriver.java:276)\n\tat org.apache.mahout.clustering.classify.ClusterClassificationDriver.run(ClusterClassificationDriver.java:135)\n\tat org.apache.mahout.clustering.canopy.CanopyDriver.clusterData(CanopyDriver.java:372)\n\tat org.apache.mahout.clustering.canopy.CanopyDriver.run(CanopyDriver.java:158)\n\tat org.apache.mahout.clustering.canopy.CanopyDriver.run(CanopyDriver.java:117)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.mahout.clustering.canopy.CanopyDriver.main(CanopyDriver.java:64)",
        "Issue Links": []
    },
    "MAHOUT-1554": {
        "Key": "MAHOUT-1554",
        "Summary": "Provide more comprehensive classification statistics",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Karol Grzegorczyk",
        "Created": "21/May/14 07:33",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "23/May/14 16:40",
        "Description": "Currently only limited classification statistics are provided. To better understand classification results, it would be worth to provide at lease average precision, recall and F1 score.",
        "Issue Links": []
    },
    "MAHOUT-1555": {
        "Key": "MAHOUT-1555",
        "Summary": "Exception thrown when a test example has the label not present in training examples",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Suneel Marthi",
        "Reporter": "Karol Grzegorczyk",
        "Created": "21/May/14 08:27",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "26/May/14 11:02",
        "Description": "Currently an IllegalArgumentException is thrown when a test example has the label (belongs to the class) not present in training examples. When the number of labels is big, such a situation is likely and valid. The example of course will be misclassified, but exception should not be thrown.",
        "Issue Links": []
    },
    "MAHOUT-1556": {
        "Key": "MAHOUT-1556",
        "Summary": "Mahout for Hadoop2 - HDP2.1.1",
        "Type": "Dependency upgrade",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Prabhat K Singh",
        "Created": "22/May/14 07:55",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "27/May/14 17:22",
        "Description": "Hi, \nI tried build and install of Mahout0.9 for hadoop HDP2.1.1 as per given methods in https://issues.apache.org/jira/browse/MAHOUT-1329, but I get errors as mentioned below.\nMethod:\nmvn clean package  -Dhadoop.profile=200  -Dhadoop2.version=2.2.0 -Dhbase.version=0.98\nmvn clean install -Dhadoop2 -Dhadoop.2.version=2.2.0\nmvn clean package -Dhadoop2 -Dhadoop.profile=200  -Dhadoop2.version=2.4.0 -Dhbase.version=0.98\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project mahout-integration: Compilation failure: Compilation failure:\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[30,31] cannot find symbol\n[ERROR] symbol:   class HBaseConfiguration\n[ERROR] location: package org.apache.hadoop.hbase\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[33,31] cannot find symbol\n[ERROR] symbol:   class KeyValue\n[ERROR] location: package org.apache.hadoop.hbase\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[47,36] cannot find symbol\n[ERROR] symbol:   class Bytes\n[ERROR] location: package org.apache.hadoop.hbase.util\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[91,42] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[92,42] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[107,26] cannot find symbol\n[ERROR] symbol:   variable HBaseConfiguration\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[138,51] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[206,26] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[207,25] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[233,15] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[265,26] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[266,25] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[278,31] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[289,41] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[290,54] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[290,14] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[302,31] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[313,41] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[314,7] cannot find symbol\n[ERROR] symbol:   class KeyValue\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[314,54] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[365,18] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[373,18] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[386,33] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[386,56] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[387,33] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[387,56] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[402,46] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] /home/user1/aws/mahout-distribution-0.9/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java:[403,46] cannot find symbol\n[ERROR] symbol:   variable Bytes\n[ERROR] location: class org.apache.mahout.cf.taste.impl.model.hbase.HBaseDataModel\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :mahout-integration",
        "Issue Links": []
    },
    "MAHOUT-1557": {
        "Key": "MAHOUT-1557",
        "Summary": "Add support for sparse training vectors in MLP",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Karol Grzegorczyk",
        "Created": "23/May/14 13:10",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "06/Apr/15 03:12",
        "Description": "When the number of input units of MLP is big, it is likely that input vector will be sparse. It should be possible to read input files in a sparse format.",
        "Issue Links": []
    },
    "MAHOUT-1558": {
        "Key": "MAHOUT-1558",
        "Summary": "Clean up classify-wiki.sh and add in a binary classification problem",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification,                                            Examples",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Andrew Palumbo",
        "Created": "23/May/14 18:00",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "24/May/14 16:03",
        "Description": "Some minor cleanups to classify-wiki.sh.   Added in a 2 class problem: United States and United Kingdom.",
        "Issue Links": []
    },
    "MAHOUT-1559": {
        "Key": "MAHOUT-1559",
        "Summary": "Add documentation for and clean up the wikipedia classifier example",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation,                                            Examples",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "23/May/14 21:46",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "05/Apr/15 20:11",
        "Description": "Add documentation for the wikipedia classifer example.",
        "Issue Links": []
    },
    "MAHOUT-1560": {
        "Key": "MAHOUT-1560",
        "Summary": "Last batch is not filled correctly in MultithreadedBatchItemSimilarities",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Jaros\u0142aw Bojar",
        "Created": "23/May/14 23:47",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "24/May/14 15:18",
        "Description": "In MultithreadedBatchItemSimilarities method queueItemIDsInBatches handles last batch incorrectly. Last batch length is calculated incorrectly. As a result last batch is either truncated or too long with superfluous positions filled with item indexes from previous batch (or zeros if it is also the first batch as in attached test).\nAttached test fails for very short model (with only 4 items) with NoSuchItemException.\nAttached patch corrects this issue.",
        "Issue Links": []
    },
    "MAHOUT-1561": {
        "Key": "MAHOUT-1561",
        "Summary": "cluster-syntheticcontrol.sh not running locally with MAHOUT_LOCAL=true",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering,                                            Examples",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Andrew Palumbo",
        "Created": "24/May/14 14:57",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "24/May/14 16:09",
        "Description": "cluster-syntheticcontrol.sh is not running locally with MAHOUT_LOCAL set.  Patch adds a check for MAHOUT_LOCAL.",
        "Issue Links": []
    },
    "MAHOUT-1562": {
        "Key": "MAHOUT-1562",
        "Summary": "Publish Scaladocs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Pat Ferrel",
        "Created": "24/May/14 17:11",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "12/Apr/15 12:35",
        "Description": "The poms that relate to Scala have the maven-scala-plugin, which will generate Scaladocs if you navigate to the scala subproject and run\nmvn scala:doc\nThey will appear in target/site/scaladocs for the subproject.\nThis is not supported from the parent mahout pom\nThis should be incorporated into the process that publishes the javadocs. It appears the Scala code is not publically browsable as Scaladocs. Not sure where this process lives but the scala stuff should probably follow that same pattern/process as javadocs",
        "Issue Links": [
            "/jira/browse/MAHOUT-1654",
            "/jira/browse/MAHOUT-1585",
            "/jira/browse/MAHOUT-1647"
        ]
    },
    "MAHOUT-1563": {
        "Key": "MAHOUT-1563",
        "Summary": "Clean up WARNINGs during build",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Andrew Musselman",
        "Created": "27/May/14 00:34",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "29/Mar/15 22:03",
        "Description": "We need to clean up warnings in the maven logs.  They seem to have piled up recently; some are about scala lib version conflicts, some are about deprecated APIs, some are about code style.\nSome may be fine for now but extra warnings in build logs feels like bad hygiene to me.\nSome examples:\n[WARNING]  Expected all dependencies to require Scala version: 2.10.3\n[WARNING]  com.twitter:chill_2.10:0.3.1 requires scala version: 2.10.0\n[WARNING] Multiple versions of scala libraries detected!\n[WARNING] /home/akm/mahout/spark/src/main/scala/org/apache/mahout/sparkbindings/drm/CheckpointedDrmBase.scala:73: warning: a pure expression does nothing in statement position; you may be omitting necessary parentheses\n[INFO]     this\n[WARNING]  Expected all dependencies to require Scala version: 2.10.3\n[WARNING]  org.apache.mahout:mahout-math-scala:1.0-SNAPSHOT requires scala version: 2.10.3\n[WARNING]  org.scalatest:scalatest_2.10:2.0 requires scala version: 2.10.0\n[WARNING] Multiple versions of scala libraries detected!\n[WARNING] /home/akm/mahout/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/package.scala:132: warning: non-variable type argument Double in type pattern Iterable[Double] is unchecked since it is eliminated by erasure\n[INFO]         case t: Iterable[Double] => t.toArray\n[WARNING] /home/akm/mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/kmeans/Job.java: Some input files use or override a deprecated API.\n[WARNING] /home/akm/mahout/examples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/kmeans/Job.java: Recompile with -Xlint:deprecation for details.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1654"
        ]
    },
    "MAHOUT-1564": {
        "Key": "MAHOUT-1564",
        "Summary": "Naive Bayes Classifier for New Text Documents",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "27/May/14 21:30",
        "Updated": "30/May/15 17:50",
        "Resolved": "01/Apr/15 21:25",
        "Description": "MapReduce and DSL Naive Bayes implementations currently lack the ability to classify a new document (outside of the training/holdout corpus).  This New feature will do the following.\n1. Vectorize a new text document using the dictionary and document frequencies from the training/holdout corpus \n\nassume the original corpus was vectorized using `seq2sparse`; step (1) will use all of the same parameters.\n\n2. Score and label a new document using a previously trained model.\nThis effort will need to be done in parallel for MRLegacy and DSL implementations.  Neither should be too much work.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1493"
        ]
    },
    "MAHOUT-1565": {
        "Key": "MAHOUT-1565",
        "Summary": "add MR2 options to MAHOUT_OPTS in bin/mahout",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9,                                            1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Gokhan Capan",
        "Reporter": "Nishkam Ravi",
        "Created": "27/May/14 23:28",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "15/Jul/14 13:22",
        "Description": "MR2 options are missing in MAHOUT_OPTS in bin/mahout and bin/mahout.cmd. Add those options.",
        "Issue Links": []
    },
    "MAHOUT-1566": {
        "Key": "MAHOUT-1566",
        "Summary": "Regular ALS factorizer with convergence test.",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "31/May/14 00:38",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "04/Jun/14 21:37",
        "Description": "ALS-related: let's start with unweighed, unregularized implementation.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1365"
        ]
    },
    "MAHOUT-1567": {
        "Key": "MAHOUT-1567",
        "Summary": "Add online sparse dictionary learning (dimensionality reduction)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.1",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Maciej Kula",
        "Created": "31/May/14 12:37",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "22/Oct/15 17:28",
        "Description": "I have recently implemented a sparse online dictionary learning algorithm, with an emphasis on learning very high-dimensional and very sparse dictionaries. It is based on J. Mairal et al 'Online Dictionary Learning for Sparse Coding' (http://www.di.ens.fr/willow/pdfs/icml09.pdf). It's an online variant of low-rank matrix factorization, suitable for sparse binary matrices (such as implicit feedback matrices).\nI would be very happy to bring this up to the Mahout standard and contribute to the main codebase \u2014 is this something you would in principle be interested in having?\nThe code (as well as some examples) are here: https://github.com/maciejkula/dictionarylearning",
        "Issue Links": []
    },
    "MAHOUT-1568": {
        "Key": "MAHOUT-1568",
        "Summary": "Build an I/O model that can replace sequence files for import/export",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "01/Jun/14 17:25",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "18/Mar/15 13:44",
        "Description": "Implement mechanisms to read and write data from/to flexible stores. These will support tuples streams and drms but with extensions that allow keeping user defined values for IDs. The mechanism in some sense can replace Sequence Files for import/export and will make the operation much easier for the user. In many cases directly consuming their input files.\nStart with text delimited files for input/output in the Spark version of ItemSimilarity\nA proposal is running with ItemSimilarity on Spark and is documented on the github wiki here: https://github.com/pferrel/harness/wiki\nComments are appreciated",
        "Issue Links": []
    },
    "MAHOUT-1569": {
        "Key": "MAHOUT-1569",
        "Summary": "Create CLI driver that supports Spark jobs",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "01/Jun/14 17:32",
        "Updated": "13/Apr/15 09:56",
        "Resolved": "18/Mar/15 13:40",
        "Description": "Create a design for CLI drivers, including an option parser, base MahoutDriver for Spark, that uses a text file I/O mechanism MAHOUT-1568\nA version of the proposal is implemented and running for ItemSimilarity on Spark. MAHOUT-1541\nA proposal is running with ItemSimilarity on Spark and is documented on the github wiki here: https://github.com/pferrel/harness/wiki\nComments are appreciated",
        "Issue Links": []
    },
    "MAHOUT-1570": {
        "Key": "MAHOUT-1570",
        "Summary": "Adding support for Apache Flink as a backend for the Mahout DSL",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Till Rohrmann",
        "Created": "03/Jun/14 12:48",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "10/Apr/16 03:08",
        "Description": "With the finalized abstraction of the Mahout DSL plans from the backend operations (MAHOUT-1529), it should be possible to integrate further backends for the Mahout DSL. Apache Flink would be a suitable candidate to act as a good execution backend. \nWith respect to the implementation, the biggest difference between Spark and Flink at the moment is probably the incremental rollout of plans, which is triggered by Spark's actions and which is not supported by Flink yet. However, the Flink community is working on this issue. For the moment, it should be possible to circumvent this problem by writing intermediate results required by an action to HDFS and reading from there.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1809",
            "/jira/browse/MAHOUT-1810",
            "/jira/browse/MAHOUT-1815",
            "/jira/browse/MAHOUT-1811",
            "/jira/browse/MAHOUT-1812",
            "/jira/browse/MAHOUT-1804",
            "/jira/browse/MAHOUT-1805",
            "/jira/browse/MAHOUT-1814",
            "/jira/browse/MAHOUT-1701",
            "/jira/browse/MAHOUT-1702",
            "/jira/browse/MAHOUT-1703",
            "/jira/browse/MAHOUT-1709",
            "/jira/browse/MAHOUT-1710",
            "/jira/browse/MAHOUT-1711",
            "/jira/browse/MAHOUT-1712",
            "/jira/browse/MAHOUT-1734",
            "/jira/browse/MAHOUT-1747",
            "/jira/browse/MAHOUT-1748",
            "/jira/browse/MAHOUT-1755",
            "/jira/browse/MAHOUT-1764",
            "/jira/browse/MAHOUT-1765",
            "/jira/browse/FLINK-393"
        ]
    },
    "MAHOUT-1571": {
        "Key": "MAHOUT-1571",
        "Summary": "Functional Views are not serialized as dense/sparse correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "04/Jun/14 21:43",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "10/Jun/14 17:58",
        "Description": "per ssc \n\nall entries of a TransposeView (and possibly other views) of a sparse matrix are serialized, resulting in OOM",
        "Issue Links": []
    },
    "MAHOUT-1572": {
        "Key": "MAHOUT-1572",
        "Summary": "blockify() to detect (naively) the data sparsity in the loaded data",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitriy Lyubimov",
        "Created": "04/Jun/14 21:57",
        "Updated": "19/Apr/16 15:37",
        "Resolved": "10/Jun/14 18:35",
        "Description": "per ssc:\n.bq a dense matrix is converted into a SparseRowMatrix with dense row vectors by blockify(), after serialization this becomes a dense matrix in sparse format (triggering OOMs)! \ni guess we can look at first row vector and go on to either DenseMatrix or SparseRowMatrix",
        "Issue Links": []
    },
    "MAHOUT-1573": {
        "Key": "MAHOUT-1573",
        "Summary": "More explicit parallelism adjustments in math-scala DRM apis; elements of automatic parallelism management",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "06/Jun/14 17:34",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/Jun/14 22:20",
        "Description": "(1) add minSplit parameter pass-thru to drmFromHDFS to be able to explicitly increase parallelism. \n(2) add parrallelism readjustment parameter to a checkpoint() call. This implies shuffle-less coalesce() translation to the data set before it is requested to be cached (if specified).\nGoing forward, we probably should try and figure how we can automate it,  at least a little bit. For example, the simplest automatic adjustment might include re-adjust parallelims on load to simply fit cluster size (95% or 180% of cluster size, for example), with some rule-of-thumb safeguards here, e.g. we cannot exceed a factor of say 8 (or whatever we configure) in splitting each original hdfs split. We should be able to get a reasonable parallelism performance out of the box on simple heuristics like that.",
        "Issue Links": []
    },
    "MAHOUT-1574": {
        "Key": "MAHOUT-1574",
        "Summary": "SparseRowMatrix needs performance improvement for times()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Ted Dunning",
        "Reporter": "Ted Dunning",
        "Created": "07/Jun/14 02:18",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "07/Jun/14 05:09",
        "Description": "According to ssc,\n> * SparseRowMatrix with sequential vectors times SparseRowMatrix with\n> sequential vectors is totally broken, it uses three nested loops and uses\n> get(row, col) on the matrices, which internally uses binary search...",
        "Issue Links": []
    },
    "MAHOUT-1575": {
        "Key": "MAHOUT-1575",
        "Summary": "Conjugate gradient assumes best case scenario for convergence",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "07/Jun/14 22:07",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "07/Jun/14 22:09",
        "Description": "The conjugate gradient solve that we have assumes that it will converge within the theoretical n steps where n is the size of the input.  In fact, convergence may not happen within exactly that number of steps due to numerical issues so we should allow for a few extra iterations.",
        "Issue Links": []
    },
    "MAHOUT-1576": {
        "Key": "MAHOUT-1576",
        "Summary": "Do a quick style pass to knock down some accumulated warnings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "09/Jun/14 00:38",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "09/Jun/14 03:15",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1577": {
        "Key": "MAHOUT-1577",
        "Summary": "FindBugs and PMD settings unrealistic",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "09/Jun/14 03:53",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "09/Jun/14 05:09",
        "Description": "We get warnings on every use of a non-final method in a constructor.  This is arguably slightly risky, but I haven't ever seen a bug from this.  Similarly, we haven't had any problems with long methods (inherent in Hadoop programs, I think) nor with putting literals after variables in comparisons.\nI am going to turn these warnings off to decrease noise levels.",
        "Issue Links": []
    },
    "MAHOUT-1578": {
        "Key": "MAHOUT-1578",
        "Summary": "Optimizations in matrix serialization",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "11/Jun/14 07:29",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "28/Mar/15 17:11",
        "Description": "MatrixWritable contains inefficient code in a few places:\n\ntype and size are stored with every vector, although they are the same for every vector\nin some places vectors are added to the matrix via assign() in places where we could directly use the instance",
        "Issue Links": []
    },
    "MAHOUT-1579": {
        "Key": "MAHOUT-1579",
        "Summary": "Implement a datamodel which can load data from hadoop filesystem directly",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Xiaomeng Huang",
        "Created": "12/Jun/14 02:59",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 23:34",
        "Description": "As we all know, FileDataModel can only load data from local filesystem.\nBut the big-data are usually stored in hadoop filesystem(e.g. hdfs).\nIf we want to deal with the data in hdfs, we must run mapred job. \nIt's necessay to implement a data model which can load data from hadoop filesystem directly.",
        "Issue Links": []
    },
    "MAHOUT-1580": {
        "Key": "MAHOUT-1580",
        "Summary": "Optimize getNumNonZeroElements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Sebastian Schelter",
        "Reporter": "Sebastian Schelter",
        "Created": "13/Jun/14 12:31",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/Jun/14 20:32",
        "Description": "getNumNonZeroElements in AbstractVector uses the nonZeroes -iterator internally which adds a lot of overhead for certain types of vectors, e.g. the dense ones. We should add custom implementations here.",
        "Issue Links": []
    },
    "MAHOUT-1581": {
        "Key": "MAHOUT-1581",
        "Summary": "Timeout-based SRM test can hit limits on Apache test infra",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ted Dunning",
        "Created": "14/Jun/14 17:44",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "15/Jul/14 22:19",
        "Description": "I put in some tests to ensure that the SRM.times method stays relatively fast and doesn't revert to the old dense behavior.  These are inherently timeout based since there is no way to count operations.\nI will extend the timeout another 10x.  The failing version of the test takes hours or more so this should still catch any regressions.",
        "Issue Links": []
    },
    "MAHOUT-1582": {
        "Key": "MAHOUT-1582",
        "Summary": "Create simpler row and column aggregation API at local level",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ted Dunning",
        "Created": "16/Jun/14 03:33",
        "Updated": "09/Oct/16 23:06",
        "Resolved": "09/Oct/16 23:06",
        "Description": "The issue is that the current row and column aggregation API makes it difficult to do anything but row by row aggregation using anonymous classes.  There is no scope for being aware of locality, nor to use the well known function definitions in Functions.  This makes lots of optimizations impossible and many of these are optimizations that we want to have.  An example would be adding up absolute values of values.  With the current API, it would be very hard to optimize for sparse matrices and the wrong direction of iteration but with a different API, this should be easy.\nWhat I suggest is an API of this form:\n\n   Vector aggregateRows(DoubleDoubleFunction combiner, DoubleFunction mapper)\n\n\nThis will produce a vector with one element per row in the original.  The nice thing here is that if the matrix is row major, we can iterate over rows and accumulate a value for each row using sparsity as available.  On the other hand, if the matrix is column major, we can keep a vector of accumulators and still use sparsity as appropriate.  \nThe use of sparsity comes in because the matrix code now has control over both of the loops involved and also has visibility into properties of the map and combine functions.  For instance, ABS(0) == 0 so if we combine with PLUS, we can use a sparse iterator.",
        "Issue Links": []
    },
    "MAHOUT-1583": {
        "Key": "MAHOUT-1583",
        "Summary": "cbind() operator for Scala DRMs",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "16/Jun/14 23:09",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "07/Jul/14 19:44",
        "Description": "Another R-like operator, cbind (stitching two matrices together). Seems to come up now and then. \nJust like with elementwise operations, and, perhaps some other, it will have two physical implementation paths, one is zip for identically distributed operators, and another one is full join in case they are not.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1365"
        ]
    },
    "MAHOUT-1584": {
        "Key": "MAHOUT-1584",
        "Summary": "Create a detailed example of how to index an arbitrary dataset and run LDA on it",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.11.2",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Nico Scherer",
        "Created": "18/Jun/14 16:49",
        "Updated": "12/Mar/16 20:31",
        "Resolved": "08/Sep/15 23:48",
        "Description": "As students from Sebastian Schelters class, we will create a detailed example of how to index an arbitraty dataset and run Mahout LDA on it. Also, we will have a look at the current dev page descriptions of LDA and see if the documentation is up to date.",
        "Issue Links": []
    },
    "MAHOUT-1585": {
        "Key": "MAHOUT-1585",
        "Summary": "Javadocs are not hosted By Mahout Quality",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Andrew Palumbo",
        "Created": "19/Jun/14 13:14",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "12/Apr/15 12:23",
        "Description": "The links to Javadocs for Math, Integration and Examples are all redirected to a password protected Build page. MR-Legacy is currently the only Javadoc being published and hosted by Mahout-Quality",
        "Issue Links": [
            "/jira/browse/MAHOUT-1562",
            "/jira/browse/MAHOUT-1647"
        ]
    },
    "MAHOUT-1586": {
        "Key": "MAHOUT-1586",
        "Summary": "Downloads must have hashes",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "collections",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebb",
        "Created": "19/Jun/14 17:58",
        "Updated": "12/Apr/15 17:24",
        "Resolved": "02/Apr/15 07:06",
        "Description": "The download archives in http://www.apache.org/dist/mahout/mahout-collections-1.0/ don't have any hashes. These are required; please add either MD5 or SHA hashes (or both) to https://dist.apache.org/repos/dist/release/mahout/mahout-collections-1.0/",
        "Issue Links": []
    },
    "MAHOUT-1587": {
        "Key": "MAHOUT-1587",
        "Summary": "Update website to reflect move to GitHub",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "22/Jun/14 20:37",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "15/Aug/14 15:45",
        "Description": "The following pages have directions for either checking out code from SVN, patching with SVN or have direct links to files in the old SVN repo:\nmahout.apache.org/developers/developer-resources.html\nmahout.apache.org/developers/version-control.html\n-http://mahout.apache.org/developers/patch-check-list.html-\nmahout.apache.org/developers/how-to-contribute.html\nmahout.apache.org/users/basics/creating-vectors-from-text.html\nmahout.apache.org/users/classification/bayesian.html\nmahout.apache.org/users/classification/twenty-newsgroups.html\nmahout.apache.org/users/clustering/k-means-clustering.html\nmahout.apache.org/users/clustering/fuzzy-k-means.html\nmahout.apache.org/users/clustering/canopy-clustering.html",
        "Issue Links": []
    },
    "MAHOUT-1588": {
        "Key": "MAHOUT-1588",
        "Summary": "Multiple input path support in recommendation job",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Pat Ferrel",
        "Reporter": "Xiaomeng Huang",
        "Created": "25/Jun/14 08:34",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "05/Apr/15 16:22",
        "Description": "Now recommendation job can only import a input path via \"--input\", and can't load file from different path. Customers may put preference data in different path. This is a very usual scenario.\nI add a option named \"--multiInput(-mi)\", and don't remove the original input option. These two input option can set together. And the modification only refer to  PreparePreferenceMatrixJob, which load data from filesystem.",
        "Issue Links": []
    },
    "MAHOUT-1589": {
        "Key": "MAHOUT-1589",
        "Summary": "mahout.cmd has duplicated content",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI",
        "Assignee": "Pat Ferrel",
        "Reporter": "Venkat Ranganathan",
        "Created": "28/Jun/14 00:29",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "01/Apr/15 18:26",
        "Description": "bin/mahout.cmd has duplicated contents.   Need to trim it",
        "Issue Links": []
    },
    "MAHOUT-1590": {
        "Key": "MAHOUT-1590",
        "Summary": "mahout unit test failures due to guava version conflict on hadoop 2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Venkat Ranganathan",
        "Created": "28/Jun/14 00:43",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "26/Mar/15 17:53",
        "Description": "Running \n   mvn clean test -Dhadoop2.version=2.4.0 \nhas many unit test failures because guava version mismatch.   \nFor example:\n======\ncompleteJobToyExample(org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest)  Time elapsed: 0.736 sec  <<< ERROR!\njava.lang.NoSuchMethodError: com.google.common.base.Stopwatch.elapsedMillis()J\n        at __randomizedtesting.SeedInfo.seed([BEBBF9ACD237F984:B570D1523391FD4E]:0)\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:278)\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:375)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:493)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)\n        at org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJob.run(ParallelALSFactorizationJob.java:172)\n        at org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.explicitExample(ParallelALSFactorizationJobTest.java:112)\n        at org.apache.mahout.cf.taste.hadoop.als.ParallelALSFactorizationJobTest.completeJobToyExample(ParallelALSFactorizationJobTest.java:71)\n=====\nhadoop mapreduce V2 is using guava v11.0.2 and mahout is using guava version 16.0\nAfter trying different versions guava version 14.0 seems to have hadoop MR V2 compatible jars and mahout needed classes. \nThe unit tests ran successfully after changing the dependency in mahout to v14.0",
        "Issue Links": [
            "/jira/browse/MAHOUT-1657"
        ]
    },
    "MAHOUT-1591": {
        "Key": "MAHOUT-1591",
        "Summary": "Parallel job in recommendation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Xiaomeng Huang",
        "Created": "04/Jul/14 02:23",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "11/Jul/14 01:30",
        "Description": "Now all job in recommendation are sequential. But some jobs are not depend on the previous job. For example, in PreparePreferenceMatrixJob.java, the input of job itemIDIndex and toUserVectors are same. So they can run concurrently. And so as RowSimilarityJob.java.",
        "Issue Links": []
    },
    "MAHOUT-1592": {
        "Key": "MAHOUT-1592",
        "Summary": "bin/maout's seqdirectory doesn't work when MAHOUT_LOCAL non-empty",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Alex Ott",
        "Created": "08/Jul/14 11:58",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "18/Mar/15 00:53",
        "Description": "trying to run seqdirectory with MAHOUT_LOCAL set to non-empty lead to following error:\n\n>mahout seqdirectory -i ${WORK_DIR}/20news-all -o ${WORK_DIR}/20news-seq -ow   13:48 0\nMAHOUT_LOCAL is set, so we don't add HADOOP_CONF_DIR to classpath.                                           \nMAHOUT_LOCAL is set, running locally\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/ott/work/mahout-head/examples/target/mahout-examples-1.0-SNAPSHOT-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/ott/work/mahout-head/examples/target/dependency/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n14/07/08 13:50:39 INFO common.AbstractJob: Command line arguments: {--charset=[UTF-8], --chunkSize=[64], --endPhase=[2147483647], --fileFilterClass=[org.apache.mahout.text.PrefixAdditionFilter], --input=[/home/ott/work/exps/mh/20news-all], --keyPrefix=[], --method=[mapreduce], --output=[/home/ott/work/exps/mh/20news-seq], --overwrite=null, --startPhase=[0], --tempDir=[temp]}\n14/07/08 13:50:39 INFO common.HadoopUtil: Deleting /home/ott/work/exps/mh/20news-seq\nException in thread \"main\" java.io.FileNotFoundException: File does not exist: /home/ott/work/url-cat-exps/mh/20news-all\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:558)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.runMapReduce(SequenceFilesFromDirectory.java:162)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.run(SequenceFilesFromDirectory.java:91)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.main(SequenceFilesFromDirectory.java:65)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\n\nBut directory exists in the specified folder:\n\nott@mercury:work/exps/mh\\>ls -lsd 20news-all                                                            13:50 0\n4 drwxrwxr-x 22 ott ott 4096 Jul  8 08:49 20news-all/\n\n\nIf I explicitly specify -xm sequential flag, then there is no error, but the task isn't performed at all:\n\nMAHOUT_LOCAL is set, so we don't add HADOOP_CONF_DIR to classpath.\nMAHOUT_LOCAL is set, running locally\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/ott/work/mahout-head/examples/target/mahout-examples-1.0-SNAPSHOT-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/ott/work/mahout-head/examples/target/dependency/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n14/07/08 13:54:19 INFO common.AbstractJob: Command line arguments: {--charset=[UTF-8], --chunkSize=[64], --endPhase=[2147483647], --fileFilterClass=[org.apache.mahout.text.PrefixAdditionFilter], --input=[/home/ott/work/exps/mh/20news-all], --keyPrefix=[], --method=[sequential], --output=[/home/ott/work/exps/mh/20news-seq], --overwrite=null, --startPhase=[0], --tempDir=[temp]}\n14/07/08 13:54:19 INFO driver.MahoutDriver: Program took 548 ms (Minutes: 0.009133333333333334)",
        "Issue Links": []
    },
    "MAHOUT-1593": {
        "Key": "MAHOUT-1593",
        "Summary": "cluster-reuters.sh does not work complaining java.lang.IllegalStateException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "jaehoon ko",
        "Created": "09/Jul/14 10:40",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "03/Apr/15 23:32",
        "Description": "When I choose \"kmeans clustering\" in cluster-reuters.sh, clusterdump complains java.lang.IllegalStateException as follows:\n\nException in thread \"main\" java.lang.IllegalStateException: /tmp/mahout-work-user/reuters-kmeans/clusters-*-final\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:78)\n        at org.apache.mahout.clustering.evaluation.ClusterEvaluator.loadClusters(ClusterEvaluator.java:93)\n        at org.apache.mahout.clustering.evaluation.ClusterEvaluator.<init>(ClusterEvaluator.java:81)\n        at org.apache.mahout.utils.clustering.ClusterDumper.printClusters(ClusterDumper.java:208)\n        at org.apache.mahout.utils.clustering.ClusterDumper.run(ClusterDumper.java:157)\n        at org.apache.mahout.utils.clustering.ClusterDumper.main(ClusterDumper.java:101)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:145)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:153)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nCaused by: java.io.FileNotFoundException: File /tmp/mahout-work-user/reuters-kmeans/clusters-*-final does not exist.\n        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:654)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:712)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$14.doCall(DistributedFileSystem.java:708)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:708)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1483)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1523)\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:70)\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)\n        ... 18 more\n\n\nOther clustering options run well.",
        "Issue Links": []
    },
    "MAHOUT-1594": {
        "Key": "MAHOUT-1594",
        "Summary": "Example factorize-movielens-1M.sh does not use HDFS",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Andrew Palumbo",
        "Reporter": "jaehoon ko",
        "Created": "09/Jul/14 11:22",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "04/Apr/15 02:02",
        "Description": "It seems that factorize-movielens-1M.sh does not use HDFS at all. All paths look local paths, not HDFS. So the example crashes immeidately because it cannot find input data from HDFS:\n\nException in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: /tmp/mahout-work-hoseog.lee/movielens/ratings.csv\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:320)\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:263)\n        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:375)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:493)\n        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)\n        at org.apache.mahout.cf.taste.hadoop.als.DatasetSplitter.run(DatasetSplitter.java:94)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n        at org.apache.mahout.cf.taste.hadoop.als.DatasetSplitter.main(DatasetSplitter.java:64)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:145)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:153)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
        "Issue Links": []
    },
    "MAHOUT-1595": {
        "Key": "MAHOUT-1595",
        "Summary": "iterateNonZero() is broken in MatrixVectorView",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Anand Avati",
        "Created": "09/Jul/14 22:03",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "28/Aug/14 10:31",
        "Description": "iterateNonZero() in MatrixVectorView is broken and behaves like a normal iterator, over all elements.",
        "Issue Links": []
    },
    "MAHOUT-1596": {
        "Key": "MAHOUT-1596",
        "Summary": "support for rbind() operator on DRMs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Anand Avati",
        "Created": "16/Jul/14 01:02",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "29/Jul/14 20:26",
        "Description": "an R like operator, cbind(), for appending rows of second matrix below the rows of first matrix to create a new matrix.",
        "Issue Links": []
    },
    "MAHOUT-1597": {
        "Key": "MAHOUT-1597",
        "Summary": "A + 1.0 (element-wise scala operation) gives wrong result if rdd is missing rows, Spark side",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "22/Jul/14 21:03",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "28/Jul/14 17:35",
        "Description": "// Concoct an rdd with missing rows\n    val aRdd: DrmRdd[Int] = sc.parallelize(\n      0 -> dvec(1, 2, 3) ::\n          3 -> dvec(3, 4, 5) :: Nil\n    ).map { case (key, vec) => key -> (vec: Vector)}\n\n    val drmA = drmWrap(rdd = aRdd)\n\n    val controlB = inCoreA + 1.0\n\n    val drmB = drmA + 1.0\n\n    (drmB -: controlB).norm should be < 1e-10\n\n\n\nshould not fail.\nit was failing due to elementwise scalar operator only evaluates rows actually present in dataset. \nIn case of Int-keyed row matrices, there are implied rows that yet may not be present in RDD. \nOur goal is to detect the condition and evaluate missing rows prior to physical operators that don't work with missing implied rows.",
        "Issue Links": []
    },
    "MAHOUT-1598": {
        "Key": "MAHOUT-1598",
        "Summary": "extend seq2sparse to handle multiple text blocks of same document",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Wolfgang Buchner",
        "Created": "25/Jul/14 11:28",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "30/Mar/15 17:28",
        "Description": "Currently the seq2sparse or in particular the org.apache.mahout.vectorizer.DictionaryVectorizer needs as input exactly one text block per document.\nI stumbled on this because i'm having an use case where one document represents a ticket which can have several text blocks in different languages. \nSo my idea was that the org.apache.mahout.vectorizer.DocumentProcessor shall tokenize each text block itself. So i can use language specific features in our Lucene Analyzer.\nUnfortunately the current implementation doesn't support this.\nBut with just minor changes this can be made possible.\nThe only thing which has to be changed would be the org.apache.mahout.vectorizer.term.TFPartialVectorReducer to handle all values of the iterable (not just the 1st one >.<)\nAn Alternative would be to change this Reducer to a Mapper, i don't get why in the 1st place this is implemented as an reducer. Is there any benefit from this?\nI will provide a PR via github.\nPlease have a look onto this and tell me if i am assuming anything wrong.",
        "Issue Links": []
    },
    "MAHOUT-1599": {
        "Key": "MAHOUT-1599",
        "Summary": "Add rand() operator to math-scala",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.0",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Felix Schueler",
        "Created": "29/Jul/14 14:14",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "18/Jun/15 20:34",
        "Description": "I was looking at the scala-math operators and missed a rand(m, n) operator that generates a m x n matrix filled with random numbers.\nI wanted random numbers from a uniform distribution over a specific interval so I implemented a simple uniform random number generator.\nthe rand(m, n, r) function takes any AbstractSamplerFunction and fills the matrix entries with the numbers generated by its sample() method.",
        "Issue Links": []
    },
    "MAHOUT-1600": {
        "Key": "MAHOUT-1600",
        "Summary": "Algorithms for computing correlation and covariance",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.2",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Nagamallikarjuna",
        "Created": "30/Jul/14 07:10",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "06/Aug/15 02:44",
        "Description": "I checked the list of Mahout algorithms, I didn't find algorithms for computing correlation and covariance. I already written those two algorithms to solve real world business problems. I am planning to contribute them to Mahout",
        "Issue Links": []
    },
    "MAHOUT-1601": {
        "Key": "MAHOUT-1601",
        "Summary": "Add javadoc for the classes - as there is no clue what the class is for .",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Harish Kayarohanam",
        "Created": "02/Aug/14 05:17",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "02/Apr/15 08:14",
        "Description": "I found that the following classes \norg.apache.mahout.cf.taste.impl.neighborhood.DummySimilarity\norg.apache.mahout.cf.taste.impl.similarity.GenericUserSimilarity\norg.apache.mahout.cf.taste.impl.similarity.LogLikelihoodSimilarity\ndid not have java doc . So I was unable to find what these classes are for .\nShall we add java doc for the same ?",
        "Issue Links": []
    },
    "MAHOUT-1602": {
        "Key": "MAHOUT-1602",
        "Summary": "Euclidean Distance Similarity Math",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering,                                            Math",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Leonardo Fernandez Sanchez",
        "Created": "04/Aug/14 09:39",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "31/Mar/15 03:51",
        "Description": "Within the file:\n/mrlegacy/src/main/java/org/apache/mahout/cf/taste/impl/similarity/EuclideanDistanceSimilarity.java\nMentions that the implementation should be sqrt / (1 + distance).\nOnce the equation is simplified, should be: \n1 / ((1 + distance) / sqrt)\nCoded:\nreturn 1.0 / ((1.0 + Math.sqrt(sumXYdiff2)) / Math.sqrt);\nBut instead is (missing grouping brackets): \n1 / (1 + distance / sqrt )\nCoded:\nreturn 1.0 / (1.0 + Math.sqrt(sumXYdiff2) / Math.sqrt);",
        "Issue Links": []
    },
    "MAHOUT-1603": {
        "Key": "MAHOUT-1603",
        "Summary": "Tweaks for Spark 1.0.x",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "04/Aug/14 22:13",
        "Updated": "13/Apr/15 10:22",
        "Resolved": "06/Mar/15 01:32",
        "Description": "Tweaks necessary current codebase on top of spark 1.0.x",
        "Issue Links": []
    },
    "MAHOUT-1604": {
        "Key": "MAHOUT-1604",
        "Summary": "Create a RowSimilarity for Spark",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI,                                            cooccurrence",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "05/Aug/14 15:28",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "18/Mar/15 13:52",
        "Description": "Using CooccurrenceAnalysis.cooccurrence create a driver that reads a text DRM or two and produces LLR similarity/cross-similarity matrices.\nThis will produce the same results as ItemSimilarity but take a Drm as input instead of individual cells.\nThe first version will only support LLR, other similarity measures will need to be in separate Jiras",
        "Issue Links": []
    },
    "MAHOUT-1605": {
        "Key": "MAHOUT-1605",
        "Summary": "Make VisualizerTest locale independent",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Anand Avati",
        "Reporter": "Frank Rosner",
        "Created": "07/Aug/14 13:42",
        "Updated": "13/Apr/15 09:56",
        "Resolved": "05/Apr/15 16:20",
        "Description": "Problem\nWhen trying to build Mahout on a machine with a locale that uses a different decimal separator, org.apache.mahout.classifier.df.tools.VisualizerTest fails because of String assertions that are locale dependent.\nExpected: humidity < 77.5 : yes\nActual: humidity < 77,5 : yes\nSolution\nMake assertions locale independent.",
        "Issue Links": []
    },
    "MAHOUT-1606": {
        "Key": "MAHOUT-1606",
        "Summary": "Add rowSums, rowMeans and diagonal extraction operations to distributed matrices",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "12/Aug/14 23:21",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "15/Aug/14 19:10",
        "Description": "per declaration. \nA need for these has been encountered while implementing proprietary bigram co-occurrence analysis  (directed and undirected occurrences).",
        "Issue Links": []
    },
    "MAHOUT-1607": {
        "Key": "MAHOUT-1607",
        "Summary": "spark-shell:scheduler.DAGScheduler: Failed to run fold at CheckpointedDrmSpark.scala:192",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9,                                            1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI",
        "Assignee": "Andrew Palumbo",
        "Reporter": "hhlin",
        "Created": "17/Aug/14 12:27",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "27/Mar/15 12:41",
        "Description": "follow the step by http://mahout.apache.org/users/sparkbindings/play-with-shell.html, mahou spark-shell startup normally,but when  exec \"val drmX = drmData(::, 0 until 4);\" ,it throw  exception as bellow:\n14/08/17 20:13:20 INFO scheduler.DAGScheduler: Failed to run fold at CheckpointedDrmSpark.scala:192\n14/08/17 20:13:20 INFO scheduler.TaskSetManager: Loss was due to java.lang.ArrayStoreException: scala.Tuple3 [duplicate 6]\n14/08/17 20:13:20 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0.0:1 failed 4 times, most recent failure: Exception failure in TID 6 on host iZ23qefud7nZ: java.lang.ArrayStoreException: scala.Tuple3\n        com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:338)\n        com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.read(DefaultArraySerializers.java:293)\n        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)\n        com.twitter.chill.WrappedArraySerializer.read(WrappedArraySerializer.scala:34)\n        com.twitter.chill.WrappedArraySerializer.read(WrappedArraySerializer.scala:21)\n        com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)\n        org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:118)\n        org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply(ParallelCollectionRDD.scala:80)\n        org.apache.spark.rdd.ParallelCollectionPartition$$anonfun$readObject$1.apply(ParallelCollectionRDD.scala:80)\n        org.apache.spark.util.Utils$.deserializeViaNestedStream(Utils.scala:120)\n        org.apache.spark.rdd.ParallelCollectionPartition.readObject(ParallelCollectionRDD.scala:80)\n        sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        java.lang.reflect.Method.invoke(Method.java:606)\n        java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)\n        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)\n        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)\n        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n        java.io.ObjectInputStream.skipCustomData(ObjectInputStream.java:1956)\n        java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1850)\n        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)\n        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)\n        java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)\n        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)\n        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)\n        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:165)\n        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        java.lang.Thread.run(Thread.java:745)\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1049)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1033)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1031)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1031)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:635)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:635)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:635)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1234)\n\tat akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)\n\tat akka.actor.ActorCell.invoke(ActorCell.scala:456)\n\tat akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)\n\tat akka.dispatch.Mailbox.run(Mailbox.scala:219)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",
        "Issue Links": []
    },
    "MAHOUT-1608": {
        "Key": "MAHOUT-1608",
        "Summary": "Add Option WikipediaToSequenceFile to remove Category Labels from Documents",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "23/Aug/14 03:01",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "29/Aug/14 00:16",
        "Description": "Currently WikipediaMapper job extracts Category labels from the text of the Wikipedia documents and leaves the label as [[Category:label]] in the document.  Add in an option to WikipediaToSequenceFile.java to remove [[Category:label]] from the text after extracting the label.",
        "Issue Links": []
    },
    "MAHOUT-1609": {
        "Key": "MAHOUT-1609",
        "Summary": "NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Andrew Palumbo",
        "Reporter": "shruti agrawal",
        "Created": "25/Aug/14 14:37",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "27/Mar/15 12:12",
        "Description": "I am trying to run random forest classifier for Lymphoma dataset with 96 instances, 4026 features, and 1 class label(ACL/non ACL). Here is one sample instance: \n0.46,0.70,0.67,-0.23,0.00,0.09,-0.02,-0.57,-0.17,-0.25,-0.47,0.04,0.13,0.44,1.57,1.69,0.26,0.93,0.26,0.94,-0.32,-0.81,0.10,0.03,-0.71,-1.16,-0.16,0.00,0.06,-1.08,-1.58,-0.53,0.86,1.96,0.88,1.06,-1.03,0.58,1.01,0.66,1.29,0.72,0.05,-0.79,0.24,0.98,1.22,0.70,0.74,0.57,0.96,-0.03,0.85,0.16,0.17,1.66,1.34,-0.17,0.40,-0.25,-0.78,-0.31,0.35,1.12,1.65,1.24,1.61,1.01,-0.19,0.32,3.58,1.74,1.37,1.71,0.75,2.46,2.68,0.07,0.87,0.98,-0.76,-0.97,-0.06,-0.03,-0.90,0.51,1.00,1.03,0.41,-0.46,-0.19,0.14,-0.36,0.35,0.44,-0.07,-0.04,-0.68,-1.16,-1.68,-1.94,-1.28,-1.28,0.51,0.72,0.12,1.15,0.24,-1.56,0.09,0.13,0.24,0.20,0.57,-0.63,-0.20,0.06,-0.76,-0.83,0.28,0.05,0.00,-0.49,-1.62,0.13,0.00,0.06,0.12,0.58,0.22,0.03,0.20,-0.43,0.27,-0.03,0.05,0.16,0.79,-0.27,-0.05,0.79,0.77,0.54,-0.51,-0.37,-0.40,-0.51,-0.79,-1.09,-1.03,-0.13,-0.46,-0.74,-1.15,-1.44,-0.01,0.07,0.14,0.04,-0.58,-0.55,-0.43,-0.29,0.24,-0.03,-0.19,-0.64,0.34,0.06,-0.53,-1.16,-0.05,-0.65,-0.32,-0.17,-0.19,-0.41,-0.65,-1.23,-1.46,-0.42,-0.88,-0.74,-1.10,-0.99,-0.28,0.28,0.54,-1.07,-0.12,-0.83,-0.21,0.71,0.46,0.21,0.34,-0.26,0.62,0.06,-0.27,-0.83,-0.01,-0.30,-1.17,-0.02,0.05,0.35,-1.23,-0.63,-0.91,-0.88,0.12,-0.50,-0.05,-0.75,-0.90,-0.79,-1.49,-1.48,-0.61,-0.21,1.08,-0.23,-0.98,-0.60,0.00,-0.03,0.09,0.20,-0.92,-0.80,0.29,-0.44,0.85,0.66,0.72,0.65,0.33,-0.18,0.03,0.02,0.08,0.42,-0.88,-0.18,1.28,1.22,0.49,-0.33,-0.40,0.16,0.35,1.06,1.23,1.80,1.73,0.43,0.27,0.24,0.22,-1.21,-1.87,-2.33,-2.87,-1.98,-1.60,-2.12,-2.36,-1.01,1.07,0.67,0.74,0.17,0.13,0.17,-0.12,0.52,1.40,1.22,1.32,1.56,1.58,1.63,1.93,1.55,0.23,0.38,0.37,0.21,0.14,0.13,0.30,-0.13,0.52,0.13,-0.33,-0.84,-0.81,-1.00,-1.33,0.61,0.71,0.51,0.17,0.05,-0.60,0.20,1.41,0.82,0.24,-0.16,-0.82,-0.29,0.25,-0.77,0.05,-0.07,-0.31,-0.31,-0.85,-1.36,-0.78,-0.34,0.13,-0.12,-0.20,0.00,0.99,1.35,0.85,0.78,1.54,0.10,-0.03,0.93,-0.84,-0.56,-0.29,0.05,-0.75,0.00,-0.13,0.33,0.35,0.24,-0.97,-0.53,-1.48,-1.19,-0.87,-2.05,-1.86,-1.85,-1.76,-1.69,-1.69,-0.77,-1.21,-1.10,-0.75,-0.68,-0.70,-1.58,-1.09,-1.00,-0.47,-1.14,-1.32,-0.75,-0.84,-1.68,-2.06,-3.92,-1.44,-0.87,-1.25,-0.10,-0.33,2.76,1.64,2.30,1.89,1.28,1.15,0.03,0.79,0.40,0.47,0.17,-1.45,-0.33,-0.71,-0.96,0.68,-0.49,-0.72,-0.09,-0.21,-0.03,0.09,-0.01,-2.94,-4.16,-4.08,-5.08,-4.55,-4.00,-4.83,-3.74,-0.78,-3.62,-0.97,-0.53,-0.82,-1.07,-1.28,-1.33,-1.09,0.53,-0.43,-0.82,-0.13,-0.12,-0.03,-0.09,0.04,0.45,0.54,0.84,0.33,-0.96,-1.08,0.58,-0.61,-0.34,0.51,0.45,0.10,0.12,-0.48,-1.07,-0.20,-0.75,-0.19,0.14,-1.05,-1.52,-0.93,-0.15,-0.72,-0.96,-2.03,-1.14,0.06,0.04,-0.25,0.49,0.40,-0.24,-0.91,-0.03,0.18,0.39,0.61,1.15,0.36,1.18,1.01,0.23,-0.82,-0.69,-0.06,0.14,0.55,0.09,0.40,0.87,1.29,0.22,0.15,0.79,0.74,0.01,0.26,0.22,0.17,-0.36,0.09,-0.21,0.91,0.55,0.52,-0.20,-0.26,-0.16,0.17,0.57,-0.45,-1.30,-0.44,-0.16,-0.17,0.69,0.46,-1.47,-2.25,-2.03,-2.40,-2.88,-2.94,-0.67,-0.79,-1.24,-0.83,-0.75,-0.50,-0.84,-0.13,0.33,-0.19,-0.74,-0.23,0.23,0.00,0.15,0.03,-0.14,0.08,0.24,0.18,0.24,0.20,-0.01,0.23,0.03,-0.34,-0.14,0.65,0.39,-0.09,0.23,-0.14,0.42,0.77,0.48,-0.27,0.78,0.06,0.07,0.55,0.34,-0.78,-0.53,-0.26,0.18,0.36,-0.02,0.36,0.89,0.63,0.96,0.88,0.59,-0.81,-0.39,-0.18,-0.43,0.79,0.15,0.05,-0.08,-0.02,0.58,1.31,0.83,0.30,0.98,-0.29,-0.19,-0.31,-0.41,-0.26,-0.62,-0.27,-0.14,0.07,0.47,-0.45,-1.69,-2.14,-0.18,0.22,-0.06,0.62,-0.67,-1.81,-1.49,-0.84,-0.85,-1.00,-0.80,-0.44,0.44,0.11,0.06,0.02,-0.06,0.15,-0.40,0.18,-0.96,-0.38,0.41,-0.37,-1.37,0.78,0.33,0.08,0.34,0.64,-0.19,0.31,0.33,0.99,0.54,-0.31,-0.37,-0.41,-0.36,-0.89,-0.29,-0.05,-0.17,-1.06,0.34,0.35,0.00,-0.37,1.22,0.66,0.69,1.30,0.75,0.98,0.23,-0.34,0.68,-0.23,-0.12,-0.13,-0.59,-0.03,-0.28,-1.39,-0.79,-0.78,-0.72,-0.79,0.22,-0.47,0.09,0.05,-0.16,-1.25,0.00,0.01,-0.64,-1.82,-0.30,-0.57,0.31,0.03,0.38,0.27,-0.71,-0.48,-0.57,0.18,0.18,0.94,1.27,0.90,0.60,-0.47,-0.63,-0.82,-0.60,-0.33,-0.10,0.73,0.96,0.75,-1.04,-0.41,-1.33,-0.72,-0.09,0.01,-0.95,-0.66,-0.62,-0.82,-0.05,0.59,-0.05,-1.05,-0.94,-0.42,0.52,0.13,-0.20,-0.10,-0.38,-0.61,-0.71,-0.95,-0.59,-0.31,-1.00,-0.20,-0.65,-0.93,-0.70,-0.73,-0.22,0.51,0.00,-0.28,-0.03,0.30,-0.23,-2.75,-3.53,-2.74,-2.44,-3.45,-3.79,-1.72,-2.69,-1.61,1.41,0.54,1.64,1.37,1.68,0.58,-1.62,-1.78,-3.38,-1.61,-1.63,-1.30,-1.95,-1.05,-1.09,-0.50,-1.14,-1.26,-2.21,-2.30,-1.77,-0.60,-0.40,-1.40,-0.54,-0.75,-1.90,0.43,-0.80,0.56,-0.77,-2.31,-2.09,-1.36,-1.54,-1.38,-0.60,-0.29,-2.12,-0.75,-1.21,-0.55,-1.67,1.17,0.62,1.18,-1.17,0.93,0.66,0.31,0.73,0.22,-0.39,-0.52,-0.66,-0.07,-0.36,-0.15,-0.16,0.76,-0.17,-0.48,-0.76,-0.56,-0.48,-0.89,-0.75,0.29,-0.49,-0.09,0.65,-0.33,-0.52,0.07,-0.12,-0.71,-0.60,-0.43,-0.46,-0.35,-0.23,0.04,0.03,0.31,-0.08,0.03,0.67,0.71,0.37,-0.16,-0.71,-0.68,-0.45,-0.77,-0.58,0.11,0.02,-0.14,-1.50,-0.21,0.47,-0.59,-1.11,-0.76,0.19,-0.94,0.09,-1.55,-1.92,-1.23,-2.37,-1.77,-0.19,0.10,-0.50,0.32,0.54,0.19,0.22,-0.41,-0.46,0.27,0.20,1.76,1.21,0.50,0.02,0.30,?,0.08,-0.12,-0.96,0.58,0.28,0.30,0.96,0.63,-0.07,0.06,0.19,0.25,0.26,0.03,-0.51,0.15,0.07,-0.90,-0.43,-1.51,-0.31,-0.60,-0.68,-0.56,-0.63,-0.26,0.01,-0.19,-0.22,-0.26,-0.03,-1.40,0.00,0.66,0.83,0.53,0.14,-0.13,-0.07,-0.45,-0.09,0.21,0.23,0.05,0.28,0.00,0.12,-1.32,-0.21,-0.09,-0.66,-0.16,-0.53,0.59,0.14,0.17,0.46,-0.26,-0.62,-0.79,-0.14,-0.68,-0.12,-0.23,-0.59,-0.64,0.34,0.61,0.09,0.00,-0.48,-0.03,0.61,-0.40,-0.57,-0.85,-0.34,-0.46,-1.16,-0.73,0.19,-0.44,0.35,0.43,0.07,-0.19,0.35,-0.37,0.05,-0.64,0.56,-0.37,-0.48,-0.19,-0.49,-0.20,0.13,0.12,-0.47,0.06,-0.34,-1.34,-1.07,0.39,-0.04,-0.53,0.21,-2.07,-0.45,0.71,-0.30,-0.76,-1.61,-0.96,-0.75,0.49,-0.67,-0.72,-0.60,-1.04,-0.58,-0.67,-0.89,-0.34,-0.57,1.16,0.61,-0.25,-0.20,-0.46,-1.14,-0.18,-0.54,-0.69,-0.54,-0.33,-0.42,-1.41,-2.02,-1.83,-0.22,-2.02,-1.10,0.02,-0.79,-1.86,-1.44,-1.55,1.06,-0.31,-0.48,-0.32,-0.04,-0.06,2.50,2.79,2.77,2.05,0.62,1.77,-0.41,0.14,-1.00,-2.20,-3.15,-1.90,-0.14,-0.49,-0.80,-0.08,0.68,-1.02,-0.73,-0.44,-0.40,-0.99,-1.16,-0.27,-0.34,-1.14,-0.57,0.20,0.22,-0.31,-0.24,-0.34,-0.47,-0.94,-0.34,0.28,-0.16,-0.12,-0.22,-0.74,-0.81,-1.07,-1.34,-1.70,0.00,-0.30,-0.68,-0.08,0.83,0.00,-1.52,-1.57,-0.04,-0.41,-0.44,0.39,1.85,1.23,0.13,-0.32,1.29,-0.77,-0.25,-0.13,-0.10,0.01,-0.05,-0.14,0.03,0.31,0.21,0.14,0.42,0.71,0.55,0.19,-0.28,-0.58,-0.07,0.63,0.22,0.61,-0.40,-0.15,-0.65,-0.12,-0.56,0.28,0.18,-0.17,0.51,0.40,-0.08,0.12,0.13,-0.14,-0.81,0.33,0.38,0.10,-0.28,-0.08,-0.61,0.51,-0.65,-0.96,-0.82,-0.51,-0.70,-0.35,0.51,-0.05,-0.26,-0.71,-0.68,0.45,-0.69,-0.32,-0.12,0.18,-0.74,-0.59,-0.41,0.29,-0.16,0.00,-0.13,-0.20,0.35,-0.05,-0.42,-0.82,-0.08,0.19,-0.76,-0.02,-0.69,0.02,-0.50,-0.68,-0.35,-0.48,-0.33,-1.16,-0.85,-0.24,-1.55,-2.07,-1.68,0.17,0.06,-0.51,0.02,0.35,0.66,0.49,-0.72,-0.17,-0.39,0.38,-0.74,-0.71,0.17,-0.31,0.10,0.03,-0.25,0.19,-0.74,1.29,-0.96,-1.12,0.80,0.07,0.79,0.64,-0.31,-0.58,-0.12,-0.08,-0.06,0.31,-0.28,-0.40,-0.24,-0.99,-1.21,-0.83,-0.48,-0.81,-0.63,-1.41,-1.00,-1.39,-0.27,-0.10,-1.03,-0.69,-0.26,-0.64,0.61,-0.06,-0.23,-0.50,-0.60,-0.55,1.04,-0.57,-0.09,-0.05,0.20,-0.75,-0.51,0.05,-0.22,-1.13,-0.16,-0.25,0.73,-0.48,-0.78,-1.26,-0.20,-0.60,-0.65,0.21,-0.40,-0.83,-1.32,-1.11,-0.72,-0.54,-0.52,-0.28,-0.54,-0.12,0.75,0.55,0.17,0.58,-1.32,-0.13,-0.57,-0.89,-1.16,-1.21,-1.61,0.19,-0.02,-0.92,-0.93,-0.85,-0.78,-0.98,-1.09,-0.19,0.02,-0.08,-0.47,-0.92,-0.99,-0.53,-0.39,-0.58,0.28,0.78,0.79,-0.81,-1.23,-1.29,-0.22,-1.26,-0.95,-0.78,-1.76,-0.38,-0.49,-1.13,0.10,0.38,0.33,0.86,0.05,-0.31,-1.48,-0.99,-0.43,-0.49,-0.77,-0.47,-2.04,-1.89,-1.15,-1.23,-0.61,-0.61,-0.96,-0.88,-0.74,-0.73,-0.01,-0.79,-0.16,-0.75,-0.36,-0.45,-0.09,-0.67,-1.64,-1.22,-3.06,-1.12,-0.30,-1.96,-2.31,0.60,0.51,1.74,1.71,1.11,0.34,-0.16,0.00,0.05,-0.06,-0.47,-0.28,-0.56,-0.09,-0.48,-0.07,0.09,-0.43,-0.49,0.02,0.04,0.25,0.42,0.00,-0.73,-0.43,-0.15,-0.55,-3.29,-3.56,0.89,-0.40,-0.59,0.28,-0.64,-1.19,-1.13,-0.10,0.14,0.07,0.03,0.18,0.14,0.33,-0.40,-0.39,0.00,0.71,0.69,1.01,-0.32,-0.13,-1.88,-0.12,0.06,-0.48,0.19,0.12,0.39,0.19,0.18,0.20,1.27,0.48,-0.83,-0.35,-0.98,0.47,-0.11,-0.68,-0.17,-0.78,-0.29,0.44,0.09,-0.20,0.01,0.13,0.21,-0.28,-0.06,-0.27,-0.50,-0.50,-0.65,-0.72,-0.69,-0.32,0.59,0.46,0.47,-0.10,-0.77,-0.54,-0.56,0.19,0.40,0.08,0.56,0.31,0.37,-0.85,-0.34,0.09,-1.35,-0.94,-1.47,-1.10,-0.87,-0.44,-0.84,-0.67,0.16,0.00,0.35,-0.15,0.55,-0.21,-0.79,-0.36,0.31,0.46,0.21,-0.13,0.15,0.34,1.08,0.46,-0.26,0.08,-0.36,0.38,-1.17,-1.72,0.67,1.23,1.04,0.01,-0.46,-1.08,-0.42,-0.63,-0.68,-0.09,-0.15,-0.30,1.02,1.46,0.99,0.06,0.72,-0.06,0.38,-0.22,-0.69,-0.16,-0.47,0.32,1.01,0.35,0.13,0.22,0.14,0.09,0.34,0.06,0.15,0.33,0.27,0.55,0.35,0.02,0.43,1.16,0.61,0.78,0.03,-0.33,0.07,0.77,0.37,0.18,0.68,0.01,0.24,-0.87,-1.22,-0.66,-0.28,-0.21,-0.15,-0.72,-0.72,-0.26,-0.27,-0.21,-0.32,0.30,0.24,-0.15,0.37,0.15,0.14,-0.15,0.15,0.53,0.81,-0.61,-0.23,0.00,0.41,0.47,-0.51,-0.22,-0.58,0.26,0.54,0.74,0.38,0.62,0.07,-0.10,0.47,-0.47,-0.23,0.06,-0.31,-0.69,-0.60,-0.79,0.31,0.00,0.34,0.27,0.43,-0.17,0.45,-0.19,0.25,1.00,1.28,-0.15,-0.03,0.32,-0.03,0.12,0.63,0.41,-0.29,0.88,0.24,1.92,0.53,0.34,0.61,0.49,0.36,0.80,1.09,0.74,0.54,0.51,1.16,-0.06,0.70,0.48,1.09,0.59,1.56,0.06,0.38,0.16,0.11,0.61,0.44,-0.05,-1.89,-0.16,-0.05,-0.41,-0.66,-0.65,-0.43,-0.14,-0.25,-0.59,-0.07,0.00,0.14,-0.57,0.19,-0.15,-0.46,-0.52,-0.69,-0.64,0.08,1.98,2.47,-0.04,0.41,0.38,0.04,-0.26,-0.07,0.16,0.22,0.00,0.75,0.06,1.02,0.04,0.55,0.07,0.32,0.89,0.32,1.02,1.78,0.56,0.26,0.85,0.66,1.75,1.37,1.03,0.27,0.48,0.12,0.39,-0.08,0.47,0.20,0.81,0.36,0.29,0.41,0.14,1.45,0.65,1.15,0.47,0.20,0.20,0.83,0.37,0.96,1.36,0.57,0.69,0.48,1.99,0.84,0.56,0.63,-0.05,0.13,1.03,0.81,0.60,-0.45,-0.67,1.13,1.51,0.69,0.89,0.64,0.57,1.03,0.82,1.33,1.65,0.14,0.24,0.05,0.44,1.04,1.45,0.98,0.58,0.74,1.35,1.10,0.47,1.04,0.36,0.55,0.30,0.26,-0.18,0.51,0.29,0.63,1.37,0.51,0.59,0.31,0.16,0.43,0.38,0.32,0.63,0.82,0.65,0.67,0.69,1.24,0.71,0.39,0.59,0.76,0.33,0.55,0.68,0.41,0.85,1.32,0.55,1.14,0.52,1.08,0.51,0.81,-0.04,0.55,1.04,0.97,1.62,0.63,0.11,1.34,0.88,0.46,0.72,0.69,0.75,0.35,1.40,0.49,0.31,0.19,0.63,0.14,0.62,0.26,0.02,0.19,1.02,0.46,-0.22,0.23,0.29,0.04,-0.04,0.06,0.12,0.56,1.45,1.46,0.76,0.01,-0.15,0.00,-0.01,0.68,0.58,0.17,0.32,0.71,-0.36,-0.19,0.24,1.10,0.17,0.22,0.35,0.51,0.66,0.74,1.25,0.15,0.55,0.77,0.32,0.38,0.42,0.27,-0.81,0.63,1.26,1.44,1.80,0.64,1.01,-0.12,-0.15,-0.20,0.10,0.16,1.16,0.77,-0.27,0.07,0.08,0.12,1.06,0.81,1.03,-0.14,-0.17,0.21,-0.14,1.37,1.54,0.99,0.62,0.15,0.06,-0.12,0.21,0.16,-0.49,0.23,0.18,0.90,0.24,-0.37,0.96,0.43,0.89,-0.43,0.26,0.36,0.23,0.16,0.13,0.00,0.61,1.18,1.33,-0.09,0.14,-0.34,0.17,0.75,0.28,1.06,0.36,0.46,0.14,0.20,0.58,-0.38,0.26,0.15,0.61,-0.20,0.40,-0.49,-0.34,-0.44,0.24,1.63,1.22,0.61,0.07,0.61,0.70,0.55,0.66,0.23,0.12,0.02,0.51,0.55,0.17,-0.09,-0.04,-0.03,-0.07,0.91,0.23,0.17,0.42,0.12,2.33,0.61,-0.03,0.15,-0.07,0.62,-0.36,0.02,0.49,0.70,0.21,0.18,-0.07,-0.08,0.09,0.44,1.39,1.03,1.53,1.65,0.73,0.73,0.72,0.49,1.23,0.31,-0.81,-0.61,0.04,0.99,0.50,0.26,0.72,-0.01,0.05,0.59,0.21,0.18,-0.46,-0.26,-0.16,-0.16,0.29,0.32,0.24,-0.06,0.47,0.45,0.15,0.37,0.03,0.25,0.08,0.12,-0.02,0.26,-0.48,0.08,0.27,0.62,1.39,1.25,-0.17,-0.99,1.03,-0.27,0.33,0.38,0.00,-0.10,-0.29,-0.40,0.02,0.17,1.10,1.23,1.51,0.01,0.58,0.55,0.32,-0.06,0.36,0.50,0.68,-0.43,0.81,1.14,1.38,1.11,0.58,0.43,0.42,0.43,0.65,-0.17,0.20,0.56,-0.07,0.04,0.21,-0.59,-0.54,-0.24,-0.49,-0.10,-0.33,-0.13,0.00,-0.32,-0.53,-0.88,-0.26,?,-0.89,-0.02,-1.20,-1.09,-0.19,0.59,-0.73,-0.66,-0.21,-1.23,-2.85,-0.31,-1.06,-0.87,-0.85,-0.59,0.16,0.32,-1.33,-0.08,0.25,-0.48,-0.59,-1.05,-0.43,0.08,-0.16,-0.87,-0.65,0.40,-0.70,-0.10,-0.77,-0.05,-1.18,-0.59,-0.68,-1.03,-0.10,-0.19,-0.55,-0.48,-1.29,-0.58,-1.23,-1.23,-0.29,-0.94,-0.83,-0.01,-0.93,-0.24,-0.69,-0.62,-1.03,-0.27,0.03,0.26,0.26,0.65,0.00,-0.70,0.70,0.20,0.77,0.34,-0.45,-0.73,0.03,-0.51,-0.74,-0.48,-0.74,-0.18,-0.13,-0.44,-0.51,-0.79,0.53,-0.59,-0.40,-0.70,-0.63,-0.59,-1.35,-0.26,-0.01,0.19,-0.52,-0.78,0.00,-0.07,-1.73,-0.09,-0.81,-0.46,-0.48,-0.65,-1.46,-2.04,-1.36,-0.65,-2.07,-1.96,-1.76,-1.08,-1.81,-0.39,-0.20,-1.00,-0.26,-0.54,-0.68,-0.28,-0.19,-0.59,-0.42,-0.81,-0.69,-0.88,0.26,-1.43,-1.85,-0.11,-0.04,-0.15,-2.89,-2.34,-0.57,-0.09,0.20,-0.32,-0.17,1.31,-0.14,-0.34,0.78,0.51,0.75,-0.25,0.32,0.18,0.21,-1.19,-0.40,-1.30,-1.79,0.33,0.30,0.28,1.07,1.01,0.79,0.84,0.57,1.04,0.08,-0.09,-0.15,-0.23,0.39,1.00,1.87,3.06,3.54,2.81,2.31,2.06,1.54,2.59,2.41,2.31,-0.26,-0.09,0.43,0.64,1.20,0.90,0.60,-0.53,-0.28,-0.47,-1.62,-0.58,0.06,0.41,-0.12,-0.49,0.89,1.00,-0.36,0.69,1.45,0.20,0.00,0.77,-0.46,-0.98,-0.75,-0.87,-0.36,-0.27,0.19,-0.85,3.25,1.83,2.01,0.77,2.01,-0.63,-0.86,-0.95,0.28,0.25,0.38,-0.29,-0.99,-0.08,0.37,0.23,0.66,0.28,1.26,1.37,1.05,0.46,0.12,2.17,1.97,0.42,-0.05,0.95,0.55,0.50,0.29,0.28,0.42,1.17,0.94,0.79,0.06,0.13,0.93,-1.37,1.46,1.24,2.07,2.22,1.49,0.96,-1.12,0.44,-0.86,-1.75,-0.81,-0.93,-0.76,-1.36,1.82,2.11,1.66,2.39,1.33,0.72,0.57,0.06,0.54,0.15,0.57,0.40,1.19,1.25,0.18,-1.35,-0.32,0.70,0.50,1.06,1.78,0.57,0.47,0.08,0.84,0.62,1.15,1.08,0.16,-0.28,-0.07,0.10,0.11,0.36,-0.25,0.17,0.17,0.22,0.14,0.29,0.63,-0.05,-0.08,-1.11,0.05,0.62,0.61,0.07,0.02,-0.11,0.20,-0.54,-0.57,-0.27,0.84,0.39,0.06,0.04,-0.41,0.28,0.08,0.00,0.07,-0.39,-0.52,-0.78,-0.21,-0.17,-0.58,-0.49,-0.60,-0.54,-0.67,0.02,-0.06,0.31,-0.75,-0.13,-0.31,0.09,0.30,0.82,-0.69,-0.06,0.14,0.26,-0.09,-0.83,0.29,-1.15,-1.26,-1.28,-1.39,0.32,0.51,1.37,0.97,1.11,-0.22,0.16,0.22,0.29,-0.71,-0.52,-0.27,-1.24,0.14,0.38,0.00,0.83,0.55,0.25,1.79,1.74,0.29,0.52,2.11,0.54,1.60,1.26,1.31,1.22,0.88,0.94,0.66,-0.39,0.06,0.05,1.18,0.69,0.66,0.94,1.24,0.00,0.93,1.21,0.25,-0.14,0.10,0.55,0.33,0.09,0.46,0.24,1.85,0.33,-0.72,0.65,-0.30,-0.70,-1.01,-0.60,0.14,0.05,0.18,0.27,1.48,1.77,3.09,4.07,1.88,2.44,2.67,3.89,4.11,2.42,1.13,1.71,3.61,3.73,1.03,0.55,1.15,0.51,0.56,0.56,0.37,0.48,1.61,1.58,0.79,0.65,0.85,1.24,1.17,1.15,1.17,0.13,0.22,-0.08,0.14,0.17,0.86,0.45,0.75,-0.80,-0.65,-0.49,-0.43,-0.40,-0.10,-1.40,-1.17,-0.25,-0.86,0.02,4.52,4.48,2.59,2.80,0.63,-0.44,-0.19,0.12,-0.03,-1.06,-0.89,0.13,-0.08,0.69,0.53,-0.34,0.78,1.04,-0.05,-0.09,0.32,0.01,0.36,-0.05,-0.59,0.44,-0.10,0.19,-0.05,0.21,0.31,0.64,0.94,1.09,0.62,0.25,0.19,0.51,0.28,-0.95,-0.56,0.02,0.20,-0.03,0.07,1.09,1.01,0.05,0.57,0.51,0.34,0.42,0.19,0.31,0.50,0.58,-0.07,-0.77,0.20,1.60,0.92,-0.08,0.04,0.20,-0.02,1.77,0.43,0.22,0.76,-0.44,-0.38,0.16,0.20,0.17,0.08,-0.16,0.02,-0.02,0.67,0.37,0.02,0.20,-0.15,-0.02,0.67,0.16,-0.05,-0.68,-0.52,-0.25,0.33,-0.26,-0.25,-0.40,0.19,-0.23,0.03,0.28,0.62,1.07,0.00,0.25,-0.38,0.15,1.05,0.66,0.11,0.29,-0.07,-0.07,-0.22,0.44,-0.40,0.14,0.25,2.09,1.61,0.50,0.61,0.15,0.27,-0.13,-0.56,-1.14,-0.68,-0.32,0.02,-0.11,0.08,0.03,-0.10,0.49,1.05,0.29,0.60,0.72,0.55,0.10,0.40,0.04,1.08,0.58,0.52,0.43,1.49,0.48,0.80,0.44,0.51,0.60,0.81,0.78,1.33,1.01,1.48,1.27,0.92,0.28,0.65,0.91,1.17,0.25,0.38,0.70,1.85,1.33,1.04,0.41,0.53,0.84,0.12,0.11,0.38,1.75,1.14,-0.28,2.10,-0.38,-0.50,-0.02,-0.81,-0.48,-0.78,0.60,0.71,0.18,-0.66,0.08,-0.10,-0.48,-1.00,-1.29,0.20,0.20,0.27,0.66,0.03,-0.07,-0.25,-0.12,0.44,0.50,0.62,1.22,0.63,0.65,0.23,0.62,0.32,0.58,0.88,0.12,0.84,-0.15,0.66,0.33,0.59,-0.05,0.31,1.73,0.55,0.52,1.56,0.72,0.02,0.24,1.19,0.12,1.50,0.87,0.55,0.51,0.78,0.74,1.21,0.66,0.55,0.32,1.48,1.28,1.24,0.77,0.27,1.22,0.47,1.35,0.19,0.35,0.76,-0.08,0.29,-0.06,-0.39,-0.31,0.04,0.86,0.91,1.17,1.06,1.04,0.64,0.27,0.95,0.90,2.15,-0.23,-0.23,1.38,0.76,0.29,0.18,1.04,0.66,0.47,1.24,1.08,1.21,1.87,1.95,0.13,1.62,0.99,0.88,0.64,0.85,0.99,1.32,1.02,1.38,1.30,2.09,0.48,1.27,2.42,2.26,1.54,1.60,1.56,1.38,1.18,0.99,0.97,0.23,-0.57,2.22,1.38,0.68,0.90,1.19,1.43,1.16,0.74,0.93,0.40,-0.34,0.85,2.32,2.23,1.14,0.56,0.70,-0.24,0.14,0.79,0.53,0.82,1.80,1.46,1.09,1.03,0.46,0.79,1.79,0.94,0.59,1.39,-0.05,0.81,-0.93,1.41,0.08,0.98,1.02,0.77,-0.04,-0.03,0.89,0.46,0.97,0.94,1.11,1.36,0.68,0.79,0.81,0.00,0.47,0.30,0.32,0.74,0.55,0.87,0.96,0.65,0.33,0.72,1.28,0.75,1.00,0.22,0.97,1.14,0.63,0.61,0.36,0.84,0.97,0.90,0.65,0.30,0.58,0.53,0.36,1.07,1.01,0.79,0.44,0.68,1.20,1.04,0.53,0.50,0.73,-0.07,0.40,0.29,0.66,1.43,0.89,1.08,1.47,1.16,1.10,0.98,0.98,0.57,0.82,0.57,1.24,0.19,1.25,1.21,0.70,0.63,0.49,1.43,1.13,0.99,0.94,-0.51,1.20,1.72,2.23,0.68,0.85,0.94,1.10,1.56,0.35,0.99,1.16,0.56,0.57,-0.11,0.47,0.80,0.85,0.03,0.04,0.03,-0.39,-0.55,-0.16,0.85,0.10,0.89,0.69,0.09,0.81,0.54,1.04,1.17,0.61,0.63,0.08,0.71,0.22,0.11,0.74,0.51,0.80,0.32,0.84,0.44,1.00,0.76,0.87,0.28,0.83,0.39,0.68,0.02,0.89,0.99,1.47,0.28,0.22,0.54,0.43,0.65,-0.56,0.31,0.84,-0.37,-0.51,-0.33,0.07,0.73,-0.03,0.45,-0.03,-0.03,-0.36,0.09,0.00,-1.12,0.26,-0.31,-0.02,0.62,-0.09,0.11,-0.23,0.98,0.73,0.13,0.29,0.36,-0.19,-0.21,0.34,0.52,0.42,0.01,-0.03,0.55,-0.17,-0.11,0.02,0.17,0.32,0.79,0.69,0.13,-0.06,0.07,1.08,1.42,0.91,1.45,-0.48,0.93,0.64,0.63,0.81,1.05,0.63,-0.53,-0.43,0.27,-0.56,0.69,-0.04,0.00,0.14,1.26,1.12,0.08,0.44,0.38,0.18,-0.58,-0.53,-0.16,-0.09,-0.76,0.05,0.33,0.42,0.08,0.24,0.13,-0.23,-0.39,-0.68,-0.28,-0.34,0.55,0.28,0.28,-0.26,0.02,0.42,-0.02,0.15,0.69,1.02,0.38,0.73,0.65,-0.47,1.42,0.23,-0.23,-0.03,0.78,0.88,0.24,0.16,0.41,0.34,0.56,1.03,0.75,0.80,0.39,1.17,0.66,0.46,1.16,0.37,1.59,2.27,2.22,1.97,1.89,1.04,0.57,0.83,0.13,-0.20,1.13,0.77,0.51,1.63,0.53,0.55,0.73,0.50,0.69,0.36,1.38,-0.30,-0.08,0.31,0.46,0.66,-0.28,0.16,0.12,-0.03,0.49,-0.22,0.10,-0.08,0.33,0.22,-0.32,1.45,0.48,0.14,-0.12,1.01,1.04,0.82,0.97,0.16,1.12,0.69,0.00,0.12,0.19,-0.08,0.22,1.21,0.39,1.90,0.40,1.40,0.70,0.18,0.72,0.16,0.51,0.81,-0.39,-0.33,-0.54,0.56,0.73,-0.61,-0.57,0.03,0.71,0.77,0.41,1.51,1.45,1.60,0.14,0.03,0.78,0.04,0.05,0.50,0.61,0.14,-0.09,0.62,0.51,1.23,0.72,1.00,0.48,0.18,0.00,-0.34,0.83,0.06,0.10,0.22,0.10,-0.23,-0.28,0.35,0.26,-0.57,0.00,-0.50,0.32,0.16,0.55,-0.38,-0.20,0.01,0.38,0.34,0.16,0.47,0.20,-0.58,-0.38,-0.10,-0.49,-0.24,0.68,0.16,-0.26,-0.91,-0.19,0.08,0.53,-0.13,-0.02,0.55,0.49,0.57,0.54,0.25,0.27,0.46,0.79,0.35,0.00,0.68,0.17,0.38,0.38,0.71,0.85,0.33,0.13,0.19,0.56,0.31,0.34,0.38,0.40,0.45,0.40,0.69,0.74,0.86,0.56,0.43,1.62,1.21,0.61,0.85,0.82,0.62,0.81,0.70,0.66,0.05,0.36,0.39,0.18,0.05,0.35,-0.18,0.47,0.96,-0.10,0.39,0.84,1.19,0.18,0.26,0.44,0.47,0.65,0.38,1.35,2.02,1.61,0.70,0.43,0.25,0.97,0.99,0.69,0.73,0.06,-0.05,0.53,1.80,0.76,1.07,1.72,-0.34,-0.38,-0.39,0.51,1.07,-0.22,-0.77,0.46,-0.98,-0.43,-0.98,-1.07,0.45,0.49,0.00,-0.15,0.01,-0.91,-0.31,0.39,0.77,0.16,0.12,-0.52,0.06,-0.72,-1.18,-0.51,-0.75,-0.30,0.07,-0.22,-0.31,-0.10,-0.52,0.55,0.07,-0.87,-0.17,-0.30,-1.35,-1.48,-1.55,0.49,-0.16,0.12,0.09,-1.55,-0.65,-1.53,-1.35,-1.34,-1.07,-1.73,-2.99,-2.54,-0.32,-0.84,-0.91,-0.31,-0.20,-0.89,-0.22,-0.10,-0.04,-0.86,-0.10,-1.12,-1.03,-0.78,-1.59,-0.64,-0.32,-0.34,-0.57,-0.57,0.00,-0.24,0.31,1.16,0.47,-0.08,-0.34,-0.24,-0.51,-0.46,-0.37,-0.39,0.00,-0.18,-0.35,-0.36,-1.08,0.18,0.04,0.00,0.08,-0.53,-0.21,-0.64,0.00,-0.70,-1.01,-0.81,-1.06,-0.35,-0.60,-1.80,-0.88,-1.87,-0.08,-0.81,0.01,0.39,-0.59,-0.92,-0.53,-0.05,0.49,-0.52,0.14,-0.33,-0.11,0.00,0.07,-0.14,-0.63,-0.53,-1.56,-0.18,-0.15,-0.36,0.27,1.06,0.50,0.00,-0.37,-0.47,-0.62,-0.47,-1.07,-1.02,-0.35,0.13,-0.53,-0.33,-0.47,0.79,-0.12,0.03,0.03,-0.31,-0.47,-0.56,-0.36,-0.74,-0.54,-0.86,-0.41,-0.16,-0.69,-0.42,-0.10,-0.34,-0.33,-0.46,-0.74,-0.91,-1.23,0.22,-0.72,-0.88,-0.62,-0.98,0.09,0.00,-0.24,-0.95,-0.33,-1.63,-0.38,-0.55,-0.64,0.91,-0.05,-0.48,0.41,0.40,1.70,0.46,0.37,0.20,-0.32,-0.95,-0.64,-0.63,-1.05,-0.45,0.12,-0.94,0.09,-0.88,-0.67,0.21,0.01,-0.68,-0.85,-0.86,-0.68,-0.10,0.13,0.20,-0.35,-0.06,0.32,0.17,-0.50,-1.08,-2.89,-1.79,-0.57,0.06,-0.07,-1.17,-0.88,-1.67,-1.48,0.06,0.12,-0.47,0.65,0.51,0.21,0.59,0.64,0.45,0.43,0.02,-0.05,-0.14,0.65,2.06,-0.51,-0.32,-0.50,0.82,1.32,-0.48,-0.54,-0.12,-0.31,-0.97,-2.40,-0.36,-0.35,-0.15,-0.46,-2.43,-2.23,-1.74,-2.40,-1.06,-0.64,-1.87,-0.49,-0.92,-0.78,-0.80,-1.51,-2.72,-2.37,-2.20,-1.45,-2.10,-1.66,-1.41,-1.67,-1.51,-2.45,-2.50,-0.38,-0.96,-0.15,-0.79,-1.57,-1.51,-0.76,-0.47,0.55,0.09,0.59,0.80,0.98,-1.29,0.48,-0.04,0.00,-0.18,-0.62,-0.90,0.00,-0.55,-0.07,-1.11,-0.29,-0.01,-0.61,-0.15,-1.12,-0.83,-1.03,0.04,1.74,0.16,-0.28,-0.29,0.11,-0.16,-0.94,-0.31,-0.42,0.32,1.66,0.75,-0.31,-0.47,0.04,0.33,-0.23,-0.47,-0.20,0.17,0.50,-0.50,-0.62,-0.04,-0.72,-0.55,-0.80,-0.63,-0.23,-0.35,-0.55,-0.20,-0.96,-0.23,0.53,-0.19,0.19,-0.16,0.40,-0.69,-0.20,-0.18,-0.25,-0.85,-1.23,-0.45,-0.22,0.50,-0.03,0.04,-0.55,-1.55,-0.26,-0.33,-0.27,-0.15,-0.01,0.67,-0.45,-0.79,-0.93,-0.84,-0.98,-1.08,-1.05,0.99,-0.18,0.00,-0.75,-0.30,-0.29,-0.04,-0.61,0.22,-0.65,0.08,0.22,-0.42,-1.19,1.47,0.89,0.92,0.11,-0.11,-0.38,-1.17,-0.85,-0.08,-0.47,-0.41,-0.94,-0.42,-0.52,-0.46,-0.34,0.50,-0.84,-1.59,-1.10,-1.53,-0.15,0.43,0.78,0.36,0.00,0.73,0.34,0.26,-0.26,-0.44,-0.44,-1.07,-1.00,-0.14,-0.28,-0.20,0.43,-0.23,0.38,-1.56,-0.55,-1.37,1.38,0.24,0.01,-1.00,-0.24,-0.30,-1.10,-1.19,2.01,4.45,-0.17,-0.05,0.02,-2.05,-0.37,-0.05,-0.94,-1.19,-1.72,-1.52,-2.32,-1.85,-0.60,-1.02,-1.04,-0.68,-0.23,-0.76,-0.56,-0.54,-0.85,0.04,-1.90,-2.24,-1.64,-1.36,-0.74,-1.48,-1.50,-2.38,-2.00,-2.00,-0.39,-2.90,-2.54,0.00,-0.94,-1.05,-1.18,-0.37,-1.12,-0.69,-1.28,-0.19,-0.40,-1.25,-1.13,-1.45,-1.67,-1.22,-1.38,-0.63,-1.03,-0.88,-0.68,-0.39,-0.52,-0.09,-0.57,-0.12,-0.65,-1.89,-0.90,-0.19,-0.43,-0.92,-0.96,-1.11,-1.41,-1.57,-1.53,-1.29,-1.40,-1.03,-0.05,0.10,-0.53,-0.52,-0.66,-0.33,-0.83,-0.14,0.14,-0.50,-0.61,-0.44,-0.53,-0.61,-0.19,0.37,-1.62,-1.26,-0.12,-1.59,-2.62,-2.88,-3.28,-1.39,-1.07,-0.88,-1.53,-0.16,-0.83,-1.01,-1.82,-2.13,-0.89,1.47,-0.54,-0.21,-0.08,-0.14,-0.87,-0.69,-0.82,-0.87,-0.12,-0.60,-1.00,-0.26,4.55,-0.45,-0.25,0.34,-1.27,-0.30,-1.05,-1.10,0.07,-0.47,-1.17,-0.61,-0.71,-0.50,0.35,-0.65,-0.29,-0.08,-0.33,-0.16,0.34,-0.56,-0.57,-0.05,-0.95,-0.29,0.10,0.20,0.53,-0.01,-0.66,0.27,0.41,-1.09,-0.22,-0.16,-1.07,-0.35,0.16,?,0.04,-0.72,0.00,-0.09,0.64,-0.01,0.42,0.08,-0.23,0.82,1.48,0.59,-0.22,-0.77,-0.15,-0.29,-0.43,-0.14,0.01,-0.42,-0.12,0.07,-0.21,-0.25,-0.32,0.30,-0.36,-0.23,-1.09,0.21,-0.95,0.21,-0.37,-0.56,-0.46,-0.86,-0.66,0.19,0.21,-0.29,-0.04,-0.44,0.00,-0.23,-0.28,0.30,-0.28,-0.10,-0.59,-0.19,0.69,0.55,-0.28,-0.52,-0.46,-0.74,-0.62,-1.68,-0.33,-0.65,-0.27,0.12,-0.46,-0.91,-0.94,-0.17,0.70,1.08,0.09,0.00,-0.02,0.38,-0.30,-0.28,-1.01,-1.60,-1.12,-0.38,-0.85,-0.43,0.11,-0.23,-0.72,-0.44,-0.12,0.52,-0.35,0.12,0.17,0.79,0.46,0.39,0.28,0.12,0.19,-1.58,-0.73,0.47,0.09,0.05,-0.21,0.24,0.40,0.02,0.79,0.64,0.16,1.22,1.37,-0.04,0.16,ACL\nI used the following command to run random forest:\nShrutis-MacBook-Pro:2.4.0 hadoop$ ./bin/hadoop jar /usr/local/Cellar/mahout/0.9/libexec/mahout-examples-0.9-job.jar org.apache.mahout.classifier.df.tools.Describe -p /user/hadoop/LymphomaExperiment/Lymphoma96x4026.arff -f /user/hadoop/LymphomaExperiment/Lymphoma96x4026.info -d 4026 N L\nI am displaying the partial output of running this command, alongwith the error I got. \n14/08/18 10:57:53 INFO mapred.LocalJobRunner: \n14/08/18 10:57:53 INFO mapred.Task: Task:attempt_local931511699_0001_m_002004_0 is done. And is in the process of committing\n14/08/18 10:57:53 INFO mapred.LocalJobRunner: \n14/08/18 10:57:53 INFO mapred.Task: Task attempt_local931511699_0001_m_002004_0 is allowed to commit now\n14/08/18 10:57:53 INFO output.FileOutputCommitter: Saved output of task 'attempt_local931511699_0001_m_002004_0' to hdfs://localhost:9000/user/hadoop/LymphomaOUTPUT1/_temporary/0/task_local931511699_0001_m_002004\n14/08/18 10:57:53 INFO mapred.LocalJobRunner: map\n14/08/18 10:57:53 INFO mapred.Task: Task 'attempt_local931511699_0001_m_002004_0' done.\n14/08/18 10:57:53 INFO mapred.LocalJobRunner: Finishing task: attempt_local931511699_0001_m_002004_0\n14/08/18 10:57:53 INFO mapred.LocalJobRunner: map task executor complete.\n14/08/18 10:57:53 WARN mapred.LocalJobRunner: job_local931511699_0001\njava.lang.Exception: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.lang.NullPointerException\n\tat org.apache.mahout.classifier.df.data.Dataset.getLabel(Dataset.java:183)\n\tat org.apache.mahout.classifier.df.data.Data.majorityLabel(Data.java:257)\n\tat org.apache.mahout.classifier.df.builder.DecisionTreeBuilder.build(DecisionTreeBuilder.java:154)\n\tat org.apache.mahout.classifier.df.Bagging.build(Bagging.java:57)\n\tat org.apache.mahout.classifier.df.mapreduce.partial.Step1Mapper.cleanup(Step1Mapper.java:154)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:148)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n14/08/18 10:57:53 INFO mapreduce.Job: Job job_local931511699_0001 failed with state FAILED due to: NA\n14/08/18 10:57:54 INFO mapreduce.Job: Counters: 20\n\tFile System Counters\n\t\tFILE: Number of bytes read=498621928416\n\t\tFILE: Number of bytes written=91072830290\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=10597575568\n\t\tHDFS: Number of bytes written=283293186\n\t\tHDFS: Number of read operations=8062380\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=4018098\n\tMap-Reduce Framework\n\t\tMap input records=96\n\t\tMap output records=96\n\t\tInput split bytes=276690\n\t\tSpilled Records=0\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=0\n\t\tGC time elapsed (ms)=2595\n\t\tTotal committed heap usage (bytes)=487032619008\n\tFile Input Format Counters \n\t\tBytes Read=10139695\n\tFile Output Format Counters \n\t\tBytes Written=279570\n14/08/18 10:57:54 ERROR mapreduce.Builder: Job failed!",
        "Issue Links": []
    },
    "MAHOUT-1610": {
        "Key": "MAHOUT-1610",
        "Summary": "Tests can be made more robust to pass in Java 8",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Integration",
        "Assignee": "Sean R. Owen",
        "Reporter": "Sean R. Owen",
        "Created": "27/Aug/14 14:15",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "28/Aug/14 16:39",
        "Description": "Right now, several tests don't seem to pass when run with Java 8 (at least on Java 8). The failures are benign, and just due to tests looking for too-specific values or expecting things like a certain ordering of hashmaps. \nThe tests can easily be made to pass both Java 8 and Java 6/7 at the same time by either relaxing the tests in a principled way, or accepting either output of two equally valid ones as correct.\n(There's also one curious compilation failure in Java 8, related to generics. It is fixable by changing to a more explicit declaration that should be equivalent. It should be entirely equivalent at compile time, and of course, at run time. I am not sure it's not just a javac bug, but, might as well work around when it's so easy.)",
        "Issue Links": []
    },
    "MAHOUT-1611": {
        "Key": "MAHOUT-1611",
        "Summary": "Preconditions.checkArgument in org.apache.mahout.utils.ConcatenateVectorsJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Haishou Ma",
        "Created": "28/Aug/14 07:22",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "09/Nov/14 17:50",
        "Description": "In org.apache.mahout.utils.ConcatenateVectorsJob line 111\nPreconditions.checkArgument(paths.length == 0, path.getName() + \" is a file, should be a directory\");\npaths.length == 0 should be paths.length > 0",
        "Issue Links": []
    },
    "MAHOUT-1612": {
        "Key": "MAHOUT-1612",
        "Summary": "NullPointerException happens during JSON output format for clusterdumper",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Guo Ruijing",
        "Created": "02/Sep/14 08:19",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "29/Mar/15 01:05",
        "Description": "1. download datafile from:\nhttp://archive.ics.uci.edu/ml/databases/synthetic_control/synthetic_control.data\n2. put data file on hdfs:\nhdfs dfs -mkdir testdata\nhdfs dfs -put synthetic_control.data testdata/\n3. run a mahout clustering job:\nmahout org.apache.mahout.clustering.syntheticcontrol.kmeans.Job\n4. run clusterdump with JSON format:\nmahout clusterdump i output/clusters*-final -p output/clusteredPoints -o /tmp/report -of JSON\nexpected:\nclusterdump with JSON format should succeeded same as CSV and TEXT\nactually:\nclusterdump with JSON format throw NullPointerException",
        "Issue Links": []
    },
    "MAHOUT-1613": {
        "Key": "MAHOUT-1613",
        "Summary": "classifier.df.tools.Describe does not handle -D parameters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Haohui Mai",
        "Created": "12/Sep/14 00:42",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "08/Aug/15 19:00",
        "Description": "classifier.df.tools.Describe does not handle -D parameters:\n\nhadoop jar mahout-examples-0.9.0.2.1.3.0-1887-job.jar org.apache.mahout.classifier.df.tools.Describe -Dio.sort.factor=30 --path /user/hdp/glass.data --file /user/hdp/glass2.info --descriptor I 9 N L\n\n\nOutput:\n\norg.apache.commons.cli2.OptionException: Unexpected -Dio.sort.factor while processing Options\n\n\nCredit to Dave Wannemacher reporting this issue.",
        "Issue Links": []
    },
    "MAHOUT-1614": {
        "Key": "MAHOUT-1614",
        "Summary": "Test failure and failure when converting from Sequnce Files due to Permissions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI,                                            Examples,                                            Math",
        "Assignee": null,
        "Reporter": "Kian Momtahan",
        "Created": "12/Sep/14 10:03",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "28/Mar/15 16:47",
        "Description": "I've noticed an issue with Mahout 0.9 when running on Windows 7.  Attempting to convert a sequence files to sparse vectors results in the error \"Failed to set permissions of path: .... mahout\\cote\\target\\mahout-SpareVectorsFromSequenceFilesTest...\\hadoop...\\mapred\\staging\\...\"\nInitially I encountered this issue in an eclipse project, but in attempting to build Mahout from source, the problem occurred while throughout many of the tests.\nFurther issues were noticed during ClusterClassification as a ThreadLeak occurs.",
        "Issue Links": []
    },
    "MAHOUT-1615": {
        "Key": "MAHOUT-1615",
        "Summary": "SparkEngine drmFromHDFS returning the same Key for all Key,Vec Pairs for Text-Keyed SequenceFiles",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "13/Sep/14 22:58",
        "Updated": "13/Apr/15 10:20",
        "Resolved": "10/Oct/14 21:41",
        "Description": "When reading in seq2sparse output from HDFS in the spark-shell of form <Text,VectorWriteable>  SparkEngine's drmFromHDFS method is creating rdds with the same Key for all Pairs:  \n\nmahout> val drmTFIDF= drmFromHDFS( path = \"/tmp/mahout-work-andy/20news-test-vectors/part-r-00000\")\n\n\nHas keys:\n{...} \n    key: /talk.religion.misc/84570\n    key: /talk.religion.misc/84570\n    key: /talk.religion.misc/84570\n{...}\n\nfor the entire set.  This is the last Key in the set.\nThe problem can be traced to the first line of drmFromHDFS(...) in SparkEngine.scala: \n\n val rdd = sc.sequenceFile(path, classOf[Writable], classOf[VectorWritable], minPartitions = parMin)\n        // Get rid of VectorWritable\n        .map(t => (t._1, t._2.get()))\n\n\nwhich gives the same key for all t._1.",
        "Issue Links": []
    },
    "MAHOUT-1616": {
        "Key": "MAHOUT-1616",
        "Summary": "Better support for hadoop dependencies of multiple versions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Gokhan Capan",
        "Reporter": "Gokhan Capan",
        "Created": "26/Sep/14 09:59",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "15/Nov/14 13:18",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1617": {
        "Key": "MAHOUT-1617",
        "Summary": "404 error on link in cluster-dumper tutorial page",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Anthony Daniell",
        "Created": "29/Sep/14 23:08",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "03/Apr/15 01:37",
        "Description": "On page:  https://mahout.apache.org/users/clustering/cluster-dumper.html\nThere is a link (\"Working With Maven in Eclipse\") contained in the text midway down which generates a 404 error.  The snippet in question is:\n\"Run the clusterdump utility as follows as a standalone Java Program through Eclipse - if you are using eclipse, setup mahout-utils as a project as specified in Working with Maven in Eclipse.\"\nError returned: The requested URL /users/developers/buildingmahout.html was not found on this server.",
        "Issue Links": []
    },
    "MAHOUT-1618": {
        "Key": "MAHOUT-1618",
        "Summary": "Cooccurrence Recommender example and documentation",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.1",
        "Component/s": "Examples",
        "Assignee": "Pat Ferrel",
        "Reporter": "Thejas Prasad",
        "Created": "06/Oct/14 00:39",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "06/Nov/15 01:58",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1619": {
        "Key": "MAHOUT-1619",
        "Summary": "HighDFWordsPruner overwrites cache files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Burke Webster",
        "Created": "07/Oct/14 16:12",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "31/Mar/15 03:01",
        "Description": "HighDFWordsPruner uses DistributedCache.setCacheFiles which will overwrite any files already in the cache.  Per the fix in MAHOUT-1498 we should be using addCacheFile, which will not overwrite existing cache files.",
        "Issue Links": []
    },
    "MAHOUT-1620": {
        "Key": "MAHOUT-1620",
        "Summary": "how to use mahout command  matrixmult ?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "zuozhibin",
        "Created": "08/Oct/14 04:31",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "31/Mar/15 21:38",
        "Description": "dear everyone :\nthis is my problem,firstly,i create two matrix files a and  b,\na = 1 3 2      b = 3 2\n       2 3 1            1 5\n       1 4 5            2 1\nthen i put them to filesystem. \ni use mahout command :mahout seqdirectory change a and b to seq file a1 and b1.\nthen,use mahout command :mahout matrixmult , a1*b1\nit works,but the anser is \nInput Path: hdfs://hacluster/user/zzb/c1/part-00000\nKey class: class org.apache.hadoop.io.IntWritable Value Class: class org.apache.mahout.math.VectorWritable\nCount: 0\nwhere am i wrong?",
        "Issue Links": []
    },
    "MAHOUT-1621": {
        "Key": "MAHOUT-1621",
        "Summary": "k-fold cross-validation in MapReduce Random Forest example?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": null,
        "Reporter": "Tawfiq Hasanin",
        "Created": "14/Oct/14 14:35",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "28/Mar/15 17:09",
        "Description": "My goal is to modify MapReduce Random Forest example by combining BuildForest.java and TestForest.java into a new class called RandomForest.java\nThe main point is to input one data file which is going to be used in training and testing; with k-fold cross-validation. \nI have a big data with hight diminutional features and small amount of instances. \nSeems to be a frustrating dead-end. is this process achievable? Or is it against MapReduce nature? \nThanks..",
        "Issue Links": []
    },
    "MAHOUT-1622": {
        "Key": "MAHOUT-1622",
        "Summary": "MultithreadedBatchItemSimilarities outputs incorrect number of similarities.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Anand Avati",
        "Reporter": "Jesse Daniels",
        "Created": "16/Oct/14 22:22",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "05/Apr/15 17:43",
        "Description": "In some cases the Output class in MultithreadedBatchItemSimilarities does not output all of the similarity pairs that it should. It is very possible for the number of active workers to go to zero while in the while loop, in which case the remaining similarities for the finished workers will not be flushed to the output. This is because the while loop is only conditioned on whether there are active workers or not. An easy fix is to also check to make sure the results structure is not empty. This way both the number of active workers must be 0 and the result set must be empty to exit the while loop.",
        "Issue Links": []
    },
    "MAHOUT-1623": {
        "Key": "MAHOUT-1623",
        "Summary": "MAHOUT.CMD contains duplicated code",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Srinivas Chamarthi",
        "Created": "20/Oct/14 01:57",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "28/Mar/15 17:22",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1624": {
        "Key": "MAHOUT-1624",
        "Summary": "Compilation errors when changing Lucene version to 4.10.1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Integration",
        "Assignee": null,
        "Reporter": "Tom Lampert",
        "Created": "30/Oct/14 13:59",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "28/Mar/15 17:21",
        "Description": "When changing Lucene version to 4_10_1 in all code and 4.10.1 in pom.xml the following compile errors (and warnings) are observed:\n[WARNING] COMPILATION WARNING : \n[INFO] -------------------------------------------------------------\n[WARNING] /mahout2/trunk/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java: Some input files use or override a deprecated API.\n[WARNING] /mahout2/trunk/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/hbase/HBaseDataModel.java: Recompile with -Xlint:deprecation for details.\n[WARNING] /mahout2/trunk/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.java: Some input files use unchecked or unsafe operations.\n[WARNING] /mahout2/trunk/integration/src/main/java/org/apache/mahout/cf/taste/impl/model/mongodb/MongoDBDataModel.java: Recompile with -Xlint:unchecked for details.\n[INFO] 4 warnings \n[INFO] -------------------------------------------------------------\n[INFO] -------------------------------------------------------------\n[ERROR] COMPILATION ERROR : \n[INFO] -------------------------------------------------------------\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[29,31] cannot find symbol\n  symbol:   class BufferedIndexOutput\n  location: package org.apache.lucene.store\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[305,47] cannot find symbol\n  symbol:   class BufferedIndexOutput\n  location: class org.apache.mahout.text.ReadOnlyFileSystemDirectory\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[163,12] incompatible types\n  required: org.apache.lucene.store.IndexOutput\n  found:    org.apache.mahout.text.ReadOnlyFileSystemDirectory.FileSystemIndexOutput\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[178,23] <anonymous org.apache.mahout.text.ReadOnlyFileSystemDirectory$1> is not abstract and does not override abstract method close() in org.apache.lucene.store.Lock\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[319,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[327,14] abstract method close() in org.apache.lucene.store.Directory cannot be accessed directly\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[324,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[335,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[340,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[345,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/LuceneSegmentRecordReader.java:[67,20] method scorer in class org.apache.lucene.search.Weight cannot be applied to given types;\n  required: org.apache.lucene.index.AtomicReaderContext,org.apache.lucene.util.Bits\n  found: org.apache.lucene.index.AtomicReaderContext,boolean,boolean,<nulltype>\n  reason: actual and formal argument lists differ in length\n[INFO] 11 errors \n[INFO] -------------------------------------------------------------\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Mahout Build Tools ................................ SUCCESS [  1.808 s]\n[INFO] Apache Mahout ..................................... SUCCESS [  0.437 s]\n[INFO] Mahout Math ....................................... SUCCESS [ 11.337 s]\n[INFO] Mahout MapReduce Legacy ........................... SUCCESS [ 15.163 s]\n[INFO] Mahout Integration ................................ FAILURE [  1.416 s]\n[INFO] Mahout Examples ................................... SKIPPED\n[INFO] Mahout Release Package ............................ SKIPPED\n[INFO] Mahout Math/Scala wrappers ........................ SKIPPED\n[INFO] Mahout Spark bindings ............................. SKIPPED\n[INFO] Mahout Spark bindings shell ....................... SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 30.612 s\n[INFO] Finished at: 2014-10-30T06:10:11-08:00\n[INFO] Final Memory: 52M/411M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:3.1:compile (default-compile) on project mahout-integration: Compilation failure: Compilation failure:\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[29,31] cannot find symbol\n[ERROR] symbol:   class BufferedIndexOutput\n[ERROR] location: package org.apache.lucene.store\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[305,47] cannot find symbol\n[ERROR] symbol:   class BufferedIndexOutput\n[ERROR] location: class org.apache.mahout.text.ReadOnlyFileSystemDirectory\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[163,12] incompatible types\n[ERROR] required: org.apache.lucene.store.IndexOutput\n[ERROR] found:    org.apache.mahout.text.ReadOnlyFileSystemDirectory.FileSystemIndexOutput\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[178,23] <anonymous org.apache.mahout.text.ReadOnlyFileSystemDirectory$1> is not abstract and does not override abstract method close() in org.apache.lucene.store.Lock\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[319,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[327,14] abstract method close() in org.apache.lucene.store.Directory cannot be accessed directly\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[324,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[335,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[340,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/ReadOnlyFileSystemDirectory.java:[345,5] method does not override or implement a method from a supertype\n[ERROR] /mahout2/trunk/integration/src/main/java/org/apache/mahout/text/LuceneSegmentRecordReader.java:[67,20] method scorer in class org.apache.lucene.search.Weight cannot be applied to given types;\n[ERROR] required: org.apache.lucene.index.AtomicReaderContext,org.apache.lucene.util.Bits\n[ERROR] found: org.apache.lucene.index.AtomicReaderContext,boolean,boolean,<nulltype>\n[ERROR] reason: actual and formal argument lists differ in length\n[ERROR] -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[ERROR] \n[ERROR] After correcting the problems, you can resume the build with the command\n[ERROR]   mvn <goals> -rf :mahout-integration",
        "Issue Links": []
    },
    "MAHOUT-1625": {
        "Key": "MAHOUT-1625",
        "Summary": "lucene2seq: failure to convert a document that does not contain a field (the field is not required)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.0",
        "Component/s": "CLI",
        "Assignee": "Frank Scholten",
        "Reporter": "Tom Lampert",
        "Created": "03/Nov/14 16:44",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "05/Aug/15 21:26",
        "Description": "When trying to convert a lucene index in which not all fields are required (and therefore in some documents the field does not exist) the following exception is thrown:\njava.lang.IllegalArgumentException: Field 'MISSING_FIELDNAME' does not exist in the index\n\tat org.apache.mahout.text.LuceneIndexHelper.fieldShouldExistInIndex(LuceneIndexHelper.java:36)\n\tat org.apache.mahout.text.LuceneSegmentRecordReader.initialize(LuceneSegmentRecordReader.java:63)\n\tat org.apache.mahout.text.LuceneSegmentInputFormat.createRecordReader(LuceneSegmentInputFormat.java:76)\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:492)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:735)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\nIt would be good to either ignore missing field values by default or to have an additional parameter that turns ignoring them on or off.",
        "Issue Links": []
    },
    "MAHOUT-1626": {
        "Key": "MAHOUT-1626",
        "Summary": "Support for required quasi-algebraic operations and starting with aggregating rows/blocks",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "Math",
        "Assignee": "Gokhan Capan",
        "Reporter": "Gokhan Capan",
        "Created": "15/Nov/14 13:11",
        "Updated": "19/Sep/16 07:46",
        "Resolved": "12/Sep/16 19:05",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/mahout/pull/62"
        ]
    },
    "MAHOUT-1627": {
        "Key": "MAHOUT-1627",
        "Summary": "Problem with ALS Factorizer MapReduce version when working with oozie because of files in distributed cache. Error: Unable to read sequence file from cache.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Srinivasarao Daruna",
        "Created": "18/Nov/14 12:15",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "30/Mar/16 20:27",
        "Description": "There is a problem with ALS Factorizer when working with distributed environment and oozie.\nSteps:\n1) Built mahout 1.0 jars and picked mahout-mrlegacy jar.\n2) I have created a Java class in which i have called ParallelALSFactorizationJob with respective inputs.\n3) Submitted the job and there are list of Map Reduce jobs which got submitted to perform the factorization.\n4) Job failed at MultithreadedSharingMapper with the error Unable to read Sequnce file \"<ourprogram>.jar\" pointing the code at org.apache.mahout.cf.taste.hadoop.als.ALS and readMatrixByRowsFromDistributedCache method.\nCause: The ALS class picks up input files which are sequential files from the distributed cache using readMatrixByRowsFromDistributedCache method. However, when we are working in oozie environment, the program jar as well being copied to distributed cache with input files. As the ALS class trying to read all the files in distributed cache, it is failing when it encounters jar. \nThe remedy would be setting a condition to pick files those are other than jars.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1634"
        ]
    },
    "MAHOUT-1628": {
        "Key": "MAHOUT-1628",
        "Summary": "Propagation of Updates in DF",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suminda Dharmasena",
        "Created": "22/Nov/14 05:02",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "09/Aug/15 02:43",
        "Description": "Given data frame :\nC = A + B\nIf some cells in A or B are updated:\n1) calculate C using the latest values in A and B when C is accessed (pull)\n2) when A or B is updated then propagate changes to C (push)\nYou can use different operator for = in the above cases",
        "Issue Links": []
    },
    "MAHOUT-1629": {
        "Key": "MAHOUT-1629",
        "Summary": "Mahout cvb on AWS EMR: p(topic|docId) doesn't make sense when using s3 folder as --input",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.12.0",
        "Component/s": "Clustering",
        "Assignee": "Andrew Musselman",
        "Reporter": "Markus Paaso",
        "Created": "24/Nov/14 07:28",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "17/Mar/16 16:02",
        "Description": "When running 'mahout cvb' command on AWS EMR having option --input with value like s3://mybucket/input/ or s3://mybucket/input/* (7 input files in my case) the content of doc-topic output is really non-sense. It seems like the docIds in doc-topic output are shuffled. But the topic model output (p(term|topic) for each topic) looks still fine.\nThe workaround is to first copy input files from s3 to cluster's hdfs with command:\n\nhadoop fs -cp s3://mybucket/input /input\n\nand then running mahout cvb with option --input /input .",
        "Issue Links": []
    },
    "MAHOUT-1630": {
        "Key": "MAHOUT-1630",
        "Summary": "Incorrect SparseColumnMatrix.numSlices() causes IndexException in toString()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Oleg Nitz",
        "Created": "25/Nov/14 15:52",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "08/Apr/15 05:23",
        "Description": "SparseColumnMatrix overrides the numSlices() method incorrectly: it returns numCols() instead of numRows(). \nAs a result, AbstractMaxtrix.toString() for wide matrices throws an exception.\nFor example, this code:\n\n    Matrix matrix = new SparseColumnMatrix(1, 2);\n    matrix.toString();\ncauses\n\norg.apache.mahout.math.IndexException: Index 1 is outside allowable range of [0,1)\n        at org.apache.mahout.math.MatrixVectorView.<init>(MatrixVectorView.java:42)\n        at org.apache.mahout.math.AbstractMatrix.viewRow(AbstractMatrix.java:290)\n        at org.apache.mahout.math.AbstractMatrix$1.computeNext(AbstractMatrix.java:68)\n        at org.apache.mahout.math.AbstractMatrix$1.computeNext(AbstractMatrix.java:59)\n        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)\n        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)\n        at org.apache.mahout.math.AbstractMatrix.toString(AbstractMatrix.java:787)",
        "Issue Links": []
    },
    "MAHOUT-1631": {
        "Key": "MAHOUT-1631",
        "Summary": "Streaming Series and DataFrames",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Suminda Dharmasena",
        "Created": "04/Dec/14 10:01",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "02/Apr/15 08:11",
        "Description": "For computation using live / streaming data, add thee functionality to have DF to which new data gets appended when them become available and can be used to process streaming data.",
        "Issue Links": []
    },
    "MAHOUT-1632": {
        "Key": "MAHOUT-1632",
        "Summary": "Please help me im stuck on using 20 newsgroups example on Windows",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Mishari SH",
        "Created": "09/Dec/14 19:15",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "27/Mar/15 06:50",
        "Description": "Hello there, I've been using hadoop & mahout on my windows OS and I started the hadoop cluster before starting the mahout in order to use the cluster for it, then, I did start the mahout to test the 20newsgroups example but it throws an exception as not a valid DFS filename as show below in details from the beginning :\nMicrosoft Windows [Version 6.1.7601]\nCopyright (c) 2009 Microsoft Corporation.  All rights reserved.\nC:\\Users\\Admin>cd\\\nC:\\>cd mahout\nC:\\mahout>cd examples\nC:\\mahout\\examples>cd bin\nC:\\mahout\\examples\\bin>classify-20newsgroups.sh\nWelcome to Git (version 1.9.4-preview20140815)\nRun 'git help git' to display the help index.\nRun 'git help <command>' to display help for specific commands.\nPlease select a number to choose the corresponding task to run\n1. cnaivebayes\n2. naivebayes\n3. sgd\n4. clean \u2013 cleans up the work area in /tmp/mahout-work-\nEnter your choice : 2\nok. You chose 2 and we'll use naivebayes\ncreating work directory at /tmp/mahout-work-\n+ echo 'Preparing 20newsgroups data'\nPreparing 20newsgroups data\n+ rm rf /tmp/mahout-work/20news-all\n+ mkdir /tmp/mahout-work-/20news-all\n+ cp R /tmp/mahout-work/20news-bydate/20news-bydate-test/alt.atheism /tmp/maho\nut-work-/20news-bydate/20news-bydate-test/comp.graphics /tmp/mahout-work-/20news\nbydate/20news-bydate-test/comp.os.ms-windows.misc /tmp/mahout-work/20news-byda\nte/20news-bydate-test/comp.sys.ibm.pc.hardware /tmp/mahout-work-/20news-bydate/2\n0news-bydate-test/comp.sys.mac.hardware /tmp/mahout-work-/20news-bydate/20news-b\nydate-test/comp.windows.x /tmp/mahout-work-/20news-bydate/20news-bydate-test/mis\nc.forsale /tmp/mahout-work-/20news-bydate/20news-bydate-test/rec.autos /tmp/maho\nut-work-/20news-bydate/20news-bydate-test/rec.motorcycles /tmp/mahout-work-/20ne\nws-bydate/20news-bydate-test/rec.sport.baseball /tmp/mahout-work-/20news-bydate/\n20news-bydate-test/rec.sport.hockey /tmp/mahout-work-/20news-bydate/20news-bydat\ne-test/sci.crypt /tmp/mahout-work-/20news-bydate/20news-bydate-test/sci.electron\nics /tmp/mahout-work-/20news-bydate/20news-bydate-test/sci.med /tmp/mahout-work-\n/20news-bydate/20news-bydate-test/sci.space /tmp/mahout-work-/20news-bydate/20ne\nws-bydate-test/soc.religion.christian /tmp/mahout-work-/20news-bydate/20news-byd\nate-test/talk.politics.guns /tmp/mahout-work-/20news-bydate/20news-bydate-test/t\nalk.politics.mideast /tmp/mahout-work-/20news-bydate/20news-bydate-test/talk.pol\nitics.misc /tmp/mahout-work-/20news-bydate/20news-bydate-test/talk.religion.misc\n /tmp/mahout-work-/20news-bydate/20news-bydate-train/alt.atheism /tmp/mahout-wor\nk-/20news-bydate/20news-bydate-train/comp.graphics /tmp/mahout-work-/20news-byda\nte/20news-bydate-train/comp.os.ms-windows.misc /tmp/mahout-work-/20news-bydate/2\n0news-bydate-train/comp.sys.ibm.pc.hardware /tmp/mahout-work-/20news-bydate/20ne\nws-bydate-train/comp.sys.mac.hardware /tmp/mahout-work-/20news-bydate/20news-byd\nate-train/comp.windows.x /tmp/mahout-work-/20news-bydate/20news-bydate-train/mis\nc.forsale /tmp/mahout-work-/20news-bydate/20news-bydate-train/rec.autos /tmp/mah\nout-work-/20news-bydate/20news-bydate-train/rec.motorcycles /tmp/mahout-work-/20\nnews-bydate/20news-bydate-train/rec.sport.baseball /tmp/mahout-work-/20news-byda\nte/20news-bydate-train/rec.sport.hockey /tmp/mahout-work-/20news-bydate/20news-b\nydate-train/sci.crypt /tmp/mahout-work-/20news-bydate/20news-bydate-train/sci.el\nectronics /tmp/mahout-work-/20news-bydate/20news-bydate-train/sci.med /tmp/mahou\nt-work-/20news-bydate/20news-bydate-train/sci.space /tmp/mahout-work-/20news-byd\nate/20news-bydate-train/soc.religion.christian /tmp/mahout-work-/20news-bydate/2\n0news-bydate-train/talk.politics.guns /tmp/mahout-work-/20news-bydate/20news-byd\nate-train/talk.politics.mideast /tmp/mahout-work-/20news-bydate/20news-bydate-tr\nain/talk.politics.misc /tmp/mahout-work-/20news-bydate/20news-bydate-train/talk.\nreligion.misc /tmp/mahout-work-/20news-all\n+ '[' 'C:\\hadp' '!=' '' ']'\n+ '[' '' == '' ']'\n+ echo 'Copying 20newsgroups data to HDFS'\nCopying 20newsgroups data to HDFS\n+ set +e\n+ 'C:\\hadp/bin/hadoop' dfs rmr /tmp/mahout-work/20news-all\n/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory\nDEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory\nrmr: DEPRECATED: Please use 'rm -r' instead.\nrmr: Pathname /C:/Users/Admin/AppData/Local/Temp/mahout-work/20news-all from h\ndfs://localhost:9000/C:/Users/Admin/AppData/Local/Temp/mahout-work-/20news-all i\ns not a valid DFS filename.\nUsage: hadoop fs [generic options] -rmr\n+ set -e\n+ 'C:\\hadp/bin/hadoop' dfs put /tmp/mahout-work/20news-all /tmp/mahout-work-/2\n0news-all\n/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory\nDEPRECATED: Use of this script to execute hdfs command is deprecated.\nInstead use the hdfs command for it.\n/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory\nput: Pathname /C:/Users/Admin/AppData/Local/Temp/mahout-work/20news-all from h\ndfs://localhost:9000/C:/Users/Admin/AppData/Local/Temp/mahout-work-/20news-all i\ns not a valid DFS filename.\nUsage: hadoop fs [generic options] -put [-f] [-p] <localsrc> ... <dst>\n+ echo 'Creating sequence files from 20newsgroups data'\nCreating sequence files from 20newsgroups data\n+ ./bin/mahout seqdirectory i /tmp/mahout-work/20news-all o /tmp/mahout-work\n/20news-seq -ow\n/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory\nRunning on hadoop, using \\hadp/bin/hadoop and HADOOP_CONF_DIR=\nMAHOUT-JOB: /c/mahout/examples/target/mahout-examples-0.9-job.jar\n/c/hadp/etc/hadoop/hadoop-env.sh: line 103: /c/hadp/bin: is a directory\n14/12/09 21:48:57 INFO common.AbstractJob: Command line arguments: {--charset=[U\nTF-8], --chunkSize=[64], --endPhase=[2147483647], --fileFilterClass=[org.apache.\nmahout.text.PrefixAdditionFilter], --input=[C:/Users/Admin/AppData/Local/Temp/ma\nhout-work-/20news-all], --keyPrefix=[], --method=[mapreduce], --output=[C:/Users\n/Admin/AppData/Local/Temp/mahout-work-/20news-seq], --overwrite=null, --startPha\nse=[0], --tempDir=[temp]}\nException in thread \"main\" java.lang.IllegalArgumentException: Pathname /C:/User\ns/Admin/AppData/Local/Temp/mahout-work-/20news-seq from C:/Users/Admin/AppData/L\nocal/Temp/mahout-work-/20news-seq is not a valid DFS filename.\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedF\nileSystem.java:187)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFi\nleSystem.java:101)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFil\neSystem.java:1068)\n        at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFil\neSystem.java:1064)\n        at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkRes\nolver.java:81)\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(Distribute\ndFileSystem.java:1064)\n        at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1398)\n        at org.apache.mahout.common.HadoopUtil.delete(HadoopUtil.java:192)\n        at org.apache.mahout.common.HadoopUtil.delete(HadoopUtil.java:200)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.run(SequenceFilesFr\nomDirectory.java:84)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n        at org.apache.mahout.text.SequenceFilesFromDirectory.main(SequenceFilesF\nromDirectory.java:65)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.\njava:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(Progra\nmDriver.java:72)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:145)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:153)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.\njava:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces\nsorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nC:\\mahout\\examples\\bin>\nPlease help me I'm new to the big data tools and I need this issue resolved as soon as possible.\nThank you,,,",
        "Issue Links": []
    },
    "MAHOUT-1633": {
        "Key": "MAHOUT-1633",
        "Summary": "Failure to execute query when solr index contains documents with different fields",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "0.11.0",
        "Component/s": "CLI",
        "Assignee": "Frank Scholten",
        "Reporter": "Tom Lampert",
        "Created": "17/Dec/14 19:00",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "05/Aug/15 21:27",
        "Description": "When using Lucene2Seq on a lucene Index that contains documents that have different fields the following error is output:\njava.lang.IllegalArgumentException: Could not create query scorer for query: tableName:code\n\tat org.apache.mahout.text.LuceneSegmentRecordReader.initialize(LuceneSegmentRecordReader.java:69)\n\tat org.apache.mahout.text.LuceneSegmentInputFormat.createRecordReader(LuceneSegmentInputFormat.java:76)\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:492)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:735)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)\nThe query that is used executes fine in Solr upon the same index. If the index does not contain documents having different fields (from the same source) the function executes without a problem.",
        "Issue Links": []
    },
    "MAHOUT-1634": {
        "Key": "MAHOUT-1634",
        "Summary": "ALS don't work when it adds new files in Distributed Cache",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.12.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Cristian Gal\u00e1n",
        "Created": "18/Dec/14 12:42",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "30/Mar/16 20:34",
        "Description": "ALS algorithm uses distributed cache to temp files, but the distributed cache have other uses too, especially to add dependencies\n(http://blog.cloudera.com/blog/2011/01/how-to-include-third-party-libraries-in-your-map-reduce-job/), so when in a hadoop's job we add a dependency library (or other file) ALS fails because it reads ALL files in Distribution Cache without distinction.\nThis occurs in the project of my company because we need to add Mahout dependencies (mahout, lucene,...) in an hadoop Configuration to run Mahout's jobs, otherwise the Mahout's job fails because it don't find the dependencies.\nI propose two options (I think two valid options):\n1) Eliminate all .jar in the return of HadoopUtil.getCacheFiles\n2) Elliminate all Path object distinct of /part-*\nI prefer the first because it's less aggressive, and I think this solution will be resolve all problems.\nPd: Sorry if my english is wrong.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1627",
            "/jira/browse/MAHOUT-1695"
        ]
    },
    "MAHOUT-1635": {
        "Key": "MAHOUT-1635",
        "Summary": "Getting an exception when I provide classification labels manually for Naive Bayes",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Classification",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Suman Somasundar",
        "Created": "18/Dec/14 22:55",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "04/Apr/15 23:01",
        "Description": "If I let the Naive Bayes program itself extract the classification labels, the program runs fine. But, I get the following error when I provide the classification labels for the dataset manually.\nError: java.lang.IllegalArgumentException: Wrong numLabels: 0. Must be > 0!\n        at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)\n        at org.apache.mahout.classifier.naivebayes.training.WeightsMapper.setup(WeightsMapper.java:45)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:169)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1640)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)",
        "Issue Links": []
    },
    "MAHOUT-1636": {
        "Key": "MAHOUT-1636",
        "Summary": "Class dependencies for the spark module are put in a job.jar, which is very inefficient",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "spark",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "23/Dec/14 16:53",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "27/Mar/15 13:34",
        "Description": "using a maven plugin and an assembly job.xml a job.jar is created with all dependencies including transitive ones. This job.jar is in mahout/spark/target and is included in the classpath when a Spark job is run. This allows dependency classes to be found at runtime but the job.jar include a great deal of things not needed that are duplicates of classes found in the main mrlegacy job.jar.  If the job.jar is removed, drivers will not find needed classes. A better way needs to be implemented for including class dependencies.\nI'm not sure what that better way is so am leaving the assembly alone for now. Whoever picks up this Jira will have to remove it after deciding on a better method.",
        "Issue Links": []
    },
    "MAHOUT-1637": {
        "Key": "MAHOUT-1637",
        "Summary": "RecommenderJob of ALS fails in the mapper because it uses the instance of other class",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.12.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Andrew Musselman",
        "Reporter": "Cristian Gal\u00e1n",
        "Created": "26/Dec/14 08:18",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "10/Apr/16 06:06",
        "Description": "In the map method of PredictionMapper when executes the next line:       Pair<OpenIntObjectHashMap<Vector>, OpenIntObjectHashMap<Vector>> uAndM = getSharedInstance();\nthe job fail because obtains the instance of other class. This occurs because I launch a local job, so the instance exists previously and for this doesn't make the new correct instance for ALS.\nThe solution that it works me is to add the next line:.\n        SharingMapper.reset();\nin method run of JobRecommender of org.apache.mahout.cf.taste.hadoop.als package \nI have to test it in my environment with distributed mapreduce, hadoop fs, zookeeper and others if it works correctly.",
        "Issue Links": []
    },
    "MAHOUT-1638": {
        "Key": "MAHOUT-1638",
        "Summary": "H2O bindings fail at drmParallelizeWithRowLabels(...)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "27/Jan/15 00:22",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "03/Apr/15 17:03",
        "Description": "The H2OHelper.drmFromMatrix(...) function fails when trying to write row label String keys to a water.fvec.Vec.:\n\n java.lang.IllegalArgumentException: Not a String\n  at water.fvec.Chunk.set_impl(Chunk.java:507)\n  at water.fvec.Chunk.set0(Chunk.java:469)\n  at water.fvec.Chunk.set(Chunk.java:371)\n  at water.fvec.Vec$Writer.set(Vec.java:803)\n  at org.apache.mahout.h2obindings.H2OHelper.drmFromMatrix(H2OHelper.java:331)\n  at org.apache.mahout.h2obindings.H2OEngine$.drmParallelizeWithRowLabels(H2OEngine.scala:83)                                                                   \n  at org.apache.mahout.math.drm.package$.drmParallelizeWithRowLabels(package.scala:67)\n\n\nThis causes an exception when calling drm.drmParallelizeWithRowLabels(...)\nTo reproduce, apply PR#72: Enable Naive Bayes Tests in h2o Module and run:\n\n $ mvn test \n\n\nfrom the h2o module:\n\n- NB Aggregator *** FAILED ***\n  java.lang.IllegalArgumentException: Not a String\n  at water.fvec.Chunk.set_impl(Chunk.java:507)\n  at water.fvec.Chunk.set0(Chunk.java:469)\n  at water.fvec.Chunk.set(Chunk.java:371)\n  at water.fvec.Vec$Writer.set(Vec.java:803)\n  at org.apache.mahout.h2obindings.H2OHelper.drmFromMatrix(H2OHelper.java:331)\n  at org.apache.mahout.h2obindings.H2OEngine$.drmParallelizeWithRowLabels(H2OEngine.scala:83)                                                                   \n  at org.apache.mahout.math.drm.package$.drmParallelizeWithRowLabels(package.scala:67)                                                                          \n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply$mcV$sp(NBTestBase.scala:91)                                                            \n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply(NBTestBase.scala:70)                                                                   \n  at org.apache.mahout.classifier.naivebayes.NBTestBase$$anonfun$2.apply(NBTestBase.scala:70)                                                                   \n  ...",
        "Issue Links": []
    },
    "MAHOUT-1639": {
        "Key": "MAHOUT-1639",
        "Summary": "streamingkmeans doesn't properly validate estimatedNumMapClusters -km",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "CLI",
        "Assignee": "Suneel Marthi",
        "Reporter": "Peter Sergeant",
        "Created": "27/Jan/15 02:41",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "25/Mar/15 03:50",
        "Description": "The value of -km isn't checked by the CLI, which means if you don't specify it, you get the rather cryptic:\n\nException in thread \"main\" java.lang.NumberFormatException: null\n\tat java.lang.Integer.parseInt(Integer.java:454)\n\tat java.lang.Integer.parseInt(Integer.java:527)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.configureOptionsForWorkers(StreamingKMeansDriver.java:252)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.run(StreamingKMeansDriver.java:239)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)\n\tat org.apache.mahout.clustering.streaming.mapreduce.StreamingKMeansDriver.main(StreamingKMeansDriver.java:491)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:68)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:139)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\n\nOther parameters give helpful error messages when required",
        "Issue Links": []
    },
    "MAHOUT-1640": {
        "Key": "MAHOUT-1640",
        "Summary": "Better collections would significantly improve vector-operation speed",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.2",
        "Component/s": "collections",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sebastiano Vigna",
        "Created": "05/Feb/15 09:13",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "08/Mar/16 20:57",
        "Description": "The collections currently used by Mahout to implement sparse vectors are extremely slow. The proposed patch (localized to RandomAccessSparseVector) uses fastutil's maps and the speed improvements in vector benchmarks are very significant. It would be interesting to see whether these improvements percolate to high-level classes using sparse vectors.\nI had to patch two unit tests (an off-by-one bug and an overfitting bug; both were exposed by the different order in which key/values were returned by iterators).\nThe included files speed-std and speed-fastutil show the speed improvement. Some more speed might be gained by using everywhere the standard java.util.Map.Entry interface instead of Element.\nDISCLAIMER: The \"Times\" set of tests has been run multiplying two identical vectors. The standard tests multiply two random vectors, so in fact they just test the speed of the underlying map remove() method, as almost all products are zero. This is not very realistic and was heavily penalizing fastutil's \"true deletions\". Better tests, with a typical overlap of nonzero entries, would be even more realistic.",
        "Issue Links": []
    },
    "MAHOUT-1641": {
        "Key": "MAHOUT-1641",
        "Summary": "Add conversion from a RDD[(String, String)] to a Drm[Int]",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.1,                                            0.11.0",
        "Component/s": "spark",
        "Assignee": "Pat Ferrel",
        "Reporter": "Erlend Hamnaberg",
        "Created": "13/Feb/15 23:00",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "03/Jun/15 00:21",
        "Description": "Hi.\nWe are using the coocurrence part of mahout as a library. We get our data from other sources, like for instance Cassandra. We dont want to write that data to disk, and read it back since we already have the data on each slave.\nI have created some conversion functions based on one of the IndexedDatasetSpark readers, cant remember which one at the moment.\nIs there interest in the community for this kind of feature? I can probably clean it up and add this as a github pull request.",
        "Issue Links": []
    },
    "MAHOUT-1642": {
        "Key": "MAHOUT-1642",
        "Summary": "Iterator class within SimilarItems class always misses the first element",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Guohua Hao",
        "Created": "19/Feb/15 17:48",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "08/Aug/15 18:43",
        "Description": "In the next() function of SimilarItemsIterator class within SimilarItems class, variable 'index' is incremented before returning the actual element at that position, therefore the first element when iterating will always be missed.",
        "Issue Links": []
    },
    "MAHOUT-1643": {
        "Key": "MAHOUT-1643",
        "Summary": "CLI arguments are not being processed in spark-shell",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.0",
        "Component/s": "CLI,                                            spark",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "05/Mar/15 23:33",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "18/Jun/15 19:44",
        "Description": "The CLI arguments are not being processed in spark-shell.  Most importantly the spark options are not being passed to the spark configuration via:\n\n$ mahout spark-shell -D:k=n\n\n\nThe arguments are preserved it through \n\n$ bin/mahout\n\nThere should be a relatively easy fix either by using the MahoutOptionParser, Scopt or by simply parsing the args array.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1714"
        ]
    },
    "MAHOUT-1644": {
        "Key": "MAHOUT-1644",
        "Summary": "[JDK8] errors when compile the module math-scala with JDK8 and maven dependency org.scala-lang:scala-library",
        "Type": "Dependency upgrade",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "zhubin",
        "Created": "10/Mar/15 03:44",
        "Updated": "13/Apr/15 10:21",
        "Resolved": "18/Mar/15 00:56",
        "Description": "[INFO] Compiling 12 source files to /root/bigtop/dl/mahout-distribution-0.9/math-scala/target/classes at 1425956054808\n[ERROR] error: error while loading ConcurrentMap, class file '/usr/java/jdk1.8.0_40/jre/lib/rt.jar(java/util/concurrent/ConcurrentMap.class)' is broken\n[INFO] (bad constant pool tag 15 at byte 2448)\n[ERROR] error: error while loading CharSequence, class file '/usr/java/jdk1.8.0_40/jre/lib/rt.jar(java/lang/CharSequence.class)' is broken\n[INFO] (bad constant pool tag 15 at byte 1501)\n[ERROR] error: error while loading Comparator, class file '/usr/java/jdk1.8.0_40/jre/lib/rt.jar(java/util/Comparator.class)' is broken\n[INFO] (bad constant pool tag 15 at byte 5003)\n[ERROR] three errors found\nThe default version of org.scala-lang:scala-library in module math-scala is 2.9.3, and in order to compile with JDK8, it needs to upgrade the scala version to 2.10.3+",
        "Issue Links": []
    },
    "MAHOUT-1645": {
        "Key": "MAHOUT-1645",
        "Summary": "mahout 1.0 snapshot not compatible with hadoop 2.2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lin Duan",
        "Created": "10/Mar/15 10:09",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "31/Mar/15 22:17",
        "Description": "Exception in thread \"main\" java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected\n        at org.apache.mahout.common.HadoopUtil.getCustomJobName(HadoopUtil.java:174)\n        at org.apache.mahout.common.AbstractJob.prepareJob(AbstractJob.java:614)",
        "Issue Links": []
    },
    "MAHOUT-1646": {
        "Key": "MAHOUT-1646",
        "Summary": "Refactor out all possible mrlegacy dependencies from Scala code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Pat Ferrel",
        "Created": "18/Mar/15 14:02",
        "Updated": "13/Apr/15 10:19",
        "Resolved": "05/Apr/15 15:52",
        "Description": "Scala/Spark code depends on the mrlegacy module even though very few things are really used. move those needed pieces to math so as to remove this dependency.",
        "Issue Links": []
    },
    "MAHOUT-1647": {
        "Key": "MAHOUT-1647",
        "Summary": "The release build is incomplete",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Pat Ferrel",
        "Created": "18/Mar/15 14:04",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "04/Apr/15 04:27",
        "Description": "The scala, spark, and h2o modules are not build and teh scaladocs are not built. None of these are released and artifacts or published.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1585",
            "/jira/browse/MAHOUT-1562"
        ]
    },
    "MAHOUT-1648": {
        "Key": "MAHOUT-1648",
        "Summary": "Update Mahout's CMS for 0.10.0",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Done",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Pat Ferrel",
        "Created": "18/Mar/15 15:03",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "12/Apr/15 14:44",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1670"
        ]
    },
    "MAHOUT-1649": {
        "Key": "MAHOUT-1649",
        "Summary": "Upgrade to Lucene 4.10.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "19/Mar/15 07:54",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "07/Apr/15 02:56",
        "Description": "Upgrade lucene2seq utility and seq2sparse to Lucene 4.10.x",
        "Issue Links": [
            "/jira/browse/MAHOUT-1650"
        ]
    },
    "MAHOUT-1650": {
        "Key": "MAHOUT-1650",
        "Summary": "Upgrade all 3rd party jars to the most recent versions and fix code to be Lucene 5.0 compatible",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "CLI",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Suneel Marthi",
        "Created": "20/Mar/15 03:17",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 17:10",
        "Description": "The title pretty much is self-descriptive, otherwise modify the code using Lucene to account for the recent changes in Lucene",
        "Issue Links": [
            "/jira/browse/MAHOUT-1649"
        ]
    },
    "MAHOUT-1651": {
        "Key": "MAHOUT-1651",
        "Summary": "[JDK8] maven test failed for Mahout 0.9 with JDK 8, and JDK 7 can pass",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "zhubin",
        "Created": "20/Mar/15 06:53",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "20/Mar/15 14:35",
        "Description": "Running org.apache.mahout.math.random.MultinomialTest\nTests run: 8, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.371 sec <<< FAILURE! - in org.apache.mahout.math.random.MultinomialTest\ntestPrime(org.apache.mahout.math.random.MultinomialTest)  Time elapsed: 0.039 sec  <<< FAILURE!\njava.lang.AssertionError: expected:<16> but was:<17>\n        at __randomizedtesting.SeedInfo.seed([AAD9BA068467B3DA:9D6D3992EDADE21A]:0)\n        at org.junit.Assert.fail(Assert.java:88)\n        at org.junit.Assert.failNotEquals(Assert.java:743)\n        at org.junit.Assert.assertEquals(Assert.java:118)\n        at org.junit.Assert.assertEquals(Assert.java:555)\n        at org.junit.Assert.assertEquals(Assert.java:542)\n        at org.apache.mahout.math.random.MultinomialTest.testPrime(MultinomialTest.java:143)",
        "Issue Links": []
    },
    "MAHOUT-1652": {
        "Key": "MAHOUT-1652",
        "Summary": "Java 7 update",
        "Type": "Dependency upgrade",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Musselman",
        "Created": "24/Mar/15 17:12",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "06/Apr/15 02:58",
        "Description": "Support Java 7",
        "Issue Links": []
    },
    "MAHOUT-1653": {
        "Key": "MAHOUT-1653",
        "Summary": "Spark 1.3",
        "Type": "Dependency upgrade",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Musselman",
        "Created": "24/Mar/15 17:16",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "10/Jul/15 19:33",
        "Description": "Support Spark 1.3",
        "Issue Links": [
            "/jira/browse/MAHOUT-1685"
        ]
    },
    "MAHOUT-1654": {
        "Key": "MAHOUT-1654",
        "Summary": "Migrate from Maven to SBT",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "build",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "24/Mar/15 17:53",
        "Updated": "12/Jul/17 07:37",
        "Resolved": "12/Mar/16 23:11",
        "Description": "Mahout modules which are Scala libraries like mahout-math-scala, mahout-spark/mahout-spark-shell, should be published across Scala binary versions to be usable to wider audience. At the moment this is not possible with Maven. We need to switch to another build tool which supports this, and SBT is most natural choice. Besides allowing us to publish Mahout Scala libraries across Scala binary versions, it is expected that this migration will help us mitigate/resolve other issues (to name a few, issue of publishing javadoc/scaladoc documentation, and long standing issue of migration to modern CLI library with sources).\nAs acceptance criteria of migration success it should be defined that both project committers and users see only improvements/benefits, everything else that was possible and available with existing Maven build should be possible with SBT.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1562",
            "/jira/browse/MAHOUT-1563",
            "https://github.com/apache/mahout/pull/331"
        ]
    },
    "MAHOUT-1655": {
        "Key": "MAHOUT-1655",
        "Summary": "Refactor module dependencies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "mrlegacy",
        "Assignee": "Andrew Musselman",
        "Reporter": "Pat Ferrel",
        "Created": "24/Mar/15 20:25",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "05/Apr/15 15:56",
        "Description": "Make a new module, call it mahout-hadoop. Move anything there that is currently in mrlegacy but used in math-scala or spark. Remove dependencies on mrlegacy altogether if possible by using other core classes.\nThe goal is to have math-scala and spark module depend on math, and a small module called mahout-hadoop (much smaller than mrlegacy).",
        "Issue Links": [
            "/jira/browse/MAHOUT-1667"
        ]
    },
    "MAHOUT-1656": {
        "Key": "MAHOUT-1656",
        "Summary": "Change SNAPSHOT version from 1.0 to 0.10.0",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Musselman",
        "Created": "26/Mar/15 00:13",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "04/Apr/15 03:58",
        "Description": "Poms still reference 1.0-SNAPSHOT but now should reference 0.10.0-SNAPSHOT. This needs to be done as a precursor for 0.10.0 release.",
        "Issue Links": []
    },
    "MAHOUT-1657": {
        "Key": "MAHOUT-1657",
        "Summary": "Examples broken due to Guava mismatch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "26/Mar/15 00:51",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "26/Mar/15 01:52",
        "Description": "Running cluster-reuters.sh with option 1 and cluster-20newsgroups.sh with option 2, for example, result in this error:\n$ ./examples/bin/cluster-reuters.sh \nPlease select a number to choose the corresponding clustering algorithm\n1. kmeans clustering\n2. fuzzykmeans clustering\n3. lda clustering\n4. streamingkmeans clustering\nEnter your choice : 1\nok. You chose 1 and we'll use kmeans Clustering\ncreating work directory at /tmp/mahout-work-akm\nMAHOUT_LOCAL is set, so we don't add HADOOP_CONF_DIR to classpath.\nMAHOUT_LOCAL is set, running locally\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/akm/mahout/examples/target/mahout-examples-1.0-SNAPSHOT-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/akm/mahout/examples/target/dependency/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nlog4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell).\nlog4j:WARN Please initialize the log4j system properly.\nlog4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\nException in thread \"main\" java.lang.NoSuchMethodError: com.google.common.base.Stopwatch.elapsedMillis()J\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:279)\n\tat org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:59)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:385)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:597)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:614)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:492)\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1296)\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1293)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1293)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1314)\n\tat org.apache.mahout.vectorizer.DocumentProcessor.tokenizeDocuments(DocumentProcessor.java:93)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.run(SparseVectorsFromSequenceFiles.java:257)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:84)\n\tat org.apache.mahout.vectorizer.SparseVectorsFromSequenceFiles.main(SparseVectorsFromSequenceFiles.java:56)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:601)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1590"
        ]
    },
    "MAHOUT-1658": {
        "Key": "MAHOUT-1658",
        "Summary": "Kmeans fails when running on HDFS",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.0",
        "Component/s": "Clustering",
        "Assignee": "Andrew Musselman",
        "Reporter": "Ha Son Hai",
        "Created": "26/Mar/15 14:54",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "11/Aug/15 23:48",
        "Description": "Hi,\nI was trying to run some examples of mahout on a hadoop platform and saw that when kmeans running in local host, it returned successfully. However, when it ran with HDFS, mahout looked for the intermediate results on localhost instead on HDFS if we use relative path.\nI have to use absolute path of the input and output if I want kmeans to run correctly.\nHere is an typical error when running on HDFS:\n15/03/26 12:15:07 INFO mapreduce.Job: Task Id : attempt_1426848955524_0062_m_000000_2, Status : FAILED\nError: java.lang.IllegalStateException: output/clusters-0\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:78)\n        at org.apache.mahout.clustering.classify.ClusterClassifier.readFromSeqFiles(ClusterClassifier.java:208)\n        at org.apache.mahout.clustering.iterator.CIMapper.setup(CIMapper.java:44)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n       at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n       at java.security.AccessController.doPrivileged(Native Method)\n       at javax.security.auth.Subject.doAs(Subject.java:415)\n       at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)\n       at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nCaused by: java.io.FileNotFoundException: File output/clusters-0 does not exist\n        at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:376)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1485)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1525)\n        at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:570)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1485)\n        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1525)\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterator.<init>(SequenceFileDirValueIterator.java:70)\n        at org.apache.mahout.common.iterator.sequencefile.SequenceFileDirValueIterable.iterator(SequenceFileDirValueIterable.java:76)\n        ... 10 more\n15/03/26 12:15:16 INFO mapreduce.Job:  map 100% reduce 0%\n15/03/26 12:15:17 INFO mapreduce.Job:  map 100% reduce 100%\n15/03/26 12:15:17 INFO mapreduce.Job: Job job_1426848955524_0062 failed with state FAILED due to: Task failed task_1426848955524_0062_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n15/03/26 12:15:17 INFO mapreduce.Job: Counters: 9\n        Job Counters\n                Failed map tasks=4\n                Launched map tasks=4\n                Other local map tasks=3\n                Rack-local map tasks=1\n                Total time spent by all maps in occupied slots (ms)=23087\n                Total time spent by all reduces in occupied slots (ms)=0\n                Total time spent by all map tasks (ms)=23087\n                Total vcore-seconds taken by all map tasks=23087\n                Total megabyte-seconds taken by all map tasks=23641088\nException in thread \"main\" java.lang.InterruptedException: Cluster Iteration 1 failed processing output/clusters-1\n        at org.apache.mahout.clustering.iterator.ClusterIterator.iterateMR(ClusterIterator.java:183)\n        at org.apache.mahout.clustering.kmeans.KMeansDriver.buildClusters(KMeansDriver.java:224)\n        at org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:147)\n        at org.apache.mahout.clustering.syntheticcontrol.kmeans.Job.run(Job.java:135)\n        at org.apache.mahout.clustering.syntheticcontrol.kmeans.Job.main(Job.java:60)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1665"
        ]
    },
    "MAHOUT-1659": {
        "Key": "MAHOUT-1659",
        "Summary": "Remove deprecated Lanczos solver from spectral clustering in mr-legacy",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering,                                            mrlegacy",
        "Assignee": "Shannon Quinn",
        "Reporter": "Shannon Quinn",
        "Created": "27/Mar/15 14:26",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "30/Mar/15 06:29",
        "Description": "Spectral clustering still has the option of using either SSVD or the Lanczos solver for dimensionality reduction. Remove the latter entirely.",
        "Issue Links": []
    },
    "MAHOUT-1660": {
        "Key": "MAHOUT-1660",
        "Summary": "Hadoop1HDFSUtil.readDRMHEader should be taking Hadoop conf",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.2",
        "Component/s": "spark",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Suneel Marthi",
        "Created": "28/Mar/15 02:48",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:01",
        "Description": "Hadoop1HDFSUtil.readDRMHEader should be taking Hadoop configuration from Context and not ignore it",
        "Issue Links": [
            "/jira/browse/MAHOUT-1733"
        ]
    },
    "MAHOUT-1661": {
        "Key": "MAHOUT-1661",
        "Summary": "Deprecate Lanczos in the code base",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Clustering",
        "Assignee": "Shannon Quinn",
        "Reporter": "Suneel Marthi",
        "Created": "29/Mar/15 19:34",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "03/Apr/15 22:50",
        "Description": "Lanczos has long been deprecated from the code base but the code doesn't reflect that.",
        "Issue Links": []
    },
    "MAHOUT-1662": {
        "Key": "MAHOUT-1662",
        "Summary": "Potential Path bug in SequenceFileVaultIterator breaks DisplaySpectralKMeans",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples,                                            mrlegacy",
        "Assignee": "Suneel Marthi",
        "Reporter": "Shannon Quinn",
        "Created": "30/Mar/15 16:07",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "04/Apr/15 04:57",
        "Description": "Received the following error when attempting to run DisplaySpectralKMeans:\nException in thread \"main\" java.lang.IllegalArgumentException: Wrong FS: file://tmp/calculations/diagonal/part-r-00000/tmp/calculations/diagonal/part-r-00000, expected: file:///\n\tat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:529)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)\n\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)\n\tat org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1750)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1774)\n\tat org.apache.mahout.common.iterator.sequencefile.SequenceFileValueIterator.<init>(SequenceFileValueIterator.java:56)\n\tat org.apache.mahout.clustering.spectral.VectorCache.load(VectorCache.java:115)\n\tat org.apache.mahout.clustering.spectral.MatrixDiagonalizeJob.runJob(MatrixDiagonalizeJob.java:77)\n\tat org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:170)\n\tat org.apache.mahout.clustering.spectral.kmeans.SpectralKMeansDriver.run(SpectralKMeansDriver.java:117)\n\tat org.apache.mahout.clustering.display.DisplaySpectralKMeans.main(DisplaySpectralKMeans.java:76)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:140)\nTracked the origin of the bug to line 54 of SequenceFileVaultIterator. PR which contains a fix is available; I would ask for independent verification before merging it with master.",
        "Issue Links": []
    },
    "MAHOUT-1663": {
        "Key": "MAHOUT-1663",
        "Summary": "Port seq2sparse to the Mahout spark-scala Environment",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Gokhan Capan",
        "Reporter": "Andrew Palumbo",
        "Created": "31/Mar/15 18:04",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 18:11",
        "Description": "Implement a scala version of seq2sparse in the spark module.  This effort is currently in progress.",
        "Issue Links": []
    },
    "MAHOUT-1664": {
        "Key": "MAHOUT-1664",
        "Summary": "Port seqdirectory to the Mahout spark Environment",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "31/Mar/15 18:07",
        "Updated": "19/Dec/16 06:36",
        "Resolved": "19/Dec/16 06:36",
        "Description": "Implement a scala version of seqdirectory in the spark module",
        "Issue Links": []
    },
    "MAHOUT-1665": {
        "Key": "MAHOUT-1665",
        "Summary": "Update hadoop commands in example scripts",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Examples",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "31/Mar/15 22:06",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "08/Apr/15 00:01",
        "Description": "Hadoop 2.x does not like the `hadoop fs` command; change that to `hadoop dfs` to retain backwards compat with v1.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1658"
        ]
    },
    "MAHOUT-1666": {
        "Key": "MAHOUT-1666",
        "Summary": "Broken Links on Website",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Tyler Bui-Palsulich",
        "Created": "01/Apr/15 16:12",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "01/Apr/15 17:19",
        "Description": "There are several broken links on the basics/quickstart page of the website:\n\nRecommender Quickstart\nSynthetic Data\n20 Newsgroups\nHMM Example & Random Forest",
        "Issue Links": []
    },
    "MAHOUT-1667": {
        "Key": "MAHOUT-1667",
        "Summary": "Support Hadoop 1.2.1 in poms",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "build",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Pat Ferrel",
        "Created": "01/Apr/15 18:32",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "04/Apr/15 16:51",
        "Description": "Need to support build for Hadoop 1.2.1 with the hadoop1 profile in poms. Errors for non-existent artifacts appear when running: \"mvn -Phadoop1 -Dhadoop.version=1.2.1 clean install\" with hadoop-auth, which does not exist for hadoop 1.2.1, along with hadoop-yarn and several other artifacts.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1655",
            "/jira/browse/MAHOUT-1673"
        ]
    },
    "MAHOUT-1668": {
        "Key": "MAHOUT-1668",
        "Summary": "Automate release process",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9,                                            0.10.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "01/Apr/15 22:57",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "17/Mar/16 17:57",
        "Description": "0.10.0 will be first release since project switched to git. Some changes have to be made in build scripts to support the release process, the Apache way. As consequence, how-to-make-release docs will likely need to be updated as well. Also, it would be nice to automate release process as much as possible, e.g. via dedicated Jenkins build job(s), so it's easy for any committer to cut out a release for vote, and after vote either finalize release or easily make a new RC - this will enable us to release faster and more often.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1443"
        ]
    },
    "MAHOUT-1669": {
        "Key": "MAHOUT-1669",
        "Summary": "Mahout nightly build lasts too long",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.2",
        "Component/s": "None",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "02/Apr/15 00:19",
        "Updated": "15/Mar/16 03:12",
        "Resolved": "15/Mar/16 03:12",
        "Description": "Mahout nightly Jenkins build job can last longer than 60min and then it gets automatically aborted by Jenkins.\nHere is example from a recently failed build (#1881):\n\nBuild timed out (after 60 minutes). Marking the build as aborted.\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Mahout Build Tools ................................ SUCCESS [14.261s]\n[INFO] Apache Mahout ..................................... SUCCESS [5.315s]\n[INFO] Mahout Math ....................................... SUCCESS [2:36.064s]\n[INFO] Mahout HDFS ....................................... SUCCESS [18.617s]\n[INFO] Mahout Map-Reduce ................................. SUCCESS [21:15.011s]\n[INFO] Mahout Integration ................................ SUCCESS [1:23.923s]\n[INFO] Mahout Examples ................................... SUCCESS [5:15.152s]\n[INFO] Mahout Release Package ............................ SUCCESS [9:43.293s]\n[INFO] Mahout Math Scala bindings ........................ SUCCESS [2:58.073s]\n[INFO] Mahout Spark bindings ............................. SUCCESS [5:09.176s]\n[INFO] Mahout Spark bindings shell ....................... SUCCESS [21.001s]\n[INFO] Mahout H2O backend ................................ FAILURE [7:29.662s]\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 57:07.897s\n[INFO] Finished at: Thu Apr 02 00:00:25 UTC 2015\n[INFO] Final Memory: 71M/772M\n[INFO] ------------------------------------------------------------------------\nchannel stopped\nBuild was aborted\nFinished: ABORTED\n\n\nWe should investigate and try to bring the build time down.\nSome initial thoughts:\n\nMR module build time dominates (1/3 of build time); we should check what is done there, is it necessary and can something (e.g. tests) be parallelized\nThere are few large artifacts produced and deployed/published by this build job; we should check if all of them are really necessary, and for necessary ones if we can reduce the artifact size to make publishing and build faster",
        "Issue Links": []
    },
    "MAHOUT-1670": {
        "Key": "MAHOUT-1670",
        "Summary": "Spark and h2o Engine Documentation",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "03/Apr/15 16:15",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "24/Oct/15 02:07",
        "Description": "It would be good to add a 2 similar overview pages for the Spark and h2o Bindings and engines.  These would go under an \"Engines\" section in the \"Mahout Environment\" menu on the site.  Topics would include: a brief overview of each engine, how the Drm API is backed in each enigne,  ie. Spark RDDs and h2o Dataframes,  Interoperability with each engine-native datastructure within an application e.g. Drm.rdd. Any other pertinent information.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1687",
            "/jira/browse/MAHOUT-1648"
        ]
    },
    "MAHOUT-1671": {
        "Key": "MAHOUT-1671",
        "Summary": "CMS Link Sweep after refactoring",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Documentation",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "03/Apr/15 16:27",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "26/Oct/15 22:07",
        "Description": "The recent refactoring likely broke any CMS links to MapReduce code on Github.  Could people please check any pages for broken links?  If you post the page and link on this JIRA, I can commit a fix pretty quickly.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1687"
        ]
    },
    "MAHOUT-1672": {
        "Key": "MAHOUT-1672",
        "Summary": "Update OnlineSummarizer to use the new T-Digest 3.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.10.0",
        "Component/s": "Math",
        "Assignee": "Ted Dunning",
        "Reporter": "Suneel Marthi",
        "Created": "04/Apr/15 01:29",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "06/Apr/15 02:29",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1673": {
        "Key": "MAHOUT-1673",
        "Summary": "Create CI job to verify hadoop1 profile builds ok",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "04/Apr/15 16:57",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "04/Nov/15 06:52",
        "Description": "It would be nice to have CI job which verifies e.g. nightly that hadoop1 profile builds without problems.\nNot sure what's the process to get a new build job.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1667"
        ]
    },
    "MAHOUT-1674": {
        "Key": "MAHOUT-1674",
        "Summary": "A'A fails getting with an index out of range for a row vector",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "s",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "05/Apr/15 16:05",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "07/Apr/15 17:14",
        "Description": "A'A and possibly A'B can fail with an index out of bounds on the row vector. This seems related to partitioning where some partitions may be empty.\nThis can be reproduce with the attached data as input into spark-itemsimilarity. This is only A data and the one large csv will complete correctly but passing in the directory of part files will exhibit the error. The data is identical except in the number of files that are used to contain the data.\nThe error occurs using the local raw filesystem and with master = local and is pretty fast to reach.",
        "Issue Links": []
    },
    "MAHOUT-1675": {
        "Key": "MAHOUT-1675",
        "Summary": "Remove MLP from codebase",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "05/Apr/15 17:29",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "08/Aug/15 18:20",
        "Description": "Remove MLP from the codebase, it is infrequently used and is not being maintained.",
        "Issue Links": []
    },
    "MAHOUT-1676": {
        "Key": "MAHOUT-1676",
        "Summary": "Mark MLP and ConcatVectorsJob deprecated in the codebase",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "05/Apr/15 18:07",
        "Updated": "02/Aug/15 23:34",
        "Resolved": "05/Apr/15 19:07",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1677": {
        "Key": "MAHOUT-1677",
        "Summary": "add a step by step tutorial for classifying text from the spark-shell",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1,                                            0.11.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "05/Apr/15 20:17",
        "Updated": "31/May/15 23:14",
        "Resolved": "23/Apr/15 20:07",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1687"
        ]
    },
    "MAHOUT-1678": {
        "Key": "MAHOUT-1678",
        "Summary": "Hadoop 1 build broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.0",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Pat Ferrel",
        "Created": "08/Apr/15 15:08",
        "Updated": "13/Apr/15 09:57",
        "Resolved": "08/Apr/15 18:12",
        "Description": "building for H1 got error below, which blocks build tests for H1\n T E S T S\n-------------------------------------------------------\nRunning org.apache.mahout.clustering.TestClusterDumper\nRunning org.apache.mahout.clustering.TestClusterEvaluator\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.033 sec - in org.apache.mahout.clustering.TestClusterDumper\nRunning org.apache.mahout.clustering.cdbw.TestCDbwEvaluator\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.089 sec - in org.apache.mahout.clustering.cdbw.TestCDbwEvaluator\nRunning org.apache.mahout.cf.taste.impl.similarity.jdbc.MySQLJDBCInMemoryItemSimilarityTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.701 sec - in org.apache.mahout.cf.taste.impl.similarity.jdbc.MySQLJDBCInMemoryItemSimilarityTest\nRunning org.apache.mahout.text.LuceneStorageConfigurationTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.903 sec - in org.apache.mahout.text.LuceneStorageConfigurationTest\nRunning org.apache.mahout.text.LuceneSegmentInputSplitTest\nTests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 3.552 sec <<< FAILURE! - in org.apache.mahout.text.LuceneSegmentInputSplitTest\ntestGetSegment(org.apache.mahout.text.LuceneSegmentInputSplitTest)  Time elapsed: 2.248 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.newInstance(Ljava/net/URI;Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/fs/FileSystem;\n\tat __randomizedtesting.SeedInfo.seed([B6AAF6EC1A001636:33AA49EC475E421B]:0)\n\tat org.apache.solr.store.hdfs.HdfsDirectory.<init>(HdfsDirectory.java:58)\n\tat org.apache.mahout.text.LuceneSegmentInputSplit.getSegment(LuceneSegmentInputSplit.java:92)\n\tat org.apache.mahout.text.LuceneSegmentInputSplitTest.assertSegmentContainsOneDoc(LuceneSegmentInputSplitTest.java:81)\n\tat org.apache.mahout.text.LuceneSegmentInputSplitTest.testGetSegment(LuceneSegmentInputSplitTest.java:59)\ntestGetSegmentNonExistingSegment(org.apache.mahout.text.LuceneSegmentInputSplitTest)  Time elapsed: 0.958 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.newInstance(Ljava/net/URI;Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/fs/FileSystem;\n\tat __randomizedtesting.SeedInfo.seed([B6AAF6EC1A001636:F16E11692CC0C088]:0)\n\tat org.apache.solr.store.hdfs.HdfsDirectory.<init>(HdfsDirectory.java:58)\n\tat org.apache.mahout.text.LuceneSegmentInputSplit.getSegment(LuceneSegmentInputSplit.java:92)\n\tat org.apache.mahout.text.LuceneSegmentInputSplitTest.testGetSegmentNonExistingSegment(LuceneSegmentInputSplitTest.java:76)\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageTest\nTests run: 10, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.904 sec - in org.apache.mahout.clustering.TestClusterEvaluator\nRunning org.apache.mahout.text.LuceneSegmentRecordReaderTest\nTests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 5.239 sec <<< FAILURE! - in org.apache.mahout.text.LuceneSegmentRecordReaderTest\ntestNonExistingIdField(org.apache.mahout.text.LuceneSegmentRecordReaderTest)  Time elapsed: 2.588 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.newInstance(Ljava/net/URI;Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/fs/FileSystem;\n\tat __randomizedtesting.SeedInfo.seed([BE4E63CDB556DEFF:25483164126E6A9]:0)\n\tat org.apache.solr.store.hdfs.HdfsDirectory.<init>(HdfsDirectory.java:58)\n\tat org.apache.mahout.text.LuceneSegmentInputSplit.getSegment(LuceneSegmentInputSplit.java:92)\n\tat org.apache.mahout.text.LuceneSegmentRecordReader.initialize(LuceneSegmentRecordReader.java:55)\n\tat org.apache.mahout.text.LuceneSegmentRecordReaderTest.testNonExistingIdField(LuceneSegmentRecordReaderTest.java:93)\ntestNonExistingField(org.apache.mahout.text.LuceneSegmentRecordReaderTest)  Time elapsed: 1.188 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.newInstance(Ljava/net/URI;Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/fs/FileSystem;\n\tat __randomizedtesting.SeedInfo.seed([BE4E63CDB556DEFF:4252F6007B6F27B1]:0)\n\tat org.apache.solr.store.hdfs.HdfsDirectory.<init>(HdfsDirectory.java:58)\n\tat org.apache.mahout.text.LuceneSegmentInputSplit.getSegment(LuceneSegmentInputSplit.java:92)\n\tat org.apache.mahout.text.LuceneSegmentRecordReader.initialize(LuceneSegmentRecordReader.java:55)\n\tat org.apache.mahout.text.LuceneSegmentRecordReaderTest.testNonExistingField(LuceneSegmentRecordReaderTest.java:104)\ntestKey(org.apache.mahout.text.LuceneSegmentRecordReaderTest)  Time elapsed: 1.126 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.newInstance(Ljava/net/URI;Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/fs/FileSystem;\n\tat __randomizedtesting.SeedInfo.seed([BE4E63CDB556DEFF:822F5B9135C6A9A9]:0)\n\tat org.apache.solr.store.hdfs.HdfsDirectory.<init>(HdfsDirectory.java:58)\n\tat org.apache.mahout.text.LuceneSegmentInputSplit.getSegment(LuceneSegmentInputSplit.java:92)\n\tat org.apache.mahout.text.LuceneSegmentRecordReader.initialize(LuceneSegmentRecordReader.java:55)\n\tat org.apache.mahout.text.LuceneSegmentRecordReaderTest.testKey(LuceneSegmentRecordReaderTest.java:70)\nRunning org.apache.mahout.text.TestSequenceFilesFromDirectory\nTests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.242 sec - in org.apache.mahout.text.SequenceFilesFromLuceneStorageTest\nRunning org.apache.mahout.text.MailArchivesClusteringAnalyzerTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.436 sec - in org.apache.mahout.text.MailArchivesClusteringAnalyzerTest\nRunning org.apache.mahout.text.LuceneSegmentInputFormatTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 3.957 sec - in org.apache.mahout.text.TestSequenceFilesFromDirectory\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.361 sec <<< FAILURE! - in org.apache.mahout.text.LuceneSegmentInputFormatTest\ntestGetSplits(org.apache.mahout.text.LuceneSegmentInputFormatTest)  Time elapsed: 2.032 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.newInstance(Ljava/net/URI;Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/fs/FileSystem;\n\tat __randomizedtesting.SeedInfo.seed([AD26F40E79D036CF:1945362A68C7FF82]:0)\n\tat org.apache.solr.store.hdfs.HdfsDirectory.<init>(HdfsDirectory.java:58)\n\tat org.apache.mahout.text.LuceneSegmentInputFormat.getSplits(LuceneSegmentInputFormat.java:56)\n\tat org.apache.mahout.text.LuceneSegmentInputFormatTest.testGetSplits(LuceneSegmentInputFormatTest.java:67)\nRunning org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest\nTests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 3.743 sec <<< FAILURE! - in org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest\ntestRun(org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest)  Time elapsed: 3.386 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.newInstance(Ljava/net/URI;Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/fs/FileSystem;\n\tat __randomizedtesting.SeedInfo.seed([95EE4C508BDE3567:37AEE7E3C7764FCB]:0)\n\tat org.apache.solr.store.hdfs.HdfsDirectory.<init>(HdfsDirectory.java:58)\n\tat org.apache.mahout.text.LuceneSegmentInputFormat.getSplits(LuceneSegmentInputFormat.java:56)\n\tat org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:1054)\n\tat org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1071)\n\tat org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)\n\tat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)\n\tat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:550)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:580)\n\tat org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJob.run(SequenceFilesFromLuceneStorageMRJob.java:60)\n\tat org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJobTest.testRun(SequenceFilesFromLuceneStorageMRJobTest.java:67)\nRunning org.apache.mahout.text.SequenceFilesFromMailArchivesTest\nTests run: 5, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 4.454 sec <<< FAILURE! - in org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest\ntestRunOptionalArguments(org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest)  Time elapsed: 1.382 sec  <<< ERROR!\njava.lang.NoSuchMethodError: org.apache.hadoop.fs.FileSystem.newInstance(Ljava/net/URI;Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/fs/FileSystem;\n\tat __randomizedtesting.SeedInfo.seed([D85D7A8B60046745:ADD14D3F8FDEAB51]:0)\n\tat org.apache.solr.store.hdfs.HdfsDirectory.<init>(HdfsDirectory.java:58)\n\tat org.apache.mahout.text.LuceneSegmentInputFormat.getSplits(LuceneSegmentInputFormat.java:56)\n\tat org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:1054)\n\tat org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1071)\n\tat org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)\n\tat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)\n\tat org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:550)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:580)\n\tat org.apache.mahout.text.SequenceFilesFromLuceneStorageMRJob.run(SequenceFilesFromLuceneStorageMRJob.java:60)\n\tat org.apache.mahout.text.SequenceFilesFromLuceneStorageDriver.run(SequenceFilesFromLuceneStorageDriver.java:118)\n\tat org.apache.mahout.text.SequenceFilesFromLuceneStorageDriverTest.testRunOptionalArguments(SequenceFilesFromLuceneStorageDriverTest.java:135)\nRunning org.apache.mahout.utils.SplitInputTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.956 sec - in org.apache.mahout.text.SequenceFilesFromMailArchivesTest\nRunning org.apache.mahout.utils.TestConcatenateVectorsJob\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.167 sec - in org.apache.mahout.utils.TestConcatenateVectorsJob\nRunning org.apache.mahout.utils.vectors.VectorHelperTest\nTests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.519 sec - in org.apache.mahout.utils.vectors.VectorHelperTest\nRunning org.apache.mahout.utils.vectors.arff.DriverTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.784 sec - in org.apache.mahout.utils.vectors.arff.DriverTest\nRunning org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest\nTests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.552 sec - in org.apache.mahout.utils.vectors.arff.ARFFVectorIterableTest\nRunning org.apache.mahout.utils.vectors.arff.ARFFTypeTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.359 sec - in org.apache.mahout.utils.vectors.arff.ARFFTypeTest\nRunning org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.386 sec - in org.apache.mahout.utils.vectors.arff.MapBackedARFFModelTest\nTests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 7.17 sec - in org.apache.mahout.utils.SplitInputTest\nRunning org.apache.mahout.utils.vectors.lucene.LuceneIterableTest\nRunning org.apache.mahout.utils.vectors.lucene.DriverTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.159 sec - in org.apache.mahout.utils.vectors.lucene.LuceneIterableTest\nRunning org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.799 sec - in org.apache.mahout.utils.vectors.lucene.CachedTermInfoTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.135 sec - in org.apache.mahout.utils.vectors.lucene.DriverTest\nRunning org.apache.mahout.utils.vectors.io.VectorWriterTest\nRunning org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.601 sec - in org.apache.mahout.utils.vectors.csv.CSVVectorIteratorTest\nRunning org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.292 sec - in org.apache.mahout.utils.vectors.io.VectorWriterTest\nTests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.523 sec - in org.apache.mahout.utils.nlp.collocations.llr.BloomTokenFilterTest\nRunning org.apache.mahout.utils.Bump125Test\nRunning org.apache.mahout.utils.regex.RegexUtilsTest\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.373 sec - in org.apache.mahout.utils.Bump125Test\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.364 sec - in org.apache.mahout.utils.regex.RegexUtilsTest\nRunning org.apache.mahout.utils.regex.RegexMapperTest\nRunning org.apache.mahout.utils.email.MailProcessorTest\nTests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.574 sec - in org.apache.mahout.utils.email.MailProcessorTest\nTests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 1.202 sec - in org.apache.mahout.utils.regex.RegexMapperTest\nResults :\nTests in error: \n  LuceneSegmentInputSplitTest.testGetSegment:59->assertSegmentContainsOneDoc:81 \u00bb NoSuchMethod\n  LuceneSegmentInputSplitTest.testGetSegmentNonExistingSegment:76 \u00bb NoSuchMethod\n  LuceneSegmentRecordReaderTest.testNonExistingIdField:93 \u00bb NoSuchMethod org.apa...\n  LuceneSegmentRecordReaderTest.testNonExistingField:104 \u00bb NoSuchMethod org.apac...\n  LuceneSegmentRecordReaderTest.testKey:70 \u00bb NoSuchMethod org.apache.hadoop.fs.F...\n  LuceneSegmentInputFormatTest.testGetSplits:67 \u00bb NoSuchMethod org.apache.hadoop...\n  SequenceFilesFromLuceneStorageMRJobTest.testRun:67 \u00bb NoSuchMethod org.apache.h...\n  SequenceFilesFromLuceneStorageDriverTest.testRunOptionalArguments:135 \u00bb NoSuchMethod\nTests run: 103, Failures: 0, Errors: 8, Skipped: 0\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary:\n[INFO] \n[INFO] Mahout Build Tools ................................ SUCCESS [7.111s]\n[INFO] Apache Mahout ..................................... SUCCESS [0.098s]\n[INFO] Mahout Math ....................................... SUCCESS [1:28.659s]\n[INFO] Mahout HDFS ....................................... SUCCESS [5.642s]\n[INFO] Mahout Map-Reduce ................................. SUCCESS [13:25.549s]\n[INFO] Mahout Integration ................................ FAILURE [1:55.954s]\n[INFO] Mahout Examples ................................... SKIPPED\n[INFO] Mahout Release Package ............................ SKIPPED\n[INFO] Mahout Math Scala bindings ........................ SKIPPED\n[INFO] Mahout Spark bindings ............................. SKIPPED\n[INFO] Mahout Spark bindings shell ....................... SKIPPED\n[INFO] Mahout H2O backend ................................ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 17:03.890s\n[INFO] Finished at: Wed Apr 08 07:11:34 PDT 2015\n[INFO] Final Memory: 56M/335M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.17:test (default-test) on project mahout-integration: There are test failures.\n[ERROR]",
        "Issue Links": []
    },
    "MAHOUT-1679": {
        "Key": "MAHOUT-1679",
        "Summary": "example script run-item-sim should work on hdfs as well as local",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "Examples",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "09/Apr/15 17:21",
        "Updated": "19/Dec/16 06:31",
        "Resolved": null,
        "Description": "mahout/examples/bin/run-item-sim does not run on a cluster or pseudo-cluster Spark + HDFS\nIt prints a warning and how to run in cluster but should just work in either mode",
        "Issue Links": [
            "/jira/browse/MAHOUT-1853"
        ]
    },
    "MAHOUT-1680": {
        "Key": "MAHOUT-1680",
        "Summary": "Rename project release artifact names to be prefixed with 'apache-mahout-*",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1",
        "Component/s": "build",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Suneel Marthi",
        "Created": "12/Apr/15 11:14",
        "Updated": "31/May/15 23:14",
        "Resolved": "14/Apr/15 11:15",
        "Description": "The artifacts generated now read as 'mahout-distribution-', need to change the build configuration to rename the artifacts as 'apache-mahout-distribution-'",
        "Issue Links": []
    },
    "MAHOUT-1681": {
        "Key": "MAHOUT-1681",
        "Summary": "Rename module - 'mahout-math-scala' to 'mahout-samsara'",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Suneel Marthi",
        "Created": "12/Apr/15 11:18",
        "Updated": "31/May/15 23:14",
        "Resolved": "14/Apr/15 21:38",
        "Description": "Rename module - 'mahout-math-scala' to 'mahout-samsara'",
        "Issue Links": []
    },
    "MAHOUT-1682": {
        "Key": "MAHOUT-1682",
        "Summary": "Create a documentation page for SPCA",
        "Type": "Documentation",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "12/Apr/15 14:54",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "04/Feb/17 00:30",
        "Description": "Following the template of the SSVD and QR pages create a page for SPCA.  This Page would go under Algorithms-> Distributed Matrix Decomposition.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1687",
            "/jira/browse/MAHOUT-1893"
        ]
    },
    "MAHOUT-1683": {
        "Key": "MAHOUT-1683",
        "Summary": "Create a reference page for DSL linalg operators",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "12/Apr/15 15:30",
        "Updated": "31/May/15 23:14",
        "Resolved": "16/Apr/15 00:58",
        "Description": "A simple reference page for explaining the DSL operators, creating a matrix, parallelizing and saving the matrix, etc.   Basicaly copying all of the code blocks  from sec 1.1 through 1.8,  sec 2.2 through 2.7 and sec 2.9 through 2.10 from:\n http://mahout.apache.org/users/sparkbindings/ScalaSparkBindings.pdf\n to the site with minimal explanation (for easy browsing).",
        "Issue Links": [
            "/jira/browse/MAHOUT-1687"
        ]
    },
    "MAHOUT-1684": {
        "Key": "MAHOUT-1684",
        "Summary": "Updated NOTICE and LICENSE texts",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "12/Apr/15 17:22",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 18:04",
        "Description": "Updates to the NOTICE and LICENSE texts is long overdue.",
        "Issue Links": []
    },
    "MAHOUT-1685": {
        "Key": "MAHOUT-1685",
        "Summary": "Move Mahout shell to Spark 1.3+",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.0",
        "Component/s": "Mahout spark shell",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Pat Ferrel",
        "Created": "12/Apr/15 18:06",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "10/Jul/15 19:34",
        "Description": "Building for Spark 1.3 we found several important APIS used by the shell are now marked package private in Spark, making them inaccessible.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1653"
        ]
    },
    "MAHOUT-1686": {
        "Key": "MAHOUT-1686",
        "Summary": "Create a documentattion page for ALS",
        "Type": "Documentation",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "website",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "12/Apr/15 22:26",
        "Updated": "03/Mar/18 22:19",
        "Resolved": null,
        "Description": "Following the template of the SSVD and QR pages create a page for ALS. This Page would go under Algorithms-> Distributed Matrix Decomposition.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1687",
            "/jira/browse/MAHOUT-1893"
        ]
    },
    "MAHOUT-1687": {
        "Key": "MAHOUT-1687",
        "Summary": "Add documentation to the website and fix any problems",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "12/Apr/15 22:28",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "05/Nov/15 02:29",
        "Description": "This will be a blanket JIRA with all website related issues linked to it.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1686",
            "/jira/browse/MAHOUT-1682",
            "/jira/browse/MAHOUT-1670",
            "/jira/browse/MAHOUT-1671",
            "/jira/browse/MAHOUT-1677",
            "/jira/browse/MAHOUT-1683",
            "/jira/browse/MAHOUT-1689",
            "/jira/browse/MAHOUT-1688"
        ]
    },
    "MAHOUT-1688": {
        "Key": "MAHOUT-1688",
        "Summary": "Fix twitter tmeline on CMS",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "12/Apr/15 22:36",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "17/Jun/15 22:37",
        "Description": "The @ApacheMahout  twitter timeline on the CMS is broken.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1687"
        ]
    },
    "MAHOUT-1689": {
        "Key": "MAHOUT-1689",
        "Summary": "Create a doc on how to write an app that uses Mahout as a lib",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Pat Ferrel",
        "Reporter": "Andrew Palumbo",
        "Created": "13/Apr/15 00:11",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "17/Mar/16 15:48",
        "Description": "Create a doc on how to write an app that uses Mahout as a lib",
        "Issue Links": [
            "/jira/browse/MAHOUT-1687"
        ]
    },
    "MAHOUT-1690": {
        "Key": "MAHOUT-1690",
        "Summary": "CLONE - Some vector dumper flags are expecting arguments.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Allen McIntosh",
        "Created": "14/Apr/15 22:36",
        "Updated": "31/May/15 23:14",
        "Resolved": "31/May/15 04:25",
        "Description": "MAHOUT-993 seems to be back.  In particular, the --sortVectors/-sort option insists on an argument, but anything you give (e.g. aardvark) is happily accepted.  I did not check the other options.",
        "Issue Links": [
            "/jira/browse/MAHOUT-993"
        ]
    },
    "MAHOUT-1691": {
        "Key": "MAHOUT-1691",
        "Summary": "iterable of vectors to matrix",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.11.1",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "15/Apr/15 16:00",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "22/Oct/15 18:07",
        "Description": "In Mahout scala bindings, instead of writing  \n\nval res = drmX.mapBlock(drmX.ncol) {\n  case (keys, block) => {\n    val copy = block.like\n    copy := block.map(row => (row - mean) / std)\n    (keys, copy)\n  }\n}\n\n\nI would like to be able to write \n\nval res = drmX.mapBlock(drmX.ncol) {\n  case (keys, block) => {\n    keys -> block.map(row => (row - mean) / std)\n  }\n}\n\n\nSolution: add a method for implicit conversion from iterable to Matrix:\n\n  implicit def iterable2Matrix(that: Iterable[Vector]): Matrix = {\n    val first = that.head\n    val nrow = that.size\n    val ncol = first.size\n\n    val m = if (first.isDense) {\n      new DenseMatrix(nrow, ncol)\n    } else {\n      new SparseRowMatrix(nrow, ncol)\n    }\n\n    that.zipWithIndex.foreach { case (row, idx) => \n      m.assignRow(idx.toInt, row)\n    }\n\n    m\n  }\n\n\nIf it sounds nice, I can send a pull request with this implemented",
        "Issue Links": []
    },
    "MAHOUT-1692": {
        "Key": "MAHOUT-1692",
        "Summary": "wrong description about spectral-clustering",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.0",
        "Component/s": "Clustering,                                            Documentation",
        "Assignee": "Shannon Quinn",
        "Reporter": "Jiahongchao",
        "Created": "16/Apr/15 05:13",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "11/Aug/15 23:56",
        "Description": "in http://mahout.apache.org/users/clustering/spectral-clustering.html\nIt says  returns values in the range [0, 1], where 0 indicates completely disconnected (or completely dissimilar) and 1 is fully connected (or identical)\nBut 0, 0, 0 should be 0,0,1 because an element is identical to itself",
        "Issue Links": []
    },
    "MAHOUT-1693": {
        "Key": "MAHOUT-1693",
        "Summary": "FunctionalMatrixView materializes row vectors in scala shell",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1",
        "Component/s": "Mahout spark shell,                                            Math",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Suneel Marthi",
        "Created": "21/Apr/15 03:17",
        "Updated": "31/May/15 23:14",
        "Resolved": "22/Apr/15 05:06",
        "Description": "FunctionalMatrixView materializes row vectors in scala shell.\nProblem first reported by a user Michael Alton, Intel:\n\"When I first tried to make a large matrix, I got an out of Java heap space error. I increased the memory incrementally until I got it to work. \u201cexport MAHOUT_HEAPSIZE=8000\u201d didn\u2019t work, but \u201cexport MAHOUT_HEAPSIZE=64000\u201d did. The question is why do we need so much memory? A 5000x5000 matrix of doubles should only take up ~200MB of space?\"\nProblem has been narrowed down to not override toString() method in FunctionalMatrixView which causes it to materialize all of the row vectors when run in Mahout Spark Shell.",
        "Issue Links": []
    },
    "MAHOUT-1694": {
        "Key": "MAHOUT-1694",
        "Summary": "the doc Introduction to ALS Recommendations bug",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.0",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Derrick Tian",
        "Created": "21/Apr/15 03:20",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "11/Aug/15 23:41",
        "Description": "Step 4: Make Recommendations\nthis step's input is  described as directory containing files of user ids\nand needs to be a SequenceFile.\nbut in the example the input is as the same as Run ALS step.\nboth like this --input $als_input\nmaybe step4 can describe as :\n$ mahout recommendfactorized --input $als_recommend_input",
        "Issue Links": []
    },
    "MAHOUT-1695": {
        "Key": "MAHOUT-1695",
        "Summary": "the bug in class org.apache.mahout.cf.taste.hadoop.als.ALS",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Derrick Tian",
        "Created": "21/Apr/15 03:30",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "30/Mar/16 20:28",
        "Description": "in the function readMatrixByRowsFromDistributedCache\nit goes to the hadoop Cached dir to get CachedFiles\nwhile other CachedFiles as -libjsrs file are also in the dir\nthis problem will cause error",
        "Issue Links": [
            "/jira/browse/MAHOUT-1634"
        ]
    },
    "MAHOUT-1696": {
        "Key": "MAHOUT-1696",
        "Summary": "QRDecomposition.solve(...) can return incorrect Matrix types",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1,                                            0.11.0",
        "Component/s": "Math",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "24/Apr/15 22:31",
        "Updated": "31/May/15 23:14",
        "Resolved": "24/Apr/15 23:41",
        "Description": "in QRDecomposition.java, QRDecomposition(Matrix A).solve(Matrix B) is returning a Matrix of type B when it should be returning a matrix of type A.  This can lead to Sparse Matrices which should be Dense and vice versa.",
        "Issue Links": []
    },
    "MAHOUT-1697": {
        "Key": "MAHOUT-1697",
        "Summary": "Math-scala and spark module docs are packaged under wrong path in bin distribution archive",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1",
        "Component/s": "Documentation",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "26/Apr/15 12:35",
        "Updated": "31/May/15 23:14",
        "Resolved": "26/Apr/15 13:28",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1698": {
        "Key": "MAHOUT-1698",
        "Summary": "Streaming K-means and Fuzzy K-means to output clusteredPoints",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.12.2",
        "Component/s": "Clustering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sujit Thumma",
        "Created": "27/Apr/15 01:49",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "29/May/16 17:25",
        "Description": "Similar to K-Means algorithm is there a way streaming K-means and Fuzzy K-means output clustered points in map-reduce? This can be useful to map document with cluster ID.  As of now only K-means can output clustered points and streaming k-means just outputs centroids.",
        "Issue Links": []
    },
    "MAHOUT-1699": {
        "Key": "MAHOUT-1699",
        "Summary": "Trim down Mahout packaging for next release",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1",
        "Component/s": "build",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Suneel Marthi",
        "Created": "28/Apr/15 03:09",
        "Updated": "31/May/15 23:14",
        "Resolved": "31/May/15 12:09",
        "Description": "Mahout 0.10.0 package size is 210MB, this needs to be trimmed down to a more manageable size.\nThis also makes it hard to package Mahout into the BigTop distro and not to mention seeking an infra waiver at the time of release for the > 200MB size.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1704",
            "/jira/browse/BIGTOP-1831"
        ]
    },
    "MAHOUT-1700": {
        "Key": "MAHOUT-1700",
        "Summary": "OutOfMemory Problem in ABtDenseOutJob in Distributed SSVD",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.9,                                            0.10.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ethan Yi",
        "Created": "28/Apr/15 03:34",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "30/Mar/16 15:23",
        "Description": "Recently, I tried mahout's hadoop ssvd(mahout-0.9 or mahout-1.0)  job. There's a java heap space out of memory problem  in ABtDenseOutJob. I found the reason, the ABtDenseOutJob map code is as below:\n    protected void map(Writable key, VectorWritable value, Context context)\n      throws IOException, InterruptedException {\n      Vector vec = value.get();\n      int vecSize = vec.size();\n      if (aCols == null) \n{\n        aCols = new Vector[vecSize];\n      }\n else if (aCols.length < vecSize) \n{\n        aCols = Arrays.copyOf(aCols, vecSize);\n      }\n\n      if (vec.isDense()) {\n        for (int i = 0; i < vecSize; i++) \n{\n          extendAColIfNeeded(i, aRowCount + 1);\n          aCols[i].setQuick(aRowCount, vec.getQuick(i));\n        }\n      } else if (vec.size() > 0) {\n        for (Vector.Element vecEl : vec.nonZeroes()) \n{\n          int i = vecEl.index();\n          extendAColIfNeeded(i, aRowCount + 1);\n          aCols[i].setQuick(aRowCount, vecEl.get());\n        }\n      }\n      aRowCount++;\n    }\nIf the input is RandomAccessSparseVector, usually with big data, it's vec.size() is Integer.MAX_VALUE, which is 2^31, then aCols = new Vector[vecSize] will introduce the OutOfMemory problem. The settlement of course should be enlarge every tasktracker's maximum memory:\n<property>\n  <name>mapred.child.java.opts</name>\n  <value>-Xmx1024m</value>\n</property>\nHowever, if you are NOT hadoop administrator or ops, you have no permission to modify the config. So, I try to modify ABtDenseOutJob map code to support RandomAccessSparseVector situation, I use hashmap to represent aCols instead of the original Vector[] aCols array, the modified code is as below:\nprivate Map<Integer, Vector> aColsMap = new HashMap<Integer, Vector>();\n    protected void map(Writable key, VectorWritable value, Context context)\n      throws IOException, InterruptedException {\n      Vector vec = value.get();\n      if (vec.isDense()) {\n        for (int i = 0; i < vecSize; i++) {\n          //extendAColIfNeeded(i, aRowCount + 1);\n          if (aColsMap.get == null) \n{\n        \t  aColsMap.put(i, new RandomAccessSparseVector(Integer.MAX_VALUE, 100));\n          }\n          aColsMap.get.setQuick(aRowCount, vec.getQuick);\n          //aCols[i].setQuick(aRowCount, vec.getQuick);\n        }\n      } else if (vec.size() > 0) {\n        for (Vector.Element vecEl : vec.nonZeroes()) {\n          int i = vecEl.index();\n          //extendAColIfNeeded(i, aRowCount + 1);\n          if (aColsMap.get == null) {        \t  aColsMap.put(i, new RandomAccessSparseVector(Integer.MAX_VALUE, 100));          }\n          aColsMap.get.setQuick(aRowCount, vecEl.get());\n          //aCols[i].setQuick(aRowCount, vecEl.get());\n        }\n      }\n      aRowCount++;\n    }\nThen the OutofMemory problem is dismissed.",
        "Issue Links": []
    },
    "MAHOUT-1701": {
        "Key": "MAHOUT-1701",
        "Summary": "Mahout DSL for Flink: implement AtB ABt and AtA operators",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "30/Apr/15 14:45",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 20:37",
        "Description": "as a part of MAHOUT-1570 implement the following operators on Flink:\n\nAtB\nABt\nAtA",
        "Issue Links": [
            "/jira/browse/MAHOUT-1750",
            "/jira/browse/MAHOUT-1751",
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1702": {
        "Key": "MAHOUT-1702",
        "Summary": "Mahout DSL for Flink: implement element-wise operators",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "30/Apr/15 14:48",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 02:38",
        "Description": "as a part of MAHOUT-1570 implement the following operators on Flink:\n\nAewB\nAewScalar",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1703": {
        "Key": "MAHOUT-1703",
        "Summary": "Mahout DSL for Flink: implement cbind and rbind",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "30/Apr/15 14:48",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 02:47",
        "Description": "as a part of MAHOUT-1570 implement the following operators on Flink:\n\ncbind\ncbind with scalar\nrbind\n\nrbind depends on mapBlock operator, so it also has to be implemented as a part of this Jira",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1704": {
        "Key": "MAHOUT-1704",
        "Summary": "Pare down dependency jar for h2o",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1,                                            0.11.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "02/May/15 04:50",
        "Updated": "31/May/15 23:14",
        "Resolved": "07/May/15 19:56",
        "Description": "The dependency jar for h2o is very large: ~68MB.  Pare this down to only include only  necessary runtime classes.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1699"
        ]
    },
    "MAHOUT-1705": {
        "Key": "MAHOUT-1705",
        "Summary": "Verify dependencies in job jar for mahout-examples",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "05/May/15 15:50",
        "Updated": "06/Sep/16 21:22",
        "Resolved": "21/Mar/16 19:45",
        "Description": "mahout-example-*-job.jar is around ~56M, and may package unused runtime libraries.  We need to go through this and make sure that there is nothing unneeded or redundant.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1706",
            "/jira/browse/MAHOUT-1880"
        ]
    },
    "MAHOUT-1706": {
        "Key": "MAHOUT-1706",
        "Summary": "Remove dependency jars from /lib in mahout binary distribution",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "06/May/15 21:16",
        "Updated": "20/Oct/20 12:06",
        "Resolved": null,
        "Description": "The mahout distribution currently is shipping ~56 MB of dependecy jars in the /lib directory of the distribution.  These are only added to the classpath by /bin/mahout in the binary distribution. It seems that we can remove them from the distribution. (we need to get the size of the distribution down)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1880",
            "/jira/browse/MAHOUT-1705",
            "https://github.com/apache/mahout/pull/129"
        ]
    },
    "MAHOUT-1707": {
        "Key": "MAHOUT-1707",
        "Summary": "Spark-itemsimilarity uses too much memory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.1",
        "Component/s": "Collaborative Filtering,                                            cooccurrence",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "13/May/15 17:23",
        "Updated": "31/May/15 23:14",
        "Resolved": "22/May/15 19:58",
        "Description": "java.lang.OutOfMemoryError: Java heap space\nThe code has an unnecessary .collect(), forcing all interaction data into memory of the client/driver. Increasing the executor memory will not help with this.\nremove this line and rebuild Mahout.\nhttps://github.com/apache/mahout/blob/mahout-0.10.x/spark/src/main/scala/org/apache/mahout/drivers/TextDelimitedReaderWriter.scala#L157\nThe errant line reads:\n    interactions.collect()\nThis forces the user action data into memory, a bad thing for memory consumption. Removing it should allow for better Spark memory management.",
        "Issue Links": []
    },
    "MAHOUT-1708": {
        "Key": "MAHOUT-1708",
        "Summary": "Replace Google/Guava in mahout-math and mahout-hdfs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Hdfs,                                            Math",
        "Assignee": "Andrew Musselman",
        "Reporter": "Pat Ferrel",
        "Created": "18/May/15 18:00",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "05/Nov/15 00:56",
        "Description": "all use of guava has been removed from the code used with Spark except the use of Preconditions. These are pretty easy to replace. \n1) remove guava from mahout-math, mahout-hdfs, poms and the spark dependency-reduced assembly.\n2) you will now get compile errors for math and hdfs so remove the imports and replace and Preconditions with asserts.\nNot sure how many errors in replacing these will be caught with unit tests so be careful.",
        "Issue Links": []
    },
    "MAHOUT-1709": {
        "Key": "MAHOUT-1709",
        "Summary": "Mahout DSL for Flink: implement slicing",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "28/May/15 09:26",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 03:14",
        "Description": "As a part of MAHOUT-1570 OpRowRange operator for Flink",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1710": {
        "Key": "MAHOUT-1710",
        "Summary": "Mahout DSL for Flink: implement right in-core matrix multiplication",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Alexey Grigorev",
        "Created": "28/May/15 09:29",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 03:00",
        "Description": "As a part of MAHOUT-1570 implement A %*% B multiplication when B is an in-core matrix",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1711": {
        "Key": "MAHOUT-1711",
        "Summary": "Mahout DSL for Flink: implement broadcasting",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Workaround",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "28/May/15 09:31",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "30/Mar/16 02:03",
        "Description": "as a part of MAHOUT-1570 implement drmBroadcast for flink",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1712": {
        "Key": "MAHOUT-1712",
        "Summary": "Mahout DSL for Flink: implement operators At, Ax, Atx",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "28/May/15 13:24",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 20:53",
        "Description": "As a part of MAHOUT-1570, implement the following operators on Flink:\n\nAt\nAx\nAtx",
        "Issue Links": [
            "/jira/browse/MAHOUT-1749",
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1713": {
        "Key": "MAHOUT-1713",
        "Summary": "Performance and parallelization improvements for AB', A'B, A'A spark physical operators",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 22:37",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:01",
        "Description": "per name. \nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1714": {
        "Key": "MAHOUT-1714",
        "Summary": "Add MAHOUT_OPTS environment when running Spark shell",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 22:55",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:01",
        "Description": "PR https://github.com/apache/mahout/pull/135",
        "Issue Links": [
            "/jira/browse/MAHOUT-1643"
        ]
    },
    "MAHOUT-1715": {
        "Key": "MAHOUT-1715",
        "Summary": "Closeable API for broadcast tensors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 22:57",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:01",
        "Description": "Spark has added explicit termination api for broadcasts in 1.1. So broadcast variables are now closeable by driver. \nSpark 1.2 seems to have done this change a bit moot since it is using reference queues to shut off broadcasts (and caches) automatically now.\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1716": {
        "Key": "MAHOUT-1716",
        "Summary": "Scala logging style",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:03",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:01",
        "Description": "Enable concise logging e.g. \ntrace(s\"blha blah $variable\") and \ntraceDo { \n ...\n}\nnote that since parameter has really late binding type (message : => String), the debug string is never computed unless the proper debug level is enabled.\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1717": {
        "Key": "MAHOUT-1717",
        "Summary": "allreduceBlock operator api and Spark implementation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:05",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "need it surprisingly often. \nallreduceMap ( map: (keys, Matrix) => Matrix, reduce: (Matrix, Matrix) => Matrix) : Matrix\nagain since we cannot guarantee we'd ask engines to serialize any type, we operate on types we do know will work (I.e. matrix blocks).\nPR: https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1718": {
        "Key": "MAHOUT-1718",
        "Summary": "Support for conversion of any type-keyed DRM into ordinally-keyed DRM",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:09",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "drm2IntKeyed(DrmLike[K]) => DrmLike[Int] where the result rows are ordinally keyed between 0 and nrow-1\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1719": {
        "Key": "MAHOUT-1719",
        "Summary": "Unary elementwise function operator and function fusions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:10",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "optimizer rewrites for something like 1 + drmX * dexp(drmX) => collapse into elementwiseFunc(f1(f2(drmX)) where f1 = 1 + x and f2 = exp. Also absolutes current scalar-elementwise physical operators used for stull like 1+drmX .\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1720": {
        "Key": "MAHOUT-1720",
        "Summary": "Support 1 cbind X, X cbind 1 etc. for both Matrix and DRM",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:15",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "we have column and row (where applicable) bind between two tensor operators. this issue enables combnations like 1 cbind X etc.\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1721": {
        "Key": "MAHOUT-1721",
        "Summary": "rowSumsMap summary for non-int-keyed DRMs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:18",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "we have rowSums() that are enabled only if matrix is int-keyed (DrmLike[Int]), because non-int keys cannot be mapped to ordinal positions. \nhowever, this api still can exist \u2013 except the output should really be Map[K,Double].\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1722": {
        "Key": "MAHOUT-1722",
        "Summary": "DRM row sampling api",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:22",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "We will ask engines to support two tiny apis for row vector sampling. \nOne api is uniform multivariate hypergeometric (k parameter is given), and another is by fraction (simple map-only probabilistic filter). Spark implementation is enclosed (Spark just has an api for both, albeit k-sampler does not have strict mathematical guarantee of the distribution, and is only for small k).\nchallenge here is that returned rows should be ordinally renumbered.\n(maybe i need to revisit this issue later, this was a pretty hasty API change, might be less than ideal in general case).\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1723": {
        "Key": "MAHOUT-1723",
        "Summary": "Optional structural \"flavor\" abstraction for in-core matrices.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:24",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "This is structural abstraction (which is optional for a matrix but implemented for most we use) that allows to interrogate matrix operands whether they are dense or sparse, row or column-major or diagonal etc. without coding to a concrete matrix implementation class.\nthis allows to build structure-optimized iterations. \nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1724": {
        "Key": "MAHOUT-1724",
        "Summary": "Optimizations of matrix-matrix in-core multiplication based on structural flavors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:27",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "Current times() operation suffers a lot. Especially when e.g. column-major iterations are done over a matrix backed by row-major sequential sparse vectors, things are dropping orders of magnitude in performance. \nThis issue finds optimal traversal gemm kernel order for current matrix; or rewrites matrix prior to operation to enable optimal traversals.\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1725": {
        "Key": "MAHOUT-1725",
        "Summary": "elementwise power operator ^",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:31",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 19:00",
        "Description": "this is a power elementwise operation on in-core and out-of-core tensor types. it is only defined on (tensor-type, Double) pair. This is consistent with R caret operator. \ne.g. computing distance between two points(vectors) x and y:\n        sqrt((x-y) ^= 2 sum)\nDanger is that for integral types in Scala interpret this as exclusive or, which is something completely different altogether .",
        "Issue Links": []
    },
    "MAHOUT-1726": {
        "Key": "MAHOUT-1726",
        "Summary": "R-like vector concatenation operator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:32",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 18:59",
        "Description": "in R, we write c(a,b)\nin Scala we write a c b or a.c(b) or perhaps a.c(b,c,d) \nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1727": {
        "Key": "MAHOUT-1727",
        "Summary": "Elementwise analogues of scala.math functions for tensor types",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:38",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 18:59",
        "Description": "what we mean here: \nlog(5)\nvlog(vec)\nmlog(mxA)\ndlog(drmA)\nin R, all these variants are enacted by single function log. Unfortunately, we cannot overload standard functions since they are defined in another package and Scala would allow such an overload.\nHence, notation conventions: \nvector functions are prepended by 'v': vlog, vabs...\nin-core matrix functions are prepended by 'm': mlog, mabs...\nDRM functions are prepended by 'd' (for distributed): dlog, dabs...\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1728": {
        "Key": "MAHOUT-1728",
        "Summary": "in-core functional assignments",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:43",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 18:59",
        "Description": "in-core functional assignments (for vector and matrices) \nmxA := \n{ (x) => x * x}\n \nmxA := \n{ (row, col, x} => ... }\nmxA ::={ (x) => ... }\nmxA ::={ (row, col, x}\n => ...}\nvec := \n{ (x) => ...}\nvec :={ (idx, x) => ..}\nvec ::= { (x) => ...}\nvec ::=\n{ (ind, x) => ...}\n\nthe `:=` assignmentn applies the function to all elements of tensor. \nthe `::=` assignment ignores zero elements of the tensor to improve performance. \nmatrix functions iterations use matrix structural flavor to optimize traversal.\nfurther examples.\nmxA := exp _ (in-place exponent)\nv ::= abs _\nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1729": {
        "Key": "MAHOUT-1729",
        "Summary": "Straighten out behavior of Matrix.iterator() and iterateNonEmpty()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:46",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 18:59",
        "Description": "SparseMatrix in particular does iterateNonEmpty() instead of iterating over all rows in default iterator. This creates contract inconsistency leading to computational errors.\nso... this fixes to guarantee that \nfor (row <- mxA)  \nvisits ALL rows in the mxA (even if they are totally empty). \nIf it is ok not to process completely 0ed rows, then the following form should be used: \nfor (row <- mxA.iterateNonEmpty) ... \nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1730": {
        "Key": "MAHOUT-1730",
        "Summary": "new mutable transposition view for in-core matrices.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:55",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 18:59",
        "Description": "in-core mxA.t rewrite with mostly two goals in mind: \n(1) enable mutability , e.g. \nfor (col <- mxA.t) col := k\n(2) translate matrix structural flavor for optimizers correctly. i.e. new SparseRowMatrix.t  carries on as column-major structure.\nAlso it does unwrapping automatically, i.e. mxA.t.t eq mxA is true.\nthere are other smaller goals hit in this rewrite, e.g. various efficient assigns. \nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1731": {
        "Key": "MAHOUT-1731",
        "Summary": "Deprecate SparseColumnMatrix.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "02/Jun/15 23:57",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 18:59",
        "Description": "seems to be neither needed nor implemented consistently. \nTo create column-major sparse matrix structure with the same properties, use new SparseRowMatrix(...).t \nPR https://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1732": {
        "Key": "MAHOUT-1732",
        "Summary": "Native support for kryo serialization of tensor types.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "03/Jun/15 00:00",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jun/15 18:58",
        "Description": "Right now, the serialization of tensor types (matrix, vector) is double-bridged thru VectorWritable and MatrixWritable.\na new writeup of kryo serializers in this issue provides direct support for serializing vector and matrix types (also accounting for their structural flavors). \nThis is a first rewrite, so improvements probably could be provided, but i so far have not found this to become a visible bottleneck again over other spark io costs.\nhttps://github.com/apache/mahout/pull/135",
        "Issue Links": []
    },
    "MAHOUT-1733": {
        "Key": "MAHOUT-1733",
        "Summary": "Move HDFSUtils to some common project",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Alexey Grigorev",
        "Created": "03/Jun/15 07:42",
        "Updated": "30/Mar/16 17:21",
        "Resolved": "15/Mar/16 03:18",
        "Description": "Hadoop1HDFSUtils doesn't seem to be Spark-specific, and in Flink-bindings we need to have similar functionality. For now I just copied this class over to the Flick bindings submodule, but it would be nice to have this in some commonly used module e.g. mahout-hdfs or in the scala bindings. \nAdditionally when the utils class will be changed in spark bindings (e.g. in MAHOUT-1660) we'll have to copy the changes - so it would be nice to avoid it",
        "Issue Links": [
            "/jira/browse/MAHOUT-1777",
            "/jira/browse/MAHOUT-1660"
        ]
    },
    "MAHOUT-1734": {
        "Key": "MAHOUT-1734",
        "Summary": "Mahout DSL for Flink: implement I/O",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Alexey Grigorev",
        "Created": "09/Jun/15 06:49",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 02:31",
        "Description": "As a part of MAHOUT-1570, implement reading DRMs from the file system and writing the results back",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570",
            "/jira/browse/FLINK-2194"
        ]
    },
    "MAHOUT-1735": {
        "Key": "MAHOUT-1735",
        "Summary": "Allow reading input from HDFS for arff.vector",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dustin Cote",
        "Created": "09/Jun/15 18:51",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "03/Apr/16 15:52",
        "Description": "Currently the arff.vector is not able to read input from HDFS.  It looks like right now, the read tries to go through TrainLogistic which uses Guava.  It would be nice to hookup to the HDFS client to read in the arff file from HDFS.",
        "Issue Links": []
    },
    "MAHOUT-1736": {
        "Key": "MAHOUT-1736",
        "Summary": "H20 implementation for allreduceBlock",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Anand Avati",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "10/Jun/15 22:20",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "24/Jun/15 18:34",
        "Description": "allreduce aggregator on matrix blocks: \n\n override def allreduceBlock[K: ClassTag](drm: CheckpointedDrm[K], bmf: BlockMapFunc2[K], rf: BlockReduceFunc)\n  : Matrix = ???\n\n\n\nThis is pretty important for custom aggregated statistics",
        "Issue Links": []
    },
    "MAHOUT-1737": {
        "Key": "MAHOUT-1737",
        "Summary": "H20 implementation of AewUnaryFunc physical operator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Anand Avati",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "10/Jun/15 22:22",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "24/Jun/15 18:34",
        "Description": "this physical operator allows to optimize elementwise expressions like \n`dlog(X * X) + 1` where elementwise function compositions are detected and fused into one physical operator by logical optimizer. \nThis is an essential support.",
        "Issue Links": []
    },
    "MAHOUT-1738": {
        "Key": "MAHOUT-1738",
        "Summary": "Flavor support for H2o in-core matrices",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Anand Avati",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "11/Jun/15 00:14",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "24/Jun/15 18:37",
        "Description": "Could be as simple as initializing with one of standard flavors. \nFeel free to enlist me for help.\nE.g. %*% requires it : \n\nddsvd - naive - q=1 *** FAILED ***\n  java.lang.RuntimeException: water.DException$DistributedException: from /130.35.26.220:54321; by class org.apache.mahout.h2obindings.ops.MapBlock$1MRTaskBMF; class java.lang.UnsupportedOperationException: Flavor support not implemented for this matrix.                                \n  at water.MRTask.getResult(MRTask.java:263)\n  at water.MRTask.doAll(MRTask.java:218)\n  at water.MRTask.doAll(MRTask.java:215)\n  at org.apache.mahout.h2obindings.ops.MapBlock.exec(MapBlock.java:111)\n  at org.apache.mahout.h2obindings.H2OEngine$.tr2phys(H2OEngine.scala:111)\n  at org.apache.mahout.h2obindings.H2OEngine$.toPhysical(H2OEngine.scala:89)\n  at org.apache.mahout.math.drm.logical.CheckpointAction.checkpoint(CheckpointAction.scala:41)\n  at org.apache.mahout.math.decompositions.DSSVD$.dssvd(DSSVD.scala:49)\n  at org.apache.mahout.math.decompositions.package$.dssvd(package.scala:87)\n  at org.apache.mahout.math.decompositions.DistributedDecompositionsSuiteBase$class.dssvdNaive(DistributedDecompositionsSuiteBase.scala:123)\n  ...\n  Cause: water.DException$DistributedException: from /130.35.26.220:54321; by class org.apache.mahout.h2obindings.ops.MapBlock$1MRTaskBMF; class java.lang.UnsupportedOperationException: Flavor support not implemented for this matrix.                                                     \n  at org.apache.mahout.math.AbstractMatrix.getFlavor(AbstractMatrix.java:827)\n  at org.apache.mahout.math.scalabindings.MMul$.apply(MMul.scala:37)\n  at org.apache.mahout.math.scalabindings.RLikeMatrixOps.$percent$times$percent(RLikeMatrixOps.scala:27)\n  at org.apache.mahout.math.decompositions.DSSVD$$anonfun$1.apply(DSSVD.scala:47)\n  at org.apache.mahout.math.decompositions.DSSVD$$anonfun$1.apply(DSSVD.scala:45)\n  at org.apache.mahout.h2obindings.ops.MapBlockHelper$.exec(MapBlockHelper.scala:47)\n  at org.apache.mahout.h2obindings.ops.MapBlockHelper.exec(MapBlockHelper.scala)\n  at org.apache.mahout.h2obindings.ops.MapBlock$1MRTaskBMF.map(MapBlock.java:105)\n  at water.MRTask.compute2(MRTask.java:428)\n  at water.MRTask.compute2(MRTask.java:372)\n  ...",
        "Issue Links": []
    },
    "MAHOUT-1739": {
        "Key": "MAHOUT-1739",
        "Summary": "maxSimilarItemsPerItem param of ItemSimilarityJob doesn't behave correct",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "lariven",
        "Created": "12/Jun/15 14:49",
        "Updated": "22/May/16 00:07",
        "Resolved": "14/Jun/15 08:37",
        "Description": "the output similar items of ItemSimilarityJob for each target item may exceed the number of similar items we set to maxSimilarItemsPerItem  parameter. the following code of ItemSimilarityJob.java about line NO. 200 may affect:\n        if (itemID < otherItemID) \n{\n          ctx.write(new EntityEntityWritable(itemID, otherItemID), new DoubleWritable(similarItem.getSimilarity()));\n        }\n else \n{\n          ctx.write(new EntityEntityWritable(otherItemID, itemID), new DoubleWritable(similarItem.getSimilarity()));\n        }\n\nDon't know why need to switch itemID with otherItemID, but I think a single line is enough:\n          ctx.write(new EntityEntityWritable(itemID, otherItemID), new DoubleWritable(similarItem.getSimilarity()));",
        "Issue Links": []
    },
    "MAHOUT-1740": {
        "Key": "MAHOUT-1740",
        "Summary": "Layout on algorithms page broken",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "16/Jun/15 20:41",
        "Updated": "28/Apr/16 02:38",
        "Resolved": "27/Oct/15 23:54",
        "Description": "http://mahout.apache.org/users/basics/algorithms.html\nOn Chrome on Linux the main body content is bleeding into the right nav.",
        "Issue Links": []
    },
    "MAHOUT-1741": {
        "Key": "MAHOUT-1741",
        "Summary": "MapReduce related; legacy",
        "Type": "Epic",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "18/Jun/15 19:00",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "03/Apr/16 13:27",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1742": {
        "Key": "MAHOUT-1742",
        "Summary": "non-legacy framework related issues",
        "Type": "Epic",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "18/Jun/15 19:01",
        "Updated": "20/Oct/20 12:43",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1743": {
        "Key": "MAHOUT-1743",
        "Summary": "Issues w.r.t algorithms on top of samsara",
        "Type": "Epic",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitriy Lyubimov",
        "Created": "18/Jun/15 19:01",
        "Updated": "09/May/16 22:05",
        "Resolved": "08/Nov/15 02:17",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1744": {
        "Key": "MAHOUT-1744",
        "Summary": "Deprecate lucene2seq",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.11.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "18/Jun/15 20:32",
        "Updated": "11/Aug/15 23:59",
        "Resolved": "02/Aug/15 21:32",
        "Description": "Deprecate lucene2seq and remove all related classes as of  mahout 0.10.2.",
        "Issue Links": []
    },
    "MAHOUT-1745": {
        "Key": "MAHOUT-1745",
        "Summary": "Purge deprecated ConcatVectorsJob from codebase",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.0",
        "Fix Version/s": "0.10.2,                                            0.11.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "18/Jun/15 20:40",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "02/Aug/15 13:11",
        "Description": "ConcatVectorsJob was deprecated in the 0.10.0 release.  Purge it in 0.10.2",
        "Issue Links": []
    },
    "MAHOUT-1746": {
        "Key": "MAHOUT-1746",
        "Summary": "Fix: mxA ^ 2, mxA ^ 0.5 to mean the same thing as mxA * mxA and mxA ::= sqrt _",
        "Type": "Blog - New Blog Request",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "23/Jun/15 20:16",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jul/15 00:33",
        "Description": "it so happens that in java, if x is of double type, Math.pow(x,2.0) and x * x produce different values approximately once in million random values.\nThis is extremely annoying as it creates rounding errors, especially with things like euclidean distance computations, which eventually may produce occasional NaNs. \nThis issue suggests to get special treatment on vector and matrix dsl to make sure identical fpu algorithms are running as follows:\nx ^ 2 <=> x * x\nx ^ 0.5 <=> sqrt",
        "Issue Links": []
    },
    "MAHOUT-1747": {
        "Key": "MAHOUT-1747",
        "Summary": "Mahout DSL for Flink: add support for different types of indexes (String, long, etc)",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.10.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "24/Jun/15 11:00",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 18:39",
        "Description": "For mahout-flink (MAHOUT-1570) rows now can only be indexed with integers. Add support for other types: Long, String, etc. \nSee FlinkEngine#toPhysical",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570",
            "/jira/browse/MAHOUT-1748"
        ]
    },
    "MAHOUT-1748": {
        "Key": "MAHOUT-1748",
        "Summary": "Mahout DSL for Flink: switch to Flink Scala API",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "24/Jun/15 11:04",
        "Updated": "30/Mar/16 17:21",
        "Resolved": "15/Mar/16 02:41",
        "Description": "In Flink-Mahout (MAHOUT-1570) Flink Java API is used because Scala API caused different strange compilation problems. \nBut Scala API handles types better than Flink Java API, so it's better to switch to Scala API. It also can solve MAHOUT-1747",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570",
            "/jira/browse/MAHOUT-1747"
        ]
    },
    "MAHOUT-1749": {
        "Key": "MAHOUT-1749",
        "Summary": "Mahout DSL for Flink: Implement Atx",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.10.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Alexey Grigorev",
        "Created": "24/Jun/15 11:05",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "27/Mar/16 03:26",
        "Description": "Now Atx is implemented through At and Ax operators, but it's not optimal, and its needs special implementation.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1712"
        ]
    },
    "MAHOUT-1750": {
        "Key": "MAHOUT-1750",
        "Summary": "Mahout DSL for Flink: Implement ABt",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.10.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "Flink,                                            Math",
        "Assignee": null,
        "Reporter": "Alexey Grigorev",
        "Created": "24/Jun/15 11:06",
        "Updated": "26/Dec/16 06:43",
        "Resolved": "26/Dec/16 06:43",
        "Description": "Now ABt is expressed through AtB, which is not optimal, and we need to have a special implementation for ABt",
        "Issue Links": [
            "/jira/browse/MAHOUT-1701"
        ]
    },
    "MAHOUT-1751": {
        "Key": "MAHOUT-1751",
        "Summary": "Mahout DSL for Flink: Implement AtA",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.10.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "24/Jun/15 11:07",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 03:21",
        "Description": "Now AtA is implemented via AtB",
        "Issue Links": [
            "/jira/browse/MAHOUT-1701"
        ]
    },
    "MAHOUT-1752": {
        "Key": "MAHOUT-1752",
        "Summary": "H2O implementation of CbindScalar",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Anand Avati",
        "Reporter": "Anand Avati",
        "Created": "24/Jun/15 16:24",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "24/Jun/15 18:38",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1753": {
        "Key": "MAHOUT-1753",
        "Summary": "First and second moment routines",
        "Type": "Blog - New Blog Request",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "24/Jun/15 17:17",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jul/15 00:34",
        "Description": "R analogs: mean(), rowCols(), rowMeans(), variance(), sd().\nSince it presents an efficiency problem to compute second and first moment as two different routines, this issue suggest to unify computation of both first and second moments in the same routine. \nFor completeness, we do both in-core and distriuted variations, by convention prepending distributed versions with \"d\" letter: \ncolMeanVars()\ncolMeanStdevs()\ndcolMeanVars()\ndcolMeanStdevs()",
        "Issue Links": []
    },
    "MAHOUT-1754": {
        "Key": "MAHOUT-1754",
        "Summary": "Distance and squared distance matrix routines",
        "Type": "Blog - New Blog Request",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "24/Jun/15 17:18",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "17/Jul/15 00:34",
        "Description": "R analogs: dist()\nthis issue suggests to provide routine for distance and squared distance matrix computation. \nSince most of the use actually requires or can use square distance routine, we suggest to provide both squared and non-squared eucledian distance matrices. \nBy convention, distributed versions are prepended by (d) letter:\ndist\nsqDist\ndsqDist\nThe points are assumed to be row-wise. \nWe also provide variation for pair-wise distance matrix of two different inputs x and y: \nsqDist(x,y)\ndsqDist(x,y).",
        "Issue Links": []
    },
    "MAHOUT-1755": {
        "Key": "MAHOUT-1755",
        "Summary": "Mahout DSL for Flink: Flush intermediate results to FS",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.10.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "25/Jun/15 13:57",
        "Updated": "30/Mar/16 17:21",
        "Resolved": "23/Mar/16 22:04",
        "Description": "Now Flink (unlike Spark) doesn't keep intermediate results in memory - therefore they should be flushed to a file system, and read back when required.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1756": {
        "Key": "MAHOUT-1756",
        "Summary": "Missing +=: and *=: operators on vectors",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2,                                            0.11.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "17/Jul/15 00:25",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "31/Jul/15 20:19",
        "Description": "Matrices support expressions like a *=: b (do elementwise a*b on b in-place) but vectors don't. sometimes it is highly desirable to do that type of expression, and for consistency, we should enable this for vectors too.",
        "Issue Links": []
    },
    "MAHOUT-1757": {
        "Key": "MAHOUT-1757",
        "Summary": "Small fix in spca formula",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.10.2,                                            0.11.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "17/Jul/15 00:28",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "31/Jul/15 20:18",
        "Description": "there's small error in the spca, Y correction inside power iterations should really be -(s_b-xi'xi*s_b). It doesn't seem to affect much the result but this is a small error (~7-th-8th digit after dot) it introduces when power iterations are used.",
        "Issue Links": []
    },
    "MAHOUT-1758": {
        "Key": "MAHOUT-1758",
        "Summary": "mahout spark-shell - get illegal acces error at startup",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.11.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "JP Bordenave",
        "Created": "17/Jul/15 21:28",
        "Updated": "09/Nov/16 18:33",
        "Resolved": "09/Aug/15 02:40",
        "Description": "Hello,\ni installed hadoop 2.6,  spark 1.4 ,sparkR,pyspark working fine, no issue\nscala 2.10\nnow i try to configure mahout with my cluster spark/hadoop, but when i start\nmahout, i get illegalaccesseror, it try tot start in local mode, i get same error, look to be incompatible with spark 1.4.x and mahout 10.1, \ncan you confirm ? patch ?\nedit: i saw in release note mahout 10.1, compatibilty 1.2.2 or less\nThanks for your info\nJP\ni set my variable nd my cluster spark\nexport SPARK_HOME=/usr/local/spark\nexport MASTER=spark://stargate:7077\n\nhduser@stargate:~$ mahout spark-shell\nMAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/usr/local/apache-mahout-distribution-0.10.1/mahout-examples-0.10.1-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/apache-mahout-distribution-0.10.1/mahout-mr-0.10.1-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/spark-1.4.1-bin-hadoop2.6/lib/spark-assembly-1.4.1-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/usr/local/apache-mahout-distribution-0.10.1/lib/slf4j-log4j12-1.7.12.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n15/07/17 23:17:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n                         _                 _\n         _ __ ___   __ _| |__   ___  _   _| |_\n        | '_ ` _ \\ / _` | '_ \\ / _ \\| | | | __|\n        | | | | | | (_| | | | | (_) | |_| | |_\n        |_| |_| |_|\\__,_|_| |_|\\___/ \\__,_|\\__|  version 0.10.0\n\n\nUsing Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.7.0_79)\nType in expressions to have them evaluated.\nType :help for more information.\njava.lang.IllegalAccessError: tried to access method org.apache.spark.repl.SparkIMain.classServer()Lorg/apache/spark/HttpServer; from class org.apache.mahout.sparkbindings.shell.MahoutSparkILoop\n        at org.apache.mahout.sparkbindings.shell.MahoutSparkILoop.createSparkContext(MahoutSparkILoop.scala:42)\n        at $iwC$$iwC.<init>(<console>:11)\n        at $iwC.<init>(<console>:18)\n        at <init>(<console>:20)\n        at .<init>(<console>:24)\n        at .<clinit>(<console>)\n        at .<init>(<console>:7)\n        at .<clinit>(<console>)\n        at $print(<console>)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)\n        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)\n        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)\n        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)\n        at org.apache.mahout.sparkbindings.shell.MahoutSparkILoop$$anonfun$initializeSpark$1.apply$mcV$sp(MahoutSparkILoop.scala:63)\n        at org.apache.mahout.sparkbindings.shell.MahoutSparkILoop$$anonfun$initializeSpark$1.apply(MahoutSparkILoop.scala:62)\n        at org.apache.mahout.sparkbindings.shell.MahoutSparkILoop$$anonfun$initializeSpark$1.apply(MahoutSparkILoop.scala:62)\n        at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)\n        at org.apache.mahout.sparkbindings.shell.MahoutSparkILoop.initializeSpark(MahoutSparkILoop.scala:62)\n        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)\n        at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:157)\n        at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)\n        at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)\n        at org.apache.mahout.sparkbindings.shell.MahoutSparkILoop.postInitialization(MahoutSparkILoop.scala:24)\n        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:991)\n        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)\n        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)\n        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)\n        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)\n        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)\n        at org.apache.mahout.sparkbindings.shell.Main$.main(Main.scala:39)\n        at org.apache.mahout.sparkbindings.shell.Main.main(Main.scala)\n\nMahout distributed context is available as \"implicit val sdc\".",
        "Issue Links": []
    },
    "MAHOUT-1759": {
        "Key": "MAHOUT-1759",
        "Summary": "Deprecate random forest",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Examples",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "04/Aug/15 04:01",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "06/Nov/15 05:19",
        "Description": "Remove create-rf-data.sh and run-rf.sh, then remove random forest code.",
        "Issue Links": []
    },
    "MAHOUT-1760": {
        "Key": "MAHOUT-1760",
        "Summary": "Modify Build to enforce specified target JDK version",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.10.2,                                            0.11.0",
        "Fix Version/s": "0.10.2",
        "Component/s": "build",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Suneel Marthi",
        "Created": "05/Aug/15 21:07",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "07/Aug/15 14:22",
        "Description": "Modify poms to enforce specified target JDK when building the project",
        "Issue Links": [
            "/jira/browse/MAHOUT-1761"
        ]
    },
    "MAHOUT-1761": {
        "Key": "MAHOUT-1761",
        "Summary": "Upgrade to Apache parent pom v17",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.10.1",
        "Fix Version/s": "0.10.2",
        "Component/s": "None",
        "Assignee": "Stevo Slavi\u0107",
        "Reporter": "Stevo Slavi\u0107",
        "Created": "05/Aug/15 22:08",
        "Updated": "07/Aug/15 15:56",
        "Resolved": "06/Aug/15 00:05",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1760"
        ]
    },
    "MAHOUT-1762": {
        "Key": "MAHOUT-1762",
        "Summary": "Pick up $SPARK_HOME/conf/spark-defaults.conf on startup",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "spark",
        "Assignee": "Trevor Grant",
        "Reporter": "Sergey Tryuber",
        "Created": "10/Aug/15 15:18",
        "Updated": "19/Dec/17 01:13",
        "Resolved": "17/Mar/16 15:21",
        "Description": "spark-defaults.conf is aimed to contain global configuration for Spark cluster. For example, in our HDP2.2 environment it contains:\n\nspark.driver.extraJavaOptions      -Dhdp.version=2.2.0.0\u20132041\nspark.yarn.am.extraJavaOptions     -Dhdp.version=2.2.0.0\u20132041\n\n\nand there are many other good things. Actually it is expected that when a user starts Spark Shell, it will be working fine. Unfortunately this does not happens with Mahout Spark Shell, because it ignores spark configuration and user has to copy-past lots of options into MAHOUT_OPTS.\nThis happens because org.apache.mahout.sparkbindings.shell.Main is executed directly in initialization script:\n\n\"$JAVA\" $JAVA_HEAP_MAX $MAHOUT_OPTS -classpath \"$CLASSPATH\" \"org.apache.mahout.sparkbindings.shell.Main\" $@\n\n\nIn contrast, in Spark shell is indirectly invoked through spark-submit in spark-shell script:\n\n\"$FWDIR\"/bin/spark-submit --class org.apache.spark.repl.Main \"$@\"\n\n\nSparkSubmit contains an additional initialization layer for loading properties file (see SparkSubmitArguments#mergeDefaultSparkProperties method).\nSo there are two possible solutions:\n\nuse proper Spark-like initialization logic\nuse thin envelope like it is in H2O Sparkling Water (sparkling-shell)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1951",
            "https://github.com/apache/mahout/pull/292"
        ]
    },
    "MAHOUT-1763": {
        "Key": "MAHOUT-1763",
        "Summary": "A minor bug in Spark binding documentation",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math,                                            spark",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Sergey Tryuber",
        "Created": "11/Aug/15 12:22",
        "Updated": "19/Mar/16 00:12",
        "Resolved": "19/Mar/16 00:12",
        "Description": "Documentation has following example:\n\ndiag(10, 3.5)\n\n\nBut in fact it should be:\n\ndiag(3.5, 10)\n\n\nSee package.scala for more details.",
        "Issue Links": []
    },
    "MAHOUT-1764": {
        "Key": "MAHOUT-1764",
        "Summary": "Mahout DSL for Flink: Add standard backend tests for Flink",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "11/Aug/15 13:25",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "15/Mar/16 02:53",
        "Description": "From github comment by Dmitriy:\nalso on the topic of test suite coverage: we need to pass our standard tests. The base clases for them are:\nhttps://github.com/apache/mahout/blob/master/math-scala/src/test/scala/org/apache/mahout/math/decompositions/DistributedDecompositionsSuiteBase.scala\nhttps://github.com/apache/mahout/blob/master/math-scala/src/test/scala/org/apache/mahout/math/drm/DrmLikeOpsSuiteBase.scala\nhttps://github.com/apache/mahout/blob/master/math-scala/src/test/scala/org/apache/mahout/math/drm/DrmLikeSuiteBase.scala\nhttps://github.com/apache/mahout/blob/master/math-scala/src/test/scala/org/apache/mahout/math/drm/RLikeDrmOpsSuiteBase.scala\nThe technique here is to take these test cases as a base class for a distributed test case (you may want to see how it was done for Spark and H2O). This is our basic assertion that our main algorithms are passing on a toy problem for a given backend.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1765": {
        "Key": "MAHOUT-1765",
        "Summary": "Mahout DSL for Flink: Add some documentation about Flink backend",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Alexey Grigorev",
        "Created": "11/Aug/15 13:30",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "08/Apr/16 21:59",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1779",
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1766": {
        "Key": "MAHOUT-1766",
        "Summary": "Increase default PermGen size for spark-shell",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Mahout spark shell",
        "Assignee": "Andrew Musselman",
        "Reporter": "Sergey Tryuber",
        "Created": "13/Aug/15 15:42",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "09/Apr/16 00:11",
        "Description": "Mahout spark-shell is run with default perm gen size (64MB). Taking into account that it depends on lots of external jars and the whole count of used Java classes is very large, we constantly observe spontaneous corresponding OOM exceptions.\nA hot fix from our side is to modify envelope bash script (added -XX:PermSize=512m):\n\n\"$JAVA\" $JAVA_HEAP_MAX -XX:PermSize=512m $MAHOUT_OPTS -classpath \"$CLASSPATH\" \"org.apache.mahout.sparkbindings.shell.Main\" $@\n\n\nOf course, more elegant solution is needed. After the applied fix, the errors had gone.",
        "Issue Links": []
    },
    "MAHOUT-1767": {
        "Key": "MAHOUT-1767",
        "Summary": "Unable to run tests on H2O enigne in distributed mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "Documentation",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Dmitry Yaraev",
        "Created": "14/Aug/15 06:32",
        "Updated": "26/Dec/16 06:45",
        "Resolved": "26/Dec/16 06:45",
        "Description": "When one follows the instructions located in README.md for H2O module and tries to run tests in the distributed mode, tests run only in the local mode. There are three steps in the instruction:\n\n\nhost-1:~/mahout$ ./bin/mahout h2o-node\n...\n.. INFO: Cloud of size 1 formed [/W.X.Y.Z:54321]\n\n\n\nhost-2:~/mahout$ ./bin/mahout h2o-node\n...\n.. INFO: Cloud of size 2 formed [/A.B.C.D:54322]\n\n\n\nhost-N:~/mahout/h2o$ mvn test\n...\n.. INFO: Cloud of size 3 formed [/E.F.G.H:54323]\n...\nAll tests passed.\n...\nhost-N:~/mahout/h2o$\n\n\n\nFirst two steps are for executing worker nodes. The last one is for executing tests. According to the instruction, after launching tests one more worker is started. And it should join to the same cloud which other worker nodes forms. But it does joined them because it has a different cloud name (or masterURL in terms of the code). If you look in the code, you can find the following:\nDistributedH2OSuite.scala\n...\nmahoutCtx = mahoutH2OContext(\"mah2out\" + System.currentTimeMillis())\n...\n\n\nAfter we removed the code which appends current time to cloud name, it started to work.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1768"
        ]
    },
    "MAHOUT-1768": {
        "Key": "MAHOUT-1768",
        "Summary": "ClassNotFoundException when running tests on H2O in distributed mode",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Dmitry Yaraev",
        "Created": "14/Aug/15 06:40",
        "Updated": "03/Mar/18 21:29",
        "Resolved": null,
        "Description": "We followed instructions described in in README.md for H2O module. When tests are executed in the distributed mode, one of the worker nodes may fail with ClassNotFoundException, because they cannot found some of the test bindings, in particular RLikeDrmOpsSuiteBase.scala. The problem can be avoided by building Mahout on each of the worker nodes with:\n\nmvn clean install",
        "Issue Links": [
            "/jira/browse/MAHOUT-1767"
        ]
    },
    "MAHOUT-1769": {
        "Key": "MAHOUT-1769",
        "Summary": "Incorrect documentation for collecting DRM to HDFS",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Mahout spark shell",
        "Assignee": "Andrew Musselman",
        "Reporter": "Sergey Tryuber",
        "Created": "17/Aug/15 13:23",
        "Updated": "19/Mar/16 00:06",
        "Resolved": "19/Mar/16 00:05",
        "Description": "There is a bug in documenation (2.3.5 Collecting to HDFS). Instead of:\n\nA.writeDRM(path = hdfsPath)\n\n\nshould be\n\nA.dfsWrite(path = \"<non_existing_hdfs_directory>\")",
        "Issue Links": []
    },
    "MAHOUT-1770": {
        "Key": "MAHOUT-1770",
        "Summary": "Can mahout run on spark with yarn-cluster mode?",
        "Type": "Question",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.2",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "dodolzg",
        "Created": "06/Sep/15 10:33",
        "Updated": "12/Mar/16 23:28",
        "Resolved": "12/Mar/16 23:28",
        "Description": "I'm using mahout 0.11.0 on spark 1.3.0.\nIt works fine with spark standalone, local, yarn-client, while it fails with yarn-cluster mode.  \nMy shell command is \n\n    mahout spark-itemsimilarity \\  \n        -i cf-data \\  \n        -o item-sim-out \\  \n        -ma yarn-cluster \\  \n        --filter1 purchase \\  \n        --filter2 view \\  \n        -ic 2 \\  \n        -rc 0 \\  \n        -fc 1  \nThe error log:  \n\n    MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.  \n    SLF4J: Class path contains multiple SLF4J bindings.  \n    SLF4J: Found binding in [jar:file:/root/apache-mahout-distribution-0.11.0/mahout-examples-0.11.0-job.jar!/org/slf4j/impl/StaticLoggerBinder.class] \n    SLF4J: Found binding in [jar:file:/root/apache-mahout-distribution-0.11.0/mahout-mr-0.11.0-job.jar!/org/slf4j/impl/StaticLoggerBinder.class] \n    SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] \n    SLF4J: Found binding in [jar:file:/root/mahout-distribution-0.9/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class] \n    SLF4J: Found binding in [jar:file:/root/apache-mahout-distribution-0.11.0/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] \n    SLF4J: See SLF4J Error Codes for an explanation.  \n    SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory] \n    15/08/27 10:20:37 INFO SparkContext: Running Spark version 1.3.0  \n    15/08/27 10:20:37 INFO SecurityManager: Changing view acls to: root  \n    15/08/27 10:20:37 INFO SecurityManager: Changing modify acls to: root  \n    15/08/27 10:20:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(root); users with modify permissions: Set(root)  \n    15/08/27 10:20:38 INFO Slf4jLogger: Slf4jLogger started  \n    15/08/27 10:20:38 INFO Remoting: Starting remoting  \n    15/08/27 10:20:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@TestCent7:45133] \n    15/08/27 10:20:38 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkDriver@TestCent7:45133] \n    15/08/27 10:20:38 INFO Utils: Successfully started service 'sparkDriver' on port 45133.  \n    15/08/27 10:20:38 INFO SparkEnv: Registering MapOutputTracker  \n    15/08/27 10:20:38 INFO SparkEnv: Registering BlockManagerMaster  \n    15/08/27 10:20:38 INFO DiskBlockManager: Created local directory at /tmp/spark-1172a284-0c8c-40ee-9cdf-c5231d8c0974/blockmgr-a142fd9e-3c4c-46d8-ad8f-0071fb31f04b  \n    15/08/27 10:20:38 INFO MemoryStore: MemoryStore started with capacity 1966.1 MB  \n    15/08/27 10:20:38 INFO HttpFileServer: HTTP File server directory is /tmp/spark-c8e3861e-3822-4932-a750-01bb0fc648b9/httpd-504b3f69-834d-4618-939d-b4b089d61378  \n    15/08/27 10:20:38 INFO HttpServer: Starting HTTP Server  \n    15/08/27 10:20:38 INFO Server: jetty-8.y.z-SNAPSHOT  \n    15/08/27 10:20:38 INFO AbstractConnector: Started SocketConnector@0.0.0.0:37966  \n    15/08/27 10:20:38 INFO Utils: Successfully started service 'HTTP file server' on port 37966.  \n    15/08/27 10:20:38 INFO SparkEnv: Registering OutputCommitCoordinator  \n    15/08/27 10:20:38 INFO Server: jetty-8.y.z-SNAPSHOT  \n    15/08/27 10:20:38 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040  \n    15/08/27 10:20:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.  \n    15/08/27 10:20:38 INFO SparkUI: Started SparkUI at Page on testcent7:4040  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-hdfs-0.11.0.jar at Page on 168.70.241:37966 with timestamp 1440642038652  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-math-0.11.0.jar at Page on 168.70.241:37966 with timestamp 1440642038654  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-math-scala_2.10-0.11.0.jar at Page on 168.70.241:37966 with timestamp 1440642038656  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-spark_2.10-0.11.0-dependency-reduced.jar at Page on 168.70.241:37966 with timestamp 1440642038661  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-spark_2.10-0.11.0.jar at Page on 168.70.241:37966 with timestamp 1440642038662  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-hdfs-0.11.0.jar at Page on 168.70.241:37966 with timestamp 1440642038662  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-math-0.11.0.jar at Page on 168.70.241:37966 with timestamp 1440642038665  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-math-scala_2.10-0.11.0.jar at Page on 168.70.241:37966 with timestamp 1440642038667  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-spark_2.10-0.11.0-dependency-reduced.jar at Page on 168.70.241:37966 with timestamp 1440642038674  \n    15/08/27 10:20:38 INFO SparkContext: Added JAR /root/apache-mahout-distribution-0.11.0/mahout-spark_2.10-0.11.0.jar at Page on 168.70.241:37966 with timestamp 1440642038676  \n    15/08/27 10:20:38 INFO YarnClusterScheduler: Created YarnClusterScheduler  \n    15/08/27 10:20:38 ERROR YarnClusterSchedulerBackend: Application ID is not set.  \n    15/08/27 10:20:38 INFO NettyBlockTransferService: Server created on 52390  \n    15/08/27 10:20:38 INFO BlockManagerMaster: Trying to register BlockManager  \n    15/08/27 10:20:38 INFO BlockManagerMasterActor: Registering block manager TestCent7:52390 with 1966.1 MB RAM, BlockManagerId(<driver>, TestCent7, 52390)  \n    15/08/27 10:20:38 INFO BlockManagerMaster: Registered BlockManager  \n    Exception in thread \"main\" java.lang.NullPointerException  \n    at org.apache.spark.deploy.yarn.ApplicationMaster$.sparkContextInitialized(ApplicationMaster.scala:580)  \n    at org.apache.spark.scheduler.cluster.YarnClusterScheduler.postStartHook(YarnClusterScheduler.scala:32)  \n    at org.apache.spark.SparkContext.<init>(SparkContext.scala:541)  \n    at org.apache.mahout.sparkbindings.package$.mahoutSparkContext(package.scala:91)  \n    at org.apache.mahout.drivers.MahoutSparkDriver.start(MahoutSparkDriver.scala:83)  \n    at org.apache.mahout.drivers.ItemSimilarityDriver$.start(ItemSimilarityDriver.scala:118)  \n    at org.apache.mahout.drivers.ItemSimilarityDriver$.process(ItemSimilarityDriver.scala:199)  \n    at org.apache.mahout.drivers.ItemSimilarityDriver$$anonfun$main$1.apply(ItemSimilarityDriver.scala:112)  \n    at org.apache.mahout.drivers.ItemSimilarityDriver$$anonfun$main$1.apply(ItemSimilarityDriver.scala:110)  \n    at scala.Option.map(Option.scala:145)  \n    at org.apache.mahout.drivers.ItemSimilarityDriver$.main(ItemSimilarityDriver.scala:110)  \n    at org.apache.mahout.drivers.ItemSimilarityDriver.main(ItemSimilarityDriver.scala)  \nThe profile:  \n\n    export JAVA_HOME=/usr/java/jdk1.7.0_60/  \n    export HADOOP_HOME=/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop  \n    export HADOOP_CONF_DIR=/etc/hadoop/conf  \n    export HADOOP_YARN_HOME=/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/hadoop-yarn  \n    export HADOOP_YARN_CONF_DIR=/etc/hadoop/conf.cloudera.yarn\n    export SPARK_HOME=/opt/cloudera/parcels/CDH-5.4.4-1.cdh5.4.4.p0.4/lib/spark  \n    export MAHOUT_HOME=/root/apache-mahout-distribution-0.11.0  \n    export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar  \n    export PATH=$HADOOP_HOME/bin:$JAVA_HOME/bin:$SPARK_HOME/bin:$PATH  \nI have no idea to do with this matter, please help me, Thanks!",
        "Issue Links": []
    },
    "MAHOUT-1771": {
        "Key": "MAHOUT-1771",
        "Summary": "Cluster dumper omits indices and 0 elements for dense vector or sparse containing 0s",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9",
        "Fix Version/s": "0.11.1",
        "Component/s": "Clustering,                                            mrlegacy",
        "Assignee": "Suneel Marthi",
        "Reporter": "Sean R. Owen",
        "Created": "08/Sep/15 08:38",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "08/Sep/15 23:38",
        "Description": "(EDIT: fixed incorrect analysis)\nBlast from the past \u2013 are patches still being accepted for \"mrlegacy\" code? Something turned up incidentally when working with a customer that looks like a minor bug in the cluster dumper code.\nIn AbstractCluster.java:\n\npublic static List<Object> formatVectorAsJson(Vector v, String[] bindings) throws IOException {\n\n    boolean hasBindings = bindings != null;\n    boolean isSparse = !v.isDense() && v.getNumNondefaultElements() != v.size();\n\n    // we assume sequential access in the output\n    Vector provider = v.isSequentialAccess() ? v : new SequentialAccessSparseVector(v);\n\n    List<Object> terms = new LinkedList<>();\n    String term = \"\";\n\n    for (Element elem : provider.nonZeroes()) {\n\n      if (hasBindings && bindings.length >= elem.index() + 1 && bindings[elem.index()] != null) {\n        term = bindings[elem.index()];\n      } else if (hasBindings || isSparse) {\n        term = String.valueOf(elem.index());\n      }\n\n      Map<String, Object> term_entry = new HashMap<>();\n      double roundedWeight = (double) Math.round(elem.get() * 1000) / 1000;\n      if (hasBindings || isSparse) {\n        term_entry.put(term, roundedWeight);\n        terms.add(term_entry);\n      } else {\n        terms.add(roundedWeight);\n      }\n    }\n\n    return terms;\n  }\n\n\nThe problem is that this never outputs any elements of a vector with value 0, but, also doesn't print indices in some cases. This means the output is smaller than the number of dimensions, but it's not possible to know where the omitted 0s are.\nIt will not output indices if the vector is a dense vector, or if the number of non-default elements is the same as the size (which includes sparse vectors even containing 0 values, if they have been set explicitly). However the iteration is over non-zero elements only. \nThe fix seems to be to print indices if the number of non-zero elements is less than size, for any vector:\n\n    boolean isSparse = v.getNumZeroElements() != v.size();\n\n\nPretty straightforward, and minor, but wanted to check with everyone before making a change.",
        "Issue Links": []
    },
    "MAHOUT-1772": {
        "Key": "MAHOUT-1772",
        "Summary": "delimiterPattern instance variable of FileDataModel should be marked as a Transient field",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Suneel Marthi",
        "Reporter": "Lina Hovanessian",
        "Created": "11/Sep/15 08:36",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "05/Nov/15 03:51",
        "Description": "I am trying to implement a recommender using Mahout. To create the DataModel I am using FileDataModel type. After creating the Recommender object , I want to save the model for later usage , just like what we have in Spark models.\nFileDataModel is implementing and inheriting Serializable classes , so this means that it can be Serialized. However  when I try to write it the \"java.io.NotSerializableException: com.google.common.base.Splitter\" exception is thrown.\nThe root cause of this exception is \"private final Splitter delimiterPattern;\" variable of FileDataModel class. It is a non-serializable third party class which is not marked as a transient field in the FileDataModel class, as a result of this we get the exception mentioned above.\nAs a workaround , I created my own \"MyFileDataModel\" which is an exact copy of \"FileDataModel\" class but the \"delimiterPattern\" variable is marked as transient and it works perfectly. It would be better to mark the field as transient in the original class.",
        "Issue Links": []
    },
    "MAHOUT-1773": {
        "Key": "MAHOUT-1773",
        "Summary": "Fix cluster-syntheticcontrol.sh for HDFS synthax",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Clustering",
        "Assignee": "Andrew Musselman",
        "Reporter": "Eduardo Niemeyer",
        "Created": "16/Sep/15 01:47",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "16/Sep/15 01:48",
        "Description": "Added '/' to some lines, so it can check the root of the HDFS, where it's possible to use -mkdir, -put and others. Otherwise the example keeps failing (Tested on Hadoop 2.4.1).",
        "Issue Links": []
    },
    "MAHOUT-1774": {
        "Key": "MAHOUT-1774",
        "Summary": "Enable maven compilation with JDK 1.8",
        "Type": "Dependency upgrade",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "29/Sep/15 19:28",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 20:29",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1775": {
        "Key": "MAHOUT-1775",
        "Summary": "FileNotFoundException caused by aborting the process of downloading Wikipedia dataset",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.1",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Bowei Zhang",
        "Created": "21/Oct/15 06:28",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "25/Oct/15 04:43",
        "Description": "When running the script examples/bin/classify-wikipedia.sh for the first time, it will create a wikixml folder and starts fetching data via curl. If this downloading process is aborted, then in the future when the script is run, it won't extract the .bz2 file (since extracion is guarded by the condition where wikixml doesn't exist) and starts to run Mahout, which will definately end up with throwing up a FileNotFoundException.",
        "Issue Links": []
    },
    "MAHOUT-1776": {
        "Key": "MAHOUT-1776",
        "Summary": "Refactor common Engine agnostic classes to Math-Scala module",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "22/Oct/15 14:33",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "23/Oct/15 22:57",
        "Description": "Refactor common classes that are used across all backend execution engines to mahout-scala.\nSome of the classes being VectorKryoSerializer, GenericMatrixKryoSerializer.\nThis is needed as these classes are common to \n{Spark, H2O, Flink}\n.bindings.",
        "Issue Links": []
    },
    "MAHOUT-1777": {
        "Key": "MAHOUT-1777",
        "Summary": "move HDFSUtil classes into the HDFS module",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "22/Oct/15 14:59",
        "Updated": "28/Apr/16 02:26",
        "Resolved": "24/Oct/15 02:03",
        "Description": "The HDFSUtil classes are used by spark, h2o and flink and implemented in each module.  Move them to the common HDFS module.  The spark implementation includes a  delete(path: String) method used by the Spark Naive Bayes CLI otherwise the others are nearly identical.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1733"
        ]
    },
    "MAHOUT-1778": {
        "Key": "MAHOUT-1778",
        "Summary": "MAhout Spark Shell doesn't work with Spark > 1.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Mahout spark shell",
        "Assignee": "Pat Ferrel",
        "Reporter": "Suneel Marthi",
        "Created": "22/Oct/15 16:11",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "05/Nov/15 03:32",
        "Description": "Mahout Spark Shell uses compute-classpath.sh from Spark for loading up the shell. The compute-classpath.sh script was removed from Spark 1.4 and above. The Mahout Spark Shell code needs to be fixed to handle the changes in Spark 1.4.",
        "Issue Links": []
    },
    "MAHOUT-1779": {
        "Key": "MAHOUT-1779",
        "Summary": "Brief overview page for the Flink engine",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "24/Oct/15 02:14",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "08/Apr/16 18:41",
        "Description": "Add a brief  overview page for the Flink bindings similar to the h2o bindings writeup found at:\nhttp://mahout.apache.org/users/environment/h2o-internals.html\nThe page would go under the \"Engines\" section in the \"Mahout Environment\" menu on the site.\nTopics would include: a brief overview of the engine, how the Drm API is backed backed by the flink engine, interoperability with the engine-native datastructures and methods and any other pertinent information.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1765"
        ]
    },
    "MAHOUT-1780": {
        "Key": "MAHOUT-1780",
        "Summary": "Multi-threaded Matrix Multiplication is slower than Single-thread variant",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.10.0,                                            0.10.1,                                            0.10.2,                                            0.11.0",
        "Fix Version/s": "0.11.1,                                            0.12.0",
        "Component/s": "Math",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Suneel Marthi",
        "Created": "25/Oct/15 11:34",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "25/Oct/15 20:36",
        "Description": "Capturing the Conversation on the subject here:\n\nTurns out that matrix view traversal (of dense matrices, anyway) is 4 times slower than regular matrix traversal in the same direction. I.e.\n\nAd %*% Bd: (106.33333333333333,85.0)\nAd(r,::) %*% Bd: (356.0,328.0)\n\nwhere r=0 until Ad.nrow.\n\nOn investigating MatrixView, it does report correct matrix flavor (as the owner's) and correct algorithm is selected (the same as for the row above). MatrixView gives an indirection(sometimes even double indirection) but it still doesn't explain the 4x performance degrade. It should not be that much different from transpose view overhead, and transpose view overhead is very small in the tests (compared to the rest of the cost)\n\nThe main difference seems to be that the algorithm over matrices ends up doing a dot over DenseVector and a DenseVector (even that the wrapper object is created inside the row iterations) whereas the inefficient algorithm does the same over VectorView wrappers. I wonder if VectorView has not been equipped to pass on the flavors of its backing vector to the vector-vector optimization.\n\nApparently the dot algorithm on vector view goes to the in-core vector-vector optimization framework (calls aggregate()) but denseVector applies custom iteration. Hence it may boil down to experiments of avec dot bvec vs. avec(::) dot bvec(::).",
        "Issue Links": []
    },
    "MAHOUT-1781": {
        "Key": "MAHOUT-1781",
        "Summary": "Dense matrix view multiplication is 4x slower than non-view one",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Dmitriy Lyubimov",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "25/Oct/15 20:27",
        "Updated": "11/Apr/16 08:42",
        "Resolved": "27/Oct/15 07:26",
        "Description": "if mxA and mxB are two in-core DenseMatrix matrices, then \nmxA(::,: %% mxB(::,: takes 4x the time of mxA %% mxB.\npossibly an issue of dot products on VectorViews vs. DenseVectors.\ndot product over DenseVectors seems to not to go through aggregate() cost-optimized framework.",
        "Issue Links": []
    },
    "MAHOUT-1782": {
        "Key": "MAHOUT-1782",
        "Summary": "Remove code for lucene2seq",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "04/Nov/15 00:39",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "04/Nov/15 01:34",
        "Description": "lucene2seq was marked deprecated in 0.11.0, remove lucene2seq from the upcoming 0.11.1 release.",
        "Issue Links": []
    },
    "MAHOUT-1783": {
        "Key": "MAHOUT-1783",
        "Summary": "Remove code for ConcatVectors Job",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.1",
        "Component/s": "Integration",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "04/Nov/15 00:42",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "04/Nov/15 01:23",
        "Description": "CancatVectorsJob has been deprecated as of 0.11.0, remove the relevant code for the upcoming 0.11.1 release.",
        "Issue Links": []
    },
    "MAHOUT-1784": {
        "Key": "MAHOUT-1784",
        "Summary": "Remove mention of Spark in comment and Javadoc of math-scala when the code is generic",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Henry Saputra",
        "Created": "05/Nov/15 22:47",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "05/Nov/15 23:17",
        "Description": "Remove mention of Spark in comment and Javadoc of math-scala module when the code is generic. This is because now mahout-samsara could support H2O and next Apache Flink as exec engines.",
        "Issue Links": []
    },
    "MAHOUT-1785": {
        "Key": "MAHOUT-1785",
        "Summary": "Replace 'spark.kryoserializer.buffer.mb' from Spark config",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.11.2",
        "Component/s": "Mahout spark shell",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "06/Nov/15 08:01",
        "Updated": "11/Apr/16 08:42",
        "Resolved": "07/Nov/15 04:01",
        "Description": "'spark.kryoserializer.buffer.mb' has been deprecated as of spark 1.4 and should be replaced by 'spark.kryoserializer.buffer'",
        "Issue Links": []
    },
    "MAHOUT-1786": {
        "Key": "MAHOUT-1786",
        "Summary": "Make classes implements Serializable for Spark 1.5+",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "Math",
        "Assignee": "Pat Ferrel",
        "Reporter": "Michel Lemay",
        "Created": "06/Nov/15 15:08",
        "Updated": "25/Jun/20 20:00",
        "Resolved": null,
        "Description": "Spark 1.5 comes with a new very efficient serializer that uses code generation.  It is twice as fast as kryo.  When using mahout, we have to set KryoSerializer because some classes aren't serializable otherwise.  \nI suggest to declare Math classes as \"implements Serializable\" where needed.  For instance, to use coocurence package in spark 1.5, we had to modify AbstractMatrix, AbstractVector, DenseVector and SparseRowMatrix to make it work without Kryo.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1892",
            "https://github.com/apache/mahout/pull/174"
        ]
    },
    "MAHOUT-1787": {
        "Key": "MAHOUT-1787",
        "Summary": "compute-classpath scripts are not being packaged in the binary distribution",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.11.1",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Nov/15 15:56",
        "Updated": "07/Nov/15 02:32",
        "Resolved": "06/Nov/15 17:55",
        "Description": "The new compute-classpath.sh  mahout-load-spark-env.sh  and  mahout-spark-class.sh scripts are not being packaged in the /bin directory of the binary distribution of Mahout 0.11.1.",
        "Issue Links": []
    },
    "MAHOUT-1788": {
        "Key": "MAHOUT-1788",
        "Summary": "spark-itemsimilarity integration test script cleanup",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "cooccurrence",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "06/Nov/15 23:44",
        "Updated": "19/Dec/16 06:03",
        "Resolved": null,
        "Description": "binary release does not contain data for itemsimilarity tests, neith binary nor source versions will run on a cluster unless data is hand copied to hdfs.\nClean this up so it copies data if needed and the data is in both versions.",
        "Issue Links": []
    },
    "MAHOUT-1789": {
        "Key": "MAHOUT-1789",
        "Summary": "Remove DataSetOps.scala from mahout-flink, replace by DataSetUtils from Flink 0.10",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.0",
        "Fix Version/s": "0.12.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "08/Nov/15 04:09",
        "Updated": "30/Mar/16 17:21",
        "Resolved": "11/Jan/16 00:11",
        "Description": "DataSetOps#zipWithIndex is not needed anymore in light of DataSetUtils available in Flink 0.10+.  \nDataSetUtils.\n{java, scala}\n from Flink 0.10+ provides methods - zipWithIndex(), zipWithUniqueId(), sample(), sampleWithSize(); all of which are needed to support Mahout-Flink backend.",
        "Issue Links": []
    },
    "MAHOUT-1790": {
        "Key": "MAHOUT-1790",
        "Summary": "SparkEngine nnz overflow resultSize when reducing.",
        "Type": "Bug",
        "Status": "Reopened",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "spark",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Michel Lemay",
        "Created": "12/Nov/15 15:59",
        "Updated": "03/Mar/18 21:21",
        "Resolved": null,
        "Description": "When counting numNonZeroElementsPerColumn in spark engine with large number of columns, we get the following error:\nERROR TaskSetManager: Total size of serialized results of nnn tasks (1031.7 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\nand then, the call stack:\norg.apache.spark.SparkException: Job aborted due to stage failure: Total size of serialized results of 267 tasks (1024.1 MB) is bigger than spark.driver.maxResultSize (1024.0 MB)\n        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)\n        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)\n        at scala.Option.foreach(Option.scala:236)\n        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)\n        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)\n        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)\n        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1942)\n        at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n        at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)\n        at org.apache.spark.rdd.RDD.reduce(RDD.scala:985)\n        at org.apache.mahout.sparkbindings.SparkEngine$.numNonZeroElementsPerColumn(SparkEngine.scala:86)\n        at org.apache.mahout.math.drm.CheckpointedOps.numNonZeroElementsPerColumn(CheckpointedOps.scala:37)\n        at org.apache.mahout.math.cf.SimilarityAnalysis$.sampleDownAndBinarize(SimilarityAnalysis.scala:286)\n        at org.apache.mahout.math.cf.SimilarityAnalysis$.cooccurrences(SimilarityAnalysis.scala:66)\n        at org.apache.mahout.math.cf.SimilarityAnalysis$.cooccurrencesIDSs(SimilarityAnalysis.scala:141)\nThis occurs because it uses a DenseVector and spark seemingly aggregate all of them on the driver before reducing.  \nI think this could be easily prevented with a treeReduce(_ += , depth)  instead of a reduce( += _)\n'depth' could be computed in function of 'n' and numberOfPartitions.. something in the line of:\n  val maxResultSize = ....\n  val numPartitions = drm.rdd.partitions.size\n  val n = drm.ncol\n  val bytesPerVector = n * 8 + overhead?\n  val maxVectors = maxResultSize / bytes / 2 + 1 // be safe\n  val depth = math.max(1, math.ceil(math.log(1 + numPartitions / maxVectors) / math.log(2)).toInt)",
        "Issue Links": []
    },
    "MAHOUT-1791": {
        "Key": "MAHOUT-1791",
        "Summary": "Automatic threading for java based mmul in the front end and the backend.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.11.1,                                            0.12.0,                                            0.11.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "15/Nov/15 00:18",
        "Updated": "03/Mar/18 22:19",
        "Resolved": null,
        "Description": "As we know, we are still struggling with decisions which path to take for bare metal accelerations in in-core math. \nMeanwhile, a simple no-brainer improvement though is to add decision paths and apply multithreaded matrix-matrix multiplication (and maybe even others; but mmul perhaps is the most prominent beneficiary here at the moment which is both easy to do and to have a statistically significant improvement) \nSo multithreaded logic addition to mmul is one path. \nAnother path is automatic adjustment of multithreading. \nIn front end, we probably want to utilize all cores available. \nin the backend, we can oversubscribe cores but probably doing so by more than 2x or 3x is unadvisable because of point of diminishing returns driven by growing likelihood of context switching overhead.",
        "Issue Links": []
    },
    "MAHOUT-1792": {
        "Key": "MAHOUT-1792",
        "Summary": "Documentation, website-related stuff.",
        "Type": "Epic",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "15/Nov/15 00:19",
        "Updated": "03/Mar/18 21:10",
        "Resolved": "08/Apr/16 18:41",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1793": {
        "Key": "MAHOUT-1793",
        "Summary": "Declare WORK_DIR earlier in example script to fix output error in help output",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.11.2",
        "Component/s": "Examples",
        "Assignee": "Suneel Marthi",
        "Reporter": "Albert Chu",
        "Created": "17/Nov/15 00:57",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "17/Nov/15 02:54",
        "Description": "Noticed this when running some examples:\n\nPlease select a number to choose the corresponding clustering algorithm\n1. kmeans clustering (runs from this example script in cluster mode only)\n2. fuzzykmeans clustering (may require increased heap space on yarn)\n3. lda clustering\n4. streamingkmeans clustering\n5. clean -- cleans up the work area in \n\n\nIt should say\n\n5. clean -- cleans up the work area in /tmp/SOMETHING\n\n\nThe issue is the variable WORK_DIR is set after this help output is output.\nIt's a trivial patch.  Git pull request to be sent shortly.",
        "Issue Links": []
    },
    "MAHOUT-1794": {
        "Key": "MAHOUT-1794",
        "Summary": "Support alternate temporary directories in example scripts.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.12.0",
        "Component/s": "Examples",
        "Assignee": "Andrew Musselman",
        "Reporter": "Albert Chu",
        "Created": "20/Nov/15 00:36",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "20/Mar/16 03:16",
        "Description": "In many of the example scripts, a directory in /tmp (e.g. /tmp/mahout-work-dir) is created for use as temporary scratch space, such as to store data files that are downloaded.\nIn a number of HPC environments, /tmp may not exist or /tmp may be very small b/c local disk drives don't exist.  It'd be convenient to be able to specify an alternate directory to use as scratch space.\nPull request via Github to be sent shortly.",
        "Issue Links": []
    },
    "MAHOUT-1795": {
        "Key": "MAHOUT-1795",
        "Summary": "Release Scala 2.11 bindings",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Mike Kaplinskiy",
        "Created": "25/Nov/15 07:25",
        "Updated": "29/Jun/17 02:49",
        "Resolved": "26/Jun/17 21:50",
        "Description": "It would be nice to ship scala 2.11 bindings for mahout-math/mahout-spark. (I'm not sure of other users, but mahout-shell isn't nearly at the top of my list here).\nIt looks simple enough for those two - the attached patch is a proof-of-concept to compile (and pass all tests) under scala 2.11. I'm not sure what the proper way to do this is, but it doesn't look too daunting. (Famous last words?)",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/179"
        ]
    },
    "MAHOUT-1796": {
        "Key": "MAHOUT-1796",
        "Summary": "Mahout LDA Changes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Ramana Uppala",
        "Created": "12/Jan/16 16:42",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "03/Apr/16 13:31",
        "Description": "We made some changes to Mahout CVB0Driver, DictionaryVectorizer, SparseVectorsFromSequenceFiles. We are working on creating patch.",
        "Issue Links": []
    },
    "MAHOUT-1797": {
        "Key": "MAHOUT-1797",
        "Summary": "Typos for SPARK_ASSEMBLY_BIN",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.11.2",
        "Component/s": "spark",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "15/Jan/16 23:25",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "16/Jan/16 03:24",
        "Description": "Some typos for SPARK_ASSEMBLY_BIN in the mahout script lines 223-230:\n    SPARK_ASSEBMLY_BIN=\"${MAHOUT_HOME}/bin/mahout-spark-class.sh\"\n    if [ -x \"${SPARK_ASSEBMLY_BIN}\" ]; then\n       SPARK_ASSEMBLY_CLASSPATH=$(\"${SPARK_ASSEBMLY_BIN}\" 2>/dev/null)\n       CLASSPATH=\"${CLASSPATH}:${SPARK_ASSEBMLY_BIN}\"\n    else\n      echo \"Cannot find Spark assembly classpath. Is 'SPARK_HOME' set?\"\n      exit -1\n    fi\nIn master:\nbob $ grep -r SPARK_ASS\nbin/mahout-spark-class.sh:SPARK_ASSEMBLY_JAR=\nbin/mahout-spark-class.sh:if [ \"$num_jars\" -eq \"0\" -a -z \"$SPARK_ASSEMBLY_JAR\" ]; then\nbin/mahout-spark-class.sh:SPARK_ASSEMBLY_JAR=\"${ASSEMBLY_DIR}/${ASSEMBLY_JARS}\"\nbin/mahout-spark-class.sh:LAUNCH_CLASSPATH=\"$SPARK_ASSEMBLY_JAR\"\nbin/mahout-spark-class.sh:export _SPARK_ASSEMBLY=\"$SPARK_ASSEMBLY_JAR\"\nbin/mahout:    SPARK_ASSEBMLY_BIN=\"${MAHOUT_HOME}/bin/mahout-spark-class.sh\"\nbin/mahout:    if [ -x \"${SPARK_ASSEBMLY_BIN}\" ]; then\nbin/mahout:       SPARK_ASSEMBLY_CLASSPATH=$(\"${SPARK_ASSEBMLY_BIN}\" 2>/dev/null)\nbin/mahout:       CLASSPATH=\"${CLASSPATH}:${SPARK_ASSEBMLY_BIN}\"\nAssuming it's safe to replace the typos with the correct spelling.",
        "Issue Links": []
    },
    "MAHOUT-1798": {
        "Key": "MAHOUT-1798",
        "Summary": "mahout -version",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.11.2",
        "Component/s": "CLI",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "04/Feb/16 05:41",
        "Updated": "12/Mar/16 20:41",
        "Resolved": "12/Mar/16 20:41",
        "Description": "Would be nice to offer a `mahout -version` flag like Java so people can find out what they're working with on systems they don't maintain.",
        "Issue Links": []
    },
    "MAHOUT-1799": {
        "Key": "MAHOUT-1799",
        "Summary": "Read null row vectors from file in TextDelimeterReaderWriter driver",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "spark",
        "Assignee": "Pat Ferrel",
        "Reporter": "Jussi Jousimo",
        "Created": "29/Feb/16 14:49",
        "Updated": "22/May/16 00:44",
        "Resolved": "22/May/16 00:06",
        "Description": "Since some row vectors in a sparse matrix can be null, Mahout writes them out to a file with the row label only. However, Mahout cannot read these files, but throws an exception when it encounters a label-only row.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/182"
        ]
    },
    "MAHOUT-1800": {
        "Key": "MAHOUT-1800",
        "Summary": "Pare down Classtag overuse",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.11.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "07/Mar/16 22:27",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "08/Mar/16 20:40",
        "Description": "currently, almost every operator requires implicit parameter for the classtag context bound of drm rowset key type, even for things like drmA + drmB.\nin reality though DAG can already infer that similarly to e.g. it infers product geometry because classtags are already embedded in the logical plan. \nfor example, classtag(drmA+drmB) == classtag(drmA) == classtag(drmB). \nNot only does the DAG already contain this information, but also it opens doors to a loss of inference, since the optimizer doesn't verify that the new context bound is actually valid by retracing the inference. So any operation may introduce an invalid row key type, and as a consequence, invalid optimization information, without any further checks.",
        "Issue Links": []
    },
    "MAHOUT-1801": {
        "Key": "MAHOUT-1801",
        "Summary": "FastUtil to improve speed of Sparse Matrix Operations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.11.2",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "08/Mar/16 19:46",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "09/Mar/16 02:46",
        "Description": "Following the PR (https://github.com/apache/mahout/pull/81) for M-1640 - fastutil for Sparse Vectors, this is to implement fastutil for Sparse Matrix operations.",
        "Issue Links": []
    },
    "MAHOUT-1802": {
        "Key": "MAHOUT-1802",
        "Summary": "Capture attached checkpoints (if cached)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.11.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "08/Mar/16 22:45",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "09/Mar/16 04:01",
        "Description": "Currently, the optimizer generates checkpoints and attaches them to actual logical elements of the DAG via CheckpointAction$cp. \nthe way it worsk today is as follows: \n\ndrmC = drmA+ drmB\n\nval cp1 = drmC.checkpoint() // checkpoint\nval cp2 = drmC.checkpoint() // cp2 == cp1\n\ndrmD = cp1 + drmE // cp1 + drmE\n\n\nbut, in: \n\ndrmD = drmC + drmE // computes drmA + drmB + drmC all over\n\n\ndrmC already has cp1 attached to it so we should assume the common computational path is the intent here regardless and should be used, instead of building plans that recompute it. That is, \ndrmD = drmC + drmE should imply cp1 + drmE as well even if checkpoint is not used explicitly.",
        "Issue Links": []
    },
    "MAHOUT-1803": {
        "Key": "MAHOUT-1803",
        "Summary": "Fix spark-shell for spark 1.5.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.1",
        "Fix Version/s": "0.11.2",
        "Component/s": "Mahout spark shell",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "09/Mar/16 02:24",
        "Updated": "12/Mar/16 20:26",
        "Resolved": "11/Mar/16 01:22",
        "Description": "After upgrading the default Spark version to 1.5.2, the spark-shell crashes on startup with:\n\nException in thread \"main\" \nException: java.lang.OutOfMemoryError thrown from the UncaughtExceptionHandler in thread \"main\"",
        "Issue Links": []
    },
    "MAHOUT-1804": {
        "Key": "MAHOUT-1804",
        "Summary": "Implement drmParallelizeWithRowLabels(...) in flink",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 03:31",
        "Updated": "30/Mar/16 17:21",
        "Resolved": "15/Mar/16 19:32",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1805": {
        "Key": "MAHOUT-1805",
        "Summary": "Implement allreduceBlock(...) in flink bindings",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 03:34",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 19:42",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1806": {
        "Key": "MAHOUT-1806",
        "Summary": "Implicit checkpoint must not request caching",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 16:04",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "15/Mar/16 17:26",
        "Description": "When a DRM is checkpointed implicitly, it should not request any caching.  E.g.:  drmA.norm.",
        "Issue Links": []
    },
    "MAHOUT-1807": {
        "Key": "MAHOUT-1807",
        "Summary": "Distributed second norm doesn't take sqrt",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 16:35",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "15/Mar/16 17:26",
        "Description": "The square root function is missing  when calculating the second norm of a DRM in the Spark module.",
        "Issue Links": []
    },
    "MAHOUT-1808": {
        "Key": "MAHOUT-1808",
        "Summary": "Some cleanup of unused operations in sparkbindings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 16:49",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "15/Mar/16 17:26",
        "Description": "Remove a few unused operations from the sparkbindings code.",
        "Issue Links": []
    },
    "MAHOUT-1809": {
        "Key": "MAHOUT-1809",
        "Summary": "Failing tests in Flink-bindings: dals and dspca",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 17:41",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "22/Mar/16 03:01",
        "Description": "dspca and dals are failing in the flink distributed decomposition suite with numerical and oom errors respectively:\ndspca Failure and stack trace:\n\n54.69239412917543 was not less than 1.0E-5\nScalaTestFailureLocation: org.apache.mahout.flinkbindings.FailingTestsSuite$$anonfun$4 at (FailingTestsSuite.scala:230)\norg.scalatest.exceptions.TestFailedException: 54.69239412917543 was not less than 1.0E-5\n\tat org.scalatest.MatchersHelper$.newTestFailedException(MatchersHelper.scala:160)\n\tat org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6231)\n\tat org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6265)\n\tat org.apache.mahout.flinkbindings.FailingTestsSuite$$anonfun$4.apply$mcV$sp(FailingTestsSuite.scala:230)\n\tat org.apache.mahout.flinkbindings.FailingTestsSuite$$anonfun$4.apply(FailingTestsSuite.scala:186)\n\tat org.apache.mahout.flinkbindings.FailingTestsSuite$$anonfun$4.apply(FailingTestsSuite.scala:186)\n\tat org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)\n\tat org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)\n\tat org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)\n\tat org.scalatest.Transformer.apply(Transformer.scala:22)\n\tat org.scalatest.Transformer.apply(Transformer.scala:20)\n\tat org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)\n\tat org.scalatest.Suite$class.withFixture(Suite.scala:1122)\n\tat org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)\n\tat org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)\n\tat org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n\tat org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)\n\tat org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)\n\tat org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)\n\tat org.apache.mahout.flinkbindings.FailingTestsSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(FailingTestsSuite.scala:48)\n\tat org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)\n\tat org.apache.mahout.flinkbindings.FailingTestsSuite.runTest(FailingTestsSuite.scala:48)\n\tat org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n\tat org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)\n\tat org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)\n\tat org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)\n\tat org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)\n\tat org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)\n\tat org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)\n\tat org.scalatest.FunSuite.runTests(FunSuite.scala:1555)\n\tat org.scalatest.Suite$class.run(Suite.scala:1424)\n\tat org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)\n\tat org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n\tat org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)\n\tat org.scalatest.SuperEngine.runImpl(Engine.scala:545)\n\tat org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)\n\tat org.apache.mahout.flinkbindings.FailingTestsSuite.org$scalatest$BeforeAndAfterAllConfigMap$$super$run(FailingTestsSuite.scala:48)\n\tat org.scalatest.BeforeAndAfterAllConfigMap$class.liftedTree1$1(BeforeAndAfterAllConfigMap.scala:248)\n\tat org.scalatest.BeforeAndAfterAllConfigMap$class.run(BeforeAndAfterAllConfigMap.scala:247)\n\tat org.apache.mahout.flinkbindings.FailingTestsSuite.run(FailingTestsSuite.scala:48)\n\tat org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)\n\tat org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)\n\tat org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)\n\tat scala.collection.immutable.List.foreach(List.scala:318)\n\tat org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)\n\tat org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)\n\tat org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)\n\tat org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)\n\tat org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)\n\tat org.scalatest.tools.Runner$.run(Runner.scala:883)\n\tat org.scalatest.tools.Runner.run(Runner.scala)\n\tat org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)\n\tat org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat com.intellij.rt.execution.application.AppMain.main(AppMain.java:144)\n\n\ndals Failure:\n\nAn exception or error caused a run to abort: Java heap space \njava.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:2271)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1893)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1874)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1188)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)\n\tat org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:300)\n\tat org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:252)\n\tat org.apache.flink.runtime.operators.util.TaskConfig.setStubWrapper(TaskConfig.java:273)\n\tat org.apache.flink.optimizer.plantranslate.JobGraphGenerator.createDataSourceVertex(JobGraphGenerator.java:893)\n\tat org.apache.flink.optimizer.plantranslate.JobGraphGenerator.preVisit(JobGraphGenerator.java:286)\n\tat org.apache.flink.optimizer.plantranslate.JobGraphGenerator.preVisit(JobGraphGenerator.java:109)\n\tat org.apache.flink.optimizer.plan.SourcePlanNode.accept(SourcePlanNode.java:86)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.OptimizedPlan.accept(OptimizedPlan.java:128)\n\tat org.apache.flink.optimizer.plantranslate.JobGraphGenerator.compileJobGraph(JobGraphGenerator.java:188)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570",
            "/jira/browse/MAHOUT-1810"
        ]
    },
    "MAHOUT-1810": {
        "Key": "MAHOUT-1810",
        "Summary": "Failing test in flink-bindings: A + B Identically partitioned (mapBlock Checkpointing issue)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 17:52",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "22/Mar/16 02:32",
        "Description": "the A + B, Identically Partitioned test in the Flink RLikeDrmOpsSuite fails.  This test failure likely indicates an issue with Flink's Checkpointing or mapBlock operator:\n\n  test(\"C = A + B, identically partitioned\") {\n    val inCoreA = dense((1, 2, 3), (3, 4, 5), (5, 6, 7))\n    val A = drmParallelize(inCoreA, numPartitions = 2)\n\n    // Create B which would be identically partitioned to A. mapBlock() by default will do the trick.\n    val B = A.mapBlock() {\n      case (keys, block) =>\n        val bBlock = block.like() := { (r, c, v) => util.Random.nextDouble()}\n        keys -> bBlock\n    }\n      // Prevent repeated computation non-determinism\n      // flink problem is here... checkpoint is not doing what it should\n      .checkpoint()\n\n    val inCoreB = B.collect\n\n    printf(\"A=\\n%s\\n\", inCoreA)\n    printf(\"B=\\n%s\\n\", inCoreB)\n\n    val C = A + B\n    val inCoreC = C.collect\n    printf(\"C=\\n%s\\n\", inCoreC)\n\n    // Actual\n    val inCoreCControl = inCoreA + inCoreB\n    (inCoreC - inCoreCControl).norm should be < 1E-10\n  }\n\n\nThe output shous clearly that the line:\n\n        val bBlock = block.like() := { (r, c, v) => util.Random.nextDouble()}\n\n\nin the mapBlock closure is being calculated more than once.\nOutput:\n\nA=\n{\n 0 =>\t{0:1.0,1:2.0,2:3.0}\n 1 =>\t{0:3.0,1:4.0,2:5.0}\n 2 =>\t{0:5.0,1:6.0,2:7.0}\n}\nB=\n{\n 0 =>\t{0:0.26203398262809574,1:0.22561543461472167,2:0.23229669514522655}\n 1 =>\t{0:0.1638068194515867,1:0.18751822418846575,2:0.20586366231381614}\n 2 =>\t{0:0.9279465706239354,1:0.2963513448240057,2:0.8866928923235948}\n}\n\nC=\n{\n 0 =>\t{0:1.7883652623225594,1:2.6401297718606216,2:3.0023341959374195}\n 1 =>\t{0:3.641411452208408,1:4.941233165480053,2:5.381282338548803}\n 2 =>\t{0:5.707434148862531,1:6.022780876943659,2:7.149772825494352}\n}",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570",
            "/jira/browse/MAHOUT-1809"
        ]
    },
    "MAHOUT-1811": {
        "Key": "MAHOUT-1811",
        "Summary": "Fix calculation of second norm of DRM in Flink",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 18:13",
        "Updated": "28/Apr/16 02:38",
        "Resolved": "15/Mar/16 18:57",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1812": {
        "Key": "MAHOUT-1812",
        "Summary": "Implement drmParallelizeEmptyLong(...) in flink Bindings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 19:01",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "15/Mar/16 19:05",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1813": {
        "Key": "MAHOUT-1813",
        "Summary": "Functional \"apply\" DSL for distributed and in-memory matrices",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 20:19",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "15/Mar/16 22:24",
        "Description": "We have functional \"Assign\" for in-memory matrices, e.g.:\n\n    mxA := { x => x + 1 }\n    mxA ::= { x=> x * 2 }\n\n\nHowever, we lack similar unary elementwise function capability with distributed matrices, because distributed matrices are logically immutable.\nThe suggestion here is to use apply(func) to augment that capability for DRMs:\n\n    drmA(x => x + 1)\n    drmA(x => 2 * x)",
        "Issue Links": []
    },
    "MAHOUT-1814": {
        "Key": "MAHOUT-1814",
        "Summary": "Implement drm2intKeyed in flink bindings",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 20:57",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "10/Apr/16 03:06",
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1815": {
        "Key": "MAHOUT-1815",
        "Summary": "dsqDist(X,Y) and dsqDist(X) failing in flink tests.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Mar/16 21:04",
        "Updated": "11/Apr/16 08:42",
        "Resolved": "17/Mar/16 22:35",
        "Description": "test(\"dsqDist(X,Y)\") {\n    val m = 100\n    val n = 300\n    val d = 7\n    val mxX = Matrices.symmetricUniformView(m, d, 12345).cloned -= 5\n    val mxY = Matrices.symmetricUniformView(n, d, 1234).cloned += 10\n    val (drmX, drmY) = (drmParallelize(mxX, 3), drmParallelize(mxY, 4))\n\n    val mxDsq = dsqDist(drmX, drmY).collect\n    val mxDsqControl = new DenseMatrix(m, n) := { (r, c, _) \u21d2 (mxX(r, ::) - mxY(c, ::)) ^= 2 sum }\n    (mxDsq - mxDsqControl).norm should be < 1e-7\n  }\n\n\nAnd \n\n test(\"dsqDist(X)\") {\n    val m = 100\n    val d = 7\n    val mxX = Matrices.symmetricUniformView(m, d, 12345).cloned -= 5\n    val drmX = drmParallelize(mxX, 3)\n\n    val mxDsq = dsqDist(drmX).collect\n    val mxDsqControl = sqDist(drmX)\n    (mxDsq - mxDsqControl).norm should be < 1e-7\n  }\n\n\nare both failing in flink tests with arrayOutOfBounds Exceptions:\n\n03/15/2016 17:02:19\tDataSink (org.apache.flink.api.java.Utils$CollectHelper@568b43ab)(5/10) switched to FINISHED \n1 [CHAIN GroupReduce (GroupReduce at org.apache.mahout.flinkbindings.blas.FlinkOpAtB$.notZippable(FlinkOpAtB.scala:78)) -> Map (Map at org.apache.mahout.flinkbindings.blas.FlinkOpMapBlock$.apply(FlinkOpMapBlock.scala:37)) -> FlatMap (FlatMap at org.apache.mahout.flinkbindings.drm.BlockifiedFlinkDrm.asRowWise(FlinkDrm.scala:93)) (8/10)] ERROR org.apache.flink.runtime.operators.BatchTask  - Error in task code:  CHAIN GroupReduce (GroupReduce at org.apache.mahout.flinkbindings.blas.FlinkOpAtB$.notZippable(FlinkOpAtB.scala:78)) -> Map (Map at org.apache.mahout.flinkbindings.blas.FlinkOpMapBlock$.apply(FlinkOpMapBlock.scala:37)) -> FlatMap (FlatMap at org.apache.mahout.flinkbindings.drm.BlockifiedFlinkDrm.asRowWise(FlinkDrm.scala:93)) (8/10)\njava.lang.ArrayIndexOutOfBoundsException: 5\n\tat org.apache.mahout.math.drm.package$$anonfun$4$$anonfun$apply$3.apply(package.scala:317)\n\tat org.apache.mahout.math.drm.package$$anonfun$4$$anonfun$apply$3.apply(package.scala:317)\n\tat org.apache.mahout.math.scalabindings.MatrixOps$$anonfun$$colon$eq$3$$anonfun$apply$2.apply(MatrixOps.scala:164)\n\tat org.apache.mahout.math.scalabindings.MatrixOps$$anonfun$$colon$eq$3$$anonfun$apply$2.apply(MatrixOps.scala:164)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat org.apache.mahout.math.scalabindings.MatrixOps$$anonfun$$colon$eq$3.apply(MatrixOps.scala:164)\n\tat org.apache.mahout.math.scalabindings.MatrixOps$$anonfun$$colon$eq$3.apply(MatrixOps.scala:164)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:727)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n\tat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat org.apache.mahout.math.scalabindings.MatrixOps.$colon$eq(MatrixOps.scala:164)\n\tat org.apache.mahout.math.drm.package$$anonfun$4.apply(package.scala:317)\n\tat org.apache.mahout.math.drm.package$$anonfun$4.apply(package.scala:311)\n\tat org.apache.mahout.flinkbindings.blas.FlinkOpMapBlock$$anonfun$1.apply(FlinkOpMapBlock.scala:39)\n\tat org.apache.mahout.flinkbindings.blas.FlinkOpMapBlock$$anonfun$1.apply(FlinkOpMapBlock.scala:38)\n\tat org.apache.flink.api.scala.DataSet$$anon$1.map(DataSet.scala:297)\n\tat org.apache.flink.runtime.operators.chaining.ChainedMapDriver.collect(ChainedMapDriver.java:78)\n\tat org.apache.mahout.flinkbindings.blas.FlinkOpAtB$$anon$6.reduce(FlinkOpAtB.scala:86)\n\tat org.apache.flink.runtime.operators.GroupReduceDriver.run(GroupReduceDriver.java:125)\n\tat org.apache.flink.runtime.operators.BatchTask.run(BatchTask.java:480)\n\tat org.apache.flink.runtime.operators.BatchTask.invoke(BatchTask.java:345)\n\tat org.apache.flink.runtime.taskmanager.Task.run(Task.java:559)\n\tat java.lang.Thread.run(Thread.java:745)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1570"
        ]
    },
    "MAHOUT-1816": {
        "Key": "MAHOUT-1570 Adding support for Apache Flink as a backend for the Mahout DSL",
        "Summary": "Implement newRowCardinality in CheckpointedFlinkDrm",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "22/Mar/16 02:22",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "22/Mar/16 02:59",
        "Description": "Need implementation of newRowCardinality() for Flink backend.",
        "Issue Links": []
    },
    "MAHOUT-1817": {
        "Key": "MAHOUT-1817",
        "Summary": "Implement caching in Flink Bindings",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Workaround",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "22/Mar/16 02:30",
        "Updated": "11/Apr/16 08:41",
        "Resolved": "29/Mar/16 01:31",
        "Description": "Flink does not have in-memory caching analogous to that of Spark.  We need find a way to honour the checkpoint() contract in Flink Bindings.",
        "Issue Links": []
    },
    "MAHOUT-1818": {
        "Key": "MAHOUT-1818",
        "Summary": "dals test failing in Flink-bindings",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "1.0.0",
        "Component/s": "Flink",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "22/Mar/16 03:21",
        "Updated": "07/Sep/16 12:12",
        "Resolved": "07/Sep/16 12:12",
        "Description": "dals test fails in Flink bindings with an OOM.  Numerically the test passes, when the matrix being decomposed in the test  lowered to the size 50 x 50.  But the default size of the matrix in the DistributedDecompositionsSuiteBase is 500 x 500. \n\njava.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:2271)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.writeBlockHeader(ObjectOutputStream.java:1893)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1874)\n\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1785)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1188)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)\n\tat org.apache.flink.util.InstantiationUtil.serializeObject(InstantiationUtil.java:300)\n\tat org.apache.flink.util.InstantiationUtil.writeObjectToConfig(InstantiationUtil.java:252)\n\tat org.apache.flink.runtime.operators.util.TaskConfig.setStubWrapper(TaskConfig.java:273)\n\tat org.apache.flink.optimizer.plantranslate.JobGraphGenerator.createDataSourceVertex(JobGraphGenerator.java:893)\n\tat org.apache.flink.optimizer.plantranslate.JobGraphGenerator.preVisit(JobGraphGenerator.java:286)\n\tat org.apache.flink.optimizer.plantranslate.JobGraphGenerator.preVisit(JobGraphGenerator.java:109)\n\tat org.apache.flink.optimizer.plan.SourcePlanNode.accept(SourcePlanNode.java:86)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.SingleInputPlanNode.accept(SingleInputPlanNode.java:199)\n\tat org.apache.flink.optimizer.plan.OptimizedPlan.accept(OptimizedPlan.java:128)\n\tat org.apache.flink.optimizer.plantranslate.JobGraphGenerator.compileJobGraph(JobGraphGenerator.java:188)",
        "Issue Links": []
    },
    "MAHOUT-1819": {
        "Key": "MAHOUT-1819",
        "Summary": "Set the default Parallelism for Flink execution in FlinkDistributedContext",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "28/Mar/16 19:10",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "28/Mar/16 21:22",
        "Description": "Remove the option to set the degree of parallelism at individual operators and set it once in FlinkDistributedContext.",
        "Issue Links": []
    },
    "MAHOUT-1820": {
        "Key": "MAHOUT-1820",
        "Summary": "Add a method to generate Tuple<PartitionId, Partition elements count>> to support Flink backend",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "29/Mar/16 22:37",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "29/Mar/16 23:28",
        "Description": "Add a method - countElementsPerPartition() that returns a Tuple2<PartitionID, PartitionCount>, this is a temporary fix until the PR for Flink-3657 is merged into Flink Codebase.",
        "Issue Links": []
    },
    "MAHOUT-1821": {
        "Key": "MAHOUT-1821",
        "Summary": "Use a mahout-flink-conf.yaml configuration file for Mahout specific Flink configuration",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "29/Mar/16 23:32",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "09/Apr/16 00:26",
        "Description": "Mahout on Flink requires a few Mahout specific configuration parameters.  These configurations should be set in a $MAHOUT_HOME/conf/mahout-flink-conf.yaml file.  The values in it will override the user's flink-conf.yaml file if the same key is set in both.",
        "Issue Links": []
    },
    "MAHOUT-1822": {
        "Key": "MAHOUT-1822",
        "Summary": "Update NOTICE.txt, License.txt to add Apache Flink",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "03/Apr/16 16:09",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "03/Apr/16 16:28",
        "Description": "Update Notice.txt to remove long purged 3rd party packages. Need to add Apache Flink to the list of 3rd party libraries and dependencies in Notice.txt, License.txt",
        "Issue Links": []
    },
    "MAHOUT-1823": {
        "Key": "MAHOUT-1823",
        "Summary": "Modify MahoutFlinkTestSuite to implement FlinkTestBase",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "05/Apr/16 20:58",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "06/Apr/16 06:37",
        "Description": "Modify the present MahoutFlinkTestSuite to implement FlinkTestBase. This spins up a FlinkMiniCluster to execute the tests on.",
        "Issue Links": []
    },
    "MAHOUT-1824": {
        "Key": "MAHOUT-1824",
        "Summary": "Optimize FlinkOpAtA to use upper triangular matrices",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Suneel Marthi",
        "Created": "06/Apr/16 07:52",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "08/Apr/16 08:08",
        "Description": "Optimize FlinkOpAtA to use upper triangular matrices (similar to what's being done in Spark backend).  \nPresently dals fails on FlinkOpAtA computation with an OOM\n\n57766 [flink-akka.actor.default-dispatcher-5] ERROR akka.actor.ActorSystemImpl  - exception on LARS\u2019 timer thread\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n57770 [flink-akka.actor.default-dispatcher-5] ERROR akka.actor.ActorSystemImpl  - Uncaught fatal error from thread [flink-scheduler-1] shutting down ActorSystem [flink]\njava.lang.OutOfMemoryError: GC overhead limit exceeded\n- dals *** FAILED ***\n  org.apache.flink.runtime.client.JobTimeoutException: Timeout while waiting for JobManager answer. Job time exceeded 21474835 seconds\n  at org.apache.flink.runtime.client.JobClient.submitJobAndWait(JobClient.java:136)\n  at org.apache.flink.runtime.minicluster.FlinkMiniCluster.submitJobAndWait(FlinkMiniCluster.scala:423)\n  at org.apache.flink.runtime.minicluster.FlinkMiniCluster.submitJobAndWait(FlinkMiniCluster.scala:409)\n  at org.apache.flink.runtime.minicluster.FlinkMiniCluster.submitJobAndWait(FlinkMiniCluster.scala:401)\n  at org.apache.flink.client.LocalExecutor.executePlan(LocalExecutor.java:190)\n  at org.apache.flink.api.java.LocalEnvironment.execute(LocalEnvironment.java:90)\n  at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:855)\n  at org.apache.flink.api.scala.ExecutionEnvironment.execute(ExecutionEnvironment.scala:638)\n  at org.apache.flink.api.scala.DataSet.collect(DataSet.scala:546)\n  at org.apache.mahout.flinkbindings.blas.FlinkOpAtA$.slim(FlinkOpAtA.scala:53)\n  ...\n  Cause: akka.pattern.AskTimeoutException: Ask timed out on [Actor[akka://flink/user/$a#372851579]] after [21474835000 ms]\n  at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:333)\n  at akka.actor.Scheduler$$anon$7.run(Scheduler.scala:117)\n  at akka.actor.LightArrayRevolverScheduler$TaskHolder.run(Scheduler.scala:476)\n  at akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(Scheduler.scala:282)\n  at akka.actor.LightArrayRevolverScheduler$$anonfun$close$1.apply(Scheduler.scala:281)\n  at scala.collection.Iterator$class.foreach(Iterator.scala:727)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)\n  at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\n  at scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n  at akka.actor.LightArrayRevolverScheduler.close(Scheduler.scala:280)",
        "Issue Links": []
    },
    "MAHOUT-1825": {
        "Key": "MAHOUT-1825",
        "Summary": "Add List of Flink algorithms to Mahout wiki page",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Documentation",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "08/Apr/16 17:51",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "08/Apr/16 20:03",
        "Description": "Need to add list of flink-based algorithms on the Mahout wiki page",
        "Issue Links": []
    },
    "MAHOUT-1826": {
        "Key": "MAHOUT-1826",
        "Summary": "Fix wikipedia example",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "08/Apr/16 20:34",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "08/Apr/16 23:40",
        "Description": "Fix the URLs for the wikipedia example before releasing .  Only 2 urls to change.  \nThis is only marked as a Blocker so that I don't forget.",
        "Issue Links": []
    },
    "MAHOUT-1827": {
        "Key": "MAHOUT-1827",
        "Summary": "Suggested changes to homepage, how to contribute",
        "Type": "Documentation",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.1",
        "Component/s": "Documentation",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Shane Curcuru",
        "Created": "08/Apr/16 21:44",
        "Updated": "22/May/16 00:07",
        "Resolved": "17/Apr/16 23:25",
        "Description": "See attached patch(es).  Feel free to modify as needed for your style guide.  I tried to not change any meanings (not being a Mahout user!) but just to include helpful links and slightly easier to understand introductions.",
        "Issue Links": []
    },
    "MAHOUT-1828": {
        "Key": "MAHOUT-1828",
        "Summary": "Change the access of blas method in sparkbindings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "10/Apr/16 17:38",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "10/Apr/16 18:47",
        "Description": "Change the access modifiers of sparkbindings/blas to be private[sparkbindings]",
        "Issue Links": []
    },
    "MAHOUT-1829": {
        "Key": "MAHOUT-1829",
        "Summary": "Add Flink module to build tools",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.11.2",
        "Fix Version/s": "0.12.0",
        "Component/s": "Flink,                                            nk",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "11/Apr/16 00:29",
        "Updated": "11/Apr/16 23:48",
        "Resolved": "11/Apr/16 01:55",
        "Description": "We need to add the Flink Module to the buildtools in order to release.",
        "Issue Links": []
    },
    "MAHOUT-1830": {
        "Key": "MAHOUT-1830",
        "Summary": "Publish scaladocs for Mahout 0.13.0 release",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.1",
        "Component/s": "Documentation",
        "Assignee": null,
        "Reporter": "Suneel Marthi",
        "Created": "12/Apr/16 08:12",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "23/Jun/17 04:25",
        "Description": "Need to publish scaladocs for Mahout 0.12.0, present scaladocs out there are from 0.10.2 release.",
        "Issue Links": []
    },
    "MAHOUT-1831": {
        "Key": "MAHOUT-1831",
        "Summary": "Integrate Flink Shell with Mahout for interactive Samsara",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "Flink",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "13/Apr/16 00:02",
        "Updated": "07/Sep/16 12:07",
        "Resolved": "07/Sep/16 12:07",
        "Description": "Integrate Flink Shell with Mahout to be able to perform interactive Samsara, similar to what's presently being done with Spark shell.",
        "Issue Links": [
            "/jira/browse/FLINK-3701",
            "/jira/browse/FLINK-3776"
        ]
    },
    "MAHOUT-1832": {
        "Key": "MAHOUT-1832",
        "Summary": "Upgrade Jackson version and references to 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "18/Apr/16 00:30",
        "Updated": "22/May/16 00:07",
        "Resolved": "18/Apr/16 00:50",
        "Description": "The code is still running off of legacy Jackson 1.x, need to upgrade to 2.x",
        "Issue Links": []
    },
    "MAHOUT-1833": {
        "Key": "MAHOUT-1833",
        "Summary": "Enhance svec function to accept cardinality as parameter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "Math",
        "Assignee": "Edmond Luo",
        "Reporter": "Edmond Luo",
        "Created": "19/Apr/16 08:20",
        "Updated": "22/May/16 00:07",
        "Resolved": "26/Apr/16 20:16",
        "Description": "It will be nice to enhance the existing svec function in org.apache.mahout.math.scalabindings\n\n  /**\n   * create a sparse vector out of list of tuple2's\n   * @param sdata cardinality\n   * @return\n   */\n  def svec(sdata: TraversableOnce[(Int, AnyVal)], cardinality: Int = -1) = {\n    val required = if (sdata.nonEmpty) sdata.map(_._1).max + 1 else 0\n    var tmp = -1\n    if (cardinality < 0) {\n      tmp = required\n    } else if (cardinality < required) {\n      throw new IllegalArgumentException(s\"Required cardinality %required but got %cardinality\")\n    } else {\n      tmp = cardinality\n    }\n    val initialCapacity = sdata.size\n    val sv = new RandomAccessSparseVector(tmp, initialCapacity)\n    sdata.foreach(t \u21d2 sv.setQuick(t._1, t._2.asInstanceOf[Number].doubleValue()))\n    sv\n  }\n\n\nSo user can specify the cardinality for the created sparse vector.\nThis is very useful and convenient if user wants to create a DRM with many sparse vectors and the vectors are not with the same actual size(but with the same logical size, e.g. rows of a sparse matrix).\nBelow code should demonstrate the case:\n\nvar cardinality = 20\nval rdd = sc.textFile(\"/some/file.txt\").map(_.split(\",\")).map(line => (line(0).toInt, Array((line(1).toInt,1)))).reduceByKey((v1, v2) => v1 ++ v2).map(row => (row._1, svec(row._2\uff0ccardinality)))\n\nval drm = drmWrap(rdd.map(row => (row._1, row._2.asInstanceOf[Vector])))\n\n// All below element wise opperations will fail for those DRM with not cardinality-consistent SparseVector\nval drm2 = drm + drm.t\nval drm3 = drm - drm.t\nval drm4 = drm * drm.t\nval drm5 = drm / drm.t\n\n\nNotice that in the last map, the svec acceptted one more cardinality parameter, so the cardinality of those created sparse vectors can be consistent.",
        "Issue Links": []
    },
    "MAHOUT-1834": {
        "Key": "MAHOUT-1834",
        "Summary": "Setup Travis CI for Mahout",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "19/Apr/16 18:37",
        "Updated": "22/May/16 00:07",
        "Resolved": "19/Apr/16 21:34",
        "Description": "We need to get Travis CI setup for mahout.  This involves setting up an account with Travis and configuring a travis.yml file.",
        "Issue Links": []
    },
    "MAHOUT-1835": {
        "Key": "MAHOUT-1835",
        "Summary": "Remove countsPerPartition in Flink/blas/package.scala",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "23/Apr/16 14:15",
        "Updated": "22/May/16 00:07",
        "Resolved": "23/Apr/16 14:18",
        "Description": "Remove countsPerPartition() method in blas/package.scala since Flink-3657 is now part of Flink 1.0.2 release.",
        "Issue Links": []
    },
    "MAHOUT-1836": {
        "Key": "MAHOUT-1836",
        "Summary": "Order and add missing paramters for DictionaryVectorizer.createTermFrequencyVectors() javadoc parameter comments.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "Documentation",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Marku",
        "Created": "27/Apr/16 09:55",
        "Updated": "22/May/16 00:07",
        "Resolved": "27/Apr/16 22:49",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1837": {
        "Key": "MAHOUT-1837",
        "Summary": "Sparse/Dense Matrix analysis for Matrix Multiplication",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Math",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "27/Apr/16 18:55",
        "Updated": "28/Aug/16 13:23",
        "Resolved": "28/Aug/16 00:01",
        "Description": "In matrix multiplication, Sparse Matrices can easily turn dense and bloat memory,  one fully dense column and one fully dense row can cause a sparse %*% sparse operation have a dense result.  \nThere are two issues here one with a quick Fix and one a bit more involved:\n\nin ABt.Scala use check the `MatrixFlavor` of the combiner and use the flavor of the Block as the resulting Sparse or Dense matrix type:\n\nval comb = if (block.getFlavor == MatrixFlavor.SPARSELIKE) {\n              new SparseMatrix(prodNCol, block.nrow).t\n            } else {\n              new DenseMatrix(prodNCol, block.nrow).t\n            }\n\n\n a simlar check needs to be made in the blockify transformation.\n\n\nMore importantly, and more involved is to do an actual analysis of the resulting matrix data in the in-core mmul class and use a matrix of the appropriate Structure as a result.",
        "Issue Links": [
            "https://github.com/apache/mahout/commit/727e5be85c0326d9c009d9cdc361fe47ffa201ad",
            "https://github.com/apache/mahout/pull/244",
            "https://github.com/apache/mahout/pull/252"
        ]
    },
    "MAHOUT-1838": {
        "Key": "MAHOUT-1838",
        "Summary": "Provide plotting capabilities for Mahout matrices and DRMs",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "None",
        "Fix Version/s": "0.12.2",
        "Component/s": "visiualization",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "29/Apr/16 04:19",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "25/May/16 23:46",
        "Description": "Add basic 2d and 3d plotting capabilities to Mahout using the Smile library: https://github.com/haifengl/smile\nI've prototyped an mplot2d class here to sample a certain percentage of a DRM's data using drmSampleKRows() and to create a 2d plot from the resulting x,y coordinate matrix, and tested from the spark-shell as a POC.\nExtending to 3d should be trivial:  \nThe Smile-Plot library has some very nice plotting features, all of which can easily integrated into mahout:\nhttp://haifengl.github.io/smile/index.html#gallery\nhttp://haifengl.github.io/smile/index.html#visualization",
        "Issue Links": []
    },
    "MAHOUT-1839": {
        "Key": "MAHOUT-1838 Provide plotting capabilities for Mahout matrices and DRMs",
        "Summary": "Simple 2d plotting of a sampled DRM",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "visiualization",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "29/Apr/16 17:08",
        "Updated": "22/May/16 00:07",
        "Resolved": "29/Apr/16 22:02",
        "Description": "As a Sub task of MAHOUT-1838.  to Begin with create a simple 2d scatter plot of a sampled mx2 DRM.",
        "Issue Links": []
    },
    "MAHOUT-1840": {
        "Key": "MAHOUT-1838 Provide plotting capabilities for Mahout matrices and DRMs",
        "Summary": "simple 3d plots",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "visiualization",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "29/Apr/16 17:13",
        "Updated": "22/May/16 00:07",
        "Resolved": "01/May/16 21:55",
        "Description": "As a subtask of MAHOUT-1838, create a simple 3d Ploting class which samples from a mx3 DRM.",
        "Issue Links": []
    },
    "MAHOUT-1841": {
        "Key": "MAHOUT-1841",
        "Summary": "Matrices.symmetricUniformView(...) returning values in the wrong range.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "30/Apr/16 00:34",
        "Updated": "10/Sep/16 03:20",
        "Resolved": "01/May/16 05:15",
        "Description": "Per javadocs, Matrices.symmetricUniformView(...) is meant to return values in on the range of [-1,1): \n\n/**\n   * Matrix view based on uniform [-1,1) distribution.\n   *\n   * @param seed generator seed\n   */\n  public static final Matrix symmetricUniformView(final int rows,\n                                                  final int columns,\n                                                  int seed) {\n    return functionalMatrixView(rows, columns, uniformSymmetricGenerator(seed), true);\n  }\n\n\nRanges being returned now are on (-.5,.5).",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/231"
        ]
    },
    "MAHOUT-1842": {
        "Key": "MAHOUT-1838 Provide plotting capabilities for Mahout matrices and DRMs",
        "Summary": "SImple 3d surface plots",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "30/Apr/16 00:37",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "25/May/16 23:39",
        "Description": "Create a class msurf which will fit accept a mx3 DRM and a sample percentage and create a 3d surface plot of the sampled points.",
        "Issue Links": []
    },
    "MAHOUT-1843": {
        "Key": "MAHOUT-1838 Provide plotting capabilities for Mahout matrices and DRMs",
        "Summary": "simple 3d Grid plots",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.2",
        "Component/s": "visiualization",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "30/Apr/16 04:33",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "25/May/16 23:41",
        "Description": "create class mgrid to plot 3d grids of functions.",
        "Issue Links": []
    },
    "MAHOUT-1844": {
        "Key": "MAHOUT-1838 Provide plotting capabilities for Mahout matrices and DRMs",
        "Summary": "add simple 2d histograms",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "visiualization",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "30/Apr/16 23:44",
        "Updated": "22/May/16 00:07",
        "Resolved": "01/May/16 21:56",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1845": {
        "Key": "MAHOUT-1838 Provide plotting capabilities for Mahout matrices and DRMs",
        "Summary": "Add simple 3d histograms",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "visiualization",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "30/Apr/16 23:45",
        "Updated": "22/May/16 00:07",
        "Resolved": "01/May/16 21:57",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1846": {
        "Key": "MAHOUT-1838 Provide plotting capabilities for Mahout matrices and DRMs",
        "Summary": "Extend all plotting capabilites to work for in-core matrices",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.2",
        "Component/s": "visiualization",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "01/May/16 22:01",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "25/May/16 23:42",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1847": {
        "Key": "MAHOUT-1847",
        "Summary": "drmSampleRows in FlinkEngine doesn't wrap Int Keys when ClassTag is of type Int",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "02/May/16 21:21",
        "Updated": "22/May/16 00:07",
        "Resolved": "02/May/16 22:52",
        "Description": "drmSampleKRows in Flinkengine doesn't rekey with Integer keys when wrapping the resulting DataSet into a DRM for a classTag of type Int.",
        "Issue Links": []
    },
    "MAHOUT-1848": {
        "Key": "MAHOUT-1848",
        "Summary": "drmSampleKRows in FlinkEngine should generate a dense or sparse matrix",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "03/May/16 00:03",
        "Updated": "22/May/16 00:07",
        "Resolved": "03/May/16 03:06",
        "Description": "drmSampleKRows in FlinkEngine should generate a dense or sparse matrix based on the type of vector in the sampled Dataset",
        "Issue Links": []
    },
    "MAHOUT-1849": {
        "Key": "MAHOUT-1849",
        "Summary": "Update home page language",
        "Type": "Documentation",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "07/May/16 04:32",
        "Updated": "19/Dec/16 20:00",
        "Resolved": "19/Dec/16 20:00",
        "Description": "Update this to remove mr reference and replace with samsara:\nApache Mahout software includes three major features:\nA simple and extensible programming environment and framework for building scalable algorithms\nA wide variety of premade algorithms for Scala + Apache Spark, H2O, Apache Flink\nMahout's mature Hadoop MapReduce algorithms",
        "Issue Links": []
    },
    "MAHOUT-1850": {
        "Key": "MAHOUT-1850",
        "Summary": "GPU-acceleration",
        "Type": "Epic",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "09/May/16 21:49",
        "Updated": "23/Dec/16 21:09",
        "Resolved": null,
        "Description": "Use GPU acceleration for matrix and vector operators on the back end eg. \n\nVienna CL\nSparse solvers, etc\nOpenCL/CUDA",
        "Issue Links": []
    },
    "MAHOUT-1851": {
        "Key": "MAHOUT-1851",
        "Summary": "Automatic probing of in-core and back-end solvers",
        "Type": "New Feature",
        "Status": "In Progress",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "09/May/16 21:53",
        "Updated": "03/Mar/18 22:19",
        "Resolved": null,
        "Description": "In general, as we potentially expand the collection of in-core and distributed solvers relying on particular sw/hw capabilities installed (lib blas, viennacl, cuda), it would be nice to have automatic and centralized capability probing  and some sort of registry/framework that enumerates enabled features.",
        "Issue Links": []
    },
    "MAHOUT-1852": {
        "Key": "MAHOUT-1852",
        "Summary": "Histogram / binning for DRMs",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.0,                                            14.1",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "09/May/16 22:01",
        "Updated": "23/Jan/20 15:10",
        "Resolved": null,
        "Description": "Implement histogram and binning capabilities for DRMs. Should be easy to do with mapBlock() and allreduceBlock().  Allow for the user to specify the number of bins, a range, or an array of ranges as bins.",
        "Issue Links": []
    },
    "MAHOUT-1853": {
        "Key": "MAHOUT-1853",
        "Summary": "Improvements to CCO (Correlated Cross-Occurrence)",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Pat Ferrel",
        "Reporter": "Andrew Palumbo",
        "Created": "09/May/16 22:02",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "16/Oct/16 17:20",
        "Description": "Improvements to CCO (Correlated Cross-Occurrence) to include auto-threshold calculation for LLR downsampling, and possible multiple fixed thresholds for A\u2019A, A\u2019B etc. This is to account for the vast difference in dimensionality between indicator types.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1878",
            "/jira/browse/MAHOUT-1679",
            "https://github.com/apache/mahout/pull/251"
        ]
    },
    "MAHOUT-1854": {
        "Key": "MAHOUT-1854",
        "Summary": "Zeppelin integration: Spark Intrepreter",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Palumbo",
        "Created": "09/May/16 22:12",
        "Updated": "10/Nov/16 15:36",
        "Resolved": "10/Nov/16 15:36",
        "Description": "Integrate Mahout with Zeppelin by creating a Zeppelin Interpreter for Mahout first.",
        "Issue Links": []
    },
    "MAHOUT-1855": {
        "Key": "MAHOUT-1855",
        "Summary": "Zeppelin integration: Visualization",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Palumbo",
        "Created": "09/May/16 22:14",
        "Updated": "29/Jan/17 20:06",
        "Resolved": "10/Nov/16 15:37",
        "Description": "Integrate Mahout and Zeppelin's visualization features.",
        "Issue Links": []
    },
    "MAHOUT-1856": {
        "Key": "MAHOUT-1856",
        "Summary": "Create a framework for new Mahout Clustering, Classification, and Optimization  Algorithms",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Palumbo",
        "Created": "09/May/16 22:18",
        "Updated": "01/Feb/17 21:39",
        "Resolved": "01/Feb/17 21:39",
        "Description": "To ensure that Mahout does not become \"A loose bag of algorithms\", Create basic traits with funtions common to each class of algorithm.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/246"
        ]
    },
    "MAHOUT-1857": {
        "Key": "MAHOUT-1838 Provide plotting capabilities for Mahout matrices and DRMs",
        "Summary": "implement 2d countour plots",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.2",
        "Component/s": "visiualization",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "10/May/16 18:05",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "25/May/16 23:43",
        "Description": "create 2d contour plots to display density of  data",
        "Issue Links": []
    },
    "MAHOUT-1858": {
        "Key": "MAHOUT-1858",
        "Summary": "Mahout visualization features including Mahout plotting and Zeppelin integration",
        "Type": "Epic",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "14/May/16 18:44",
        "Updated": "19/Dec/16 02:42",
        "Resolved": "19/Dec/16 02:42",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1859": {
        "Key": "MAHOUT-1859",
        "Summary": "Disable non working  msurf and mgrid before Mahout 0.12.1 release",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "18/May/16 16:55",
        "Updated": "22/May/16 00:07",
        "Resolved": "18/May/16 17:53",
        "Description": "Surface plotting and Grid plotting of DRMs are not finished and are \"Fix Version 0.13.0 \" so Disable them before we release Mahout 0.12.1",
        "Issue Links": []
    },
    "MAHOUT-1860": {
        "Key": "MAHOUT-1860",
        "Summary": "Add Stack Image to the top of the front page of the  Website",
        "Type": "Documentation",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "website",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "22/May/16 00:28",
        "Updated": "03/Mar/18 22:20",
        "Resolved": null,
        "Description": "Add a variant of stack.svg - the image of the mahout stack (pg. 64 in the book) \"Above the fold\" on the site.  This image seems to help people grasp \"what mahout is\" very quickly.",
        "Issue Links": []
    },
    "MAHOUT-1861": {
        "Key": "MAHOUT-1861",
        "Summary": "New Mahout Clustering, Classification, Sketching and Optimization  Algorithms",
        "Type": "Epic",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.13.0,                                            14.2",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Palumbo",
        "Created": "23/May/16 21:53",
        "Updated": "08/Oct/20 14:46",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1862": {
        "Key": "MAHOUT-1862",
        "Summary": "Native Mahout integration",
        "Type": "Epic",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "25/May/16 23:57",
        "Updated": "01/Mar/18 04:09",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1863": {
        "Key": "MAHOUT-1863",
        "Summary": "cluster-syntheticcontrol.sh errors out with \"Input path does not exist\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.12.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Albert Chu",
        "Created": "26/May/16 00:41",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "26/May/16 23:56",
        "Description": "Running cluster-syntheticcontrol.sh on 0.12.0 resulted in this error:\n\nException in thread \"main\" org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://apex156:54310/user/achu/testdata\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n\tat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:301)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:318)\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196)\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:415)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\n\tat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)\n\tat org.apache.mahout.clustering.conversion.InputDriver.runJob(InputDriver.java:108)\n\tat org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job.run(Job.java:133)\n\tat org.apache.mahout.clustering.syntheticcontrol.fuzzykmeans.Job.main(Job.java:62)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\n\nIt appears cluster-syntheticcontrol.sh breaks under 0.12.0 due to patch\n\ncommit 23267a0bef064f3351fd879274724bcb02333c4a\n\n\none change in question\n\n-    $DFS -mkdir testdata\n+    $DFS -mkdir ${WORK_DIR}/testdata\n\n\nnow requires that the -p option be specified to -mkdir. This fix is simple.\nAnother change:\n\n-    $DFS -put ${WORK_DIR}/synthetic_control.data testdata\n+    $DFS -put ${WORK_DIR}/synthetic_control.data ${WORK_DIR}/testdata\n\n\nappears to break the example b/c in:\nexamples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/fuzzykmeans/Job.java\nexamples/src/main/java/org/apache/mahout/clustering/syntheticcontrol/kmeans/Job.java\nthe file 'testdata' is hard coded into the example as just 'testdata'. ${WORK_DIR}/testdata needs to be passed in as an option.\nReverting the lines listed above fixes the problem.  However, the reverting presumably breaks the original problem listed in MAHOUT-1773.\nI originally attempted to fix this by simply passing in the option \"--input ${WORK_DIR}/testdata\" into the command in the script. However, a number of other options are required if one option is specified.\nI considered modifying the above Job.java files to take a minimal number of arguments and set the rest to some default, but that would have also required changes to DefaultOptionCreator.java to make required options non-optional, which I didn't want to go down the path of determining what other examples had requires/non-requires requirements.\nSo I just passed in every required option into cluster-syntheticcontrol.sh to fix this, using whatever defaults were hard coded into the Job.java files above.\nI'm sure there's a better way to do this, and I'm happy to supply a patch, but thought I'd start with this.  \nGithub pull request to be sent shortly.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/235"
        ]
    },
    "MAHOUT-1864": {
        "Key": "MAHOUT-1864",
        "Summary": "Twenty Newsgroups Classification Example fails in case running with MAHOUT_LOCAL=true",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Examples",
        "Assignee": "Andrew Palumbo",
        "Reporter": "giriraj sharma",
        "Created": "27/May/16 07:44",
        "Updated": "28/Nov/17 21:10",
        "Resolved": "07/Sep/16 12:08",
        "Description": "Twenty Newsgroups Classification Example fails in case running with MAHOUT_LOCAL=true or else when HADOOP_HOME env variable is not set.\nNewsgroups lists instructions in order to run this classifier. When running in standalone mode(MAHOUT_LOCAL=true), i.e., running $ ./examples/bin/classify-20newsgroups.sh, the script runs ./examples/bin/set-dfs-commands.sh internally to export hadoop related env variables.\nset-dfs-commands.sh attempts to check for hadoop version despite running with MAHOUT_LOCAL set as true. IMHO, the script works fine considering the prerequisites, but, it will as well make sense if we can update the script ./examples/bin/set-dfs-commands.sh to export hadoop env varibales only in case MAHOUT_LOCAL is not set to true.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/236"
        ]
    },
    "MAHOUT-1865": {
        "Key": "MAHOUT-1865",
        "Summary": "Remove Hadoop 1 support.",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "27/May/16 20:43",
        "Updated": "07/Sep/16 12:17",
        "Resolved": "07/Sep/16 04:34",
        "Description": "Remove support for Hadoop 1.\n1. Disable Jenkins Hadoop 1 build.\n2. Remove Hadoop 1 profile from the root pom.xml.\n3. Refactor any Hadoop1 specific code outside of the mr module.\n4. Update documentation.\n5. Notify user@.\nHadoop1HDFSUtils is likely the only code that this will affect.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1880",
            "https://github.com/apache/mahout/pull/253"
        ]
    },
    "MAHOUT-1866": {
        "Key": "MAHOUT-1855 Zeppelin integration: Visualization",
        "Summary": "Add matrix-to-tsv string function",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.12.2",
        "Component/s": "visiualization",
        "Assignee": "Suneel Marthi",
        "Reporter": "Trevor Grant",
        "Created": "29/May/16 03:12",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "29/May/16 17:07",
        "Description": "Need a function to convert a matrix to a tsv string which can then be plotted by\n\nZeppelin %table visualization packages\nPassed to R / Python via Zeppelin Resource Manager\n\nIt has been noted that a matrix can be registered as an RDD and passed across contexts directly in Spark, however this breaks the 'backend agnoistic' philosophy.  Until H20 and Flink also both support Python / R environments it is more reasonable to use tab-seperated-value strings.\nFurther, matrices might be extremely large and unfit for being directly converted to tsvs.  It may be wise to introduce some sort of safety valve for preventing excessively large matrices from being materialized into local memory (eg. supposing the user hasn't called their own sampling method on a matrix).",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/237"
        ]
    },
    "MAHOUT-1867": {
        "Key": "MAHOUT-1867",
        "Summary": "upgrade 3rd party jars prior to next release",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.12.2",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "30/May/16 00:53",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "30/May/16 03:49",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1868": {
        "Key": "MAHOUT-1868",
        "Summary": "purge smile plotting from the codebase",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.12.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "31/May/16 23:01",
        "Updated": "13/Jun/16 15:21",
        "Resolved": "01/Jun/16 04:25",
        "Description": "Remove Experimental Smile based Plotting and all related code from the mahout codebase.\nMahout Smile based plotting has been subsumed by Zeppelin Integration via MAHOUT-1854.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/238"
        ]
    },
    "MAHOUT-1869": {
        "Key": "MAHOUT-1869",
        "Summary": "Create a runtime performance measuring framework for mahout",
        "Type": "Story",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "build,                                            Classification,                                            Collaborative Filtering,                                            Math",
        "Assignee": null,
        "Reporter": "Saikat Kanjilal",
        "Created": "03/Jun/16 04:33",
        "Updated": "12/Oct/16 18:26",
        "Resolved": "12/Oct/16 18:26",
        "Description": "This proposal will outline a runtime performance module used to measure the performance of various algorithms in mahout in the three major areas, clustering, regression and classification. The module will be a spray/scala/akka application which will be run by any current or new algorithm in mahout and will display a csv file and a set of zeppelin plots outlining the various criteria for performance. The goal of releasing any new build in mahout will be to run a set of tests for each of the algorithms to compare and contrast some benchmarks from one release to another.\ngithub repo is here:  https://github.com/skanjila/mahout, will send pull request when I have 1 algorithm operational\nArchitecture\nThe run time performance application will run on top of spray/scala and akka and will make async api calls into the various mahout algorithms to generate a cvs file containing data representing the run time performance measurement calculations for each algorithm of interest as well as a set of zeppelin plots for displaying some of these results. The spray scala architecture will leverage the zeppelin server to create the visualizations. The discussion below centers around two types of algorithms to be addressed by the application.\nClustering\nThe application will consist of a set of rest APIs to do the following:\na) A method to load and execute the run time perf module and takes as inputs the name of the algorithm (kmeans, fuzzy kmeans) and a location of a set of files containing various sizes of data sets\n/algorithm=clustering/fileLocation=/path/to/files/of/different/datasets/clusters=12,20,30,40 and finally a set of values for the number of clusters to use for each of the different sizes of the datasets\nThe above API call will return a runId which the client program can then use to monitor the module\nb) A method to monitor the application to ensure that its making progress towards generating the zeppelin plots\n/monitor/runId=456\nThe above method will execute asynchronously by calling into the mahout kmeans (fuzzy kmeans) clustering implementations and will generate zeppelin plots showing the normalized time on the y axis and the number of clusters in the x axis. The spray/scala akka framework will allow the client application to receive a callback when the run time performance calculations are actually completed. For now the calculations for measuring run time performance will contain: a) the ratio of the number of points clustered correctly to the total number of points b) the total time taken for the algorithm to run . These items will be represented in separate zeppelin plots.\nRegression\na) The runtime performance module will run the likelihood ratio test with a different set of features in every run . We will introduce a rest API to run the likelihood ratio test and return the results, this will once again be an sync call through the spray/akka stack.\nb) The run time performance module will contain the following metrics for every algorithm: 1) cpu usage 2) memory usage 3) time taken for algorithm to converge and run to completion. These metrics will be reported on top of the zeppelin graphs for both the regression and the different clustering algorithms mentioned above.\nHow does the application get run.  The run time performance measuring application will get invoked from the command line, eventually it would be worthwhile to hook this into some sort of integration test suite to certify the different mahout releases.",
        "Issue Links": [
            "/jira/browse/REEF-1419"
        ]
    },
    "MAHOUT-1870": {
        "Key": "MAHOUT-1870",
        "Summary": "Add import and export capabilities for DRMs to and from Apache Arrow",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "08/Jun/16 16:23",
        "Updated": "03/Mar/18 21:24",
        "Resolved": null,
        "Description": "We need to add the capability to import DRMs from and export DRMs to Apache Arrow.   This will be part of a greater effort to make integration more seamless with other projects. In some cases (eg. exporting to csv or tsv we will allow for a a loss in precision).",
        "Issue Links": []
    },
    "MAHOUT-1871": {
        "Key": "MAHOUT-1871",
        "Summary": "Kmeans - java.lang.IllegalStateException: No input clusters found..... Check your -c argument",
        "Type": "Question",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Not A Bug",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.13.0",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Juan Carlos Sipan Robles",
        "Created": "12/Jun/16 16:39",
        "Updated": "14/Oct/16 03:34",
        "Resolved": "14/Oct/16 03:34",
        "Description": "By using the kmeans with the following parameters gives the following error.\n16/06/12 17:35:43 INFO KMeansDriver: convergence: 0.5 max Iterations: 10\n16/06/12 17:35:43 INFO CodecPool: Got brand-new decompressor [.deflate]\nException in thread \"main\" java.lang.IllegalStateException: No input clusters found in /mdb/clustered_data/part-randomSeed. Check your -c argument.\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.buildClusters(KMeansDriver.java:213)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:147)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.run(KMeansDriver.java:110)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.mahout.clustering.kmeans.KMeansDriver.main(KMeansDriver.java:47)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)\n\tat org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)\n\tat org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:152)\n\tat org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n[SSH] exit-status: 1\nFinished: FAILURE\nCommand Execution:\nhdfs dfs -rm -R /mdb/mahout_vectors/\nhdfs dfs -rm -R /mdb/mahout_seq/\nhdfs dfs -rm -R /mdb/mahout_data/\nhdfs dfs -rm -R /mdb/clustered_data/\necho ##### SE ELIMINAN LAS CARPETAS DE HDFS#####\nhdfs dfs -mkdir /mdb/mahout_vectors/\nhdfs dfs -mkdir /mdb/mahout_seq/\nhdfs dfs -mkdir /mdb/mahout_data/\nhdfs dfs -mkdir /mdb/clustered_data/\necho ##### subimos el fichero #####\nhdfs dfs -put $fichero /mdb/mahout_data/\necho ##### generamos ficheros secuenciales#####\nmahout seqdirectory -i /mdb/mahout_data/ -o /mdb/mahout_seq -c UTF-8 -chunk 64 -xm sequential\necho ##### generamos los vectores #####\nmahout seq2sparse -i /mdb/mahout_seq/ -o /mdb/mahout_vectors/ --namedVector\necho ##### ejecutamos el kmeans #####\nmahout kmeans -i /mdb/mahout_vectors/tfidf-vectors/ -c /mdb/clustered_data -o /mdb/mahout_data -dm org.apache.mahout.common.distance.EuclideanDistanceMeasure -x 10 -k 20 -ow --clustering",
        "Issue Links": []
    },
    "MAHOUT-1872": {
        "Key": "MAHOUT-1872",
        "Summary": "Implementation Issue of getting number of users in ParallelALSFactorizationJob.java",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0,                                            0.12.1,                                            0.13.0,                                            0.12.2",
        "Fix Version/s": "1.0.0,                                            0.13.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Tarun Gulyani",
        "Created": "14/Jun/16 20:30",
        "Updated": "21/Jan/17 05:12",
        "Resolved": "21/Jan/17 05:12",
        "Description": "Code \"int numUsers = (int) userRatings.getCounters().findCounter(Stats.NUM_USERS).getValue();\" is calling in \"ParallelALSFactorizationJob.java\" after completion of \"average rating\" job which is called after \"user rating\" job. Therefor JobClient not able to get the information of number of user directly from Application Master and to get this information it redirect to Job History Server. \nTherefore ALS job able to run successfully unless Job History Server is running.",
        "Issue Links": []
    },
    "MAHOUT-1873": {
        "Key": "MAHOUT-1873",
        "Summary": "Use densityAnalysis() in all necessary operations",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "16/Jun/16 16:00",
        "Updated": "23/Jun/17 04:06",
        "Resolved": null,
        "Description": "Find all places in which densityAnalysis(...) can be used to determine ideal matrix structure and implement it.  Eg in ABt, AtB, and possibly Kryo serializers.  Ensure when doing this that it is not redundant; Ie. the call is not made by both the Kryo serializer and the distributed operation.",
        "Issue Links": []
    },
    "MAHOUT-1874": {
        "Key": "MAHOUT-1874",
        "Summary": "Broken markdown in Downloads section on Mahout site",
        "Type": "Documentation",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": "Andrew Musselman",
        "Reporter": "Kyriakos Georgiou",
        "Created": "24/Jun/16 15:55",
        "Updated": "28/Aug/16 04:03",
        "Resolved": "28/Aug/16 03:56",
        "Description": "There's a space that breaks the closing back-ticks around ```~/.bash_profile`` ` at https://svn.apache.org/viewvc/mahout/site/mahout_cms/trunk/content/general/downloads.mdtext?view=markup#l22",
        "Issue Links": []
    },
    "MAHOUT-1875": {
        "Key": "MAHOUT-1875",
        "Summary": "Use faster shallowCopy for dense matices in blockify drm/package.blockify(..)",
        "Type": "Improvement",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "25/Jun/16 16:15",
        "Updated": "03/Mar/18 21:22",
        "Resolved": null,
        "Description": "In sparkbindings.drm/package.blockify(...), after testing the density of an incoming block, use DenseMatrix(blockAsArrayOfDoubles, true) to shallow copy the backing vector array into the DenseMatrix.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/264"
        ]
    },
    "MAHOUT-1876": {
        "Key": "MAHOUT-1876",
        "Summary": "Mahout fails to read from lucene index of solr-5.5.2",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Raviteja Lokineni",
        "Created": "19/Jul/16 16:04",
        "Updated": "11/Aug/16 14:48",
        "Resolved": "11/Aug/16 05:45",
        "Description": "Command: \n\nbin/mahout lucene.vector --dir ~/softwares/solr-6.1.0/server/solr/nlp-core/data/index --output /tmp/solr-nlp-core/out.vec --field rspns_val --dictOut /tmp/solr-nlp-core/dictionary.txt --norm 2\n\nStacktrace:\n\nhadoop binary is not in PATH,HADOOP_HOME/bin,HADOOP_PREFIX/bin, running locally\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/lok268/softwares/apache-mahout-distribution-0.12.2/mahout-examples-0.12.2-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/lok268/softwares/apache-mahout-distribution-0.12.2/mahout-mr-0.12.2-job.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/lok268/softwares/apache-mahout-distribution-0.12.2/lib/slf4j-log4j12-1.7.19.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\nException in thread \"main\" org.apache.lucene.index.IndexFormatTooNewException: Format version is not supported (resource: ChecksumIndexInput(MMapIndexInput(path=\"/home/lok268/softwares/solr-6.1.0/server/solr/nlp-core/data/index/segments_2\"))): 6 (needs to be between 0 and 1)\n        at org.apache.lucene.codecs.CodecUtil.checkHeaderNoMagic(CodecUtil.java:148)\n        at org.apache.lucene.index.SegmentInfos.read(SegmentInfos.java:329)\n        at org.apache.lucene.index.StandardDirectoryReader$1.doBody(StandardDirectoryReader.java:56)\n        at org.apache.lucene.index.SegmentInfos$FindSegmentsFile.run(SegmentInfos.java:843)\n        at org.apache.lucene.index.StandardDirectoryReader.open(StandardDirectoryReader.java:52)\n        at org.apache.lucene.index.DirectoryReader.open(DirectoryReader.java:66)\n        at org.apache.mahout.utils.vectors.lucene.Driver.dumpVectors(Driver.java:89)\n        at org.apache.mahout.utils.vectors.lucene.Driver.main(Driver.java:277)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)\n        at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:145)\n        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:153)\n        at org.apache.mahout.driver.MahoutDriver.main(MahoutDriver.java:195)",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/247",
            "https://github.com/apache/mahout/pull/248"
        ]
    },
    "MAHOUT-1877": {
        "Key": "MAHOUT-1877",
        "Summary": "Switch to Flink 1.1.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "08/Aug/16 23:40",
        "Updated": "11/Aug/16 05:52",
        "Resolved": "09/Aug/16 00:41",
        "Description": "Switch to Flink 1.1.0",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/249"
        ]
    },
    "MAHOUT-1878": {
        "Key": "MAHOUT-1878",
        "Summary": "implement quartile type thresholds for indicator matrix downsampling",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "Collaborative Filtering,                                            cooccurrence",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "21/Aug/16 01:01",
        "Updated": "03/Mar/18 21:29",
        "Resolved": null,
        "Description": "https://issues.apache.org/jira/browse/MAHOUT-1853\nsecond half of the above, see discussion of downsampling by fraction of matrix retained, perhaps using t-digest.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1853"
        ]
    },
    "MAHOUT-1879": {
        "Key": "MAHOUT-1879",
        "Summary": "Lazy density analysis of DRMs in CheckpointedDrm",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "01/Sep/16 17:17",
        "Updated": "03/Mar/18 22:19",
        "Resolved": null,
        "Description": "Add in a lazy value for density analysis of Checkpointed Drms that can be accessed at Checkpoint Barriers.  \neg. \n\nlazy val mxTest: SparseRowMatrix = drmSampleKRows(...)\nlazy val isDense: Boolean = densistyAnalysis(mxTest)",
        "Issue Links": []
    },
    "MAHOUT-1880": {
        "Key": "MAHOUT-1880",
        "Summary": "Remove H2O Bindings from the release binaries",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "build",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Sep/16 21:11",
        "Updated": "07/Sep/16 01:47",
        "Resolved": "07/Sep/16 01:01",
        "Description": "Since the H2O bindings are very large (~20m) we will no longer continue to ship them in the binary release.  They will be continue available through  source builds.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1706",
            "/jira/browse/MAHOUT-1705",
            "/jira/browse/MAHOUT-1865",
            "https://github.com/apache/mahout/pull/254"
        ]
    },
    "MAHOUT-1881": {
        "Key": "MAHOUT-1881",
        "Summary": "flink-config.yaml is not copied to $MAHOUT_HOME/conf in Binary Distro",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "Flink",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "08/Sep/16 13:07",
        "Updated": "08/Sep/16 15:47",
        "Resolved": "08/Sep/16 14:59",
        "Description": "The $MAHOUT_HOME/flink-config.yaml file which is used to configure the Flink degree of parallesm, number of tasks, and temp caching directory is not included in the $MAHOUT_HOME/conf in the binary distribution.  It seems like the whole directory is written over with mr .props files during the release process.  \nThe file exists in the source repository.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/255"
        ]
    },
    "MAHOUT-1882": {
        "Key": "MAHOUT-1882",
        "Summary": "SequentialAccessSparseVector inerateNonZeros is incorrect.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "15/Sep/16 21:40",
        "Updated": "03/Mar/18 21:19",
        "Resolved": null,
        "Description": "In SequentialAccessSparseVector a bug is noted.  When Cuonting Non-Zero elements NonDefaultIterator can, under certain circumstances give an incorrect iterator of size different from the actual non-zeroCounts.\n\n @Override\n  public Iterator<Element> iterateNonZero() {\n\n    // TODO: this is a bug, since nonDefaultIterator doesn't hold to non-zero contract.\n    return new NonDefaultIterator();\n  }",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/290"
        ]
    },
    "MAHOUT-1883": {
        "Key": "MAHOUT-1883",
        "Summary": "Create a type if IndexedDataset that filters unneeded data for CCO",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Collaborative Filtering",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "01/Oct/16 21:23",
        "Updated": "04/Jun/18 14:45",
        "Resolved": "16/Oct/16 17:19",
        "Description": "The collaborative filtering CCO algo uses drms for each \"indicator\" type. The input must have the same set of user-id and so the row rank for all input matrices must be the same.\nIn the past we have padded the row-id dictionary to include new rows only in secondary matrices. This can lead to very large amounts of data processed in the CCO pipeline that does not affect the results. Put another way if the row doesn't exist in the primary matrix, there will be no cross-occurrence in the other calculated cooccurrences matrix.\nif we are calculating P'P and P'S, S will not need rows that don't exist in P so this Jira is to create an IndexedDataset companion object that takes an RDD[(String, String)] of interactions but that uses the dictionary from P for row-ids and filters out all data that doesn't correspond to P. The companion object will create the row-ids dictionary if it is not passed in, and use it to filter if it is passed in.\nWe have seen data that can be reduced by many orders of magnitude using this technique. This could be handled outside of Mahout but always produces better performance and so this version of data-prep seems worth including.\nIt does not affect the CLI version yet but could be included there in a future Jira.",
        "Issue Links": []
    },
    "MAHOUT-1884": {
        "Key": "MAHOUT-1884",
        "Summary": "Allow specification of dimensions of a DRM",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Sebastian Schelter",
        "Created": "03/Oct/16 06:50",
        "Updated": "03/Mar/18 21:24",
        "Resolved": null,
        "Description": "Currently, in many cases, a DRM must be read to compute its dimensions when a user calls nrow or ncol. This also implicitly caches the corresponding DRM.\nIn some cases, the user actually knows the matrix dimensions (e.g., when the matrices are synthetically generated, or when some metadata about them is known). In such cases, the user should be able to specify the dimensions upon creating the DRM and the caching should be avoided.",
        "Issue Links": []
    },
    "MAHOUT-1885": {
        "Key": "MAHOUT-1885",
        "Summary": "Inital Implementation of VCL Bindings",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "Math",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Oct/16 23:52",
        "Updated": "26/Jan/17 04:56",
        "Resolved": "26/Jan/17 04:26",
        "Description": "Push a working experimental branch of VCL bindings into master.  There is still a lot of work to be done.  All tests are passing, At the moment there am opening this JIRA mostly to get a number for PR and to test profiles against on travis.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1908",
            "/jira/browse/MAHOUT-1907",
            "https://github.com/apache/mahout/pull/261"
        ]
    },
    "MAHOUT-1886": {
        "Key": "MAHOUT-1886",
        "Summary": "MathJax not rendering",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "07/Oct/16 00:33",
        "Updated": "07/Oct/16 01:44",
        "Resolved": "07/Oct/16 01:44",
        "Description": "MathJax is not rendering on the d-ssvd page.",
        "Issue Links": []
    },
    "MAHOUT-1887": {
        "Key": "MAHOUT-1887",
        "Summary": "Document org.apache.mahout.classifier.sgd.RunLogistic",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Karl-Philipp Richter",
        "Created": "08/Oct/16 00:28",
        "Updated": "19/Dec/16 02:16",
        "Resolved": null,
        "Description": "It'd be nice to know from the class comment where to get input data from for `org.apache.mahout.classifier.sgd.RunLogistic` of the examples module.\nexperienced with mahout-0.12.2-24-gb5fe4aa",
        "Issue Links": []
    },
    "MAHOUT-1888": {
        "Key": "MAHOUT-1888",
        "Summary": "Performance Bug with Mahout Vector Serialization",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "spark",
        "Assignee": "Suneel Marthi",
        "Reporter": "Suneel Marthi",
        "Created": "09/Oct/16 23:21",
        "Updated": "14/Oct/16 03:18",
        "Resolved": "14/Oct/16 02:40",
        "Description": "Identified a performance bug with Mahout Vector serialization in DistributedSparkSuite.\nAdd the following\n\n.set(\"spark.kryo.registrationRequired\", \"true\")",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/260"
        ]
    },
    "MAHOUT-1889": {
        "Key": "MAHOUT-1889",
        "Summary": "Mahout doesn't work with Spark 2.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sergey Svynarchuk",
        "Created": "17/Oct/16 11:00",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "19/Dec/16 02:13",
        "Description": "In Spark 2.0 was changes path to libraries and classpath. If change classpath to correct for Spark 2.0, all Spark job failed with java.lang.NoSuchMethodError: , because Spark API was changed.\nExample for spark-shell:\n\n./bin/mahout spark-shell\n\nException in thread \"main\" java.lang.NoSuchMethodError: org.apache.spark.repl.SparkILoop.setPrompt(Ljava/lang/String;)V\n\tat org.apache.mahout.sparkbindings.shell.MahoutSparkILoop.<init>(MahoutSparkILoop.scala:58)\n\tat org.apache.mahout.sparkbindings.shell.Main$.main(Main.scala:32)\n\tat org.apache.mahout.sparkbindings.shell.Main.main(Main.scala)",
        "Issue Links": [
            "/jira/browse/MAHOUT-1894"
        ]
    },
    "MAHOUT-1890": {
        "Key": "MAHOUT-1890",
        "Summary": "Infinite loop in OpenLongObjectHashMap",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "collections",
        "Assignee": null,
        "Reporter": "Michael M.",
        "Created": "21/Oct/16 19:16",
        "Updated": "21/Oct/16 19:42",
        "Resolved": "21/Oct/16 19:31",
        "Description": "It seems that OpenLongObjectHashMap<> (org.apache.mahout:mahout-collections:1.0) can enter a state where containsKey (indexOfKey) ends up in an infinite loop, stuck in this loop:\nOpenLongObjectHashMap.java\n    while (state[i] != FREE && (state[i] == REMOVED || table[i] != key)) {\n      i -= decrement;\n      //hashCollisions++;\n      if (i < 0) {\n        i += length;\n      }\n    }\n\n\nI haven't identified a minimum set of operations necessary to reach this state, but I have generated a (fairly large) test that achieves it:\nTestOpenLongObjectHashMap.java\n\nimport java.util.Arrays;\nimport java.util.List;\nimport java.util.function.Consumer;\nimport org.apache.mahout.math.map.OpenLongObjectHashMap;\nimport org.junit.Test;\n\npublic class TestOpenLongObjectHashMap {\n\n    private static final List<Consumer<OpenLongObjectHashMap<Long>>> transcript =\n            Arrays.asList(add(66546), add(66847), add(71319), del(71319), add(80177), del(80177), add(88428),\n                          add(88861), add(92709), del(92709), add(94392), del(94392), add(99506), del(99506),\n                          add(104232), add(104968), del(104232), del(104968), add(111042), del(111042), add(123271),\n                          del(123271), add(130887), del(66847), add(131387), add(131537), add(131569), del(131537),\n                          add(135253), del(135253), add(138781), del(138781), add(141689), del(141689), add(144224),\n                          del(144224), add(147237), del(147237), add(152945), add(153646), del(152945), add(154915),\n                          del(154915), add(155621), del(155621), add(158464), add(158724), del(158724), del(158464),\n                          add(174017), del(174017), add(176818), del(176818), add(178344), add(178716), del(178344),\n                          add(178956), del(178956), del(178716), add(181714), del(181714), add(188533), del(188533),\n                          add(189152), del(189152), del(131569), add(193603), add(193614), add(193632), del(193614),\n                          add(193650), add(193661), add(193662), del(193662), add(193691), add(193761), del(193661),\n                          del(193761), add(193801), add(193812), del(193650), del(131387), del(193603), add(193837),\n                          del(193837), add(194160), add(194175), del(194160), add(194224), del(194175), add(195507),\n                          add(195617), add(195838), add(196272), add(196402), add(196410), del(196272), del(195507),\n                          add(196426), add(196427), del(193691), add(196439), add(196440), del(195838), add(196449),\n                          del(196426), add(196460), add(196634), del(196449), del(196402), del(196439), del(196440),\n                          add(197250), add(197482), add(197531), del(193632), add(197983), del(197250), del(197983),\n                          del(196634), add(199455), add(199648), add(200356), add(200397), add(200711), del(200356),\n                          del(199648), add(201133), del(200711), add(201209), del(201209), del(196410), del(200397),\n                          add(205160), del(205160), del(197531), del(196427), add(207533), add(207546), del(196460),\n                          del(197482), add(207555), add(207556), add(207660), add(207820), del(207555), del(207556),\n                          del(207660), add(208887), add(208944), del(208944), add(210976), del(210976), add(212301),\n                          del(208887), add(213198), del(207546), del(213198), add(213321), del(212301), add(213402),\n                          add(214597), del(214597), add(224378), add(225915), add(229080), del(213402), add(229346),\n                          del(225915), del(224378), add(231030), add(231151), add(231373), del(231030), del(231373),\n                          del(229080), add(232821), del(232821), add(233121), add(233146), del(233146), del(233121),\n                          add(233947), del(233947), del(229346), add(234011), add(234078), add(234093), del(207820),\n                          add(234582), del(207533), add(234590), add(235295), add(235463), add(235815), del(235295),\n                          del(234582), del(235815), add(236133), del(236133), del(235463), add(239925), add(239957),\n                          del(239957), add(242934), del(242934), add(243629), add(244092), del(234078), del(243629),\n                          del(234011), add(244298), add(244413), add(244427), add(244695), del(244092), del(244695),\n                          add(245241), del(239925), del(234093), add(247267), del(244298), del(231151), add(247766),\n                          add(247772), add(247773), add(247892), del(244413), add(249689), add(249831), del(249831),\n                          del(245241), add(253283), del(249689), del(253283), add(254814), del(254814), add(256746),\n                          add(258382), del(244427), add(259392), del(256746), del(258382), add(263266), add(266622),\n                          add(267835), del(267835), del(266622), add(270727), add(270786), add(271043), del(271043),\n                          del(270727), add(274128), del(274128), del(270786), add(276954), del(276954), add(277773),\n                          add(277827), del(277773), add(278443), del(278443), del(277827), add(280444), add(280476),\n                          add(286186), add(286193), del(286186), add(286205), del(234590), del(201133), del(247267),\n                          add(286322), add(286330), del(263266), del(199455), del(286205), add(286376), add(286378),\n                          del(280476), del(259392), add(286434), add(286539), add(288067), add(290067), del(290067),\n                          add(290809), del(290809), del(288067), add(295950), del(280444), add(296556), add(297276),\n                          del(296556), add(297704), add(297907), add(297936), add(297947), add(297956), del(286539),\n                          del(297936), add(298595), add(298616), add(298617), add(298643), del(297947), add(298700),\n                          add(298701), add(299170), del(299170), del(295950), del(297276), del(286322), add(299644),\n                          add(299750), del(286378), add(299751), del(286434), add(299900), add(300040), del(300040),\n                          add(300165), del(299900), add(302324), del(302324), add(304371), del(304371), add(306117),\n                          del(306117), add(306751), add(307193), del(306751), del(307193), add(307561), del(299644),\n                          add(307848), add(307894), del(307894), del(307848), add(308326), del(307561), del(308326),\n                          add(311198), del(311198), add(314100), add(314250), add(314454), add(315195), add(315750),\n                          add(317668), add(318585), add(319988), add(320038), add(320634), del(320634), add(321154),\n                          add(322377), add(322405), del(322405), del(321154), add(322989), add(323488), add(323489),\n                          del(323488), del(322989), add(324668), del(323489), add(325651), add(325675), del(325651),\n                          add(326387), add(326654), del(326387), del(326654), del(325675), add(327454), add(327844),\n                          add(327888), add(328541), del(327844), del(324668), add(329002), add(329792), del(329792),\n                          del(329002), add(331123), del(328541), del(331123), add(331626), add(333869), del(333869),\n                          add(333939), add(333940), del(333940), del(333939), add(334532), del(334532), del(298616),\n                          del(298617), add(336577), add(336578), add(336723), del(317668), add(337556), add(337557),\n                          del(336723), add(338153), del(338153), add(339818), add(340153), del(340153), del(339818),\n                          add(340253), del(340253), add(341507), del(322377), add(342285), del(341507), del(331626),\n                          add(343334), del(300165), add(343339), del(343339), del(343334), add(343453), add(343515),\n                          del(343515), add(343958), del(343958), del(195617), add(345163), add(346229), del(346229),\n                          del(345163), add(348094), add(348581), add(348958), del(343453), del(348958), del(348581),\n                          add(349217), add(349218), del(349218), del(349217), del(342285), add(350002), add(350833),\n                          del(350833), add(352160), del(352160), add(354943), del(354943), add(359170), add(359465),\n                          add(359480), del(359465), del(359480), del(350002), del(359170), add(363634), del(363634),\n                          add(364606), add(367965), del(367965), add(369082), del(369082), del(364606), add(370023),\n                          del(370023), add(371449), add(371450), del(371450), del(371449), add(371526), del(371526),\n                          add(371834), add(371928), del(371834), del(371928), add(372151), add(372228), add(372276),\n                          del(372276), del(372228), add(372364), add(372601), add(372602), del(372151), del(372364),\n                          del(372602), add(373037), add(373046), del(373037), add(373288), del(373288), del(373046),\n                          del(337557), add(375867), add(376358), add(376582), del(376582), add(378272), del(378272),\n                          add(380800), add(381955), add(381958), del(381958), del(372601), add(382278));\n\n    public static Consumer<OpenLongObjectHashMap<Long>> add(final long id) {\n        return (map) -> map.put(id, id);\n    }\n\n    public static Consumer<OpenLongObjectHashMap<Long>> del(final long id) {\n        return (map) -> map.removeKey(id);\n    }\n\n    @Test\n    public void spinsForever() {\n        final OpenLongObjectHashMap<Long> map = new OpenLongObjectHashMap<>();\n        for (final Consumer<OpenLongObjectHashMap<Long>> consumer : transcript) {\n            consumer.accept(map);\n        }\n        map.clear();\n        map.put(254, 254L);\n        map.put(2829, 2829L);\n        // We get this far, containsKey ends up spinning in OpenLongObjectHashMap::indexOfKey\n        map.containsKey(2866);\n    }\n}",
        "Issue Links": []
    },
    "MAHOUT-1891": {
        "Key": "MAHOUT-1891",
        "Summary": "Website: update link to nips06-mapreducemulticore.pdf in FAQ",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Documentation",
        "Assignee": "Trevor Grant",
        "Reporter": "Alexander Bezzubov",
        "Created": "06/Nov/16 11:31",
        "Updated": "10/Nov/16 15:20",
        "Resolved": "10/Nov/16 15:20",
        "Description": "Right now in https://mahout.apache.org/general/faq.html#where-can-i-find-the-origins-of-the-mahout-project words \"Map-Reduce for Machine Learning on Multicore\" are a link to nips06-mapreducemulticore.pdf wich results in 404.\nIt's probably better to use http://ai.stanford.edu/~ang/papers/nips06-mapreducemulticore.pdf\nWould be glad to submit a drive-by patch\\PR but from https://mahout.apache.org/developers/how-to-update-the-website.html it looks like it's hosted in Apache CMS and only a commiter can do that.",
        "Issue Links": []
    },
    "MAHOUT-1892": {
        "Key": "MAHOUT-1892",
        "Summary": "Can't broadcast vector in Mahout-Shell",
        "Type": "Bug",
        "Status": "In Progress",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Trevor Grant",
        "Created": "07/Nov/16 19:54",
        "Updated": "03/Mar/18 21:18",
        "Resolved": null,
        "Description": "When attempting to broadcast a Vector in Mahout's spark-shell with `mapBlock` we get serialization errors.  *NOTE* scalars can be broadcast without issue.\nI did some testing in the \"Zeppelin Shell\" for lack of a better term.  See https://github.com/apache/zeppelin/pull/928\nThe `mapBlock` same code I ran in the spark-shell below, also generated errors.  However, wrapping a mapBlock into a function in a compiled jar https://github.com/apache/mahout/pull/246/commits/ccb5da65330e394763928f6dc51d96e38debe4fb#diff-4a952e8e09ae07e0b3a7ac6a5d6b2734R25 and then running said function from the Mahout Shell or in the \"Zeppelin Shell\" (using Spark or Flink as a runner) works fine.  \nConsider\n```\nmahout> val inCoreA = dense((1, 2, 3), (3, 4, 5))\nval A = drmParallelize(inCoreA)\nval v: Vector = dvec(1,1,1)\nval bcastV = drmBroadcast(v)\nval drm2 = A.mapBlock() {\n    case (keys, block) =>\n        for(row <- 0 until block.nrow) block(row, : -= bcastV\n    keys -> block    \n}\ndrm2.checkpoint()\n```\nWhich emits the stack trace:\n```\norg.apache.spark.SparkException: Task not serializable\n        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:304)\n        at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)\n        at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)\n        at org.apache.spark.SparkContext.clean(SparkContext.scala:2032)\n        at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:318)\n        at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:317)\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)\n        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)\n        at org.apache.spark.rdd.RDD.withScope(RDD.scala:310)\n        at org.apache.spark.rdd.RDD.map(RDD.scala:317)\n        at org.apache.mahout.sparkbindings.blas.MapBlock$.exec(MapBlock.scala:33)\n        at org.apache.mahout.sparkbindings.SparkEngine$.tr2phys(SparkEngine.scala:338)\n        at org.apache.mahout.sparkbindings.SparkEngine$.toPhysical(SparkEngine.scala:116)\n        at org.apache.mahout.math.drm.logical.CheckpointAction.checkpoint(CheckpointAction.scala:41)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:58)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:68)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:70)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:72)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:74)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:76)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:78)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:80)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:82)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:84)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:86)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:88)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:90)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:92)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:94)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:96)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:98)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:100)\n        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:102)\n        at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:104)\n        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:106)\n        at $iwC$$iwC$$iwC.<init>(<console>:108)\n        at $iwC$$iwC.<init>(<console>:110)\n        at $iwC.<init>(<console>:112)\n        at <init>(<console>:114)\n        at .<init>(<console>:118)\n        at .<clinit>(<console>)\n        at .<init>(<console>:7)\n        at .<clinit>(<console>)\n        at $print(<console>)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)\n        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)\n        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)\n        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)\n        at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)\n        at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)\n        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)\n        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)\n        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)\n        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)\n        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)\n        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)\n        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)\n        at org.apache.mahout.sparkbindings.shell.Main$.main(Main.scala:37)\n        at org.apache.mahout.sparkbindings.shell.Main.main(Main.scala)\nCaused by: java.io.NotSerializableException: org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark\nSerialization stack:\n\nobject not serializable (class: org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark, value: org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@2d1c2fe7)\nfield (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, name: A, type: interface org.apache.mahout.math.drm.CheckpointedDrm)\nobject (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC, $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC@291be813)\nfield (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1, name: $outer, type: class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC)\nobject (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1, <function1>)\nfield (class: org.apache.mahout.sparkbindings.blas.MapBlock$$anonfun$exec$2, name: bmf$1, type: interface scala.Function1)\nobject (class org.apache.mahout.sparkbindings.blas.MapBlock$$anonfun$exec$2, <function1>)\n        at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)\n        at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)\n        at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)\n        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)\n        ... 66 more\n```",
        "Issue Links": [
            "/jira/browse/MAHOUT-1786"
        ]
    },
    "MAHOUT-1893": {
        "Key": "MAHOUT-1893",
        "Summary": "Fix Algorithm list on mahout.apache.org",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "11/Dec/16 05:45",
        "Updated": "04/Mar/18 02:45",
        "Resolved": "02/Feb/17 20:07",
        "Description": "mahout.apache.org ->Algorithms lists only thin-QR and DSSVD.  Update with all current algorithms.\nALS, SPCA, point them to https://mahout.apache.org/users/sparkbindings/home.html",
        "Issue Links": [
            "/jira/browse/MAHOUT-1686",
            "/jira/browse/MAHOUT-1682"
        ]
    },
    "MAHOUT-1894": {
        "Key": "MAHOUT-1894",
        "Summary": "Add support for Spark 2x backend",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "1.0.0,                                            0.13.0,                                            0.14.0,                                            14.1",
        "Component/s": "spark",
        "Assignee": "Trevor Grant",
        "Reporter": "Suneel Marthi",
        "Created": "12/Dec/16 20:31",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "24/Feb/17 14:01",
        "Description": "add support for Spark 2.x as backend execution engine.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1889",
            "https://github.com/apache/mahout/pull/271"
        ]
    },
    "MAHOUT-1895": {
        "Key": "MAHOUT-1895",
        "Summary": "Add convenience methods for converting Vectors to Scala types",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "15/Dec/16 18:08",
        "Updated": "04/Mar/18 02:47",
        "Resolved": "21/Jan/17 05:45",
        "Description": "While dense and sparse vectors may be created from `TraversableOnce[Double]` such as `Array[Double]` and `TraversableOnce[(Int, AnyVal)]` such as `Map[Int,Double]` respectively. Converting back into this format is somewhat tedious.  We should add convenience methods to take care of this.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/262"
        ]
    },
    "MAHOUT-1896": {
        "Key": "MAHOUT-1896",
        "Summary": "Add convenience methods for interacting with Spark ML",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "15/Dec/16 22:50",
        "Updated": "04/Mar/18 02:47",
        "Resolved": "21/Jan/17 05:46",
        "Description": "Currently the method for ingesting RDDs to DRM is `drmWrap`.  This is a flexible method, however there are many cases when the RDD to be wrapped is either RDD[org.apache.spark.mllib.lingalg.Vector], RDD[org.apache.spark.mllib.regression.LabeledPoint], or DataFrame[Row] (as is the case when working with SparkML.  It makes sense to create convenience methods for converting these types to DRM.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/263"
        ]
    },
    "MAHOUT-1897": {
        "Key": "MAHOUT-1897",
        "Summary": "Mahout Shell is running with a lag",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.1",
        "Component/s": "Mahout spark shell",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "19/Dec/16 19:52",
        "Updated": "04/May/17 18:38",
        "Resolved": "04/May/17 18:38",
        "Description": "There is a noticeable lag in the mahout spark-shell.  When compared to Spark's spark-shell it is easy to see. This makes things like Auto Complete difficult to work with.  \nSince the mahout spark-shell is just an extension of the Spark spark-shell, this shouldn't be happening.  \nThis slow down makes the shell clunky and a bit difficult to work with.  Often times when people want to try out mahout, they play around with the shell, so It would be good to get this fixed up, so people don't leave with a bad impression of mahout.",
        "Issue Links": []
    },
    "MAHOUT-1898": {
        "Key": "MAHOUT-1898",
        "Summary": "Mahout for parsing/analysing scanned medical images",
        "Type": "Question",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "Classification,                                            Clustering,                                            Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Santhosh Kumar V S",
        "Created": "26/Dec/16 10:52",
        "Updated": "26/Dec/16 23:26",
        "Resolved": "26/Dec/16 23:26",
        "Description": "We have a set of scaned images of medical records -this is unstructured data - Our requirement is to parse these documents and store in a structured format possibly in a \nRelation DB or Schemaless DB like MongoDB\nHow Mahout will help in this usecase ?",
        "Issue Links": []
    },
    "MAHOUT-1899": {
        "Key": "MAHOUT-1899",
        "Summary": "Add optional manual configuration of AtA's number of partitions",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Bertrand Dechoux",
        "Created": "27/Dec/16 15:26",
        "Updated": "27/Dec/16 15:27",
        "Resolved": null,
        "Description": "Allow override when estimation is not adequate because AtA is indeed more dense...",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/265"
        ]
    },
    "MAHOUT-1900": {
        "Key": "MAHOUT-1900",
        "Summary": "Add a getter to DenseMatrix for the double[][] values field.",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "Math",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Jan/17 21:26",
        "Updated": "16/Jan/17 22:35",
        "Resolved": "08/Jan/17 06:31",
        "Description": "It may be possible to add something like getBackingDataStructure() all the way up to AbstractMatrix and return Iterators for Sparse Matrices and Arrays[][] for Dense Matrices.\nHowever, for no I think that we only need a getter for DenseMatrix.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/267"
        ]
    },
    "MAHOUT-1901": {
        "Key": "MAHOUT-1901",
        "Summary": "Remove h20 from the Binary Release Build",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Suneel Marthi",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Jan/17 21:28",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "07/Jan/17 15:50",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/mahout/pull/266"
        ]
    },
    "MAHOUT-1902": {
        "Key": "MAHOUT-1902",
        "Summary": "Parse Spark and Mahout variable arguments from the Mahout spark-shell",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "1.0.0,                                            0.14.0,                                            0.13.1,                                            14.1",
        "Component/s": "Mahout spark shell",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "09/Jan/17 18:20",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "24/Feb/17 14:36",
        "Description": "The Spark spark-shell allows for spark variables to be supplied at the command line, e.g.:\n\nspark-shell --driver-memory 4g\n\n\nThe mahout shell does not have an argument parser for CLI vars.  We should allow for variables to be set in the same fashion; e.g.:\n\nmahout spark-shell --driver-memory 4g\n\n\nThis would be a good issue for someone who wants to start working on mahout, to get familiar with the codebase.",
        "Issue Links": []
    },
    "MAHOUT-1903": {
        "Key": "MAHOUT-1903",
        "Summary": "Fix VCL vector %*% vector implementation",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "10/Jan/17 23:06",
        "Updated": "04/Mar/18 02:45",
        "Resolved": "26/Feb/17 21:41",
        "Description": "Vector %% vector and vector %% Matrix need to have memory allocation Re-written.  Currently they are commented out in tests",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/286"
        ]
    },
    "MAHOUT-1904": {
        "Key": "MAHOUT-1904",
        "Summary": "Create a test harness to test mahout across different hardware configurations",
        "Type": "Task",
        "Status": "In Progress",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "11/Jan/17 00:25",
        "Updated": "03/Mar/18 22:18",
        "Resolved": null,
        "Description": "Creat a set of simple scala programs to be run as a test harness for Linux amd/intel, mac, and avx2(default).",
        "Issue Links": []
    },
    "MAHOUT-1905": {
        "Key": "MAHOUT-1905",
        "Summary": "Update javacpp version",
        "Type": "Dependency upgrade",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "16/Jan/17 21:41",
        "Updated": "22/Jan/17 00:37",
        "Resolved": "22/Jan/17 00:37",
        "Description": "To do after branch is merged; before thorough testing and release.",
        "Issue Links": []
    },
    "MAHOUT-1906": {
        "Key": "MAHOUT-1906",
        "Summary": "Ensure customJars are added to the MahoutContext under certain conditions for spark 1.6+",
        "Type": "Dependency upgrade",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "16/Jan/17 22:21",
        "Updated": "17/Jan/17 01:36",
        "Resolved": "17/Jan/17 01:36",
        "Description": "When creating MahoutContext: if 1.6+ && backend is spark-standalone && addMahoutContextJars = false && customJars.isEmpty() Context creation fails with serialization error on task serialization. \nFix is to ensure that jars are added in this case.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/268"
        ]
    },
    "MAHOUT-1907": {
        "Key": "MAHOUT-1907",
        "Summary": "Correctly implement VCL vector %*% matrix multiplication.",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "17/Jan/17 00:10",
        "Updated": "04/Mar/18 02:44",
        "Resolved": "26/Feb/17 21:42",
        "Description": "Vector/Matrix Multiplication is currently commented out in the VCL PR.  after much discussion have found the Bug, will implement it after VCL is pushed.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1885"
        ]
    },
    "MAHOUT-1908": {
        "Key": "MAHOUT-1908",
        "Summary": "Create properties for VCL on mac",
        "Type": "Task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2,                                            14.1",
        "Fix Version/s": "0.13.2,                                            14.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "17/Jan/17 00:20",
        "Updated": "23/Jan/20 15:07",
        "Resolved": null,
        "Description": "Create a set of properties to run OMP on mac- OS X darwin and >. OpenMP is not supported directly by CLANG in mac g++ (in most versions).",
        "Issue Links": [
            "/jira/browse/MAHOUT-1885"
        ]
    },
    "MAHOUT-1909": {
        "Key": "MAHOUT-1909",
        "Summary": "Cache Modular Backend solvers after probing",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "22/Jan/17 03:26",
        "Updated": "03/Mar/18 21:25",
        "Resolved": null,
        "Description": "Modular Backend solvers are lazily computed Instantiated upon their usage.  After their first call, cache the solver in the RootSolverFactory.",
        "Issue Links": []
    },
    "MAHOUT-1910": {
        "Key": "MAHOUT-1910",
        "Summary": "Remove .zip archive from build",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "23/Jan/17 04:26",
        "Updated": "04/Mar/18 02:44",
        "Resolved": "24/Feb/17 22:28",
        "Description": "remove the .zip archves from the mahout binary disributions and keep only the .tar.gz:\nIn assembly/bin.xml:\n\n  <formats>\n    <format>dir</format>\n    <format>tar.gz</format>\n    <format>zip</format>\n  </formats>",
        "Issue Links": []
    },
    "MAHOUT-1911": {
        "Key": "MAHOUT-1911",
        "Summary": "Fix Create Mahout Context after VCL Merge",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "26/Jan/17 02:49",
        "Updated": "04/Mar/18 02:43",
        "Resolved": "02/Feb/17 23:12",
        "Description": "While the VCL bindings were under development I commented out a large section of getMahoutContext() in SparkBindings/package.scala\nThis code has been changed slightly so need to make sure it is replaced and working after rebasing and merging VCL. \nI.e.:\n\n   // context specific jars\n        val mcjars = findMahoutContextJars(closeables)\n\n        if (log.isDebugEnabled) {\n          log.debug(\"Mahout jars:\")\n          mcjars.foreach(j => log.debug(j))\n        }\n\n        sparkConf.setJars(jars = mcjars.toSeq ++ customJars)\n        if (!(customJars.size > 0)) sparkConf.setJars(customJars.toSeq)\n\n      } else {\n        // In local mode we don't care about jars, do we?\n        sparkConf.setJars(customJars.toSeq",
        "Issue Links": [
            "/jira/browse/MAHOUT-1934"
        ]
    },
    "MAHOUT-1912": {
        "Key": "MAHOUT-1912",
        "Summary": "Fix  Spark CLI Driver tests in after commented out with VCL binding commit",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "26/Jan/17 03:00",
        "Updated": "04/Mar/18 02:43",
        "Resolved": "26/Feb/17 00:14",
        "Description": "Spark CLI Drivers were crashing after when biuilt with `-Pviennacl` so I commented them out to focus on the VCL work.. These must be uncommented and Debugged before the release.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/283"
        ]
    },
    "MAHOUT-1913": {
        "Key": "MAHOUT-1913",
        "Summary": "Clean Up of VCL bindings",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "26/Jan/17 03:35",
        "Updated": "04/Mar/18 02:43",
        "Resolved": "27/Feb/17 23:35",
        "Description": "Much cleanup of vcl bindings commit",
        "Issue Links": []
    },
    "MAHOUT-1914": {
        "Key": "MAHOUT-1914",
        "Summary": "Spark tests are not picking up OpenCL/OpenMP jars",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "26/Jan/17 03:48",
        "Updated": "03/Mar/18 21:23",
        "Resolved": null,
        "Description": "Spark tests are not picking up the correct back-end solver Jars for VCL classes. This is new, and must be fixed before release.",
        "Issue Links": []
    },
    "MAHOUT-1915": {
        "Key": "MAHOUT-1915",
        "Summary": "mahoot+bug",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "abdessamad",
        "Created": "26/Jan/17 14:58",
        "Updated": "04/Feb/17 05:13",
        "Resolved": "26/Jan/17 21:42",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1916": {
        "Key": "MAHOUT-1916",
        "Summary": "mahout bug",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "sarra sarra",
        "Created": "26/Jan/17 15:15",
        "Updated": "27/Jan/17 00:11",
        "Resolved": "27/Jan/17 00:11",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1917": {
        "Key": "MAHOUT-1917",
        "Summary": "mahout_bug",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "sarra sarra",
        "Created": "26/Jan/17 15:45",
        "Updated": "26/Jan/17 21:42",
        "Resolved": "26/Jan/17 21:42",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1918": {
        "Key": "MAHOUT-1918",
        "Summary": "Use traits when probing VCL",
        "Type": "Test",
        "Status": "In Progress",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "26/Jan/17 21:50",
        "Updated": "31/Mar/20 15:08",
        "Resolved": null,
        "Description": "currently we have\neg.\n\n \n      clazz = Class.forName(\"org.apache.mahout.viennacl.opencl.GPUMMul$\").getField(\"MODULE$\").get(null).asInstanceOf[MMBinaryFunc]\n\n\nTo instantiate a Solver.. It is being cast to a MMBinaryFunc\ncast this to at MMulSolver Trait and change the corresponding class GPUMMul to extend this.",
        "Issue Links": []
    },
    "MAHOUT-1919": {
        "Key": "MAHOUT-1919",
        "Summary": "Flink Module breaks the build regularly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "29/Jan/17 22:00",
        "Updated": "04/Mar/18 02:45",
        "Resolved": "27/Feb/17 23:51",
        "Description": "OOM Errors thrown by the flink module regularly break the Knightly Build.  These should be addressed before the 0.13.0 release... \nOne possibility is to Downgrade the Flink dependency in the root pom to the orignal development dep: (1.0.x I believe)?",
        "Issue Links": []
    },
    "MAHOUT-1920": {
        "Key": "MAHOUT-1920",
        "Summary": "Spark Dependency Reduced Jar Needs Open-MP",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "31/Jan/17 21:38",
        "Updated": "01/Feb/17 21:39",
        "Resolved": "01/Feb/17 21:39",
        "Description": "The spark dependency reduced jar includes:\norg.apache.mahout:mahout-native-viennacl_2.10\nbut not\norg.apache.mahout:mahout-native-viennacl-omp_2.10\nIn mahout/spark/src/main/assembly/dependency-reduced.xml",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/270"
        ]
    },
    "MAHOUT-1921": {
        "Key": "MAHOUT-1921",
        "Summary": "DSSVD Propagates cache hint",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Math",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 02:31",
        "Updated": "07/Feb/17 04:07",
        "Resolved": "07/Feb/17 04:07",
        "Description": "The DSSVD does lots of check pointing, but currently only the default checkpoint cacheHint is given.  \nThe user should be able to pass a checkpointing hint as this can lead to dramatic performance issues in some cases.\nhttps://github.com/apache/mahout/blob/master/math-scala/src/main/scala/org/apache/mahout/math/decompositions/DSSVD.scala",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/274"
        ]
    },
    "MAHOUT-1922": {
        "Key": "MAHOUT-1922",
        "Summary": "DSPCA Propagates cache hint",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Math",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 02:33",
        "Updated": "07/Feb/17 04:32",
        "Resolved": "07/Feb/17 04:07",
        "Description": "The DSPCA does lots of check pointing, but currently only the default checkpoint cacheHint is given.\nThe user should be able to pass a checkpointing hint as this can lead to dramatic performance issues in some cases.\nhttps://github.com/apache/mahout/blob/master/math-scala/src/main/scala/org/apache/mahout/math/decompositions/DSPCA.scala",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/275"
        ]
    },
    "MAHOUT-1923": {
        "Key": "MAHOUT-1923",
        "Summary": "dqrThin Propagates cache hint",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Math",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 02:36",
        "Updated": "07/Feb/17 04:32",
        "Resolved": "07/Feb/17 04:07",
        "Description": "The user should be able to pass a checkpointing hint as this can lead to dramatic performance issues in some cases.\nhttps://github.com/apache/mahout/blob/b5fe4aab22e7867ae057a6cdb1610cfa17555311/math-scala/src/main/scala/org/apache/mahout/math/decompositions/DQR.scala#L50",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/276"
        ]
    },
    "MAHOUT-1924": {
        "Key": "MAHOUT-1924",
        "Summary": "Add test for Cochrane-Orcutt Procedure",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 03:33",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "27/Feb/17 05:03",
        "Description": "The Cochrane-Orcutt procedure needs an R prototype based test.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/282",
            "https://github.com/apache/mahout/pull/287"
        ]
    },
    "MAHOUT-1925": {
        "Key": "MAHOUT-1925",
        "Summary": "Cochrane-Orcutt should optionally calculate dwstat",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 03:35",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "27/Feb/17 05:03",
        "Description": "The Cochrane Orcutt procedure should optionally calculate the Durbin Watson test statistic on each iteration and save it to an array in a similar way to how the betas of each iteration are done currently.\nFurther- Cochrane Orcutt should terminate early if Durbin Watson test statistic is within acceptable range.",
        "Issue Links": []
    },
    "MAHOUT-1926": {
        "Key": "MAHOUT-1926",
        "Summary": "Linear Regression Model can generate p-values outside of [0,1]",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 03:38",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "27/Feb/17 05:03",
        "Description": "Anecdotally, the p-values reported by the ordinary least squares model (utilizing the functionality in the parent, LinearRegressionFitter) have been reported outside the range of [0,1] which is nonsensical. \nNeed to recreate problem with toy data set, fix, and add test.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/288"
        ]
    },
    "MAHOUT-1927": {
        "Key": "MAHOUT-1927",
        "Summary": "Need test for Durbin-Watson Test Statistic",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 03:40",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "24/Feb/17 14:00",
        "Description": "The Durbin-Watson Test statistic test, needs an R prototype and unit test in the RegressionsTestsSuiteBase",
        "Issue Links": []
    },
    "MAHOUT-1928": {
        "Key": "MAHOUT-1928",
        "Summary": "Add Logistic Regression",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "None",
        "Component/s": "Algorithms",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 03:43",
        "Updated": "01/Feb/17 03:43",
        "Resolved": null,
        "Description": "Add logistic regression and add wrapper to create logistic regression classifier",
        "Issue Links": []
    },
    "MAHOUT-1929": {
        "Key": "MAHOUT-1929",
        "Summary": "Add Generalized Linear Models",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "None",
        "Component/s": "Algorithms",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 03:44",
        "Updated": "17/Mar/17 23:22",
        "Resolved": "17/Mar/17 23:22",
        "Description": "Implement generalize Linear Models (GLM)\nhttps://en.wikipedia.org/wiki/Generalized_linear_model",
        "Issue Links": [
            "/jira/browse/MAHOUT-1941"
        ]
    },
    "MAHOUT-1930": {
        "Key": "MAHOUT-1930",
        "Summary": "Add test for StandardScaler",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 03:46",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "20/Feb/17 13:39",
        "Description": "Add test for StandardScaler",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/280"
        ]
    },
    "MAHOUT-1931": {
        "Key": "MAHOUT-1931",
        "Summary": "Add test for MeanCenter",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "0.13.0",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 03:47",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "24/Feb/17 13:59",
        "Description": "Add test for MeanCenter preprocessor\nShould include tests of functionality for setCenters method.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/281"
        ]
    },
    "MAHOUT-1932": {
        "Key": "MAHOUT-1932",
        "Summary": "Interchangeable Solvers",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "Algorithms",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "01/Feb/17 13:10",
        "Updated": "03/Mar/18 21:22",
        "Resolved": null,
        "Description": "Currently all algorithms are solving 'closed' form.  \nIt would be good to create open form solvers and optionally invoke them. \nTwo such solvers are Stochastic Gradient Descent, and Genetic Algorithms. \nAn abstract solver trait, and implementations of at least these two solving mechanisms (to avoid bias in choosing what to include/not include in the Solver trait). \nRefactoring current algorithms to accept optional Solver.",
        "Issue Links": []
    },
    "MAHOUT-1933": {
        "Key": "MAHOUT-1933",
        "Summary": "Migrate website from CMS to Jekyll",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "Documentation,                                            website",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "02/Feb/17 01:35",
        "Updated": "29/Nov/17 20:50",
        "Resolved": "29/Nov/17 20:50",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/mahout/pull/304"
        ]
    },
    "MAHOUT-1934": {
        "Key": "MAHOUT-1934",
        "Summary": "OpenMP jars aren't being picked up in distributed Spark",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Trevor Grant",
        "Created": "02/Feb/17 15:04",
        "Updated": "04/Mar/18 02:45",
        "Resolved": "26/Feb/17 00:38",
        "Description": "When executing the following:\n```\nimport org.apache.mahout.math.decompositions._\ndrmX.checkpoint() \nval (U, V, s) = dspca(drmX, k = 30, q = 1)\n```\nI see the following:\n```\nimport org.apache.mahout.math.decompositions._\nres1: org.apache.mahout.math.drm.CheckpointedDrm[Int] = org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@2e8dd198\n[INFO] Creating org.apache.mahout.viennacl.opencl.GPUMMul solver\n[WARN] Unable to create class GPUMMul: attempting OpenMP version\n[INFO] Creating org.apache.mahout.viennacl.openmp.OMPMMul solver\norg.apache.mahout.viennacl.openmp.OMPMMul$\n[INFO] Unable to create class OMPMMul: falling back to java version\ndrmReduxX: org.apache.mahout.math.drm.DrmLike[Int] = \nOpMapBlock(OpTimesRightMatrix(org.apache.mahout.s...\n```\nThe code executes fine, but it seems the OpenMP is not getting picked up.\nTo be clear I build these jars with -Pviennacl-omp\nI the issue persists through other operations as well.  I am running on a 3 node Yarn cluster/ spark 1.6.3",
        "Issue Links": [
            "/jira/browse/MAHOUT-1911",
            "https://github.com/apache/mahout/pull/272",
            "https://github.com/apache/mahout/pull/273"
        ]
    },
    "MAHOUT-1935": {
        "Key": "MAHOUT-1935",
        "Summary": "Inefficient use of XtX calculation in Ordinary Least Squares",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "03/Feb/17 16:40",
        "Updated": "07/Feb/17 04:32",
        "Resolved": "07/Feb/17 04:08",
        "Description": "In line 59 of OrdinaryLeastSquaresModel we calculate and collect XtX, but then don't bother to use it (so it is in essence calculated twice).\n~https://github.com/apache/zeppelin/blob/master/spark/src/main/java/org/apache/zeppelin/spark/SparkInterpreter.java~\nNot sure how that link got in there. It is unrelated.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/277"
        ]
    },
    "MAHOUT-1936": {
        "Key": "MAHOUT-1936",
        "Summary": "FactorMap finds column maximums incorrectly on large data sets",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "03/Feb/17 16:45",
        "Updated": "04/Mar/18 02:46",
        "Resolved": "07/Feb/17 04:08",
        "Description": "FactorMap's fit method does not properly find the maximum of the column. \nLikely due to an impropper allreduceBlock here\nhttps://github.com/apache/mahout/blob/master/math-scala/src/main/scala/org/apache/mahout/math/algorithms/preprocessing/AsFactor.scala#L40\nAlso, factorMap in this instance might be more appropriately named \"factorMax\"",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/278"
        ]
    },
    "MAHOUT-1937": {
        "Key": "MAHOUT-1937",
        "Summary": "Model should be able to import/export to PMML",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "03/Feb/17 16:54",
        "Updated": "08/Oct/20 14:46",
        "Resolved": null,
        "Description": "The Predictive Model Markup Language is a generic format for specifying models in XML form.\nhttps://en.wikipedia.org/wiki/Predictive_Model_Markup_Language",
        "Issue Links": []
    },
    "MAHOUT-1938": {
        "Key": "MAHOUT-1938",
        "Summary": "When building on linux, haswell properties are not working.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.1",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "05/Feb/17 07:12",
        "Updated": "04/Mar/18 02:43",
        "Resolved": "27/Feb/17 02:34",
        "Description": "got a failure when building on linux with haswell.properties:\n\nWarning: Could not load platform properties for class org.apache.mahout.viennacl.openmp.OMPMMul$\nGenerating /vol0/mahout/viennacl-omp/target/classes/org/apache/mahout/viennacl/openmp/javacpp/jniViennaCL.cpp\nCompiling /vol0/mahout/viennacl-omp/target/classes/org/apache/mahout/viennacl/openmp/javacpp/linux-haswell/libjniViennaCL.so\ng++ -I/usr/include/viennacl -I/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.121.x86_64/include -I/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.121.x86_64/include/linux /vol0/mahout/viennacl-omp/target/classes/org/apache/mahout/viennacl/openmp/javacpp/jniViennaCL.cpp -msse3 -ffast-math -fopenmp -fpermissive -Wl,-rpath,$ORIGIN/ -Wl,-z,noexecstack -Wl,-Bsymbolic -march=haswell -m64 -Wall -O3 -fPIC -shared -s -o libjniViennaCL.so -lOpenCL\n/vol0/mahout/viennacl-omp/target/classes/org/apache/mahout/viennacl/openmp/javacpp/jniViennaCL.cpp:1:0: error: bad value (haswell) for -march= switch\n // Generated by JavaCPP version 1.2.4: DO NOT EDIT THIS FILE\n ^\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 14.721 s\n\n\n\nNeed to fix",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/284"
        ]
    },
    "MAHOUT-1939": {
        "Key": "MAHOUT-1939",
        "Summary": "fastutil version clash with spark distributions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Dmitriy Lyubimov",
        "Created": "11/Feb/17 00:04",
        "Updated": "04/Mar/18 02:45",
        "Resolved": "27/Feb/17 03:08",
        "Description": "Version difference in fast util breaks sparse algebra (specifically, RandomAccessSparseVector in assign, e.g., vec *= 5).\nobserved version in CDH:\n file:/opt/cloudera/parcels/CDH-5.9.0-1.cdh5.9.0.p0.21/jars/fastutil-6.3.jar\nmahout uses 7.0.12\njava.lang.UnsupportedOperationException\n        at it.unimi.dsi.fastutil.ints.AbstractInt2DoubleMap$BasicEntry.setValue(AbstractInt2DoubleMap.java:146)\n        at org.apache.mahout.math.RandomAccessSparseVector$RandomAccessElement.set(RandomAccessSparseVector.java:235)\n        at org.apache.mahout.math.VectorView$DecoratorElement.set(VectorView.java:181)\n        at org.apache.mahout.math.AbstractVector.assign(AbstractVector.java:536)\n        at org.apache.mahout.math.scalabindings.RLikeVectorOps.$div$eq(RLikeVectorOps.scala:45)\n    ...",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/285",
            "https://github.com/apache/mahout/pull/293"
        ]
    },
    "MAHOUT-1940": {
        "Key": "MAHOUT-1940",
        "Summary": "Provide a Java API to  SimilarityAnalysis and any other needed APIs",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Algorithms,                                            cooccurrence",
        "Assignee": null,
        "Reporter": "James Mackey",
        "Created": "12/Feb/17 15:48",
        "Updated": "14/Feb/17 22:42",
        "Resolved": null,
        "Description": "We want to port the functionality from org.apache.mahout.math.cf.SimilarityAnalysis.scala to java for easy integration with a java project we will be creating that derives a similarity measure from the co-occurrence and cross-occurrence matrix.",
        "Issue Links": []
    },
    "MAHOUT-1941": {
        "Key": "MAHOUT-1941",
        "Summary": "Use the existing regression infrastructure to implement Logistic Regression using Samsara",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Saikat Kanjilal",
        "Created": "12/Feb/17 19:34",
        "Updated": "03/Mar/18 21:29",
        "Resolved": null,
        "Description": "The goal is to reuse this chunk of mahout infrastructure to implement logistic regression:  https://github.com/apache/mahout/tree/master/math-scala/src/main/scala/org/apache/mahout/math/algorithms/regression\nI need this for a work related POC and will get started by extending the RegressorModel.scala, will also put forward a design proposal on the dev mailing list.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1929"
        ]
    },
    "MAHOUT-1942": {
        "Key": "MAHOUT-1942",
        "Summary": "Algorithms should be applicable to incore matrices",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "24/Feb/17 14:08",
        "Updated": "18/Dec/17 15:02",
        "Resolved": "18/Dec/17 15:02",
        "Description": "The functions in `org.apache.mahout.math.algorithms` should be able to be applied to incore matrices as well.",
        "Issue Links": []
    },
    "MAHOUT-1943": {
        "Key": "MAHOUT-1943",
        "Summary": "last minute release details for 0.13.0",
        "Type": "Epic",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "25/Feb/17 21:52",
        "Updated": "28/Apr/17 01:06",
        "Resolved": "28/Apr/17 01:06",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1944": {
        "Key": "MAHOUT-1944",
        "Summary": "remove derby metadb from dir structure before cutting RC",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "25/Feb/17 21:55",
        "Updated": "04/Mar/18 02:44",
        "Resolved": "28/Feb/17 06:09",
        "Description": "spark-shell leaves some derby directories around.   {{metastore_db/}.  Make sure these are gone before packaging up final RC.",
        "Issue Links": []
    },
    "MAHOUT-1945": {
        "Key": "MAHOUT-1945",
        "Summary": "mvn -Pmahout-release,apache-release,hadoop2 package fails",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "25/Feb/17 22:51",
        "Updated": "04/Mar/18 02:44",
        "Resolved": "27/Feb/17 23:52",
        "Description": "The first command for the release fails in mahout math:  \n\n$ mvn -Pmahout-release,apache-release,hadoop2 package\n {...}\n [ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.9.1:jar (attach-javadocs) on project mahout-math: MavenReportException: Error while creating archive: [ERROR] Exit code: 1 - /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Matrix.java:184: warning: no @return [ERROR] Matrix like(int rows, int columns); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Matrix.java:412: warning: no @return [ERROR] MatrixFlavor getFlavor(); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:230: error: bad use of '>' [ERROR] * @param power The power to use. Must be >= 0. May also be {@link Double#POSITIVE_INFINITY}. See the Wikipedia link [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:246: error: bad use of '>' [ERROR] * @param power The power to use. Must be > 1. Cannot be {@link Double#POSITIVE_INFINITY}. [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:252: error: self-closing element not allowed [ERROR] * Return the k-norm of the vector. <p/> See http://en.wikipedia.org/wiki/Lp_space <p> [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:260: warning: no @return [ERROR] double norm(double power); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:412: warning: no @return [ERROR] double getLengthSquared(); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:417: warning: no @param for v [ERROR] double getDistanceSquared(Vector v); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:417: warning: no @return [ERROR] double getDistanceSquared(Vector v); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:422: warning: no @return [ERROR] double getLookupCost(); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:428: warning: no @return [ERROR] double getIteratorAdvanceCost(); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Vector.java:433: warning: no @return [ERROR] boolean isAddConstantTime(); [ERROR] ^ [ERROR] /home/andy/sandbox/mahout/math/src/main/java/org/apache/mahout/math/Algebra.java:40: warning: no @param for a [ERROR] public static double hypot(double a, double b) { [ERROR] ^\n{...}",
        "Issue Links": []
    },
    "MAHOUT-1946": {
        "Key": "MAHOUT-1946",
        "Summary": "ViennaCL not being picked up by JNI",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Musselman",
        "Created": "26/Feb/17 22:15",
        "Updated": "04/Mar/18 02:43",
        "Resolved": "27/Feb/17 02:05",
        "Description": "Using the PR for MAHOUT-1938 but probably in master as well:\nscala> :load ./examples/bin/SparseSparseDrmTimer.mscala\nLoading ./examples/bin/SparseSparseDrmTimer.mscala...\ntimeSparseDRMMMul: (m: Int, n: Int, s: Int, para: Int, pctDense: Double, seed: Long)Long\nscala> timeSparseDRMMMul(100,100,100,1,.02,1234L)\n[INFO] Creating org.apache.mahout.viennacl.opencl.GPUMMul solver\n[INFO] Successfully created org.apache.mahout.viennacl.opencl.GPUMMul solver\ngpuRWCW\n17/02/26 13:18:54 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)\njava.lang.UnsatisfiedLinkError: no jniViennaCL in java.library.path\n\tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)\n\tat java.lang.Runtime.loadLibrary0(Runtime.java:870)\n\tat java.lang.System.loadLibrary(System.java:1122)\n\tat org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:726)\n\tat org.bytedeco.javacpp.Loader.load(Loader.java:501)\n\tat org.bytedeco.javacpp.Loader.load(Loader.java:434)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.loadLib(Context.scala:63)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.<init>(Context.scala:65)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.<clinit>(Context.scala)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.org$apache$mahout$viennacl$opencl$GPUMMul$$gpuRWCW(GPUMMul.scala:171)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$$anonfun$11.apply(GPUMMul.scala:77)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$$anonfun$11.apply(GPUMMul.scala:77)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.apply(GPUMMul.scala:127)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.apply(GPUMMul.scala:33)\n\tat org.apache.mahout.math.scalabindings.RLikeMatrixOps.$percent$times$percent(RLikeMatrixOps.scala:37)\n\tat org.apache.mahout.sparkbindings.blas.ABt$.org$apache$mahout$sparkbindings$blas$ABt$$mmulFunc$1(ABt.scala:98)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$6.apply(ABt.scala:113)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$6.apply(ABt.scala:113)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$pairwiseApply$1.apply(ABt.scala:209)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$pairwiseApply$1.apply(ABt.scala:209)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n17/02/26 13:18:54 ERROR SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-0,5,main]\njava.lang.UnsatisfiedLinkError: no jniViennaCL in java.library.path\n\tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)\n\tat java.lang.Runtime.loadLibrary0(Runtime.java:870)\n\tat java.lang.System.loadLibrary(System.java:1122)\n\tat org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:726)\n\tat org.bytedeco.javacpp.Loader.load(Loader.java:501)\n\tat org.bytedeco.javacpp.Loader.load(Loader.java:434)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.loadLib(Context.scala:63)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.<init>(Context.scala:65)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.<clinit>(Context.scala)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.org$apache$mahout$viennacl$opencl$GPUMMul$$gpuRWCW(GPUMMul.scala:171)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$$anonfun$11.apply(GPUMMul.scala:77)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$$anonfun$11.apply(GPUMMul.scala:77)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.apply(GPUMMul.scala:127)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.apply(GPUMMul.scala:33)\n\tat org.apache.mahout.math.scalabindings.RLikeMatrixOps.$percent$times$percent(RLikeMatrixOps.scala:37)\n\tat org.apache.mahout.sparkbindings.blas.ABt$.org$apache$mahout$sparkbindings$blas$ABt$$mmulFunc$1(ABt.scala:98)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$6.apply(ABt.scala:113)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$6.apply(ABt.scala:113)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$pairwiseApply$1.apply(ABt.scala:209)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$pairwiseApply$1.apply(ABt.scala:209)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n17/02/26 13:18:54 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, localhost): java.lang.UnsatisfiedLinkError: no jniViennaCL in java.library.path\n\tat java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)\n\tat java.lang.Runtime.loadLibrary0(Runtime.java:870)\n\tat java.lang.System.loadLibrary(System.java:1122)\n\tat org.bytedeco.javacpp.Loader.loadLibrary(Loader.java:726)\n\tat org.bytedeco.javacpp.Loader.load(Loader.java:501)\n\tat org.bytedeco.javacpp.Loader.load(Loader.java:434)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.loadLib(Context.scala:63)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.<init>(Context.scala:65)\n\tat org.apache.mahout.viennacl.opencl.javacpp.Context$.<clinit>(Context.scala)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.org$apache$mahout$viennacl$opencl$GPUMMul$$gpuRWCW(GPUMMul.scala:171)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$$anonfun$11.apply(GPUMMul.scala:77)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$$anonfun$11.apply(GPUMMul.scala:77)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.apply(GPUMMul.scala:127)\n\tat org.apache.mahout.viennacl.opencl.GPUMMul$.apply(GPUMMul.scala:33)\n\tat org.apache.mahout.math.scalabindings.RLikeMatrixOps.$percent$times$percent(RLikeMatrixOps.scala:37)\n\tat org.apache.mahout.sparkbindings.blas.ABt$.org$apache$mahout$sparkbindings$blas$ABt$$mmulFunc$1(ABt.scala:98)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$6.apply(ABt.scala:113)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$6.apply(ABt.scala:113)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$pairwiseApply$1.apply(ABt.scala:209)\n\tat org.apache.mahout.sparkbindings.blas.ABt$$anonfun$pairwiseApply$1.apply(ABt.scala:209)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191)\n\tat org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:64)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n17/02/26 13:18:54 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n17/02/26 13:18:54 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: /tmp/spark-34fcf1f5-a8e6-46f8-bdd6-dd1abab2ded1\njava.io.IOException: Failed to delete: /tmp/spark-34fcf1f5-a8e6-46f8-bdd6-dd1abab2ded1\n\tat org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)\n\tat org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)\n\tat org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)\n\tat org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1817)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)\n\tat scala.util.Try$.apply(Try.scala:161)\n\tat org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)\n\tat org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)\n\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)",
        "Issue Links": []
    },
    "MAHOUT-1947": {
        "Key": "MAHOUT-1947",
        "Summary": "Make a profile for flink module",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "27/Feb/17 22:56",
        "Updated": "28/Apr/17 01:01",
        "Resolved": "28/Apr/17 01:01",
        "Description": "We removed the flink module from the root pom due to intermittent OOM errors, so let's add a flink profile for a release soon.",
        "Issue Links": []
    },
    "MAHOUT-1948": {
        "Key": "MAHOUT-1948",
        "Summary": "A release step causes errors for some people",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "02/Mar/17 03:16",
        "Updated": "03/Mar/18 21:30",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1949": {
        "Key": "MAHOUT-1949",
        "Summary": "Create Docker Base IT Framework",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Mar/17 16:14",
        "Updated": "28/Apr/17 14:44",
        "Resolved": "28/Apr/17 01:07",
        "Description": "As we move to multiple spark/scala/native/etc versions we need a more sane method for testings. \nMany projects use a docker based test env.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/296"
        ]
    },
    "MAHOUT-1950": {
        "Key": "MAHOUT-1950",
        "Summary": "Unread Block Data in Spark Shell Pseudo Cluster",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "None",
        "Component/s": "Mahout spark shell",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "05/Mar/17 01:11",
        "Updated": "09/Mar/17 20:35",
        "Resolved": "06/Mar/17 05:45",
        "Description": "When doing an operation in the Spark Shell on a Pseudo Cluster, a `java.lang.IllegalStateException: unread block data` error is thrown. \nResearch and stack trace implies there is some issue with serialization.  Other issues with spark in cluster mode, hint that the Kryo Jars aren't being shipped around.\nToying has shown that:\n`$SPARK_HOME/bin/spark-shell --jars \"/opt/mahout/math-scala/target/mahout-math-scala_2.10-0.13.0-SNAPSHOT.jar,/opt/mahout/math/target/mahout-math-0.13.0-SNAPSHOT.jar,/opt/mahout/spark/target/mahout-spark_2.10-0.13.0-SNAPSHOT.jar,/opt/mahout/spark/target/mahout-spark_2.10-0.13.0-SNAPSHOT-dependency-reduced.jar\" -i $MAHOUT_HOME/bin/load-shell.scala --conf spark.kryo.referenceTracking=false --conf spark.kryo.registrator=org.apache.mahout.sparkbindings.io.MahoutKryoRegistrator --conf spark.kryoserializer.buffer=32k --conf spark.kryoserializer.buffer.max=600m --conf spark.serializer=org.apache.spark.serializer.KryoSerializer`\nworks, and should be used in place of:\nhttps://github.com/apache/mahout/blob/master/bin/mahout#L294",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/291"
        ]
    },
    "MAHOUT-1951": {
        "Key": "MAHOUT-1951",
        "Summary": "Drivers don't run with remote Spark",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Classification,                                            CLI,                                            Collaborative Filtering",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "06/Mar/17 16:32",
        "Updated": "01/Nov/17 01:31",
        "Resolved": "01/Nov/17 01:31",
        "Description": "Missing classes when running these jobs because the dependencies-reduced jar, passed to Spark for serialization purposes, does not contain all needed classes.\nFound by a user.",
        "Issue Links": [
            "/jira/browse/MAHOUT-1762"
        ]
    },
    "MAHOUT-1952": {
        "Key": "MAHOUT-1952",
        "Summary": "Allow pass-through of params for driver's CLI to spark-submit",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "Classification,                                            CLI,                                            Collaborative Filtering",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "06/Mar/17 16:50",
        "Updated": "03/Mar/18 21:25",
        "Resolved": null,
        "Description": "remove driver CLI args that are dups of what spark-submit can do and allow passthrough of arbitrary extra CLI to spar-submit using spark-submit parsing.",
        "Issue Links": []
    },
    "MAHOUT-1953": {
        "Key": "MAHOUT-1953",
        "Summary": "jars in $MAHOUT_HOME should be deleted on mvn clean",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "08/Mar/17 15:23",
        "Updated": "23/Jun/17 04:19",
        "Resolved": "23/Jun/17 04:19",
        "Description": "MAHOUT-1950 copies jars to $MAHOUT_HOME to be picked up.  \nThey should be deleted by mvn clean",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/312"
        ]
    },
    "MAHOUT-1954": {
        "Key": "MAHOUT-1954",
        "Summary": "Create a test module for GPU bindings.",
        "Type": "Test",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "09/Mar/17 00:51",
        "Updated": "03/Mar/18 21:31",
        "Resolved": null,
        "Description": "Quick and dirty copy of VCL bindings to see how easily we can copy over the  bindings to a different set.",
        "Issue Links": []
    },
    "MAHOUT-1955": {
        "Key": "MAHOUT-1955",
        "Summary": "Viennacl jars are not being picked up by the shell startup script",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "Mahout spark shell",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "16/Mar/17 05:54",
        "Updated": "04/Mar/18 02:44",
        "Resolved": "18/Mar/17 20:09",
        "Description": "When building for ViennaCL for OpenCL or OpenMP: \n\n$ mvn clean install -Pviennacl -Phadoop2 -DskipTests\n\n\nor\n\n$ mvn clean install -Pviennacl-omp -Phadoop2 -DskipTests\n\n\nThe mahout-native-*_2.10.jar s are not being picked up by the shell which  are now in the top-level directory after the build. Fix is to add a copy plugin to the viennacl and viennacl-omp pom.xml s.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/294"
        ]
    },
    "MAHOUT-1956": {
        "Key": "MAHOUT-1956",
        "Summary": "Update Readme Page",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Palumbo",
        "Created": "16/Mar/17 17:06",
        "Updated": "06/Mar/19 04:19",
        "Resolved": "06/Mar/19 04:19",
        "Description": "Need to update the README page for new build instructions.  We may be able to use the instructions that I recently sent out.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/297"
        ]
    },
    "MAHOUT-1957": {
        "Key": "MAHOUT-1957",
        "Summary": "Ensure that ViennaCL Jars are included in the binary distribution.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "18/Mar/17 20:09",
        "Updated": "04/Mar/18 02:44",
        "Resolved": "28/Apr/17 01:03",
        "Description": "The assembly/bin.xml for the binary distribution artifact does not include the ViennaCL Jars:\nmahout-native-viennacl_2.10.jar\nmahout-native-viennacl-omp_2.10.jar\nhttps://github.com/apache/mahout/blob/master/distribution/src/main/assembly/bin.xml\nThese need to be included.",
        "Issue Links": []
    },
    "MAHOUT-1958": {
        "Key": "MAHOUT-1958",
        "Summary": "CityBlockSimilarity.itemSimilarities can overflow",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Hao Zhong",
        "Created": "21/Mar/17 08:24",
        "Updated": "20/Oct/20 12:40",
        "Resolved": null,
        "Description": "The CityBlockSimilarity.itemSimilarities method has the following code:\nCityBlockSimilarity.java\n      int preferring2 = dataModel.getNumUsersWithPreferenceFor(itemID2s[i]);\n      int intersection = dataModel.getNumUsersWithPreferenceFor(itemID1, itemID2s[i]);\n\n\nHere, the two methods return long values, and can overflow. Indeed, LogLikelihoodSimilaritydoItemSimilarity once had the same problem. The fixed code is\nLogLikelihoodSimilaritydoItemSimilarity.java\n    long preferring1 = dataModel.getNumUsersWithPreferenceFor(itemID1);\n    long numUsers = dataModel.getNumUsers();\n\n\nPlease refer to MAHOUT-738 for details.",
        "Issue Links": []
    },
    "MAHOUT-1959": {
        "Key": "MAHOUT-1959",
        "Summary": "BallKMeans.iterativeAssignment can set wrong weights.",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Hao Zhong",
        "Created": "21/Mar/17 09:59",
        "Updated": "20/Oct/20 12:40",
        "Resolved": null,
        "Description": "I notice that the BallKMeans.iterativeAssignment method uses the following code to calculate weights:\nBallKMeans.java\nfor (WeightedVector datapoint : datapoints) {\n        Centroid closestCentroid = (Centroid) centroids.searchFirst(datapoint, false).getValue();\n        closestCentroid.setWeight(closestCentroid.getWeight() + datapoint.getWeight());\n      }\n\n\nIn MAHOUT-1237, the buggy code is the same way to calculate the weight:\nClusteringUtils.java\nfor (Vector vector : datapoints) {\n      Centroid closest = (Centroid) centroids.searchFirst(vector, false).getValue();\n      totalCost += closest.getWeight();\n    }\n\n\nThe fixed code is as follow:\nClusteringUtils.java\nfor (Vector vector : datapoints) {\n      totalCost += centroids.searchFirst(vector, false).getWeight();\n    }\n\n\nI am not quite sure whether BallKMeans.iterativeAssignment sets the right weights. Please check it.",
        "Issue Links": []
    },
    "MAHOUT-1960": {
        "Key": "MAHOUT-1960",
        "Summary": "Flipped sign in the SparseSparseDrmTimer.mscala causes Desnse Matrix multiplication",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "26/Mar/17 04:05",
        "Updated": "04/Mar/18 02:44",
        "Resolved": "28/Apr/17 01:06",
        "Description": "In SparseSparseDrmTimer.mscala with the signature: \n def timeSparseDRMMMul(m: Int, n: Int, s: Int, para: Int, pctDense: Double = .20, seed: Long = 1234L): Long = {\nthe pctDense argument indicates the percent of nonZero elements in the matrix. \nAs is the code to produce a random DRM for testing is as follows:\n\n  val drmA = drmParallelizeEmpty(m , s, para).mapBlock(){\n       case (keys,block:Matrix) =>\n         val R =  scala.util.Random\n         R.setSeed(seed)\n         val blockB = new SparseRowMatrix(block.nrow, block.ncol) \n         blockB := {x => if (R.nextDouble > pctDense) R.nextDouble else x }\n       (keys -> blockB)\n}\n\n\nThe greater than sign in line:\n    blockB := \n{x => if (R.nextDouble > pctDense) R.nextDouble else x }\n \nshould be a less than sign; i.e.:\n     blockB := \n{x => if (R.nextDouble < pctDense) R.nextDouble else x }\n \nThis incorrect sign produces matrices of Incorrect density and can cause OOM errors.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/298"
        ]
    },
    "MAHOUT-1961": {
        "Key": "MAHOUT-1961",
        "Summary": "Convert old .mapred API",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "Math",
        "Assignee": null,
        "Reporter": "Hao Zhong",
        "Created": "29/Mar/17 00:46",
        "Updated": "21/May/17 03:28",
        "Resolved": "21/May/17 03:28",
        "Description": "MAHOUT-1427 tries to migrate the old .mapred API to new .mapreduce API. Although TimesSquaredJob.java is thus modified, I notice that other files such as MatrixMultiplicationJob still use the old .mapred API. Shall we thoroughly migrate all the callsites?",
        "Issue Links": []
    },
    "MAHOUT-1962": {
        "Key": "MAHOUT-1962",
        "Summary": "Add F-test to Linear Regression  Fitness Tests",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.12.0,                                            0.12.1,                                            0.13.0,                                            0.12.2",
        "Fix Version/s": "0.13.1",
        "Component/s": "Algorithms",
        "Assignee": null,
        "Reporter": "Dustin VanStee",
        "Created": "29/Mar/17 19:18",
        "Updated": "21/May/17 04:08",
        "Resolved": "21/May/17 03:48",
        "Description": "This update will modify org.apache.mahout.math.algorithms.regression.tests.FittnessTests.scala and add an overall Ftest for significance of one or more parameters being not equal to zero.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/300"
        ]
    },
    "MAHOUT-1963": {
        "Key": "MAHOUT-1963",
        "Summary": "Signs are Flipped in the ViennaCL Sparse %*% Sparse tests",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "30/Mar/17 03:11",
        "Updated": "23/Jun/17 20:42",
        "Resolved": "23/May/17 22:54",
        "Description": "Here aand in other places, In the ViennaCL (both modules) Unit tests, the signs are flipped when adding random data to matrices.\n\n    // add some sparse data with a 20% threshold\n    mxA := { (_, _, v) => if (r.nextDouble()  .20) r.nextDouble() else v }\n    mxB := { (_, _, v) => if (r.nextDouble() < .20) r.nextDouble() else v }",
        "Issue Links": []
    },
    "MAHOUT-1964": {
        "Key": "MAHOUT-1964",
        "Summary": "Logo in the spark-shell is broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "Mahout spark shell",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "16/Apr/17 04:13",
        "Updated": "29/Nov/17 21:13",
        "Resolved": "29/Nov/17 21:13",
        "Description": "Mahout logo in the shell has a few characters misplaced.",
        "Issue Links": []
    },
    "MAHOUT-1965": {
        "Key": "MAHOUT-1965",
        "Summary": "Update CI to cover multiple spark/scala builds",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.1",
        "Component/s": "Integration",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "19/Apr/17 18:29",
        "Updated": "23/Jun/17 04:41",
        "Resolved": "22/Apr/17 18:50",
        "Description": "We are targeting to release for Spark 1.6/2.0/2.1 in 0.13.1 and Scala 2.10/2.11/2.11 respectively.\nNeed to update TravisCI to cover all of these.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/303"
        ]
    },
    "MAHOUT-1966": {
        "Key": "MAHOUT-1966",
        "Summary": "No longer java 1.7 compatible",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "19/Apr/17 23:34",
        "Updated": "20/Apr/17 12:37",
        "Resolved": "20/Apr/17 12:37",
        "Description": "When building ViennaCL or OMP modules with oracle java 7 we get the following errors:\n```\n[ERROR] /home/travis/build/rawkintrevo/mahout/viennacl/src/main/scala/org/apache/mahout/viennacl/opencl/javacpp/CompressedMatrix.scala:33: error: in class CompressedMatrix, multiple overloaded alternatives of constructor CompressedMatrix define default arguments.\n[ERROR] final class CompressedMatrix(defaultCtr: Boolean = true) extends Pointer {\n[ERROR]             ^\n[ERROR] /home/travis/build/rawkintrevo/mahout/viennacl/src/main/scala/org/apache/mahout/viennacl/opencl/javacpp/DenseColumnMatrix.scala:37: error: in class DenseColumnMatrix, multiple overloaded alternatives of constructor DenseColumnMatrix define default arguments.\n[ERROR] final class DenseColumnMatrix(initDefault:Boolean = true) extends MatrixBase {\n[ERROR]             ^\n[ERROR] /home/travis/build/rawkintrevo/mahout/viennacl/src/main/scala/org/apache/mahout/viennacl/opencl/javacpp/DenseRowMatrix.scala:32: error: in class DenseRowMatrix, multiple overloaded alternatives of constructor DenseRowMatrix define default arguments.\n[ERROR] class DenseRowMatrix(initDefault: Boolean = true) extends MatrixBase {\n[ERROR]       ^\n[ERROR] /home/travis/build/rawkintrevo/mahout/viennacl/src/main/scala/org/apache/mahout/viennacl/opencl/javacpp/VCLVector.scala:30: error: in class VCLVector, multiple overloaded alternatives of constructor VCLVector define default arguments.\n[ERROR] final class VCLVector(defaultCtr: Boolean = true) extends VectorBase {\n[ERROR]             ^\n```\nThis is alleviated by moving to openjdk8",
        "Issue Links": []
    },
    "MAHOUT-1967": {
        "Key": "MAHOUT-1967",
        "Summary": "Make Flink Profile",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.1",
        "Component/s": "Flink",
        "Assignee": "Aditya AS",
        "Reporter": "Trevor Grant",
        "Created": "20/Apr/17 01:55",
        "Updated": "23/Jun/17 04:40",
        "Resolved": "22/Apr/17 04:19",
        "Description": "Currently Flink module is just commented out of the pom. This is tacky. \nFlink should have profile which by default is disabled.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/305"
        ]
    },
    "MAHOUT-1968": {
        "Key": "MAHOUT-1968",
        "Summary": "Create Profiles for Scala 2.11 and Scala 2.10",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "None",
        "Component/s": "Integration",
        "Assignee": "Aditya AS",
        "Reporter": "Trevor Grant",
        "Created": "21/Apr/17 22:38",
        "Updated": "21/May/17 03:24",
        "Resolved": "21/May/17 03:24",
        "Description": "As we move towards 0.13.1 goal of multiple scala/spark support, we should have each as a profile.  2.10 will probably continue to be the default, but while you're in there, might as well do the 2.10 as well because day will come soon when its not default any more.",
        "Issue Links": []
    },
    "MAHOUT-1969": {
        "Key": "MAHOUT-1969",
        "Summary": "Create Profiles for Spark 1.6, 2.0.2, 2.1.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "21/Apr/17 22:40",
        "Updated": "29/Jun/17 14:13",
        "Resolved": "29/Jun/17 13:38",
        "Description": "Create profiles for spark 1.6, 2.0.2 and 2.1.0.  Spark 1.6 should be default. \nUpdate CI tests to use profiles instead of variable setting.\nFurther- Spark 1.6 should invoke scala 2.10 profile by default, Spark 2.x should invoke scala 2.11.\nAs such Mahout-1968 is a blocker",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/329"
        ]
    },
    "MAHOUT-1970": {
        "Key": "MAHOUT-1970",
        "Summary": "Utilize Spark Pseudoclusters in TravisCi",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "22/Apr/17 03:26",
        "Updated": "23/Jun/17 04:39",
        "Resolved": "22/Apr/17 04:19",
        "Description": "In RCs we always need to test everything in Spark pseudoclusters, as bugs occasionally work there way in here (jars not shipping, etc.- things don't always quite the same in a pseudocluster as they do in local[*])\nAs we profliferate our supported Spark versions, we need to automate this with TravisCI",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/306"
        ]
    },
    "MAHOUT-1971": {
        "Key": "MAHOUT-1971",
        "Summary": "Aggregate Transpose Bug",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "22/Apr/17 18:37",
        "Updated": "22/Apr/17 19:30",
        "Resolved": "22/Apr/17 19:00",
        "Description": "In the At operation of Flink and spark there is a line that looks like:\ncase (row, blockRow) => colV(row) = blockA(blockRow, blockCol)\nthat ought to be \ncase (row, blockRow) => colV(row) += blockA(blockRow, blockCol)\nSince it isnt'- the 'aggregate transpose' does not operate as expected. \nThis needs to be updated in the Spark and Flink bindings (same one char fix). \nTest coverage should be updated to catch this.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/307"
        ]
    },
    "MAHOUT-1972": {
        "Key": "MAHOUT-1972",
        "Summary": "Create Quickstart Writeup for Mahout 0.13.0 documentation page",
        "Type": "Documentation",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "Documentation,                                            website",
        "Assignee": null,
        "Reporter": "Dustin VanStee",
        "Created": "23/Apr/17 02:51",
        "Updated": "03/Mar/18 21:26",
        "Resolved": null,
        "Description": "A new quickstart page needs to be constructed to help first time users quickly do something using Mahout.   Ideas could be using the Mahout shell, or maybe a mahout and your favorite notebook quick start.\nFile to modify is website/_pages/docs/0.13.0",
        "Issue Links": []
    },
    "MAHOUT-1973": {
        "Key": "MAHOUT-1973",
        "Summary": "When building profiles conditionally (say Flink, Viennacl) a hadoop.version related error occurs. Need to check if conditional building of other modules also has this error and fix the issue.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.1",
        "Component/s": "build",
        "Assignee": "Aditya AS",
        "Reporter": "Aditya AS",
        "Created": "23/Apr/17 09:27",
        "Updated": "28/Jun/17 01:05",
        "Resolved": "26/Jun/17 21:44",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/mahout/pull/321"
        ]
    },
    "MAHOUT-1974": {
        "Key": "MAHOUT-1974",
        "Summary": "CUDA support",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nikolay Sakharnykh",
        "Created": "27/Apr/17 02:31",
        "Updated": "15/Jul/17 22:23",
        "Resolved": null,
        "Description": "Implement native CUDA bindings using JCuda",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/310",
            "https://github.com/apache/mahout/pull/318"
        ]
    },
    "MAHOUT-1975": {
        "Key": "MAHOUT-1975",
        "Summary": "Pull Request Template",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.0,                                            0.13.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "01/May/17 19:47",
        "Updated": "05/May/17 04:29",
        "Resolved": "03/May/17 23:39",
        "Description": "Create a template for guiding new pull requests.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/313"
        ]
    },
    "MAHOUT-1976": {
        "Key": "MAHOUT-1976",
        "Summary": "Add Canopy Clustering Algorithm",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.2",
        "Fix Version/s": "0.13.1",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "03/May/17 23:23",
        "Updated": "23/Jun/17 04:35",
        "Resolved": "21/May/17 03:49",
        "Description": "Primarily, we need to lay out the clustering section of the Algorihtms Framework.\nThe Canopy Clustering Algorithm is very simple and yet very useful as a preprocessing step for more advanced clustering algorithms such as KMeans and Hierarchical Clustering. \nhttps://en.wikipedia.org/wiki/Canopy_clustering_algorithm\nThe majority of the \"work\" on this PR will be creating the framework. \nIt is also one of the Legacy MR algorithms that would be nice to port.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/314"
        ]
    },
    "MAHOUT-1977": {
        "Key": "MAHOUT-1977",
        "Summary": "Update History of Mahout Page on Website",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "05/May/17 01:56",
        "Updated": "05/May/17 01:56",
        "Resolved": null,
        "Description": "The Apache Mahout project has a long and rich history.  \nThe file \nhttps://github.com/apache/mahout/blob/master/website/front/community/history.md\nis currently stubbed out with some headers (that are only directional).  While currently active members could fill this out with some facts, it would be really great if we could get past PMCs/ PMC Chairs/ Committers to fill in relevant portions of the \"History of the Apache Mahout Project\", preserving the history of the project with first person accounts, as opposed to something reconstructed from oral tradition. \nContributors are welcome to leave their pieces in the comments and active members can add to the code base if this is more convenient (of course contributors are always welcome to add content directly via pull request as well). \nThank you in advance.",
        "Issue Links": []
    },
    "MAHOUT-1978": {
        "Key": "MAHOUT-1978",
        "Summary": "Implement \"a provably good seeding method for k-means\"",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "Clustering",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "05/May/17 20:04",
        "Updated": "25/Jun/20 20:50",
        "Resolved": null,
        "Description": "When Building out our algorithm library, Implement ASSUMPTION-FREE K-MC^2 from https://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means.pdf",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/361"
        ]
    },
    "MAHOUT-1979": {
        "Key": "MAHOUT-1979",
        "Summary": "Remove references to develop branch",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "07/May/17 20:16",
        "Updated": "07/May/17 23:33",
        "Resolved": "07/May/17 23:07",
        "Description": "There are references to a develop branch on the (git hub based) website, and in the PR template.\nRemove these",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/315"
        ]
    },
    "MAHOUT-1980": {
        "Key": "MAHOUT-1980",
        "Summary": "Create Tutorial for Contributing Algorithms",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "07/May/17 20:28",
        "Updated": "08/May/17 00:06",
        "Resolved": "07/May/17 23:30",
        "Description": "Make a walk through explaining how to contribute an algorithm in a way that is consistent with the new 'algorithms' framework.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/316"
        ]
    },
    "MAHOUT-1981": {
        "Key": "MAHOUT-1981",
        "Summary": "Update Home Page",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "08/May/17 12:39",
        "Updated": "23/Feb/18 20:46",
        "Resolved": "23/Feb/18 20:46",
        "Description": "Need landing page that gives users high level understanding of project. 3-6 talking points.",
        "Issue Links": []
    },
    "MAHOUT-1982": {
        "Key": "MAHOUT-1982",
        "Summary": "Website Search Bar Functionality",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "08/May/17 12:40",
        "Updated": "09/May/17 05:22",
        "Resolved": "09/May/17 05:22",
        "Description": "Search bar on website reboot is only for show- add functionality",
        "Issue Links": []
    },
    "MAHOUT-1983": {
        "Key": "MAHOUT-1983",
        "Summary": "Clean up Stubs before launching",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "08/May/17 12:45",
        "Updated": "13/May/17 08:01",
        "Resolved": "08/May/17 23:04",
        "Description": "There are many stubs of pages 'that would be nice' but haven't materialized including \nwebsite/docs/native-solvers (need seperate JIRA, as these are high priority documentation that needs to be produced)\nlinks in nav-bar to non-existent page w tutorials on creating own distributed backend and native solver (also move this to JIRA)\ncomb through rest of site looking for TODO and stub pages- remove / clean up where appropriate.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/317"
        ]
    },
    "MAHOUT-1984": {
        "Key": "MAHOUT-1984",
        "Summary": "Establish procedure for publishing new website",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "08/May/17 13:53",
        "Updated": "29/Nov/17 20:50",
        "Resolved": "29/Nov/17 20:50",
        "Description": "Figure out how the new website will be published and write up how-to doc",
        "Issue Links": []
    },
    "MAHOUT-1985": {
        "Key": "MAHOUT-1985",
        "Summary": "Add page of recent and upcoming talks",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "08/May/17 13:57",
        "Updated": "08/May/17 23:02",
        "Resolved": "08/May/17 23:02",
        "Description": "probably belongs in `website/front/community/talks.md`",
        "Issue Links": []
    },
    "MAHOUT-1986": {
        "Key": "MAHOUT-1986",
        "Summary": "Refactor Navbar on Homepage",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "19/May/17 05:15",
        "Updated": "04/Mar/18 02:47",
        "Resolved": null,
        "Description": "Remove Algorithms, MapReduce Basics, Map Reduce Algorithsm\nAdd \"Docs\"\n4 item under docs= \"0.13.1-SNAPSHOT\" which points to \nmahout.apache.org/docs/0.13.1-SNAPSHOT\nhttp://mahout.apache.org/docs/0.13.1-SNAPSHOT/scaladocs/math-scala/index.html\nhttp://mahout.apache.org/docs/0.13.1-SNAPSHOT/scaladocs/spark/index.html\nhttp://mahout.apache.org/docs/0.13.1-SNAPSHOT/javadocs/index.html\nLogic: this de-emphasizes the map-reduce, however all material in those drop downs has been ported to the 'docs'",
        "Issue Links": []
    },
    "MAHOUT-1987": {
        "Key": "MAHOUT-1987",
        "Summary": "clean up old feature branches",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "19/May/17 05:25",
        "Updated": "21/May/17 03:13",
        "Resolved": "21/May/17 03:13",
        "Description": "the following feature branches are no longer required and they clutter things up:\n`flink-bindings`\n`mahout-1865`\n`spark-1.2`\nUnless there is a reason these still need to exist?",
        "Issue Links": []
    },
    "MAHOUT-1988": {
        "Key": "MAHOUT-1988",
        "Summary": "ViennaCL and OMP not building for Scala 2.11",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Palumbo",
        "Created": "24/May/17 23:39",
        "Updated": "26/Jun/17 22:12",
        "Resolved": "26/Jun/17 21:43",
        "Description": "After building mahout against scala 2.11: \n\nmvn clean install -Dscala.version=2.11.4 -Dscala.compat.version=2.11 -Phadoop2  -DskipTests\n\n\nViennaCL jars are built hard-coded to scala 2.10.  This is currently blocking the 0.13.1 release. \n\nmahout-h2o_2.11-0.13.1-SNAPSHOT.jar\nmahout-hdfs-0.13.1-SNAPSHOT.jar\nmahout-math-0.13.1-SNAPSHOT.jar\nmahout-math-scala_2.11-0.13.1-SNAPSHOT.jar\nmahout-mr-0.13.1-SNAPSHOT.jar\nmahout-native-cuda_2.10-0.13.0-SNAPSHOT.jar\nmahout-native-cuda_2.10-0.13.1-SNAPSHOT.jar\nmahout-native-viennacl_2.10-0.13.1-SNAPSHOT.jar\nmahout-native-viennacl-omp_2.10-0.13.1-SNAPSHOT.jar\nmahout-spark_2.11-0.13.1-SNAPSHOT-dependency-reduced.jar\nmahout-spark_2.11-0.13.1-SNAPSHOT.jar",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/326"
        ]
    },
    "MAHOUT-1989": {
        "Key": "MAHOUT-1989",
        "Summary": "Paul Komarek's Thesis URL is 403'ing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.1",
        "Component/s": "Documentation",
        "Assignee": "Dustin VanStee",
        "Reporter": "Clay B.",
        "Created": "25/May/17 05:58",
        "Updated": "27/Jun/17 01:59",
        "Resolved": "27/Jun/17 01:59",
        "Description": "Going to http://www.autonlab.org/autonweb/14709/version/4/part/5/data/komarek:lr_thesis.pdf?branch=main&language=en e.g. from https://mahout.apache.org/users/classification/logistic-regression.html results in a 403. The current URL is from his lab. Using a web search I find that his thesis is hosted at http://repository.cmu.edu/cgi/viewcontent.cgi?article=1221&context=robotics which is likely to be stable.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/322"
        ]
    },
    "MAHOUT-1990": {
        "Key": "MAHOUT-1990",
        "Summary": "Implement Multilayer Perceptron",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.13.2",
        "Fix Version/s": "None",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "28/May/17 16:09",
        "Updated": "21/Oct/20 11:19",
        "Resolved": "21/Oct/20 11:19",
        "Description": "Following strategy\nIt should- \n1. implement incoreMLPs which can be 'plugged together' for purposes of back propegation (this makes for easy extension into more complex networks)\n2. implement a common distributed MLP which maps out incoreMLPs and then averages parameters\n3. regression and classifier wrappers around the base MLP to reduce duplication of code\n4. would be nice to make distributed and incore neural network 'trait' for consistent API across all future neural networks.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/323"
        ]
    },
    "MAHOUT-1991": {
        "Key": "MAHOUT-1991",
        "Summary": "Implement naive DBSCAN Algorithm - O(n^2) complexity",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Algorithms",
        "Assignee": "Aditya AS",
        "Reporter": "Aditya AS",
        "Created": "06/Jun/17 08:34",
        "Updated": "25/Jun/20 20:01",
        "Resolved": null,
        "Description": "Implement the naive DBSCAN algorithm in Mahout Samsara, as part of the Algorithms Framework.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/334"
        ]
    },
    "MAHOUT-1992": {
        "Key": "MAHOUT-1992",
        "Summary": "Mahout PMC + Chair Can't Create Scrum Boards",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "24/Jun/17 21:33",
        "Updated": "24/Jun/17 21:43",
        "Resolved": "24/Jun/17 21:43",
        "Description": "Hi all,\nI've been having a problem getting the JIRA Permissions/ownership changed over to myself since being made Mahout Chair.\u2002\u2002I'm unable to create a scrum board, and get the attached dialog when I try.",
        "Issue Links": []
    },
    "MAHOUT-1993": {
        "Key": "MAHOUT-1993",
        "Summary": "ViennaCL dependency-reduced.xml assembly scala versions are hardcoded to scala 2.10",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "24/Jun/17 22:01",
        "Updated": "27/Jun/17 22:29",
        "Resolved": "27/Jun/17 22:29",
        "Description": "in spark/src/assembly/dependency-reduced.xml ViennaCL deps to be shipped are hard-coded to scala-2.10.:\n\n    <include>org.apache.mahout:mahout-native-viennacl_2.10</include>\n    <include>org.apache.mahout:mahout-native-viennacl-omp_2.10</include>",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/327"
        ]
    },
    "MAHOUT-1994": {
        "Key": "MAHOUT-1994",
        "Summary": "Remove ViennCL jars upon mvn clean.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "24/Jun/17 22:10",
        "Updated": "27/Jun/17 22:35",
        "Resolved": "27/Jun/17 22:11",
        "Description": "currently, after running `mvn clean` from $MAHOUT_HOME, VinenaCL jars are not removed from the base level directory.\nadd in something similar to: \n\n <plugin>\n        <artifactId>maven-clean-plugin</artifactId>\n        <version>3.0.0</version>\n        <configuration>\n          <filesets>\n            <fileset>\n              <directory>../</directory>\n              <includes>\n                <include>mahout-spark*.jar</include>\n              </includes>\n              <followSymlinks>false</followSymlinks>\n            </fileset>\n          </filesets>\n        </configuration>\n      </plugin>\n    </plugins>\n  </build>\n\n\nin both vienncl and viennacl-omp module pom.xmls",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/328"
        ]
    },
    "MAHOUT-1995": {
        "Key": "MAHOUT-1995",
        "Summary": "An incorrect file path",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Hao Zhong",
        "Created": "04/Jul/17 01:49",
        "Updated": "03/Mar/18 21:30",
        "Resolved": null,
        "Description": "The SSVDHelper_sniffInputLabelType method has the following code:\n\n static Class<? extends Writable> sniffInputLabelType(Path[] inputPath,\n                                                       Configuration conf)\n    throws IOException {\n    FileSystem fs = FileSystem.get(conf);\n    ...\n\n\nHere, when FileSystem.get(conf)  has no URL, it can cause errors. MAHOUT-971 fixed a similar bug, and the buggy code is:\n\npublic void run() throws IOException {\n\n    Deque<Closeable> closeables = Lists.newLinkedList();\n    try {\n      Class<? extends Writable> labelType =\n        sniffInputLabelType(inputPath, conf);\n      FileSystem fs = FileSystem.get(conf);\n    ...\n\n\nThe fixed code is as follow:\n\npublic void run() throws IOException {\n\n    Deque<Closeable> closeables = Lists.newLinkedList();\n    try {\n      Class<? extends Writable> labelType =\n        sniffInputLabelType(inputPath, conf);\n      FileSystem fs = FileSystem.get(outputPath.toUri(), conf);\n    ...",
        "Issue Links": []
    },
    "MAHOUT-1996": {
        "Key": "MAHOUT-1996",
        "Summary": "AdaptiveLogisticModelParameters can throw NPE",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "None",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Hao Zhong",
        "Created": "04/Jul/17 02:23",
        "Updated": "03/Mar/18 21:30",
        "Resolved": null,
        "Description": "The AdaptiveLogisticModelParameters_saveTo method has the following code:\n\n public void saveTo(OutputStream out) throws IOException {\n    if (alr != null) {\n      alr.close();\n    }\n    setTargetCategories(getCsvRecordFactory().getTargetCategories());\n    write(new DataOutputStream(out));\n  }\n\n\nThe above code can throw NPE. MAHOUT-1196 fixed a similar bug. Its buggy code is identical:\n\n public void saveTo(OutputStream out) throws IOException {\n    if (alr != null) {\n      alr.close();\n    }\n    setTargetCategories(getCsvRecordFactory().getTargetCategories());\n    write(new DataOutputStream(out));\n  }\n\n\nThe fixed code is as follow:\n\npublic void saveTo(OutputStream out) throws IOException {\n    Closeables.close(lr, false);\n    targetCategories = getCsvRecordFactory().getTargetCategories();\n    write(new DataOutputStream(out));\n  }",
        "Issue Links": []
    },
    "MAHOUT-1997": {
        "Key": "MAHOUT-1997",
        "Summary": "mahout_issue",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "maya babuji",
        "Created": "05/Jul/17 20:55",
        "Updated": "23/Feb/18 20:47",
        "Resolved": "23/Feb/18 20:47",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-1998": {
        "Key": "MAHOUT-1998",
        "Summary": "Update POMs for Spark Build Names",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.2",
        "Fix Version/s": "0.13.2",
        "Component/s": "spark",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "10/Jul/17 00:50",
        "Updated": "11/Jul/17 14:07",
        "Resolved": "11/Jul/17 14:07",
        "Description": "As discussed on dev@m.a.o we need to come up with naming conventions for publishing artifacts against multiple spark build versions.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/330"
        ]
    },
    "MAHOUT-1999": {
        "Key": "MAHOUT-1999",
        "Summary": "Automate Release Process with Build Script",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.2",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "10/Jul/17 14:25",
        "Updated": "21/Oct/20 11:18",
        "Resolved": "21/Oct/20 11:18",
        "Description": "With the proliferation of binaries across multiple Scala / Spark versions- a release script should be implemented to support multiple builds.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/350"
        ]
    },
    "MAHOUT-2000": {
        "Key": "MAHOUT-2000",
        "Summary": "Add Spark 2.2.0 as supported binary release",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Dustin VanStee",
        "Created": "12/Jul/17 15:28",
        "Updated": "21/Oct/20 11:14",
        "Resolved": "21/Oct/20 11:14",
        "Description": "Spark 2.2.0 was recently released.  This JIRA is for making sure Mahout is supported with the spark 2.2.0 backend, and also publishing the binary.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/332",
            "https://github.com/apache/mahout/pull/335"
        ]
    },
    "MAHOUT-2001": {
        "Key": "MAHOUT-2001",
        "Summary": "Automatic Probing for GPU/OMP modules at the creation of a MahoutDistributedContext",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2,                                            14.2,                                            15.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "14/Jul/17 01:35",
        "Updated": "02/Nov/19 09:37",
        "Resolved": null,
        "Description": "Currently at each %*% call, we probe for the existence of a GPU or OpenMP Mahout jar on the classpath.  Update the RootSolverFactory to probe for these checks upon instantiation of a MahoutDistributedContext and cache any existing solver on the classpath to the ((SolverFactory}}'s   \n\nprotected[backend] val solverMap: Map[ClassTag[_], Any]",
        "Issue Links": []
    },
    "MAHOUT-2002": {
        "Key": "MAHOUT-2002",
        "Summary": "Mahout CUDA integrarion",
        "Type": "Epic",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Nikolay Sakharnykh",
        "Reporter": "Andrew Palumbo",
        "Created": "14/Jul/17 20:48",
        "Updated": "16/Jul/17 00:46",
        "Resolved": null,
        "Description": "Mahout CUDA module",
        "Issue Links": []
    },
    "MAHOUT-2003": {
        "Key": "MAHOUT-2003",
        "Summary": "Check to see if CUSPARSE library requires sorted CSC format",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "14/Jul/17 20:52",
        "Updated": "16/Jul/17 00:47",
        "Resolved": "15/Jul/17 23:49",
        "Description": "Test whether CUSPARSE requires sorted CSC(R) if not, it should be an easy win-  to    shave some time off of the conversion to compressed format without any sorting.",
        "Issue Links": []
    },
    "MAHOUT-2004": {
        "Key": "MAHOUT-2004",
        "Summary": "Properly route all CUDAMMul calls to the correct algorithms",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "14/Jul/17 21:04",
        "Updated": "03/Mar/18 21:28",
        "Resolved": null,
        "Description": "CUDAMMul has been implemented and is currently routing Sparse Sparse and Dense Dense mmul calls correctly, Sparse Dense must be properly routed.. (as well as Dense Sparse).",
        "Issue Links": []
    },
    "MAHOUT-2005": {
        "Key": "MAHOUT-2005",
        "Summary": "In Spark 2, use SparkSession.builder instead of SQLContext",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "14/Jul/17 22:23",
        "Updated": "03/Mar/18 21:30",
        "Resolved": null,
        "Description": "When compiling on Spark 2+\n`/mahout/spark/src/test/scala/org/apache/mahout/sparkbindings/drm/DrmLikeSuite.scala:128: warning: constructor SQLContext in class SQLContext is deprecated: Use SparkSession.builder instead\n[WARNING]     val sqlContext= new org.apache.spark.sql.SQLContext(sc)\n[WARNING]                     ^`",
        "Issue Links": []
    },
    "MAHOUT-2006": {
        "Key": "MAHOUT-2006",
        "Summary": "AsFactor has unexpected behavior when partitions not set",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "0.13.2",
        "Component/s": "Algorithms",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "15/Jul/17 05:48",
        "Updated": "03/Mar/18 21:28",
        "Resolved": null,
        "Description": "```\nval drmA = drmParallelize(dense((0.0), (0.0), (1.0), (0.0), (2.0)), numPartitions = 2)\nval factorizer = new AsFactor().fit(drmA)\nval factoredA = factorizer.transform(drmA).collect\n```\nYields:\n```\ndrmA: org.apache.mahout.math.drm.CheckpointedDrm[Int] = org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@75dcf2b2\nfactorizer: org.apache.mahout.math.algorithms.preprocessing.AsFactorModel = org.apache.mahout.math.algorithms.preprocessing.AsFactorModel@13b49f81\nfactoredA: org.apache.mahout.math.Matrix = \n{\n 0 =>\t\n{0:1.0}\n 1 =>\t\n{0:1.0}\n 2 =>\t\n{1:1.0}\n 3 =>\t\n{0:1.0}\n 4 =>\t{}\n}\n```\nas expected, however\n```\nval drmA = drmParallelize(dense((0.0), (0.0), (1.0), (0.0), (2.0)))\nval factorizer = new AsFactor().fit(drmA)\nval factoredA = factorizer.transform(drmA).collect\n```\nYields:\n```\ndrmA: org.apache.mahout.math.drm.CheckpointedDrm[Int] = org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark@75dcf2b2\nfactorizer: org.apache.mahout.math.algorithms.preprocessing.AsFactorModel = org.apache.mahout.math.algorithms.preprocessing.AsFactorModel@13b49f81\nfactoredA: org.apache.mahout.math.Matrix = \n{\n 0 =>\t{}\n 1 =>\t{}\n 2 =>\t{}\n 3 =>\t{}\n 4 =>\t{}\n}\n```",
        "Issue Links": []
    },
    "MAHOUT-2007": {
        "Key": "MAHOUT-2007",
        "Summary": "Fix wikipedia xml dump url in examples",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": "jack ai",
        "Reporter": "Andrew Palumbo",
        "Created": "15/Jul/17 22:27",
        "Updated": "28/Nov/17 22:06",
        "Resolved": "28/Nov/17 22:06",
        "Description": "sometime after mahout 0.12.2, the URL for the xml dump changed and now 404s out.  This is a one line fix.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/339"
        ]
    },
    "MAHOUT-2008": {
        "Key": "MAHOUT-2008",
        "Summary": "Create scripts in examples/bin for launching a Spark Cluster and configuring for CUDA 8",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "16/Jul/17 00:46",
        "Updated": "03/Mar/18 21:27",
        "Resolved": null,
        "Description": "Create a script using e.g.: spark-ec2, to launch an ec2 spark cluster, Create another to RSYNC JCuda/cuda 8 installs across the cluster.  Add these to the $MAHOUT_HOME/examples/bin dir.",
        "Issue Links": []
    },
    "MAHOUT-2009": {
        "Key": "MAHOUT-2009",
        "Summary": "Allow User Defined threshold for densityAnalysys()",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Palumbo",
        "Created": "21/Aug/17 18:53",
        "Updated": "03/Mar/18 21:27",
        "Resolved": null,
        "Description": "Stochastic `densityAnalysis(...)` is is heavily used in the breaking down of drms to in-core  matrix and rebuilding DRMs after operations:\nhttps://github.com/apache/mahout/blob/08e02602e947ff945b9bd73ab5f0b45863df3e53/math-scala/src/main/scala/org/apache/mahout/math/scalabindings/package.scala#L431\nthe density is currently hard-coded to a quite high threshold (25%) this should either be lowered or should allow for the user to define a setting.",
        "Issue Links": []
    },
    "MAHOUT-2010": {
        "Key": "MAHOUT-2010",
        "Summary": "Set Log Levels to Debug in current Solver probing code",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "21/Aug/17 18:56",
        "Updated": "03/Mar/18 21:23",
        "Resolved": null,
        "Description": "Log4j Log levels are set to [INFO] in GPU probing.  set these to Debug to reduce verbosity.  \nThese logs are meant for debugging a cluster.",
        "Issue Links": []
    },
    "MAHOUT-2011": {
        "Key": "MAHOUT-2011",
        "Summary": "Add RTree module - Will be used by other component algorithms",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Aditya AS",
        "Reporter": "Aditya AS",
        "Created": "01/Sep/17 14:10",
        "Updated": "23/Jun/21 14:04",
        "Resolved": "23/Jun/21 14:04",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2012": {
        "Key": "MAHOUT-2012",
        "Summary": "Hard Coded Block in Canopy Cluster (Samsara)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.0,                                            0.13.1",
        "Fix Version/s": "None",
        "Component/s": "Algorithms",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "05/Sep/17 21:05",
        "Updated": "25/Sep/17 12:38",
        "Resolved": "25/Sep/17 12:38",
        "Description": "The same dataset which is used in the tests is hard coded in the Canopy function\nhttps://github.com/apache/mahout/blob/master/math-scala/src/main/scala/org/apache/mahout/math/algorithms/clustering/Canopy.scala#L131\nHowever renders algorithm useless.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/337"
        ]
    },
    "MAHOUT-2013": {
        "Key": "MAHOUT-2013",
        "Summary": "out of memory issue",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.12.2",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Ugur Ilker",
        "Created": "17/Sep/17 18:12",
        "Updated": "03/Mar/18 21:31",
        "Resolved": null,
        "Description": "I have been implementing SVD++ recommender system and slope-one recommender (with Mahout 0.7) with Movielens 10 Million dataset. I have set the training set rate as 0.8. I got out-of-memory errors. I have checked Java Heap size as well. I wonder if it is about the algorithms itself.",
        "Issue Links": []
    },
    "MAHOUT-2014": {
        "Key": "MAHOUT-2014",
        "Summary": "Expose Mahout's Algorithms in Spark ML Pipelines",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "spark",
        "Assignee": null,
        "Reporter": "Holden Karau",
        "Created": "21/Sep/17 20:27",
        "Updated": "21/Sep/17 20:28",
        "Resolved": null,
        "Description": "To increase usage of Mahout algorithms in the Spark community we can expose them in the standard Spark ML pipeline interface.",
        "Issue Links": []
    },
    "MAHOUT-2015": {
        "Key": "MAHOUT-2014 Expose Mahout's Algorithms in Spark ML Pipelines",
        "Summary": "Expose OLS in Spark ML Pipelines",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Holden Karau",
        "Reporter": "Holden Karau",
        "Created": "21/Sep/17 20:28",
        "Updated": "25/Jun/20 20:01",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/mahout/pull/340"
        ]
    },
    "MAHOUT-2016": {
        "Key": "MAHOUT-2014 Expose Mahout's Algorithms in Spark ML Pipelines",
        "Summary": "Auto generate the remaining algorithms wrappers into Spark ML",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "spark",
        "Assignee": null,
        "Reporter": "Holden Karau",
        "Created": "21/Sep/17 20:28",
        "Updated": "21/Sep/17 20:28",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2017": {
        "Key": "MAHOUT-2014 Expose Mahout's Algorithms in Spark ML Pipelines",
        "Summary": "Figure out how keep data as DRM in between stages",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "spark",
        "Assignee": null,
        "Reporter": "Holden Karau",
        "Created": "21/Sep/17 20:30",
        "Updated": "12/Nov/19 00:05",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/mahout/pull/376"
        ]
    },
    "MAHOUT-2018": {
        "Key": "MAHOUT-2018",
        "Summary": "missing dash delimiter in mahout-spark module pom.xml",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "24/Sep/17 00:50",
        "Updated": "30/Sep/17 19:38",
        "Resolved": "30/Sep/17 19:38",
        "Description": "<copy file=\"target/mahout-spark_${scala.compat.version}-${version}-spark_${spark.compat.version}.jar\" tofile=\"../mahout-spark_${scala.compat.version}-${version}spark_${spark.compat.version}.jar\" />\n\n\nis misnaming a jar on copy as e.g.:   mahout-spark_1.3.1-SNAPSHOTspark_1.6.jar",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/341"
        ]
    },
    "MAHOUT-2019": {
        "Key": "MAHOUT-2019",
        "Summary": "SparseRowMatrix assign ops user for loops instead of iterateNonZero and so can be optimized",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "14.2",
        "Component/s": "Math",
        "Assignee": "Pat Ferrel",
        "Reporter": "Pat Ferrel",
        "Created": "03/Oct/17 01:25",
        "Updated": "08/Oct/20 14:46",
        "Resolved": null,
        "Description": "DRMs get blockified into SparseRowMatrix instances if the density is low. But SRM inherits the implementation of method like \"assign\" from AbstractMatrix, which uses nest for loops to traverse rows. For multiplying 2 matrices that are extremely sparse, the kind if data you see in collaborative filtering, this is extremely wasteful of execution time. Better to use a sparse vector's iterateNonZero Iterator for some function types.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/342"
        ]
    },
    "MAHOUT-2020": {
        "Key": "MAHOUT-2020",
        "Summary": "Maven repo structure malformed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "0.13.1",
        "Component/s": "build",
        "Assignee": "Trevor Grant",
        "Reporter": "Pat Ferrel",
        "Created": "03/Oct/17 20:16",
        "Updated": "27/Nov/17 17:42",
        "Resolved": "27/Nov/17 17:42",
        "Description": "The maven repo is built with scala 2.10 always in the parent pom's \n{scala.compat.version}\n even when you only ask for Scala 2.11, this leads to the 2.11 jars never being found.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/331"
        ]
    },
    "MAHOUT-2021": {
        "Key": "MAHOUT-2021",
        "Summary": "Add Eigenfaces Example",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "14.2",
        "Component/s": "Examples",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "03/Oct/17 22:09",
        "Updated": "08/Oct/20 14:46",
        "Resolved": null,
        "Description": "Small mods on this ought to about do it. \nhttps://github.com/rawkintrevo/cylons/tree/master/eigenfaces",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/360"
        ]
    },
    "MAHOUT-2022": {
        "Key": "MAHOUT-2022",
        "Summary": "Add docs to website explaining how to build Spark Jars wtih SBT",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "03/Oct/17 22:10",
        "Updated": "28/Nov/17 22:05",
        "Resolved": "28/Nov/17 22:05",
        "Description": "libraryDependencies += \"org.apache.mahout\" % \"mahout-spark_2.11\" % \"0.13.1-SNAPSHOT\" classifier \"spark_2.1\"",
        "Issue Links": []
    },
    "MAHOUT-2023": {
        "Key": "MAHOUT-2023",
        "Summary": "Drivers broken, scopt classes not found",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.13.1",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": "Trevor Grant",
        "Reporter": "Pat Ferrel",
        "Created": "05/Oct/17 18:44",
        "Updated": "21/Oct/20 11:21",
        "Resolved": "21/Oct/20 11:21",
        "Description": "Type `mahout spark-itemsimilarity` after Mahout is installed properly and you get a fatal exception due to missing scopt classes.\nProbably a build issue related to incorrect versions of scopt being looked for.",
        "Issue Links": [
            "/jira/browse/MAHOUT-2093",
            "https://github.com/apache/mahout/pull/346",
            "https://github.com/apache/mahout/pull/353"
        ]
    },
    "MAHOUT-2024": {
        "Key": "MAHOUT-2024",
        "Summary": "Update doap_Mahout.rdf",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "jack ai",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Oct/17 02:13",
        "Updated": "08/Oct/20 14:46",
        "Resolved": null,
        "Description": "Update the doap_Mahout.rdf file to the current most recent release.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/348"
        ]
    },
    "MAHOUT-2025": {
        "Key": "MAHOUT-2025",
        "Summary": "Add a script to launch an EC2 cluster, install Mahout and JCuda or ViennaCL to Examples",
        "Type": "Task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0,                                            14.1",
        "Fix Version/s": "14.2,                                            15.0",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Oct/17 06:49",
        "Updated": "23/Jan/20 15:04",
        "Resolved": null,
        "Description": "Create a script for $MAHOUT_HOME/examples/bin to launch a spark cluster on EC2 using e.g.: spark-ec2, to:\n\nlaunch an EC2 Cluster\nInstall Mahout\nInstall either JCuda or ViennaCL\nRSYNC CUDA drivers and libraries in the case of JCuda\n\nMuch of this is already done in the .travis.yml configuration.  \nThis should lessen the learning curve for those wishing to make use of the native solvers.",
        "Issue Links": [
            "/jira/browse/MAHOUT-2074"
        ]
    },
    "MAHOUT-2026": {
        "Key": "MAHOUT-2026",
        "Summary": "Add NiFi wrappers for Matrix Multiply",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "08/Oct/17 16:08",
        "Updated": "12/Nov/19 08:12",
        "Resolved": null,
        "Description": "Write a custom NiFi wrapper for Matrix Multiplication.  This will be the beginning of a broader effort to incorporate Mahout into NiFi.",
        "Issue Links": []
    },
    "MAHOUT-2027": {
        "Key": "MAHOUT-2027",
        "Summary": "spark-ec2 launch scripts with ViennaCL/JCuda installation",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0",
        "Fix Version/s": "0.13.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "02/Nov/17 18:43",
        "Updated": "25/Jun/20 20:01",
        "Resolved": null,
        "Description": "To make things easier for new users, add a couple of scripts for launching an ec2 cluster, installing spark,HDFS, mahout and JCuda/ViennaCL.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/349"
        ]
    },
    "MAHOUT-2028": {
        "Key": "MAHOUT-2028",
        "Summary": "Silhoutte indexing for clustering evaluation",
        "Type": "Proposal",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0,                                            0.14.0,                                            0.13.1,                                            0.13.2",
        "Fix Version/s": "None",
        "Component/s": "Algorithms,                                            Clustering",
        "Assignee": null,
        "Reporter": "Stefano Dalla Palma",
        "Created": "18/Nov/17 09:32",
        "Updated": "05/Dec/17 20:21",
        "Resolved": null,
        "Description": "What about implementing clustering evaluation measure like Silhouette indexing? We could use one or more of the currently implemented distance measures and some clustering algorithm, like k-means, and start measuring the goodness of that clustering, i.e. with Silhouette in this case.",
        "Issue Links": []
    },
    "MAHOUT-2029": {
        "Key": "MAHOUT-2029",
        "Summary": "Broken links in Developer section in website navbar.",
        "Type": "Bug",
        "Status": "Patch Available",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andr\u00e9 de Santi Oliveira",
        "Reporter": "Juha Syrj\u00e4l\u00e4",
        "Created": "08/Feb/18 22:50",
        "Updated": "20/Oct/20 12:42",
        "Resolved": null,
        "Description": "https://mahout.apache.org/\u00a0has following broken links in navbars Developers section\n\nBuilding Mahout from Source\u00a0\nIssues Tracking (JIRA)\nRelease Notes\n\nFix for first two is to remove trailing slash from the URL",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/355"
        ]
    },
    "MAHOUT-2030": {
        "Key": "MAHOUT-2030",
        "Summary": "Add DCG and nDCG metrics (rank) evaluator",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Collaborative Filtering",
        "Assignee": null,
        "Reporter": "Marta Raczynska",
        "Created": "27/Feb/18 20:16",
        "Updated": "03/Mar/18 21:31",
        "Resolved": null,
        "Description": "Add common ranking metrics evaluator:\n\nDiscounted cumulative gain (DCG)\nNormalized discounted cumulative gain (nDCG)\n\nThis implementation attempts to create a new class:\u00a0org.apache.mahout.cf.taste.impl.eval.DCGRecommenderEvaluator which implements\u00a0org.apache.mahout.cf.taste.eval.RecommenderEvaluator.",
        "Issue Links": []
    },
    "MAHOUT-2031": {
        "Key": "MAHOUT-2031",
        "Summary": "Cleaning POMs, rearranging code, moving MR to community",
        "Type": "Epic",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:32",
        "Updated": "08/Aug/18 14:01",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2032": {
        "Key": "MAHOUT-2032",
        "Summary": "Delete files from lib/ on mvn clean",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:33",
        "Updated": "08/Oct/20 14:53",
        "Resolved": "04/Jun/18 16:38",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2033": {
        "Key": "MAHOUT-2033",
        "Summary": "MR can't find OpenIntHash and other things like that",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:34",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "29/Jun/18 16:10",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2034": {
        "Key": "MAHOUT-2034",
        "Summary": "Add \"examples\" module",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:35",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "27/Jun/18 13:14",
        "Description": "Should split up MR and post-MR examples into different submodules",
        "Issue Links": []
    },
    "MAHOUT-2035": {
        "Key": "MAHOUT-2035",
        "Summary": "Update travis.yml for new directory structures",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:35",
        "Updated": "08/Oct/20 14:46",
        "Resolved": null,
        "Description": "Add tests of all examples (of current Mahout). \u00a0Make sure examples are never broken on a new patch.",
        "Issue Links": []
    },
    "MAHOUT-2036": {
        "Key": "MAHOUT-2036",
        "Summary": "Create profiles",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:36",
        "Updated": "08/Oct/20 14:53",
        "Resolved": "06/Jun/18 14:37",
        "Description": "There were about 1000 profiles that previously existed. \u00a0\nCurrently we have `-Pmahout-mr` . \u00a0Add reasonable profiling scheme and add a page to website explaining all profiles and what they do.",
        "Issue Links": []
    },
    "MAHOUT-2037": {
        "Key": "MAHOUT-2037",
        "Summary": "Add release plugins",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:37",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "05/Jun/18 16:13",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2038": {
        "Key": "MAHOUT-2038",
        "Summary": "Add checkstyle plugin",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:37",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "20/Jun/18 14:25",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2039": {
        "Key": "MAHOUT-2039",
        "Summary": "Add maven-enforcer plugin",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:38",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "19/Jun/18 15:44",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2040": {
        "Key": "MAHOUT-2040",
        "Summary": "Add maven-surefire plugin",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:38",
        "Updated": "08/Oct/20 14:53",
        "Resolved": "20/Jun/18 14:25",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2041": {
        "Key": "MAHOUT-2041",
        "Summary": "Add java/scala doc plugins",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:38",
        "Updated": "08/Oct/20 14:53",
        "Resolved": "18/Jun/18 15:28",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2042": {
        "Key": "MAHOUT-2042",
        "Summary": "Delete directories no longer in use",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:39",
        "Updated": "08/Oct/20 14:53",
        "Resolved": "27/Jun/18 14:51",
        "Description": "Wait until the end so we're sure we wont need anything else from them.",
        "Issue Links": []
    },
    "MAHOUT-2043": {
        "Key": "MAHOUT-2043",
        "Summary": "Move Kryo dependency to top of community engines and engines plugin",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:39",
        "Updated": "08/Oct/20 14:53",
        "Resolved": "27/Jun/18 14:28",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2044": {
        "Key": "MAHOUT-2044",
        "Summary": "Update bin/mahout to reflect no longer using mapReduce",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:40",
        "Updated": "08/Oct/20 14:45",
        "Resolved": null,
        "Description": "add community/mahout-mr/bin/mahout to do all of the old map reduce stuff.",
        "Issue Links": []
    },
    "MAHOUT-2045": {
        "Key": "MAHOUT-2045",
        "Summary": "delete refactor-readme.md",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "04/Jun/18 14:41",
        "Updated": "08/Oct/20 14:53",
        "Resolved": "27/Jun/18 14:51",
        "Description": "At the end. its a working document to help keep track, however the checklists are now being handled by Jira issues.",
        "Issue Links": []
    },
    "MAHOUT-2046": {
        "Key": "MAHOUT-2046",
        "Summary": "Clean up \"Building Mahout Page\"",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Trevor Grant",
        "Created": "06/Jun/18 22:57",
        "Updated": "08/Oct/20 14:45",
        "Resolved": null,
        "Description": "The page makes it seem exceptionally complex to build Mahout (which it is not).\nSimplify it with \"basic\" version first then spin off into \"How to install ViennaCL\" (maybe new page), and other more advanced topics.",
        "Issue Links": []
    },
    "MAHOUT-2047": {
        "Key": "MAHOUT-2047",
        "Summary": "Version bump JDK to 1.8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "18/Jun/18 16:23",
        "Updated": "13/Jan/19 08:04",
        "Resolved": "19/Jun/18 15:44",
        "Description": "Jenkins no longer supports 1.7, and spark 2.3 requires it.\u00a0\nTime to move along with the times.",
        "Issue Links": []
    },
    "MAHOUT-2048": {
        "Key": "MAHOUT-2048",
        "Summary": "There are duplicate content pages which need redirects instead",
        "Type": "Planned Work",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.13.0,                                            0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "website",
        "Assignee": "Andrew Musselman",
        "Reporter": "Pat Ferrel",
        "Created": "27/Jun/18 19:56",
        "Updated": "08/Oct/20 14:45",
        "Resolved": null,
        "Description": "I have duplicated content in 3 places in the `website/` directory. We need to have one place for the real content and replace the dups with redirect to the actual content. This looks like is may be true for several other pages and honestly I'm not sure if they are all needed but there are many links out in the wild that point to the old path for the CCO recommender pages so we should do this for the ones below at least. Better yet we may want to clean out any other dups unless someone knows why not.\nTLDR;\nActual content:\nmahout/website/docs/latest/algorithms/recommenders/index.md\nmahout/website/docs/latest/algorithms/recommenders/cco.md\n\u00a0\nDups to be replaced with redirects to the above content. I vaguely remember all these different site structures so there may be links to them in the wild.\nmahout/website/recommender-overview.md =>\u00a0mahout/website/docs/latest/algorithms/recommenders/index.md\nmahout/website/users/algorithms/intro-cooccurrence-spark.md =>\u00a0mahout/website/docs/latest/algorithms/recommenders/cco.md\nmahout/website/users/recommender/quickstart.md\u00a0=>\u00a0mahout/website/docs/latest/algorithms/recommenders/index.md\nmahout/website/users/recommender/intro-cooccurrence-spark.md\u00a0=>\u00a0mahout/website/docs/latest/algorithms/recommenders/cco.md",
        "Issue Links": []
    },
    "MAHOUT-2049": {
        "Key": "MAHOUT-2049",
        "Summary": "WEBSITE - Mailing list page issue",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "29/Jun/18 03:02",
        "Updated": "29/Jun/18 03:03",
        "Resolved": null,
        "Description": "https://confluence.atlassian.com/jira/jira-documentation-1556.html\nhas some style issues (just one of many I know). \u00a0Also, IRC- are we still using that? delete (unless someone is and wants more people to join it. I don't know how to get on IRC).",
        "Issue Links": []
    },
    "MAHOUT-2050": {
        "Key": "MAHOUT-2050",
        "Summary": "check / fix if needed MR examples",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "29/Jun/18 14:10",
        "Updated": "08/Oct/20 14:45",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2051": {
        "Key": "MAHOUT-2051",
        "Summary": "MixedGradient throws a random IllegalStateException",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.0.0,                                            0.12.0,                                            0.11.2,                                            0.12.1,                                            0.13.0,                                            0.12.2,                                            0.14.0,                                            0.13.1,                                            0.13.2",
        "Fix Version/s": "1.0.0,                                            0.12.0,                                            0.11.2,                                            0.12.1,                                            0.13.0,                                            0.12.2,                                            0.13.2,                                            14.2",
        "Component/s": "Classification",
        "Assignee": null,
        "Reporter": "Peter Schaumann",
        "Created": "18/Jul/18 11:10",
        "Updated": "08/Oct/20 14:45",
        "Resolved": null,
        "Description": "The\u00a0function apply in the class\u00a0MixedGradient throws an exception when both variables\u00a0hasZero and\u00a0hasOne are false. In order for both of them to be true, the RNG has to pick the basic gradient until\u00a0both are set to true, otherwise\u00a0an IllegalStateException is thrown.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/357"
        ]
    },
    "MAHOUT-2052": {
        "Key": "MAHOUT-2052",
        "Summary": "Broken codeblock in a samsara tutorial html file.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "JieunKim",
        "Created": "19/Jul/18 02:17",
        "Updated": "25/Jun/20 20:01",
        "Resolved": null,
        "Description": "As you can see below,\n there is a broken code block and the tag <pre> is shown in a samsara tutorial html file.\n\u00a0\n\nThis is because of\n<pre class=\"codehilite\"></pre>\nand it should be changed to\n<div class=\"codehilite\"><pre> ... </pre></div>",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/358"
        ]
    },
    "MAHOUT-2053": {
        "Key": "MAHOUT-2053",
        "Summary": "Please add OWASP Dependency Check to the core build (pom.xml) and all sub-componet builds.",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.0.0,                                            0.13.0,                                            0.14.0,                                            0.13.1,                                            0.13.2",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Albert Baker",
        "Created": "30/Jul/18 01:07",
        "Updated": "31/Jul/18 22:43",
        "Resolved": null,
        "Description": "Please add OWASP Dependency Check to the build (pom.xml). OWASP DC makes an outbound REST call to MITRE Common Vulnerabilities & Exposures (CVE) to perform a lookup for each dependant .jar to list any/all known vulnerabilities for each jar. This step is needed because a manual MITRE CVE lookup/check on the main component does not include checking for vulnerabilities in components or in dependant libraries.\nOWASP Dependency check : https://www.owasp.org/index.php/OWASP_Dependency_Check has plug-ins for most Java build/make types (ant, maven, ivy, gradle).\nAlso, add the appropriate command to the nightly build to generate a report of all known vulnerabilities in any/all third party libraries/dependencies that get pulled in. example : mvn -Powasp -Dtest=false -DfailIfNoTests=false clean aggregate\nGenerating this report nightly/weekly will help inform the project's development team if any dependant libraries have a reported known vulnerailities. Project teams that keep up with removing vulnerabilities on a weekly basis will help protect businesses that rely on these open source componets.",
        "Issue Links": []
    },
    "MAHOUT-2054": {
        "Key": "MAHOUT-2054",
        "Summary": "Mailing List Page",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "website",
        "Assignee": "Andrew Musselman",
        "Reporter": "Trevor Grant",
        "Created": "08/Aug/18 14:01",
        "Updated": "08/Oct/20 14:45",
        "Resolved": null,
        "Description": "The mailing list page:\nhttp://mahout.apache.org/general/mailing-lists,-irc-and-archives.html\nNeeds cleaned up and there should be a link in the nav-bar to it.",
        "Issue Links": []
    },
    "MAHOUT-2055": {
        "Key": "MAHOUT-2055",
        "Summary": "Implementation of Baum-Welch algorithm for HMM in Mahout Samsara",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Algorithms",
        "Assignee": null,
        "Reporter": "Manogna Vemulapati",
        "Created": "16/Oct/18 06:03",
        "Updated": "16/Oct/18 06:03",
        "Resolved": null,
        "Description": "The description can be found at this link, and in the PDF attached below.\nhttps://github.com/manognavemulapati/mahout/blob/gh-pages/baum_welch_tex.pdf",
        "Issue Links": []
    },
    "MAHOUT-2056": {
        "Key": "MAHOUT-2056",
        "Summary": "Moderators needed for general list",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebb",
        "Created": "24/Jan/19 14:00",
        "Updated": "24/Jan/19 18:18",
        "Resolved": "24/Jan/19 18:18",
        "Description": "There are currently no moderators for the general@ list [1]\nSome volunteers need to step up.\nIn the meantime I have added myself, but that can only be temporary.\n[1] https://whimsy.apache.org/roster/committee/mahout#mail",
        "Issue Links": []
    },
    "MAHOUT-2057": {
        "Key": "MAHOUT-2057",
        "Summary": "Example in README results in class not found",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "06/Mar/19 04:28",
        "Updated": "08/Oct/20 14:45",
        "Resolved": null,
        "Description": "Running the example in the README gives a class not found:\n\"java.lang.NoClassDefFoundError: it/unimi/dsi/fastutil/ints/Int2DoubleOpenHashMap\"\n\u00a0\nIf that's just us still using something that's been removed, it's not a deal-breaker for me as long as we fix it in a quick point release.\n\u00a0\nPending that being a simple fix my vote is +1 binding, and if Andy's not back from vacation and his proxy works that's +2 binding from me and Andy.\n\u00a0\n\u00a0\nbob $ export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\nbob $ export MAHOUT_HOME=//home/akm/a/src/test/repository.apache.org/content/repositories/orgapachemahout-1052/org/apache/mahout/mahout/0.14.0\nbob $ export SPARK_HOME=/home/akm/a/src/spark-2.1.0-bin-hadoop2.7\nbob $ MASTER=local[2] mahout-0.14.0/bin/mahout spark-shell\nAdding lib/ to CLASSPATH\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n19/03/04 09:07:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n19/03/04 09:07:44 WARN Utils: Your hostname, Bob resolves to a loopback address: 127.0.1.1; using 10.0.1.2 instead (on interface eno1)\n19/03/04 09:07:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n19/03/04 09:07:53 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException\nSpark context Web UI available at http://10.0.1.2:4040\nSpark context available as 'sc' (master = local[2], app id = local-1551719265339).\nSpark session available as 'spark'.\nLoading /home/akm/a/src/test/repository.apache.org/content/repositories/orgapachemahout-1052/org/apache/mahout/mahout/0.14.0/mahout-0.14.0/bin/load-shell.scala...\nimport org.apache.mahout.math._\nimport org.apache.mahout.math.scalabindings._\nimport org.apache.mahout.math.drm._\nimport org.apache.mahout.math.scalabindings.RLikeOps._\nimport org.apache.mahout.math.drm.RLikeDrmOps._\nimport org.apache.mahout.sparkbindings._\nsdc: org.apache.mahout.sparkbindings.SparkDistributedContext = org.apache.mahout.sparkbindings.SparkDistributedContext@749ffdc7\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 _\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 _\n_ __ __\u00a0\u00a0 __ _| |\u00a0\u00a0 ___\u00a0 _\u00a0\u00a0 _| |\n\u00a0'_ ` _ \\ / ` | ' \\ / _ | | | | __|\n\u00a0| | | | (| | | | | () | || | |\n| || ||_,|| ||_/ _,|_|\u00a0 version 0.14.0\nThat file does not exist\nWelcome to\n\u00a0\u00a0\u00a0\u00a0\u00a0 ____\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 __\n\u00a0\u00a0\u00a0\u00a0 / _/\u00a0 ___ _____/ /_\n\u00a0\u00a0\u00a0 \\ \\/ _ \\/ _ `/ __/\u00a0 '/\n\u00a0\u00a0 /__/ ./_,// //_\\\u00a0\u00a0 version 2.1.0\n\u00a0\u00a0\u00a0\u00a0\u00a0 /_/\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \nUsing Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_191)\nType in expressions to have them evaluated.\nType :help for more information.\nscala> :load /home/akm/a/src/test/repository.apache.org/content/repositories/orgapachemahout-1052/org/apache/mahout/mahout/0.14.0/mahout-0.14.0/examples/bin/SparseSparseDrmTimer.mscala\nLoading /home/akm/a/src/test/repository.apache.org/content/repositories/orgapachemahout-1052/org/apache/mahout/mahout/0.14.0/mahout-0.14.0/examples/bin/SparseSparseDrmTimer.mscala...\ntimeSparseDRMMMul: (m: Int, n: Int, s: Int, para: Int, pctDense: Double, seed: Long)Long\nscala> timeSparseDRMMMul(1000,1000,1000,1,.02,1234L)\n19/03/04 09:13:13 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 1)\njava.lang.NoClassDefFoundError: it/unimi/dsi/fastutil/ints/Int2DoubleOpenHashMap\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:49)\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:44)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.immutable.Range.foreach(Range.scala:160)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:195)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$class.isEmpty(Iterator.scala:330)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1336)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:55)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.Task.run(Task.scala:99)\n\u00a0\u00a0\u00a0 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0\u00a0\u00a0 at java.lang.Thread.run(Thread.java:748)\n19/03/04 09:13:13 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 0)\njava.lang.NoClassDefFoundError: it/unimi/dsi/fastutil/ints/Int2DoubleOpenHashMap\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:49)\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:44)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.immutable.Range.foreach(Range.scala:160)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:195)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$class.isEmpty(Iterator.scala:330)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1336)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:55)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.Task.run(Task.scala:99)\n\u00a0\u00a0\u00a0 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0\u00a0\u00a0 at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: it.unimi.dsi.fastutil.ints.Int2DoubleOpenHashMap\n\u00a0\u00a0\u00a0 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\u00a0\u00a0\u00a0 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\u00a0\u00a0\u00a0 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\u00a0\u00a0\u00a0 ... 35 more\n19/03/04 09:13:13 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NoClassDefFoundError: it/unimi/dsi/fastutil/ints/Int2DoubleOpenHashMap\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:49)\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:44)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.immutable.Range.foreach(Range.scala:160)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:195)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$class.isEmpty(Iterator.scala:330)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1336)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:55)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.Task.run(Task.scala:99)\n\u00a0\u00a0\u00a0 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0\u00a0\u00a0 at java.lang.Thread.run(Thread.java:748)\n19/03/04 09:13:13 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job\n19/03/04 09:13:13 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 0, localhost, executor driver): java.lang.NoClassDefFoundError: it/unimi/dsi/fastutil/ints/Int2DoubleOpenHashMap\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:49)\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:44)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.immutable.Range.foreach(Range.scala:160)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:195)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$class.isEmpty(Iterator.scala:330)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1336)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:55)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.Task.run(Task.scala:99)\n\u00a0\u00a0\u00a0 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0\u00a0\u00a0 at java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.ClassNotFoundException: it.unimi.dsi.fastutil.ints.Int2DoubleOpenHashMap\n\u00a0\u00a0\u00a0 at java.net.URLClassLoader.findClass(URLClassLoader.java:382)\n\u00a0\u00a0\u00a0 at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\u00a0\u00a0\u00a0 at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\u00a0\u00a0\u00a0 ... 35 more\n19/03/04 09:13:13 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 1, localhost, executor driver): java.lang.NoClassDefFoundError: it/unimi/dsi/fastutil/ints/Int2DoubleOpenHashMap\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:49)\n\u00a0\u00a0\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:44)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.immutable.Range.foreach(Range.scala:160)\n\u00a0\u00a0\u00a0 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:200)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:195)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\u00a0\u00a0\u00a0 at scala.collection.Iterator$class.isEmpty(Iterator.scala:330)\n\u00a0\u00a0\u00a0 at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1336)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:55)\n\u00a0\u00a0\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0\u00a0\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\u00a0\u00a0\u00a0 at org.apache.spark.scheduler.Task.run(Task.scala:99)\n\u00a0\u00a0\u00a0 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0\u00a0\u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0\u00a0\u00a0 at java.lang.Thread.run(Thread.java:748)\nDriver stacktrace:\n\u00a0 at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)\n\u00a0 at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)\n\u00a0 at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)\n\u00a0 at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\u00a0 at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\u00a0 at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)\n\u00a0 at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\u00a0 at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)\n\u00a0 at scala.Option.foreach(Option.scala:257)\n\u00a0 at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)\n\u00a0 at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)\n\u00a0 at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)\n\u00a0 at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)\n\u00a0 at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\u00a0 at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)\n\u00a0 at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)\n\u00a0 at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)\n\u00a0 at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)\n\u00a0 at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)\n\u00a0 at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:935)\n\u00a0 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\u00a0 at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\u00a0 at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n\u00a0 at org.apache.spark.rdd.RDD.collect(RDD.scala:934)\n\u00a0 at org.apache.mahout.sparkbindings.drm.CheckpointedDrmSpark.collect(CheckpointedDrmSpark.scala:128)\n\u00a0 at org.apache.mahout.math.drm.package$.drm2InCore(package.scala:98)\n\u00a0 at timeSparseDRMMMul(<console>:87)\n\u00a0 ... 60 elided\nCaused by: java.lang.NoClassDefFoundError: it/unimi/dsi/fastutil/ints/Int2DoubleOpenHashMap\n\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:49)\n\u00a0 at org.apache.mahout.math.RandomAccessSparseVector.<init>(RandomAccessSparseVector.java:44)\n\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11$$anonfun$apply$2.apply(SparkEngine.scala:200)\n\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0 at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\u00a0 at scala.collection.immutable.Range.foreach(Range.scala:160)\n\u00a0 at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\u00a0 at scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:200)\n\u00a0 at org.apache.mahout.sparkbindings.SparkEngine$$anonfun$11.apply(SparkEngine.scala:195)\n\u00a0 at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\u00a0 at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\u00a0 at scala.collection.Iterator$class.isEmpty(Iterator.scala:330)\n\u00a0 at scala.collection.AbstractIterator.isEmpty(Iterator.scala:1336)\n\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:55)\n\u00a0 at org.apache.mahout.sparkbindings.drm.package$$anonfun$blockify$1.apply(package.scala:53)\n\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0 at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:796)\n\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0 at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\u00a0 at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\n\u00a0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\n\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\n\u00a0 at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n\u00a0 at org.apache.spark.scheduler.Task.run(Task.scala:99)\n\u00a0 at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)\n\u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\u00a0 at java.lang.Thread.run(Thread.java:748)\nscala>",
        "Issue Links": []
    },
    "MAHOUT-2058": {
        "Key": "MAHOUT-2058",
        "Summary": "Website publishing README",
        "Type": "Documentation",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "06/Mar/19 04:29",
        "Updated": "04/Jun/21 15:32",
        "Resolved": "04/Jun/21 15:32",
        "Description": "Would be good to have info on how to publish changes to the website in the website/README file.",
        "Issue Links": []
    },
    "MAHOUT-2059": {
        "Key": "MAHOUT-2059",
        "Summary": "Update web site with 0.14.0 release, fix download button",
        "Type": "Documentation",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "06/Mar/19 04:38",
        "Updated": "06/Mar/19 04:38",
        "Resolved": null,
        "Description": "Download button on home page is linked to a broken mirror currently also (http://mirror.stjschools.org/public/apache/mahout/0.13.0/apache-mahout-distribution-0.13.0.tar.gz)\nChange it to use dynamic mirror.",
        "Issue Links": []
    },
    "MAHOUT-2060": {
        "Key": "MAHOUT-2060",
        "Summary": "Fix Java Docs",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0,                                            0.14.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "07/Mar/19 17:41",
        "Updated": "07/Mar/19 19:03",
        "Resolved": null,
        "Description": "When building Javadocs with Java8 errors are thrown and builds crash.\u00a0\nThe band aid was (in Jenkins) to add\u00a0-Dmaven.javadoc.skip=true to nightly and quality builds.\u00a0\n\u00a0\nBetter to fix the javadoc errors (which expose as warnings on mvn install).",
        "Issue Links": []
    },
    "MAHOUT-2061": {
        "Key": "MAHOUT-2061",
        "Summary": "Automate post release tasks",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "07/Mar/19 22:26",
        "Updated": "07/Mar/19 22:26",
        "Resolved": null,
        "Description": "Should\n [1] Update website, moving \"docs/latest\" to \"docs/<old_version>/api/docs\"\n[2] Should update navbar to reflect that change\n[3] Should create release notes\n[4] Should remind user to write a little blog post about it in `website/_posts`",
        "Issue Links": []
    },
    "MAHOUT-2062": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2063": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2064": {
        "Key": "MAHOUT-2064",
        "Summary": "Jars not published to Maven repos",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.1",
        "Component/s": "build",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Musselman",
        "Created": "26/Mar/19 19:09",
        "Updated": "23/Jan/20 15:06",
        "Resolved": "23/Jan/20 15:06",
        "Description": "Need to build a binary artifact along with source artifact so it can be released by nexus.",
        "Issue Links": []
    },
    "MAHOUT-2065": {
        "Key": "MAHOUT-2065",
        "Summary": "[SECURITY]newCachedThreadPool() has higher risk in causing OutOfMemoryError",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "bd2019us",
        "Created": "01/Apr/19 01:48",
        "Updated": "25/Jun/20 20:01",
        "Resolved": null,
        "Description": "Location :\u00a0community/mahout-mr/mr/src/main/java/org/apache/mahout/clustering/streaming/mapreduce/StreamingKMeansDriver.java:427\nExecutors.newCachedThreadPool() is not secure when the number of threads is not bounded, which can cause OutOfMemoryError and crash the program. For security, using newFixedThreadPool(int) should be preferred, which can be freely configured manually on demand.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/370"
        ]
    },
    "MAHOUT-2066": {
        "Key": "MAHOUT-2066",
        "Summary": "Data Science Course in Marathahalli. Training provided by Industry Experts. Best Data science training institute in Marathahalli. Call Now",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Data Science training in Marathahalli",
        "Created": "29/Apr/19 11:49",
        "Updated": "11/Feb/20 11:41",
        "Resolved": "11/Feb/20 11:41",
        "Description": "<a href=\u201dhttps://www.dataminax.com/courses/data-science-training-in-marathahalli\">Data Science training in Marathahalli</a>",
        "Issue Links": []
    },
    "MAHOUT-2067": {
        "Key": "MAHOUT-2067",
        "Summary": "Update to latest RAT release, fixes in pom.xml and commit template",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "0.14.1,                                            14.1",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Philipp Ottlinger",
        "Created": "09/May/19 21:14",
        "Updated": "08/Oct/20 14:52",
        "Resolved": "22/May/19 18:29",
        "Description": "Apart from updating to the latest RAT release;\nconfigure maven repositories to use https for security reasons (ManInTheMiddle-Attacks, see recent mails from ASF Security team).\nGithub template contains link to wrong ASF-Jira-project (Zeppelin).\n\nThanks",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/372",
            "https://github.com/apache/mahout/pull/374"
        ]
    },
    "MAHOUT-2068": {
        "Key": "MAHOUT-2068",
        "Summary": "Release not rolled back, download link broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "0.14.0,                                            14.1",
        "Component/s": "build",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "30/May/19 21:48",
        "Updated": "08/Oct/20 14:53",
        "Resolved": "30/May/19 22:54",
        "Description": "http://mahout.apache.org has a current version of 0.14.1, and the link to download is broken.",
        "Issue Links": []
    },
    "MAHOUT-2069": {
        "Key": "MAHOUT-2069",
        "Summary": "Typo in core/pom.xml",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "30/May/19 23:59",
        "Updated": "31/May/19 00:42",
        "Resolved": "31/May/19 00:42",
        "Description": "typo in {core/pom.xml} breaks build:\nhad cursor on IDE and tyed root for some reason.\n\n\r\n-\u00a0\u00a0\u00a0 <dependency>\r\n+\u00a0\u00a0\u00a0 <dependency>root\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 <groupId>org.scala-lang</groupId>\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 <artifactId>scala-reflect</artifactId>\r\n\u00a0\u00a0\u00a0\u00a0 </dependency>",
        "Issue Links": []
    },
    "MAHOUT-2070": {
        "Key": "MAHOUT-2070",
        "Summary": "move release branch for 14.1 release  from mahout-14.2  to mahout-14.1",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "14.1",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "16/Oct/19 05:44",
        "Updated": "17/Oct/19 08:34",
        "Resolved": "16/Oct/19 07:41",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2071": {
        "Key": "MAHOUT-2071",
        "Summary": "fix bin.xml and assembly to allow for a binary apache-mahout-14.1.tar.gz release",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "17/Oct/19 07:46",
        "Updated": "12/Nov/19 08:23",
        "Resolved": "12/Nov/19 07:58",
        "Description": "the problem is theoretically solved ... we were overwriting the apache parent pom.xml'shttps://github.com/apache/mahout/blob/mahout-14.1/pom.xml#L21-L25 apache-release profile in our own, ...i removed the apache-release profile from ours and added what was left to mahout-release.so i expect that it should be working now. \u00a0the thing that was breaking everything (im gambling this whole theory on is that by overwriting the parent pom'sdeclaration of </mahout.skip.distribution>:\n \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 <skipAssembly>${mahout.skip.distribution}</skipAssembly>\n https://github.com/apache/mahout/blob/mahout-14.1/pom.xml#L512",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/382"
        ]
    },
    "MAHOUT-2072": {
        "Key": "MAHOUT-2072",
        "Summary": "upgrade to scala 2.12 spark 2.4.4 as defaults.",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "17/Oct/19 07:49",
        "Updated": "11/Mar/20 14:48",
        "Resolved": "17/Oct/19 08:35",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2073": {
        "Key": "MAHOUT-2073",
        "Summary": "Fix jenkins back up to release nightly snapshots.",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Palumbo",
        "Created": "17/Oct/19 09:46",
        "Updated": "06/Dec/19 22:43",
        "Resolved": "06/Dec/19 21:21",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2074": {
        "Key": "MAHOUT-2074",
        "Summary": "Dockerfile(s)",
        "Type": "New Feature",
        "Status": "In Progress",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "14.1",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "Joe Olson",
        "Reporter": "Andrew Palumbo",
        "Created": "17/Oct/19 15:31",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "Have a [WIP] Dockerfile for which (assuming a binary release,) pulls the appropriate version of Spark and places both Spark and Mahout in /opt/spark and /opt/mahout respectively.\nWould like to add full build mahout build capabilities (this should not be difficult) in a second file.\nthese files currently use an ENTRYPOINT[\"entrypoint.sh\"] command and some environment variables (none uncommon to Spark or Mahout aside from a $MAHOUT_CLASSPATH env variable). \u00a0\nthe entrypiont.sh essentially. cheeks to see if the command is form a worker or a driver, and runs as such.\u00a0 Currently I'm just dumping the entire $MAHOUT_HOME/lib/*.jar into the $MAHOUT_CLASSPATH and adding it to the SPARK_CLASSPATH.\nIf the\u00a0entrypoint.sh file detects a driver. it will launch\u00a0spark-submit.\u00a0 IIRC, which I so not think that I do, spark submit can handle any driver pferrel Does this sound correct.\u00a0 Otherwise we just add the mahout class to be passed to spark-submit class as a command parameter.\u00a0\nThough this may be better to migrate in 14.2 or 15.0 to an entire new build-chain. E.g. CMake. \u00a0 (I would suggest) given our large amount of native code (hopefully soon to be added\u00a0)\u00a0\nThough its nearly finished may want to punt this Dockerfile for 14.1, or mark it Experimental, is likely a better option.",
        "Issue Links": [
            "/jira/browse/MAHOUT-2025"
        ]
    },
    "MAHOUT-2075": {
        "Key": "MAHOUT-2075",
        "Summary": "Cross Compile and Deploy artifacts for Scala 2.11 and 2.12",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "02/Nov/19 09:29",
        "Updated": "12/Nov/19 08:01",
        "Resolved": "12/Nov/19 08:01",
        "Description": "pferrel, Mahout's current largest known customer, PMC member and contributor, has need for Scala 2.11 modules .\u00a0 rawkintrevo has created a\u00a0change-scala-versions.sh. Script; we Should be easily able to If only by hand for this version, compile and deploy Scala 2.11 modules To nexus .\u00a0 For the next releases, we may want to consider keeping a Scala 2.11 branch.",
        "Issue Links": []
    },
    "MAHOUT-2076": {
        "Key": "MAHOUT-2076",
        "Summary": "add a /release directory with a skeleton settings.xml for releases and instructions/scripts to release/deploy and any mixture therof",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "release",
        "Assignee": "Joe Olson",
        "Reporter": "Andrew Palumbo",
        "Created": "22/Nov/19 07:40",
        "Updated": "20/Oct/20 12:23",
        "Resolved": null,
        "Description": "add a /release directory with\u00a0settings.xml and all necessary variable changes to make a release candidate and to deploy.\u00a0\n\n\r\nchange-scalaversion.sh 2.11\r\nmvn release:prepare\u00a0-Papache-release\r\nmvn release:perform -Papache-release\r\nchange-scala-version 2.12\r\nmvn release:prepare\u00a0-Papache-release\r\nmvn release:perform -Papache-release\n\n\u00a0\nIt may be as simple as adding your personal GPG information to the `~/.m2/settings.xml` file. and running a script like above.\u00a0 Spark and Flink have\u00a0 examples which could be overkill:\nSpark:\nhttps://github.com/apache/spark/blob/master/dev/make-distribution.sh\nhttps://github.com/apache/spark/tree/master/dev/create-release\nFlink:\nhttps://github.com/apache/flink/tree/master/tools/releasing\nI would also propose that we consider a\u00a0 Makefile for our builds or cmake, as we are moving closer to python and .cpp modules. Could be useful, we should at least consider it.",
        "Issue Links": [
            "/jira/browse/MAHOUT-2084",
            "https://github.com/apache/mahout/pull/384"
        ]
    },
    "MAHOUT-2077": {
        "Key": "MAHOUT-2077",
        "Summary": "Clean up aftor behemoth PR",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "22/Nov/19 09:52",
        "Updated": "23/Jun/21 14:10",
        "Resolved": "23/Jun/21 14:10",
        "Description": "left some files around afrter pr#382 -\u00a0 MAHOUT-2071.\u00a0 there are sume dupliucate files and extra files around.",
        "Issue Links": []
    },
    "MAHOUT-2078": {
        "Key": "MAHOUT-2078",
        "Summary": "investigat Apache arrow as a backend for in core matricies.",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "05/Dec/19 11:10",
        "Updated": "05/Dec/19 11:14",
        "Resolved": null,
        "Description": "Start lokinging into Apache Arrow as an alterneate memory back end for mahout in-core and matrices.",
        "Issue Links": []
    },
    "MAHOUT-2079": {
        "Key": "MAHOUT-2079",
        "Summary": "investigat Cuda as a backend for in core matricies.",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "05/Dec/19 11:17",
        "Updated": "05/Dec/19 11:17",
        "Resolved": null,
        "Description": "with nvidia card\u00a0 memories now exceeding what used to be max for some machines.. 16 G easily it would be good to be able to dumb into Cuda memory directly from a statment or assignment.\u00a0 look into backing in-core matrices with CIDA memory or CUDA Shared memory.",
        "Issue Links": []
    },
    "MAHOUT-2080": {
        "Key": "MAHOUT-2080",
        "Summary": "fix change-scala-version.sh",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Dec/19 18:45",
        "Updated": "20/Oct/20 12:22",
        "Resolved": "20/Oct/20 12:22",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2081": {
        "Key": "MAHOUT-2081",
        "Summary": "Sloppy Java docs must be fixed",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "14.1",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "06/Dec/19 18:57",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "Starting in Java 8 java doc quality is much more strictly enforced.\u00a0\u00a0\nOver the years we've been pretty loose with our doc quality.\u00a0\nAt the moment this is commented out, but what we need is someone to go through and fix all the old java doc errors.",
        "Issue Links": []
    },
    "MAHOUT-2082": {
        "Key": "MAHOUT-2082",
        "Summary": "bump master to 2.12",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "06/Dec/19 19:05",
        "Updated": "23/Jan/20 14:57",
        "Resolved": "23/Jan/20 14:57",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2083": {
        "Key": "MAHOUT-2083",
        "Summary": "Release sources contain pom.XmlBackup and are mising engine and hdfs submodules",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "14.1",
        "Fix Version/s": "14.1",
        "Component/s": "release",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Trevor Grant",
        "Created": "22/Jan/20 19:56",
        "Updated": "23/Jan/20 16:20",
        "Resolved": "23/Jan/20 15:12",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/mahout/pull/388"
        ]
    },
    "MAHOUT-2084": {
        "Key": "MAHOUT-2084",
        "Summary": "a default source distribution for  is being created under $MAHOUT_HOME/target",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "22/Jan/20 20:27",
        "Updated": "11/Feb/20 07:32",
        "Resolved": "11/Feb/20 07:32",
        "Description": "make this behavior stop:\n\n\r\n$ mvn clean package install -DskipTests -Papache-release\n\n\u00a0\n\n\r\n{...}\r\n\r\n$ tree -L 1 \r\n{...}\r\n\u251c\u2500\u2500 lib\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mahout-core_2.11-14.1-SNAPSHOT.jar\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mahout-hdfs_2.11-14.1-SNAPSHOT.jar\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mahout-spark-cli-drivers_2.11-14.1-SNAPSHOT.jar\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mahout-spark_2.11-14.1-SNAPSHOT-dependency-reduced.jar\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 mahout-spark_2.11-14.1-SNAPSHOT.jar\r\n\u251c\u2500\u2500 mahout.iml\r\n\u251c\u2500\u2500 pom.xml\r\n\u251c\u2500\u2500 target\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 archive-tmp\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mahout-14.1-SNAPSHOT-source-release.zip\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mahout-14.1-SNAPSHOT-source-release.zip.asc\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mahout-14.1-SNAPSHOT.pom\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 mahout-14.1-SNAPSHOT.pom.asc\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 maven-shared-archive-resources\r\n\u2514\u2500\u2500 website\r\n{...}\n\n\u00a0\n\n\r\n \u251c\u2500\u2500 mahout-14.1-SNAPSHOT-source-release.zip\n\nseems to be a default behavior of the maven-release-plugin.\u00a0 We do not want this source distribution.\u00a0 We Have our assembled distributions in the /distribution directories:\n\n\r\n$ tree -L 2 distribution/\r\n\r\ndistribution/\r\n\u251c\u2500\u2500 KEYS\r\n\u251c\u2500\u2500 distribution.iml\r\n\u251c\u2500\u2500 distribution_2.11.iml\r\n\u251c\u2500\u2500 distribution_2.12.iml\r\n\u251c\u2500\u2500 pom.xml\r\n\u251c\u2500\u2500 src\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 main\r\n\u2514\u2500\u2500 target\r\n    \u251c\u2500\u2500 distribution_2.11-14.1-SNAPSHOT.pom\r\n    \u251c\u2500\u2500 distribution_2.11-14.1-SNAPSHOT.pom.asc\r\n    \u2514\u2500\u2500 maven-shared-archive-resources \n\nthis target should contain the assembled disrtribution as described in the {{$MAHOUT_HOME/distribution/src/main/resources/rassembly/ }} poms:\n\n\r\ndistribution/src/main/resources/assembly$ tree\r\n.\r\n\u251c\u2500\u2500 bin.xml\r\n\u2514\u2500\u2500 src.xml",
        "Issue Links": [
            "/jira/browse/MAHOUT-2076",
            "https://github.com/apache/mahout/pull/389"
        ]
    },
    "MAHOUT-2085": {
        "Key": "MAHOUT-2085",
        "Summary": "Add scm tag for each pom.xml per asf release Guidelines",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "23/Jan/20 15:16",
        "Updated": "23/Jan/20 17:34",
        "Resolved": "23/Jan/20 17:29",
        "Description": "https://www.apache.org/dev/publishing-maven-artifacts.html \nIs pretty adament about having a SCM tag in each pom.  Since our project has a pretty complicated structure, adding this tag back in to stay with guidelines.",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/390"
        ]
    },
    "MAHOUT-2086": {
        "Key": "MAHOUT-2086",
        "Summary": "use consistent SBT resolvable jar naming scheme with the correct convention",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "14.1",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "23/Jan/20 15:46",
        "Updated": "23/Jan/20 18:35",
        "Resolved": "23/Jan/20 18:34",
        "Description": "currently we build jars for each module, module_x, in its respective target directory.. naming that module as\n\n\r\n $MAHOUT_HOME/module_x/target/mahout-module-x_${scala.compat.version}-{project.version}.jar.\n\nand then copy using an ant plugin via the pom.xml each jar file to the /lib directory.\u00a0 pferrel\u00a0 has had issues resolving via SBT.\u00a0\u00a0\nIt seems that our artifacts should be resolvable by SBT e.g, for core math, Scala_2.12, mahout v14.1:\n\n\r\n libraryDependencies += \"org.apache.mahout\" % \"mahout-core_2.12\" % \"14.1\"\n\nCurrently we have our Spark build pegged to 2.4.3.\nHad trouble finding A naming convention for artifacts in the Scala docs.\u00a0 However, this Spark style guide[1], makes the case for:\nmodule-x_scalaVersion-projectVersion.jar\n\u00a0\nThis means that only the distribution artifacts jars need to be changed, If anything.\n\u00a0\n\u00a0\nJAR Files\nYou can build projects that support multiple Spark versions or just a single Spark version.\nProjects that support a single Spark version\nJAR files built for a specific Spark version should be named like this:\n\u00a0\nspark-testing-base_2.11-2.1.0_0.6.0.jar\nGenerically:\n\u00a0\nspark-testing-base_scalaVersion-sparkVersion_projectVersion.jar\n{{}}\nIf you're using sbt assembly, you can use the following line of code to build a JAR file using the correct naming conventions.\n assemblyJarName in assembly := s\"${name.value}${scalaBinaryVersion.value}-${sparkVersion.value}${version.value}.jar\"\n If you're using sbt package, you can add this code to your build.sbt file to generate a JAR file that follows the naming conventions.\n artifactName :=\n\n{ (sv: ScalaVersion, module: ModuleID, artifact: Artifact) => artifact.name + \"_\" + sv.binary + \"-\" + sparkVersion.value + \"_\" + module.revision + \".\" + artifact.extension }\nProjects that support multiple Spark versions\nJAR files built for multiple Spark version should be named like this:\nspark-testing-base_2.11-0.6.0.jar\nGenerically:\nspark-testing-base_scalaVersion-projectVersion.jar\n{{}}\nIf you're using sbt assembly, you can use the following line of code to build a JAR file using the correct naming conventions.\n assemblyJarName in assembly := s\"${name.value}_${scalaBinaryVersion.value}-${version.value}.jar\"\n If you're using sbt package, you can add this code to your build.sbt file to generate a JAR file that follows the naming conventions.\n artifactName :=\n\n{ (sv: ScalaVersion, module: ModuleID, artifact: Artifact) => artifact.name + \"_\" + sv.binary + \"-\" + module.revision + \".\" + artifact.extension }\n\n\u00a0\n\u00a0\n[1] https://github.com/mrpowers/spark-style-guide",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/391"
        ]
    },
    "MAHOUT-2087": {
        "Key": "MAHOUT-2087",
        "Summary": "Seven mentor provides AWS SysOps tRAINING IN PUNE",
        "Type": "Story",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "pallavi patil",
        "Created": "27/Jan/20 13:19",
        "Updated": "11/Feb/20 07:26",
        "Resolved": "11/Feb/20 07:26",
        "Description": "[SevenMentor|https://www.sevenmentor.com/] is best training institute in pune. we provides various classes. [AWS SysOps Training in Pune|https://www.sevenmentor.com/aws-sysops-training-in-pune.php] is one of them. If you are interested to enroll for AWS SysOps classes in Pune then seven mentor is best option for you.We provide complete syllabus with well experienced trainers.",
        "Issue Links": []
    },
    "MAHOUT-2088": {
        "Key": "MAHOUT-2088",
        "Summary": "update Apache Parent pom to latest version (23)",
        "Type": "Bug",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "11/Feb/20 07:29",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "we are currently using Apache parent pom 18 which is from 2016:\nhttps://maven.apache.org/pom/asf/\nUpgrade to most recent pom: version 23.  This may alleviate some of our build issues?",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/392"
        ]
    },
    "MAHOUT-2089": {
        "Key": "MAHOUT-2089",
        "Summary": "generate sha-256 and sha-512 checksums for all release artifacts",
        "Type": "Bug",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "build,                                            release",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "11/Feb/20 10:36",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "SHA-256 and sha-512 artifacts are not being generated during release",
        "Issue Links": []
    },
    "MAHOUT-2090": {
        "Key": "MAHOUT-2090",
        "Summary": "RC4 - distribution module is not included in src distribution",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "14.1",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "11/Feb/20 11:19",
        "Updated": "21/Feb/20 16:51",
        "Resolved": "21/Feb/20 16:51",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2091": {
        "Key": "MAHOUT-2091",
        "Summary": "Apache Rat check fails: Too many files with unapproved license",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.1",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "21/Feb/20 07:18",
        "Updated": "25/Jun/20 20:48",
        "Resolved": "21/Feb/20 16:50",
        "Description": "running mvn clean apache-rat:check -e -DskipTests=true the build fails with\n\u00a0\n\n\r\nERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.13:check (default-cli) on project mahout: Too many files with unapproved license: 2 See RAT report in: /Users/colleenpalumbo/sandbox/mahout/target/rat.txt -> [Help 1]\r\n org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.rat:apache-rat-plugin:0.13:check (default-cli) on project mahout: Too many files with unapproved license: 2 See RAT report in: /Users/colleenpalumbo/sandbox/mahout/target/rat.txt",
        "Issue Links": [
            "https://github.com/apache/mahout/pull/393"
        ]
    },
    "MAHOUT-2092": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2093": {
        "Key": "MAHOUT-2093",
        "Summary": "Mahout Source Broken",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.0,                                            0.13.2,                                            14.1",
        "Fix Version/s": "14.2",
        "Component/s": "Algorithms,                                            Collaborative Filtering,                                            Documentation",
        "Assignee": "Pat Ferrel",
        "Reporter": "Stefan Goldener",
        "Created": "27/Feb/20 15:44",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "Seems like newer versions of Mahout do have problems with spark bindings e.g.\u00a0mahout spark-itemsimilarity or\u00a0mahout spark-rowsimilarity do not work due to class not found exceptions.\u00a0\n\n\r\nError: Could not find or load main class org.apache.mahout.drivers.RowSimilarityDriver\r\n\n\n\n\r\nError: Could not find or load main class org.apache.mahout.drivers.ItemSimilarityDriver\r\n\n\nwhereas\u00a0mahout spark-shell\u00a0works flawlessly.\nHere is a short Dockerfile to show the issue:\n\n\r\nFROM openjdk:8-alpine\r\nENV spark_uid=185\r\nENV SCALA_MAJOR=2.11\r\nENV SCALA_MAJOR_MINOR=2.11.12\r\nENV HADOOP_MAJOR=2.7\r\nENV SPARK_MAJOR_MINOR=2.4.5\r\nENV MAHOUT_MAJOR_MINOR=0.14.0\r\nENV MAHOUT_VERSION=mahout-${MAHOUT_MAJOR_MINOR}\r\nENV MAHOUT_BASE=/opt/mahout\r\nENV MAHOUT_HOME=${MAHOUT_BASE}/${MAHOUT_VERSION}\r\nENV SPARK_VERSION=spark-${SPARK_MAJOR_MINOR}\r\nENV SPARK_BASE=/opt/spark\r\nENV SPARK_HOME=${SPARK_BASE}/${SPARK_VERSION}\r\nENV MAVEN_OPTS=\"-Xmx2g -XX:ReservedCodeCacheSize=1g\"\r\nENV SPARK_SRC_URL=\"https://archive.apache.org/dist/spark/${SPARK_VERSION}/${SPARK_VERSION}.tgz\"\r\nENV MAHOUT_SRC_URL=\"https://archive.apache.org/dist/mahout/${MAHOUT_MAJOR_MINOR}/mahout-${MAHOUT_MAJOR_MINOR}-source-release.zip\"\r\nENV ZINC_PORT=3030\r\n\r\n### build spark\r\nRUN set -ex && \\\r\n    apk upgrade --no-cache && \\\r\n    ln -s /lib /lib64 && \\\r\n    apk add --no-cache bash python py-pip tini libc6-compat linux-pam krb5 krb5-libs nss curl openssl git maven && \\\r\n    pip install setuptools && \\\r\n    mkdir -p ${MAHOUT_HOME} && \\\r\n    mkdir -p ${SPARK_BASE} && \\\r\n    curl  -LfsS ${SPARK_SRC_URL} -o ${SPARK_HOME}.tgz  && \\\r\n    tar -xzvf ${SPARK_HOME}.tgz -C ${SPARK_BASE}/ && \\\r\n    rm ${SPARK_HOME}.tgz && \\\r\n    export PATH=$PATH:$MAHOUT_HOME/bin:$MAHOUT_HOME/lib:$SPARK_HOME/bin:$JAVA_HOME/bin && \\\r\n    bash ${SPARK_HOME}/dev/change-scala-version.sh ${SCALA_MAJOR} && \\\r\n    bash ${SPARK_HOME}/dev/make-distribution.sh --name ${DATE}-${REVISION} --pip --tgz -DzincPort=${ZINC_PORT} \\\r\n            -Phadoop-${HADOOP_MAJOR} -Pkubernetes -Pkinesis-asl -Phive -Phive-thriftserver -Pscala-${SCALA_MAJOR}\r\n    \r\n### build mahout\r\nRUN curl -LfsS $MAHOUT_SRC_URL -o ${MAHOUT_BASE}.zip  && \\\r\n    unzip ${MAHOUT_BASE}.zip -d ${MAHOUT_BASE} && \\ \r\n    rm ${MAHOUT_BASE}.zip && \\\r\n    cd ${MAHOUT_HOME} && \\\r\n    mvn -Dspark.version=${SPARK_MAJOR_MINOR} -Dscala.version=${SCALA_MAJOR_MINOR} -Dscala.compat.version=${SCALA_MAJOR} -DskipTests -Dmaven.javadoc.skip=true clean package \r\n\n\ndocker build . -t mahout-test\n docker run -it mahout-test /bin/bash",
        "Issue Links": [
            "/jira/browse/MAHOUT-2102",
            "/jira/browse/MAHOUT-2100",
            "/jira/browse/MAHOUT-2023"
        ]
    },
    "MAHOUT-2094": {
        "Key": "MAHOUT-2094",
        "Summary": "Advanced Excel Training In Pune",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "aakashraj raj",
        "Created": "29/Feb/20 12:38",
        "Updated": "31/Mar/20 21:14",
        "Resolved": "01/Mar/20 08:18",
        "Description": "Become a Job-ready person with\u00a0Advance Excel Training in Pune. SevenMentor's Best Excel Training in Pune offers in-depth knowledge of MS Excel.",
        "Issue Links": []
    },
    "MAHOUT-2095": {
        "Key": "MAHOUT-2095",
        "Summary": "Inconsistent library versions notice.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kaifeng Huang",
        "Created": "01/Mar/20 12:37",
        "Updated": "01/Mar/20 12:37",
        "Resolved": null,
        "Description": "Hi. I have implemented a tool to detect library version inconsistencies. Your project have 2 inconsistent libraries.\nTake com.google.guava:guava for example, this library is declared as version 14.0.1 in core, 11.0.2 in community/mahout-mr/mr and etc... Such version inconsistencies may cause unnecessary maintenance effort in the long run. For example, if two modules become inter-dependent, library version conflict may happen. It has already become a common issue and hinders development progress. Thus a version harmonization is necessary.\nProvided we applied a version harmonization, I calculated the cost it may have to harmonize to all upper versions including an up-to-date one. The cost refers to POM config changes and API invocation changes. Take com.google.guava:guava for example, if we harmonize all the library versions into 28.1-android. The concern is, how much should the project code adapt to the newer library version. We list an effort table to quantify the harmonization cost.\nThe effort table is listed below. It shows the overall harmonization effort by modules. The columns represents the number of library APIs and API calls(NA,NAC), deleted APIs and API calls(NDA,NDAC) as well as modified API and API calls(NMA,NMAC). Modified APIs refers to those APIs whose call graph is not the same as previous version. Take the first row for example, if upgrading the library into version 28.1-android. Given that 84 APIs is used in module community/mahout-mr/mr, 10 of them is deleted in a recommended version(which will throw a NoMethodFoundError unless re-compiling the project), 68 of them is regarded as modified which could break the former API contract.\n\n\n\nIndex\nModule\nNA(NAC)\nNDA(NDAC)\nNMA(NMAC)\n\n\n1\ncommunity/mahout-mr/mr\n84(163)\n10(12)\n68(139)\n\n\n2\ncore\n19(37)\n2(2)\n17(35)\n\n\n\nAlso we provided another table to show the potential files that may be affected due to library API change, which could help to spot the concerned API usage and rerun the test cases. The table is listed below.\n\n\n\nModule\nFile\nType\nAPI\n\n\ncore\ncore/src/test/java/org/apache/mahout/math/set/HashUtilsTest.java\nmodify\ncom.google.common.collect.Lists.newArrayList()\n\n\ncore\ncore/src/test/java/org/apache/mahout/math/set/HashUtilsTest.java\nmodify\ncom.google.common.collect.HashMultiset.create()\n\n\ncore\ncore/src/main/java/org/apache/mahout/math/als/AlternatingLeastSquaresSolver.java\nmodify\ncom.google.common.base.Preconditions.checkArgument(boolean)\n\n\n4\n..\n..\n..\n\n\n\n\u00a0\nIf you are interested, you can have a more complete and detailed report in the attached PDF file.",
        "Issue Links": []
    },
    "MAHOUT-2096": {
        "Key": "MAHOUT-2096",
        "Summary": "next() Called On Possible Empty iterator()",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "Algorithms",
        "Assignee": null,
        "Reporter": "Shangru Li",
        "Created": "03/Mar/20 00:43",
        "Updated": "14/Jul/20 19:10",
        "Resolved": null,
        "Description": "Consider this function:\n\n\r\n protected void reduce ( VarIntWritable index, Iterable<VarLongWritable> ids, Context\r\n\u00a0\u00a0 ctx) throws IOException, InterruptedException {\r\n\u00a0\u00a0\u00a0\u00a0 ctx.write(index, ids.iterator().next());\r\n }\r\n\n\nThe line `ctx.write(index, ids.iterator().next());` does not check for empty iterator when calling the next() function, which could result in an exception.\nFunction can be found on the GitHub repository here.\nLine 395, in file /src/main/java/org/apache/mahout/cf/taste/hadoop/als/ParallelALSFactorizationJob.java",
        "Issue Links": []
    },
    "MAHOUT-2097": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2098": {
        "Key": "MAHOUT-2098",
        "Summary": "Release 0.14 mahout-mr to maven central repo",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.14.0",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Robert Gonciarz",
        "Created": "06/Mar/20 20:35",
        "Updated": "20/Oct/20 12:21",
        "Resolved": "20/Oct/20 12:21",
        "Description": "Please release the newest artefacts to central maven repository.\nThe last version I may find there is 0.13:\n[https://mvnrepository.com/artifact/org.apache.mahout/mahout-mr\n]",
        "Issue Links": []
    },
    "MAHOUT-2099": {
        "Key": "MAHOUT-2099",
        "Summary": "Using Mahout as a Library in Spark Cluster",
        "Type": "Question",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "cooccurrence,                                            Math",
        "Assignee": null,
        "Reporter": "Tariq Jawed",
        "Created": "22/Mar/20 16:39",
        "Updated": "28/Mar/20 01:06",
        "Resolved": null,
        "Description": "I have a Spark Cluster already setup, and this is the environment not in my direct control, but they do allow FAT JARs to be installed with the dependencies. I tried to package my Spark Application with some mahout code for SimilarityAnalysis, added Mahout library in POM file, and they are successfully packaged.\nThe problem however is that I am getting this error while using existing Spark Context to build Distributed Spark Context for\nMahout\n[EDIT]AP:\n\n\r\npom.xml\r\n\r\n{...}\r\n\r\ndependency>\r\n <groupId>org.apache.mahout</groupId>\r\n <artifactId>mahout-math</artifactId>\r\n <version>0.13.0</version>\r\n </dependency>\r\n <dependency>\r\n <groupId>org.apache.mahout</groupId>\r\n <artifactId>mahout-math-scala_2.10</artifactId>\r\n <version>0.13.0</version>\r\n </dependency>\r\n <dependency>\r\n <groupId>org.apache.mahout</groupId>\r\n <artifactId>mahout-spark_2.10</artifactId>\r\n <version>0.13.0</version>\r\n </dependency>\r\n <dependency>\r\n <groupId>com.esotericsoftware</groupId>\r\n <artifactId>kryo</artifactId>\r\n <version>5.0.0-RC5</version>\r\n </dependency>\r\n\r\n \n\n\u00a0\nCode:\n\n\r\n\r\nimplicit val sc: SparkContext = sparkSession.sparkContext\r\n\r\nimplicit val msc: SparkDistributedContext = sc2sdc(sc)\r\n\r\nError:\r\n\r\nERROR TaskSetManager: Task 7.0 in stage 10.0 (TID 58) had a not serializable result: org.apache.mahout.math.DenseVector\r\n\r\n\u00a0\r\n\r\nAnd if I try to build the context using mahoutSparkContext() then its giving me the error that MAHOUT_HOME not found.\u00a0\r\n\r\nCode:\r\n\r\nimplicit val msc = mahoutSparkContext(masterUrl = \"local\", appName = \"CooccurrenceDriver\")\r\n\r\nError:\r\n\r\nMAHOUT_HOME is required to spawn mahout-based spark jobs\r\n\r\n\u00a0\n\nMy question is that how do I proceed in this situation? should I have to ask the administrators of the Spark environment to install Mahout library, or is there anyway I can proceed packaging my application as fat JAR.",
        "Issue Links": []
    },
    "MAHOUT-2100": {
        "Key": "MAHOUT-2100",
        "Summary": "Dependencies (Scopt e.g.) are not being picked on the classpath nor shipped in ./lib",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Blocker",
        "Resolution": "Unresolved",
        "Affects Version/s": "14.1",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "24/Mar/20 21:08",
        "Updated": "20/Oct/20 12:04",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/MAHOUT-2093"
        ]
    },
    "MAHOUT-2101": {
        "Key": "MAHOUT-2101",
        "Summary": "Mahout local file distribution",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Goldener",
        "Created": "25/Mar/20 07:58",
        "Updated": "25/Mar/20 08:06",
        "Resolved": null,
        "Description": "At the moment Mahout is heavily based on HDFS. Although MAHOUT_LOCAL is using the local File system it is not possible to use MAHOUT_LOCAL=true and a SPARK ONLY Cluster.\nMy suggestion is to improve the Mahout code to support local files and distribute them via SPARK. There are multiple options for that e.g. Spark SQL, DataFrames, Datasets or RDD's.\nThis will also allow Mahout to use the new SPARK Kubernetes features and hence be highly scalable.\nProbably the best improvement would be mahout using spark context and just reading the files via sc.textFile(\"file:///path to the file/\")\nThis would then look just like this. While the only problem now is just how the file is read (the executors cannot find the file because it's only existing on the driver)\n\n\r\nmahout spark-itemsimilarity -i /tmp/file.txt -o ~/dataout/out.txt -rd ',' -f1 pur -rc 0 -fc 1 -ic 2 -os -sem 10g -ma k8s://localhost:8080 -D:spark.dynamicAllocation.enabled=true -D:spark.shuffle.service.enabled=true -D:spark.executor.instances=5 -D:spark.kubernetes.container.image=$CONAINER_IMAGE -D:spark.kubernetes.namespace=$NAMESPACE -D:spark.driver.host=$IP",
        "Issue Links": []
    },
    "MAHOUT-2102": {
        "Key": "MAHOUT-2102",
        "Summary": "transitive and direct dependencies not being piched up in ~/lib by /bin/mahout",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "14.1",
        "Fix Version/s": "14.2",
        "Component/s": "None",
        "Assignee": "Andrew Palumbo",
        "Reporter": "Andrew Palumbo",
        "Created": "31/Mar/20 01:52",
        "Updated": "02/Feb/21 22:11",
        "Resolved": "02/Feb/21 22:11",
        "Description": "need to ensure dependencies are added to /lib/dependencies or packed into each module jar to be picked up bu ./bin/mahout\nhttps://github.com/apache/mahout/compare/master...andrewpalumbo:dep-copy-2?expand=1",
        "Issue Links": [
            "/jira/browse/MAHOUT-2093"
        ]
    },
    "MAHOUT-2103": {
        "Key": "MAHOUT-2103",
        "Summary": "provides AWS SysOps tRAINING IN PUNE",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "kira yosuf",
        "Created": "02/Apr/20 04:48",
        "Updated": "02/Apr/20 18:02",
        "Resolved": "02/Apr/20 18:01",
        "Description": "Spam deleted",
        "Issue Links": []
    },
    "MAHOUT-2104": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2105": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2106": {
        "Key": "MAHOUT-2106",
        "Summary": "Remove words \"whitelist\" and \"blacklist\" from codebase",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "07/Jun/20 21:26",
        "Updated": "23/Jun/20 12:08",
        "Resolved": "09/Jun/20 23:43",
        "Description": "Whitelist words can be found here:\nhttps://github.com/apache/mahout/search?q=whitelist&unscoped_q=whitelist\nIn inline comments in BloomTokenFilter\n\u00a0\nNo known instances of \"blacklist\"",
        "Issue Links": []
    },
    "MAHOUT-2107": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2108": {
        "Key": "MAHOUT-2108",
        "Summary": "Code comment with issue number",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "ackelcn",
        "Created": "23/Jun/20 07:32",
        "Updated": "25/Jun/20 20:46",
        "Resolved": null,
        "Description": "When I read the code of mahout, I find some comments with issue numbers. One of them comes from ClusterClassificationDriver.java:\n\n\r\n // belongs to which cluster - fix for MAHOUT-1410\r\n      Class<? extends Writable> keyClass = vw.getFirst().getClass();\r\n      Vector vector = vw.getSecond().get();\r\n      if (!keyClass.equals(NamedVector.class)) {\r\n        if (keyClass.equals(Text.class)) {\r\n          vector = new NamedVector(vector, vw.getFirst().toString());\r\n        } else if (keyClass.equals(IntWritable.class)) {\r\n          vector = new NamedVector(vector, Integer.toString(((IntWritable) vw.getFirst()).get()));\r\n        }\r\n      }                    \n\nThese comments are quite useful for other programmers and me to understand the code, but I notice that not all issue numbers are written in code comments. It can be already quite tedious to write them into commit messages\u00a0\n\u00a0\nTo handle the problem, I implemented a tool to automatically instrument issue numbers into code comments. I tried my tool on activemq, and the instrumented version is https://github.com/ackelcn/mahout\u00a0\n\u00a0\nTo avoid confusion, if there is already an issue number in code comments, my tool ignored the issue number. All my generated comments start from //IC, so it is easy to find them.\n\u00a0\nWould you please some feedbacks to my tool? Please feel free to merge my generated comments in your code, if you feel that some are useful.",
        "Issue Links": []
    },
    "MAHOUT-2109": {
        "Key": "MAHOUT-2109",
        "Summary": "Add release notes to source bundle",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "14.2",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "30/Jun/20 14:54",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "Typically RELEASE_NOTES are added alongside the source bundle.",
        "Issue Links": []
    },
    "MAHOUT-2110": {
        "Key": "MAHOUT-2110",
        "Summary": "Deprecate MD5, upgrade to SHA512",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "14.2",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "30/Jun/20 14:55",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "Per https://infra.apache.org/release-distribution.html",
        "Issue Links": []
    },
    "MAHOUT-2111": {
        "Key": "MAHOUT-2111",
        "Summary": "The name of the source bundle doesn't contain \"apache\" in it",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.1",
        "Component/s": "build",
        "Assignee": "Christofer Dutz",
        "Reporter": "Andrew Musselman",
        "Created": "17/Jul/20 19:09",
        "Updated": "21/Jul/20 06:57",
        "Resolved": "21/Jul/20 06:57",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2112": {
        "Key": "MAHOUT-2112",
        "Summary": "Your keys don't seem to be in the ASF web of trust",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "17/Jul/20 19:10",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "Perhaps attending a Key-Signing party would be a good idea.\nIn this time of quarantine, I think a virtual key-signing party would be great.",
        "Issue Links": []
    },
    "MAHOUT-2113": {
        "Key": "MAHOUT-2113",
        "Summary": "MD5 and SHA1 hashes are considered deprecated and SHA512 should be used",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.1",
        "Component/s": "build",
        "Assignee": "Christofer Dutz",
        "Reporter": "Andrew Musselman",
        "Created": "17/Jul/20 19:12",
        "Updated": "21/Jul/20 06:57",
        "Resolved": "21/Jul/20 06:57",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2114": {
        "Key": "MAHOUT-2114",
        "Summary": "Archive contains LICENSE and LICENSE.txt as well as NOTICE and NOTICE.txt",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Andrew Musselman",
        "Created": "17/Jul/20 19:22",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "Perhaps removing the file ending of the NOTICE.txt and the LICENSE.txt would eliminate this",
        "Issue Links": []
    },
    "MAHOUT-2115": {
        "Key": "MAHOUT-2115",
        "Summary": "Add RELEASE_NOTES",
        "Type": "Task",
        "Status": "Reopened",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.2",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "17/Jul/20 19:24",
        "Updated": "08/Oct/20 14:44",
        "Resolved": null,
        "Description": "Is this something we can do through JIRA?",
        "Issue Links": []
    },
    "MAHOUT-2116": {
        "Key": "MAHOUT-2116",
        "Summary": "NOTICE.txt does not contain current date",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.1",
        "Component/s": "build",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Musselman",
        "Created": "17/Jul/20 19:25",
        "Updated": "22/Jul/20 20:18",
        "Resolved": "22/Jul/20 20:18",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2117": {
        "Key": "MAHOUT-2117",
        "Summary": "RAT scan without exclusions shows: \"443 Unknown Licenses\"",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.14.1",
        "Fix Version/s": "14.1",
        "Component/s": "build",
        "Assignee": "Christofer Dutz",
        "Reporter": "Andrew Musselman",
        "Created": "17/Jul/20 19:26",
        "Updated": "08/Oct/20 14:46",
        "Resolved": "08/Oct/20 14:46",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2118": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2119": {
        "Key": "MAHOUT-2119",
        "Summary": "Migrate from Jenkins to Git Actions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "07/Aug/20 13:16",
        "Updated": "20/Oct/20 12:19",
        "Resolved": "20/Oct/20 12:19",
        "Description": "Per mailing list from cdutz\u00a0 [1] we need to migrate away from current Jenkins CI build.\nTalked to infra- moving to git actions.\n\u00a0\n\u00a0[1] https://lists.apache.org/thread.html/r957fde25cb62bb4042089cbf03eed47cc491ae41bb0e6af5232782e7%40%3Cdev.mahout.apache.org%3E",
        "Issue Links": []
    },
    "MAHOUT-2120": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2121": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2122": {
        "Key": "MAHOUT-2122",
        "Summary": "Create a getting started docker container",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "10/Sep/20 20:21",
        "Updated": "13/Nov/20 18:12",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2123": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2124": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2125": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2126": {
        "Key": "MAHOUT-2126",
        "Summary": "Update Zeppelin Containers",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "14.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "08/Oct/20 12:49",
        "Updated": "20/Oct/20 15:38",
        "Resolved": null,
        "Description": "[ ]Create new \"Intro to Mahout\" Notebooks\n[ ]Update container with 14.1 Binaries\n[ ]Get Apache Docker repo access (see INFRA-20953)\n[ ]Push container to repo",
        "Issue Links": []
    },
    "MAHOUT-2127": {
        "Key": "MAHOUT-2127",
        "Summary": "Fix broken DistanceMetrics.scala hyperlink",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "14.2",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Oct/20 22:28",
        "Updated": "15/Oct/20 01:26",
        "Resolved": "15/Oct/20 01:26",
        "Description": "The http://mahout.apache.org/docs/latest/algorithms/clustering/distance-metrics.html displays a hyperlink for \nhttps://github.com/apache/mahout/blob/master/math-scala/src/main/scala/org/apache/mahout/math/algorithms/common/DistanceMetrics.scala\n... which should be\nhttps://github.com/apache/mahout/blob/trunk/core/src/main/scala/org/apache/mahout/math/algorithms/common/distance/DistanceMetrics.scala\nPR coming up...",
        "Issue Links": []
    },
    "MAHOUT-2128": {
        "Key": "MAHOUT-2128",
        "Summary": "Fix download page",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "14.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "16/Oct/20 16:53",
        "Updated": "16/Oct/20 16:54",
        "Resolved": null,
        "Description": "announce-owner@apache.org \n\n\nThu, Oct 8, 12:41 PM (8 days ago)\n\n\n\n\n\n\n\u00a0\n\u00a0\n\n\n\n\n\n\n\n\nto akm \n\n\n\n\n\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0Hi! This is the ezmlm program. I'm managing the\nannounce@apache.org mailing list.\n I'm sorry, your message (enclosed) was not accepted by the moderator.\n If the moderator has made any comments, they are shown below.\n >>>>> -------------------- >>>>>\n I'm afraid this announcement is not valid, as the download links must be to a project-specific download page that supports the mirror system.\n Specifically, the announcement must not include \"download the release artifacts and signatures from https://downloads.apache.org/mahout/14.1/\"\n Further in the announcement is a proper link \"Download the release artifacts and signatures at https://mahout.apache.org/general/downloads.html\"\n But the downloads.html does not support the mirror system including instructions on obtaining the KEYS file for verification.\n Please correct the download page and the announcement and resubmit.\n Regards,\n Craig",
        "Issue Links": []
    },
    "MAHOUT-2129": {
        "Key": "MAHOUT-2129",
        "Summary": "Rebuild docs and add to website nav",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "14.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Musselman",
        "Created": "16/Oct/20 16:55",
        "Updated": "04/Jun/21 15:29",
        "Resolved": "04/Jun/21 15:29",
        "Description": "Nav still has 0.13.0 docs only",
        "Issue Links": []
    },
    "MAHOUT-2130": {
        "Key": "MAHOUT-2130",
        "Summary": "Add talks page to website",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "14.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "16/Oct/20 16:57",
        "Updated": "24/Oct/20 14:53",
        "Resolved": null,
        "Description": "Add page with talks and slides, add posts in the news section to them",
        "Issue Links": []
    },
    "MAHOUT-2131": {
        "Key": "MAHOUT-2131",
        "Summary": "Commit hook for website",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "14.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Musselman",
        "Created": "16/Oct/20 17:03",
        "Updated": "20/Oct/20 12:07",
        "Resolved": "20/Oct/20 12:07",
        "Description": "Turn off website build on pull request",
        "Issue Links": []
    },
    "MAHOUT-2132": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2133": {
        "Key": "MAHOUT-2133",
        "Summary": "Quickstart Page needs update",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "06/Nov/20 18:39",
        "Updated": "04/Jun/21 15:50",
        "Resolved": "04/Jun/21 15:29",
        "Description": "https://github.com/apache/mahout/blob/trunk/website/docs/latest/quickstart.md",
        "Issue Links": []
    },
    "MAHOUT-2134": {
        "Key": "MAHOUT-2134",
        "Summary": "Update zeppelin tutorial",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "06/Nov/20 18:41",
        "Updated": "23/Jun/21 14:02",
        "Resolved": "23/Jun/21 14:02",
        "Description": "https://github.com/apache/mahout/tree/trunk/website/docs/latest/tutorials/misc/mahout-in-zeppelin\n\u00a0\nMaybe just delete in favor of new zeppelin docker",
        "Issue Links": []
    },
    "MAHOUT-2135": {
        "Key": "MAHOUT-2135",
        "Summary": "Fix website build",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "06/Jan/21 21:05",
        "Updated": "02/Feb/21 20:07",
        "Resolved": "02/Feb/21 20:07",
        "Description": "When closing a PR the following error occurs on the gitaction\n```\nad-m/github-push-action@master is not allowed to be used in apache/mahout. Actions in this workflow must be: created by GitHub, verified in the GitHub Marketplace, within a repository owned by apache or match the following: apache/, conda-incubator/setup-miniconda@, dawidd6/action-download-artifact@, gradle/wrapper-validation-action@, peaceiris/actions-gh-pages@, peaceiris/actions-hugo@, peter-evans/create-pull-request@, scacap/action-surefire-report@, shivammathur/setup-php@, shogo82148/actions-setup-perl@. \n```\nWill contact infra fro new git action to use and update site",
        "Issue Links": []
    },
    "MAHOUT-2136": {
        "Key": "MAHOUT-2136",
        "Summary": "Implement Ridge Regression Algorithm",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jose Francisco Hernandez Santa Cruz",
        "Reporter": "Jose Francisco Hernandez Santa Cruz",
        "Created": "08/Jan/21 17:02",
        "Updated": "02/Feb/21 20:06",
        "Resolved": "02/Feb/21 20:06",
        "Description": "Ridge regression is a regularized approach of the linear regression by OLS method. Non-full rank matrices are allowed as the regularization parameter adds a diagonal matrix to the optimization equation breaking collinearity by adding a bias component, thus reducing standard error.\n\u00a0\nhttps://en.wikipedia.org/wiki/Tikhonov_regularization",
        "Issue Links": []
    },
    "MAHOUT-2137": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2138": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2139": {
        "Key": "MAHOUT-2139",
        "Summary": "build failed on AArch64, Fedora 33",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Lutz Weischer",
        "Created": "12/Mar/21 13:08",
        "Updated": "15/Mar/23 00:16",
        "Resolved": "15/Mar/23 00:16",
        "Description": "[jw@cn05 mahout]$ mvn install -DskipTests\n[INFO] Scanning for projects...\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Build Order:\n[INFO]\n[INFO] Apache Mahout                                                      [pom]\n[INFO] Mahout Core                                                        [jar]\n[INFO] Mahout Engine                                                      [pom]\n[INFO] - Mahout HDFS Support                                              [jar]\n[INFO] - Mahout Spark Engine                                              [jar]\n[INFO] Mahout Distribution                                                [pom]\n[INFO]\n[INFO] ---------------------< org.apache.mahout:mahout >---------------------\n[INFO] Building Apache Mahout 14.2-SNAPSHOT                               [1/6]\n[INFO] -------------------------------[ pom ]--------------------------------\n[INFO]\n[INFO] \u2014 apache-rat-plugin:0.13:check (default) @ mahout \u2014\n[INFO] Added 2 additional default licenses.\n[INFO] Enabled default license matchers.\n[INFO] Will parse SCM ignores for exclusions...\n[INFO] Parsing exclusions from /home/jw/apache/mahout/.gitignore\n[INFO] Finished adding exclusions from SCM ignore files.\n[INFO] 92 implicit excludes (use -debug for more details).\n[INFO] 21 explicit excludes (use -debug for more details).\n[INFO] 1188 resources included (use -debug for more details)\n[INFO] Rat check: Summary over all files. Unapproved: 1, unknown: 1, generated: 0, approved: 1180 licenses.\n[INFO] Added 2 additional default licenses.\n[INFO] Enabled default license matchers.\n[INFO] Will parse SCM ignores for exclusions...\n[INFO] Parsing exclusions from /home/jw/apache/mahout/.gitignore\n[INFO] Finished adding exclusions from SCM ignore files.\n[INFO] 92 implicit excludes (use -debug for more details).\n[INFO] 21 explicit excludes (use -debug for more details).\n[INFO] 1188 resources included (use -debug for more details)\n[WARNING] Files with unapproved licenses:\n  docker/getting-started/interpreter.json\n[INFO] ------------------------------------------------------------------------\n[INFO] Reactor Summary for Apache Mahout 14.2-SNAPSHOT:\n[INFO]\n[INFO] Apache Mahout ...................................... FAILURE [  6.210 s]\n[INFO] Mahout Core ........................................ SKIPPED\n[INFO] Mahout Engine ...................................... SKIPPED\n[INFO] - Mahout HDFS Support .............................. SKIPPED\n[INFO] - Mahout Spark Engine .............................. SKIPPED\n[INFO] Mahout Distribution ................................ SKIPPED\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time:  6.770 s\n[INFO] Finished at: 2021-03-12T14:04:00+01:00\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal org.apache.rat:apache-rat-plugin:0.13:check (default) on project mahout: Too many files with unapproved license: 1 See RAT report in: /home/jw/apache/mahout/target/rat.txt -> [Help 1]\n[ERROR]\n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR]\n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException\n[jw@cn05 mahout]$",
        "Issue Links": [
            "/jira/browse/MAHOUT-2164"
        ]
    },
    "MAHOUT-2140": {
        "Key": "MAHOUT-2140",
        "Summary": "Upgrade Log4j per CVE-2021-44228",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Grant",
        "Created": "14/Dec/21 00:17",
        "Updated": "16/Feb/23 21:37",
        "Resolved": "16/Feb/23 21:37",
        "Description": "CVE-2021-44228 was a pretty big vulnerability. Since we're still on log4j 1.x the upgrade is non-trivial, but also maybe not at serious for us.",
        "Issue Links": []
    },
    "MAHOUT-2141": {
        "Key": "MAHOUT-2141",
        "Summary": "Discussion and planning epic for adding blockchain data sources and analytics use cases",
        "Type": "Epic",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "07/Jan/22 21:41",
        "Updated": "26/Jan/22 22:35",
        "Resolved": "26/Jan/22 22:35",
        "Description": "About\nDiscussion point for adding ethereum-compatible blockchains as data sources and some pertinent use cases.\nWe will add stories as children to this epic.\nProposal is to use ethereum-compatible ledgers as they adhere to the same standard for tokens (https://ethereum.org/en/developers/docs/standards/tokens), for instance for smart contracts (https://ethereum.org/en/developers/docs/smart-contracts).\nHow to Get Started{}\nTo explore concepts and data, get a copy of go-ethereum (geth: https://geth.ethereum.org/docs/install-and-build/installing-geth). Run it against a network and it will grab historical records. The Goerli test network's entire three years of data is only 32GB, so there are small enough data sets to play with, and the data files are stored on your local disk, by default in ~/.ethereum.\n\u00a0\nThere are libraries that interact live with any given ledger including Web3JS (https://web3js.readthedocs.io/en/v1.5.2/) and Web3.py (https://web3py.readthedocs.io/en/stable/), so reading out of ledgers is simple.\n\u00a0\nReading and indexing the actual data might mean writing custom parsers for Mahout and Lucene, and possibly getting into decompiling bytecode back into readable Solidity code.\n\u00a0\nSome Starter Discussions * Is there a place to persist each ledger we use as a data source, or would this pull down the ledger data every time for a new instance?\n\nShould we build a live demo of this to run on the mahout.a.o web site?\n\n\u00a0\nExample Use Cases\n\nSearch-indexes of given ledgers\nComputed similarity to other accounts on the same ledger based on activity history\nTime-series analysis of gas (transaction) fees across multiple ledgers\nTime-series analysis of transactions (overall # per week/month/year/custom period, by user account etc.) for a list of ledgers. (Comparative analysis of usage)\nMax/Min range of transactions for different ledgers",
        "Issue Links": []
    },
    "MAHOUT-2142": {
        "Key": "MAHOUT-2142",
        "Summary": "Blockchain Data and Analytics",
        "Type": "Epic",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "07/Jan/22 21:47",
        "Updated": "07/Jan/22 21:48",
        "Resolved": null,
        "Description": "About\nDiscussion and planning epic for adding blockchain data sources and analytics use cases. Proposal is to provide a new data source, namely any number of ethereum-compatible ledgers, and pick a few compelling use cases to build out this year.\nWe will add children to this epic for specific work items.\nExample Use Cases\n\nSearch-indexes of given ledgers\nComputed similarity to other accounts on the same ledger based on activity history\nTime-series analysis of gas (transaction) fees across multiple ledgers\nTime-series analysis of transactions (overall # per week/month/year/custom period, by user account etc.) for a list of ledgers. (Comparative analysis of usage)\nMax/Min range of transactions for different ledgers\n\n\u00a0\nHow to Get Started\nTo explore ledger operations and data, get a copy of go-ethereum (geth: https://geth.ethereum.org/docs/install-and-build/installing-geth) and run it against a network to get all historical records. The Goerli test network's entire three years of data is only 32GB, so there are small enough data sets to play with, and the data files are stored on your local disk by default at ~/ethereum.\n\u00a0\nThere are libraries that interact live with any given ledger including Web3JS (https://web3js.readthedocs.io/en/v1.5.2/) and Web3.py (https://web3py.readthedocs.io/en/stable/), so reading out of ledgers is simple.\n\u00a0\nReading and indexing the actual data might mean writing custom parsers for Mahout and Lucene, and possibly getting into decompiling bytecode back into readable Solidity code, so there are pieces we would need to plan out.",
        "Issue Links": []
    },
    "MAHOUT-2143": {
        "Key": "MAHOUT-2142 Blockchain Data and Analytics",
        "Summary": "Research Ethereum Parser",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "26/Jan/22 23:33",
        "Updated": "26/Jan/22 23:33",
        "Resolved": null,
        "Description": "Research and write proof of concept parser for ethereum-compatible networks.",
        "Issue Links": []
    },
    "MAHOUT-2144": {
        "Key": "MAHOUT-2142 Blockchain Data and Analytics",
        "Summary": "Research Ethereum Indexer",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "26/Jan/22 23:34",
        "Updated": "26/Jan/22 23:34",
        "Resolved": null,
        "Description": "Research and write proof of concept search indexer for ethereum-compatible network data files.",
        "Issue Links": []
    },
    "MAHOUT-2145": {
        "Key": "MAHOUT-2142 Blockchain Data and Analytics",
        "Summary": "Research Ethereum Data Storage",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "26/Jan/22 23:38",
        "Updated": "26/Jan/22 23:38",
        "Resolved": null,
        "Description": "Research feasibility and usefulness of:\n\nPulling down an entire chain\nSampling a chain\nParsing and projecting useful fields, discarding unused data\nSelecting date range\nFiltering to just fetch specific types of blocks, such as ERC-20 tokens\nStoring locally or persistent (on S3, e.g.)\nStoring a small example data file in source control with the project",
        "Issue Links": []
    },
    "MAHOUT-2146": {
        "Key": "MAHOUT-2146",
        "Summary": "Web site not publishing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "29/Nov/22 00:28",
        "Updated": "08/Feb/23 23:20",
        "Resolved": "08/Feb/23 23:20",
        "Description": "I updated the markdown for https://mahout.apache.org/general/who-we-are\u00a0 and tried two ways to trigger a website publish:\n(1) Bare commit\n(2) Pull request\nBut the changes aren't showing up on the live site.\nSource change: https://github.com/apache/mahout/commit/a27628c3fadde7a9c57fc85cff01cdd455c5d45b\nSee website/general/who-we-are.md for edited version.",
        "Issue Links": []
    },
    "MAHOUT-2147": {
        "Key": "MAHOUT-2147",
        "Summary": "Add scaladocs for Canopy Clutering Algorithm",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "03/Feb/23 18:33",
        "Updated": "05/Feb/23 09:16",
        "Resolved": "03/Feb/23 19:06",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2148": {
        "Key": "MAHOUT-2148",
        "Summary": "Add scaladocs for distance metrics",
        "Type": "Documentation",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "06/Feb/23 18:30",
        "Updated": "08/Feb/23 23:31",
        "Resolved": "08/Feb/23 23:31",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2149": {
        "Key": "MAHOUT-2149",
        "Summary": "Migrate off Travis",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Jowanza Joseph",
        "Reporter": "Andrew Musselman",
        "Created": "06/Feb/23 19:36",
        "Updated": "20/Mar/23 18:46",
        "Resolved": null,
        "Description": "INFRA would like us to move off Travis and onto GitHub Actions, Jenkins, or Buildbot. Jenkins and Buildbot are in-house.",
        "Issue Links": []
    },
    "MAHOUT-2150": {
        "Key": "MAHOUT-2150",
        "Summary": "Mailing lists link broken on website",
        "Type": "Documentation",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "06/Feb/23 23:32",
        "Updated": "08/Feb/23 23:28",
        "Resolved": "08/Feb/23 23:28",
        "Description": "From https://mahout.apache.org/developers/how-to-contribute\nhttps://mahout.apache.org/general/mailing-lists,-irc-and-archives.html is a 404",
        "Issue Links": []
    },
    "MAHOUT-2151": {
        "Key": "MAHOUT-2151",
        "Summary": "Add scaladocs for AsFactor",
        "Type": "Documentation",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "08/Feb/23 14:53",
        "Updated": "08/Feb/23 23:29",
        "Resolved": "08/Feb/23 23:29",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2152": {
        "Key": "MAHOUT-2152",
        "Summary": "Web site check",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Musselman",
        "Created": "09/Feb/23 19:39",
        "Updated": "29/Mar/23 21:39",
        "Resolved": null,
        "Description": "There are several low-hanging items we could fix on the web site to conform with ASF expectations; see https://whimsy.apache.org/site/project/mahout",
        "Issue Links": [
            "/jira/browse/MAHOUT-2161"
        ]
    },
    "MAHOUT-2153": {
        "Key": "MAHOUT-2153",
        "Summary": "Download page improvements",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "09/Feb/23 19:46",
        "Updated": "22/Feb/23 22:13",
        "Resolved": null,
        "Description": "The Announce (announce@apache.org) list has rejected our release announcements previously due to the following non-compliance issues:\n\u00a0\nSee https://infra.apache.org/release-download-pages.html#download-page for more info on what's required.\n\u00a0\nHi! This is the ezmlm program. I'm managing the\nannounce@apache.org\u00a0mailing list.\nI'm sorry, your message (enclosed) was not accepted by the moderator.\nIf the moderator has made any comments, they are shown below.\n>>>>> -------------------- >>>>>\nThe announce message currently links to\nhttp://www.apache.org/dist/mahout/0.14.0\nHowever, downloads must use the ASF mirrors for all but the KEYS, sigs and\nhashes.\nThe download page\nhttps://mahout.apache.org/general/downloads.html\nhas the same problem.\nIt also links to the git repo, which includes code that has not been\nformally released.\nDownload pages must only link to released code.\n[Pointers to source code repos should be on developer-oriented pages only]\nPlease have a look the download pages for Tomcat or Httpd for examples of\nhow to do it.",
        "Issue Links": []
    },
    "MAHOUT-2154": {
        "Key": "MAHOUT-2154",
        "Summary": "Update NOTICE",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "10/Feb/23 19:43",
        "Updated": "22/Feb/23 22:20",
        "Resolved": null,
        "Description": "Our NOTICE is out of date. I know there are things like viennacl and others that we are making use of that aren't listed.",
        "Issue Links": []
    },
    "MAHOUT-2155": {
        "Key": "MAHOUT-2155",
        "Summary": "Javadocs for RandomWrapper",
        "Type": "Documentation",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "14/Feb/23 21:30",
        "Updated": "22/Feb/23 22:50",
        "Resolved": "22/Feb/23 22:50",
        "Description": null,
        "Issue Links": []
    },
    "MAHOUT-2156": {
        "Key": "MAHOUT-2156",
        "Summary": "Add \"how to do your first JIRA\" notes",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Musselman",
        "Created": "22/Feb/23 22:21",
        "Updated": "22/Feb/23 22:51",
        "Resolved": null,
        "Description": "Add some language for new contributors in https://mahout.apache.org/developers/how-to-contribute about how to grab a ticket and identify manageable first tasks.",
        "Issue Links": []
    },
    "MAHOUT-2157": {
        "Key": "MAHOUT-2157",
        "Summary": "Add \"first ticket\" tag or type in JIRA",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "jira",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "22/Feb/23 22:23",
        "Updated": "22/Feb/23 22:50",
        "Resolved": null,
        "Description": "Find or create \"good first ticket\" type for issues in JIRA",
        "Issue Links": []
    },
    "MAHOUT-2158": {
        "Key": "MAHOUT-2158",
        "Summary": "Edit website",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Musselman",
        "Created": "22/Feb/23 22:24",
        "Updated": "29/Mar/23 21:49",
        "Resolved": "29/Mar/23 21:49",
        "Description": "Do a scrub on pages and language on the website, remove unneeded material and consolidate where viable.",
        "Issue Links": []
    },
    "MAHOUT-2159": {
        "Key": "MAHOUT-2159",
        "Summary": "Add Docker instructions to nav",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Andrew Musselman",
        "Reporter": "Andrew Musselman",
        "Created": "22/Feb/23 22:36",
        "Updated": "22/Feb/23 23:03",
        "Resolved": null,
        "Description": "Add reference to https://mahout.apache.org/docs/latest/tutorials/misc/getting-started-with-zeppelin/ for people to get started.",
        "Issue Links": []
    },
    "MAHOUT-2160": {
        "Key": "MAHOUT-2160",
        "Summary": "Add link to http://www.apache.org/ on website footer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Trevor Grant",
        "Reporter": "Trevor Grant",
        "Created": "23/Feb/23 19:12",
        "Updated": "23/Feb/23 21:46",
        "Resolved": "23/Feb/23 21:46",
        "Description": "related to #2152",
        "Issue Links": []
    },
    "MAHOUT-2161": {
        "Key": "MAHOUT-2161",
        "Summary": "Fix broken ASF link in Sidebar",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Musselman",
        "Created": "23/Feb/23 21:47",
        "Updated": "29/Mar/23 21:38",
        "Resolved": "29/Mar/23 21:38",
        "Description": "Broken link to ASF",
        "Issue Links": [
            "/jira/browse/MAHOUT-2152"
        ]
    },
    "MAHOUT-2162": {
        "Key": "MAHOUT-2162",
        "Summary": "Dependabot security PR",
        "Type": "Dependency",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "27/Feb/23 17:19",
        "Updated": "27/Feb/23 17:19",
        "Resolved": null,
        "Description": "https://github.com/apache/mahout/pull/417\nNeed to assess how important this is, decide to hard deprecate Hadoop MapReduce-based jobs, etc.",
        "Issue Links": [
            "/jira/browse/MAHOUT-2163"
        ]
    },
    "MAHOUT-2163": {
        "Key": "MAHOUT-2163",
        "Summary": "Dependabot security PR #2",
        "Type": "Dependency",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "27/Feb/23 17:19",
        "Updated": "27/Feb/23 17:20",
        "Resolved": null,
        "Description": "https://github.com/apache/mahout/pull/418\nNeed to assess how important this is, decide to hard deprecate Hadoop MapReduce-based jobs, etc.",
        "Issue Links": [
            "/jira/browse/MAHOUT-2162"
        ]
    },
    "MAHOUT-2164": {
        "Key": "MAHOUT-2164",
        "Summary": "Fix license issues for RAT",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Musselman",
        "Created": "27/Feb/23 22:58",
        "Updated": "27/Feb/23 22:59",
        "Resolved": null,
        "Description": "Revisit license headers in json dir for rat errors per:\nhttps://github.com/apache/mahout/pull/427",
        "Issue Links": [
            "/jira/browse/MAHOUT-2139"
        ]
    },
    "MAHOUT-2165": {
        "Key": "MAHOUT-2165",
        "Summary": "Docker image for web site build",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Trevor Grant",
        "Reporter": "Andrew Musselman",
        "Created": "29/Mar/23 21:46",
        "Updated": "29/Mar/23 21:46",
        "Resolved": null,
        "Description": "Create a Docker image for updating web site",
        "Issue Links": []
    }
}