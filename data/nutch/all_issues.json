{
    "NUTCH-1": {
        "Key": "NUTCH-1",
        "Summary": "test",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Doug Cutting",
        "Created": "11/Feb/05 04:23",
        "Updated": "29/Mar/05 03:50",
        "Resolved": "29/Mar/05 03:50",
        "Description": "This is a test bug.",
        "Issue Links": []
    },
    "NUTCH-2": {
        "Key": "NUTCH-2",
        "Summary": "UpdateDatabaseTool ignores url-filters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "21/Feb/05 07:05",
        "Updated": "24/May/05 04:14",
        "Resolved": "24/May/05 04:14",
        "Description": "UpdateDatabaseTool ignores url-filters\nUnder some constraints the updatedatase-tool does not\ncheck the url-filters. So the webdb can grow with\nunwanted urls which can get part of the next fetchlist. \nThe patch below changes the process so that before\nanything else the filters are checked. the unwanted\nurls do not get part of the webdb anymore.",
        "Issue Links": []
    },
    "NUTCH-3": {
        "Key": "NUTCH-3",
        "Summary": "multi values of header discarded",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Stefan Groschupf",
        "Reporter": "Stefan Groschupf",
        "Created": "21/Feb/05 07:08",
        "Updated": "13/Mar/06 20:34",
        "Resolved": "13/Mar/06 20:34",
        "Description": "orignal by: phoebe\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1111185&group_id=59548&atid=491356\nmulti values of header discarded\nEach successive setting of a header value deletes the\nprevious one.\nThis patch allows multi values to be retained, such as\ncookies, using lf cr as a delimiter for each values.\n\u2014 /tmp/HttpResponse.java 2005-01-27\n19:57:55.000000000 -0500\n+++ HttpResponse.java 2005-01-27 20:45:01.000000000 -0500\n@@ -324,7 +324,19 @@\n}\nString value = line.substring(valueStart);\n\nheaders.put(key, value);\n+//Spec allows multiple values, such as Set-Cookie -\nusing lf cr as delimiter\n+ if ( headers.containsKey(key)) {\n+ try \nUnknown macro: {+ Object obj= headers.get(key);+ if ( obj != null) {\n+ String oldvalue=\nheaders.get(key).toString();\n+ value = oldvalue +\n\"\\r\\n\" + value;\n+ }+ } \n catch (Exception e) \n{\n+ e.printStackTrace();\n+ }\n+ }\n+ headers.put(key, value);\n}\n\nprivate Map parseHeaders(PushbackInputStream in,\nStringBuffer line)\n@@ -399,5 +411,3 @@\n}",
        "Issue Links": []
    },
    "NUTCH-4": {
        "Key": "NUTCH-4",
        "Summary": "Serious bug: OutOfMemoryError: Java heap space",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Stefan Groschupf",
        "Created": "21/Feb/05 07:10",
        "Updated": "23/Apr/05 02:47",
        "Resolved": "08/Apr/05 05:35",
        "Description": "posted by: msashnikov\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1110947&group_id=59548&atid=491356\nSerious bug: OutOfMemoryError: Java heap space\nNutch 0.6 throws the following exception when the \nsearch phrase includes just a single quote. Something \nlike \"java or ja\"va.\nHere is the exception:\njavax.servlet.ServletException: Java heap space\norg.apache.jasper.runtime.PageContextImpl.doH\nandlePageException(PageContextImpl.java:845)\norg.apache.jasper.runtime.PageContextImpl.han\ndlePageException(PageContextImpl.java:778)\norg.apache.jsp.search_jsp._jspService\n(org.apache.jsp.search_jsp:685)\norg.apache.jasper.runtime.HttpJspBase.service\n(HttpJspBase.java:99)\njavax.servlet.http.HttpServlet.service\n(HttpServlet.java:802)\norg.apache.jasper.servlet.JspServletWrapper.se\nrvice(JspServletWrapper.java:325)\norg.apache.jasper.servlet.JspServlet.serviceJsp\nFile(JspServlet.java:295)\norg.apache.jasper.servlet.JspServlet.service\n(JspServlet.java:245)\njavax.servlet.http.HttpServlet.service\n(HttpServlet.java:802)\nroot cause \njava.lang.OutOfMemoryError: Java heap space",
        "Issue Links": []
    },
    "NUTCH-5": {
        "Key": "NUTCH-5",
        "Summary": "Hit limiter off-by-one bug",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andy Liu",
        "Created": "08/Mar/05 08:39",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "14/Apr/05 02:21",
        "Description": "When re-searching for more raw hits, the first result of the next site is skipped.\nFrom NutchBean.java\nsnip\n// get the next raw hit\n            if (rawHitNum >= hits.getLength()) {\n                // optimize query by prohibiting more matches on some excluded sites\n                Query optQuery = (Query) query.clone();\n                for (int i = 0; i < excludedSites.size(); i++) {\n                    if (i == MAX_PROHIBITED_TERMS) \n{\n                        break;\n                    }\n\n                    optQuery.addProhibitedTerm(((String) excludedSites.get),\n                        IndexSearcher.HIT_LIMIT_FIELD);\n                }\n                numHitsRaw = (int) (numHitsRaw * RAW_HITS_FACTOR);\n                LOG.info(\"re-searching for \" + numHitsRaw +\n                    \" raw hits, query: \" + optQuery);\n                hits = searcher.search(optQuery, numHitsRaw);\n                LOG.info(\"found \" + hits.getTotal() + \" raw hits\");\n                rawHitNum = 0;\n                continue;\n            }\nsnip\nrawHitNum is reset to 0, but the for loop increments it by one and skips the next result.",
        "Issue Links": []
    },
    "NUTCH-6": {
        "Key": "NUTCH-6",
        "Summary": "nutch illustration",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "11/Mar/05 07:14",
        "Updated": "29/Mar/05 03:41",
        "Resolved": "29/Mar/05 03:41",
        "Description": "just the graphic",
        "Issue Links": []
    },
    "NUTCH-7": {
        "Key": "NUTCH-7",
        "Summary": "analyze tool takes up all the disk space when there are circular links",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Phoebe Miller",
        "Created": "11/Mar/05 12:52",
        "Updated": "09/Aug/05 05:00",
        "Resolved": "09/Aug/05 05:00",
        "Description": "It is repeatable by running an instance with these seeds:\nhttp://www.acf.hhs.gov/programs/ofs/forms.htm/grants/grants/grants/grants/data/grants/data/data/data/data/grants/data/grants/grants/grants/process.htm\nhttp://www.acf.hhs.gov/programs/ofs/\nand limit it (for best effect) to just:\n.acf.hhs.gov/\nLet it go for about 12 cycles to build it up and the temp file size roughly doubles with each segment.\n]$ ls -l /db/tmpdir2344la/\n...\n1503641425 Mar 10 17:42 scoreEdits.0.unsorted\nfor a very small db:\nStats for net.nutch.db.WebDBReader@89cf1e\n-------------------------------\nNumber of pages: 6916\nNumber of links: 8085\nscoreEdits.0.sorted.0 contains rows of links that looked like the first seed url, but with more grants/ and data/ in the sub dirs.\nIn the File:\n.DistributedAnalysisTool.java\n 345                     if (curIndex - startIndex > extent) \n{\n 346                         break;\n 347                     }\nis the hard stop.\nFurther down the score is written:\n381  for (int i = 0; i < outLinks.length; i++) {\n...\n385     scoreWriter.append(outLinks[i].getURL(), score);\nPutting a check here stops the tmpdir.../scoreEdits.0 file growth\nbut the links themselves should not be produced in the generation either.",
        "Issue Links": []
    },
    "NUTCH-8": {
        "Key": "NUTCH-8",
        "Summary": "fetcher.retry.max no longer in use",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kelvin Tan",
        "Created": "12/Mar/05 07:27",
        "Updated": "29/Mar/05 03:48",
        "Resolved": "15/Mar/05 04:49",
        "Description": "In current Subversion code, the configuration entry fetcher.retry.max is no longer being used. It should either be removed from nutch-default.xml, or the functionality (re-)implemented in Fetcher.",
        "Issue Links": []
    },
    "NUTCH-9": {
        "Key": "NUTCH-9",
        "Summary": "JSP pages do not compile",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Piotr Kosiorowski",
        "Created": "13/Mar/05 01:19",
        "Updated": "19/Mar/05 05:54",
        "Resolved": "19/Mar/05 05:54",
        "Description": "In latest SVN version (but it was introduced in CVS I think) there is a simple error in two JSP pages.\nrefine-query-init.jsp :\nString urls = org.apache.nutch.util.NutchConf.get(\"extension.ontology.urls\");\nshould be:\nString urls = org.apache.nutch.util.NutchConf.get().get(\"extension.ontology.urls\");\nAnd in search.jsp:\n NutchConf.get().getInt(\"extension.clustering.hits-to-cluster\", 100);\nshould be:\n NutchConf.get().getInt(\"extension.clustering.hits-to-cluster\", 100);",
        "Issue Links": []
    },
    "NUTCH-10": {
        "Key": "NUTCH-10",
        "Summary": "extension points are defined multiple times",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "17/Mar/05 04:02",
        "Updated": "20/Aug/05 00:58",
        "Resolved": "20/Aug/05 00:58",
        "Description": "An extension point should be only defined one time but actually extension points are defined multiple times in different plugin.xml",
        "Issue Links": []
    },
    "NUTCH-11": {
        "Key": "NUTCH-11",
        "Summary": "Link.java needs a <pre> tag so javadoc renders",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Spencer",
        "Created": "18/Mar/05 02:20",
        "Updated": "05/Apr/05 03:06",
        "Resolved": "05/Apr/05 03:05",
        "Description": "Trivial issue, javadoc for net.nutch.db.Link doesn't render properly due to a lack of a <pre>..</pre> at the top.\nPlease change this:\n/*********************************************\n\nThis is the field in the Link Database.\nEach row is a Link:\ntype   name    description\n---------------------------------------------------------------\nbyte   VERSION - A byte indicating the version of this entry.\n128bit FROM_ID - The MD5 hash of the source of the link.\n64bit  DOMAIN_ID - The 8-byte MD5Hash of the source's domain.\nstring TO_URL  - The URL destination of the link.\nstring ANCHOR  - The anchor text of the link.\nboolean TARGET_HAS_OUTLINK   - Whether the target of the link has outlinks.\n *\n@author Mike Cafarella\n *************************************************/\n\nTo this:\n/*********************************************\n\nThis is the field in the Link Database.\n<pre>\nEach row is a Link:\ntype   name    description\n---------------------------------------------------------------\nbyte   VERSION - A byte indicating the version of this entry.\n128bit FROM_ID - The MD5 hash of the source of the link.\n64bit  DOMAIN_ID - The 8-byte MD5Hash of the source's domain.\nstring TO_URL  - The URL destination of the link.\nstring ANCHOR  - The anchor text of the link.\nboolean TARGET_HAS_OUTLINK   - Whether the target of the link has outlinks.\n</pre>\n *\n@author Mike Cafarella\n *************************************************/",
        "Issue Links": []
    },
    "NUTCH-12": {
        "Key": "NUTCH-12",
        "Summary": "WebDBReader options to print incoming links",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Spencer",
        "Created": "19/Mar/05 04:03",
        "Updated": "22/Jan/08 14:09",
        "Resolved": "22/Jan/08 14:09",
        "Description": "It seems that the options to WeDBReader.main() don't show incoming links, so I added code to do this consistently.\nI added 2 new options:\n[1]    -linktourl URL\nPrints out the links to one URL.\n[2]    -dumplinksto\nLike -dumplinks but instead prints out incoming links.\nWhat follows is output from \"svn diff -N\" which I believe is how things are supposed to be submitted.\ndave@smo-eng-10 /cygdrive/f/proj/java/nutch/latest/nutch/trunk/src/java/org/apache/nutch/db\n$ svn diff -N WebDBReader.java\nIndex: WebDBReader.java\n===================================================================\n\u2014 WebDBReader.java    (revision 158116)\n+++ WebDBReader.java    (working copy)\n@@ -409,7 +409,7 @@\n      */\n     public static void main(String argv[]) throws FileNotFoundException, IOException {\n         if (argv.length < 2) \n{\n-            System.out.println(\"Usage: java org.apache.nutch.db.WebDBReader (-local | -ndfs <namenode:port>) <db> [-pageurl url] | [-pagemd5 md5] | [-dumppageu\nrl] | [-dumppagemd5] | [-toppages <k>] | [-linkurl url] | [-linkmd5 md5] | [-dumplinks] | [-stats]\");\n+            System.out.println(\"Usage: java org.apache.nutch.db.WebDBReader (-local | -ndfs <namenode:port>) <db> [-pageurl url] | [-pagemd5 md5] | [-dumppageu\nrl] | [-dumppagemd5] | [-toppages <k>] | [-linkurl url] | [-linktourl url] [-linkmd5 md5] | [-dumplinks] | [-dumplinksto] | [-stats]\");\n             return;\n\n         }\n@@ -521,6 +521,35 @@\n                     System.out.println();\n                   }\n                 }\n+            } else if (\"-linktourl\".equals(cmd)) {\n+                String url = argv[i++];\n+                Link links[] = reader.getLinks( new UTF8( url.trim()));\n+                System.out.println(\"Found \" + links.length + \" incoming links.\");\n+                for ( int j = 0; j < links.length; j++) {\n+                    MD5Hash from = links[ j].getFromID();\n+                    Page[] ps = reader.getPages( from);\n+                    for( int k = 0; k < ps.length; k++) \n{\n+                        System.out.println( \" from \" + ps[ k].getURL().toString());\n+                    }\n+                }\n+            } else if (\"-dumplinksto\".equals(cmd)) {\n+                System.out.println(reader);\n+                System.out.println();\n+                Enumeration e = reader.pagesByMD5();\n+                while (e.hasMoreElements()) {\n+                  Page page = (Page) e.nextElement();\n+                  Link[] links = reader.getLinks( page.getURL());\n+                  if ( links.length > 0) {\n+                      System.out.println( \"These pages link to \" + page.getURL());\n+                      for ( int j = 0; j < links.length; j++) {\n+                          MD5Hash from = links[ j].getFromID();\n+                          Page[] ps = reader.getPages( from);\n+                          for( int k = 0; k < ps.length; k++) \n{\n+                              System.out.println( \" from \" + ps[ k].getURL().toString());\n+                          }\n+                      }\n+                  }\n+                }\n             } else if (\"-stats\".equals(cmd)) {\n                 System.out.println(\"Stats for \" + reader);\n                 System.out.println(\"-------------------------------\");",
        "Issue Links": []
    },
    "NUTCH-13": {
        "Key": "NUTCH-13",
        "Summary": "If dns points to 127.0.0.1, the url is also crawled",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Matthias Jaekle",
        "Created": "19/Mar/05 06:44",
        "Updated": "01/Apr/11 14:27",
        "Resolved": "01/Apr/11 14:27",
        "Description": "For example www.tik24.de points to 127.0.0.1.\nIf you follow a link to www.tik24.de fetcher will crawl content from your own machine.\nWrong DNS entries could create unwanted entries in segments.",
        "Issue Links": []
    },
    "NUTCH-14": {
        "Key": "NUTCH-14",
        "Summary": "NullPointerException NutchBean.getSummary",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "20/Mar/05 03:21",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "26/Mar/06 00:34",
        "Description": "In heavy load scenarios this may happens when connection broke.\njava.lang.NullPointerException\n        at java.util.Hashtable.get(Hashtable.java:333)\n        at net.nutch.ipc.Client.getConnection(Client.java:276)\n        at net.nutch.ipc.Client.call(Client.java:251)\n        at net.nutch.searcher.DistributedSearch$Client.getSummary(DistributedSearch.java:418)\n        at net.nutch.searcher.NutchBean.getSummary(NutchBean.java:236)\n        at org.apache.jsp.search_jsp._jspService(org.apache.jsp.search_jsp:396)\n        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:99)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:325)\n        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:295)\n        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:245)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:214)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:825)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.processConnection(Http11Protocol.java:738)\n        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:526)\n        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)\n        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)\n        at java.lang.Thread.run(Thread.java:552)",
        "Issue Links": []
    },
    "NUTCH-15": {
        "Key": "NUTCH-15",
        "Summary": "ipc client timeout should be configurable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "20/Mar/05 23:20",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "05/Apr/05 03:44",
        "Description": "The timeout of the ipc client is hard coded to 10 seconds. \nUnder heavy load it is common that for example that search servers crashs, since they are just overloaded.\nIn  case just one search server crahs  every query took 10 seconds since in any case we wait for the crashed server.\nTo make the timeout configurable would be a fist step, however we may can discuss implement a client / server similar to the dfs where the client is announcing itself to the server and the server add dynamically new clients or remove clients that fails for a given time period.\nThis would be more confortable for production secarios since crashes are more oftern under heavy load and in case more server are involved.",
        "Issue Links": []
    },
    "NUTCH-16": {
        "Key": "NUTCH-16",
        "Summary": "boost documents matching a url pattern",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Stefan Groschupf",
        "Created": "23/Mar/05 07:14",
        "Updated": "16/Jul/11 17:53",
        "Resolved": "16/Jul/11 17:53",
        "Description": "The attached patch is a plugin that allows to boost documents matching a url pattern. \nThis could be useful to rank documents from a intranet higher then external pages.\nA README comes with the patch.\nAny comments are welcome.",
        "Issue Links": []
    },
    "NUTCH-17": {
        "Key": "NUTCH-17",
        "Summary": "NekoHTML's DOMFragmentParser hangs on certain URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Kelvin Tan",
        "Created": "26/Mar/05 19:55",
        "Updated": "01/Jan/07 21:26",
        "Resolved": "29/Jun/05 05:19",
        "Description": "I've tracked down occasional fetcher hangs to NekoHTML's DOMFragmentParser hanging certain HTML documents, for example, http://www.inlandrevenue.gov.uk/charities/chapter_3.htm.\nThe thread dump on the hung parser is:\n\"CompilerThread0\" daemon prio=1 tid=0x080c4c18 nid=0x47da waiting on condition [0x00000000..0x8a3daf68]\n\"Signal Dispatcher\" daemon prio=1 tid=0x080c3d60 nid=0x47d9 waiting on condition [0x00000000..0x00000000]\n\"Finalizer\" daemon prio=1 tid=0x080b8818 nid=0x47d8 in Object.wait() [0x8a2a0000..0x8a2a0680]\n        at java.lang.Object.wait(Native Method)\n\nwaiting on <0x4a60d058> (a java.lang.ref.ReferenceQueue$Lock)\n        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116)\nlocked <0x4a60d058> (a java.lang.ref.ReferenceQueue$Lock)\n        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132)\n        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)\n\n\"Reference Handler\" daemon prio=1 tid=0x080b7b50 nid=0x47d7 in Object.wait() [0x8a21f000..0x8a21f800]\n        at java.lang.Object.wait(Native Method)\n\nwaiting on <0x4a60d0d8> (a java.lang.ref.Reference$Lock)\n        at java.lang.Object.wait(Object.java:474)\n        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)\nlocked <0x4a60d0d8> (a java.lang.ref.Reference$Lock)\n\n\"main\" prio=1 tid=0x0805c170 nid=0x47d1 waiting on condition [0xbfffc000..0xbfffcec8]\n        at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:99)\n        at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:393)\n        at java.lang.StringBuffer.append(StringBuffer.java:225)\n\nlocked <0x45910118> (a java.lang.StringBuffer)\n        at org.apache.xerces.dom.CharacterDataImpl.appendData(Unknown Source)\n        at org.cyberneko.html.parsers.DOMFragmentParser.characters(Unknown Source)\n        at org.cyberneko.html.filters.DefaultFilter.characters(Unknown Source)\n        at org.cyberneko.html.HTMLTagBalancer.characters(Unknown Source)\n        at org.cyberneko.html.HTMLScanner$ContentScanner.scanCharacters(Unknown Source)\n        at org.cyberneko.html.HTMLScanner$ContentScanner.scan(Unknown Source)\n        at org.cyberneko.html.HTMLScanner.scanDocument(Unknown Source)\n        at org.cyberneko.html.HTMLConfiguration.parse(Unknown Source)\n        at org.cyberneko.html.HTMLConfiguration.parse(Unknown Source)\n        at org.cyberneko.html.parsers.DOMFragmentParser.parse(Unknown Source)\n        at net.nutch.parse.html.HtmlParser.getParse(HtmlParser.java:157)\n        at net.nutch.parse.ParserChecker.main(ParserChecker.java:74)\n\n\"VM Thread\" prio=1 tid=0x080b4f30 nid=0x47d6 runnable\n\"VM Periodic Task Thread\" prio=1 tid=0x080c75f8 nid=0x47dc waiting on condition\nUsing the URL mentioned above, I was able to successfully parse the file using a normal NekoHTML DocumentParser.",
        "Issue Links": [
            "/jira/browse/NUTCH-424"
        ]
    },
    "NUTCH-18": {
        "Key": "NUTCH-18",
        "Summary": "Windows servers include illegal characters in URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:26",
        "Updated": "01/Apr/11 14:29",
        "Resolved": "01/Apr/11 14:25",
        "Description": "Transfered from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1110243&group_id=59548&atid=491356\nsubmitted by:\nKen Meltsner\nWhile spidering our intranet, I found that IIS may include \nillegal characters in URLs \u2013 specifically, characters with \nthe high bit set to produce non-English letters. In \naddition, both Firefox and IE will accept URLs with high-\nbit characters, but Java won't.\nWhile this may not be Nutch's (or Java's) fault, it would \nhelp if high-bit characters (and other illegal characters) \nin URLs could be escaped (using percent-hex notation) \nas part of the URL fix-up process, probably right after \nthe hostname lower-case conversion.\nExample document name in Portuguese(with high-bit \ncharacters) taken from a longer URL:\nNota%20tecnica%20-%20Altera\u00e7\u00e3o%20de%\n20escopo.doc\nand with percent-escaped characters:\nNota%20tecnica%20-%20Altera%e7%e3o%20de%\n20escopo.doc",
        "Issue Links": []
    },
    "NUTCH-19": {
        "Key": "NUTCH-19",
        "Summary": "Space in Java.exe path chokes bin/nutch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:29",
        "Updated": "18/Apr/05 04:25",
        "Resolved": "18/Apr/05 04:25",
        "Description": "Transfered from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1110223&group_id=59548&atid=491356\nsubmitted by:\nKen Meltsner\nexecutable cause problems for the nutch script.\nThis is likely to happen on Windows since the new JDK (v \n1.5) installs at c:\\Program Files\\Java\\jdk1.5.0 by default.\nSuggest fix is to wrap the first argument of the final \nexec in the script with double quotes:\nexec \"$JAVA\" $JAVA_HEAP_MAX $NUTCH_OPTS -\nclasspath \"$CLASSPATH\" $CLASS \"$@\"\nKen",
        "Issue Links": []
    },
    "NUTCH-20": {
        "Key": "NUTCH-20",
        "Summary": "Extract urls from plain texts",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:35",
        "Updated": "20/Aug/05 06:21",
        "Resolved": "20/Aug/05 06:22",
        "Description": "Some parsers have no Outlinks returned. E.g. the Word-Parser.\nThis class is able to extract (absolute) hyperlinks from a plain String (content)  and generates outlinks from them.\nThis would be very usful for parser which have no explicite extraction of hyperlinks.\nExcample:\nOutlink[] links = OutlinkExtractor.getOutlinks(\"Nutch is located at http://www.apache.org and ...\");\nWill return an array of Outlinks containing the one element of \"http://www.apache.org\".\n\ntransfered from: http://sourceforge.net/tracker/index.php?func=detail&aid=1109328&group_id=59548&atid=491356\nsubmitted  by: Stephan Strittmatter",
        "Issue Links": []
    },
    "NUTCH-21": {
        "Key": "NUTCH-21",
        "Summary": "parser plugin for MS PowerPoint slides",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:39",
        "Updated": "03/Sep/05 00:59",
        "Resolved": "03/Sep/05 00:59",
        "Description": "transfered from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1109321&group_id=59548&atid=491356\nsubmitted by:\nStephan Strittmatter",
        "Issue Links": []
    },
    "NUTCH-22": {
        "Key": "NUTCH-22",
        "Summary": "ontology supported query refinement",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:45",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "18/Apr/05 04:17",
        "Description": "transfered from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1042000&group_id=59548&atid=491356\nsubmitted by; \nmichael j pan",
        "Issue Links": []
    },
    "NUTCH-23": {
        "Key": "NUTCH-23",
        "Summary": "content text/xml parser",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:47",
        "Updated": "25/Mar/06 22:48",
        "Resolved": "25/Mar/06 22:47",
        "Description": "transfered from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1041997&group_id=59548&atid=491356\nsubmitted by:\nmichael j pan",
        "Issue Links": []
    },
    "NUTCH-24": {
        "Key": "NUTCH-24",
        "Summary": "Cannot handle incorrectly cased Content-Type",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:51",
        "Updated": "24/Mar/06 13:57",
        "Resolved": "24/Mar/06 13:57",
        "Description": "transfered from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=1014459&group_id=59548&atid=491356\nsubmitted by:\nboconnor\nNot really a bug but in many cases web servers give the \nincorrect header of \"Content-type\" instead of \"Content-\nType\" - notice the lowercase \"t\".\nThis is particullary true of IIS. This results in the inability \nto resolve the content type (or in fact resolving it to \nnothing) resulting in the document not getting indexed.",
        "Issue Links": []
    },
    "NUTCH-25": {
        "Key": "NUTCH-25",
        "Summary": "needs 'character encoding' detector",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "26/Sep/07 14:05",
        "Description": "transferred from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=995730&group_id=59548&atid=491356\nsubmitted by:\nJungshik Shin\nthis is a follow-up to bug 993380 (figure out 'charset'\nfrom the meta tag).\nAlthough we can cover a lot of ground using the 'C-T'\nheader field in in the HTTP header and the\ncorresponding meta tag in html documents (and in case\nof XML, we have to use a similar but a different\n'parsing'), in the wild, there are a lot of documents\nwithout any information about the character encoding\nused. Browsers like Mozilla and search engines like\nGoogle use character encoding detectors to deal with\nthese 'unlabelled' documents. \nMozilla's character encoding detector is GPL/MPL'd and\nwe might be able to port it to Java. Unfortunately,\nit's not fool-proof. However, along with some other\nheuristic used by Mozilla and elsewhere, it'll be\npossible to achieve a high rate of the detection. \nThe following page has links to some other related pages.\nhttp://trainedmonkey.com/week/2004/26\nIn addition to the character encoding detection, we\nalso need to detect the language of a document, which\nis even harder and should be a separate bug (although\nit's related).",
        "Issue Links": []
    },
    "NUTCH-26": {
        "Key": "NUTCH-26",
        "Summary": "New Http Authentication mechanism",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:55",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "transferred from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=990560&group_id=59548&atid=491356\nsubmitted by:\nMatt\nHere's a patch and lib (commons-codec used for Base64 \nencoding) which implements hasic http authentication. \nI've attempted to build it so we can add more \nauthentication methods at a later time.\nThis also includes the previously discussed \nMultiProperties class which allows multiple headers with \nthe same name (as opposed to Properties which allows \nonly a single).\nI believe both John and Doug have had some comments \non this.\nMatt",
        "Issue Links": []
    },
    "NUTCH-27": {
        "Key": "NUTCH-27",
        "Summary": "Patch to get a status of running Fetcher",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:57",
        "Updated": "03/Jul/05 04:45",
        "Resolved": "03/Jul/05 04:45",
        "Description": "transferred from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=987298&group_id=59548&atid=491356\nsubmitted by:\nAndrzej Bialecki \nWhen running a Fetcher it helps to know how many pages\nhave been processed so far. Currently this information\nis printed to a log, but it would be useful to be able\nto get it programmatically whenever the managing\napplication needs it.\nThe attached patch provides this functionality.",
        "Issue Links": []
    },
    "NUTCH-28": {
        "Key": "NUTCH-28",
        "Summary": "No support for https",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Mar/05 23:59",
        "Updated": "29/Jun/05 05:16",
        "Resolved": "29/Jun/05 05:16",
        "Description": "transferred from:\nhttp://sourceforge.net/tracker/index.php?func=detail&aid=986240&group_id=59548&atid=491356\nsubmitted by:\nKonstantin Ignatyev\nCrawl tool does not support https protocol.\nI have created very simple one based on\ncommons-httpclient and attached it to the report. It\nseems working although required commons-httpclient.jar\nand commons-logging.jar in lib directory.",
        "Issue Links": []
    },
    "NUTCH-29": {
        "Key": "NUTCH-29",
        "Summary": "PluginManifestParser export lib typo",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "27/Mar/05 06:46",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "29/Mar/05 04:43",
        "Description": "There is a typo in the PluginManifestParser class when it tries to read the export sub-element tag of the library element within the plugin.xml file. The typo makes the xml parser search for \"extport\" instead of \"export\" and thus it is unable to load exported libraries into the proper list within the PluginDescriptor class.",
        "Issue Links": []
    },
    "NUTCH-30": {
        "Key": "NUTCH-30",
        "Summary": "rss feed parser",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "29/Mar/05 02:56",
        "Updated": "13/Aug/05 00:20",
        "Resolved": "13/Aug/05 00:20",
        "Description": "A simple rss feed parser supporting:\nrss and atom:\n+ version 0.3\n+  version 09\n+ version 10\n+ version 20\nConverting of different rss versions  is done via xslt. \nThe xslt was contributed by Frank Henze - Thanks!",
        "Issue Links": []
    },
    "NUTCH-31": {
        "Key": "NUTCH-31",
        "Summary": "Where is the nutch page rank arithmetic?",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "zhangjin",
        "Created": "30/Mar/05 13:27",
        "Updated": "30/Mar/05 19:45",
        "Resolved": "30/Mar/05 19:45",
        "Description": "Recently I  study the nutch,but  I don't know Where the nutch  page\narithmetic is! I hope  you can tell me which java file it is.Thanks\nvery much!\nMy msn is prettysino@hotmail.com!",
        "Issue Links": []
    },
    "NUTCH-32": {
        "Key": "NUTCH-32",
        "Summary": "Nutch Webapp could only be deployed on root namespace",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Jerome Charron",
        "Created": "30/Mar/05 20:38",
        "Updated": "27/Sep/06 00:45",
        "Resolved": "05/Apr/05 07:29",
        "Description": "Some links in the JSP pages are using absolute paths instead of relative ones.",
        "Issue Links": [
            "/jira/browse/NUTCH-43"
        ]
    },
    "NUTCH-33": {
        "Key": "NUTCH-33",
        "Summary": "MIME content type detector (using magic char sequences)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "John Xing",
        "Reporter": "Jerome Charron",
        "Created": "30/Mar/05 22:11",
        "Updated": "19/Apr/05 02:08",
        "Resolved": "18/Apr/05 13:13",
        "Description": "Extension based content-type detector is not suffisant in some cases.\nThe solution is to add a content type detector based on some magic char sequences like in apache httpd for instance.\n(Note: I created this issue only to keep a trace, but I'm currently working on it)",
        "Issue Links": []
    },
    "NUTCH-34": {
        "Key": "NUTCH-34",
        "Summary": "Parsing different content formats",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7.2,                                            0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stephan Strittmatter",
        "Created": "01/Apr/05 18:30",
        "Updated": "24/Mar/06 13:54",
        "Resolved": "24/Mar/06 13:53",
        "Description": "At the moment Nuch is set up to filter content by config the xml-config file.\nThere it is also set global how many bytes are loaded.\nI think it yould be better to let the parser plugins \"register\" themselfe in some registry where every plugin could tell the fetcher, that:\n1. this document type is wanted (because the parser plugin is \n   installed and activated)\n2. how much of the content is required (some plugins need the whole \n   content and some not)",
        "Issue Links": []
    },
    "NUTCH-35": {
        "Key": "NUTCH-35",
        "Summary": "modify XML parsing code in Nutch to use single API",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Stefan Groschupf",
        "Reporter": "Chris A. Mattmann",
        "Created": "02/Apr/05 03:07",
        "Updated": "15/Apr/05 02:47",
        "Resolved": "15/Apr/05 02:47",
        "Description": "Nutch uses more than a single XML reading API to parse xml configuration files and perform other such XML operations. The goal of this particular reported issue would be to modfiy the nutch xml parsing functionality to use a single XML reading and writing API. As suggested by Doug, it would be nice if the XML reading/writing could be done through the native Java classes provided with the JDK. I can get this done by the end of the month if its assigned to me.\nThanks!\n--Chris",
        "Issue Links": []
    },
    "NUTCH-36": {
        "Key": "NUTCH-36",
        "Summary": "Chinese in Nutch",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Jack Tang",
        "Created": "05/Apr/05 11:14",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:26",
        "Description": "Nutch now support Chinese in very simple way: NutchAnalysis segments CJK term word-by-word. \nSo, if I search Chinese term 'FooBar'(two Chinese words: 'Foo' and 'Bar'), the result in web gui will highlight 'FooBar' and 'Foo', 'Bar'. While we expect Nutch only highlights 'FooBar'.",
        "Issue Links": []
    },
    "NUTCH-37": {
        "Key": "NUTCH-37",
        "Summary": "Javadoc Warnings",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Jerome Charron",
        "Created": "05/Apr/05 20:10",
        "Updated": "27/Aug/05 06:05",
        "Resolved": "27/Aug/05 06:05",
        "Description": "There's a lot of Javadoc Warnings when building trunk. Such as the traditionals:\n\nUnclosed }\nbad @link\n...",
        "Issue Links": []
    },
    "NUTCH-38": {
        "Key": "NUTCH-38",
        "Summary": "distributed search improvement",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "07/Apr/05 04:13",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "08/Apr/05 05:36",
        "Description": "Running nutch web application with separate search servers can be pain to manage. Search servers go down, new segments are added, old ones removes and so on, almost all of these actions require some work to be done also at web app level (restart usually helps).\nThis simple enhancement tries to makes it just a little bit easier adding following features:\n-new segments can be added to searchservers without restarting the frontend.\n-defective search servers are not queried until tey come back online\n-watchdog keeps an eye for your searchservers and writes a simple log abt statistics",
        "Issue Links": []
    },
    "NUTCH-39": {
        "Key": "NUTCH-39",
        "Summary": "pagination in search result",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Jack Tang",
        "Created": "07/Apr/05 18:37",
        "Updated": "01/Apr/11 14:26",
        "Resolved": "01/Apr/11 14:26",
        "Description": "Now in nutch search.jsp, user navigate all search result using \"Next\" button. And google like pagination will feel better.",
        "Issue Links": []
    },
    "NUTCH-40": {
        "Key": "NUTCH-40",
        "Summary": "TestSegmentMergeTool fail",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "14/Apr/05 05:52",
        "Updated": "05/May/05 03:51",
        "Resolved": "05/May/05 03:51",
        "Description": "ant clean && ant test\n...\n050413 224204 * Deleting old segments...\n050413 224204 Finished SegmentMergeTool: INPUT: 5000 -> OUTPUT: 500 entries in 4.57 s (1250.0 entries/sec).\njava.io.FileNotFoundException: /tmp/.smttest24441/segments/seg0/fetcher/data (File exists)\n\tat java.io.RandomAccessFile.open(Native Method)\n\tat java.io.RandomAccessFile.<init>(RandomAccessFile.java:204)\n\tat org.apache.nutch.tools.TestSegmentMergeTool.testCorruptSegmentMerge(TestSegmentMergeTool.java:216)",
        "Issue Links": []
    },
    "NUTCH-41": {
        "Key": "NUTCH-41",
        "Summary": "Replace CVS by SVN within tutorial of Documentation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael Wechner",
        "Created": "15/Apr/05 06:24",
        "Updated": "20/Apr/05 06:54",
        "Resolved": "20/Apr/05 06:53",
        "Description": "CVS replaced by SVN within tutorial of Documentation",
        "Issue Links": []
    },
    "NUTCH-42": {
        "Key": "NUTCH-42",
        "Summary": "enhance search.jsp such that it can also returns XML",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7.2,                                            0.8",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Michael Wechner",
        "Created": "15/Apr/05 07:30",
        "Updated": "01/Jan/06 06:32",
        "Resolved": "01/Jan/06 06:32",
        "Description": "Enhance search.jsp such that by specifying a parameter format=xml the JSP will return an XML, whereas if no format is being specified then it will return HTML",
        "Issue Links": []
    },
    "NUTCH-43": {
        "Key": "NUTCH-43",
        "Summary": "replace / by request.getContextPath()+/",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Joost Baaij",
        "Created": "15/Apr/05 19:12",
        "Updated": "27/Sep/06 00:45",
        "Resolved": "24/May/05 04:04",
        "Description": "Many pages refer to the absolute root /.\nThis should be changed to request.getContextPath() + \"/\"",
        "Issue Links": [
            "/jira/browse/NUTCH-32"
        ]
    },
    "NUTCH-44": {
        "Key": "NUTCH-44",
        "Summary": "too many search results",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": "Dennis Kubes",
        "Reporter": "Emilijan Mirceski",
        "Created": "18/Apr/05 06:38",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "18/Feb/08 06:39",
        "Description": "There should be a limitation (user defined) on the number of results the search engine can return. \nFor example, if one modifies the seach url as:\nhttp://<my>/search.jsp?query=<some quiery>&hitsPerPage=20000&hitsPerSite=0\nThe search will try to return 20,000 pages which isn't good for the server side performance. \nIs it possible to have a setting in the config xml files to control this?\nThanks,\nEmilijan",
        "Issue Links": []
    },
    "NUTCH-45": {
        "Key": "NUTCH-45",
        "Summary": "Log corrupt segments in SegmentMergeTool",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.7.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Otis Gospodnetic",
        "Created": "18/Apr/05 13:35",
        "Updated": "21/Jan/06 16:56",
        "Resolved": "21/Jan/06 16:56",
        "Description": "Just added a LOG.warning line when corrupt segments are encountered, otherwise they just get skipped silently.",
        "Issue Links": []
    },
    "NUTCH-46": {
        "Key": "NUTCH-46",
        "Summary": "the NDFS problem(Could not obtain new output block for file)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "zhangjin",
        "Created": "19/Apr/05 18:30",
        "Updated": "15/Jul/05 05:54",
        "Resolved": "15/Jul/05 05:54",
        "Description": "Ndfs is very important to  distributed handling.But I never get good result by NDfS, The error is Like \"\nHit uncaught exception java.io.IOException java.io.IOException: Could not obtain new output block for file  1.txt\"\nPlease tell me\nThanks thanks very much!\nmsn:prettysino@hotmail.com\nmail:prettykely@gmail.com\nblog0:http://blog.csdn.net/prettyheart\nblog1:http://soacn.blogchina.com",
        "Issue Links": []
    },
    "NUTCH-47": {
        "Key": "NUTCH-47",
        "Summary": "Configure host filter to do wildcard prefixes - *.redhat.com",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "byron miller",
        "Created": "21/Apr/05 02:07",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "Right now you can configure the max results per host for query response, but that seems limited to exact host matches such as \"www.redhat.com\".\nIn many ways it would be nice to include the capability to match hosts by wildcard.\nFor example search for redhat on mozdex.com:\nhttp://www.mozdex.com/search.jsp?query=redhat\nAnd you will see:\nwww.apac.redhat.com \nwww.europe.redhat.com \nwww.in.redhat.com \nCould this be fixed so that *.redhat.com is under \"find more sources under redhat.com\" or something like that?\nI may be able to tweak the other processes, but i can envision a problem of people creating www1 www2 www3 or using other country codes for the same/similar content filling up pages of serps for what could be other relevent information.",
        "Issue Links": []
    },
    "NUTCH-48": {
        "Key": "NUTCH-48",
        "Summary": "\"Did you mean\"  query enhancement/refignment feature request",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": "Dennis Kubes",
        "Reporter": "byron miller",
        "Created": "21/Apr/05 06:21",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "Looking to implement a \"Did you mean\" feature for query result pages that return < = x amount of results to invoke a response that would recommend a fixed/related or spell checked query to try.\nNote from Doug to users list:\nDavid Spencer has worked on this some.\nhttp://www.searchmorph.com/weblog/index.php?id=23\nI think the code on his site might be more recent than what's committed\nto the lucene/contrib directory.",
        "Issue Links": []
    },
    "NUTCH-49": {
        "Key": "NUTCH-49",
        "Summary": "Flag for generate to fetch only new pages to complement the -refetchonly flag",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Luke Baker",
        "Created": "21/Apr/05 22:30",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 20:27",
        "Description": "It would be useful, especially for research/testing purposes, to have a flag for the FetchListTool that make sure to only include URLs in the fetchlist that have not already been fetched (according to the information from the webdb that you're generating the fetchlist from).",
        "Issue Links": []
    },
    "NUTCH-50": {
        "Key": "NUTCH-50",
        "Summary": "Benchmarks & Performance goals",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "byron miller",
        "Created": "22/Apr/05 22:37",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "21/Jun/10 14:54",
        "Description": "I am interested in developing a strategy and toolset used to benchmark nutch search.  Please give your feedback on the following approaches or recommendations for setting standards and goals.\nExample test case(s).\nJDK 1.4.x 32 bit/Linux Platform\nSingle Node/2 gigs of memory\nSingle Index/Segment\n1 million pages  \n\u2013 single node \u2013\nJDK 1.4.x 32 bit/Linux Platform\nSingle Node/2 gigs of memory\nSingle Index/Segment\n10 million pages\nJDK 1.4.x 32 bit/Linux Platform\nSingle Node/2 gigs of memory\nSingle Index/Segment\n10 million pages\n\u2013 dual node \u2013\nJDK 1.4.2 32 bit/Linux Platform\n2 Node/2 gigs of memory\n2 Indexes/Segments (1 per node)\n1 million pages\nJDK 1.4.2 32 bit/Linux Platform\n2 Node/2 gigs of memory\n2 Indexes/Segments (1 per node)\n1 million pages\n\u2013 test queries \u2013\n\nsingle term\nterm AND term\nexact \"small phrase\"\nlang:en term\nterm cluster\n\n\u2014 standards ----\n10 results per page\n---------------------\nFor me a testcase will help prove scalability, bottlenecks, application environments, settings and such.  The amount of customizations availble is where we need to really look at setting the best base for X amount of documents and some type of scalability scale.  For example a 10 node system may only scale x percent better for x reasons and x is the bottleneck for that scenerio.\nTest cases would serve multiple purposes for returning performance, response time and application stability. \nTools/possibilities:\n\nJMX components\nhttp://grinder.sourceforge.net/\nJMeter\nothers???\n\n---------------------\nQuery \"stuffing\" - use of dictionary that contains broad & vastly different terms. Something that could be scripted as a \"warm up\" for production systems as well.  Possibly combine terms from our logs of common search queries to use as a benchmark?\nWhat is your feedback/ideas on building a good test case/stress testing system/framework?",
        "Issue Links": []
    },
    "NUTCH-51": {
        "Key": "NUTCH-51",
        "Summary": "Removing a plugin after fetch but before indexing causes errors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "byron miller",
        "Created": "25/Apr/05 07:07",
        "Updated": "25/May/06 17:27",
        "Resolved": "24/May/05 03:45",
        "Description": "I created a segment of 4 million pages with a normal fetch. I created an index on this segment and it took over 30 hours to complete - when it did i had errors running a search that the nutchbean couldn't find the title value.\nI did some researching and it looks like i should have ran a dedup or something else.\nSo to debug i tried to re-index without some of the plugins enabled, and that is when i get this error:\nroot@elect [/home2/mozdex/nutch]# bin/nutch segread -fix segments/20050420222001/\nrun with heapsize 1500\n-Xmx1500m\n050424 180125 parsing file:/home2/mozdex/nutch/conf/nutch-default.xml\n050424 180126 parsing file:/home2/mozdex/nutch/conf/nutch-site.xml\n050424 180126 No FS indicated, using default:local\nroot@elect [/home2/mozdex/nutch]# bin/nutch index segments/20050420222001 -dir /home/work\nrun with heapsize 1500\n-Xmx1500m\n050424 180152 parsing file:/home2/mozdex/nutch/conf/nutch-default.xml\n050424 180152 parsing file:/home2/mozdex/nutch/conf/nutch-site.xml\n050424 180152 No FS indicated, using default:local\n050424 180152 indexing segment: segments/20050420222001\n050424 180152 * Opening segment 20050420222001\n050424 180153 * Indexing segment 20050420222001\n050424 180153 Plugins: looking in: /home2/mozdex/nutch/build/plugins\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/protocol-http/plugin.xml\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/parse-html/plugin.xml\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/parse-text/plugin.xml\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/parse-pdf/plugin.xml\n050424 180154 not including: /home2/mozdex/nutch/build/plugins/parse-msword\n050424 180154 not including: /home2/mozdex/nutch/build/plugins/parse-ext\n050424 180154 not including: /home2/mozdex/nutch/build/plugins/index-basic\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/query-basic/plugin.xml\n050424 180154 not including: /home2/mozdex/nutch/build/plugins/query-more\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/query-site/plugin.xml\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/query-url/plugin.xml\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/urlfilter-regex/plugin.xml\n050424 180154 not including: /home2/mozdex/nutch/build/plugins/urlfilter-prefix\n050424 180154 not including: /home2/mozdex/nutch/build/plugins/creativecommons\n050424 180154 not including: /home2/mozdex/nutch/build/plugins/language-identifier\n050424 180154 parsing: /home2/mozdex/nutch/build/plugins/clustering-carrot2/plugin.xml\n050424 180155 parsing: /home2/mozdex/nutch/build/plugins/ontology/plugin.xml\nException in thread \"main\" java.lang.ExceptionInInitializerError\n        at org.apache.nutch.indexer.IndexSegment.indexPages(IndexSegment.java:145)\n        at org.apache.nutch.indexer.IndexSegment.main(IndexSegment.java:254)\nCaused by: java.lang.RuntimeException: org.apache.nutch.indexer.IndexingFilter not found.\n        at org.apache.nutch.indexer.IndexingFilters.<clinit>(IndexingFilters.java:36)\n        ... 2 more\nIt seems when you generate a fetchlist with the plugin you can't process the index without the plugin.\nIf this is true, a friendly error message would be nice and perhaps a utility to \"fix\" the segment so it can be processed without the plugin.",
        "Issue Links": [
            "/jira/browse/NUTCH-284"
        ]
    },
    "NUTCH-52": {
        "Key": "NUTCH-52",
        "Summary": "Parser plugin for MS Excel files",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Rohit Kulkarni",
        "Created": "26/Apr/05 15:05",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "11/Feb/06 02:09",
        "Description": "Nutch plugin to parse MSExcel files (using jakarta poi) and based on the MSPowerPointParser plugin by Stephan Strittmatter.",
        "Issue Links": []
    },
    "NUTCH-53": {
        "Key": "NUTCH-53",
        "Summary": "Parser plugin for Zip files",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Rohit Kulkarni",
        "Created": "26/Apr/05 15:16",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "05/Sep/05 05:57",
        "Description": "Nutch plugin to parse Zip files (using java.util.zip)",
        "Issue Links": []
    },
    "NUTCH-54": {
        "Key": "NUTCH-54",
        "Summary": "Fetcher  improvements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "30/Apr/05 15:55",
        "Updated": "02/Jun/05 07:23",
        "Resolved": "02/Jun/05 07:19",
        "Description": "Fetcher improvements.",
        "Issue Links": []
    },
    "NUTCH-55": {
        "Key": "NUTCH-55",
        "Summary": "Create dmoz.org search plugin - incorporate the dmoz.org title/category/description if available &",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "byron miller",
        "Created": "03/May/05 00:27",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "28/Dec/05 22:20",
        "Description": "I am looking into the possibility of creating a dmoz.org plugin, so if you seed from the dmoz.org rdf the data you pull in could be used to extend the data you fetch.\nPossibilities:  Searchable dmoz.org data or nutch summary + dmoz.org category in serps.\nofcourse the data from dmoz.org isn't as descriptive as it used to be, but i think being able to integrate the category and href to a base url where the category resolves would be a nice feature (and homage to the dmoz.org data)",
        "Issue Links": []
    },
    "NUTCH-56": {
        "Key": "NUTCH-56",
        "Summary": "Crawling sites with 403 Forbidden robots.txt",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Andy Liu",
        "Created": "03/May/05 02:45",
        "Updated": "03/Jul/05 05:39",
        "Resolved": "03/Jul/05 05:39",
        "Description": "If a 403 error is encountered when trying to access the robots.txt file, Nutch does not crawl any pages from that site.  This behavior is consistent with the RFC recommendation for the robot exclusion protocol.  \nHowever, Google does crawl sites that exhibit this type of behavior, because most webmasters of these sites are unaware of robots.txt conventions and do want their site to be crawled.",
        "Issue Links": []
    },
    "NUTCH-57": {
        "Key": "NUTCH-57",
        "Summary": "text and html files unrecognized",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Marc Delerue",
        "Created": "09/May/05 23:20",
        "Updated": "03/Jul/05 04:42",
        "Resolved": "03/Jul/05 04:42",
        "Description": "While crawling : \nhttp://XXX.XXX.XXX.XXX/yyyyy.txtorg.apache.nutch.util.mime.MimeTypeException : invalid Sub Type plain \nand\nhttp://XXX.XXX.XXX.XXX/yyyyy.htmlorg.apache.nutch.util.mime.MimeTypeException : invalid Sub Type html\nThe html and text files are fetched but not indexed.",
        "Issue Links": []
    },
    "NUTCH-58": {
        "Key": "NUTCH-58",
        "Summary": "NullPointerException while coping NDFS file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Piotr Kosiorowski",
        "Created": "11/May/05 21:27",
        "Updated": "08/Jul/05 19:34",
        "Resolved": "08/Jul/05 19:34",
        "Description": "While using TestClient to copy NDFS file eg.:\n    TestClient -cp /testfile abc\nNullPointerException is thrown when destination file has no parent directory. \nI am adding it to issue tracking so it is not lost in mailing list archives.",
        "Issue Links": []
    },
    "NUTCH-59": {
        "Key": "NUTCH-59",
        "Summary": "meta data support in webdb",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "23/May/05 01:56",
        "Updated": "22/Jan/08 14:18",
        "Resolved": "22/Jan/08 14:18",
        "Description": "Meta data support in web db would very usefully for a new set of nutch feature that needs long life meta data. \nActually page meta data need to be regenerated or lookup every 30 days a page is re-fetched, in a long context web db meta data would bring a dramatically performance improvement for such tasks.\nFurthermore Storage of meta data in webdb would make a new generation of linklist generation filters possible.",
        "Issue Links": []
    },
    "NUTCH-60": {
        "Key": "NUTCH-60",
        "Summary": "Bad language identifier plugin performances",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Jerome Charron",
        "Created": "26/May/05 07:05",
        "Updated": "03/Jul/05 04:30",
        "Resolved": "03/Jul/05 04:30",
        "Description": "As reported by Stefan Groschupf (http://www.mail-archive.com/nutch-developers@lists.sourceforge.net/msg04090.html) the language identifier plugin consumes a lot of processing time.\nSome optimizations and/or configuration options are required.",
        "Issue Links": []
    },
    "NUTCH-61": {
        "Key": "NUTCH-61",
        "Summary": "Adaptive re-fetch interval. Detecting umodified content",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "06/Jun/05 05:38",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "30/May/07 18:35",
        "Description": "Currently Nutch doesn't adjust automatically its re-fetch period, no matter if individual pages change seldom or frequently. The goal of these changes is to extend the current codebase to support various possible adjustments to re-fetch times and intervals, and specifically a re-fetch schedule which tries to adapt the period between consecutive fetches to the period of content changes.\nAlso, these patches implement checking if the content has changed since last fetching; protocol plugins are also changed to make use of this information, so that if content is unmodified it doesn't have to be fetched and processed.",
        "Issue Links": []
    },
    "NUTCH-62": {
        "Key": "NUTCH-62",
        "Summary": "Add html META tag information into metaData in index-more plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Jack Tang",
        "Created": "07/Jun/05 10:50",
        "Updated": "22/May/13 03:54",
        "Resolved": "09/Dec/12 07:53",
        "Description": "Now(version dev-0.7), only some metaData  in http response such as type, date, content-length are available int the index-more plugin. And we cannot index/sotre the meta data in html header (<META> exactly)",
        "Issue Links": []
    },
    "NUTCH-63": {
        "Key": "NUTCH-63",
        "Summary": "the distributed search client generate too much logging statements",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "18/Jun/05 05:58",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "09/Jul/05 00:36",
        "Description": "For each query (depending on the number of segments) tooo many logging statements are generated.\nAfter a short timethis generates gigs of log files. \nThis logging  should change to debug:  \nLOG.info(\"Client: segment \"segments[j]\" at \"+addr);",
        "Issue Links": []
    },
    "NUTCH-64": {
        "Key": "NUTCH-64",
        "Summary": "no results after a restart of a search--server (without tomcat restart)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael Nebel",
        "Created": "30/Jun/05 18:31",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "15/Feb/06 04:53",
        "Description": "After restarting the search-server without restarting the tomcat,  the resultpage stays occasionally white. The reason:\n2005-06-25 12:56:16 StandardWrapperValve[jsp]: Servlet.service() for servlet jsp threw exception\njava.lang.NullPointerException\n        at java.util.Hashtable.get(Hashtable.java:333)\n        at net.nutch.ipc.Client.getConnection(Client.java:278)\n        at net.nutch.ipc.Client.call(Client.java:253)\n        at net.nutch.searcher.DistributedSearch$Client.getSummary(DistributedSearch.java:418)\n        at net.nutch.searcher.NutchBean.getSummary(NutchBean.java:248)\n        at org.apache.jsp.meta_jsp._jspService(meta_jsp.java:115)\n        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:324)\n        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:292)\n        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:236)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:237)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:157)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:214)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:198)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:152)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:137)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:117)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:102)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:929)\n        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:160)\n        at org.apache.jk.server.JkCoyoteHandler.invoke(JkCoyoteHandler.java:300)\n        at org.apache.jk.common.HandlerRequest.invoke(HandlerRequest.java:374)\n        at org.apache.jk.common.ChannelSocket.invoke(ChannelSocket.java:743)\n        at org.apache.jk.common.ChannelSocket.processConnection(ChannelSocket.java:675)\n        at org.apache.jk.common.SocketConnection.runIt(ChannelSocket.java:866)\n        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:683)\n        at java.lang.Thread.run(Thread.java:534)",
        "Issue Links": []
    },
    "NUTCH-65": {
        "Key": "NUTCH-65",
        "Summary": "index-more plugin can't parse large set of  modification-date",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "YourSoft",
        "Created": "01/Jul/05 18:55",
        "Updated": "02/Sep/05 07:24",
        "Resolved": "02/Sep/05 07:24",
        "Description": "I found a problem in MoreIndexingFilter.java.\nWhen I indexing segments, I get large list of error messages:\ncan't parse errorenous date: Wed, 10 Sep 2003 11:59:14 or\ncan't parse errorenous date: Wed, 10 Sep 2003 11:59:14GMT\nI modifiing source code (I don't make a 'patch'):\nOriginal (lines 137-138):\nDateFormat df = new SimpleDateFormat(\"EEE MMM dd HH:mm:ss yyyy zzz\");\nDate d = df.parse(date);\nNew:\nDateFormat df = new SimpleDateFormat(\"EEE, MMM dd HH:mm:ss yyyy\", Locale.US);\nDate d = df.parse(date.substring(0,25));\nThe modified code works fine.",
        "Issue Links": []
    },
    "NUTCH-66": {
        "Key": "NUTCH-66",
        "Summary": "Cookies are not being read properly",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "CC Chaman",
        "Created": "03/Jul/05 05:31",
        "Updated": "21/Jul/05 06:38",
        "Resolved": "21/Jul/05 06:38",
        "Description": "Cookies that do not begin with a period are not being accepted. For example \"cnn.com\" instead of the RFC \".cnn.com\". But A LOT of sites seem to not know the standard. It would be nice if the plugin accepted those cookies as well.",
        "Issue Links": []
    },
    "NUTCH-67": {
        "Key": "NUTCH-67",
        "Summary": "I  want  crawl the websites including news.yahoo.com,game.yahoo.com,blog.yahoo.com,etc!",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "zhangjin",
        "Created": "04/Jul/05 12:33",
        "Updated": "27/Nov/05 23:11",
        "Resolved": "27/Nov/05 23:11",
        "Description": "how do  I  config them in the crawl-urlfilter.txt? I  config  them below,but  it is not successful.\n\nThe url filter file used by the crawl command.\n\n\nBetter for intranet crawling.\nBe sure to change MY.DOMAIN.NAME to your domain name.\n\n\nEach non-comment, non-blank line contains a regular expression\nprefixed by '+' or '-'.  The first matching pattern in the file\ndetermines whether a URL is included or ignored.  If no pattern\nmatches, the URL is ignored.\n\n\nskip file:, ftp:, & mailto: urls\n-^(file|ftp|mailto):\n\n\nskip image and other suffixes we can't yet parse\n-\\.(gif|GIF|jpg|JPG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe)$\n\n\nskip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n\naccept hosts in MY.DOMAIN.NAME\n#+^http://([a-z0-9]*\\.)*MY.DOMAIN.NAME/\n+^http://([a-z0-9]*\\.)*yahoo.com/\nskip everything else\n#-.\nbut It can not work, and can not crawl the  domain name (DOMAIN.NAME) inluding  news.yahoo.com,game.yahoo.com,blog.yahoo.com\nwhy?",
        "Issue Links": []
    },
    "NUTCH-68": {
        "Key": "NUTCH-68",
        "Summary": "A tool to generate arbitrary fetchlists",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "05/Jul/05 17:04",
        "Updated": "17/Jan/07 19:56",
        "Resolved": "17/Jan/07 19:56",
        "Description": "This is a tool to generate arbitrary fetchlists out of plain-text URL lists. I found it useful quite often, e.g. when I had to fetch certain specific pages without adding them to DB, or for testing purposes.",
        "Issue Links": []
    },
    "NUTCH-69": {
        "Key": "NUTCH-69",
        "Summary": "fetcher.threads.per.host ignored",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Matthias Jaekle",
        "Created": "08/Jul/05 23:18",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "08/Jul/05 23:38",
        "Description": "Fetcher ignores 'maximum threads per host'.\nIf you fetch less domains with multiple threads, some webservers feel attacked or could not serve you any more.\nSo you loose lots of existing pages in your segments.",
        "Issue Links": []
    },
    "NUTCH-70": {
        "Key": "NUTCH-70",
        "Summary": "duplicate pages - virtual hosts in db.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "YourSoft",
        "Created": "11/Jul/05 18:09",
        "Updated": "14/Mar/08 23:58",
        "Resolved": "14/Mar/08 23:58",
        "Description": "Dear Developers,\nI have a problem with nutch:\n\nThere are many sites duplicates in the webdb and in the segments.\nThe source of this problem is:\nIf the site make 'virtual hosts' (like Apache), e.g. www.origo.hu, origo.hu, origo.matav.hu, origo.matavnet.hu etc.: the result pages are the same, only the inlinks are differents.\nThe ip address is the same.\nWhen search, all virtualhosts are in the results.\n\nGoogle only show one of these virtual hosts, the nutch show all. The result nutch db is larger, and this case slower, than google.\nHave any idea, how to remove these duplicates?\nRegards,\n    Ferenc",
        "Issue Links": []
    },
    "NUTCH-71": {
        "Key": "NUTCH-71",
        "Summary": "Search web page doesn't not focus on query input",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Christophe Noel",
        "Created": "12/Jul/05 21:17",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "19/Aug/05 05:23",
        "Description": "In search.html and search.jsp , keyboard cursor does not focus in the form query input.\nI've made a patch for en and fr search.html and for search.jsp.",
        "Issue Links": []
    },
    "NUTCH-72": {
        "Key": "NUTCH-72",
        "Summary": "Query basic filter with correction feature",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Christophe Noel",
        "Created": "15/Jul/05 20:26",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "This plugin improves query-basic plugin with a correction feature.\nLucene includes FuzzyQuery feature which consists of searching not only for matching terms, but searching for very similar terms too.\nThis plugin should be used instead of query-basic, for people looking for an easy solution about users query requests correction.\nCorrection Query Plugin can be used as follows :\nSolution 1 :  If you want to search for very similar terms, add autocorrectionmod as the first term of the query (example : 'nutch engine' -> 'autocorrectionmod nutch engine')\nSolution 2 : Create a new search.jsp page which include a \"correction\" checkbox management (<input type=\"checkbox\" name=\"autocorrection\" value=\"true\"> may automatically add 'autocorrectionmod' as the first term of the query) \nQueryFuzzy knows a big problem : it is very slow for large index !\nSo Correction Query Plugin works as follows :\n\nit is not useful for big indexes\nit only works for 5 characters and more words\nit only look for words matching with the 2 first characters (to improve performance this should be set to 3/4)\nit only works for 65 % matching suffixes (algorithm is levenstein)\n\nPLease give your opinion about it.",
        "Issue Links": []
    },
    "NUTCH-73": {
        "Key": "NUTCH-73",
        "Summary": "A page for CSV results",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "web gui",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Christophe Noel",
        "Created": "15/Jul/05 21:04",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "26/Oct/10 04:31",
        "Description": "This jsp page allow users to get a CSV results pages.\nI don't know if it's very useful but as someone ask XML results page, i think it's nearly the same.",
        "Issue Links": []
    },
    "NUTCH-74": {
        "Key": "NUTCH-74",
        "Summary": "French Analyzer Plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Jerome Charron",
        "Reporter": "Christophe Noel",
        "Created": "19/Jul/05 21:30",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "06/Feb/09 14:17",
        "Description": "This is DRAFT for a new plugin for French Analysis (all java file come from Lucene project sandbox)... This includes ISO LATIN1 accent filter, plurial forms removing, ...\nAnalyze-frech should be used instead of NutchDocumentAnalysis as described by Jerome Charron in New Language Identifier project. It should be used also as a query-parser in Nutch searcher.\nWe miss an EXTENSION-POINT to include this kind of plugin in Nutch. Could anyone help me to build this new Extension Point please ?",
        "Issue Links": [
            "/jira/browse/NUTCH-261"
        ]
    },
    "NUTCH-75": {
        "Key": "NUTCH-75",
        "Summary": "Patch for WebDBReader to get more detailed information about WebDBs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthias Jaekle",
        "Created": "21/Jul/05 00:19",
        "Updated": "31/Mar/08 05:25",
        "Resolved": "31/Mar/08 05:25",
        "Description": "The patch offers information to watch\n\nall outlinks from a specific url\nall links to a specific url as csv-file\nmost links to a specific domain as csv-file\ndetailed stats with the amount of deleted links, pages without outlinks, links with anchor texts, links without anchor texts, average length of anchortexts.",
        "Issue Links": []
    },
    "NUTCH-76": {
        "Key": "HADOOP-8",
        "Summary": "NDFS DataNode advertises localhost as it's address",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Peter Sandstr\u00f6m",
        "Created": "24/Jul/05 23:46",
        "Updated": "18/May/15 04:15",
        "Resolved": "24/Feb/06 08:13",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-77": {
        "Key": "NUTCH-77",
        "Summary": "Project URL in JIRA",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stephan Strittmatter",
        "Created": "02/Aug/05 21:02",
        "Updated": "09/Aug/05 02:40",
        "Resolved": "09/Aug/05 02:40",
        "Description": "The project URL on JIRA should be updated from\nhttp://incubator.apache.org/nutch/\nto\nhttp://lucene.apache.org/nutch/",
        "Issue Links": []
    },
    "NUTCH-78": {
        "Key": "NUTCH-78",
        "Summary": "German texts on website",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthias Jaekle",
        "Created": "06/Aug/05 01:43",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "09/Aug/05 02:11",
        "Description": "The German properties-files with the texts to present on the websites were incomplete, or with wrong spellings.\nPlease find attached the corrected files.",
        "Issue Links": []
    },
    "NUTCH-79": {
        "Key": "NUTCH-79",
        "Summary": "Fault tolerant searching.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Piotr Kosiorowski",
        "Created": "09/Aug/05 01:40",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:27",
        "Description": "I have finally managed to prepare first version of fault tolerant searching I have promised long time ago. \nIt reads server configuration from search-groups.txt file (in startup directory or directory specified by searcher.dir) if no search-servers.txt file is present. If search-servers.txt  is presentit would be read and handled as previously.\n---------------------------------------------------\nFormat of search-groups.txt:\n\n<pre>\nsearch.group.count=[int]\nsearch.group.name.[i]=[string] (for i=0 to count-1)\n\nFor each name:\n[name].part.count=[int] partitionCount\n[name].part.[i].host=[string] (for i=0 to partitionCount-1)\n[name].part.[i].port=int (for i=0 to partitionCount-1)\n\nExample:\nsearch.group.count=2\nsearch.group.name.0=master\nsearch.group.name.1=backup\n\nmaster.part.count=2\nmaster.part.0.host=host1\nmaster.part.0.port=7777\nmaster.part.1.host=host2\nmaster.part.1.port=7777\n\nbackup.part.count=2\nbackup.part.0.host=host3\nbackup.part.0.port=7777\nbackup.part.1.host=host4\nbackup.part.1.port=7777\n</pre>.\n------------------------------------------------\n\nIf more than one search group is defined in configuration file requests are distributed among groups in round-robin fashion. If one of the servers from the group fails to respond the whole group is treated as inactive and removed from the pool used to distributed requests. There is a separate recovery thread that every \"searcher.recovery.delay\" seconds (default 60) tries to check if inactive became alive and if so adds it back to the pool of active groups.",
        "Issue Links": []
    },
    "NUTCH-80": {
        "Key": "NUTCH-80",
        "Summary": "Web UI only works when project deployed in root",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "AJ Banck",
        "Created": "15/Aug/05 20:26",
        "Updated": "16/Aug/05 03:58",
        "Resolved": "16/Aug/05 03:58",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-81": {
        "Key": "NUTCH-81",
        "Summary": "Webapp only works when deployed in root",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "AJ Banck",
        "Created": "15/Aug/05 20:31",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "07/Feb/06 06:10",
        "Description": "Index.jsp does a redirect (not forward) to the language folder.\nThe links in the html however are relative to the language folder, not the application root.\nNot sure what the desired behavoir is, change the html (where is it generated?) or the redirect.",
        "Issue Links": []
    },
    "NUTCH-82": {
        "Key": "NUTCH-82",
        "Summary": "Nutch Commands should run on Windows without external tools",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "AJ Banck",
        "Created": "16/Aug/05 03:21",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Dec/08 13:52",
        "Description": "Currently there is only a shellscript to run the Nutch commands. This should be platform independant.\nBest would be Ant tools, or scripts generated by a template tool to avoid replication.",
        "Issue Links": []
    },
    "NUTCH-83": {
        "Key": "NUTCH-83",
        "Summary": "Release deliverable as zip",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "AJ Banck",
        "Created": "16/Aug/05 04:10",
        "Updated": "01/Apr/11 14:28",
        "Resolved": "01/Apr/11 14:28",
        "Description": "Like Lucene, Nutch could be delivered as a .zip file so it can be used with default tools on Windows.",
        "Issue Links": []
    },
    "NUTCH-84": {
        "Key": "NUTCH-84",
        "Summary": "Fetcher for constrained crawls",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Kelvin Tan",
        "Created": "25/Aug/05 08:01",
        "Updated": "13/Apr/11 23:09",
        "Resolved": "13/Apr/11 23:09",
        "Description": "As posted http://marc.theaimsgroup.com/?l=nutch-developers&m=112476980602585&w=2",
        "Issue Links": []
    },
    "NUTCH-85": {
        "Key": "NUTCH-85",
        "Summary": "pdf parser caused fetcher hangs.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "25/Aug/05 18:12",
        "Updated": "20/Sep/05 16:09",
        "Resolved": "20/Sep/05 16:09",
        "Description": "We notice that fetcher hangs caused by pdfbox.\nA thread handles a pdf parsing and may hangs and is never again available. \nThis happens as many times as threads are active and than the complete fetch process hangs.\nFull thread dump Java HotSpot(TM) Client VM (1.4.2_08-b03 mixed mode):\n\"fetcher160\" prio=1 tid=0x083c9720 nid=0x16de runnable [b1669000..b166a238]\n\tat org.pdfbox.cmaptypes.CMap.addMapping(CMap.java:119)\n\tat org.pdfbox.cmapparser.CMapParser.parse(CMapParser.java:183)\n\tat org.pdfbox.pdmodel.font.PDFont.parseCmap(PDFont.java:532)\n\tat org.pdfbox.pdmodel.font.PDFont.encode(PDFont.java:358)\n\tat org.pdfbox.util.PDFStreamEngine.showString(PDFStreamEngine.java:261)\n\tat org.pdfbox.util.operator.ShowText.process(ShowText.java:63)\n\tat org.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:405)\n\tat org.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:385)\n\tat org.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:168)\n\tat org.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:232)\n\tat org.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:205)\n\tat org.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:180)\n\tat org.pdfbox.util.PDFTextStripper.getText(PDFTextStripper.java:108)\n\tat org.apache.nutch.parse.pdf.PdfParser.getParse(PdfParser.java:123)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.handleFetch(Fetcher.java:239)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:148)\n\"fetcher82\" prio=1 tid=0xb4637d78 nid=0x59aa runnable [b4379000..b437a238]\n\tat java.nio.charset.CoderResult$1.create(CoderResult.java:207)\n\tat java.nio.charset.CoderResult$Cache.get(CoderResult.java:196)\n\nlocked <0xb94fa908> (a java.nio.charset.CoderResult$1)\n\tat java.nio.charset.CoderResult$Cache.access$200(CoderResult.java:178)\n\tat java.nio.charset.CoderResult.malformedForLength(CoderResult.java:217)\n\tat sun.nio.cs.UnicodeDecoder.decodeLoop(UnicodeDecoder.java:71)\n\tat java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:538)\n\tat java.lang.StringCoding$CharsetSD.decode(StringCoding.java:192)\n\tat java.lang.StringCoding.decode(StringCoding.java:230)\n\tat java.lang.String.<init>(String.java:320)\n\tat java.lang.String.<init>(String.java:346)\n\tat org.pdfbox.cmapparser.CMapParser.createStringFromBytes(CMapParser.java:230)\n\tat org.pdfbox.cmapparser.CMapParser.parse(CMapParser.java:182)\n\tat org.pdfbox.pdmodel.font.PDFont.parseCmap(PDFont.java:532)\n\tat org.pdfbox.pdmodel.font.PDFont.encode(PDFont.java:358)\n\tat org.pdfbox.util.PDFStreamEngine.showString(PDFStreamEngine.java:261)\n\tat org.pdfbox.util.operator.ShowText.process(ShowText.java:63)\n\tat org.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:405)\n\tat org.pdfbox.util.PDFStreamEngine.processOperator(PDFStreamEngine.java:385)\n\tat org.pdfbox.util.PDFStreamEngine.processStream(PDFStreamEngine.java:168)\n\tat org.pdfbox.util.PDFTextStripper.processPage(PDFTextStripper.java:232)\n\tat org.pdfbox.util.PDFTextStripper.processPages(PDFTextStripper.java:205)\n\tat org.pdfbox.util.PDFTextStripper.writeText(PDFTextStripper.java:180)\n\tat org.pdfbox.util.PDFTextStripper.getText(PDFTextStripper.java:108)\n\tat org.apache.nutch.parse.pdf.PdfParser.getParse(PdfParser.java:123)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.handleFetch(Fetcher.java:239)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:148)",
        "Issue Links": []
    },
    "NUTCH-86": {
        "Key": "NUTCH-86",
        "Summary": "LanguageIdentifier API enhancements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6,                                            0.7,                                            0.8",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Jerome Charron",
        "Created": "31/Aug/05 19:31",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "14/Jul/10 17:40",
        "Description": "More informations can be found on the following thread on Nutch-Dev mailing list:\nhttp://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00569.html\nSummary:\n1. LanguageIdentifier API changes. The similarity methods should return an ordered array of language-code/score pairs instead of a simple String containing the language-code.\n2. Ensure consistency between LanguageIdentifier scoring and NGramProfile.getSimilarity().",
        "Issue Links": []
    },
    "NUTCH-87": {
        "Key": "NUTCH-87",
        "Summary": "Efficient site-specific crawling for a large number of sites",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.2,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "AJ Chen",
        "Created": "03/Sep/05 05:22",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "There is a gap between whole-web crawling and single (or handful) site crawling. Many applications actually fall in this gap, which usually require to crawl a large number of selected sites, say 100000 domains. Current CrawlTool is designed for a handful of sites. So, this request calls for a new feature or improvement on CrawTool so that \"nutch crawl\" command can efficiently deal with large number of sites. One requirement is to add or change smallest amount of code so that this feature can be implemented sooner rather than later. \nThere is a discussion about adding a URLFilter to implement this requested feature, see the following thread - \nhttp://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00726.html\nThe idea is to use a hashtable in URLFilter for looking up regex for any given domain. Hashtable will be much faster than list implementation currently used in RegexURLFilter.  Fortunately, Matt Kangas has implemented such idea before for his own application and is willing to make it available for adaptation to Nutch. I'll be happy to help him in this regard.  \nBut, before we do it, we would like to hear more discussions or comments about this approach or other approaches. Particularly, let us know what potential downside will be for hashtable lookup in a new URLFilter plugin.\nAJ Chen",
        "Issue Links": []
    },
    "NUTCH-88": {
        "Key": "NUTCH-88",
        "Summary": "Enhance ParserFactory plugin selection policy",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "indexer",
        "Assignee": "Jerome Charron",
        "Reporter": "Jerome Charron",
        "Created": "08/Sep/05 18:06",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "15/Oct/05 08:44",
        "Description": "The ParserFactory choose the Parser plugin to use based on the content-types and path-suffix defined in the parsers plugin.xml file.\nThe selection policy is as follow:\nContent type has priority: the first plugin found whose \"contentType\" attribute matches the beginning of the content's type is used. \nIf none match, then the first whose \"pathSuffix\" attribute matches the end of the url's path is used.\nIf neither of these match, then the first plugin whose \"pathSuffix\" is the empty string is used.\nThis policy has a lot of problems when no matching is found, because a random parser is used (and there is a lot of chance this parser can't handle the content).\nOn the other hand, the content-type associated to a parser plugin is specified in the plugin.xml of each plugin (this is the value used by the ParserFactory), AND the code of each parser checks itself in its code if the content-type is ok (it uses an hard-coded content-type value, and not uses the value specified in the plugin.xml => possibility of missmatches between content-type hard-coded and content-type delcared in plugin.xml).\nA complete list of problems and discussion aout this point is available in:\n\nhttp://www.mail-archive.com/nutch-user%40lucene.apache.org/msg00744.html\nhttp://www.mail-archive.com/nutch-dev%40lucene.apache.org/msg00789.html",
        "Issue Links": []
    },
    "NUTCH-89": {
        "Key": "NUTCH-89",
        "Summary": "parse-rss null pointer exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.7,                                            0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Michael Nebel",
        "Created": "10/Sep/05 20:58",
        "Updated": "24/Sep/05 01:16",
        "Resolved": "24/Sep/05 01:16",
        "Description": "The rss-parser causes an exception. The reason is a syntax error in the page. Hitting this pages, the parser trys to add an outlink with \"null\" as anchor.  The anchor  of a outlink must no be null. \njava.lang.NullPointerException\n        at org.apache.nutch.io.UTF8.writeString(UTF8.java:236)\n        at org.apache.nutch.parse.Outlink.write(Outlink.java:51)\n        at org.apache.nutch.parse.ParseData.write(ParseData.java:111)\n        at org.apache.nutch.io.SequenceFile$Writer.append(SequenceFile.java:137)\n        at org.apache.nutch.io.MapFile$Writer.append(MapFile.java:127)\n        at org.apache.nutch.io.ArrayFile$Writer.append(ArrayFile.java:39)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.outputPage(Fetcher.java:281)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.handleFetch(Fetcher.java:261)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:148)\nException in thread \"main\" java.lang.RuntimeException: SEVERE error logged.  Exiting fetcher.\n        at org.apache.nutch.fetcher.Fetcher.run(Fetcher.java:354)\n        at org.apache.nutch.fetcher.Fetcher.main(Fetcher.java:488)\n        at org.apache.nutch.tools.CrawlTool.main(CrawlTool.java:140)\nI suggest the following patch:\nIndex: src/plugin/parse-rss/src/java/org/apache/nutch/parse/rss/RSSParser.java\n===================================================================\n\u2014 src/plugin/parse-rss/src/java/org/apache/nutch/parse/rss/RSSParser.java     (revision 279397)\n+++ src/plugin/parse-rss/src/java/org/apache/nutch/parse/rss/RSSParser.java     (working copy)\n@@ -157,11 +157,13 @@\n                 if (r.getLink() != null) {\n                     try {\n                         // get the outlink\n\ntheOutlinks.add(new Outlink(r.getLink(), r\n.getDescription()));\n+                       if (r.getDescription()!= null ) \n{\n+                           theOutlinks.add(new Outlink(r.getLink(), r.getDescription()));\n+                       }\n else \n{\n+                           theOutlinks.add(new Outlink(r.getLink(), \"\"));\n+                       }\n                     } catch (MalformedURLException e) {\nLOG\n.info(\"nutch:parse-rss:RSSParser Exception: MalformedURL: \"\n+                        LOG.info(\"nutch:parse-rss:RSSParser Exception: MalformedURL: \"\n                                         + r.getLink()\n                                         + \": Attempting to continue processing outlinks\");\n                         e.printStackTrace();\n@@ -185,12 +187,13 @@\n\n                     if (whichLink != null) {\n                         try {\n\ntheOutlinks.add(new Outlink(whichLink, theRSSItem\n.getDescription()));\n-\n+                           if (theRSSItem.getDescription()!=null) \n{\n+                               theOutlinks.add(new Outlink(whichLink, theRSSItem.getDescription()));\n+                           }\n else \n{\n+                               theOutlinks.add(new Outlink(whichLink, \"\"));\n+                           }\n                         } catch (MalformedURLException e) {\nLOG\n.info(\"nutch:parse-rss:RSSParser Exception: MalformedURL: \"\n+                            LOG.info(\"nutch:parse-rss:RSSParser Exception: MalformedURL: \"\n                                             + whichLink\n                                             + \": Attempting to continue processing outlinks\");\n                             e.printStackTrace();",
        "Issue Links": []
    },
    "NUTCH-90": {
        "Key": "NUTCH-90",
        "Summary": "reduce logging output of IndexSegment",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Michael Nebel",
        "Created": "10/Sep/05 21:23",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "15/Feb/06 04:59",
        "Description": "I think, LOG.fine would be enough  \nIndex: src/java/org/apache/nutch/indexer/IndexSegment.java\n===================================================================\n\u2014 src/java/org/apache/nutch/indexer/IndexSegment.java (revision 279397)\n+++ src/java/org/apache/nutch/indexer/IndexSegment.java (working copy)\n@@ -142,10 +142,10 @@\n               // run filters to add more fields to the document\n               doc = IndexingFilters.filter(doc, parse, fetcherOutput);\n\n+     \n               // add the document to the index\n               NutchAnalyzer analyzer = AnalyzerFactory.get(doc.get(\"lang\"));\nLOG.info(\" Indexing [\" + doc.getField(\"url\").stringValue() + \"]\" +\n+              LOG.fine(\" Indexing [\" + doc.getField(\"url\").stringValue() + \"]\" + \n                        \" with analyzer \" + analyzer +\n                        \" (\" + doc.get(\"lang\") + \")\");\n               //LOG.info(\" Doc is \" + doc);",
        "Issue Links": []
    },
    "NUTCH-91": {
        "Key": "NUTCH-91",
        "Summary": "empty encoding causes exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.7.2,                                            0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael Nebel",
        "Created": "10/Sep/05 21:35",
        "Updated": "10/Mar/06 05:17",
        "Resolved": "10/Mar/06 05:17",
        "Description": "I found some sites, where the header says:  \"Content-Type: text/html; charset=\". This causes an exception in the HtmlParser. My suggestion:\nIndex: src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java\n===================================================================\n\u2014 src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java  (revision 279397)\n+++ src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java  (working copy)\n@@ -120,7 +120,7 @@\n       byte[] contentInOctets = content.getContent();\n       InputSource input = new InputSource(new ByteArrayInputStream(contentInOctets));\n       String encoding = StringUtil.parseCharacterEncoding(contentType);\n\nif (encoding!=null) {\n+      if (encoding!=null && !\"\".equals(encoding)) {\n         metadata.put(\"OriginalCharEncoding\", encoding);\n         if ((encoding = StringUtil.resolveEncodingAlias(encoding)) != null) {\n           metadata.put(\"CharEncodingForConversion\", encoding);",
        "Issue Links": []
    },
    "NUTCH-92": {
        "Key": "NUTCH-92",
        "Summary": "DistributedSearch incorrectly scores results",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "16/Sep/05 04:27",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "When running search servers in a distributed setup, using DistributedSearch$Server and Client, total scores are incorrectly calculated. The symptoms are that scores differ depending on how segments are deployed to Servers, i.e. if there is uneven distribution of terms in segment indexes (due to segment size or content differences) then scores will differ depending on how many and which segments are deployed on a particular Server. This may lead to prioritizing of non-relevant results over more relevant ones.\nThe underlying reason for this is that each IndexSearcher (which uses local index on each Server) calculates scores based on the local IDFs of query terms, and not the global IDFs from all indexes together. This means that scores arriving from different Servers to the Client cannot be meaningfully compared, unless all indexes have similar distribution of Terms and similar numbers of documents in them. However, currently the Client mixes all scores together, sorts them by absolute values and picks top hits. These absolute values will change if segments are un-evenly deployed to Servers.\nCurrently the workaround is to deploy the same number of documents in segments per Server, and to ensure that segments contain well-randomized content so that term frequencies for common terms are very similar.\nThe solution proposed here (as a result of discussion between ab and cutting, patches are coming) is to calculate global IDFs prior to running the query, and pre-boost query Terms with these global IDFs. This will require one more RPC call per each query (this can be optimized later, e.g. through caching). Then the scores will become normalized according to the global IDFs, and Client will be able to meaningfully compare them. Scores will also become independent of the segment content or local number of documents per Server. This will involve at least the following changes:\n\nchange NutchSimilarity.idf(Term, Searcher) to always return 1.0f. This enables us to manipulate scores independently of local IDFs.\n\n\nadd a new method to Searcher interface, int[] getDocFreqs(Term[]), which will return document frequencies for query terms.\n\n\nmodify getSegmentNames() so that it returns also the total number of documents in each segment, or implement this as a separate method (this will be called once during segment init)\n\n\nin DistributedSearch$Client.search() first make a call to servers to return local IDFs for the current query, and calculate global IDFs for each relevant Term in that query.\n\n\nmultiply the TermQuery boosts by idf(totalDocFreq, totalIndexedDocs), and PhraseQuery boosts by the sum of the idf(totalDocFreqs, totalIndexedDocs) for all of its terms\n\nThis solution should be applicable with only minor changes to all branches, but initially the patches will be relative to trunk/ .\nComments, suggestions and review are welcome!",
        "Issue Links": []
    },
    "NUTCH-93": {
        "Key": "NUTCH-93",
        "Summary": "DF error on long filesystem name",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Shuji Umino",
        "Created": "17/Sep/05 03:21",
        "Updated": "21/Sep/05 11:39",
        "Resolved": "21/Sep/05 11:39",
        "Description": "java.util.NoSuchElementException happened on start datanode.\nmy system use LVM, default installed filesystem name is 'VolGroup00-LogVol00',\ndivided two lines on type 'df' command.\n---------------------------------------------------------------------------------------\n#df -k\n/dev/mapper/VolGroup00-LogVol00\n                     152559732   1279408 143530692   1% /                  <---  return next line\n/dev/sda2               101105      9098     86786  10% /boot\nnone                    257352         0    257352   0% /dev/shm\n---------------------------------------------------------------------------------------\n[org.apache.nutch.ndfs.DF] fixed source \n        StringTokenizer tokens =\n          new StringTokenizer(lines.readLine(), \" \\t\\n\\r\\f%\");\n        this.filesystem = tokens.nextToken();       \n        if (!tokens.hasMoreTokens()) \n{\n        \t//for long filesystem name\n        \ttokens = new StringTokenizer(lines.readLine(), \" \\t\\n\\r\\f%\");\n        }",
        "Issue Links": []
    },
    "NUTCH-94": {
        "Key": "NUTCH-94",
        "Summary": "MapFile.Writer throwing 'File exists error'.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.6",
        "Fix Version/s": "0.7.2",
        "Component/s": "fetcher",
        "Assignee": "Piotr Kosiorowski",
        "Reporter": "Michael Couck",
        "Created": "19/Sep/05 18:00",
        "Updated": "25/Mar/06 19:48",
        "Resolved": "25/Mar/06 19:48",
        "Description": "Running Nutch inside a server JVM or multiple times in the same JVM, MapFile.Writer doesn't get collected or closed by the WebDBWriter and the associated files and directories are not deleted, consequently throws a File exists error in the constructor of MapFile.Writer.\nSeems that this portion of code is very heavily integrated into Nutch and I am hesitant to look for a solution personally as a retrofit will be necessary with every release.\nHas anyone got any ideas, had the same issue, any solutions?\nRegards\nMichael",
        "Issue Links": []
    },
    "NUTCH-95": {
        "Key": "NUTCH-95",
        "Summary": "DeleteDuplicates depends on the order of input segments",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.6,                                            0.7,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "21/Sep/05 05:58",
        "Updated": "17/Jan/08 20:32",
        "Resolved": "17/Jan/08 20:32",
        "Description": "DeleteDuplicates depends on what order the input segments are processed, which in turn depends on the order of segment dirs returned from NutchFileSystem.listFiles(File). In most cases this is undesired and may lead to deleting wrong records from indexes. The silent assumption that segments at the end of the listing are more recent is not always true.\nHere's the explanation:\n\nDedup first deletes the URL duplicates by computing MD5 hashes for each URL, and then sorting all records by (hash, segmentIdx, docIdx). SegmentIdx is just an int index to the array of open IndexReaders - and if segment dirs are moved/copied/renamed then entries in that array may change their  order. And then for all equal triples Dedup keeps just the first entry. Naturally, if segmentIdx is changed due to dir renaming, a different record will be kept and different ones will be deleted...\n\n\nthen Dedup deletes content duplicates, again by computing hashes for each content, and then sorting records by (hash, segmentIdx, docIdx). However, by now we already have a different set of undeleted docs depending on the order of input segments. On top of that, the same factor acts here, i.e. segmentIdx changes when you re-shuffle the input segment dirs - so again, when identical entries are compared the one with the lowest (segmentIdx, docIdx) is picked.\n\nSolution: use the fetched date from the first record in each segment to determine the order of segments. Alternatively, modify DeleteDuplicates to use the newer algorithm from SegmentMergeTool. This algorithm works by sorting records using tuples of (urlHash, contentHash, fetchDate, score, urlLength). Then:\n1. If urlHash is the same, keep the doc with the highest fetchDate  (the latest version, as recorded by Fetcher).\n2. If contentHash is the same, keep the doc with the highest score, and then if the scores are the same, keep the doc with the shortest url.\nInitial fix will be prepared for the trunk/ and then backported to the release branch.",
        "Issue Links": [
            "/jira/browse/NUTCH-371"
        ]
    },
    "NUTCH-96": {
        "Key": "NUTCH-96",
        "Summary": "MapFile.Writer throws directory exists exception if run multiple times in the same JVM or server JVM.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7.2",
        "Component/s": "fetcher",
        "Assignee": "Piotr Kosiorowski",
        "Reporter": "Michael Couck",
        "Created": "21/Sep/05 17:55",
        "Updated": "25/Mar/06 19:49",
        "Resolved": "25/Mar/06 19:49",
        "Description": "I added a bug to the 0.6 version, but I found the same behaviour in the 0.7 version. Specifically the MapFile.Writer doesn't get closed and deleted by WebDBWriter and throws an exception if the directory already exists. Still reluctant to solve this if the solution is not going to get integrated into the official code of Nutch as a retrofit will be necessary with every version released, however I will fix this and submit the patch for evaluation if this is not evaluated in the next couple of days by the Nutch team.\nRegards\nMichael",
        "Issue Links": []
    },
    "NUTCH-97": {
        "Key": "NUTCH-97",
        "Summary": "make datanode starting port configurable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "26/Sep/05 18:17",
        "Updated": "29/Sep/05 22:28",
        "Resolved": "29/Sep/05 22:28",
        "Description": "Actually the ndfs datanode port is hardcoded to 7000. In case 7000 is blocked the datanode will iterate up until it find a free port.\nThis behavior is difficult to manage in case a fire wall is installed.\nIt would be good in case it is possible to be able configure the port, as it is possible with the namenode port as well.",
        "Issue Links": []
    },
    "NUTCH-98": {
        "Key": "NUTCH-98",
        "Summary": "RobotRulesParser interprets robots.txt incorrectly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Jeff Bowden",
        "Created": "29/Sep/05 15:27",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "Here's a simple example that the current RobotRulesParser gets wrong:\nUser-agent: *\nDisallow: /\nAllow: /rss\nThe problem is that the isAllowed function takes the first rule that matches and incorrectly decides that URLs starting with \"/rss\" are Disallowed.  The correct algorithm is to take the longest rule that matches.  I will attach a patch that fixes this.",
        "Issue Links": []
    },
    "NUTCH-99": {
        "Key": "NUTCH-99",
        "Summary": "ports are hardcoded or random",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "29/Sep/05 22:21",
        "Updated": "15/Nov/05 07:40",
        "Resolved": "15/Nov/05 07:40",
        "Description": "Ports of tasktracker are random and the port of the datanode is hardcoded to 7000 as strting port.",
        "Issue Links": []
    },
    "NUTCH-100": {
        "Key": "NUTCH-100",
        "Summary": "New plugin urlfilter-db",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Gal Nitzan",
        "Created": "30/Sep/05 05:37",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "Hi,\nI have written a new plugin, based on the URLFilter interface: urlfilter-db .\nThe purpose of this plugin is to filter domains, i.e. I would like to crawl the world but to fetch only certain domains.\nThe plugin uses a caching system (SwarmCache, easier to deploy than JCS) and on the back-end a database.\nFor each url\n   filter is called\nend for\nfilter\n get the domain name from url\n  call cache.get domain\n  if not in cache try the database\n  if in database cache it and return it\n  return null\nend filter\nThe plugin reads the cache size, jdbc driver, connection string, table to use and domain field from nutch-site.xml",
        "Issue Links": []
    },
    "NUTCH-101": {
        "Key": "NUTCH-101",
        "Summary": "RobotRulesParser",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Fuad Efendi",
        "Created": "30/Sep/05 14:40",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "20/Jun/09 04:13",
        "Description": "I noticed this code in protocol-http & protocol-httpclient plugins:\n      } else if ( (line.length() >= 6)\n                  && (line.substring(0, 6).equalsIgnoreCase(\"Allow:\")) ) {\nHowever, according to the original 1994 protocol description, there is NO \"Allow:\" field. To allow, simply use \"Disallow:  \". http://www.robotstxt.org/wc/norobots.html\nPlease, try to test with www.newegg.com/robots.txt\n\ntheir site has this:\nUser-agent: *\nDisallow: \n\nAnd Nutch does not work with New Egg, but it should!\nSorry guys, I don't have enough time to double-ensure, could you please verify all this...\nI noticed strange discussion at nutch-agent:lucene.apache.org, it seems that we need to test ......./robots.txt\nUser-agent: ia_archiver\nDisallow: /\nUser-agent: Googlebot-Image\nDisallow: /\nUser-agent: Nutch\nDisallow: /\nUser-agent: TurnitinBot\nDisallow: /    \n\neverything according to standard protocol. Can you retest please whether it works with multiline? It's a standard!\n\nI see this in code:\n   StringTokenizer tok = new StringTokenizer(agentNames, \",\");\nComma separated? It's not accepted standard yet...\nSorry WebExpertsAmerica, I really didn't have any time to make any test...\nPlease do not execute tests against production sites.\nThanks!",
        "Issue Links": []
    },
    "NUTCH-102": {
        "Key": "NUTCH-102",
        "Summary": "jobtracker does not start when webapps is in src",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "30/Sep/05 22:42",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "19/Jan/06 07:03",
        "Description": "When starting the jobtracker from NUTCH_HOME by \nbin/nutch-daemon.sh start jobtracker\nThe jobtracker search for the webapps folder in NUTCH_HOME, but it is under src/\nWhen manually copy the webapps folder into NUTCH_HOME jobtracker starts without any problems. \nException in thread \"main\" java.lang.NullPointerException\n        at org.apache.nutch.mapred.JobTrackerInfoServer.<init>(JobTrackerInfoServer.java:67)\n        at org.apache.nutch.mapred.JobTracker.<init>(JobTracker.java:232)\n        at org.apache.nutch.mapred.JobTracker.startTracker(JobTracker.java:43)\n        at org.apache.nutch.mapred.JobTracker.main(JobTracker.java:1043)",
        "Issue Links": []
    },
    "NUTCH-103": {
        "Key": "NUTCH-103",
        "Summary": "Vivisimo like treeview and url redirect",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "robert benea",
        "Created": "05/Oct/05 00:48",
        "Updated": "01/Apr/11 14:28",
        "Resolved": "01/Apr/11 14:28",
        "Description": "First, I modified cluster.jsp and now the cluster has a vivisimo look. I used javascript to show the treeview.  Another small change is that I call the cluster recursively twice, so that two levels of clustering are shown.\nSecond, I added redirect.jsp in order to log the links that were clicked during search and because of that search.jsp is changed as well.\nThe code is not clean as all started as an experiment, I hope someone else finds it useful and clean it up . \nTo install it just copy the files where you deployed the nutch.war and will work auto-magically.\nRegards,\nR.",
        "Issue Links": [
            "/jira/browse/NUTCH-265"
        ]
    },
    "NUTCH-104": {
        "Key": "NUTCH-104",
        "Summary": "Nutch query parser does not support CJK bi-gram segmentation.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jack Tang",
        "Created": "06/Oct/05 03:12",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:29",
        "Description": "I customize one query filter using \"test\" as my field.  And when i try to search \"test:(c1)(c2)(c3)\", the query object which is generated by NutchAnalysis is wrong. Now the result is\n         test:(c1)(c2) [DEFAULT](c2)(c3).\nHowever, the expected result is\n         test:\"(c1)(c2) (c2)(c3)\".",
        "Issue Links": []
    },
    "NUTCH-105": {
        "Key": "NUTCH-105",
        "Summary": "Network error during robots.txt fetch causes file to be ignored",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "07/Oct/05 00:42",
        "Updated": "24/Sep/06 15:30",
        "Resolved": "19/Sep/06 16:08",
        "Description": "Earlier we had a small network glitch which prevented us from retrieving\nthe robots.txt file for a site we were crawling at the time:\n        nutch-root-tasktracker-sbider1.sitebuildit.com.log:051005 193021\n        task_m_h02y5t  Couldn't get robots.txt for\nhttp://www.japanesetranslator.co.uk/portfolio/:\n        org.apache.commons.httpclient.ConnectTimeoutException: The host\n        did not accept the connection within timeout of 10000 ms\n        nutch-root-tasktracker-sbider1.sitebuildit.com.log:051005 193031\n        task_m_h02y5t  Couldn't get robots.txt for\nhttp://www.japanesetranslator.co.uk/translation/:\n        org.apache.commons.httpclient.ConnectTimeoutException: The host\n        did not accept the connection within timeout of 10000 ms\nNutch then assumed that because we were unable to retrieve the file due\nto network issues, that it didn't exist and we could crawl the entire\nwebsite. Nutch then successfully grabbed a few pages which were listed\nin the robots.txt as being disallowed.\nI think Nutch should continue attempting to retrieve the robots.txt file\nuntil, at very least, we are able to establish a connection to the host,\notherwise the host should be ignored until the next round of fetches.\nThe webmaster of japanesetranslator.co.uk filed a complaint informing us\nof the issue.",
        "Issue Links": []
    },
    "NUTCH-106": {
        "Key": "HADOOP-19",
        "Summary": "Datanode corruption",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1.0",
        "Fix Version/s": "0.1.0",
        "Component/s": "None",
        "Assignee": "Doug Cutting",
        "Reporter": "Rod Taylor",
        "Created": "07/Oct/05 11:46",
        "Updated": "08/Jul/09 16:41",
        "Resolved": "29/Mar/06 03:16",
        "Description": "Our admins accidentally started a second nutch datanode pointing to the same directories as one already running (same machine) which in turn caused the entire contents of the datanode to go disappear.\nThis happened because the blocking was based on the username (since fixed in our start scripts) and it was started as two different users.\nThe ndfs.name.dir and ndfs.data.dir directories were both completely devoid of content, where they had about 150GB not all that much earlier.\nI think the solution is improved interlocking within the data directory itself (file locked with flock or something similar).",
        "Issue Links": [
            "/jira/browse/HADOOP-56"
        ]
    },
    "NUTCH-107": {
        "Key": "NUTCH-107",
        "Summary": "Typo in plugin/urlfilter-regex/plugin.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7.2,                                            0.8",
        "Component/s": "None",
        "Assignee": "Piotr Kosiorowski",
        "Reporter": "Stephen Cross",
        "Created": "07/Oct/05 11:53",
        "Updated": "12/Oct/05 04:49",
        "Resolved": "12/Oct/05 04:49",
        "Description": "There is a typo in the extension id definition of the urlfilter-regex plugin.xml\n   <extension id=\"org.apache.nutch.net.urlfiler\"\n              name=\"Nutch Regex URL Filter\"\n              point=\"org.apache.nutch.net.URLFilter\">\nshould be:\n   <extension id=\"org.apache.nutch.net.urlfilter\"\n              name=\"Nutch Regex URL Filter\"\n              point=\"org.apache.nutch.net.URLFilter\">\nNote that  'urlfiler' should be 'urlfilter'",
        "Issue Links": []
    },
    "NUTCH-108": {
        "Key": "NUTCH-108",
        "Summary": "tasktracker crashs when reconnecting to a new jobtracker.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "09/Oct/05 19:44",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "06/Jan/06 05:14",
        "Description": "051008 213532 Lost connection to JobTracker [/192.168.200.100:7020].  Retrying...\n051008 213537 Client connection to 192.168.200.100:7020: starting\n051008 213537 Client connection to 192.168.200.105:7030: closing\n051008 213537 Server connection on port 7030 from 192.168.200.105: exiting\n051008 213537 Server connection on port 7030 from 192.168.200.102: exiting\n051008 213537 Client connection to 192.168.200.102:7030: closing\n051008 213537 task_m_1iswra done; removing files.\n051008 213537 Server connection on port 7030 from 192.168.200.101: exiting\n051008 213537 Client connection to 192.168.200.101:7030: closing\nException in thread \"main\" java.util.ConcurrentModificationException\n        at java.util.TreeMap$EntryIterator.nextEntry(TreeMap.java:1026)\n        at java.util.TreeMap$ValueIterator.next(TreeMap.java:1057)\n        at org.apache.nutch.mapred.TaskTracker.close(TaskTracker.java:134)\n        at org.apache.nutch.mapred.TaskTracker.run(TaskTracker.java:285)\n        at org.apache.nutch.mapred.TaskTracker.main(TaskTracker.java:629)",
        "Issue Links": []
    },
    "NUTCH-109": {
        "Key": "NUTCH-109",
        "Summary": "Nutch - Fetcher - Performance Test - new Protocol-HTTPClient-Innovation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Fuad Efendi",
        "Created": "11/Oct/05 09:01",
        "Updated": "10/Nov/05 05:01",
        "Resolved": "10/Nov/05 05:01",
        "Description": "1. TCP connection costs a lot, not only for Nutch and end-point web servers, but also for intermediary network equipment \n2. Web Server creates Client thread and hopes that Nutch really uses HTTP/1.1, or at least Nutch sends \"Connection: close\" before closing in JVM \"Socket.close()\" ...\nI need to perform very objective tests, probably 2-3 days; new plugin crawled/parsed 23,000 pages for 1,321 seconds; it seems that existing http-plugin needs few days...\nI am using separate network segment with Windows XP (Nutch), and Suse Linux (Apache HTTPD + 120,000 pages)\nPlease find attached new plugin based on http://www.innovation.ch/java/HTTPClient/\nPlease note: \nClass HttpFactory contains cache of HTTPConnection objects; each object run each thread; each object is absolutely thread-safe, so we can send multiple GET requests using single instance:\n   private static int CLIENTS_PER_HOST = NutchConf.get().getInt(\"http.clients.per.host\", 3);\nI'll add more comments after finishing tests...",
        "Issue Links": []
    },
    "NUTCH-110": {
        "Key": "NUTCH-110",
        "Summary": "OpenSearchServlet outputs illegal xml characters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Michael Stack",
        "Created": "13/Oct/05 09:13",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "21/Jun/06 02:12",
        "Description": "OpenSearchServlet does not check text-to-output for illegal xml characters; dependent on  search result, its possible for OSS to output xml that is not well-formed.  For example, if text has the character FF character in it \u2013 -- i.e. the ascii character at position (decimal) 12 \u2013  the produced XML will show the FF character as '\f' The character/entity '\f' is not legal in XML according to http://www.w3.org/TR/2000/REC-xml-20001006#NT-Char.",
        "Issue Links": []
    },
    "NUTCH-111": {
        "Key": "HADOOP-10",
        "Summary": "ndfs.replication is not documented within the nutch-default.xml configuration file.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "14/Oct/05 06:02",
        "Updated": "08/Jul/09 16:41",
        "Resolved": "07/Feb/06 04:15",
        "Description": "ndfs.replication is not documented within the nutch-default.xml configuration file.",
        "Issue Links": []
    },
    "NUTCH-112": {
        "Key": "NUTCH-112",
        "Summary": "Link in cached.jsp page to cached content is an absolute link",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.7.2,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "16/Oct/05 02:42",
        "Updated": "07/Dec/05 07:03",
        "Resolved": "07/Dec/05 07:03",
        "Description": "The link in the cached.jsp page that points to the cached content uses an absolute link, of the form \"/servlet/cached?idx=xxx&id=yyy\". This causes an error when the user goes to click on the link and the Nutch war is not deployed at the root context of the application server. The link should be of the form \"./servlet/cached?idx=xxx&id=yyy\", i.e., a relative link to correct this problem.\nI've attached a small patch that fixes the error. I've tested the patch in my local environment and it fixes the error.",
        "Issue Links": []
    },
    "NUTCH-113": {
        "Key": "NUTCH-113",
        "Summary": "Disable permanent DNS-to-IP caching for JVM 1.4",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.2,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Fuad Efendi",
        "Created": "16/Oct/05 04:24",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "DNS-to-IP mapping may change during long crawls, by default JVM 1.4 caches it forever.\nSome related discussions at Jakarta-HttpClient-User\nhttp://mail-archives.apache.org/mod_mbox/jakarta-httpclient-user/200506.mbox/%3c20050627022440.SVIL13442.lakermmtao05.cox.net@zeus%3e\nhttp://java.sun.com/j2se/1.4.2/docs/guide/net/properties.html\n   networkaddress.cache.ttl (default: -1) \n   Specified in java.security to indicate the caching policy for successful name lookups from the name service.. The value is specified as as integer to indicate the number of seconds to cache the successful lookup. \n   A value of -1 indicates \"cache forever\". \nWe probably need this code in org.apache.nutch.fetcher.Fetcher:\n  private static final int FETCHER_DNS_TTL_MINUTES =\n    NutchConf.get().getInt(\"fetcher.dns.ttl.minutes\", 120);\n  static \n{\n    java.security.Security.setProperty(\"networkaddress.cache.ttl\", \"\" + FETCHER_DNS_TTL_MINUTES*60);\n  }\n\n\nAnd, new property in nutch-default.xml:\n<property>\n  <name>fetcher.dns.ttl.minutes</name>\n  <value>120</value>\n  <description>DNS-to-IP cache, Time-to-Live</description>\n</property>",
        "Issue Links": []
    },
    "NUTCH-114": {
        "Key": "NUTCH-114",
        "Summary": "getting number of urls and links from crawldb",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "16/Oct/05 14:14",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "02/Dec/05 17:45",
        "Description": "We need a tool that provide basic statistics about the crawldb.",
        "Issue Links": []
    },
    "NUTCH-115": {
        "Key": "NUTCH-115",
        "Summary": "jobtracker.jsp shows too much information",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "19/Oct/05 00:41",
        "Updated": "22/Jan/08 14:21",
        "Resolved": "22/Jan/08 14:21",
        "Description": "We've been working for several days without any problems or restarts required. The jobtracker.jsp page now has several megabytes worth of information being displayed.\nJobs within the jobtracker daemon should be completely removed from the jobs TreeMap after some time has past (say 48 hours) to prevent unbounded growth.",
        "Issue Links": []
    },
    "NUTCH-116": {
        "Key": "NUTCH-116",
        "Summary": "TestNDFS a JUnit test specifically for NDFS",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher,                                            indexer",
        "Assignee": null,
        "Reporter": "Paul Baclace",
        "Created": "19/Oct/05 11:16",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "02/Dec/05 05:29",
        "Description": "TestNDFS is a JUnit test for NDFS using \"pseudo multiprocessing\" (or more strictly, pseudo distributed) meaning all daemons run in one process and sockets are used to communicate between daemons.  \nThe test permutes various block sizes, number of files, file sizes, and number of datanodes.  After creating 1 or more files and filling them with random data, one datanode is shutdown, and then the files are verfified. Next, all the random test files are deleted and we test for leakage (non-deletion) by directly checking the real directories corresponding to the datanodes still running.",
        "Issue Links": []
    },
    "NUTCH-117": {
        "Key": "NUTCH-117",
        "Summary": "Crawl crashes with java.io.IOException: already exists: C:\\nutch\\crawl.intranet\\oct18\\db\\webdb.new\\pagesByURL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1",
        "Fix Version/s": "0.7.2",
        "Component/s": "None",
        "Assignee": "Piotr Kosiorowski",
        "Reporter": "Stephen Cross",
        "Created": "19/Oct/05 21:42",
        "Updated": "26/Mar/06 04:23",
        "Resolved": "26/Mar/06 04:23",
        "Description": "I started a crawl using the command line using nutch 0.7.1.\nnutch-daemon.sh start crawl urls.txt -dir oct18 -threads 4 -depth 20\nAfter crawling for over 15 hours the crawl crached with the following exception:\n051019 050543 status: segment 20051019050438, 30 pages, 0 errors, 1589818 bytes, 48020 ms\n051019 050543 status: 0.6247397 pages/s, 258.65167 kb/s, 52993.934 bytes/page\n051019 050544 Updating C:\\nutch\\crawl.intranet\\oct18\\db\n051019 050544 Updating for C:\\nutch\\crawl.intranet\\oct18\\segments\\20051019050438\n051019 050544 Processing document 0\n051019 050544 Finishing update\n051019 050544 Processing pagesByURL: Sorted 47 instructions in 0.02 seconds.\n051019 050544 Processing pagesByURL: Sorted 2350.0 instructions/second\nException in thread \"main\" java.io.IOException: already exists: C:\\nutch\\crawl.intranet\\oct18\\db\\webdb.new\\pagesByURL\n        at org.apache.nutch.io.MapFile$Writer.<init>(MapFile.java:86)\n        at org.apache.nutch.db.WebDBWriter$CloseProcessor.closeDown(WebDBWriter.java:549)\n        at org.apache.nutch.db.WebDBWriter.close(WebDBWriter.java:1544)\n        at org.apache.nutch.tools.UpdateDatabaseTool.close(UpdateDatabaseTool.java:321)\n        at org.apache.nutch.tools.UpdateDatabaseTool.main(UpdateDatabaseTool.java:371)\n        at org.apache.nutch.tools.CrawlTool.main(CrawlTool.java:141)\nThis was on the 14th segement from the requested depth of 20. Doing a quick Google on the exception brings up a few previous posts with the same error but no definitive answer, seems to have been occuring since nutch 0.6.",
        "Issue Links": []
    },
    "NUTCH-118": {
        "Key": "NUTCH-118",
        "Summary": "FAQ link points to invalid URL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Steve Betts",
        "Created": "21/Oct/05 00:17",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "15/Feb/06 01:00",
        "Description": "The default FAQ link on the search.asp page points to  http://www.nutch.org/faq.html. That page works, but redirects to http://lucene.apache.org/nutch/faq.html. This returns a 404, and should instead point to http://wiki.apache.org/nutch/FAQ (I think).",
        "Issue Links": []
    },
    "NUTCH-119": {
        "Key": "NUTCH-119",
        "Summary": "Regexp to extract outlinks incorrect",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1,                                            0.7.2,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "S\u00e9bastien Le Callonnec",
        "Created": "21/Oct/05 04:31",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "The regexp which extracts outlinks is incorrect.  It extracts in-line CSS styles, and leaves out link such as \"<a\n> href=/sitemap.html>browse</a>\".  This has been reported by Earl Cahill on the user list.",
        "Issue Links": []
    },
    "NUTCH-120": {
        "Key": "NUTCH-120",
        "Summary": "one \"bad\" link on a page kills parsing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Earl Cahill",
        "Created": "21/Oct/05 04:39",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 14:55",
        "Description": "Since the try in src/java/org/apache/nutch/parse/OutlinkExtractor.java, getOutlinks method loops around the whole\nwhile (matcher.contains(input, pattern)) {\n...\n}\nloop, if one url causes an exception, no more links will be extracted.",
        "Issue Links": []
    },
    "NUTCH-121": {
        "Key": "NUTCH-121",
        "Summary": "SegmentReader for mapred",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Rod Taylor",
        "Created": "21/Oct/05 10:34",
        "Updated": "30/Dec/05 03:37",
        "Resolved": "30/Dec/05 03:37",
        "Description": "The attached segment reader will dump segments approximately the same as 0.7 did with the -dump flag.\nChanges in the structure of the Data objects themselves causes slightly different output to be generated.",
        "Issue Links": []
    },
    "NUTCH-122": {
        "Key": "NUTCH-122",
        "Summary": "block numbers need a better random number generator",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            indexer",
        "Assignee": null,
        "Reporter": "Paul Baclace",
        "Created": "21/Oct/05 13:26",
        "Updated": "11/Feb/22 02:46",
        "Resolved": "15/Jun/06 22:01",
        "Description": "In order to support billions of block numbers, a better PRNG than java.util.Random is needed.  To reach billions with low probability of collision, 64 bit random numbers are needed (the Birthday Problem is the model for the number of bits needed; the result is that twice as many bits are needed as the number of bits to count the expected number of items.) The built-in java.util.Random keeps only 48 bits of state which is only sufficient for 2^24 items.  Using repeated calls to or more than one instance of Random does not increase its total entropy.  \n  Analysis\n    util.Random is a linear congruential generator (LCG) identical to drand48.\n      util.Random keeps 48 bits of state and gangs together 2 consecutive values to return 64 bit values.\n      LCGs suffer from periodicity in the low order bits which would make modular binning less than random.\n        \"low order bits\" could mean least significant byte.\n      LCGs have periods in the range 106 to 109 when using 32 bit words, a range of poor to fair.\n      seed = ( 0x5DEECE66DL * seed + 0xBL ) & ((1L << 48) - 1);\n        the origin of 0x5DEECE66D, a non-prime, is shrouded in the mists of time.\n      Results of the Birthday Spacings Test look good.\n     References\nhttp://www.math.utah.edu/~beebe/java/random/README\nhttp://www.pierssen.com/arcview/upload/esoterica/randomizer.html\nRecommended alternative:    MersenneTwister\n      Matsumoto and Nishimura (1998).\n      Longest period of any known generator 2^19937 or about 10^6001.\n      A period that exceeds the number of unique values seems ideal; obviously a shorter period than the number of unique values (like util.Random)  is a problem).\n      Faster than java.util.Random (Random was recent tweaked, however).\n      Excellent result for Diehard Birthday Spacings Test.\n      Can be seeded with up to 624 32 bit integers.\nDoug Cutting wrote on nutch-dev:\n> It just occurred to me that perhaps we could simply use sequential block numbering. \n>  All block ids are generated centrally on the namenode.  \nResponse from Paul Baclace:\nI'm not sure what the advantage of sequential block numbers would be\nsince long period PRNG block numbering does not even need to store\nit's state, just pick a new starting place.\nSequential block numbering does have the downside that picking a datanode based on (BlockNum % DataNodeCount) would devolve into round robin.  Any attempt to pass the sequence through a hash ends up becoming a random number generator.\nSequential numbering provides contiguous numbers, but after G.C. that would be lost, so no advantage there.\nWhen human beings eyeball block numbers, many with small differences are more likely to be misread than many that are totally different.\nIf block numbering is sequential, then there is a temptation to use 32 bits instead of 64, but 32 bits leads to wrap-around and uh oh. \nFSNamesystem uses Random to help pick a target datanode, but it could just use the randomness of block numbers.",
        "Issue Links": []
    },
    "NUTCH-123": {
        "Key": "NUTCH-123",
        "Summary": "Cache.jsp some times generate NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "YourSoft",
        "Created": "05/Nov/05 02:24",
        "Updated": "15/Feb/06 01:09",
        "Resolved": "15/Feb/06 01:09",
        "Description": "There is a problem with the following line in the cached.jsp:\n  String contentType = (String) metaData.get(\"Content-Type\");\nIn the segments data there is some times not equals \"Content-Type\", there are \"content-type\" or \"Content-type\" etc.\nThe solution, insert these lines over the above line:\nfor (Enumeration eNum = metaData.propertyNames(); eNum.hasMoreElements() {\n\tcontent = (String) eNum.nextElement();\n\tif (\"content-type\".equalsIgnoreCase (content)) \n{\n\t\tbreak;\n\t}\n}\nfinal String contentType = (String) metaData.get(content);\nRegards,\n                Ferenc",
        "Issue Links": []
    },
    "NUTCH-124": {
        "Key": "NUTCH-124",
        "Summary": "protocol-httpclient does not follow redirects when fetching robots.txt",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.2,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Doug Cutting",
        "Created": "05/Nov/05 12:37",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "08/Nov/05 03:15",
        "Description": "If a site's robots.txt redirects, protocol-httpclient does not correctly fetch the robots.txt and effectively ignores it for the site.  See http://www.webmasterworld.com/forum11/3008.htm.",
        "Issue Links": []
    },
    "NUTCH-125": {
        "Key": "NUTCH-125",
        "Summary": "OpenOffice Parser plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "18/Nov/05 07:31",
        "Updated": "26/Apr/06 02:13",
        "Resolved": "26/Apr/06 02:13",
        "Description": "A simple parser for StarOffice SXW and OpenDocument ODT files. This plugin does not use the UNO bridge in OpenOffice , but rather uses standard ZipInputStream, and parses content.xml and meta.xml inside these files to extract metadata and plain text.\nThis plugin uses dom4j, because of easy XPath node selection, but this dependency could be removed.",
        "Issue Links": []
    },
    "NUTCH-126": {
        "Key": "NUTCH-126",
        "Summary": "Fetching via https does not work with a proxy (patch)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Fritz Elfert",
        "Created": "18/Nov/05 18:12",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "15/Mar/08 00:16",
        "Description": "Trying to fetch content from an SSL-Server using a proxy does not work due to a bug in the protocol-httpclient plugin.\nThe attached patch fixes this problem.\nCiao\n -Fritz",
        "Issue Links": []
    },
    "NUTCH-127": {
        "Key": "NUTCH-127",
        "Summary": "uncorrect values using -du, or ls does not return items",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.2,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "19/Nov/05 00:36",
        "Updated": "08/Jun/11 21:32",
        "Resolved": "24/Jan/06 03:47",
        "Description": "The ndfs client return uncorrect values by using du or ls does not return items.\nIt looks like there is a problem with the virtual file strcuture, since -du only reads the meta data, isn't it?\nWe had moved some data from folder to folder and after that we notice that a folder with zero items has a size.\n[nutch@myservernutch-0.8-dev]$ bin/nutch ndfs -du indexes/\n051118 092409 parsing file:/home/nutch/nutch-0.8-dev/conf/nutch-default.xml\n051118 092409 parsing file:/home/nutch/nutch-0.8-dev/conf/nutch-site.xml\n051118 092409 No FS indicated, using default:192.168.200.3:50000\n051118 092409 Client connection to 192.168.200.3:50000: starting\nFound 1 items\n/user/nutch/indexes/20051022033721      974606348\n[nutch@myservernutch-0.8-dev]$ bin/nutch ndfs -du indexes/20051022033721/\n051118 092416 parsing file:/home/nutch/nutch-0.8-dev/conf/nutch-default.xml\n051118 092416 parsing file:/home/nutch/nutch-0.8-dev/conf/nutch-site.xml\n051118 092416 No FS indicated, using default:192.168.200.3:50000\n051118 092416 Client connection to 192.168.200.3:50000: starting\nFound 0 items\n[nutch@myservernutch-0.8-dev]$ bin/nutch ndfs -ls indexes/20051022033721\n051118 093331 parsing file:/home/nutch/nutch-0.8-dev/conf/nutch-default.xml\n051118 093332 parsing file:/home/nutch/nutch-0.8-dev/conf/nutch-site.xml\n051118 093332 No FS indicated, using default:192.168.200.3:50000\n051118 093332 Client connection to 192.168.200.3:50000: starting\nFound 0 items\nSo may the mv tool has a problem, the du or the ls tool. :-O Any ideas where to search for the problem? Dubugging ndfs is tricky.",
        "Issue Links": []
    },
    "NUTCH-128": {
        "Key": "NUTCH-128",
        "Summary": "second configuration nodes overwrites first node",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "25/Nov/05 00:12",
        "Updated": "22/Jan/08 14:22",
        "Resolved": "22/Jan/08 14:21",
        "Description": "It could happen that there are two identically key definitions in one nutch-default.xml or nutch-site.xml, in such a case the second values overwrites the first value of the same  configuration file. May we can at least log a warning.",
        "Issue Links": []
    },
    "NUTCH-129": {
        "Key": "NUTCH-129",
        "Summary": "rtf-parser does not work when opened with wordpad files and saved",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "raghavendra prabhu",
        "Created": "25/Nov/05 21:56",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "The above thing failed as wordpad seems to rewrite control information \nCant we use RTFEdit kit to do the parser and it will be not a LGPL issue also",
        "Issue Links": []
    },
    "NUTCH-130": {
        "Key": "NUTCH-130",
        "Summary": "Be explicit about target JVM when building (1.4.x?)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Doug Cutting",
        "Reporter": "Michael Stack",
        "Created": "30/Nov/05 03:43",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "02/Dec/05 03:25",
        "Description": "Below is patch for nutch build.xml.  It stipulates the target JVM is 1.4.x.  Without explicit target, a nutch built with 1.5.x java defaults to a 1.5.x java target and won't run in a 1.4.x JVM.  Can be annoying (From the ant javac doc, regards the target attribute: \"We highly recommend to always specify this attribute.\").\n[debord 282] nutch > svn diff -u build.xml\nSubcommand 'diff' doesn't accept option '-u [--show-updates]'\nType 'svn help diff' for usage.\n[debord 283] nutch > svn diff build.xml\nIndex: build.xml\n===================================================================\n\u2014 build.xml   (revision 349779)\n+++ build.xml   (working copy)\n@@ -72,6 +72,8 @@\n      destdir=\"${build.classes}\"\n      debug=\"${debug}\"\n      optimize=\"${optimize}\"\n+     target=\"1.4\"\n+     source=\"1.4\"\n      deprecation=\"${deprecation}\">\n       <classpath refid=\"classpath\"/>\n     </javac>",
        "Issue Links": []
    },
    "NUTCH-131": {
        "Key": "NUTCH-131",
        "Summary": "Non-documented variable: mapred.child.heap.size",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "04/Dec/05 04:37",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "06/Jan/06 06:08",
        "Description": "Got complaints about lack of heap space. Seems it was the children out of room for reduce of a updatedb.",
        "Issue Links": []
    },
    "NUTCH-132": {
        "Key": "NUTCH-132",
        "Summary": "Add ability to sort on more than one column",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "James Nelson",
        "Created": "07/Dec/05 06:27",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:29",
        "Description": "Currently nutch only allows sorting to be specified by passing in a string of the field name to be sorted on, a null string results in sorting by score. I'd like to be able to sort on multiple fields, including score instead of being restricted to just one.",
        "Issue Links": []
    },
    "NUTCH-133": {
        "Key": "NUTCH-133",
        "Summary": "ParserFactory does not work as expected",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1,                                            0.7.2,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "07/Dec/05 07:04",
        "Updated": "09/Dec/05 05:58",
        "Resolved": "08/Dec/05 20:05",
        "Description": "Marcel Schnippe detect a set of problems until working with different content and parser types, we worked together to identify the problem source.\nFrom our point of view this described problems could be the source for many other problems daily described in the mailing lists.\nFind a conclusion of the problems below.\nProblem:\nSome servers returns mixed case but correct header keys like 'Content-type' or 'content-Length'  in the http response header.\nThat's why for example a get(\"Content-Type\") fails and a page is detected as zip using the magic content type detection mechanism. \nAlso we note that this a common reason why pdf parsing fails since Content-Length does return the correct value. \nSample:\nreturns \"text/HTML\" or \"application/PDF\" or Content-length\nor this url:\nhttp://www.lanka.info/dictionary/EnglishToSinhala.jsp\nSolution:\nFirst just write only lower case keys into the properties and later convert all keys that are used to query the metadata to lower case as well.\ne.g.:\nHttpResponse.java, line 353:\nuse lower cases here and for all keys used to query header properties (also content-length) change:  String key = line.substring(0, colonIndex); to  String key = line.substring(0, colonIndex) .toLowerCase();\nProblem:\nMimeTypes based discovery (magic and url based) is only done in case the content type was not delivered by the web server, this happens not that often, mostly this was a problem with mixed case keys in the header.\nsee:\n public Content toContent() {\n    String contentType = getHeader(\"Content-Type\");\n    if (contentType == null) {\n      MimeType type = null;\n      if (MAGIC) \n{\n        type = MIME.getMimeType(orig, content);\n      }\n else \n{\n        type = MIME.getMimeType(orig);\n      }\n      if (type != null) \n{\n          contentType = type.getName();\n      }\n else \n{\n          contentType = \"\";\n      }\n    }\n    return new Content(orig, base, content, contentType, headers);\n  }\nSolution:\nUse the content-type information as it is from the webserver and move the content type discovering from Protocol plugins to the Component where the parsing is done - to the ParseFactory.\nThan just create a list of parsers for the content type returned by the server and the custom detected content type. In the end we can iterate over all parser until we got a successfully parsed status.\nProblem:\nContent will be parsed also if the protocol reports a exception and has a non successful status, in such a case the content is new byte[0] in any case.\nSolution:\nFetcher.java, line 243.\nChange:   if (!Fetcher.this.parsing ) { .. to \n if (!Fetcher.this.parsing || !protocolStatus.isSuccess()) \n{\n       // TODO we may should not write out here emthy parse text and parse date, i suggest give outputpage a parameter parsed true / false\n          outputPage(new FetcherOutput(fle, hash, protocolStatus),\n                content, new ParseText(\"\"),\n                new ParseData(new ParseStatus(ParseStatus.NOTPARSED), \"\", new Outlink[0], new Properties()));\n        return null;\n      }\n\n\nProblem:\nActually the configuration of parser is done based on plugin id's, but one plugin can have several extentions, so  normally a plugin can provide several parser, but this is no limited just wrong values are used in the configuration process. \nSolution:\nChange plugin id to  extension id in the parser configuration file and also change this code in the parser factory to use extension id's everywhere.\nProblem:\nthere is not a clear differentiation between content type and mime type. \nI'm notice that some plugins call metaData.get(\"Content-Type) or content.getContentType();\nActually in theory this can return different values, since the content type could be detected by the MimesTypes util and is not the same as delivered in the http response header.\nAs mentioned actually content type is only detected by the MimeTypes util in case the header does not contains any content type informations or had problems with mixed case keys.\nSolution:\nTake the content type property out of the meta data and clearly restrict the access of this meta data into the own getter method.\nProblem:\nMost protocol plugins  checking if content type is null only in this case the MimeTypes util is used. Since my patch move the mime type detection to the parser factory - where from my point of view - is the right place, it is now unneccary code we can remove from the protocol plugins. I never found a case where no content type was returned just mixed case keys was used. \nSolution. \nRemove this detection code, since it is now in the parser factory.\nI didn't change this since more code I  change, I guess there is a  less chance to get the patch into the sources, I suggest we open a low priority issue and once we change the plugins we can remove it.\nProblem:\nThis is not a problem, but a 'code smells' (Martin Fowler) There are empty test methods in TestMimeType\n  /** Test of <code>getExtensions</code> method. */\n    public void testGetExtensions() {\n    }\nSolution:\nImplement these tests or remove the test methods.",
        "Issue Links": []
    },
    "NUTCH-134": {
        "Key": "NUTCH-134",
        "Summary": "Summarizer doesn't select the best snippets",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.7.1,                                            0.7.2,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Andrzej Bialecki",
        "Created": "07/Dec/05 23:10",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "13/May/06 15:53",
        "Description": "Summarizer.java tries to select the best fragments from the input text, where the frequency of query terms is the highest. However, the logic in line 223 is flawed in that the excerptSet.add() operation will add new excerpts only if they are not already present - the test is performed using the Comparator that compares only the numUniqueTokens. This means that if there are two or more excerpts, which score equally high, only the first of them will be retained, and the rest of equally-scoring excerpts will be discarded, in favor of other excerpts (possibly lower scoring).\nTo fix this the Set should be replaced with a List + a sort operation. To keep the relative position of excerpts in the original order the Excerpt class should be extended with an \"int order\" field, and the collected excerpts should be sorted in that order prior to adding them to the summary.",
        "Issue Links": [
            "/jira/browse/NUTCH-257",
            "/jira/browse/NUTCH-262"
        ]
    },
    "NUTCH-135": {
        "Key": "NUTCH-135",
        "Summary": "http header meta data are case insensitive in the real world (e.g. Content-Type or content-type)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.7.1",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "10/Dec/05 05:49",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "11/Dec/05 09:44",
        "Description": "As described in issue nutch-133, some webservers return http header meta data not standard conform case insensitive.\nThis provides many negative side effects, for example query thet content type from the meta data return null also in case the webserver returns a content type, but the key is not standard conform e.g. lower case. Also this has effects to the pdf parser that queries the content length etc.",
        "Issue Links": []
    },
    "NUTCH-136": {
        "Key": "NUTCH-136",
        "Summary": "mapreduce segment generator generates  50 % less  than excepted urls",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "13/Dec/05 07:50",
        "Updated": "25/Jan/06 07:20",
        "Resolved": "25/Jan/06 07:20",
        "Description": "We notice that segments generated with the map reduce segment generator contains only 50 % of the expected urls. We had a crawldb with 40 000 urls and the generate commands only created a 20 000 pages segment. This also happened with the topN parameter, we everytime got around 50 % of the expected urls.\nI tested the PartitionUrlByHost and it looks like it does its work. However we fixed the problem by changing two things:\nFirst we set the partition to a normal hashPartitioner.\nSecond we changed Generator.java line 48:\nlimit = job.getLong(\"crawl.topN\",Long.MAX_VALUE)/job.getNumReduceTasks();\nto:\nlimit = job.getLong(\"crawl.topN\",Long.MAX_VALUE);\nNow it works as expected. \nHas anyone a idea what the real source of this problem can be?\nIn general this is bug has the effect that all map reduce users fetch only 50 % of it's urls per iteration.",
        "Issue Links": []
    },
    "NUTCH-137": {
        "Key": "NUTCH-137",
        "Summary": "footer is not displayed in search result page",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Kuro Kurosaka",
        "Created": "14/Dec/05 07:54",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "15/Feb/06 00:58",
        "Description": "I installed nutch.war under webapps/nutch, not webapps/TOMCAT as written in the tutorial.\nThen I visited nutch with URL http://localhost:8080/nutch/en/search.html\nand did some search.\nAt the bottom of the search result page, I see an error message:\nThe requested resource (/nutch/en/include/footer.html) is not available\nThe reason I didn't start from http://localhost:8080/nutch/ was because it showed a lots of broken images.\nI applied the proposed patch that I've found in this bug report:\nhttp://issues.apache.org/jira/browse/NUTCH-81\nThis fixed the broken links, and I could start from http://localhost:8080/nutch/ but the footer is not still displayed.\nIs this perhaps related to the fact that my language setting is Japanese? (But this shouldn't matter because Tomcat is running as a service which my account setting should not affect.)",
        "Issue Links": []
    },
    "NUTCH-138": {
        "Key": "NUTCH-138",
        "Summary": "non-Latin-1 characters cannot be submitted for search",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Kuro Kurosaka",
        "Created": "14/Dec/05 08:05",
        "Updated": "03/Jan/06 05:51",
        "Resolved": "03/Jan/06 05:06",
        "Description": "The search.html currently specifies GET method for query submission.\nTomcat 5.x only allows ISO-8859-1 (aka Latin-1) code set to be submitted over GET because of some restrictions of HTML or HTTP spec they discovered. (If my memory is correct, non ISO-8859-1 characters were woking OK over GET with older versions of Tomcat as far as setCharacterEncoding() is called properly.)\nTo allow proper transmission of non-ISO-8859-1, POST method should be used.  Here's a proposed patch:\n\n\n\n\n\nsearch.html\tTue Dec 13 15:02:15 2005\n\n\nsearch-org.html\tTue Dec 13 15:02:07 2005\n***************\n\n\n59,65 ****\n  </span><span class=\"bodytext\">\n  <center>\n\n\n\n\n\n! <form name=\"search\" action=\"../search.jsp\" method=\"post\"> \n  <input name=\"query\" size=\"44\">\u00a0<input type=\"submit\" value=\"Search\">\n  <a href=\"help.html\">help</a>\n\u2014 59,65 ----\n  </span><span class=\"bodytext\">\n  <center>\n! <form name=\"search\" action=\"../search.jsp\" method=\"get\"> \n  <input name=\"query\" size=\"44\">\u00a0<input type=\"submit\" value=\"Search\">\n  <a href=\"help.html\">help</a>\nBTW, I am aware that Nutch and Lucene won't hanlde non Western languages well as packaged.",
        "Issue Links": []
    },
    "NUTCH-139": {
        "Key": "NUTCH-139",
        "Summary": "Standard metadata property names in the ParseData metadata",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.7.2,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "14/Dec/05 13:02",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "09/Feb/06 06:51",
        "Description": "Currently, people are free to name their string-based properties anything that they want, such as having names of \"Content-type\", \"content-TyPe\", \"CONTENT_TYPE\" all having the same meaning. Stefan G. I believe proposed a solution in which all property names be converted to lower case, but in essence this really only fixes half the problem right (the case of identifying that \"CONTENT_TYPE\"\nand \"conTeNT_TyPE\" and all the permutations are really the same). What about\nif I named it \"Content     Type\", or \"ContentType\"?\n I propose that a way to correct this would be to create a standard set of named Strings in the ParseData class that the protocol framework and the parsing framework could use to identify common properties such as \"Content-type\", \"Creator\", \"Language\", etc.\n The properties would be defined at the top of the ParseData class, something like:\n public class ParseData\n{\n\n   .....\n\n    public static final String CONTENT_TYPE = \"content-type\";\n    public static final String CREATOR = \"creator\";\n\n   ....\n\n}\n\n\nIn this fashion, users could at least know what the name of the standard properties that they can obtain from the ParseData are, for example by making a call to ParseData.getMetadata().get(ParseData.CONTENT_TYPE) to get the content type or a call to ParseData.getMetadata().set(ParseData.CONTENT_TYPE, \"text/xml\"); Of course, this wouldn't preclude users from doing what they are currently doing, it would just provide a standard method of obtaining some of the more common, critical metadata without pouring over the code base to figure out what they are named.\nI'll contribute a patch near the end of the this week, or beg. of next week that addresses this issue.",
        "Issue Links": []
    },
    "NUTCH-140": {
        "Key": "NUTCH-140",
        "Summary": "Add alias capability in parse-plugins.xml file that allows mimeType->extensionId mapping",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "14/Dec/05 13:10",
        "Updated": "21/Feb/06 19:01",
        "Resolved": "21/Feb/06 19:01",
        "Description": "Jerome and I have been talking about an idea to address the current issue raised by Stefan G. about having a mapping of mimeType->list of pluginIds rather than mimeType->list of extensionIds in the parse-plugins.xml file. We've come up with the following proposed update that would seemingly fix this problem.\n  We propose to have the concept of \"aliases\" in the parse-plugins.xml file, defined at the end of the file, something lie:\n <parse-plugins>\n    ....\n   <mimeType name=\"text/html\">\n      <plugin id=\"parse-html\"/>\n   </mimeType>\n    .....\n   <aliases>\n   <alias name=\"parse-html\"\nextension-point=\"org.apache.nutch.parse.html.HtmlParser\"/>\n   ....\n   <alias name=\"parse-html2\" extension-point=\"my.other.html.Parser\"/>\n   ....\n   </aliases>\n</parse-plugins>\nWhat do you guys think? This approach would be flexible enough to allow the mapping of extensionIds to mimeTypes, but without impacting the current \"pluginId\" concept.\nComments welcome.",
        "Issue Links": []
    },
    "NUTCH-141": {
        "Key": "NUTCH-141",
        "Summary": "jobdetails.jsp doesnt work on webbrowser \"safari\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.7.2,                                            0.8",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Marko Bauhardt",
        "Created": "14/Dec/05 19:52",
        "Updated": "15/Dec/05 06:33",
        "Resolved": "15/Dec/05 06:33",
        "Description": "line 21: <title Nutch MapReduce Job Details</title>\nThe title tag are invalid. Missing brace.",
        "Issue Links": []
    },
    "NUTCH-142": {
        "Key": "NUTCH-142",
        "Summary": "NutchConf should use the thread context classloader",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7",
        "Fix Version/s": "0.7.2,                                            0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Mike Cannon-Brookes",
        "Created": "16/Dec/05 07:21",
        "Updated": "05/Jan/06 05:37",
        "Resolved": "05/Jan/06 05:37",
        "Description": "Right now NutchConf uses it's own static classloader which is evil in a J2EE scenario.\nThis is simply fixed. Line 52:\n   private ClassLoader classLoader = NutchConf.class.getClassLoader();\nShould be:\n   private ClassLoader classLoader = Thread.currentThread().getContextClassLoader();\nThis means no matter where Nutch classes are loaded from, it will use the correct J2EE classloader to try to find configuration files (ie from WEB-INF/classes).",
        "Issue Links": []
    },
    "NUTCH-143": {
        "Key": "NUTCH-143",
        "Summary": "Improper error numbers returned on exit",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "16/Dec/05 14:25",
        "Updated": "30/Aug/06 22:16",
        "Resolved": "30/Aug/06 22:16",
        "Description": "Nutch does not obey standard command line error numbers which can make it difficult to script around commands.\nBoth of the below should have exited with an error number larger than 0 causing the shell script to enter into the 'Failed' case.\nbash-3.00$ /opt/nutch/bin/nutch updatedb && echo \"==>Success\" || echo \"==>Failed\"\nUsage: <crawldb> <segment>\n==>Success\nbash-3.00$ /opt/nutch/bin/nutch readdb && echo \"==>Success\" || echo \"==>Failed\"\nUsage: CrawlDbReader <crawldb> (-stats | -dump <out_dir> | -url <url>)\n        <crawldb>       directory name where crawldb is located\n        -stats  print overall statistics to System.out\n        -dump <out_dir> dump the whole db to a text file in <out_dir>\n        -url <url>      print information on <url> to System.out\n==>Success\nNote that the nutch shell script functions as expected:\nbash-3.00$ /opt/nutch/bin/nutch  && echo \"==>Success\" || echo \"==>Failed\"\nUsage: nutch COMMAND\nwhere COMMAND is one of:\n  crawl             one-step crawler for intranets\n  readdb            read / dump crawl db\n  readlinkdb        read / dump link db\n  admin             database administration, including creation\n  inject            inject new urls into the database\n  generate          generate new segments to fetch\n  fetch             fetch a segment's pages\n  parse             parse a segment's pages\n  updatedb          update crawl db from segments after fetching\n  invertlinks       create a linkdb from parsed segments\n  index             run the indexer on parsed segments and linkdb\n  merge             merge several segment indexes\n  dedup             remove duplicates from a set of segment indexes\n  server            run a search server\n  namenode          run the NDFS namenode\n  datanode          run an NDFS datanode\n  ndfs              run an NDFS admin client\n  jobtracker        run the MapReduce job Tracker node\n  tasktracker       run a MapReduce task Tracker node\n  job               manipulate MapReduce jobs\n or\n  CLASSNAME         run the class named CLASSNAME\nMost commands print help when invoked w/o parameters.\n==>Failed",
        "Issue Links": [
            "/jira/browse/HADOOP-59"
        ]
    },
    "NUTCH-144": {
        "Key": "NUTCH-144",
        "Summary": "corrupt language identifier tri files and bad language recognition for german",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Bernhard Messer",
        "Created": "18/Dec/05 01:50",
        "Updated": "28/Jul/14 11:08",
        "Resolved": "01/Apr/11 14:30",
        "Description": "Hi,\ni had a look at the generated language guesser tri files. As far as i can say, several of them (de.ngp, da.ngp, es.ngp) seems to be corrupt which leeds to bad language recognition ratio. For example the german tri file should contain the german special characters \"\u00e4\", \"\u00f6\", \"\u00fc\" with their frequency. The text \"gr\u00fcne H\u00fcte\" which is typical german, is recognized as danish. May be the problem comes from wrong character encoding during training.\nJerome, could you provide the training files so that the language identifier can be retrained ?\nregards\n Bernhard",
        "Issue Links": []
    },
    "NUTCH-145": {
        "Key": "NUTCH-145",
        "Summary": "build of war file fails on Chinese (zh) .xml files due to UTF-8 BOM",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "Kuro Kurosaka",
        "Created": "20/Dec/05 08:58",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "21/Dec/05 03:32",
        "Description": "When I ran ant build from within Eclipse, it failed on src/web/include/zh/header.xml and src/web/pages/zh/*.xml because \"document does not h ave a root element\" (translated from Japanese message).\nAt a closer look at these files, they have an invisible Unicode UTF-8 BOM character, that is EF BB BF in hex, or \\357\\273\\277 in octal, at the beginning.\nPerhaps JDK 1.4.x UTF-8 converter does not handle the BOM for UTF-8 files. (Note that BOM was orginially intended to be used to UTF-16 and UTF-32 encodings to self-identify the endianness.  But Microsoft started using UTF-8-ized BOM as a character encoding signature.)\nAlso noticed was, they use MS-DOS style end-of-line sequence, CR followed by LF, unlike other ??/*.xml files which use UNIX style EOL.\nFixed files are available.",
        "Issue Links": []
    },
    "NUTCH-146": {
        "Key": "NUTCH-146",
        "Summary": "mapred.job.tracker.info.port is defined 2 times in the nutch-default.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Stefan Groschupf",
        "Created": "21/Dec/05 02:42",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "21/Dec/05 03:26",
        "Description": "Sorry my fault, was not deleted in the port patch.",
        "Issue Links": []
    },
    "NUTCH-147": {
        "Key": "NUTCH-147",
        "Summary": "nutch map reduce does not work in windows map reduce runs in a loop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "raghavendra prabhu",
        "Created": "22/Dec/05 12:44",
        "Updated": "24/Dec/05 04:46",
        "Resolved": "24/Dec/05 04:46",
        "Description": "Description \nCrawl Starts \nand i am able to see the initial messages\nThen the map reduce process starts and it continues to run in a loop \nI do not find the same problem in linux(linux it works perfectly)\nBelow is loop into which i run into \nclustering.OnlineClusterer)\n051222 182058   Nutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n051222 182058   Nutch Content Parser (org.apache.nutch.parse.Parser)\n051222 182058   Ontology Model Loader (org.apache.nutch.ontology.Ontology)\n051222 182058   Nutch Analysis (org.apache.nutch.analysis.NutchAnalyzer)\n051222 182058   Nutch Query Filter (org.apache.nutch.searcher.QueryFilter)\n051222 182058 found resource crawl-urlfilter.txt at file:/G:/trunklatest/conf/cr\nawl-urlfilter.txt\n051222 182058 crawl\\url.txt:0+25\n051222 182059 crawl\\url.txt:0+25\n051222 182059  map -521216%\n051222 182100 crawl\\url.txt:0+25\n051222 182100  map -1107496%\n051222 182101 crawl\\url.txt:0+25\n051222 182101  map -1678544%\n051222 182102 crawl\\url.txt:0+25\n051222 182102  map -2265900%\n051222 182103 crawl\\url.txt:0+25\n051222 182103  map -2849416%\n051222 182104 crawl\\url.txt:0+25\n051222 182104  map -3422908%\n051222 182105 crawl\\url.txt:0+25\nThe same thing continues",
        "Issue Links": []
    },
    "NUTCH-148": {
        "Key": "NUTCH-148",
        "Summary": "org.apache.nutch.tools.CrawlTool throws error while doing deleteduplicates",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "raghavendra prabhu",
        "Created": "22/Dec/05 18:49",
        "Updated": "24/Dec/05 04:43",
        "Resolved": "24/Dec/05 04:43",
        "Description": "I get the following error while running org.apache.nutch.tools.CrawlTool\nThe error actually is in deleteduplicates \n51223 001121 Reading url hashes...\n051223 001121 Sorting url hashes...\n051223 001121 Deleting url duplicates...\n051223 001121 Error moving bad file \nG:\\apache-tomcat-5.5.12\\webapps\\crux\\WEB-INF\n\\classes\\ddup-workingdir\\ddup-20051223001121: java.io.IOException: \nCreateProcess\n: df -k  G:\\apache-tomcat-5.5.12\\webapps\\crux\\WEB-INF\\classes\\ddup-workingdir\\ddup-20051223001121 error=2\nIt throws the error here in NFSDataInputStream.java\nThe exception is org.apache.nutch.fs.ChecksumException: Checksum \nerror: G:\\apach\ne-tomcat-5.5.12\\webapps\\crux\\WEB-INF\\classes\\ddup-workingdir\\ddup-20051223001121 at 0",
        "Issue Links": []
    },
    "NUTCH-149": {
        "Key": "NUTCH-149",
        "Summary": "outlinks not shown properly in cached.jsp",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "raghavendra prabhu",
        "Created": "23/Dec/05 03:36",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "08/Feb/06 06:22",
        "Description": "Description \nWhen i index a site and look at the site results in cached.jsp\nThe url is shown as \nhttp://localhost:8080/<the link>\nThe site name should come instead of http://localhost",
        "Issue Links": []
    },
    "NUTCH-150": {
        "Key": "NUTCH-150",
        "Summary": "OutlinkExtractor extremely slow on some non-plain text",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.7.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Paul Baclace",
        "Created": "23/Dec/05 12:15",
        "Updated": "24/Oct/06 15:32",
        "Resolved": "07/Jan/06 06:42",
        "Description": "While using mime settings which aggressively parsed everything by default, rather than having conf/parse-plugins.xml  associate parse-default with *, some parse tasks took an incredibly long time to finish.  For instance, a single postscript file took 9 hours to parse.  Stacktraces indicated this to be a problem with OutlinkExtractor.getOutlinks(...) during the call to reg expr match().  \nAnalysis:  The regular expression matching in OutlinkExtractor.getOutlinks(...) encounters parasitic cases which have extremely long runtimes when non-plain-text is processed.\nWorkaround 1:  Avoid treating non-plain-text, especially postscript files, as text or html.\nWorkaround 2:  kill -SIGQUIT  the child TaskRunner process, this will interrupt the match() and the process will continue.  This might need to be done multiple times.  (In theory, SIGQUIT is not supposed to do this, but in practice it does.)",
        "Issue Links": []
    },
    "NUTCH-151": {
        "Key": "NUTCH-151",
        "Summary": "CommandRunner can hang after the main thread exec is finished and has inefficient busy loop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Paul Baclace",
        "Created": "24/Dec/05 10:01",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "12/Jan/06 02:09",
        "Description": "I encountered a case where the JVM of a Tasktracker child did not exit after the main thread returned; a thread dump showed only the threads named STDOUT and STDERR from CommandRunner as non-daemon threads, and both were doing a read().\nCommandRunner usually works correctly when the subprocess is expected to be finished before the timeout or when no timeout is used. By usually, I mean in the absence of external thread interrupts.  The busy loop that waits for the process to finish has a sleep that is skipped over by an exception; this causes the waiting main thread to compete with the subprocess in a tight loop and effectively reduces the available cpu by 50%.",
        "Issue Links": []
    },
    "NUTCH-152": {
        "Key": "NUTCH-152",
        "Summary": "TaskRunner io pipes are not setDaemon(true), cleanup and exception errors are incomplete, max heap too small",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Paul Baclace",
        "Created": "27/Dec/05 11:50",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "17/Jan/08 20:27",
        "Description": "1. io pipes should be setDaemon(true) so that process cannot hang.\n2. error messages for Exceptions are incomplete since e.getMessage() is used and it can be empty (NullPointerException has an empty message).   Change this to e.toString() which always has more meaning.\n3. a separate thread is not used for the subprocess stdout pipe, but it must be a separate thread if setDaemon(true).\n4. TaskRunner.kill()  does not stop the io pipe threads, but it should.\n5. If InterruptedException occurs, it was assumed to be for the current (main) thread, but it should check this with Thread.interrupted() otherwise spurious thread interrupts will be rethrown as IOException.\n6. A recent run had some Tasktracker child processes that ran out of heap.  The default max heap size should be larger.",
        "Issue Links": []
    },
    "NUTCH-153": {
        "Key": "NUTCH-153",
        "Summary": "TextParser is only supposed to parse plain text, but if given postscript, it can take hours and then fail",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Paul Baclace",
        "Created": "27/Dec/05 12:29",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 15:02",
        "Description": "If TextParser is given postscript, it can take hours and then fail.  This can be avoided with careful configuration, but if the server MIME type is wrong and the basename of the URL has no \"file extension\", then the this parser will take a long time and fail every time.\nAnalysis: The real problem is OutlinkExtractor.java as reported with bug NUTCH-150, but the problem cannot be entirely addressed with that patch since the first call to reg expr match() can take a long time, despite quantifier limits.  \nSuggested fix: Reject files with \"%!PS-Adobe\" in the first 40 characters of the file.\nActual experience has shown that for safety and fail-safe reasons, it is worth protecting against GIGO directly in TextParse for this case, even though the suggested fix is not a general solution.  (A general solution would be a timeout on match().)",
        "Issue Links": []
    },
    "NUTCH-154": {
        "Key": "NUTCH-154",
        "Summary": "Unable to add/update new files to fetchlist/fetcher and thus index, when u rerun crawl tool on same db.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Arun Kumar",
        "Created": "28/Dec/05 17:03",
        "Updated": "28/Dec/05 22:18",
        "Resolved": "28/Dec/05 22:18",
        "Description": "I have modified crawl tool to rerun on same db, I am facing problem when re-running the crawl tool. Problem I am facing is that it is unable to fetch/crawl the files which are new additions to the urls. Can anyone suggest what is possible remedy for that.\n  with thanx",
        "Issue Links": []
    },
    "NUTCH-155": {
        "Key": "NUTCH-155",
        "Summary": "Remove web gui from the distribution to \"contrib\" and use OpenSearch Servlet",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "nutch.newbie",
        "Created": "28/Dec/05 23:05",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 15:05",
        "Description": "Web gui JSP search pages should be moved to a contrib folder.  It would be better to focus on OpenSearch Servlet based XML results. For example in the current tutorial at -\nhttp://lucene.apache.org/nutch/tutorial.html under the searching section one could imagine to add a script OpenSearch. (i.e. >>bin/nutch OpenSearch \"search term\"--> Bingo XML results. ) \nTherefore I suggest - It is better that web gui moves to contrib. I also forsee posting PHP or Perl, Ruby, XSLT or other language based GUI being developed and have it under the contrib as an addition to JSP pages. \n\nCurrent implementation focuses on JSP pages, tomcat, etc. has nothing to do with Nutch. But has everything to do with How Nutch needs to be deployed. And to my mind Nutch can be deployed in many ways. So why just JSP and tomcat will get the core attention.\n\nThe above wish is not new, I have seen others in Jira having similler thinking. Furthermore Nutch is becoming big in size, the plugins are also growing it would be good idea to have a contrib directory just like Lucene. Some of the plugin could also move there. Plugins like clustering, ontology (i.e. not required for basic indexing/searching) etc are not given that it should be part of the distribution. The point I try to make here is its up to the search engine operator to download the plugins rather then everyone gets \"everything.tar\" model.\nAbove is still a wish",
        "Issue Links": []
    },
    "NUTCH-156": {
        "Key": "NUTCH-156",
        "Summary": "nutch-daemon.sh should not overwrite old logs by default",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Paul Baclace",
        "Created": "29/Dec/05 08:04",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "21/Jun/06 03:01",
        "Description": "nutch-daemon.sh creates a log file with the name pattern \n  \"$NUTCH_LOG_DIR/nutch-$NUTCH_IDENT_STRING-$command-`hostname`.log\"\nevery time it is run.  This can overwrite a previous log without warning.  As such, it is too easy to accidently lose a log that might contain unique failure information.  \nWorkaround:   one must remember to put aside the old logs before restarting a daemon.\nSuggested fix:\n  Change the log name pattern in nutch-daemon.sh to include an ISO date:\n   \"$NUTCH_LOG_DIR/nutch-$NUTCH_IDENT_STRING-$command-$(hostname)-$(date '+%Y%m%dT%H%M%S').log\"",
        "Issue Links": []
    },
    "NUTCH-157": {
        "Key": "NUTCH-157",
        "Summary": "Problem during parsing msword document . It fetching properly but parsing is not working. Please show me the way how can i parse it",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "karamjit",
        "Created": "29/Dec/05 21:24",
        "Updated": "15/Mar/08 00:18",
        "Resolved": "15/Mar/08 00:18",
        "Description": "Ms word document  not parsing.\nError messages :----------\nPage from url Path in fetch ====file:/D:/karam/Atlantis_Tools/Crawl_Files/compareFVAJ.doc\n060301 173204 fetching  file:/D:/karam/Atlantis_Tools/Crawl_Files/compareFVAJ.doc\n060301 173204 Parsing file:/D:/karam/Atlantis_Tools/Crawl_Files/compareFVAJ.doc with [org.apache.nutch.parse.msword.MSWordParser@1e3cd51]\n060301 173204 fetch of file:/D:/karam/Atlantis_Tools/Crawl_Files/compareFVAJ.doc failed with: java.lang.NoSuchMethodError: org.apache.poi.hpsf.SummaryInformation.getEditTime()J\n060301 173204 Could not clean the content-type [], Reason is [org.apache.nutch.util.mime.MimeTypeException: The type can not be null or empty]. Using its raw version...\n060301 173204 Parsing file:/D:/karam/Atlantis_Tools/Crawl_Files/compareFVAJ.doc with [org.apache.nutch.parse.text.TextParser@b25b9d]\n060301 173205 status: segment 20060301173203, 1 pages, 1 errors, 35840 bytes, 1000 ms\n060301 173205 status: 1.0 pages/s, 280.0 kb/s, 35840.0 bytes/page",
        "Issue Links": []
    },
    "NUTCH-158": {
        "Key": "NUTCH-158",
        "Summary": "Process Sitemap data in text, rss or xml format as well as OAI-PMH",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "byron miller",
        "Created": "30/Dec/05 04:58",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "Add support to the fetcher to look for sitemap files, download them and process them into webdb.\nPerhaps create a robots.txt directive that can be used to create a standard format for sitemaps in RSS, XML or text format (one line per url) and process that.\nI would love to see someone stomp on proprietary sitemap features or making things so google specific as they are today \n\nRSS format/Atom Format (standard)\nXML meta descroption\nOAI-PMH meta description (http://www.openarchives.org/OAI/openarchivesprotocol.html)\n\nPerhaps even a \"pre crawler\" that will scour for these to inject into the web db to help build your link map so you could even just index topN.",
        "Issue Links": []
    },
    "NUTCH-159": {
        "Key": "NUTCH-159",
        "Summary": "Specify temp/working directory for crawl",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher,                                            indexer",
        "Assignee": null,
        "Reporter": "byron miller",
        "Created": "01/Jan/06 03:04",
        "Updated": "17/Jan/08 20:30",
        "Resolved": "17/Jan/08 20:30",
        "Description": "I ran a crawl of 100k web pages and got:\norg.apache.nutch.fs.FSError: java.io.IOException: No space left on device\n        at org.apache.nutch.fs.LocalFileSystem$LocalNFSFileOutputStream.write(LocalFileSystem.java:149)\n        at org.apache.nutch.fs.FileUtil.copyContents(FileUtil.java:65)\n        at org.apache.nutch.fs.LocalFileSystem.renameRaw(LocalFileSystem.java:178)\n        at org.apache.nutch.fs.NutchFileSystem.rename(NutchFileSystem.java:224)\n        at org.apache.nutch.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:80)\nCaused by: java.io.IOException: No space left on device\n        at java.io.FileOutputStream.writeBytes(Native Method)\n        at java.io.FileOutputStream.write(FileOutputStream.java:260)\n        at org.apache.nutch.fs.LocalFileSystem$LocalNFSFileOutputStream.write(LocalFileSystem.java:147)\n        ... 4 more\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:308)\n        at org.apache.nutch.crawl.Fetcher.fetch(Fetcher.java:335)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:107)\nbyron@db02:/data/nutch$ df -k\nIt appears crawl created a /tmp/nutch directory that filled up even though i specified a db directory.\nNeed to add a parameter to the command line or make a globaly configurable /tmp (work area) for the nutch instance so that crawls won't fail.",
        "Issue Links": []
    },
    "NUTCH-160": {
        "Key": "NUTCH-160",
        "Summary": "Use standard Java Regex library rather than org.apache.oro.text.regex",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "01/Jan/06 03:12",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "10/Jan/06 06:59",
        "Description": "org.apache.oro.text.regex is based on perl 5.003 which has some corner cases which perform poorly. The standard regular expression libraries for Java (1.4 and later) do not seen to contain these issues.",
        "Issue Links": []
    },
    "NUTCH-161": {
        "Key": "NUTCH-161",
        "Summary": "Change Plain text parser to use parser.character.encoding.default property for fall back encoding",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Sami Siren",
        "Reporter": "Kuro Kurosaka",
        "Created": "03/Jan/06 08:35",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "15/May/07 18:31",
        "Description": "The value of the property parser.character.encoding.default is used as a fallback character encoding (charset) when HTML parser cannot find the charset information in HTTP Content-Type header or in META HTTP-EQUIV tag.  But the plain text parser behaves differently.  It just uses the system encoding (Java VM file.encodings, which in turn derives from the OS and the locale of the environment from which the JVM was spawned).  This is not pretty.  To gurantee a consistent behavior, plain text parser should use the value of the same property.\nThough not tested, these changes in ./src/plugin/parse-text/src/java/org/apache/nutch/parse/text/TextParser.java should do it:\nInsert this statement in the class definition:\n  private static String defaultCharEncoding =\n    NutchConf.get().get(\"parser.character.encoding.default\", \"windows-1252\");\nReplace this:\n      text = new String(content.getContent());    // use default encoding\nwith this:\n      text = new String(content.getContent(), defaultCharEncoding );    // use default encoding",
        "Issue Links": []
    },
    "NUTCH-162": {
        "Key": "NUTCH-162",
        "Summary": "country code \"jp\" is used instead of language code \"ja\" for Japanese",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Kuro Kurosaka",
        "Created": "04/Jan/06 07:13",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "In locale switching link for Japanese, \"jp\" is used as language code but it is an ISO country code.  The language code \"ja\" should be used.\nBy the way, I don't think many users are familiar with the ISO language codes.  A Canadian user may click on \"ca\" uknowoing that ca stands for Catalan, not Canadian English or French. Rather than listing the language code, listing the language names in the prospective languages may be better. (I say \"may be\" because the browser could show some language names in corrupted text if the current font does not support that language \u2014 this is a difficult problem.)",
        "Issue Links": []
    },
    "NUTCH-163": {
        "Key": "NUTCH-163",
        "Summary": "LogFormatter design",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Par Daniel Fagerstrohm",
        "Created": "04/Jan/06 22:45",
        "Updated": "22/Jan/08 14:24",
        "Resolved": "22/Jan/08 14:24",
        "Description": "In Nutch project LogFormatter has duplicated functionality:\n1) Logger records format and\n2) Severe error handler\nThe first usage is standard and usually could be overwritten by a user of the package by modifying logging.properties file.\nThe second usage is much more problematic because it affects the behavior of the whole application (not only Nutch package). To support the error handling LogFormatter enforce usage of the formatter class by all classes of the whole application which uses Nutch package. This is done by overwriting all the system handlers (class java.util.logging.Handler). This operation prevents the application to use its own log formatter. Also this cause LogFormatter.hasLoggedSevere() to be sensitive to all severe records in the big system but not only to relevant. More than that this flag, LogFormatter.loggedSevere is never cleaned what means if an application had one, even unrelated severe record, tools like Fetcher will never run until the application will be restarted.\nI would like to suggest the following solutions:\n1) To separate the functionality of log formatting and error handling or\n2) Change LogFormatter class to be affected only by nutch package functions\nFor my opinion the first solution is much better especially if error handling will be encapsulated for each task. I have found the following usages of LogFormatter.hasLoggedSevere():\n\nFetcher\nURLFilterChecker\nParseSegment\nUnfortunately I'm not familiar enough with the usages above to implement this solution that why I suggest the second one.\nI have rewritten my own implementation of LogFormatter class which is used for more than a year in www.rawsugar.com application.\nI could provide the file but do not know how to attach it to the issue. I hope this change will be accepted by the community.",
        "Issue Links": [
            "/jira/browse/NUTCH-258"
        ]
    },
    "NUTCH-164": {
        "Key": "NUTCH-164",
        "Summary": "Locale (language) choice by first session has global effect to all sessions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Kuro Kurosaka",
        "Created": "05/Jan/06 03:51",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "Here's a report posted on nutch-users ML by Sergio [redsun@redsun.homeip.net] on 1/02/2006:\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\nI just installed nutch in a Fedora Core 3 server.\nOnce installed, I crawled a small site to test it. I opened my navigator\n(mozilla 1.7 which reports by default ES-ES locales, and everything was ok).\nThen I asked a friend of mine  (the owner of the server) to test it. He did\na search with an EN-US locale navigator, and the search page appeared in\nSpanish.\nAfter a few hours, I did the following: I restarted tomcat, I changed the\nlocale of my mozilla to EN, and I opened the search page. Now I always get\nEnglish search page even if I open with a mozilla ES-ES locale.\nI wrote a message to my friend:\n\"nutch keeps the locale of the first navigator that makes a request for all\nother requests. By this reason, yesterday as the first request was from my\nES locale browser, you saw the page in Spanish with your browser that\nreports EN locale. There is a way to make this work:\n\nMaking sure that, after the server is restarted, the first request is done\nby a browser that reports EN locale.\"\n<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n\nThis happened in my environment too.  After taking a look the code, I believe this is caused by\nuse of the default message bundle in search.jsp.  The code snipplet looks like:\n    <i18n:bundle baseName=\"org.nutch.jsp.search\"/>\n    ...\n    <title>Nutch: <i18n:message key=\"title\"/></title>\n    ...\nThe default message bundle probably has the application scope.  Because of that, the first\nsetting of the language has global effect to every session created afterward.\nThe right fix is to limit the scope to the session by inserting the scope specifier, as in:\n<i18n:bundle scope=\"session\" baseName=\"org.nutch.jsp.search\"/>\nOther JSP files need to be inspected for the same issue and should be fixed as well.",
        "Issue Links": []
    },
    "NUTCH-165": {
        "Key": "NUTCH-165",
        "Summary": "object pooling for nutch bean --- to impriove performance",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "raghavendra prabhu",
        "Created": "06/Jan/06 20:13",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "25/Mar/06 19:44",
        "Description": "We can create an object pooling for NutchBean using\nhttp://jakarta.apache.org/commons/pool/\nServer controls the NutchBean instantiation and free up.\nModifications will be required in search.jsp  which should call object pooling mechanism\n(NutchBean will be rotated among different users)",
        "Issue Links": []
    },
    "NUTCH-166": {
        "Key": "NUTCH-166",
        "Summary": "secure jobtracker info pages with a password",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "08/Jan/06 08:08",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "21/Jun/06 01:58",
        "Description": "Since people often post stack-traces in the mailing list that contains ip addresses it is easy for others to view the info pages of the jobtracker. \nThis may contains more security critical informations like more ip addresses and internal host-names etc.\nTherefore this patch adds a Basic password authentication  to the jetty server. \nThe user name is 'admin'  and the password can be configured in the nutch configuration file.",
        "Issue Links": []
    },
    "NUTCH-167": {
        "Key": "NUTCH-167",
        "Summary": "Observation of <META NAME=\"ROBOTS\" CONTENT=\"NOARCHIVE\"> directive",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "0.9.0",
        "Component/s": "indexer,                                            web gui",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Ed Whittaker",
        "Created": "08/Jan/06 11:43",
        "Updated": "07/Mar/07 23:37",
        "Resolved": "07/Mar/07 23:37",
        "Description": "Though not strictly a bug, this issue is potentially serious for users of Nutch who deploy live systems who might be threatened with legal action for caching copies of copyrighted material. The major search engines all observe this directive (even though apparently it's not stanard) so there's every reason why Nutch should too.",
        "Issue Links": []
    },
    "NUTCH-168": {
        "Key": "NUTCH-168",
        "Summary": "setting http.content.limit to -1 seems to break text parsing on some files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Jerry Russell",
        "Created": "10/Jan/06 07:13",
        "Updated": "15/Mar/08 00:20",
        "Resolved": "15/Mar/08 00:20",
        "Description": "Setting http.content limit to -1 (which is supposed to mean no limit causes some pages not to index. I have seen this in some PDFs and this one URL in particular. The steps to reproduce are below:\nReproduce:\n  1) install fresh nutch-0.7\n  2) configure urlfilters to allow any URL\n  3) create urllist with only the following URL: http://www.circuitsonline.net/circuits/view/71\n  4) perform a crawl with a depth of 1\n  5) do segread and see that the content is there\n  6) change the http.content.limit to -1 in nutch-default.xml \n  7) repeat the crawl to a new directory \n  8) do segread and see that the content is not there\ncontact jerry@circuitscout.com for more information.",
        "Issue Links": []
    },
    "NUTCH-169": {
        "Key": "NUTCH-169",
        "Summary": "remove static NutchConf",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "10/Jan/06 21:41",
        "Updated": "01/Feb/06 01:09",
        "Resolved": "01/Feb/06 01:09",
        "Description": "Removing the static NutchConf.get is required for a set of improvements and new features.\n+ it allows a better integration of nutch in j2ee or other systems.\n+ it allows the management of nutch from a web based gui (a kind of nutch appliance) which will improve the usability and also increase the user acceptance of nutch\n+ it allows to change configuration properties until runtime\n+ it allows to implement NutchConf as a abstract class or interface to provide other configuration value sources than xml files. (community request)",
        "Issue Links": []
    },
    "NUTCH-170": {
        "Key": "HADOOP-18",
        "Summary": "Crash with multiple temp directories",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "11/Jan/06 02:45",
        "Updated": "08/Jul/09 16:51",
        "Resolved": "07/Mar/06 06:30",
        "Description": "A brief read of the code indicated it may be possible to use multiple local directories using something like the below:\n  <property>\n    <name>mapred.local.dir</name>\n    <value>/local,/local1,/local2</value>\n    <description>The local directory where MapReduce stores intermediate\n    data files.\n    </description>\n  </property>\nThis failed with the below exception during either the generate or update phase (not entirely sure which).\njava.lang.ArrayIndexOutOfBoundsException\n        at java.util.zip.CRC32.update(CRC32.java:51)\n        at org.apache.nutch.fs.NFSDataInputStream$Checker.read(NFSDataInputStream.java:92)\n        at org.apache.nutch.fs.NFSDataInputStream$PositionCache.read(NFSDataInputStream.java:156)\n        at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:313)\n        at java.io.DataInputStream.readFully(DataInputStream.java:176)\n        at org.apache.nutch.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:55)\n        at org.apache.nutch.io.DataOutputBuffer.write(DataOutputBuffer.java:89)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:378)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:301)\n        at org.apache.nutch.io.SequenceFile$Reader.next(SequenceFile.java:323)\n        at org.apache.nutch.mapred.SequenceFileRecordReader.next(SequenceFileRecordReader.java:60)\n        at org.apache.nutch.segment.SegmentReader$InputFormat$1.next(SegmentReader.java:80)\n        at org.apache.nutch.mapred.MapTask$2.next(MapTask.java:106)\n        at org.apache.nutch.mapred.MapRunner.run(MapRunner.java:48)\n        at org.apache.nutch.mapred.MapTask.run(MapTask.java:116)\n        at org.apache.nutch.mapred.TaskTracker$Child.main(TaskTracker.java:604)",
        "Issue Links": []
    },
    "NUTCH-171": {
        "Key": "NUTCH-171",
        "Summary": "Bring back multiple segment support for Generate / Update",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Rod Taylor",
        "Created": "12/Jan/06 08:26",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Dec/08 13:53",
        "Description": "We find it convenient to be able to run generate once for -topN 300M and have multiple independent segments to work with (lower overhead) \u2013 then run update on all segments which succeeded simultaneously.\nThis reactivates -numFetchers and fixes updatedb to handle multiple provided segments again.\nRadu Mateescu wrote the attached patch for us with the below description (lightly edited):\nThe implementation of -numFetchers in 0.8 improperly plays with the number of reduce tasks in order to generate a given number of fetch lists. Basically, what it does is this: before the second reduce (map-reduce is applied twice for generate), it sets the number of reduce tasks to numFetchers and ideally, because each reduce will create a file like part-00000, part-00001, etc in the ndfs, we'll end up with the number of desired fetched lists. But this behaviour is incorrect for the following reasons:\n1. the number of reduce tasks is orthogonal to the number of segments somebody wants to create. The number of reduce tasks should be chosen based on the physical topology rather then the number of segments someone might want in ndfs\n2. if in nutch-site.xml you specify a value for mapred.reduce.tasks property, the numFetchers seems to be ignored\nTherefore , I changed this behaviour to work like this: \n\ngenerate will create numFetchers segments\neach reduce task will write in all segments (assuming there are enough values to be written) in a round-robin fashion\nThe end results for 3 reduce tasks and 2 segments will look like this :\n\n/opt/nutch/bin>./nutch ndfs -ls segments\n060111 122227 parsing file:/opt/nutch/conf/nutch-default.xml\n060111 122228 parsing file:/opt/nutch/conf/nutch-site.xml\n060111 122228 Client connection to 192.168.0.1:5466: starting\n060111 122228 No FS indicated, using default:master:5466\nFound 2 items\n/user/root/segments/20060111122144-0    <dir>\n/user/root/segments/20060111122144-1    <dir>\n/opt/nutch/bin>./nutch ndfs -ls segments/20060111122144-0/crawl_generate\n060111 122317 parsing file:/opt/nutch/conf/nutch-default.xml\n060111 122317 parsing file:/opt/nutch/conf/nutch-site.xml\n060111 122318 No FS indicated, using default:master:5466\n060111 122318 Client connection to 192.168.0.1:5466: starting\nFound 3 items\n/user/root/segments/20060111122144-0/crawl_generate/part-00000  1276\n/user/root/segments/20060111122144-0/crawl_generate/part-00001  1289\n/user/root/segments/20060111122144-0/crawl_generate/part-00002  1858\n/opt/nutch/bin>./nutch ndfs -ls segments/20060111122144-1/crawl_generate\n060111 122333 parsing file:/opt/nutch/conf/nutch-default.xml\n060111 122334 parsing file:/opt/nutch/conf/nutch-site.xml\n060111 122334 Client connection to 192.168.0.1:5466: starting\n060111 122334 No FS indicated, using default:master:5466\nFound 3 items\n/user/root/segments/20060111122144-1/crawl_generate/part-00000  1207\n/user/root/segments/20060111122144-1/crawl_generate/part-00001  1236\n/user/root/segments/20060111122144-1/crawl_generate/part-00002  1841",
        "Issue Links": []
    },
    "NUTCH-172": {
        "Key": "NUTCH-172",
        "Summary": "Segment merger",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Mike Alulin",
        "Created": "13/Jan/06 04:56",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "12/Jul/06 04:01",
        "Description": "The map reduce version missing segment merging that can be very important when one wants to have frequent crawls of updated pages only.",
        "Issue Links": []
    },
    "NUTCH-173": {
        "Key": "NUTCH-173",
        "Summary": "PerHost Crawling Policy ( crawl.ignore.external.links )",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.7.1,                                            0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Philippe EUGENE",
        "Created": "13/Jan/06 18:17",
        "Updated": "19/Jul/06 17:34",
        "Resolved": "19/Jul/06 17:34",
        "Description": "There is two major way of crawl in Nutch.\nIntranet Crawl : forbidden all, allow somes few host\nWhole-web crawl : allow all, forbidden few thinks\nI propose a third type of crawl.\nDirectory Crawl : The purpose of this crawl is to manage few thousands of host wihtout managing rules pattern in UrlFilterRegexp.\nI made two patch for : 0.7, 0.7.1 and 0.8-dev\nI propose a new boolean property in nutch-site.xml : crawl.ignore.external.links, with false value at default.\nBy default this new feature don't modify the behavior of nutch crawler.\nWhen you setup this property to true, the crawler don't fetch external links of the host.\nSo the crawl is limited to the host that you inject at the beginning at the crawl.\nI know there is some proposal of new crawl policy using the CrawlDatum in 0.8-dev branch. \nThis feature colud be a easiest way to add quickly new crawl feature to nutch, waiting for a best way to improve crawl policy.\nI post two patch.\nSorry for my very poor english \n\u2013\nPhilippe",
        "Issue Links": [
            "/jira/browse/NUTCH-271"
        ]
    },
    "NUTCH-174": {
        "Key": "NUTCH-174",
        "Summary": "Problem encountered with ant during compilation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "0.7.2,                                            0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthias G\u00fcnter",
        "Created": "15/Jan/06 00:08",
        "Updated": "15/Jan/06 03:16",
        "Resolved": "15/Jan/06 03:16",
        "Description": "There is a directory missing which causes ant to fail.\nError message:\nBUILD FAILED\n/home/guenter/workspace/lucene/nutch-0.7.1/build.xml:76: The following error occurred while executing this line:\n/home/guenter/workspace/lucene/nutch-0.7.1/src/plugin/build.xml:9: The following error occurred while executing this line:\n/home/guenter/workspace/lucene/nutch-0.7.1/src/plugin/build-plugin.xml:85: srcdir \"/home/guenter/workspace/lucene/nutch-0.7.1/src/plugin/nutch-extensionpoints/src/java\" does not exist!\nCompilation worked, when I omitted line 9 in nutch-0.7.1/src/plugin/build.xml:\n     <!-- <ant dir=\"nutch-extensionpoints\" target=\"deploy\"/>  -->\nHowever, I guess that is not what was intended.",
        "Issue Links": []
    },
    "NUTCH-175": {
        "Key": "NUTCH-175",
        "Summary": "No input directories specified in: while crawing in nightly build from the 14.1.2006: sh ./nutch crawl urllist.txt -dir tmpdir",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthias G\u00fcnter",
        "Created": "15/Jan/06 05:05",
        "Updated": "22/Jan/08 14:10",
        "Resolved": "22/Jan/08 14:10",
        "Description": "guenter@deimos:~/workspace/lucene/nutch-nightly/bin> sh ./nutch crawl urllist.txt -dir tmpdir\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/nutch-default.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/crawl-tool.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/mapred-default.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/nutch-site.xml\n060114 205612 crawl started in: tmpdir\n060114 205612 rootUrlDir = urllist.txt\n060114 205612 threads = 10\n060114 205612 depth = 5\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/nutch-default.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/crawl-tool.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/nutch-site.xml\n060114 205612 Injector: starting\n060114 205612 Injector: crawlDb: tmpdir/crawldb\n060114 205612 Injector: urlDir: urllist.txt\n060114 205612 Injector: Converting injected urls to crawl db entries.\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/nutch-default.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/crawl-tool.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/mapred-default.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/mapred-default.xml\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/nutch-site.xml\n060114 205612 Running job: job_n0o7ps\n060114 205612 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/nutch-default.xml\n060114 205613 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/mapred-default.xml\n060114 205613 parsing /tmp/nutch/mapred/local/localRunner/job_n0o7ps.xml\n060114 205613 parsing file:/home/guenter/workspace/lucene/nutch-nightly/conf/nutch-site.xml\njava.io.IOException: No input directories specified in: NutchConf: nutch-default.xml , mapred-default.xml , /tmp/nutch/mapred/local/localRunner/job_n0o7ps.xml , nutch-site.xml\n        at org.apache.nutch.mapred.InputFormatBase.listFiles(InputFormatBase.java:85)\n        at org.apache.nutch.mapred.InputFormatBase.getSplits(InputFormatBase.java:95)\n        at org.apache.nutch.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:63)\n060114 205613  map 0%\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:308)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:102)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:105)\nurllist.txt contains\nhttp://www.mentor.ch\nPS: Is there a committer or developer (near Switzerland) who can support (paid support) with a mixed index for intranet, some internet sites and scanning of local drives (P:\\ , S:\\ etc)",
        "Issue Links": []
    },
    "NUTCH-176": {
        "Key": "NUTCH-176",
        "Summary": "Using -dir: creates an error, when the directory already exists",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthias G\u00fcnter",
        "Created": "15/Jan/06 22:10",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "20/Jan/06 05:48",
        "Description": "In my opinion -dir should work even, when the directory already exists.\nThe error message is: \nguenter@deimos:~/workspace/lucene/nutch-0.7.1/bin> sh ./nutch crawl ../../urllist.txt  -dir tmpdir\n060115 140500 parsing file:/home/guenter/workspace/lucene/nutch-0.7.1/conf/nutch-default.xml\n060115 140500 parsing file:/home/guenter/workspace/lucene/nutch-0.7.1/conf/crawl-tool.xml\n060115 140500 parsing file:/home/guenter/workspace/lucene/nutch-0.7.1/conf/nutch-site.xml\n060115 140500 No FS indicated, using default:local\nException in thread \"main\" java.lang.RuntimeException: tmpdir already exists.\n        at org.apache.nutch.tools.CrawlTool.main(CrawlTool.java:121)",
        "Issue Links": []
    },
    "NUTCH-177": {
        "Key": "NUTCH-177",
        "Summary": "Default installation seems to produce working entity of nutch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthias G\u00fcnter",
        "Created": "15/Jan/06 22:19",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "20/Jan/06 05:46",
        "Description": "I downloaded 0.7.1 and installed it.\nThen changed crawl-urlfilter.txt for apache.org\nThen I added an urllist.txt  and tried scanning.\nApparently the URL has been ignored, even when it matched the rule in the crawl-url-filter.txt\nguenter@deimos:~/workspace/lucene/nutch-0.7.1/bin> sh ./nutch crawl ../../urllist.txt\n060115 141534 parsing file:/home/guenter/workspace/lucene/nutch-0.7.1/conf/nutch-default.xml\n060115 141534 parsing file:/home/guenter/workspace/lucene/nutch-0.7.1/conf/crawl-tool.xml\n060115 141534 parsing file:/home/guenter/workspace/lucene/nutch-0.7.1/conf/nutch-site.xml\n060115 141534 No FS indicated, using default:local\n060115 141534 crawl started in: crawl-20060115141534\n060115 141534 rootUrlFile = ../../urllist.txt\n060115 141534 threads = 10\n060115 141534 depth = 5\n060115 141535 Created webdb at LocalFS,/home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/db\n060115 141535 Starting URL processing\n060115 141535 Plugins: looking in: /home/guenter/workspace/lucene/nutch-0.7.1/plugins\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/query-more\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/query-site/plugin.xml\n060115 141535 impl: point=org.apache.nutch.searcher.QueryFilter class=org.apache.nutch.searcher.site.SiteQueryFilter\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/parse-html/plugin.xml\n060115 141535 impl: point=org.apache.nutch.parse.Parser class=org.apache.nutch.parse.html.HtmlParser\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/parse-text/plugin.xml\n060115 141535 impl: point=org.apache.nutch.parse.Parser class=org.apache.nutch.parse.text.TextParser\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/parse-ext\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/parse-pdf\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/parse-rss\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/query-basic/plugin.xml\n060115 141535 impl: point=org.apache.nutch.searcher.QueryFilter class=org.apache.nutch.searcher.basic.BasicQueryFilter\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/index-more\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/parse-js\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/urlfilter-regex/plugin.xml\n060115 141535 impl: point=org.apache.nutch.net.URLFilter class=org.apache.nutch.net.RegexURLFilter\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/protocol-ftp\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/parse-msword\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/creativecommons\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/ontology\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/nutch-extensionpoints/plugin.xml\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/protocol-file\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/protocol-http/plugin.xml\n060115 141535 impl: point=org.apache.nutch.protocol.Protocol class=org.apache.nutch.protocol.http.Http\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/clustering-carrot2\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/language-identifier\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/urlfilter-prefix\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/query-url/plugin.xml\n060115 141535 impl: point=org.apache.nutch.searcher.QueryFilter class=org.apache.nutch.searcher.url.URLQueryFilter\n060115 141535 parsing: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/index-basic/plugin.xml\n060115 141535 impl: point=org.apache.nutch.indexer.IndexingFilter class=org.apache.nutch.indexer.basic.BasicIndexingFilter\n060115 141535 not including: /home/guenter/workspace/lucene/nutch-0.7.1/plugins/protocol-httpclient\n060115 141535 found resource crawl-urlfilter.txt at file:/home/guenter/workspace/lucene/nutch-0.7.1/conf/crawl-urlfilter.txt\n..060115 141535 Added 0 pages\n060115 141535 FetchListTool started\n060115 141535 Overall processing: Sorted 0 entries in 0.0 seconds.\n060115 141535 Overall processing: Sorted NaN entries/second\n060115 141535 FetchListTool completed\n060115 141536 logging at INFO\n060115 141537 Updating /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/db\n060115 141537 Updating for /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141535\n060115 141537 Finishing update\n060115 141537 Update finished\n060115 141537 FetchListTool started\n060115 141537 Overall processing: Sorted 0 entries in 0.0 seconds.\n060115 141537 Overall processing: Sorted NaN entries/second\n060115 141537 FetchListTool completed\n060115 141537 logging at INFO\n060115 141538 Updating /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/db\n060115 141538 Updating for /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141537\n060115 141538 Finishing update\n060115 141538 Update finished\n060115 141538 FetchListTool started\n060115 141538 Overall processing: Sorted 0 entries in 0.0 seconds.\n060115 141538 Overall processing: Sorted NaN entries/second\n060115 141538 FetchListTool completed\n060115 141538 logging at INFO\n060115 141539 Updating /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/db\n060115 141539 Updating for /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141538\n060115 141539 Finishing update\n060115 141539 Update finished\n060115 141539 FetchListTool started\n060115 141540 Overall processing: Sorted 0 entries in 0.0 seconds.\n060115 141540 Overall processing: Sorted NaN entries/second\n060115 141540 FetchListTool completed\n060115 141540 logging at INFO\n060115 141541 Updating /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/db\n060115 141541 Updating for /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141539\n060115 141541 Finishing update\n060115 141541 Update finished\n060115 141541 FetchListTool started\n060115 141541 Overall processing: Sorted 0 entries in 0.0 seconds.\n060115 141541 Overall processing: Sorted NaN entries/second\n060115 141541 FetchListTool completed\n060115 141541 logging at INFO\n060115 141542 Updating /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/db\n060115 141542 Updating for /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141541\n060115 141542 Finishing update\n060115 141542 Update finished\n060115 141542 Updating /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments from /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/db\n060115 141542  reading /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141535\n060115 141542  reading /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141537\n060115 141542  reading /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141538\n060115 141542  reading /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141539\n060115 141542  reading /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141541\n060115 141542 Sorting pages by url...\n060115 141542 Getting updated scores and anchors from db...\n060115 141542 Sorting updates by segment...\n060115 141542 Updating segments...\n060115 141542 Done updating /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments from /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/db\n060115 141542 indexing segment: /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141535\n060115 141542 * Opening segment 20060115141535\n060115 141542 * Indexing segment 20060115141535\n060115 141542 * Optimizing index...\n060115 141542 * Moving index to NFS if needed...\n060115 141542 DONE indexing segment 20060115141535: total 0 records in 0.035 s (NaN rec/s).\n060115 141543 done indexing\n060115 141543 indexing segment: /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141537\n060115 141543 * Opening segment 20060115141537\n060115 141543 * Indexing segment 20060115141537\n060115 141543 * Optimizing index...\n060115 141543 * Moving index to NFS if needed...\n060115 141543 DONE indexing segment 20060115141537: total 0 records in 0.076 s (NaN rec/s).\n060115 141543 done indexing\n060115 141543 indexing segment: /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141538\n060115 141543 * Opening segment 20060115141538\n060115 141543 * Indexing segment 20060115141538\n060115 141543 * Optimizing index...\n060115 141543 * Moving index to NFS if needed...\n060115 141543 DONE indexing segment 20060115141538: total 0 records in 0.012 s (NaN rec/s).\n060115 141543 done indexing\n060115 141543 indexing segment: /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141539\n060115 141543 * Opening segment 20060115141539\n060115 141543 * Indexing segment 20060115141539\n060115 141543 * Optimizing index...\n060115 141543 * Moving index to NFS if needed...\n060115 141543 DONE indexing segment 20060115141539: total 0 records in 0.013 s (NaN rec/s).\n060115 141543 done indexing\n060115 141543 indexing segment: /home/guenter/workspace/lucene/nutch-0.7.1/bin/crawl-20060115141534/segments/20060115141541\n060115 141543 * Opening segment 20060115141541\n060115 141543 * Indexing segment 20060115141541\n060115 141543 * Optimizing index...\n060115 141543 * Moving index to NFS if needed...\n060115 141543 DONE indexing segment 20060115141541: total 0 records in 0.02 s (NaN rec/s).\n060115 141543 done indexing\n060115 141543 Reading url hashes...\n060115 141543 Sorting url hashes...\n060115 141543 Deleting url duplicates...\n060115 141543 Deleted 0 url duplicates.\n060115 141543 Reading content hashes...\n060115 141543 Sorting content hashes...\n060115 141543 Deleting content duplicates...\n060115 141543 Deleted 0 content duplicates.\n060115 141543 Duplicate deletion complete locally.  Now returning to NFS...\n060115 141543 DeleteDuplicates complete\n060115 141543 Merging segment indexes...\n060115 141543 crawl finished: crawl-20060115141534\nguenter@deimos:~/workspace/lucene/nutch-0.7.1/bin>",
        "Issue Links": []
    },
    "NUTCH-178": {
        "Key": "NUTCH-178",
        "Summary": "in search.jsp must be session creation \"false\"",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "YourSoft",
        "Created": "15/Jan/06 23:16",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "04/Feb/06 04:29",
        "Description": "Switch off the session creation in the search.jsp.\nI think it is add a little performance improvements.\nreplace first  3 lines:\n<%@ page \n  contentType=\"text/html; charset=UTF-8\"\n  pageEncoding=\"UTF-8\"\nwith:\n<%@ page \n  contentType=\"text/html; charset=UTF-8\"\n  pageEncoding=\"UTF-8\"\n  session=\"false\"",
        "Issue Links": []
    },
    "NUTCH-179": {
        "Key": "NUTCH-179",
        "Summary": "Proposition: Enable Nutch to use a parser plugin not just based on content type",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Gal Nitzan",
        "Created": "16/Jan/06 07:36",
        "Updated": "20/Jan/06 04:40",
        "Resolved": "20/Jan/06 04:40",
        "Description": "Sorry, please close this issue.\nI figured that if I set my parse plugin first. I can always be called first and than decide if I want to parse or not.",
        "Issue Links": []
    },
    "NUTCH-180": {
        "Key": "NUTCH-180",
        "Summary": "Performance problem with widely used keywords",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Mike Alulin",
        "Created": "16/Jan/06 08:58",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:30",
        "Description": "It looks like Nutch is very slow when the search phrase includes a few widely used keywords. For example \"I 1 2 3 4 5 6 7 8 9 0\" typed without the quotes to Yahoo, Google, or MSN is processed in less than a second. Nutch on the other hand requires much more time for this even on smaller databases. For example this phrase made objectssearch.com think more than 1 minute although their DB is much smaller than DBs of the big 3 guys. On my test Nutch DB with only 3M pages this phrase took a few seconds to process.\nUnfortunately I do not know much about search algorithms, but it looks like Nutch do have some space to improve the search performance. The current implementation can be easily \"killed\" by a few search requests like this. Just a couple of dozen of such requests makes my server with 2 Opterons think for a minute or two with 100% CPU utilization.",
        "Issue Links": []
    },
    "NUTCH-181": {
        "Key": "HADOOP-9",
        "Summary": "mapred.local.dir  temp dir. space allocation limited by smallest area",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.19.0",
        "Component/s": "None",
        "Assignee": "Ariel Shemaiah Rabkin",
        "Reporter": "Paul Baclace",
        "Created": "17/Jan/06 10:09",
        "Updated": "06/Nov/10 05:27",
        "Resolved": "12/Aug/08 21:21",
        "Description": "When mapred.local.dir is used to specify multiple  temp dir. areas, space allocation limited by smallest area because the temp dir. selection algorithm is \"round robin starting from a randomish point\".   When round robin is used with approximately constant sized chunks, the smallest area runs out of space first, and this is a fatal error. \nWorkaround: only list local fs dirs in mapred.local.dir with similarly-sized available areas.\nI wrote a patch to JobConf (currenly being tested) which uses df to check available space (once a minute or less often) and then uses an efficient roulette selection to do allocation weighted by magnitude of available space.",
        "Issue Links": []
    },
    "NUTCH-182": {
        "Key": "NUTCH-182",
        "Summary": "Log when db.max configuration limits reached",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Matt Kangas",
        "Created": "20/Jan/06 10:48",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "Followup to http://www.nabble.com/Re%3A-Can%27t-index-some-pages-p2480833.html\nThere are three \"db.max\" parameters currently in nutch-default.xml:\n\ndb.max.outlinks.per.page\ndb.max.anchor.length\ndb.max.inlinks\n\nHaving values that are too low can result in a site being under-crawled. However, currently there is nothing written to the log when these limits are hit, so users have to guess when they need to raise these values.\nI suggest that we add three new log messages at the appropriate points:\n\n\"Exceeded db.max.outlinks.per.page for URL \"\n\"Exceeded db.max.anchor.length for URL \"\n\"Exceeded db.max.inlinks for URL \"",
        "Issue Links": []
    },
    "NUTCH-183": {
        "Key": "HADOOP-7",
        "Summary": "MapReduce has a series of problems concerning task-allocation to worker nodes",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Mike Cafarella",
        "Created": "21/Jan/06 05:40",
        "Updated": "18/May/15 04:11",
        "Resolved": "07/Feb/06 03:38",
        "Description": "The MapReduce JobTracker is not great at allocating tasks to TaskTracker worker nodes.\nHere are the problems:\n1) There is no speculative execution of tasks\n2) Reduce tasks must wait until all map tasks are completed before doing any work\n3) TaskTrackers don't distinguish between Map and Reduce jobs.  Also, the number of\ntasks at a single node is limited to some constant.  That means you can get weird deadlock\nproblems upon machine failure.  The reduces take up all the available execution slots, but they\ndon't do productive work, because they're waiting for a map task to complete.  Of course, that\nmap task won't even be started until the reduce tasks finish, so you can see the problem...\n4) The JobTracker is so complicated that it's hard to fix any of these.\nThe right solution is a rewrite of the JobTracker to be a lot more flexible in task handling.\nIt has to be a lot simpler.  One way to make it simpler is to add an abstraction I'll call\n\"TaskInProgress\".  Jobs are broken into chunks called TasksInProgress.  All the TaskInProgress\nobjects must be complete, somehow, before the Job is complete.\nA single TaskInProgress can be executed by one or more Tasks.  TaskTrackers are assigned Tasks.\nIf a Task fails, we report it back to the JobTracker, where the TaskInProgress lives.  The TIP can then\ndecide whether to launch additional  Tasks or not.\nSpeculative execution is handled within the TIP.  It simply launches multiple Tasks in parallel.  The\nTaskTrackers have no idea that these Tasks are actually doing the same chunk of work.  The TIP\nis complete when any one of its Tasks are complete.",
        "Issue Links": []
    },
    "NUTCH-184": {
        "Key": "NUTCH-184",
        "Summary": "Serbian (sr, Cyrilic) and Serbo-Croatian (sh, Latin) translation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "Ivan Sekulovic",
        "Created": "24/Jan/06 20:19",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "15/Feb/06 04:37",
        "Description": "I've created translation for Serbian (sr, Cyrilic) and Serbo-Croatian (sh, Latin) languages.\nFiles will be attached with this issue.",
        "Issue Links": []
    },
    "NUTCH-185": {
        "Key": "NUTCH-185",
        "Summary": "XMLParser is configurable xml parser plugin.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.2,                                            0.8,                                            0.8.1",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher,                                            indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Rida Benjelloun",
        "Created": "25/Jan/06 01:45",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "26/Nov/09 03:16",
        "Description": "Xml parser  is configurable plugin. It use XPath and namespaces to do the mapping between the XML elements and Lucene fields. \nInformations :\n1- Copy \"xmlparser-conf.xml\" to the nutch/conf dir\n2- To index your custom XML file, you have to modify the \"xmlparser-conf.xml\". \nThis parser uses namespaces and XPATH to parse XML content\nThe config file do the mapping between the XML noeds (using XPATH) and lucene field. \nExample : <field name=\"dctitle\" xpath=\"//dc:title\" type=\"Text\" boost=\"1.4\" /> \n3- The xmlIndexerProperties encapsulate a set of fields associated to a namespace. \nIf the namespace is found in the xml document, the fields represented by the namespace will be indexed.\nExample : \n<xmlIndexerProperties type=\"filePerDocument\" namespace=\" http://purl.org/dc/elements/1.1/\">\n  <field name=\"dctitle\" xpath=\"//dc:title\" type=\"Text\" boost=\" 1.4\" /> \n  <field name=\"dccreator\" xpath=\"//dc:creator\" type=\"keyword\" boost=\" 1.0\" /> \n</xmlIndexerProperties>\n4- It is possible to define a default namespace that will be applied when the parser \ndidn't find any namespace in the document or when the namespace found in the xml document doesn't match with the namespace defined in the xmlIndexerProperties. \nExample :\n<xmlIndexerProperties type=\"filePerDocument\" namespace=\"default\">\n  <field name=\"xmlcontent\" xpath=\"//*\" type=\"Unstored\" boost=\"1.0\" /> \n</xmlIndexerProperties>",
        "Issue Links": [
            "/jira/browse/NUTCH-767",
            "/jira/browse/NUTCH-562"
        ]
    },
    "NUTCH-186": {
        "Key": "NUTCH-186",
        "Summary": "mapred-default.xml is over ridden by nutch-site.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Gal Nitzan",
        "Created": "25/Jan/06 07:03",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "17/Jan/08 20:26",
        "Description": "If mapred.map.tasks and mapred.reduce.tasks are defined in nutch-site.xml and also in mapred-default.xml the definitions from nutch-site.xml are those that will take effect.\nSo if a user mistakenly copies those entries into nutch-site.xml from the nutch-default.xml she will not understand what happens.\nI would like to propose removing these setting completely from the nutch-default.xml and put it only in mapred-default.xml where it belongs.\nI will be happy to supply a patch for that  if the proposition accepted.",
        "Issue Links": []
    },
    "NUTCH-187": {
        "Key": "NUTCH-187",
        "Summary": "Cannot start Nutch datanodes on Windows outside of a cygwin environment  because of DF",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dominik Friedrich",
        "Created": "25/Jan/06 17:43",
        "Updated": "08/Jun/11 21:32",
        "Resolved": "15/Jun/06 22:05",
        "Description": "Currently you cannot start Nutch datanodes on Windows outside of a cygwin environment because it relies on the df command to read the free disk space.",
        "Issue Links": [
            "/jira/browse/HADOOP-33"
        ]
    },
    "NUTCH-188": {
        "Key": "NUTCH-188",
        "Summary": "Add searchable mailing list links to http://lucene.apache.org/nutch/mailing_lists.html",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andy Liu",
        "Created": "27/Jan/06 01:49",
        "Updated": "22/Feb/06 19:56",
        "Resolved": "22/Feb/06 19:56",
        "Description": "Post links to searchable mail archives on nutch.org",
        "Issue Links": []
    },
    "NUTCH-189": {
        "Key": "NUTCH-189",
        "Summary": "Injection infinite loop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andy Liu",
        "Created": "27/Jan/06 01:53",
        "Updated": "15/Mar/08 00:24",
        "Resolved": "15/Mar/08 00:24",
        "Description": "f you inject the crawldb with a url file that doesn't end with a line feed, an infinite loop is entered.\n060104 160950 Running job: job_7uku5w\n060104 160952  map 0%\n060104 160954  map 50%\n060104 160957  map -2631%\n060104 160959  map -259756%\n060104 161002  map -538552%\n060104 161006  map -818413%\n060104 161009  map -1098421%\n060104 161011  map -1377851%\n060104 161014  map -1657718%\n060104 161018  map -1939534%\n060104 161021  map -2218515%\n060104 161023  map -2588212%\n060104 161026  map -2868787%\n060104 161030  map -3147637%",
        "Issue Links": []
    },
    "NUTCH-190": {
        "Key": "NUTCH-190",
        "Summary": "ParseUtil drops reason for failed parse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Michael Stack",
        "Created": "27/Jan/06 07:32",
        "Updated": "27/Jan/06 08:48",
        "Resolved": "27/Jan/06 08:48",
        "Description": "Doing the below:\n    Parse parse;\n    ParseStatus parseStatus;\n    try \n{\n      parse = ParseUtil.parse(content);\n      parseStatus = parse.getData().getStatus();\n    }\n catch (Exception e) \n{\n      parseStatus = new ParseStatus(e);\n    }\n    if (!parseStatus.isSuccess()) \n{\n      LOG.warning(\"Error parsing: \" + url + \": \" + parseStatus);\n      parse = null;\n    }\n\n...on failure, the LOG.warning never prints out the reason for failure.  Here's an example: \"Error parsing: http://www.dfrc.nasa.gov/DTRS/1967/PDF/H-478.pdf: failed(0,0)\".\nParseUtil is dropping messages lovingly crafted by parsers.",
        "Issue Links": []
    },
    "NUTCH-191": {
        "Key": "HADOOP-12",
        "Summary": "InputFormat used in job must be in JobTracker classpath (not loaded from job JAR)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Bryan Pendleton",
        "Created": "31/Jan/06 07:00",
        "Updated": "08/Jul/09 16:51",
        "Resolved": "10/Feb/06 08:01",
        "Description": "During development, I've been creating/tweaking custom InputFormat implementations. However, when you try to run a job against a running cluster, you get:\n  Exception in thread \"main\" java.io.IOException: java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: my.custom.InputFormat\n          at org.apache.nutch.ipc.Client.call(Client.java:294)\n          at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)\n          at $Proxy0.submitJob(Unknown Source)\n          at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)\n          at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)\n          at com.parc.uir.wikipedia.WikipediaJob.main(WikipediaJob.java:85)\nThis error goes away if I restart the TaskTrackers/JobTracker with a classpath which includes the needed code. Other classes (Mapper, Reducer) appear to be available out of the jar file specified in the JobConf, but not the InputFormat. Obviously, it's less than idea to have to restart the JobTracker whenever there's a change to a job-specific class.",
        "Issue Links": [
            "/jira/browse/HADOOP-16"
        ]
    },
    "NUTCH-192": {
        "Key": "NUTCH-192",
        "Summary": "meta data support for CrawlDatum",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "31/Jan/06 09:17",
        "Updated": "10/Feb/06 10:05",
        "Resolved": "10/Feb/06 10:05",
        "Description": "Supporting meta data in CrawlDatum would help to get a set of new nutch features realized and makes a lot possible to smaller special focused search engines.",
        "Issue Links": []
    },
    "NUTCH-193": {
        "Key": "NUTCH-193",
        "Summary": "move NDFS and MapReduce to a separate project",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Doug Cutting",
        "Reporter": "Doug Cutting",
        "Created": "01/Feb/06 02:52",
        "Updated": "08/Jun/11 21:32",
        "Resolved": "04/Feb/06 09:49",
        "Description": "The NDFS and MapReduce code should move from Nutch to a new Lucene sub-project named Hadoop.\nMy plan is to do this as follows:\n1. Move all code in the following packages from Nutch to Hadoop:\norg.apache.nutch.fs\norg.apache.nutch.io\norg.apache.nutch.ipc\norg.apache.nutch.mapred\norg.apache.nutch.ndfs\nThese packages will all be renamed to org.apache.hadoop, and Nutch code will be updated to reflect this.\n2. Move selected classes from Nutch to Hadoop, as follows:\norg.apache.nutch.util.NutchConf -> org.apache.hadoop.conf.Configuration\norg.apache.nutch.util.NutchConfigurable -> org.apache.hadoop.Configurable \norg.apache.nutch.util.NutchConfigured -> org.apache.hadoop.Configured\norg.apache.nutch.util.Progress -> org.apache.hadoop.util.Progress\norg.apache.nutch.util.LogFormatter-> org.apache.hadoop.util.LogFormatter\norg.apache.nutch.util.Daemon -> org.apache.hadoop.util.Daemon\n3. Add a jar containing all of the above the Nutch's lib directory.\nDoes this plan sound reasonable?",
        "Issue Links": [
            "/jira/browse/HADOOP-1"
        ]
    },
    "NUTCH-194": {
        "Key": "NUTCH-194",
        "Summary": "Nutch-169 introduced two tiny bugs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Marko Bauhardt",
        "Created": "01/Feb/06 06:35",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Feb/06 21:59",
        "Description": "1.) In a loop in QueryFilters the Cache was overwrite in any iteration.\n2.) the PluginClass hasn't any access to the nuchConf.",
        "Issue Links": []
    },
    "NUTCH-195": {
        "Key": "HADOOP-16",
        "Summary": "RPC call times out while indexing map task is computing splits",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.1.0",
        "Fix Version/s": "0.1.0",
        "Component/s": "None",
        "Assignee": "Mike Cafarella",
        "Reporter": "Chris Schneider",
        "Created": "01/Feb/06 08:25",
        "Updated": "08/Jul/09 16:51",
        "Resolved": "03/Mar/06 08:09",
        "Description": "We've been using Nutch 0.8 (MapReduce) to perform some internet crawling. Things seemed to be going well until...\n060129 222409 Lost tracker 'tracker_56288'\n060129 222409 Task 'task_m_10gs5f' has been lost.\n060129 222409 Task 'task_m_10qhzr' has been lost.\n   ........\n   ........\n060129 222409 Task 'task_r_zggbwu' has been lost.\n060129 222409 Task 'task_r_zh8dao' has been lost.\n060129 222455 Server handler 8 on 8010 caught: java.net.SocketException: Socket closed\njava.net.SocketException: Socket closed\n        at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:99)\n        at java.net.SocketOutputStream.write(SocketOutputStream.java:136)\n        at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)\n        at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)\n        at java.io.DataOutputStream.flush(DataOutputStream.java:106)\n        at org.apache.nutch.ipc.Server$Handler.run(Server.java:216)\n060129 222455 Adding task 'task_m_cia5po' to set for tracker 'tracker_56288'\n060129 223711 Adding task 'task_m_ffv59i' to set for tracker 'tracker_25647'\nI'm hoping that someone could explain why task_m_cia5po got added to tracker_56288 after this tracker was lost.\nThe Crawl .main process died with the following output:\n060129 221129 Indexer: adding segment: /user/crawler/crawl-20060129091444/segments/20060129200246\nException in thread \"main\" java.io.IOException: timed out waiting for response\n    at org.apache.nutch.ipc.Client.call(Client.java:296)\n    at org.apache.nutch.ipc.RPC$Invoker.invoke(RPC.java:127)\n    at $Proxy1.submitJob(Unknown Source)\n    at org.apache.nutch.mapred.JobClient.submitJob(JobClient.java:259)\n    at org.apache.nutch.mapred.JobClient.runJob(JobClient.java:288)\n    at org.apache.nutch.indexer.Indexer.index(Indexer.java:263)\n    at org.apache.nutch.crawl.Crawl.main(Crawl.java:127)\nHowever, it definitely seems as if the JobTracker is still waiting for the job to finish (no failed jobs).\nDoug Cutting's response:\nThe bug here is that the RPC call times out while the map task is computing splits.  The fix is that the job tracker should not compute splits until after it has returned from the submitJob RPC.  Please submit a bug in Jira to help remind us to fix this.",
        "Issue Links": [
            "/jira/browse/HADOOP-12"
        ]
    },
    "NUTCH-196": {
        "Key": "NUTCH-196",
        "Summary": "lib-xml and lib-log4j plugins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "02/Feb/06 02:34",
        "Updated": "29/Mar/06 18:14",
        "Resolved": "29/Mar/06 18:15",
        "Description": "Many places in Nutch use XML. Parsing XML using the JDK API is painful. I propose to add one (or more) library plugins with JDOM, DOM4J, Jaxen, etc. This should simplify the current deployment, and help plugin writers to use the existing API.\nSimilarly, many plugins use log4j. Either we add it to the /lib, or we could create a lib-log4j plugin.",
        "Issue Links": []
    },
    "NUTCH-197": {
        "Key": "NUTCH-197",
        "Summary": "NullPointerException in TaskRunner if application jar does not have \"lib\" directory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Owen O'Malley",
        "Created": "02/Feb/06 08:01",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "02/Feb/06 08:20",
        "Description": "When running a map/reduce application from a jar file, if the jar file does not have a \"lib\" directory the job dies with a NullPointerException.",
        "Issue Links": []
    },
    "NUTCH-198": {
        "Key": "NUTCH-198",
        "Summary": "SWF parser",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "02/Feb/06 21:24",
        "Updated": "13/Feb/06 15:11",
        "Resolved": "04/Feb/06 03:50",
        "Description": "This is a parser for the Flash SWF files. It uses JavaSWF2 library (BSD license), and uses some heuristic to extract as much text as possible (including potential links) from ActionScript sections.\nIf there are no objections, I'd like to add it soon.",
        "Issue Links": []
    },
    "NUTCH-199": {
        "Key": "HADOOP-17",
        "Summary": "tool to mount ndfs on linux",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fs",
        "Assignee": null,
        "Reporter": "John Xing",
        "Created": "03/Feb/06 15:59",
        "Updated": "03/Aug/06 17:46",
        "Resolved": "07/Feb/06 04:18",
        "Description": "tool to mount ndfs on linux. It depends on fuse and fuse-j.",
        "Issue Links": [
            "/jira/browse/HADOOP-4"
        ]
    },
    "NUTCH-200": {
        "Key": "NUTCH-200",
        "Summary": "OpenSearch Servlet ist broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "Michael Nebel",
        "Created": "03/Feb/06 22:33",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "07/Feb/06 05:13",
        "Description": "the last path from 31.1.2006 seems to have the servlet boken. My suggestion to fix:\n\u2014 OpenSearchServlet.java      2006-02-03 14:30:33.000000000 +0100\n+++ src/java/org/apache/nutch/searcher/OpenSearchServlet.java   2006-02-01 09:29:19.000000000 +0100\n@@ -59,10 +59,10 @@\n   private NutchBean bean;\n   private NutchConf nutchConf;\n\npublic void init(ServletConfig config) throws ServletException {\n+  public void init(ServletConfig config, NutchConf nutchConf) throws ServletException {\n     try \n{\n-      this.nutchConf = new NutchConf();\n       bean = NutchBean.get(config.getServletContext(), nutchConf);\n+      this.nutchConf = nutchConf;\n     }\n catch (IOException e) \n{\n       throw new ServletException(e);\n     }\n\nOpensearch also needs Xalan. This seems not to be included anymore.",
        "Issue Links": []
    },
    "NUTCH-201": {
        "Key": "NUTCH-201",
        "Summary": "add support for subcollections",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "04/Feb/06 05:22",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "06/Jun/06 03:12",
        "Description": "Subcollection is a subset of an index. Subcollections are defined\nby urlpatterns in form of white/blacklist. So to get the page into\nsubcollection it must match the whitelist and not the blacklist.\nSubcollection definitions are read from a file subcollections.xml\nand the format is as follows (imagine here that you are crawling all\nthe virtualhosts from apache.org and you wan't to tag pages with\nurl pattern \"http://lucene.apache.org/\" to be part of subcollection\nlucene.\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<subcollections>\n       <subcollection>\n               <name>lucene</name>\n               <id>lucene</id>\n               <whitelist>http://lucene.apache.org/</whitelist>\n               <blacklist />\n       </subcollection>\n</subcollections>\nplugin contains indexingfilter, query filter and supporting classes",
        "Issue Links": []
    },
    "NUTCH-202": {
        "Key": "HADOOP-20",
        "Summary": "Mapper, Reducer need an occasion to cleanup after the last record is processed.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.1.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michel Tourn",
        "Created": "04/Feb/06 09:06",
        "Updated": "08/Jul/09 16:51",
        "Resolved": "10/Feb/06 02:22",
        "Description": "Mapper, Reducer need an occasion to do some cleanup after the last record is processed.\nProposal (patch attached)\nin interface Mapper:\n add method void finished();\nin interface Reducer:\n add method void finished();\nfinished() methods are called from MapTask, CombiningCollector, ReduceTask.\n------------\nKnown limitation: Fetcher (a multithreaded MapRunnable) does not call finished().\nThis is not currently a problem bec. fetcher Map/Reduce modules do not do anything in finished().\nThe right way to add finished() support to Fetcher would be to wait for all threads to finish, \nthen do:\n     if (collector instanceof CombiningCollector) ((CombiningCollector)collector).finished();\n------------\npatch begins: (svn trunk)\nIndex: src/test/org/apache/nutch/mapred/MapredLoadTest.java\n===================================================================\n\u2014 src/test/org/apache/nutch/mapred/MapredLoadTest.java\t(revision 374781)\n+++ src/test/org/apache/nutch/mapred/MapredLoadTest.java\t(working copy)\n@@ -69,6 +69,8 @@\n                 out.collect(new IntWritable(Math.abs(r.nextInt())), new IntWritable(randomVal));\n             }\n         }\n+        public void finished() \n{\n+        }\n     }\n     static class RandomGenReducer implements Reducer {\n         public void configure(JobConf job) {\n@@ -81,6 +83,8 @@\n                 out.collect(new UTF8(\"\" + val), new UTF8(\"\"));\n             }\n         }\n+        public void finished() {+        }\n     }\n     static class RandomCheckMapper implements Mapper {\n         public void configure(JobConf job) \n{\n@@ -92,6 +96,8 @@\n \n             out.collect(new IntWritable(Integer.parseInt(str.toString().trim())), new IntWritable(1));\n         }\n+        public void finished() \n{\n+        }\n     }\n     static class RandomCheckReducer implements Reducer {\n         public void configure(JobConf job) {\n@@ -106,6 +112,8 @@\n             }\n             out.collect(new IntWritable(keyint), new IntWritable(count));\n         }\n+        public void finished() {+        }\n     }\n     int range;\nIndex: src/test/org/apache/nutch/fs/TestNutchFileSystem.java\n===================================================================\n\u2014 src/test/org/apache/nutch/fs/TestNutchFileSystem.java\t(revision 374783)\n+++ src/test/org/apache/nutch/fs/TestNutchFileSystem.java\t(working copy)\n@@ -155,6 +155,8 @@\n       reporter.setStatus(\"wrote \" + name);\n     }\n+    \n+    public void finished() {}\n   }\n   public static void writeTest(NutchFileSystem fs, boolean fastCheck)\n@@ -247,6 +249,9 @@\n       reporter.setStatus(\"read \" + name);\n     }\n+    \n+    public void finished() {}\n+    \n   }\n   public static void readTest(NutchFileSystem fs, boolean fastCheck)\n@@ -339,6 +344,9 @@\n         in.close();\n       }\n     }\n+    \n+    public void finished() {}\n+    \n   }\n   public static void seekTest(NutchFileSystem fs, boolean fastCheck)\nIndex: src/java/org/apache/nutch/indexer/DeleteDuplicates.java\n===================================================================\n\u2014 src/java/org/apache/nutch/indexer/DeleteDuplicates.java\t(revision 374776)\n+++ src/java/org/apache/nutch/indexer/DeleteDuplicates.java\t(working copy)\n@@ -225,6 +225,7 @@\n         }\n       }\n     }\n+    public void finished() {}\n   }\n   private NutchFileSystem fs;\n@@ -265,6 +266,8 @@\n       reader.close();\n     }\n   }\n+  \n+  public void finished() {}\n   /** Write nothing. */\n   public RecordWriter getRecordWriter(final NutchFileSystem fs,\nIndex: src/java/org/apache/nutch/indexer/Indexer.java\n===================================================================\n\u2014 src/java/org/apache/nutch/indexer/Indexer.java\t(revision 374778)\n+++ src/java/org/apache/nutch/indexer/Indexer.java\t(working copy)\n@@ -227,6 +227,8 @@\n     output.collect(key, new ObjectWritable(doc));\n   }\n+  \n+  public void finished() {}\n   public void index(File indexDir, File crawlDb, File linkDb, File[] segments)\n     throws IOException \n{\nIndex: src/java/org/apache/nutch/segment/SegmentReader.java\n===================================================================\n--- src/java/org/apache/nutch/segment/SegmentReader.java\t(revision 374778)\n+++ src/java/org/apache/nutch/segment/SegmentReader.java\t(working copy)\n@@ -143,7 +143,9 @@\n     }\n     output.collect(key, new ObjectWritable(dump.toString()));\n   }\n-\n+  \n+  public void finished() {}\n+  \n   public void reader(File segment) throws IOException {\n     LOG.info(\"Reader: segment: \" + segment);\nIndex: src/java/org/apache/nutch/mapred/Mapper.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/Mapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/Mapper.java\t(working copy)\n@@ -39,4 +39,9 @@\n   void map(WritableComparable key, Writable value,\n            OutputCollector output, Reporter reporter)\n     throws IOException;\n+\n+  /** Called after the last \n{@link #map}\n call on this Mapper object.\n+      Typical implementations do nothing.\n+  */\n+  void finished();\n }\nIndex: src/java/org/apache/nutch/mapred/lib/RegexMapper.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/lib/RegexMapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/RegexMapper.java\t(working copy)\n@@ -53,4 +53,5 @@\n       output.collect(new UTF8(matcher.group(group)), new LongWritable(1));\n     }\n   }\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/InverseMapper.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/lib/InverseMapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/InverseMapper.java\t(working copy)\n@@ -38,4 +38,6 @@\n     throws IOException \n{\n     output.collect((WritableComparable)value, key);\n   }\n+\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/IdentityReducer.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/lib/IdentityReducer.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/IdentityReducer.java\t(working copy)\n@@ -42,4 +42,5 @@\n     }\n   }\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/IdentityMapper.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/lib/IdentityMapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/IdentityMapper.java\t(working copy)\n@@ -39,4 +39,5 @@\n     output.collect(key, val);\n   }\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/LongSumReducer.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/lib/LongSumReducer.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/LongSumReducer.java\t(working copy)\n@@ -47,4 +47,6 @@\n     // output sum\n     output.collect(key, new LongWritable(sum));\n   }\n+\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/lib/TokenCountMapper.java\t(working copy)\n@@ -50,4 +50,6 @@\n       output.collect(new UTF8(st.nextToken()), new LongWritable(1));\n     }\n   }\n+\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/mapred/ReduceTask.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/ReduceTask.java\t(revision 374781)\n+++ src/java/org/apache/nutch/mapred/ReduceTask.java\t(working copy)\n@@ -275,6 +275,7 @@\n       }\n     } finally {\n+      reducer.finished();\n       in.close();\n       lfs.delete(new File(sortedFile));           // remove sorted\n       out.close(reporter);\nIndex: src/java/org/apache/nutch/mapred/MapTask.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/MapTask.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/MapTask.java\t(working copy)\n@@ -50,7 +50,7 @@\n   public void write(DataOutput out) throws IOException \n{\n     super.write(out);\n     split.write(out);\n-    \n+\n   }\n   public void readFields(DataInput in) throws IOException \n{\n     super.readFields(in);\n@@ -126,6 +126,10 @@\n         }\n\n       } finally {\n+        if (combining) \n{\n+          ((CombiningCollector)collector).finished();\n+        }\n+\n         in.close();                               // close input\n       }\n     } finally {\n@@ -147,5 +151,5 @@\n   public NutchConf getConf() \n{\n     return this.nutchConf;\n   }\n\n+\n }\nIndex: src/java/org/apache/nutch/mapred/MapRunner.java\n===================================================================\n\n\n\nsrc/java/org/apache/nutch/mapred/MapRunner.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/MapRunner.java\t(working copy)\n@@ -38,18 +38,22 @@\n   public void run(RecordReader input, OutputCollector output,\n                   Reporter reporter)\n     throws IOException {\n\n\n\n\nwhile (true) {\n// allocate new key & value instances\nWritableComparable key =\n(WritableComparable)job.newInstance(inputKeyClass);\nWritable value = (Writable)job.newInstance(inputValueClass);\n+    try \nUnknown macro: {+      while (true) {\n+        // allocate new key & value instances\n+        WritableComparable key =\n+          (WritableComparable)job.newInstance(inputKeyClass);\n+        Writable value = (Writable)job.newInstance(inputValueClass);\n \n-      // read next key & value\n-      if (!input.next(key, value))\n-        return;\n+        // read next key & value\n+        if (!input.next(key, value))\n+          return;\n \n-      // map pair to output\n-      mapper.map(key, value, output, reporter);\n+        // map pair to output\n+        mapper.map(key, value, output, reporter);\n+      }+    } \n finally \n{\n+        mapper.finished();\n     }\n   }\n\nIndex: src/java/org/apache/nutch/mapred/CombiningCollector.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/CombiningCollector.java\t(revision 374780)\n+++ src/java/org/apache/nutch/mapred/CombiningCollector.java\t(working copy)\n@@ -78,4 +78,9 @@\n     count = 0;\n   }\n+  public synchronized void finished()\n+  \n{\n+    combiner.finished();\n+  }\n+\n }\nIndex: src/java/org/apache/nutch/mapred/Reducer.java\n===================================================================\n\u2014 src/java/org/apache/nutch/mapred/Reducer.java\t(revision 374737)\n+++ src/java/org/apache/nutch/mapred/Reducer.java\t(working copy)\n@@ -38,4 +38,10 @@\n   void reduce(WritableComparable key, Iterator values,\n               OutputCollector output, Reporter reporter)\n     throws IOException;\n+\n+  /** Called after the last \n{@link #reduce}\n call on this Reducer object.\n+      Typical implementations do nothing.\n+  */\n+  void finished();\n+\n }\nIndex: src/java/org/apache/nutch/crawl/CrawlDbReader.java\n===================================================================\n\u2014 src/java/org/apache/nutch/crawl/CrawlDbReader.java\t(revision 374737)\n+++ src/java/org/apache/nutch/crawl/CrawlDbReader.java\t(working copy)\n@@ -50,9 +50,9 @@\n /**\n\nRead utility for the CrawlDB.\n\n\n*\n+ *\n\n\n@author Andrzej Bialecki\n\n\n*\n+ *\n  */\n public class CrawlDbReader \n{\n \n@@ -68,6 +68,7 @@\n       output.collect(new UTF8(\"retry\"), new LongWritable(cd.getRetriesSinceFetch()));\n       output.collect(new UTF8(\"score\"), new LongWritable((long) (cd.getScore() * 1000.0)));\n     }\n+    public void finished() {}\n   }\n\n   public static class CrawlDbStatReducer implements Reducer \n{\n@@ -121,6 +122,7 @@\n         output.collect(new UTF8(\"avg score\"), new LongWritable(total / cnt));\n       }\n     }\n+    public void finished() {}\n   }\n   public static class CrawlDbDumpReducer implements Reducer {\n@@ -133,8 +135,11 @@\n     public void configure(JobConf job) {\n     }\n+\n+    public void finished() \n{\n+    }\n   }\n\n+\n   public void processStatJob(String crawlDb, NutchConf config) throws IOException \n{\n     LOG.info(\"CrawlDb statistics start: \" + crawlDb);\n     File tmpFolder = new File(crawlDb, \"stat_tmp\" + System.currentTimeMillis());\n@@ -219,7 +224,7 @@\n       System.out.println(\"not found\");\n     }\n   }\n+\n   public void processDumpJob(String crawlDb, String output, NutchConf config) throws IOException \n{\n \n     LOG.info(\"CrawlDb dump: starting\");\n@@ -270,4 +275,5 @@\n     }\n     return;\n   }\n+\n }\nIndex: src/java/org/apache/nutch/crawl/LinkDb.java\n===================================================================\n\n\n\nsrc/java/org/apache/nutch/crawl/LinkDb.java\t(revision 374779)\n+++ src/java/org/apache/nutch/crawl/LinkDb.java\t(working copy)\n@@ -118,7 +118,8 @@\n     output.collect(key, result);\n   }\n\n\n\n\n\n-\n+  public void finished() {}\n+\t\n   public void invert(File linkDb, File segmentsDir) throws IOException \n{\n     LOG.info(\"LinkDb: starting\");\n     LOG.info(\"LinkDb: linkdb: \" + linkDb);\nIndex: src/java/org/apache/nutch/crawl/Injector.java\n===================================================================\n--- src/java/org/apache/nutch/crawl/Injector.java\t(revision 374779)\n+++ src/java/org/apache/nutch/crawl/Injector.java\t(working copy)\n@@ -65,6 +65,8 @@\n                                              interval));\n       }\n     }\n+    \n+    public void finished() {}\n   }\n   /** Combine multiple new entries for a url. */\n@@ -76,6 +78,7 @@\n       throws IOException \n{\n       output.collect(key, (Writable)values.next()); // just collect first value\n     }\n+    public void finished() {}\n   }\n   /** Construct an Injector. */\nIndex: src/java/org/apache/nutch/crawl/Generator.java\n===================================================================\n\u2014 src/java/org/apache/nutch/crawl/Generator.java\t(revision 374779)\n+++ src/java/org/apache/nutch/crawl/Generator.java\t(working copy)\n@@ -63,6 +63,8 @@\n       output.collect(crawlDatum, key);          // invert for sort by score\n     }\n+    public void finished() {}\n+    \n     /** Partition by host (value). */\n     public int getPartition(WritableComparable key, Writable value,\n                             int numReduceTasks) \n{\nIndex: src/java/org/apache/nutch/crawl/CrawlDbReducer.java\n===================================================================\n--- src/java/org/apache/nutch/crawl/CrawlDbReducer.java\t(revision 374781)\n+++ src/java/org/apache/nutch/crawl/CrawlDbReducer.java\t(working copy)\n@@ -115,4 +115,5 @@\n     }\n   }\n+  public void finished() {}\n }\nIndex: src/java/org/apache/nutch/parse/ParseSegment.java\n===================================================================\n\u2014 src/java/org/apache/nutch/parse/ParseSegment.java\t(revision 374776)\n+++ src/java/org/apache/nutch/parse/ParseSegment.java\t(working copy)\n@@ -78,6 +78,8 @@\n     throws IOException \n{\n     output.collect(key, (Writable)values.next()); // collect first value\n   }\n+  \n+  public void finished() {}\n   public void parse(File segment) throws IOException {\n     LOG.info(\"Parse: starting\");",
        "Issue Links": []
    },
    "NUTCH-203": {
        "Key": "NUTCH-203",
        "Summary": "ParseSegment throws InstantiationException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Marko Bauhardt",
        "Created": "06/Feb/06 17:40",
        "Updated": "13/Mar/06 20:22",
        "Resolved": "13/Mar/06 20:22",
        "Description": "ParseSegment has no default constructor",
        "Issue Links": []
    },
    "NUTCH-204": {
        "Key": "NUTCH-204",
        "Summary": "multiple field values in HitDetails",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "07/Feb/06 08:21",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "28/Feb/06 07:31",
        "Description": "Improvement as Howie Wang suggested.\nhttp://mail-archives.apache.org/mod_mbox/lucene-nutch-dev/200601.mbox/%3c43D7D45A.2070609@nutch.org%3e",
        "Issue Links": []
    },
    "NUTCH-205": {
        "Key": "NUTCH-205",
        "Summary": "Wrong 'fetch date' for non available pages",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.7.1",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "M.Oliver Scheele",
        "Created": "07/Feb/06 19:39",
        "Updated": "23/Sep/06 19:49",
        "Resolved": "23/Sep/06 19:49",
        "Description": "Web-Pages that couldn't be fetched because of a time-out wouldn't be refetched anymore.\nThe next fetch in the web-db is set to Long.max.\nExample:\n-------------\nWhile fetching our URLs, we got some errors like this:\n60202 154316 fetch of http://www.test-domain.de/crawl_html/page_2.html  failed with: java.lang.Exception: org.apache.nutch.protocol.RetryLater: Exceeded ttp.max.delays: retry later.\nThat seems to be ok and indicates some network problems.\nThe problem is that the entry in the Webdb shows the following:\nPage 4: Version: 4\nURL: http://www.test-domain.de/crawl_html/page_2.html\nID: b360ec931855b0420776909bd96557c0\nNext fetch: Sun Aug 17 07:12:55 CET 292278994\nRetries since fetch: 0\nRetry interval: 0 days\nThe 'Next fetch' date is set to the year '292278994'.\nProbably I wouldn't be able to see the refetch alive. \nA page that couldn't be crawled because of networks-problems,\nshould be refetched with the next crawl (== set next fetch date current time + 1h).\nPossible Bug-Fixing:\n----------------------------\nWhen updating the web-db the method updateForSegment() in the UpdateDatabaseTool.class,\nset the fetch-date always to Long.max for any (unknown) exception during fetching.\nThe RETRY status is not always set correctly.\nChange the following lines:\n} else if (fo.getProtocolStatus().getCode() == ProtocolStatus.RETRY &&\n                       page.getRetriesSinceFetch() < MAX_RETRIES) \n{\n\n              pageRetry(fo);                      // retry later\n\n            }\n else \n{\n              pageGone(fo);                       // give up: page is gone\n            }",
        "Issue Links": []
    },
    "NUTCH-206": {
        "Key": "NUTCH-206",
        "Summary": "search server throws InstantiationException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jimmy",
        "Created": "08/Feb/06 00:45",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "13/Mar/06 20:19",
        "Description": "060207 230215 23 Server connection on port 8888 from 127.0.0.1 caught: java.lang\n.RuntimeException: java.lang.InstantiationException: org.apache.nutch.searcher.Q\nuery\njava.lang.RuntimeException: java.lang.InstantiationException: org.apache.nutch.s\nearcher.Query\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:23\n8)\n        at org.apache.hadoop.ipc.RPC$Invocation.readFields(RPC.java:88)\n        at org.apache.hadoop.ipc.Server$Connection.run(Server.java:138)\nCaused by: java.lang.InstantiationException: org.apache.nutch.searcher.Query\n        at java.lang.Class.newInstance0(Class.java:335)\n        at java.lang.Class.newInstance(Class.java:303)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:23\n1)\n        ... 2 more\n060207 230215 23 Server connection on port 8888 from 127.0.0.1: exiting\n060207 230225 24 Server connection on port 8888 from 127.0.0.1: starting",
        "Issue Links": []
    },
    "NUTCH-207": {
        "Key": "NUTCH-207",
        "Summary": "Bandwidth target for fetcher rather than a thread count",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.9",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Rod Taylor",
        "Created": "08/Feb/06 03:44",
        "Updated": "18/May/14 03:44",
        "Resolved": "16/May/14 13:20",
        "Description": "Increases or decreases the number of threads from the starting value (fetcher.threads.fetch) up to a maximum (fetcher.threads.maximum) to achieve a target bandwidth (fetcher.threads.bandwidth).\nIt seems to be able to keep within 10% of the target bandwidth even when large numbers of errors are found or when a number of large pages is run across.\nTo achieve more accurate tracking Nutch should keep track of protocol overhead as well as the volume of pages downloaded.",
        "Issue Links": []
    },
    "NUTCH-208": {
        "Key": "NUTCH-208",
        "Summary": "http: proxy exception list:",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            1.3,                                            nutchgora",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Matthias G\u00fcnter",
        "Created": "09/Feb/06 00:28",
        "Updated": "28/May/15 00:27",
        "Resolved": "28/May/15 00:27",
        "Description": "I suggest that a parameter is added to nutch-default.xml which allows to generate a proxy exception list. \n<property>\n  <name>http.proxy.exception.list</name>\n  <value></value>\n  <description>URL's and hosts that don't use the proxy (e.g. intranets)</description>\n</property>\nThis is useful when scanning intranet/internet combinations from behind a firewall. A preliminary patch is added to this extend to this request, showing the changes. We will test it and update it if necessary. this also reflects the reality in web browsers, where there is in most cases an exception list.",
        "Issue Links": []
    },
    "NUTCH-209": {
        "Key": "NUTCH-209",
        "Summary": "include nutch jar in mapred jobs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Doug Cutting",
        "Created": "10/Feb/06 03:38",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "10/Feb/06 08:19",
        "Description": "I just added a simple way in Hadoop to specify the job jar file.  When constructing a JobConf one can specify a class whose containing jar is set to be the job's jar.  To take advantage of this in Nutch, we could add a util class:\npublic class NutchJob extends JobConf {\n  public NutchJob(Configuration conf) \n{\n    super(conf, NutchJob.class);\n  }\n}\nThen change all of the places where we construct a JobConf to instead construct a NutchJob.\nFinally, we should add an ant target called 'job' that constructs a job jar, containing all of the classes and the plugins, and make this the default target.  This way all Nutch code can be distributed with each job as it is submitted, and daemons would only need to be restarted when Hadoop code is updated.\nDoes this sound reasonable?",
        "Issue Links": []
    },
    "NUTCH-210": {
        "Key": "NUTCH-210",
        "Summary": "Context.xml file for Nutch web application",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.7.2,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "15/Feb/06 16:29",
        "Updated": "27/Mar/06 23:55",
        "Resolved": "27/Mar/06 23:55",
        "Description": "Currently the nutch web gui references a few parameters that are highly dynamic, e.g., searcher.dir. These dynamic properties are read from the configuration files, such as nutch-default.xml. One problem I'm noticing however is that in order to change the parameter in the built webapp (the WAR file), I am required to change the parameter first in the checked out Nutch source tree, rebuild the webapp, then redploy. Or, if I'm feeling really gutsty, I can go poke around in the unpackaged WAR file if the servlet container exposes it to me, and try and modify the nutch-default.xml file that way. However, I think that it would be really nice (and highly useful for that matter) to factor out some of the more dynamic parameters of the web application into a separate deliverable Context.xml file that would accompany the webapp. The Context.xml file would be deployed in the webapps directory, as oppossed to the WAR file itself, and the parameters could be updated there, and changed as many times as necessary, without rebuilding the WAR file. \nOf course this will involve making minor modifications in the web GUI to where some of the dynamic parameters are read from (i.e., make it read them from the Context.xml file (using application.getParameter most likely). Right now the only one I can think of is searcher.dir, but I'm sure that there are others (in particular the searcher.dir one is the most annoying for me). \nThe timeframe on this patch will be within the next month.\nThanks,\n  Chris",
        "Issue Links": []
    },
    "NUTCH-211": {
        "Key": "NUTCH-211",
        "Summary": "FetchedSegments leave readers open",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Stefan Groschupf",
        "Reporter": "Stefan Groschupf",
        "Created": "15/Feb/06 20:10",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "17/Feb/06 08:32",
        "Description": "I have a case here where the NutchBean is instantiated more than once, however I do cache the nutch bean, but in some situations the bean needs to re created. The problem is the  FetchedSegments leaves open all reads it uses. So a nio Exception is thrown as soon I try to create the NutchBean again. \nI would suggest to add a close method to  FetchedSegments and all involved objects to be able cleanly shutting down the NutchBean.\nAny comments? Would a patch be welcome?\nCaused by: java.nio.channels.ClosedChannelException\nat sun.nio.ch.FileChannelImpl.ensureOpen(FileChannelImpl.java:89)\nat sun.nio.ch.FileChannelImpl.position(FileChannelImpl.java:272)\nat org.apache.nutch.fs.LocalFileSystem$LocalNFSFileInputStream.seek(LocalFileSystem.java:83)\nat org.apache.nutch.fs.NFSDataInputStream$Checker.seek(NFSDataInputStream.java:66)\nat org.apache.nutch.fs.NFSDataInputStream$PositionCache.seek(NFSDataInputStream.java:162)\nat org.apache.nutch.fs.NFSDataInputStream$Buffer.seek(NFSDataInputStream.java:191)\nat org.apache.nutch.fs.NFSDataInputStream.seek(NFSDataInputStream.java:241)\nat org.apache.nutch.io.SequenceFile$Reader.seek(SequenceFile.java:403)\nat org.apache.nutch.io.MapFile$Reader.seek(MapFile.java:329)\nat org.apache.nutch.io.MapFile$Reader.get(MapFile.java:374)\nat org.apache.nutch.mapred.MapFileOutputFormat.getEntry(MapFileOutputFormat.java:76)\nat org.apache.nutch.searcher.FetchedSegments$Segment.getEntry(FetchedSegments.java:93)\nat org.apache.nutch.searcher.FetchedSegments$Segment.getParseText(FetchedSegments.java:84)\nat org.apache.nutch.searcher.FetchedSegments.getSummary(FetchedSegments.java:147)\nat org.apache.nutch.searcher.NutchBean.getSummary(NutchBean.java:321)",
        "Issue Links": []
    },
    "NUTCH-212": {
        "Key": "NUTCH-212",
        "Summary": "ant build problem with locale-sr",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Alain Fankhauser",
        "Created": "17/Feb/06 21:07",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "21/Feb/06 23:11",
        "Description": "Problem while executing ant from eclipse: \nbuild.xml: \n    <antcall target=\"generate-locale\">\n      <param name=\"doc.locale\" value=\"sr\"/>\n    </antcall>\nerrormessage:\ngenerate-locale:\n[echo] Generating docs for locale=sr\n[xslt] Transforming into C:\\eclipse_projects\\nutchTrunk\\docs\\sr\n[xslt] Processing C:\\eclipse_projects\\nutchTrunk\\src\\web\\pages\\sr\\about.xml to C:\\eclipse_projects\\nutchTrunk\\docs\\sr\\about.html\n[xslt] Loading stylesheet C:\\eclipse_projects\\nutchTrunk\\build\\docs\\sr\\nutch-page.xsl\n[xslt] C:/eclipse_projects/nutchTrunk/src/web/pages/sr/about.xml:1: Fatal Error! Dokumentwurzelelement fehlt\n[xslt] Failed to process C:\\eclipse_projects\\nutchTrunk\\src\\web\\pages\\sr\\about.xml\nBUILD FAILED\nC:\\eclipse_projects\\nutchTrunk\\build.xml:393: The following error occurred while executing this line:\nC:\\eclipse_projects\\nutchTrunk\\build.xml:324: Fatal error during transformation",
        "Issue Links": []
    },
    "NUTCH-213": {
        "Key": "NUTCH-213",
        "Summary": "checkstyle",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Stefan Groschupf",
        "Created": "19/Feb/06 06:24",
        "Updated": "22/May/13 03:54",
        "Resolved": "30/Apr/13 20:46",
        "Description": "Adding checkstyle target to ant build file to support contributers verifying whitespace problems.",
        "Issue Links": []
    },
    "NUTCH-214": {
        "Key": "NUTCH-214",
        "Summary": "Added Links to web site to search mailling list",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jake Vanderdray",
        "Created": "21/Feb/06 05:58",
        "Updated": "21/Feb/06 20:15",
        "Resolved": "21/Feb/06 20:15",
        "Description": "This is a patch file that adds links on the mailling_lists page of the the nutch web site to search the list archives.\nIndex: mailing_lists.xml\n===================================================================\n\u2014 mailing_lists.xml   (revision 378387)\n+++ mailing_lists.xml   (working copy)\n@@ -23,6 +23,7 @@\n       <ul>\n         <li><a href=\"mailto:nutch-user-subscribe@lucene.apache.org\">Subscribe to List</a></li>\n         <li><a href=\"mailto:nutch-user-unsubscribe@lucene.apache.org\">Unsubscribe from List</a></li>\n+        <li><a href=\"http://www.mail-archive.com/nutch-user%40lucene.apache.org/\">Search List Archive</a></li>\n         <li><a href=\"http://lucene.apache.org/mail/nutch-user/\">View List Archive</a></li>\n       </ul>\n       <note>In order to post to the list, it is necessary to first subscribe to it.</note>\n@@ -41,6 +42,7 @@\n       <ul>\n         <li><a href=\"mailto:nutch-dev-subscribe@lucene.apache.org\">Subscribe to List</a></li>\n         <li><a href=\"mailto:nutch-dev-unsubscribe@lucene.apache.org\">Unsubscribe from List</a></li>\n+        <li><a href=\"http://www.mail-archive.com/nutch-dev%40lucene.apache.org/\">Search List Archive</a></li>\n         <li><a href=\"http://lucene.apache.org/mail/nutch-dev/\">View List Archive</a></li>\n       </ul>\n       <note>In order to post to the list, it is necessary to first subscribe to it.</note>\n@@ -56,6 +58,7 @@\n       <ul>\n         <li><a href=\"mailto:nutch-commits-subscribe@lucene.apache.org\">Subscribe to List</a></li>\n         <li><a href=\"mailto:nutch-commits-unsubscribe@lucene.apache.org\">Unsubscribe from List</a></li>\n+        <li><a href=\"http://www.mail-archive.com/nutch-commits%40lucene.apache.org/\">Search List Archive</a></li>\n         <li><a href=\"http://lucene.apache.org/mail/nutch-commits/\">View List Archive</a></li>\n       </ul>\n     </section>\n@@ -75,6 +78,7 @@\n       <ul>\n         <li><a href=\"mailto:nutch-agent-subscribe@lucene.apache.org\">Subscribe to List</a></li>\n         <li><a href=\"mailto:nutch-agent-unsubscribe@lucene.apache.org\">Unsubscribe from List</a></li>\n+        <li><a href=\"http://www.mail-archive.com/nutch-agent%40lucene.apache.org/\">Search List Archive</a></li>\n         <li><a href=\"http://lucene.apache.org/mail/nutch-agent/\">View List Archive</a></li>\n       </ul>",
        "Issue Links": []
    },
    "NUTCH-215": {
        "Key": "NUTCH-215",
        "Summary": "Plugin execution order",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Enrico Triolo",
        "Created": "21/Feb/06 19:26",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 20:25",
        "Description": "This patch allows nutch to automatically guess the correct order of execution of plugins, depending on their dependencies.\nThis means that, for example, if plugin A depends on plugin B (as stated in the plugins.xml file), then B will be executed before A.",
        "Issue Links": []
    },
    "NUTCH-216": {
        "Key": "NUTCH-216",
        "Summary": "cannot build in windows",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "bin zhu",
        "Created": "25/Feb/06 01:08",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "25/Feb/06 04:13",
        "Description": "Buildfile: build.xml\ninit:\n[mkdir] Created dir: C:\\data\\asf\\nutch-trunk\\build\n[mkdir] Created dir: C:\\data\\asf\\nutch-trunk\\build\\classes\n[mkdir] Created dir: C:\\data\\asf\\nutch-trunk\\build\\test\n[mkdir] Created dir: C:\\data\\asf\\nutch-trunk\\build\\test\\classes\n[mkdir] Created dir: C:\\data\\asf\\nutch-trunk\\build\\hadoop\n[unjar] Expanding: C:\\data\\asf\\nutch-trunk\\lib\\hadoop-0.1-dev.jar into C:\\da\nta\\asf\\nutch-trunk\\build\\hadoop\nBUILD FAILED\nC:\\data\\asf\\nutch-trunk\\build.xml:60: Execute failed: java.io.IOException: Creat\neProcess: tar xzf .././build/hadoop/bin.tgz error=2",
        "Issue Links": []
    },
    "NUTCH-217": {
        "Key": "NUTCH-217",
        "Summary": "InstantiationException when deserializing Query (no parameterless constructor)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dawid Weiss",
        "Created": "27/Feb/06 16:41",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "12/Mar/06 18:10",
        "Description": "I've been playing with the trunk. The distributed searcher complains with an instantiation exception when deserializing Query. A quick code inspection shows that Query doesn't have any parameterless constructor.",
        "Issue Links": []
    },
    "NUTCH-218": {
        "Key": "NUTCH-218",
        "Summary": "need DOAP file for Nutch",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Doug Cutting",
        "Created": "01/Mar/06 02:11",
        "Updated": "13/Mar/06 20:26",
        "Resolved": "13/Mar/06 20:26",
        "Description": "Can someone please draft a DOAP file for Nutch, so that we're listed at http://projects.apache.org/?\nA DOAP generator is at:\nhttp://projects.apache.org/create.html\nPlease attach it to this bug report.  Thanks.",
        "Issue Links": []
    },
    "NUTCH-219": {
        "Key": "NUTCH-219",
        "Summary": "file.content.limit & ftp.content.limit should be changed to -1 to be consistent with http",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Richard Braman",
        "Created": "03/Mar/06 10:21",
        "Updated": "03/Mar/06 16:21",
        "Resolved": "03/Mar/06 16:21",
        "Description": "file and ftp are 0 for no trunccation, but http need be -1\nThis is easily missed when configuting even by expereienced users.\nHere is the help I got in nutch-user from Jermoe, who is a developer.\n>Edit your nutch-site.xml (or nutch-default.xml) and change the\nhttp.content.limit (set it to 0 if you don't want no content truncation at >all).\n>J\u00e9r\u00f4me",
        "Issue Links": []
    },
    "NUTCH-220": {
        "Key": "NUTCH-220",
        "Summary": "PDF Box can't parse document: java.lang.NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Richard Braman",
        "Created": "03/Mar/06 10:25",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Mar/08 16:23",
        "Description": "This error was fixed in the ltest build of PDFBOx, which should be tested with nutch.\n>> 060228 160354 fetch okay, but can't parse\n>> http://www.mstc.state.ms.us/info/stats/transfer/tran0704.pdf, reason:\n>> failed(2,0): Can't be handled as pdf document. \n>> java.lang.NullPointerException\nYes, the NPE should be fixed.\n Ben\nRichard Braman wrote:\n> Hi Bn,\n>\n> We actually got to the bottom of all of them except for 1... The \n> content truncatetion was due to an inconsistancy bug in nutch config .\n> The no permission to extract text is actually true, the author, the NC\n> Department of revenue put this restriction on all of their files (I have\n> asked them to remove it as it hampers public accessability).  The Null\n> pointer exception is the only one to deal with that may be due to the\n> parsing bug .  Is this one that you are referring to?\n>\n> ----Original Message----\n> From: Ben Litchfield ben@csh.rit.edu\n> Sent: Thursday, March 02, 2006 4:07 PM\n> To: Richard Braman\n> Cc: nutch-dev@lucene.apache.org; nutch-user@lucene.apache.org;\n> pdfbox-user@lists.sourceforge.net\n> Subject: Re: [PDFBox-user] PDF Parse Error\n>\n>\n>\n> I believe these errors are due to a parsing bug in PDFBox that has \n> been fixed since the 0.7.2 release.  Please give the nightly \n> build(should be a drop in replacement) a try from \n> http://www.pdfbox.org/dist and let me know if you are still having \n> issues.\n>\n> Ben",
        "Issue Links": []
    },
    "NUTCH-221": {
        "Key": "NUTCH-221",
        "Summary": "prepare nutch for upcoming lucene 2.0",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "04/Mar/06 03:30",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "05/Mar/06 19:57",
        "Description": "Remove all deprecated uses of lucene as they will vanish in 2.0",
        "Issue Links": []
    },
    "NUTCH-222": {
        "Key": "NUTCH-222",
        "Summary": "Exception in thread \"main\" java.lang.NoClassDefFoundError: invertlink",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Richard Braman",
        "Created": "05/Mar/06 01:04",
        "Updated": "05/Mar/06 01:16",
        "Resolved": "05/Mar/06 01:16",
        "Description": "When trying to invertlinks before indexing, following the tutorial, I get the following error.\nrich@machinename /cygdrive/t/nutch-0.7.1\n$ bin/nutch invertlink taxcrawl/db/ -dir taxcrawl/segments/*\nrun java in C:\\Program Files\\Java\\jdk1.5.0_04\nException in thread \"main\" java.lang.NoClassDefFoundError: invertlink",
        "Issue Links": []
    },
    "NUTCH-223": {
        "Key": "NUTCH-223",
        "Summary": "Crawl.java uses Integer.MAX_VALUE for -topN where Generator.java uses Long.MAX_VALUE for -topN",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Jeff Ritchie",
        "Created": "05/Mar/06 09:47",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Mar/08 16:43",
        "Description": "Here is a patch for Crawl.java so that it the default value for -topN is the same as Generator.java\n\n\n\n\n\nCrawl.java  Fri Mar  3 23:54:28 2006\n\n\nfix/Crawl.java      Sat Mar  4 19:38:27 2006\n***************\n\n\n59,65 ****\n      File dir = new File(\"crawl-\" + getDate());\n      int threads = job.getInt(\"fetcher.threads.fetch\", 10);\n      int depth = 5;\n!     int topN = Integer.MAX_VALUE;\n\n\n\n\n\n      for (int i = 0; i < args.length; i++) {\n        if (\"-dir\".equals(args[i])) {\n\u2014 59,65 ----\n      File dir = new File(\"crawl-\" + getDate());\n      int threads = job.getInt(\"fetcher.threads.fetch\", 10);\n      int depth = 5;\n!     int topN = Long.MAX_VALUE;\n      for (int i = 0; i < args.length; i++) {\n        if (\"-dir\".equals(args[i])) {",
        "Issue Links": []
    },
    "NUTCH-224": {
        "Key": "NUTCH-224",
        "Summary": "Nutch doesn't handle Korean text at all",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Kuro Kurosaka",
        "Created": "07/Mar/06 06:56",
        "Updated": "01/Apr/11 14:58",
        "Resolved": "01/Apr/11 14:58",
        "Description": "I was browing NutchAnalysis.jj and found that\nHungul Syllables (U+AC00 ... U+D7AF; U+xxxx means\na Unicode character of the hex value xxxx) are not\npart of LETTER or CJK class.  This seems to me that\nNutch cannot handle Korean documents at all.\nI posted the above message at nutch-user ML and Cheolgoo Kang [appler@gmail.com]\nreplied as:\n------------------------------------------------------------------------------------\nThere was similar issue with Lucene's StandardTokenizer.jj.\nhttp://issues.apache.org/jira/browse/LUCENE-444\nand\nhttp://issues.apache.org/jira/browse/LUCENE-461\nI'm have almost no experience with Nutch, but you can handle it like\nthose issues above.\n------------------------------------------------------------------------------------\nBoth fixes should probably be ported back to NuatchAnalysis.jj.",
        "Issue Links": []
    },
    "NUTCH-225": {
        "Key": "NUTCH-225",
        "Summary": "Changed the links to the tutorial to point to the wiki",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jake Vanderdray",
        "Created": "08/Mar/06 05:32",
        "Updated": "10/Mar/06 04:33",
        "Resolved": "10/Mar/06 04:33",
        "Description": "This is a patch to repoint tutorial links on the nutch site to the wiki.\nIndex: site.xml\n===================================================================\n\u2014 site.xml    (revision 384005)\n+++ site.xml    (working copy)\n@@ -26,7 +26,7 @@\n   <docs label=\"Documentation\">    \n     <faq         label=\"FAQ\"              href=\"ext:faq\" />    \n     <wiki        label=\"Wiki\"             href=\"ext:wiki\" />    \n\n<tutorial    label=\"Tutorial\"         href=\"tutorial.html\" />\n+    <tutorial    label=\"Tutorial\"         href=\"ext:tutorial\" />\n     <webmasters  label=\"Robot     \"       href=\"bot.html\" />\n     <i18n        label=\"i18n\"             href=\"i18n.html\" />\n     <apidocs     label=\"API Docs\"         href=\"apidocs/index.html\" />\n@@ -48,6 +48,7 @@\n     <wiki      href=\"http://wiki.apache.org/nutch/\" />\n     <faq       href=\"http://wiki.apache.org/nutch/FAQ\" /> \n     <store     href=\"http://www.cafepress.com/nutch/\" />\n+    <tutorial  href=\"http://wiki.apache.org/nutch/NutchTutorial\" />\n   </external-refs>\n\n </site>\nIndex: i18n.xml\n===================================================================\n\u2014 i18n.xml    (revision 384005)\n+++ i18n.xml    (working copy)\n@@ -188,7 +188,7 @@\n href=\"http://jakarta.apache.org/tomcat/\">Tomcat</a> installed.</p>\n <p>An index is also required.  You can collect your own by working\n-through the <a href=\"http://lucene.apache.org/nutch/tutorial.html\">tutorial</a>.\n+through the <a href=\"http://wiki.apache.org/nutch/NutchTutorial\">tutorial</a>.\n Once you have an index, follow the steps outlined at the end of the\n tutorial for searching.</p>",
        "Issue Links": []
    },
    "NUTCH-226": {
        "Key": "NUTCH-226",
        "Summary": "CrawlDb Filter tool",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "09/Mar/06 03:09",
        "Updated": "22/Jan/08 14:13",
        "Resolved": "22/Jan/08 14:13",
        "Description": "A tool to filter a existing crawlDb",
        "Issue Links": []
    },
    "NUTCH-227": {
        "Key": "NUTCH-227",
        "Summary": "Basic Query Filter no more uses Configuration",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Jerome Charron",
        "Created": "09/Mar/06 23:55",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "10/Mar/06 00:55",
        "Description": "Since NUTCH-169, the BasicIndexingFilter has no way to retrieve its configuration parameters (query.url.boost, query.anchor.boost, query.title.boost, query.host.boost, query.phrase.boost) : The setConf(Configuration) method is never called by the QueryFilters class.\nMore generaly, we should provide a way for QueryFilter to be Configurable. Two solutions:\n1. The QueryFilters checks that a QueryFilter implements Configurable and then call the setConf() method.\n2. QueryFilter extends Configurable => all QueryFilter must implement Configurable.\nMy preference goes to 1, and if there is no objection, I will commit a patch in the next few days.",
        "Issue Links": []
    },
    "NUTCH-228": {
        "Key": "NUTCH-228",
        "Summary": "Clustering plugin descriptor broken (fix included)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dawid Weiss",
        "Created": "12/Mar/06 16:46",
        "Updated": "12/Mar/06 17:44",
        "Resolved": "12/Mar/06 17:44",
        "Description": "The plugin descriptor for clustering-carrot2 is currently broken (points to a missing JAR). I'm adding a patch fixing this to this issue in a minute.",
        "Issue Links": []
    },
    "NUTCH-229": {
        "Key": "NUTCH-229",
        "Summary": "improved handling of plugin folder configuration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "13/Mar/06 08:08",
        "Updated": "13/Mar/06 20:15",
        "Resolved": "13/Mar/06 20:15",
        "Description": "Currently nutch only supports absoluth path or realative path that are part of the classpath. \nThere are cases where it would be useful to be able using relative paaths that  are not in the classpath for example have a centralized plugin repository on a shared hdd in cluster or running nutch inside a ide etc.",
        "Issue Links": []
    },
    "NUTCH-230": {
        "Key": "NUTCH-230",
        "Summary": "OPIC score for outlinks should be based on # of valid links, not total # of links.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kenneth William Krugler",
        "Created": "14/Mar/06 13:02",
        "Updated": "03/Apr/06 20:23",
        "Resolved": "03/Apr/06 20:23",
        "Description": "In ParseOutputFormat.java, the write() method currently divides the page score by the # of outlinks:\n          score /= links.length;\nIt then loops over the links, and any that pass the normalize/filter gauntlet get added to the crawl output.\nBut this means that any filtered links result in some amount of the page's OPIC score being \"lost\".\nFor Nutch 0.7, I built a list of valid (post-filter) links, and then used that to determine the per-link OPIC score, after which I iterated over the list, adding entries to the crawl output.",
        "Issue Links": []
    },
    "NUTCH-231": {
        "Key": "NUTCH-231",
        "Summary": "Invalid CSS entries",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "AJ Banck",
        "Created": "16/Mar/06 02:34",
        "Updated": "28/Mar/06 00:56",
        "Resolved": "28/Mar/06 00:56",
        "Description": "/lucene/nutch/trunk/src/web/include/style.html has invalid entries CSS entries.\nonMouseOver is not a CSS attribute. The height attributes should have a 'px' postfix.",
        "Issue Links": []
    },
    "NUTCH-232": {
        "Key": "NUTCH-232",
        "Summary": "Search.jsp has multiple search forms creating invalid html / incorrect focus function",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "AJ Banck",
        "Created": "16/Mar/06 02:39",
        "Updated": "28/Mar/06 21:01",
        "Resolved": "28/Mar/06 21:01",
        "Description": "Search.jsp can output has multiple forms with the name 'search'. \nThis causes the function queryfocus to throw errors as there is not a unique element named 'search'.\nAlso, the about.html page is generated with the queryfocus function causing errors as the about page doesn't have a search entry",
        "Issue Links": []
    },
    "NUTCH-233": {
        "Key": "NUTCH-233",
        "Summary": "wrong regular expression hang reduce process for ever",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "16/Mar/06 11:09",
        "Updated": "10/Mar/07 02:41",
        "Resolved": "09/Mar/07 22:42",
        "Description": "Looks like that the expression \".(/.+?)/.?\\1/.*?\\1/\" in regex-urlfilter.txt wasn't compatible with java.util.regex that is actually used in the regex url filter. \nMay be it was missed to change it when the regular expression packages was changed.\nThe problem was that until reducing a fetch map output the reducer hangs forever since the outputformat was applying the urlfilter a url that causes the hang.\n060315 230823 task_r_3n4zga     at java.lang.Character.codePointAt(Character.java:2335)\n060315 230823 task_r_3n4zga     at java.util.regex.Pattern$Dot.match(Pattern.java:4092)\n060315 230823 task_r_3n4zga     at java.util.regex.Pattern$Curly.match1(Pattern.java:\nI changed the regular expression to \".*(/[^/])/[^/]\\1/[^/]+\\1/\" and now the fetch job works. (thanks to Grant and Chris B. helping to find the new regex)\nHowever may people can review it and can suggest improvements, since the old regex would match :\n\"abcd/foo/bar/foo/bar/foo/\" and so will the new one match it also. But the old regex would also match :\n\"abcd/foo/bar/xyz/foo/bar/foo/\" which the new regex will not match.",
        "Issue Links": []
    },
    "NUTCH-234": {
        "Key": "NUTCH-234",
        "Summary": "Clustering extension code cleanups and a real JUnit test case for the current implementation.",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dawid Weiss",
        "Created": "17/Mar/06 22:31",
        "Updated": "22/Mar/06 00:43",
        "Resolved": "22/Mar/06 00:43",
        "Description": "I've cleaned up the code a bit and added a real test case for the clustering extension. This is in preparation for upgrading to the most recent Carrot2 codebase and I didn't want to mix these two patches together. I'd appreciate if somebody could review this patch so that I can integrate the newest C2 code this weekend. Thanks.",
        "Issue Links": []
    },
    "NUTCH-235": {
        "Key": "NUTCH-235",
        "Summary": "Duplicate Inlink values",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "19/Mar/06 03:19",
        "Updated": "21/Mar/06 07:22",
        "Resolved": "21/Mar/06 07:22",
        "Description": "Reading the code for LinkDb.reduce():  if we have page duplicates in input segments, or if we have two copies of the same input segment, we will create the same Inlink values (satisfying Inlink.equals()) multiple times. Since Inlinks is a facade for List, and not a Set, we will get duplicate Inlink-s in Inlinks (if you know what I mean   .\nThe problem is easy to test: create a new linkdb based on 2 identical segments. This problem also makes it more difficult to properly implement LinkDB updating mechanism (i.e. incremental invertlinks).\nI propose to change Inlinks to use a Set semantics, either explicitly by using a HashSet or implicitly by checking if a value to be added already exists. If there are no objections I'll commit this change shortly.",
        "Issue Links": []
    },
    "NUTCH-236": {
        "Key": "NUTCH-236",
        "Summary": "PdfParser and RSSParser Log4j appender redirection",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Jason Calabrese",
        "Created": "19/Mar/06 10:44",
        "Updated": "13/Jun/06 17:08",
        "Resolved": "13/Jun/06 17:06",
        "Description": "I just found a bug in the way the log messages from Hadoop LogFormatter are \nadded as a new appender to the Log4j rootLogger in the PdfParser and RSSParser.\nSince a new Log4j appender is created and added to the root logger each time \nthese classes are loaded log messages start getting repeated.\nI'm using Nutch/Hadoop inside an other application so other may not be seeing \nthis problem.\nI think the simple fix is as easy as setting a name for the new appender \nbefore adding it and then at the begining of the constructor checking to see \nif it's already been added.\nAlso as the comment says in both the PdfParser and RSSParser this code should \nbe moved to a common place.\nI'd be happy to make these changes and submit a patch, but I wanted to know it \nthe change would be welcome first.  Also does anyone know a good place for \nthe new util method?  Maybe a new static method on LogFormatter, but then the \nlog4j jar would need to be added to the to the common lib and the classpath.\nIt would also be good to create a property in nutch-site.xml that could disable this logging appender redirection.\nLike I said above I'd be more than happy to do this work, I'll just need some guidance to follow the project's conventions.",
        "Issue Links": [
            "/jira/browse/NUTCH-303"
        ]
    },
    "NUTCH-237": {
        "Key": "NUTCH-237",
        "Summary": "Carrot2 clustering plugin upgrade.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dawid Weiss",
        "Created": "23/Mar/06 19:38",
        "Updated": "22/Aug/07 14:27",
        "Resolved": "22/Aug/07 14:27",
        "Description": "This is an upgrade of the clustering plugin to the newest release (1.0.2).",
        "Issue Links": []
    },
    "NUTCH-238": {
        "Key": "NUTCH-238",
        "Summary": "NDFSck - fsck utility for NDFS (pre-Hadoop)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "24/Mar/06 02:47",
        "Updated": "08/Jun/11 21:32",
        "Resolved": "03/Apr/06 20:22",
        "Description": "This is a utility to check health status of NDFS. NOTE: this is compatible ONLY with pre-Hadoop Nutch versions! (Another version has been submitted for Hadoop volumes).",
        "Issue Links": []
    },
    "NUTCH-239": {
        "Key": "NUTCH-239",
        "Summary": "I changed httpclient to use javax.net.ssl instead of com.sun.net.ssl",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.2",
        "Fix Version/s": "0.7.2",
        "Component/s": "fetcher",
        "Assignee": "Piotr Kosiorowski",
        "Reporter": "Jake Vanderdray",
        "Created": "25/Mar/06 05:50",
        "Updated": "25/Mar/06 19:38",
        "Resolved": "25/Mar/06 19:38",
        "Description": "I made the following changes in order to get the dependency on com.sun.ssl out of the 0.7 branch.  The same changes have already been applied to the 0.8 branch (Revision 379215) thanks to ab.  There is still a dependency on using the Sun JRE.  In order to get it to work with the IBM JRE I had to change SunX509 to IbmX509, but I didn't include that change in this patch.  \nThanks,\nJake.\nIndex: DummySSLProtocolSocketFactory.java\n===================================================================\n\u2014 DummySSLProtocolSocketFactory.java  (revision 388638)\n+++ DummySSLProtocolSocketFactory.java  (working copy)\n@@ -22,8 +22,8 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n-import com.sun.net.ssl.SSLContext;\n-import com.sun.net.ssl.TrustManager;\n+import javax.net.ssl.SSLContext;\n+import javax.net.ssl.TrustManager;\n public class DummySSLProtocolSocketFactory implements ProtocolSocketFactory {\nIndex: DummyX509TrustManager.java\n===================================================================\n\u2014 DummyX509TrustManager.java  (revision 388638)\n+++ DummyX509TrustManager.java  (working copy)\n@@ -10,9 +10,9 @@\n import java.security.cert.CertificateException;\n import java.security.cert.X509Certificate;\n-import com.sun.net.ssl.TrustManagerFactory;\n-import com.sun.net.ssl.TrustManager;\n-import com.sun.net.ssl.X509TrustManager;\n+import javax.net.ssl.TrustManagerFactory;\n+import javax.net.ssl.TrustManager;\n+import javax.net.ssl.X509TrustManager;\n import org.apache.commons.logging.Log; \n import org.apache.commons.logging.LogFactory;\n@@ -57,4 +57,12 @@\n     public X509Certificate[] getAcceptedIssuers() \n{\n         return this.standardTrustManager.getAcceptedIssuers();\n     }\n+   \n+    public void checkClientTrusted(X509Certificate[] arg0, String arg1) throws CertificateException \n{\n+       // do nothing\n+    }\n+\n+    public void checkServerTrusted(X509Certificate[] arg0, String arg1) throws CertificateException {+       // do nothing+    }\n }",
        "Issue Links": []
    },
    "NUTCH-240": {
        "Key": "NUTCH-240",
        "Summary": "Scoring API: extension point, scoring filters and an OPIC plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "29/Mar/06 05:20",
        "Updated": "13/May/06 07:56",
        "Resolved": "13/May/06 07:56",
        "Description": "This patch refactors all places where Nutch manipulates page scores, into a plugin-based API. Using this API it's possible to implement different scoring algorithms. It is also much easier to understand how scoring works.\nMultiple scoring plugins can be run in sequence, in a manner similar to URLFilters.\nIncluded is also an OPICScoringFilter plugin, which contains the current implementation of the scoring algorithm. Together with the scoring API it provides a fully backward-compatible scoring.",
        "Issue Links": []
    },
    "NUTCH-241": {
        "Key": "HADOOP-114",
        "Summary": "Non-informative error message",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.2.0",
        "Fix Version/s": "0.2.0",
        "Component/s": "None",
        "Assignee": "Doug Cutting",
        "Reporter": "Rod Taylor",
        "Created": "31/Mar/06 00:52",
        "Updated": "08/Jul/09 16:51",
        "Resolved": "19/Apr/06 02:50",
        "Description": "060330 105006 mapred.child.heap.size is deprecated. Use mapred.child.heap.size instead. Meantime, interpolated child.heap.size into child.java.opt: -Xmx200m\nThe instructions inform you to use the deprecated option.",
        "Issue Links": []
    },
    "NUTCH-242": {
        "Key": "NUTCH-242",
        "Summary": "Add optional -urlFiltering to updatedb",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "31/Mar/06 01:19",
        "Updated": "30/Aug/06 22:14",
        "Resolved": "30/Aug/06 22:14",
        "Description": "Allow filtering the URLs completely out of the database during an updatedb run. This allows the regex-urlfilter.xml rules to be changed and the non-matching entries to be expunged from the database.\nMerging the functionality with updatedb was done for efficiency reasons since this eats up CPU time only where a separate job would use IO and CPU time.",
        "Issue Links": []
    },
    "NUTCH-243": {
        "Key": "NUTCH-243",
        "Summary": "Some meta-refresh urls get ignored due to matching regular expression",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dennis Kubes",
        "Created": "05/Apr/06 05:35",
        "Updated": "17/Mar/08 16:49",
        "Resolved": "17/Mar/08 16:49",
        "Description": "On fetching of pages with meta-refresh tags the url is taken at face value without any filtering.  Some urls, such as those used by struts return with a jsessionid or with query strings.  Examples are:\nhttp://www.somesite.com;jsessionid=3123123412ADBE3344...\nhttp://www.somesite.com?querystring=value\nThe RegexURLFilter will match these urls according to the following regex inside of the regex-urlfilter.txt file:\n-[?*!@=]\nShould these urls be cleaned up to allow processing and not match the previous URL filter or should they be ignored as they currently are?",
        "Issue Links": [
            "/jira/browse/NUTCH-255"
        ]
    },
    "NUTCH-244": {
        "Key": "NUTCH-244",
        "Summary": "Inconsistent handling of property values boundaries / unable to set db.max.outlinks.per.page to infinite",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "AJ Banck",
        "Created": "05/Apr/06 16:08",
        "Updated": "07/Apr/06 01:05",
        "Resolved": "07/Apr/06 01:05",
        "Description": "Some properties like file.content.limit support using negative numbers (-1) to 'disable' a limitation.\nOther properties do not support this. \nI tried disabling the limit set by db.max.outlinks.per.page, but this isn't possible.",
        "Issue Links": []
    },
    "NUTCH-245": {
        "Key": "NUTCH-245",
        "Summary": "DTD for plugin.xml configuration files",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.7.2,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher,                                            indexer,                                            web gui",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "08/Apr/06 04:13",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "15/Apr/06 06:58",
        "Description": "Currently, the plugin.xml file does not have a DTD or XML Schema associated with it, and most people just go look at an existing plugin's plugin.xml file to determine what are the allowable elements, etc. There should be an explicit plugin DTD file that describes the plugin.xml file. I'll look at the code and attach a plugin.dtd file for the Nutch conf directory later today. This way, people can use the DTD file to automatically (using tools such as XMLSpy) generate plugin.xml files that can then be validated.",
        "Issue Links": []
    },
    "NUTCH-246": {
        "Key": "NUTCH-246",
        "Summary": "segment size is never as big as topN or crawlDB size in a distributed deployement",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "11/Apr/06 21:51",
        "Updated": "22/Mar/07 10:08",
        "Resolved": "22/Mar/07 10:08",
        "Description": "I didn't reopen NUTCH-136 since it is may related to the hadoop split.\nI tested this on two different deployement (with 10 ttrackers + 1 jobtracker and 9 ttracks and 1 jobtracker).\nDefining map and reduce task number in a mapred-default.xml does not solve the problem. (is in nutch/conf on all boxes)\nWe verified that it is not  a problem of maximum urls per hosts and also not a problem of the url filter.\nLooks like the first job of the Generator (Selector) already got to less entries to process. \nMay be this is somehow releasted to split generation or configuration inside the distributed jobtracker since it runs in a different jvm as the jobclient.\nHowever we was not able to find the source for this problem.\nI think that should be fixed before  publishing a nutch 0.8.",
        "Issue Links": []
    },
    "NUTCH-247": {
        "Key": "NUTCH-247",
        "Summary": "robot parser to restrict.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Sami Siren",
        "Reporter": "Stefan Groschupf",
        "Created": "12/Apr/06 09:46",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Feb/09 09:55",
        "Description": "If the agent name and the robots agents are not proper configure the Robot rule parser uses LOG.severe to log the problem but solve it also. \nLater on the fetcher thread checks for severe errors and stop if there is one.\nRobotRulesParser:\nif (agents.size() == 0) \n{\n      agents.add(agentName);\n      LOG.severe(\"No agents listed in 'http.robots.agents' property!\");\n    }\n else if (!((String)agents.get(0)).equalsIgnoreCase(agentName)) \n{\n      agents.add(0, agentName);\n      LOG.severe(\"Agent we advertise (\" + agentName\n                 + \") not listed first in 'http.robots.agents' property!\");\n    }\n\nFetcher.FetcherThread:\n if (LogFormatter.hasLoggedSevere())     // something bad happened\n            break;  \nI suggest to use warn or something similar instead of severe to log this problem.",
        "Issue Links": [
            "/jira/browse/NUTCH-258"
        ]
    },
    "NUTCH-248": {
        "Key": "NUTCH-248",
        "Summary": "add support for internationalized domain names",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "16/Apr/06 00:32",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 20:22",
        "Description": "Internationalized domain names are gaining ground and so nutch should give a little bit more support to this feature, atleast we need punycode encoding/decoding functionality so we can display/enter internationalized domain names in ui.",
        "Issue Links": []
    },
    "NUTCH-249": {
        "Key": "NUTCH-249",
        "Summary": "black- white list url filtering",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Dennis Kubes",
        "Reporter": "Stefan Groschupf",
        "Created": "17/Apr/06 20:59",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "Existing url filter mechanisms need to process each url against each filter pattern. For very large filter sets this may be does not scale very well.",
        "Issue Links": []
    },
    "NUTCH-250": {
        "Key": "NUTCH-250",
        "Summary": "Generate to log truncation caused by generate.max.per.host",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Doug Cutting",
        "Reporter": "Rod Taylor",
        "Created": "20/Apr/06 07:57",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "21/Apr/06 02:18",
        "Description": "LOG.info() hosts which have had their generate lists truncated.\nThis can inform admins about potential abusers or excessively large sites that they may wish to block with rules.",
        "Issue Links": []
    },
    "NUTCH-251": {
        "Key": "NUTCH-251",
        "Summary": "Administration GUI",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "22/Apr/06 04:43",
        "Updated": "16/Sep/11 09:52",
        "Resolved": "01/Apr/11 14:56",
        "Description": "Having a web based administration interface would help to make nutch administration and management much more user friendly.",
        "Issue Links": []
    },
    "NUTCH-252": {
        "Key": "NUTCH-252",
        "Summary": "Launching a segread/readdb command kills any running nutch commands",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chris Schneider",
        "Created": "22/Apr/06 06:29",
        "Updated": "22/Jan/08 14:29",
        "Resolved": "22/Jan/08 14:29",
        "Description": "I use a simple script to conduct a whole-web crawl (generate, fetch, updatedb, and repeat until target depth reached). While this is running, I monitor the progress via the jobtracker's browser-based UI. Sometimes there's a fairly long pause after one mapreduce job completes and the next one gets launched, so I mistakenly assume that depth has been reached. I then launch a segread -list or readdb -stats command to summarize the results. Doing so apparently kills any active jobs with absolutely no warning in any of the logs, the console output, or the jobtracker's UI. The jobs just stop writing to the logs and any child processes disappear. Usually, the jobtracker and tasktrackers remain up and respond to subsequent commands.",
        "Issue Links": []
    },
    "NUTCH-253": {
        "Key": "NUTCH-253",
        "Summary": "Normalize Host during Generate",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "24/Apr/06 10:36",
        "Updated": "23/Sep/06 19:00",
        "Resolved": "23/Sep/06 19:00",
        "Description": "Extend URL Normalizer to allow for normalizion of the Hostname during Generate. By default no rules are applied.\nIn short, this allows foo.bar.com, bif.baz.bar.com and bar.com to be counted as being the same for generate.max.per.host if an appropriate regex is used.\nAdd \"urlnormalizer-regex\" to plugin.includes in nutch-site.xml in order to enable it.\nSince several modules now extend the urlnormalizer base we use a \"scope\" parameter within plugin.xml to allow differentiation between the various urlnormalizer modules to select the right module for Generate.",
        "Issue Links": []
    },
    "NUTCH-254": {
        "Key": "NUTCH-254",
        "Summary": "Fetcher throws NullPointer if redirect URL is filtered",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dennis Kubes",
        "Created": "25/Apr/06 03:59",
        "Updated": "22/May/06 02:48",
        "Resolved": "25/Apr/06 05:56",
        "Description": "Inside the Fetcher class if a redirect URL is filtered, for example jessionid pages are filtered with the default URL filter, then a NullPointerException is thrown when Fetcher trys to print out that the url was skipped for being an identical url.  It is not an identical URL but a filtered url.  So what we really need is two different checks.  One for null url and one for identical url.  I have included a patch that handles this.",
        "Issue Links": []
    },
    "NUTCH-255": {
        "Key": "NUTCH-255",
        "Summary": "Regular Expression for RegexUrlNormalizer to remove jsessionid",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dennis Kubes",
        "Created": "26/Apr/06 00:42",
        "Updated": "22/Sep/08 15:12",
        "Resolved": "22/Sep/08 15:12",
        "Description": "Some URLs are filtered out by the crawl url filter for special characters (by default).  One of these is the jsessionid urls such as:\nhttp://www.somesite.com;jsessionid=A8D7D812B5EFD3099F099A760F779E3B?query=string\nWe want to get rid of the jessionid and keep everything else so that it looks like this:\nhttp://www.somesite.com?query=string\nBelow is a regular expression for the regex-normalize.xml file used by the RegexUrlNormalizer that sucessfully removes jsessionid strings while leaving the hostname and querystring.  I have also attached a patch for the regex-normalize.xml.template file that adds the following expression.\n<regex>\n  <pattern>(.*)(;jsessionid=[a-zA-Z0-9]\n{32}\n)(.*)</pattern>\n  <substitution>$1$3</substitution>\n</regex>",
        "Issue Links": [
            "/jira/browse/NUTCH-243",
            "/jira/browse/NUTCH-279"
        ]
    },
    "NUTCH-256": {
        "Key": "NUTCH-256",
        "Summary": "Cannot open filename ....index.done.crc",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Doug Cutting",
        "Reporter": "Michael Stack",
        "Created": "28/Apr/06 06:04",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "29/Apr/06 04:41",
        "Description": "Trying to copy indices out of DFS I always get:\n[bregeon] workspace > ./hadoop/bin/hadoop dfs -get outputs .\n060427 160317 parsing file:/home/stack/workspace/hadoop-local-conf/hadoop-default.xml\n060427 160317 parsing file:/home/stack/workspace/hadoop-local-conf/hadoop-site.xml\n060427 160318 No FS indicated, using default:localhost:9001\n060427 160318 Client connection to 127.0.0.1:9001: starting\n060427 160318 Problem opening checksum file: /user/stack/outputs/indexes/part-00000/index.done.  Ignoring with exception org.apache.hadoop.ipc.RemoteException: java.io.IOException: Cannot open filename /user/stack/outputs/indexes/part-00000/.index.done.crc\n        at org.apache.hadoop.dfs.NameNode.open(NameNode.java:130)\n        at sun.reflect.GeneratedMethodAccessor2.invoke(Unknown Source)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:589)\n        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:240)\n        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:218)",
        "Issue Links": []
    },
    "NUTCH-257": {
        "Key": "NUTCH-257",
        "Summary": "Summary#toString always Entity encodes -- problem for OpenSearchServlet#description field",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Michael Stack",
        "Created": "29/Apr/06 03:37",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "10/May/06 19:49",
        "Description": "All search result data we display in search results has to be explicitly Entity.encoded outputing in search.jsp ( title, url, etc.) except Summaries.  Its already Entity.encoded.  This is fine when outputing HTML but it gets in the way when outputing otherwise \u2013 as xml for example.  I'd suggest we not make any presumption about how search results are used.\nThe problem becomes especially acute when the text language is other than english.\nHere is an example of what a Czech description field in an OpenSearchServlet hit record looks like:\n<description><span class=\"ellipsis\"> ... </span>V&#283;deck&aacute; knihovna v Olomouci Bezru&#269;ova 2, Olomouc 9, 779 11, &#268;esk&aacute; republika &nbsp; tel. +420-585223441 &nbsp; fax +420-585225774 http://www.&lt;span class=\"highlight\">vkol</span>.cz/ &nbsp;&nbsp; info@<span class=\"highlight\">vkol</span>.cz Otev&#345;eno : &nbsp; po-p&aacute; &nbsp; 8 30 -19 00 &nbsp;&nbsp;&nbsp; so &nbsp; 9 00 -13 00 &nbsp;&nbsp;&nbsp; ne &nbsp; zav&#345;eno V katalogu s &uacute;pln&yacute;m &#269;asov&yacute;m<span class=\"ellipsis\"> ... </span>03 Organizace 20/12 Odkazy 19/04 Hledej 23/03 &nbsp; 23/03 &nbsp; Po&#269;et p&#345;&iacute;stup&#367; od 1.9.1998. Statistiky . [ ] &nbsp; [ Nahoru ] <span class=\"highlight\">VKOL</span></description>\nHere is same description field with Entity.encoding disabled:\n<description><span class=\"ellipsis\"> ... </span>tisky statistiky knihovny WWW serveru st?edov?k\u00e9 rukopisy studovny CD-ROM historick\u00fdch fond? hlavn\u00ed Internet N?meck\u00e9 knihovny v\u00e1zan\u00fdch novin SVKOL viz <span class=\"highlight\">VKOL</span> ?atna T telefonn\u00ed ?\u00edsla knihovny zam?stnanc? U V vazba v?cn\u00fd popis veden\u00ed knihovny vedouc\u00ed odd?len\u00ed video <span class=\"highlight\">VKOL</span> voln\u00fd v\u00fdb?r v\u00fdp?j?ka v\u00fdro?n\u00ed zpr\u00e1va v\u00fdstavy W webmaster WWW odkazy X Y Z - ? zamluven\u00ed knihy zahrani?n\u00ed periodika zpracov\u00e1n\u00ed fondu<span class=\"highlight\">VKOL</span> - hledej Hledej [ <span class=\"highlight\">VKOL</span> ] [ Novinky ] [ Katalog ] [ Slu?by ] [ Aktivity ] [ Pr?vodce ] [ Dokumenty ] [ Region\u00e1ln\u00ed fce ] [ Organizace ] [ Odkazy ] [ Hledej ] [     ] [     ] Obsah full-textov\u00e9 vyhled\u00e1v\u00e1n\u00ed, 19/04/2003 rejst?\u00edk vybran\u00fdch<span class=\"ellipsis\"> ... </span></description>\nNotice how the Czech characters in the first are all numerically encoded: i.e. #NNN;.\nI'd suggest that Summary#toString() become Summary#toEntityEncodedString() and that toString return raw aggregation of Fragments.  Would likely require adding methods to the HitSummarizer interface so can ask for either raw text or entity encoded with addition to NutchBean so can ask for either.  Or, better I'd suggest is that Summarizer never return Entity.encoded text.  Let that happen in search.jsp (I can make patch to do the latter if its amenable).",
        "Issue Links": [
            "/jira/browse/NUTCH-134"
        ]
    },
    "NUTCH-258": {
        "Key": "NUTCH-258",
        "Summary": "Once Nutch logs a SEVERE log item, Nutch fails forevermore",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Scott Ganyo",
        "Created": "29/Apr/06 03:39",
        "Updated": "10/Aug/11 12:04",
        "Resolved": "13/Feb/07 15:01",
        "Description": "Once a SEVERE log item is written, Nutch shuts down any fetching forevermore.  This is from the run() method in Fetcher.java:\n    public void run() {\n      synchronized (Fetcher.this) \n{activeThreads++;}\n // count threads\n      try {\n        UTF8 key = new UTF8();\n        CrawlDatum datum = new CrawlDatum();\n        while (true) {\n          if (LogFormatter.hasLoggedSevere())     // something bad happened\n            break;                                // exit\nNotice the last 2 lines.  This will prevent Nutch from ever Fetching again once this is hit as LogFormatter is storing this data as a static.\n(Also note that \"LogFormatter.hasLoggedSevere()\" is also checked in org.apache.nutch.net.URLFilterChecker and will disable this class as well.)\nThis must be fixed or Nutch cannot be run as any kind of long-running service.  Furthermore, I believe it is a poor decision to rely on a logging event to determine the state of the application - this could have any number of side-effects that would be extremely difficult to track down.  (As it has already for me.)",
        "Issue Links": [
            "/jira/browse/NUTCH-277",
            "/jira/browse/NUTCH-303",
            "/jira/browse/NUTCH-247",
            "/jira/browse/NUTCH-163"
        ]
    },
    "NUTCH-259": {
        "Key": "NUTCH-259",
        "Summary": "Problem in IndexSorter after dedup",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Michael",
        "Created": "29/Apr/06 05:17",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "When trying to run IndexSorter i'm getting an error:\nException in thread \"main\" java.lang.IllegalArgumentException: attempt to access a deleted document\n        at org.apache.lucene.index.SegmentReader.document(SegmentReader.java:282)\n        at org.apache.lucene.index.FilterIndexReader.document(FilterIndexReader.java:104)\n        at org.apache.nutch.indexer.IndexSorter$SortingReader.document(IndexSorter.java:170)\n        at org.apache.lucene.index.SegmentMerger.mergeFields(SegmentMerger.java:186)\n        at org.apache.lucene.index.SegmentMerger.merge(SegmentMerger.java:88)\n        at org.apache.lucene.index.IndexWriter.addIndexes(IndexWriter.java:579)\n        at org.apache.nutch.indexer.IndexSorter.sort(IndexSorter.java:240)\n        at org.apache.nutch.indexer.IndexSorter.main(IndexSorter.java:291)",
        "Issue Links": []
    },
    "NUTCH-260": {
        "Key": "NUTCH-260",
        "Summary": "Three new plugins that parse, index and query meta tags defined in the configuration",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.2",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Jake Vanderdray",
        "Created": "03/May/06 01:39",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:36",
        "Description": "These plugins allow you to define meta tags in you're nutch-site file that you want to include in parseing, indexing and searching.  The query plugin must replace query-basic.  The format for adding query terms to nutch-site.xml is:\n<property>\n  <name>meta.names</name>\n  <value>keywords,recommended</value>\n  <description>This is a comma seperated list of meta tag names that will\n  be parsed, indexed and searched against when parse-meta, index-meta and\n  query-meta are used.</description>\n</property>\n<property>\n  <name>meta.boosts</name>\n  <value>1.0,5.0</value>\n  <description>Comma seperated list of boost values when searching using\n  query-meta.  The order of the values should match the order of meta.names.\n  </description>\n</property>\nMeta tags found are assumed to have either a single value or be a comma seperated list of values.  The values found are added to the index as lucene keywords (i.e. meta name=keywords values=\"First Thing, Second Thing\" would result in two keyword fields named \"keywords\".  The first would countain \"First Thing\" and the second would contain \"Second Thing\").\nI had to replace the query-basic plugin in order to allow matches in the meta fields to return hits even if there were no matches in any of the default fields.  The query-basic field only returns hits when every search term is found in at least one default field.  I needed hits returned if matches were found in at least one field for every term, and/or the entire search phrase appeared in a meta index field.\nOne known bug is that common terms are not getting stripped out of the fields' values before they get indexed, so \"The Next Big Thing\" could not be matched because the query engine will strip out \"the\" from all queries.  I intend to fix this by stipping out common terms from meta fields before indexing them.\nAnother issue is that searching for \"Next Big Thing\" would not match meta index values for \"Next\", \"Big\" or \"Thing\".  You can consider that a bug or a feature depending on how you look at it.\nThese plugins were written for and only work on the 0.7.2 branch.\nI'm going to attache a tarball of the source of these three plugins after I create the issue.  To use the plugins, you'll need to untar them in your src/plugins directory and add them to the ant build.xml directive (and of course add them in your nutch-site.xml file).  If these end up getting added to the project, I'll write up documentation on the wiki.",
        "Issue Links": []
    },
    "NUTCH-261": {
        "Key": "NUTCH-261",
        "Summary": "Multi Language Support",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.7.2,                                            0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Jerome Charron",
        "Reporter": "Jerome Charron",
        "Created": "04/May/06 05:13",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "06/Feb/09 13:42",
        "Description": "Add multi-lingual support in Nutch, as described in http://wiki.apache.org/nutch/MultiLingualSupport\nThe document analysis part is actually implemented, and two analysis plugins (fr and de) are provided for testing (not deployed by default).\nThe query analysis part is missing for a complete multi-lingual support.",
        "Issue Links": [
            "/jira/browse/NUTCH-74"
        ]
    },
    "NUTCH-262": {
        "Key": "NUTCH-261 Multi Language Support",
        "Summary": "Summary excerpts and highlights problems",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Jerome Charron",
        "Created": "04/May/06 05:27",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "06/Feb/09 14:12",
        "Description": "There is some problems selecting and highlighting snippets for summary when multi-lingual support is used.",
        "Issue Links": [
            "/jira/browse/NUTCH-134"
        ]
    },
    "NUTCH-263": {
        "Key": "NUTCH-263",
        "Summary": "MapWritable.equals() doesn't work properly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "04/May/06 08:36",
        "Updated": "09/May/06 05:18",
        "Resolved": "09/May/06 05:18",
        "Description": "MapWritable.equals() is sensitive to the order in which map entries have been created. E.g. this fails but it should succeed:\n    MapWritable map1 = new MapWritable();\n    MapWritable map2 = new MapWritable();\n    map1.put(new UTF8(\"key1\"), new UTF8(\"val1\"));\n    map1.put(new UTF8(\"key2\"), new UTF8(\"val2\"));\n    map2.put(new UTF8(\"key2\"), new UTF8(\"val2\"));\n    map2.put(new UTF8(\"key1\"), new UTF8(\"val1\"));\n    assertTrue(map1.equals(map2));\nUsers expect that this should not be the case, i.e. this class should follow the same rules as Map.equals() (\"Returns true if the given object is also a map and the two Maps represent the same mappings\").",
        "Issue Links": []
    },
    "NUTCH-264": {
        "Key": "NUTCH-264",
        "Summary": "Tools for merging and filtering CrawlDb-s and LinkDb-s",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "04/May/06 09:46",
        "Updated": "09/May/06 05:17",
        "Resolved": "09/May/06 05:17",
        "Description": "This patch contains implementations and unit tests for two new commands:\n\nmergedb: merges one or more CrawlDb-s, optionally filtering urls through the current URLFilters.\n\n\nmergelinkdb: as above, only for LinkDb-s. Optional filtering is applied both to toUrls and fromUrls in Inlinks.",
        "Issue Links": []
    },
    "NUTCH-265": {
        "Key": "NUTCH-265",
        "Summary": "Getting Clustered results in better form.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kris K",
        "Created": "08/May/06 19:45",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "The cluster results are coming with title and link to URL. For improvement it should be clustered keyword phrases (Like  Vivisimo type). Any person can share their views on it.",
        "Issue Links": [
            "/jira/browse/NUTCH-103"
        ]
    },
    "NUTCH-266": {
        "Key": "NUTCH-266",
        "Summary": "hadoop bug when doing updatedb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Eugen Kochuev",
        "Created": "09/May/06 03:37",
        "Updated": "27/Oct/06 04:31",
        "Resolved": "08/Aug/06 19:31",
        "Description": "I constantly get the following error message\n060508 230637 Running job: job_pbhn3t\n060508 230637 c:/nutch/crawl-20060508230625/crawldb/current/part-00000/data:0+245\n060508 230637 c:/nutch/crawl-20060508230625/segments/20060508230628/crawl_fetch/part-00000/data:0+296\n060508 230637 c:/nutch/crawl-20060508230625/segments/20060508230628/crawl_parse/part-00000:0+5258\n060508 230637 job_pbhn3t\njava.io.IOException: Target /tmp/hadoop/mapred/local/reduce_qnd5sx/map_qjp7tf.out already exists\n        at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:162)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:62)\n        at org.apache.hadoop.fs.LocalFileSystem.renameRaw(LocalFileSystem.java:191)\n        at org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:306)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:101)\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:341)\n        at org.apache.nutch.crawl.CrawlDb.update(CrawlDb.java:54)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:114)",
        "Issue Links": []
    },
    "NUTCH-267": {
        "Key": "NUTCH-267",
        "Summary": "Indexer doesn't consider linkdb when calculating boost value",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Chris Schneider",
        "Created": "09/May/06 08:57",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "Before OPIC was implemented (Nutch 0.7, very early Nutch 0.8-dev), if indexer.boost.by.link.count was true, the indexer boost value was scaled based on the log of the # of inbound links:\n    if (boostByLinkCount)\n      res *= (float)Math.log(Math.E + linkCount);\nThis is no longer true (even before Andrzej implemented scoring filters). Instead, the boost value is just the square root (or some other scorePower) of the page score. Shouldn't the invertlinks command, which creates the linkdb, have some affect on the boost value calculated during indexing (either via the OPICScoringFilter or some other built-in filter)?",
        "Issue Links": []
    },
    "NUTCH-268": {
        "Key": "NUTCH-268",
        "Summary": "Generator and lib-http use different definitions of \"unique host\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "13/May/06 06:19",
        "Updated": "16/May/06 05:21",
        "Resolved": "16/May/06 05:21",
        "Description": "Generator uses a host name, as extracted from URL, to determine the maximum number of URLs from a unique host (when generator.max.per.host is set > 0). This supposedly should prevent the situation where fetchlists become dominated by URLs coming from the same hosts, which in turn would clash with \"politeness\" rules.\nHowever, http plugins (lib-http HttpBase.blockAddr) don't use host name, and instead use it's IP address (explicitly doing a DNS lookup on the host name extracted from URL). This leads to the following undesirable behavior:\n\nif DNS name resolves to different IPs (round-robin balancing), then technically we are in violation of the \"politeness\" rules, because lib-http doesn't see this as a conflict and permits concurrent accesses to the same host name.\n\n\nif different DNS names resolve to the same IP address (very common: CNAME-s, subdomains, web hosting, etc) then the purpose of generate.max.per.host is defeated, because lib-http will block more frequently than intended, leading to excessive numbers of  \"Exceeded http.max.delays\" exceptions.\n\nProposed solution: synchronize Generator and lib-http in their interpretation of \"unique host\". Introduce a boolean property which instructs both Generator and lib-http to use in both places either IP addresses or host names as \"unique hosts\".",
        "Issue Links": []
    },
    "NUTCH-269": {
        "Key": "NUTCH-269",
        "Summary": "CrawlDbReducer: OOME because no upper-bound on inlinks count",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Michael Stack",
        "Created": "16/May/06 05:29",
        "Updated": "22/May/13 03:54",
        "Resolved": "08/Jan/10 12:02",
        "Description": "A CrawlDB update repeatedly OOME'd because an URL had hundreds of thousands of inlinks (The british foriegn office likes putting a clear.gif multiple times into each page: http://www.fco.gov.uk/Xcelerate/graphics/images/fcomain/clear.gif).",
        "Issue Links": []
    },
    "NUTCH-270": {
        "Key": "NUTCH-61 Adaptive re-fetch interval. Detecting umodified content",
        "Summary": "Apply just the applicable portions of the patch to protocol.httpclient.Http.java",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Jeremy Calvert",
        "Created": "19/May/06 02:34",
        "Updated": "17/Jun/07 09:08",
        "Resolved": "17/Jun/07 09:08",
        "Description": "This seems to be two issues in one.  Adaptive scheduling AND content change detection.\nI don't see any reason not to apply the patch to allow content change detection.  That is, the parts of th patch to support changing the signature HttpResponse(URL url, long lastModified).  It'd be especially useful for those of us who refetch feeds fairly frequently.",
        "Issue Links": []
    },
    "NUTCH-271": {
        "Key": "NUTCH-271",
        "Summary": "Meta-data per URL/site/section",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "19/May/06 04:12",
        "Updated": "19/Jul/06 18:53",
        "Resolved": "19/Jul/06 18:21",
        "Description": "We have the need to index sites and attach additional meta-data-tags to them. Afaik this is not yet possible, or is there a \"workaround\" I don't see? What I think of is using meta-tags per start-url, only indexing content below that URL, and have the ability to limit searches upon those meta-tags. E.g.\nhttp://www.example1.com/something1/   -> meta-tag \"companybranch1\"\nhttp://www.example2.com/something2/   -> meta-tag \"companybranch2\"\nhttp://www.example3.com/something3/   -> meta-tag \"companybranch1\"\nhttp://www.example4.com/something4/   -> meta-tag \"companybranch3\"\nsearch for everything in companybranch1 or across 1 and 3 or similar",
        "Issue Links": [
            "/jira/browse/NUTCH-173"
        ]
    },
    "NUTCH-272": {
        "Key": "NUTCH-272",
        "Summary": "Max. pages to crawl/fetch per site (emergency limit)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "19/May/06 22:06",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "If I'm right, there is no way in place right now for setting an \"emergency limit\" to fetch a certain max. number of pages per site. Is there an \"easy\" way to implement such a limit, maybe as a plugin?",
        "Issue Links": []
    },
    "NUTCH-273": {
        "Key": "NUTCH-273",
        "Summary": "When a page is redirected, the original url is NOT updated.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Lukas Vlcek",
        "Created": "20/May/06 16:23",
        "Updated": "02/May/13 02:28",
        "Resolved": "28/Dec/06 00:18",
        "Description": "[Excerpt from maillist, sender: Andrzej Bialecki]\nWhen a page is redirected, the original url is NOT updated - so, CrawlDB will never know that a redirect occured, it won't even know that a fetch occured... This looks like a bug.\nIn 0.7 this was recorded in the segment, and then it would affect the Page status during updatedb. It should do so 0.8, too...",
        "Issue Links": [
            "/jira/browse/NUTCH-353",
            "/jira/browse/NUTCH-322",
            "/jira/browse/NUTCH-371"
        ]
    },
    "NUTCH-274": {
        "Key": "NUTCH-274",
        "Summary": "Empty row in/at end of URL-list results in error",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8.2,                                            0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Neufeind",
        "Created": "21/May/06 07:40",
        "Updated": "28/Dec/06 00:20",
        "Resolved": "28/Dec/06 00:20",
        "Description": "This is minor - but it's a little unclean \nReproduce: Have a URL-file with one URL followed by a newline, thus producing an empty line.\nOutcome: Fetcher-threads try to fetch two URLs at the same time. First one is fine - but second is empty and therefor fails proper protocol-detection.\n60521 022639   Nutch Analysis (org.apache.nutch.analysis.NutchAnalyzer)\n060521 022639   Nutch Query Filter (org.apache.nutch.searcher.QueryFilter)\n060521 022639 found resource parse-plugins.xml at file:/home/mm/nutch-nightly/conf/parse-plugins.xml\n060521 022639 Using URL normalizer: org.apache.nutch.net.BasicUrlNormalizer\n060521 022639 fetching http://www.bild.de/\n060521 022639 fetching \n060521 022639 fetch of  failed with: org.apache.nutch.protocol.ProtocolNotFound: java.net.MalformedURLException: no protocol: \n060521 022639 http.proxy.host = null\n060521 022639 http.proxy.port = 8080\n060521 022639 http.timeout = 10000\n060521 022639 http.content.limit = 65536\n060521 022639 http.agent = NutchCVS/0.8-dev (Nutch; http://lucene.apache.org/nutch/bot.html; nutch-agent@lucene.apache.org)\n060521 022639 fetcher.server.delay = 1000\n060521 022639 http.max.delays = 1000\n060521 022640 ParserFactory:Plugin: org.apache.nutch.parse.text.TextParser mapped to contentType text/xml via parse-plugins.xml, but\n its plugin.xml file does not claim to support contentType: text/xml\n060521 022640 ParserFactory:Plugin: org.apache.nutch.parse.html.HtmlParser mapped to contentType text/xml via parse-plugins.xml, but\n its plugin.xml file does not claim to support contentType: text/xml\n060521 022640 ParserFactory: Plugin: org.apache.nutch.parse.rss.RSSParser mapped to contentType text/xml via parse-plugins.xml, but \nnot enabled via plugin.includes in nutch-default.xml\n060521 022640 Using Signature impl: org.apache.nutch.crawl.MD5Signature\n060521 022640  map 0%  reduce 0%\n060521 022640 1 pages, 1 errors, 1.0 pages/s, 40 kb/s, \n060521 022640 1 pages, 1 errors, 1.0 pages/s, 40 kb/s,",
        "Issue Links": []
    },
    "NUTCH-275": {
        "Key": "NUTCH-275",
        "Summary": "Fetcher not parsing XHTML-pages at all",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "21/May/06 08:00",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "07/Jun/06 20:08",
        "Description": "Server reports page as \"text/html\" - so I thought it would be processed as html.\nBut something I guess evaluated the headers of the document and re-labeled it as \"text/xml\" (why not text/xhtml?).\nFor some reason there is no plugin to be found for indexing text/xml (why does TextParser not feel responsible?).\nLinks inside this document are NOT indexed at all - no digging this website actually stops here.\nFunny thing: For some magical reasons the dtd-files referenced in the header seem to be valid links for the fetcher and as such are indexed in the next round (if urlfilter allows).\n060521 025018 fetching http://www.secreturl.something/\n060521 025018 http.proxy.host = null\n060521 025018 http.proxy.port = 8080\n060521 025018 http.timeout = 10000\n060521 025018 http.content.limit = 65536\n060521 025018 http.agent = NutchCVS/0.8-dev (Nutch; http://lucene.apache.org/nutch/bot.html; nutch-agent@lucene.apache.org)\n060521 025018 fetcher.server.delay = 1000\n060521 025018 http.max.delays = 1000\n060521 025018 ParserFactory:Plugin: org.apache.nutch.parse.text.TextParser mapped to contentType text/xml via parse-plugins.xml, but\n its plugin.xml file does not claim to support contentType: text/xml\n060521 025018 ParserFactory: Plugin: org.apache.nutch.parse.rss.RSSParser mapped to contentType text/xml via parse-plugins.xml, but \nnot enabled via plugin.includes in nutch-default.xml\n060521 025019 Using Signature impl: org.apache.nutch.crawl.MD5Signature\n060521 025019  map 0%  reduce 0%\n060521 025019 1 pages, 0 errors, 1.0 pages/s, 40 kb/s, \n060521 025019 1 pages, 0 errors, 1.0 pages/s, 40 kb/s,",
        "Issue Links": []
    },
    "NUTCH-276": {
        "Key": "NUTCH-276",
        "Summary": "db.score.link.internal problem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Eugen Kochuev",
        "Created": "22/May/06 01:02",
        "Updated": "23/Sep/06 19:47",
        "Resolved": "23/Sep/06 19:47",
        "Description": "The issue was discovered while communicating with Andrzej Bialecki.\n> Another questions are about db.score.injected and\n> db.score.link.internal parameters. They are listed in the\n> nutch-default.conf, but are never referenced in  the code.\n>   \ndb.score.injected is used in the above-mentioned OPIC scoring plugin, \nand in CrawlDbReducer. db.score.link.internal might be used in these \nplaces, but isn't - please file a bug report, this needs to be fixed (if \nwe really want it to be fixed, i.e. if we really want to distinguish \nbetween internal/external links when calculating score contributions and \nsetting initial scores).",
        "Issue Links": [
            "/jira/browse/NUTCH-324"
        ]
    },
    "NUTCH-277": {
        "Key": "NUTCH-277",
        "Summary": "Fetcher dies because of \"max. redirects\" (avoiding infinite loop)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Neufeind",
        "Created": "22/May/06 02:55",
        "Updated": "19/Mar/07 23:46",
        "Resolved": "19/Mar/07 23:46",
        "Description": "Error in the logs is:\n060521 213401 SEVERE Narrowly avoided an infinite loop in execute\norg.apache.commons.httpclient.RedirectException: Maximum redirects (100) exceeded\n        at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:183)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:396)\n        at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:324)\n        at org.apache.nutch.protocol.httpclient.HttpResponse.<init>(HttpResponse.java:87)\n        at org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:97)\n        at org.apache.nutch.protocol.http.api.RobotRulesParser.isAllowed(RobotRulesParser.java:394)\n        at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:173)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:135)\nThis happens during normal crawling. Unfortunately I don't know how to further track this down. But it's problematic, since it actually makes the fetcher die.\nWorkaround (for the symptom) is in NUTCH-258 (avoid dying on SEVERE logentry). That works for me, crawling works fine and it does not hang/crash.  However this is working around the problems not solving them - I know. But it helps for the moment ...\nHope somebody can help - this loops quite important to track down to me.",
        "Issue Links": [
            "/jira/browse/NUTCH-258"
        ]
    },
    "NUTCH-278": {
        "Key": "NUTCH-278",
        "Summary": "Fetcher-status might need clarification: kbit/s instead of kb/s shown",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Stefan Neufeind",
        "Created": "22/May/06 06:09",
        "Updated": "22/May/13 03:54",
        "Resolved": "26/Jun/10 05:54",
        "Description": "In Fetcher.java, method reportStatus() there is\n        + Math.round(((((float)bytes)*8)/1024)/elapsed)+\" kb/s, \";\nIs that a bit misleading, since the user reading the status might guess it's \"kilobytes\" (kb) whereas \"kbit/s\" would be more clear in this case?",
        "Issue Links": []
    },
    "NUTCH-279": {
        "Key": "NUTCH-279",
        "Summary": "Additions for regex-normalize",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Neufeind",
        "Created": "22/May/06 20:09",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "03/Feb/09 15:16",
        "Description": "Imho needed:\n1) Extend normalize-rules to commonly used session-id's etc.\n2) Ship a checker to check rules easily by hand",
        "Issue Links": [
            "/jira/browse/NUTCH-255"
        ]
    },
    "NUTCH-280": {
        "Key": "NUTCH-280",
        "Summary": "url query causes NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Grant Glouser",
        "Created": "23/May/06 06:41",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "24/May/06 00:27",
        "Description": "A search such as \"url:java.sun.com\" causes a NullPointerException.\nThe cause is setConf() in URLQueryFilter (in the query-url plugin) that overrides FieldQueryFilter.setConf(), but does not call super.setConf().  The superclass, FieldQueryFilter, depends on setConf in order to initialize its commonGrams.  When FieldQueryFilter tries to access commonGrams later on, it throws a NullPointerException.\nThis bug only affects phrase URL queries, so a simple URL query like url:sun would not hit it.\nHere is a simple patch which fixes this problem.  An alternative would be to remove all configuration fields and methods from URLQueryFilter since it does not use them.\nIndex: src/plugin/query-url/src/java/org/apache/nutch/searcher/url/URLQueryFilter.java\n===================================================================\n\u2014 src/plugin/query-url/src/java/org/apache/nutch/searcher/url/URLQueryFilter.java\t(revision 1260)\n+++ src/plugin/query-url/src/java/org/apache/nutch/searcher/url/URLQueryFilter.java\t(working copy)\n@@ -31,6 +31,7 @@\n   }\n   public void setConf(Configuration conf) \n{\n+    super.setConf(conf);\n     this.conf = conf;\n   }",
        "Issue Links": []
    },
    "NUTCH-281": {
        "Key": "NUTCH-281",
        "Summary": "cached.jsp: base-href needs to be outside comments",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "24/May/06 07:50",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "see cached.jsp\n<base href=\"...\">\ndoes not take effect when showing a cached page because of the comments around it",
        "Issue Links": []
    },
    "NUTCH-282": {
        "Key": "NUTCH-282",
        "Summary": "Showing too few results on a page (Paging not correct)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "24/May/06 08:14",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "02/Jun/06 23:33",
        "Description": "I did a search and got back the  value \"itemsPerPage\" from opensearch. But the output shows \"results 1-8\" and I have a total of 46 searchresults.\nSame happens for the webinterface.\nWhy aren't \"enough\" results fetched?\nThe problem might be somewhere in the area of where Nutch should only display a certaian number of websites per site.",
        "Issue Links": [
            "/jira/browse/NUTCH-288"
        ]
    },
    "NUTCH-283": {
        "Key": "NUTCH-283",
        "Summary": "If the Fetcher times out and abandons Fetcher Threads, severe errors will occur on those Threads",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Scott Ganyo",
        "Created": "25/May/06 00:10",
        "Updated": "01/Apr/11 14:56",
        "Resolved": "01/Apr/11 14:56",
        "Description": "If a Fetcher has chosen to time out and has abandoned outstanding Fetcher Threads, resources that those Fetcher Threads may be using are closed.  This naturally causes any abandoned Fetcher Threads to fail when they later attempt to finish up their work in progress.\nI have a patch that addresses this that I am attaching.",
        "Issue Links": []
    },
    "NUTCH-284": {
        "Key": "NUTCH-284",
        "Summary": "NullPointerException during index",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "25/May/06 02:30",
        "Updated": "02/Jun/06 22:57",
        "Resolved": "02/Jun/06 22:55",
        "Description": "For  quite a few this \"reduce > sort\" has been going on. Then it fails. What could be wrong with this?\n060524 212613 reduce > sort\n060524 212614 reduce > sort\n060524 212615 reduce > sort\n060524 212615 found resource common-terms.utf8 at file:/home/mm/nutch-nightly-prod/conf/common-terms.utf8\n060524 212615 found resource common-terms.utf8 at file:/home/mm/nutch-nightly-prod/conf/common-terms.utf8\n060524 212619 Optimizing index.\n060524 212619 job_jlbhhm\njava.lang.NullPointerException\n        at org.apache.nutch.indexer.Indexer$OutputFormat$1.write(Indexer.java:111)\n        at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:269)\n        at org.apache.nutch.indexer.Indexer.reduce(Indexer.java:253)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:282)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:114)\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:341)\n        at org.apache.nutch.indexer.Indexer.index(Indexer.java:287)\n        at org.apache.nutch.indexer.Indexer.main(Indexer.java:304)",
        "Issue Links": [
            "/jira/browse/NUTCH-51"
        ]
    },
    "NUTCH-285": {
        "Key": "NUTCH-285",
        "Summary": "LinkDb Fails rename doesn't create parent directories",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dennis Kubes",
        "Created": "25/May/06 04:38",
        "Updated": "25/May/06 07:43",
        "Resolved": "25/May/06 07:43",
        "Description": "The LinkDb install method fails to correctly rename (move) the LinkDb working directory to the final directory if the parent directories do not exist.\nFor example if I am creating a linkdb by the name of crawl/linkdb the install method trys to rename the working linkdb directory (something like linkdb-20060523 in root of DFS) to crawl/linkdb/current.  But if the crawl/linkdb directory does not already exist then the rename fails and the linkdb-20060523 working directory stays in the root directory of the DFS for the user.\nThe attached patch adds a mkdirs command to the install method to ensure that the parent directories exist before trying to rename.",
        "Issue Links": []
    },
    "NUTCH-286": {
        "Key": "NUTCH-286",
        "Summary": "Handling common error-pages as 404",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "25/May/06 06:39",
        "Updated": "02/Jun/06 23:36",
        "Resolved": "02/Jun/06 23:36",
        "Description": "Idea: Some pages from some software-packages/scripts report an \"http 200 ok\" even though a specific page could not be found. Example I just found  is:\nhttp://www.deteimmobilien.de/unternehmen/nbjmup;Uipnbt/IfsctuAefufjnnpcjmjfo/ef\nThat's a typo3-page explaining in it's standard-layout and wording: \"The requested page did not exist or was inaccessible.\"\nSo I had the idea if somebody might create a plugin that could find commonly used formulations for \"page does not exist\" etc. and turn the page into a 404 before feeding them  into the nutch-index  - although the server responded with status 200 ok.",
        "Issue Links": []
    },
    "NUTCH-287": {
        "Key": "NUTCH-287",
        "Summary": "Exception when searching with sort",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "25/May/06 19:10",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "02/Jun/06 22:54",
        "Description": "Running a search with      &sort=url works.\nBut when using    &sort=title   I get the following exception.\n2006-05-25 14:04:25 StandardWrapperValve[jsp]: Servlet.service() for servlet jsp threw exception\njava.lang.RuntimeException: Unknown sort value type!\n        at org.apache.nutch.searcher.IndexSearcher.translateHits(IndexSearcher.java:157)\n        at org.apache.nutch.searcher.IndexSearcher.search(IndexSearcher.java:95)\n        at org.apache.nutch.searcher.NutchBean.search(NutchBean.java:239)\n        at org.apache.jsp.search_jsp._jspService(search_jsp.java:257)\n        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:94)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:324)\n        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:292)\n        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:236)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:214)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:198)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:152)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:137)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:118)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:102)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:929)\n        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:160)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:799)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.processConnection(Http11Protocol.java:705)\n        at org.apache.tomcat.util.net.TcpWorkerThread.runIt(PoolTcpEndpoint.java:577)\n        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)\n        at java.lang.Thread.run(Thread.java:595)\nWhat is in those lines is:\n      WritableComparable sortValue;               // convert value to writable\n      if (sortField == null) \n{\n        sortValue = new FloatWritable(scoreDocs[i].score);\n      }\n else {\n        Object raw = ((FieldDoc)scoreDocs[i]).fields[0];\n        if (raw instanceof Integer) \n{\n          sortValue = new IntWritable(((Integer)raw).intValue());\n        }\n else if (raw instanceof Float) \n{\n          sortValue = new FloatWritable(((Float)raw).floatValue());\n        }\n else if (raw instanceof String) \n{\n          sortValue = new UTF8((String)raw);\n        }\n else \n{\n          throw new RuntimeException(\"Unknown sort value type!\");\n        }\n      }\nSo I thought that maybe raw is an instance of something \"strange\" and tried raw.getClass().getName() or also raw.toString() to track the cause down - but that always resulted in a NullPointerException. So it seems I'm having raw being null for some strange reason.\nWhen I try with \"title2\" (or something none-existing) I get a different error that title2 is unknown / not indexed. So I suspect that title should be fine here ...\nIf there is any information I can help out with, let me know.",
        "Issue Links": []
    },
    "NUTCH-288": {
        "Key": "NUTCH-288",
        "Summary": "hitsPerSite-functionality \"flawed\": problems writing a page-navigation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "25/May/06 22:25",
        "Updated": "16/Aug/11 11:53",
        "Resolved": "16/Aug/11 11:53",
        "Description": "The deduplication-functionality on a per-site-basis (hitsPerSite = 3) leads to problems when trying to offer a page-navigation (e.g. allow the user to jump to page 10). This is because dedup is done after fetching.\nRSS shows a maximum number of 7763 documents (that is without dedup!), I set it to display 10 items per page. My \"naive\" approach was to estimate I have 7763/10 = 777 pages. But already when moving to page 3 I got no more searchresults (I guess because of dedup). And when moving to page 10 I  got an exception (see below).\n2006-05-25 16:24:43 StandardWrapperValve[OpenSearch]: Servlet.service() for servlet OpenSearch threw exception\njava.lang.NegativeArraySizeException\n        at org.apache.nutch.searcher.Hits.getHits(Hits.java:65)\n        at org.apache.nutch.searcher.OpenSearchServlet.doGet(OpenSearchServlet.java:149)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:689)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:214)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardContextValve.invokeInternal(StandardContextValve.java:198)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:152)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:137)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:118)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:102)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n        at org.apache.catalina.core.StandardValveContext.invokeNext(StandardValveContext.java:104)\n        at org.apache.catalina.core.StandardPipeline.invoke(StandardPipeline.java:520)\n        at org.apache.catalina.core.ContainerBase.invoke(ContainerBase.java:929)\n        at org.apache.coyote.tomcat5.CoyoteAdapter.service(CoyoteAdapter.java:160)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:799)\n        at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.processConnection(Http11Protocol.java:705)\n        at org.apache.tomcat.util.net.TcpWorkerThread.runIt(PoolTcpEndpoint.java:577)\n        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)\n        at java.lang.Thread.run(Thread.java:595)\nOnly workaround I see for the moment: Fetching RSS without duplication, dedup myself and cache the RSS-result to improve performance. But a cleaner solution would imho be nice. Is there a performant way of doing deduplication and knowing for sure how many documents are available to view? For sure this would mean to dedup all search-results first ...",
        "Issue Links": [
            "/jira/browse/NUTCH-282"
        ]
    },
    "NUTCH-289": {
        "Key": "NUTCH-289",
        "Summary": "CrawlDatum should store IP address",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Doug Cutting",
        "Created": "27/May/06 03:36",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "If the CrawlDatum stored the IP address of the host of it's URL, then one could:\n\npartition fetch lists on the basis of IP address, for better politeness;\ntruncate pages to fetch per IP address, rather than just hostname.  This would be a good way to limit the impact of domain spammers.\n\nThe IP addresses could be resolved when a CrawlDatum is first created for a new outlink, or perhaps during CrawlDB update.",
        "Issue Links": []
    },
    "NUTCH-290": {
        "Key": "NUTCH-290",
        "Summary": "parse-pdf: Garbage indexed when text-extraction not allowed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "28/May/06 20:34",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "It seems that garbage (or undecoded text?) is indexed when text-extraction for a PDF is not allowed.\nExample-PDF:\nhttp://www.task-switch.nl/Dutch/articles/Management_en_Architectuur_v3.pdf",
        "Issue Links": [
            "/jira/browse/NUTCH-335",
            "/jira/browse/NUTCH-338"
        ]
    },
    "NUTCH-291": {
        "Key": "NUTCH-291",
        "Summary": "OpenSearchServlet should return \"date\" as well as \"lastModified\"",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": "Dennis Kubes",
        "Reporter": "Stefan Neufeind",
        "Created": "28/May/06 23:45",
        "Updated": "25/Mar/09 14:49",
        "Resolved": "25/Mar/09 14:49",
        "Description": "Currently lastModified is provided by OpenSearchServlet - but only in case the date lastModified-date is known.\nSince you can sort by \"date\" (which is lastModified or if not present the fetchdate), it might be useful if OpenSearchServlet could provide \"date\" as well.",
        "Issue Links": []
    },
    "NUTCH-292": {
        "Key": "NUTCH-292",
        "Summary": "OpenSearchServlet: OutOfMemoryError: Java heap space",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "Stefan Neufeind",
        "Created": "29/May/06 00:36",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "21/Jun/06 02:54",
        "Description": "java.lang.RuntimeException: java.lang.OutOfMemoryError: Java heap space\n\torg.apache.nutch.searcher.FetchedSegments.getSummary(FetchedSegments.java:203)\n\torg.apache.nutch.searcher.NutchBean.getSummary(NutchBean.java:329)\n\torg.apache.nutch.searcher.OpenSearchServlet.doGet(OpenSearchServlet.java:155)\n\tjavax.servlet.http.HttpServlet.service(HttpServlet.java:689)\n\tjavax.servlet.http.HttpServlet.service(HttpServlet.java:802)\nThe URL I use is:\n[...]something[...]/opensearch?query=mysearch&start=0&hitsPerSite=3&hitsPerPage=20&sort=url\nIt seems to be a problem specific to the date I'm working with. Moving the start from 0 to 10 or changing the query works fine.\nOr maybe it doesn't have to do with sorting but it's just that I hit one \"bad search-result\" that has a broken summary?\n!! The problem is repeatable. So if anybody has an idea where to search / what to fix, I can easily try that out !!",
        "Issue Links": []
    },
    "NUTCH-293": {
        "Key": "NUTCH-293",
        "Summary": "support for Crawl-delay in Robots.txt",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "02/Jun/06 00:22",
        "Updated": "19/Jul/06 22:05",
        "Resolved": "19/Jul/06 22:05",
        "Description": "Nutch need support for Crawl-delay defined in robots.txt, it is not a standard but a de-facto standard.\nSee:\nhttp://help.yahoo.com/help/us/ysearch/slurp/slurp-03.html\nWebmasters start blocking nutch since we do not support it.",
        "Issue Links": []
    },
    "NUTCH-294": {
        "Key": "NUTCH-294",
        "Summary": "Topic-maps of related searchwords",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "02/Jun/06 13:55",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "Would it be possible to offer a user  \"topic-maps\"? It's when you search for something and get topic-related words that might also be of interest for you. I wonder if that's somehow possible with the ngram-index for \"did you mean\" (see separate feature-enhancement-bug for this), but we'd need to have a relation between words (in what context do they occur).\nFor the webfrontend usually trees are used  - which for some users offer quite impressive eye-candy  E.g. see this advertisement by Novell where I've just seen a similar \"topic-map\" as well:\nhttp://www.novell.com/de-de/company/advertising/defineyouropen.html",
        "Issue Links": []
    },
    "NUTCH-295": {
        "Key": "NUTCH-295",
        "Summary": "More description for fetcher.threads.fetch property",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Dennis Kubes",
        "Created": "02/Jun/06 23:57",
        "Updated": "27/Jun/11 11:42",
        "Resolved": "27/Jun/11 11:41",
        "Description": "Added some description to the fetcher.threads.fetch property to explain the number of threads running in a cluster. Patch is attached.",
        "Issue Links": []
    },
    "NUTCH-296": {
        "Key": "NUTCH-296",
        "Summary": "Image Search",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Thomas Delnoij",
        "Created": "03/Jun/06 23:53",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Sep/11 16:42",
        "Description": "Per the discussion in the Nutch-User mailing list, there is a wish for an \"Image Search\" add-on component that will index images.\nMust have:\n\nretrieve outlinks to image files from fetched pages\ngenerate thumbnails from images\nthumbnails are stored in the segments as ImageWritable that contains the compressed binary data and some meta data\n\nShould have:\n\nimplemented as hadoop map reduce job\nshould be seperate from main Nutch codeline as it breaks general Nutch logic of one url == one index document.\n\nCould  have:\n\nstore the original image in the segments\n\nWould like to have:\n\nsearch interface for image index\nparameterizable thumbnail generation (width, height, quality)",
        "Issue Links": []
    },
    "NUTCH-297": {
        "Key": "NUTCH-296 Image Search",
        "Summary": "sandbox svn folder",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "04/Jun/06 00:12",
        "Updated": "13/Apr/11 23:36",
        "Resolved": "13/Apr/11 23:36",
        "Description": "Having a svn sandbox repository would allow people to work on a image search.\nShould be outside of nutch/trunk, may be nutch/sandbox/imageSearch/trunk ?",
        "Issue Links": []
    },
    "NUTCH-298": {
        "Key": "NUTCH-298",
        "Summary": "if a 404 for a robots.txt is returned a NPE is thrown",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "04/Jun/06 02:43",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "06/Jun/06 04:46",
        "Description": "What happen:\nIs no RobotRuleSet is in the cache for a host, we create try to fetch the robots.txt.\nIn case http response code is not 200 or 403 but for example 404 we do \" robotRules = EMPTY_RULES; \" (line: 402)\nEMPTY_RULES is a RobotRuleSet created with the default constructor.\ntmpEntries and entries is null and will never changed.\nIf we now try to fetch a page from the host that use the EMPTY_RULES is used and we call isAllowed in the RobotRuleSet.\nIn this case a NPE is thrown in this line:\n if (entries == null) {\n        entries= new RobotsEntry[tmpEntries.size()];\npossible Solution:\nWe can intialize the tmpEntries by default and also remove other null checks and initialisations.",
        "Issue Links": []
    },
    "NUTCH-299": {
        "Key": "NUTCH-299",
        "Summary": "Bittorrent Parser",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Hasan Diwan",
        "Created": "04/Jun/06 06:03",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "BitTorrent information file parser",
        "Issue Links": []
    },
    "NUTCH-300": {
        "Key": "NUTCH-300",
        "Summary": "Clustering API improvements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "05/Jun/06 22:19",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "This patch adds support for retrieving original document scores (from NutchBean), as well as cluster-level relevance scores (from Clusterer). Both methods may improve visual representation of the clusters, where individual items may be visually differentiated depending on their query relevance and cluster relevance. A modified cluster.jsp illustrates this feature.",
        "Issue Links": []
    },
    "NUTCH-301": {
        "Key": "NUTCH-301",
        "Summary": "CommonGrams loads analysis.common.terms.file for each query",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chris Schneider",
        "Created": "07/Jun/06 09:49",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "08/Jun/06 05:19",
        "Description": "The move away from static objects toward instance variables has resulted in CommonGrams constructor parsing its analysis.common.terms.file for each query. I'm not certain how large a performance impact this really is, but it seems like something you'd want to avoid doing for each query. Perhaps the solution is to keep around an instance of the CommonGrams object itself?",
        "Issue Links": []
    },
    "NUTCH-302": {
        "Key": "NUTCH-302",
        "Summary": "java doc of CrawlDb is wrong",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Stefan Groschupf",
        "Created": "07/Jun/06 23:28",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "21/Jun/06 01:55",
        "Description": "CrawlDb has the same java doc as Injector.",
        "Issue Links": []
    },
    "NUTCH-303": {
        "Key": "NUTCH-303",
        "Summary": "logging improvements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Jerome Charron",
        "Created": "07/Jun/06 23:55",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "13/Jun/06 04:00",
        "Description": "Switch to the apache commons logging facade.\nSee HADOOP-211 and following thread http://www.mail-archive.com/nutch-developers%40lists.sourceforge.net/msg08706.html",
        "Issue Links": [
            "/jira/browse/NUTCH-307",
            "/jira/browse/NUTCH-236",
            "/jira/browse/NUTCH-309",
            "/jira/browse/NUTCH-310",
            "/jira/browse/NUTCH-258"
        ]
    },
    "NUTCH-304": {
        "Key": "NUTCH-304",
        "Summary": "Change JIRA email address for nutch issues from apache incubator",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chris A. Mattmann",
        "Created": "09/Jun/06 11:13",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "03/Oct/06 22:22",
        "Description": "The default email address for Nutch issues in JIRA should be changed from nutch-dev@incubator.apache.org to nutch-dev@lucene.apache.org. Could one of the commiters with appropriate jira privileges update the email?",
        "Issue Links": []
    },
    "NUTCH-305": {
        "Key": "NUTCH-305",
        "Summary": "Update crawl and url filter lists to exclude jpeg|JPEG|bmp|BMP",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "chris finne",
        "Created": "09/Jun/06 13:51",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "21/Nov/06 18:39",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-306": {
        "Key": "NUTCH-306",
        "Summary": "DistributedSearch.Client liveAddresses concurrency problem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Grant Glouser",
        "Created": "10/Jun/06 08:33",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "28/Jun/06 02:34",
        "Description": "Under heavy load, hits returned by DistributedSearch.Client can become out of sync with the Client's live server list.\nDistributedSearch.Client maintains an array of live search servers (liveAddresses).  This array is updated at intervals by a watchdog thread.  When the Client returns hits from a search, it tracks which hits came from which server by saving an index into the liveAddresses array (as Hit.indexNo).\nThe problem occurs when the search servers cannot service some remote procedure calls before the client times out (due to heavy load, for example).  If the Client returns some Hits from a search, and then the array of liveAddresses changes while the Hits are still being used, the indexNos for those Hits can become invalid, referring to different servers than the Hit originated from (or no server at all!).\nSymptoms of this problem include:\n\nArrayIndexOutOfBoundsException (when the array of liveAddresses shrinks, a Hit from the last server in liveAddresses in the previous update cycle now has an indexNo past the end of the array)\n\n\nIOException: read past EOF (suppose a hit comes back from server A with a doc number of 1000.  Then the watchdog thread updates liveAddresses and now the Hit looks like it came from server B, but server B only has 900 documents.  Trying to get details for the hit will read past EOF in server B's index.)\n\n\nOf course, you could also get a \"silent\" failure in which you find a hit on server A, but the details/summary are fetched from server B.  To the user, it would simply look like an incorrect or nonsense hit.\n\nWe have solved this locally by removing the liveAddresses array.  Instead, the watchdog thread updates an array of booleans (same size as the array of defaultAddresses) that indicate whether that address responded to the latest call from the watchdog thread.  Hit.indexNo is then always an index into the complete array of defaultAddresses, so it is stable and always valid.  Callers of getDetails()/getSummary()/etc. must still be aware that these methods may return null when the corresponding server is unable to respond.",
        "Issue Links": []
    },
    "NUTCH-307": {
        "Key": "NUTCH-307",
        "Summary": "wrong configured log4j.properties",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Jerome Charron",
        "Reporter": "Stefan Groschupf",
        "Created": "19/Jun/06 20:35",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "22/Jun/06 03:22",
        "Description": "In nutch/conf is only one  log4j.properties and it define:\nlog4j.appender.DRFA.File=${nutch.log.dir}/${nutch.log.file}\nnutch.log.dir and nutch.log.file is only defined in the bin/nutch script. \nIn case of starting a distributed nutch instance with bin/start-all the remove tasktracker crash with:\n java.io.FileNotFoundException: / (Is a directory)\ncr06:   at java.io.FileOutputStream.openAppend(Native Method)\ncr06:   at java.io.FileOutputStream.<init>(FileOutputStream.java:177)\ncr06:   at java.io.FileOutputStream.<init>(FileOutputStream.java:102)\ncr06:   at org.apache.log4j.FileAppender.setFile(FileAppender.java:289)\ncr06:   at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163)\ncr06:   at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215)\ncr06:   at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256)\nsince the hadoop scripts used to start the tasktrackers and datanodes never define the nutch log properties but the log4j.properties require such a definition.\nI suggest to leave the log4j.properties as it is in hadoop but define the hadoop property names in the bin/nutch script instead of intriduce new variable names.",
        "Issue Links": [
            "/jira/browse/NUTCH-303"
        ]
    },
    "NUTCH-308": {
        "Key": "NUTCH-308",
        "Summary": "Maximum search time limit",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "22/Jun/06 07:58",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "27/Jun/06 02:40",
        "Description": "With large indexes it may often happen that search servers don't respond in time. The IPC code time-outs such calls, and the front-end DistributedSearch.Client no longer expects any results from the timed-out Server. However, the search thread is still running on the Server, until it completes (which may be even tens of seconds later), consuming server resources. Subsequent requests will run much slower, eventually leading to 100% CPU/disk utilization and an avalanche of timeouts.\nThis patch adds support for a maximum search time limit. After this time is exceeded, the search thread is stopped, partial results (if any) are returned and the total number of results is estimated.\nThis patch tries to minimize the overhead related to time-keeping by using a version of safe unsynchronized timer.",
        "Issue Links": []
    },
    "NUTCH-309": {
        "Key": "NUTCH-309",
        "Summary": "Uses commons logging Code Guards",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Jerome Charron",
        "Created": "22/Jun/06 17:47",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "14/Jul/10 17:46",
        "Description": "\"Code guards are typically used to guard code that only needs to execute in support of logging, that otherwise introduces undesirable runtime overhead in the general case (logging disabled). Examples are multiple parameters, or expressions (e.g. string + \" more\") for parameters. Use the guard methods of the form log.is<Priority>() to verify that logging should be performed, before incurring the overhead of the logging method call. Yes, the logging methods will perform the same check, but only after resolving parameters.\"\n(description extracted from http://jakarta.apache.org/commons/logging/guide.html#Code_Guards)",
        "Issue Links": [
            "/jira/browse/NUTCH-303",
            "/jira/browse/NUTCH-454"
        ]
    },
    "NUTCH-310": {
        "Key": "NUTCH-310",
        "Summary": "Review Log Levels",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Jerome Charron",
        "Created": "22/Jun/06 17:52",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "08/Aug/10 18:53",
        "Description": "Review of logs content and logs levels (see Commons Logging Best Parctices : http://jakarta.apache.org/commons/logging/guide.html#Message_Priorities_Levels)",
        "Issue Links": [
            "/jira/browse/NUTCH-303"
        ]
    },
    "NUTCH-311": {
        "Key": "NUTCH-311",
        "Summary": "Page with tens of thousands of links OOME'd.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael Stack",
        "Created": "23/Jun/06 11:07",
        "Updated": "13/Apr/11 23:12",
        "Resolved": "13/Apr/11 23:12",
        "Description": "Came across a page that caused OOME because no upper-bound on link count  in a CrawlDatum.",
        "Issue Links": []
    },
    "NUTCH-312": {
        "Key": "NUTCH-312",
        "Summary": "Fix for upcoming incompatibility with Hadoop-0.4",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Milind Barve",
        "Created": "28/Jun/06 03:42",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "29/Jun/06 04:54",
        "Description": "I have submitted a patch to Hadoop fixing tasktracker-latency issues. That patch introduces incompatibility with current nutch code, because the interface for OutputFormat will change. I will soon submit a patch for nutch that will fix this upcoming incompatibility with Hadoop.",
        "Issue Links": []
    },
    "NUTCH-313": {
        "Key": "NUTCH-313",
        "Summary": "moreFrom property in search.properties cannot be translated into Japanese.  Compound text issue.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Kuro Kurosaka",
        "Created": "28/Jun/06 05:30",
        "Updated": "13/Apr/11 23:36",
        "Resolved": "13/Apr/11 23:36",
        "Description": "The \"moreFrom\" property in search.properties is currently defined as:\nmoreFrom = more from\nand is used, in search.jsp, to construct HTML fragement that would be viewed as:\n(more from http://xxxx.yyy.zzz)\nThe current code hard-codes the URL to be lead by \"more from\" or its translation.\nThis cannot be translated into Japanese naturally because the Japanese language\nexpects the URL to come at the beginning of the clause.\nJust like \"hits\" property, moreFrom should use the place holder within so that\nthe English property would look like:\nmoreFrom = more from \n{0}\n\nThe fix is trivial but it requires modifications to search.jsp, search.properties and search_xx.properties at the same time.",
        "Issue Links": []
    },
    "NUTCH-314": {
        "Key": "NUTCH-314",
        "Summary": "Multiple language identifier instances",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Enrico Triolo",
        "Created": "28/Jun/06 19:30",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jan/13 19:38",
        "Description": "In my application I often need to perform the inject -> generate -> .. -> index loop multiple times, since users can 'suggest' new web pages to be crawled and indexed.\nI also need to enable the language identifier plugin.\nEverything seems to work correctly, but after some time I get an OutOfMemoryException. Actually the time isn't important, since I noticed that the problem arises when the user submits many urls (~100). As I said, for each submitted url a new loop is performed (similar to the one in the Crawl.main method).\nUsing a profiler (specifically, netbeans profiler) I found out that for each submitted url a new LanguageIdentifier instance is created, and never released. With the memory inspector tool I can see as many instances of LanguageIdentifier and NGramProfile$NGramEntry as the number of fetched pages, each of them occupying about 180kb. Forcing garbage collection doesn't release much memory.\nMaybe we should cache its instance in the conf as we do for many others objects in Nutch.",
        "Issue Links": []
    },
    "NUTCH-315": {
        "Key": "NUTCH-315",
        "Summary": "CrawlDbReader usage text - implementation mismatch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "29/Jun/06 00:43",
        "Updated": "18/Apr/07 15:46",
        "Resolved": "26/Jul/06 06:47",
        "Description": "Usege text talks about printing to System.out, implementation uses logging (which by default does not go to System.out)",
        "Issue Links": []
    },
    "NUTCH-316": {
        "Key": "NUTCH-316",
        "Summary": "Confusion about query languages",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Kuro Kurosaka",
        "Created": "01/Jul/06 06:33",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "In 2006-6-16 nightly source code, src/web/jsp/search.jsp has these lines:\n  String queryLang = request.getParameter(\"lang\");\n  if (queryLang == null) \n{ queryLang = \"\"; }\n  Query query = Query.parse(queryString, queryLang, nutchConf);\nAccording to the observation of URLs shown in the browser, the lang parameter reflects the language\nof the GUI (the language in which GUI elements are labeled) as the user clicks on the two letter code \nnear the bottom of each Nutch GUI screen.\nThe Java API Doc on Query is not clear about what queryLang is meant.  Is this the language of\nthe query (how query should be lemmatized, if supported by the analyzer, and what stop word list\nshould be applied), is is this the language of the documents to be searched?\nAlthough the two concepts above are closely related, they are not tied to the GUI language at all.\nI, as Japanese user, might prefer to see all GUIs in Japanese, but I would still need to\nsearch English documents for Englsh words.  The current implementation of search.jsp seems\nto restrict search domain to the documents of the GUI language in one way (by treating the\nterms to be from the GUI language), or the other (restricting the search domain to the documents\nof the GI language).\nTo be perfect, there should be a drop-down list from which the language of query analyzer\nis selected, and a set of check boxes from which the document languages can be selected,\nin addition to the existing line of two letter language codes from which the GUI language is choosen.\nBut that would be too clutering.  \nGoogle uses a separate configuration screen to let the user to choose a set of languages\nof the documents to be searched.  That might be a good middle-of-the-road approach.\nBecause of the lack of language processing on search terms, Google does not need to know\nthe language of the query.  Nutch GUI might want to have a drop down list from which a language\nof the query can be choosen, with the GUI language pre-selected.",
        "Issue Links": []
    },
    "NUTCH-317": {
        "Key": "NUTCH-317",
        "Summary": "Clarify what the queryLanguage argument of Query.parse(...) means",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kuro Kurosaka",
        "Created": "01/Jul/06 06:39",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "06/Jul/06 23:47",
        "Description": "API document on \n  Query.parse(String queryString,\n                          String queryLang,\n                          Configuration conf)\ndoes not explain what queryLang is, and should be explained.\nThere can be at least two interpretations:\n(1) Create a Query that restricts the search to include only the documents written in the specified language. So this would\nbe equivalent of specifying \"lang:xx\" where xx is a two-letter language code.\n(2) Create a Query interpreting the queryString according to the rules of the specified languages.  In reality, this is used to\nselect the proper language Analyzer to parse the query string.\nI am guessing that (2) is intended.",
        "Issue Links": []
    },
    "NUTCH-318": {
        "Key": "NUTCH-318",
        "Summary": "log4j not proper configured, readdb doesnt give any information",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Stefan Groschupf",
        "Created": "11/Jul/06 02:05",
        "Updated": "24/Sep/06 15:30",
        "Resolved": "01/Aug/06 16:25",
        "Description": "In the latest .8 sources the readdb command doesn't dump any information anymore. \nThis is realeated to the miss configured log4j.properties file. \nchanging:\nlog4j.rootLogger=INFO,DRFA\nto:\nlog4j.rootLogger=INFO,DRFA,stdout\ndumps the information to the console, but not in a nice way. \nWhat makes me wonder  is that these information should be also in the log file, but the arn't, so there are may be even here problems.\nAlso what is the different between hadoop-XXX-jobtracker-XXX.out and hadoop-XXX-jobtracker-XXX.log ?? Shouldn't there just one of them?",
        "Issue Links": []
    },
    "NUTCH-319": {
        "Key": "NUTCH-319",
        "Summary": "OPICScoringFilter should use logging API instead of printStackTrace",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "15/Jul/06 22:43",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "19/Jul/06 23:55",
        "Description": "OPICScoringFilter line 107 should be a logging not a   e.printStackTrace(LogUtil.getWarnStream(LOG)), isn't it?",
        "Issue Links": []
    },
    "NUTCH-320": {
        "Key": "NUTCH-320",
        "Summary": "DmozParser does not output urls to stdout",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "17/Jul/06 06:51",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "17/Jul/06 06:55",
        "Description": "DmozParser does not output list of urls to stdout but to a log file instead. Original functionality needs to be restored.",
        "Issue Links": []
    },
    "NUTCH-321": {
        "Key": "NUTCH-321",
        "Summary": "Scoring API deficiency",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "17/Jul/06 13:52",
        "Updated": "19/Jul/06 22:40",
        "Resolved": "19/Jul/06 22:40",
        "Description": "Currently the method ScoringFilter.updateDbScore() doesn't use the \"old\" value from existing CrawlDB. Instead it uses the value taken from the fetchlist from the current segment, which represents a snapshot of the \"old\" value taken at the moment of generating the fetchlist.\nThe problem with this approach is that if/when we add a possibility to interleave generate/fetch/update cycles, the initial score values in CrawlDatum instance that comes from the current segment could be already outdated, if another updatedb was run in the meantime, which changed the DB score.\nFor this reason we should always assume that the value from CrawlDB, if exists, represents the most recent version of CrawlDatum before the update, and use this instance as a base.",
        "Issue Links": []
    },
    "NUTCH-322": {
        "Key": "NUTCH-322",
        "Summary": "Fetcher discards ProtocolStatus, doesn't store redirected pages",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "19/Jul/06 12:10",
        "Updated": "28/Dec/06 00:16",
        "Resolved": "28/Dec/06 00:16",
        "Description": "Fetcher doesn't store ProtocolStatus in output segments. ProtocolStatus contains important information, such as protocol-level response code, lastModified time, and possibly other messages.\nI propose that ProtocolStatus should be stored inside CrawlDatum.metaData, which is then stored into crawl_fetch (in Fetcher.FetcherThread.output()). In addition, if ProtocolStatus contains a valid lastModified time, that CrawlDatum's modified time should also be set to this value.\nAdditionally, Fetcher doesn't store redirected pages. Content of such pages is silently discarded. When Fetcher translates from protocol-level status to crawldb-level status it should probably store such pages with the following translation of status codes:\n\nProtocolStatus.TEMP_MOVED -> CrawlDatum.STATUS_DB_RETRY. This code indicates a transient change, so we probably shouldn't mark the initial URL as bad.\n\n\nProtocolStatus.MOVED -> CrawlDatum.STATUS_DB_GONE. This code indicates a permanent change, so the initial URL is no longer valid, i.e. it will always result in redirects.",
        "Issue Links": [
            "/jira/browse/NUTCH-273"
        ]
    },
    "NUTCH-323": {
        "Key": "NUTCH-323",
        "Summary": "CrawlDatum.set just reference a mapWritable of a other object but not copy it.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "19/Jul/06 21:37",
        "Updated": "19/Jul/06 22:33",
        "Resolved": "19/Jul/06 22:33",
        "Description": "Using CrawlDatum.set(aOtherCrawlDatum) copies the data from one CrawlDatum to a other. \nAlso a reference of the MapWritable is passed. Means both project share the same mapWritable and its content. \nThis causes problems with concurent manipulate mapWritables and its key-value tuples.",
        "Issue Links": []
    },
    "NUTCH-324": {
        "Key": "NUTCH-324",
        "Summary": "db.score.link.internal and db.score.link.external are ignored",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "19/Jul/06 23:47",
        "Updated": "01/Aug/06 17:00",
        "Resolved": "24/Jul/06 15:26",
        "Description": "Configuration properties db.score.link.external and db.score.link.internal  are ignored.\nIn case of e.g. message board webpages or pages that have larger navigation menus on each page having a lower impact of internal links makes a lot of sense for scoring.\nAlso for web spam this is a serious problem, since now spammers can setup just one domain with dynamically generated pages and this highly manipulate the nutch scores. \nSo I also suggest that we give db.score.link.internal by default a value of something like 0.25.",
        "Issue Links": [
            "/jira/browse/NUTCH-276"
        ]
    },
    "NUTCH-325": {
        "Key": "NUTCH-325",
        "Summary": "UrlFilters.java throws NPE in case urlfilter.order contains Filters that are not in plugin.includes",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "20/Jul/06 21:54",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "06/Jan/07 09:43",
        "Description": "In URLFilters constructor we use an array as long as we have filters defined in the urlfilter.order property. \nIn case those filters are not included in the plugin.include property end up putting null entries into the array.\nThis cause a NPE in URLFilters line 82.",
        "Issue Links": []
    },
    "NUTCH-326": {
        "Key": "NUTCH-326",
        "Summary": "WordExtractor throws java.util.NoSuchElementException on some documents",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.1,                                            0.7.2",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Tom Jensen",
        "Created": "21/Jul/06 21:57",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "At line 156 in org.apache.nutch.parse.msword.WordExtractor it will on occassion throw a java.util.NoSuchElementException because there is no checking as to whether or not the Iterator has been exhausted.  Suggest adding this:\n        if (!textIt.hasNext()) \n{\n        \tbreak;\n        }\n\njust before line 156.  Tested with problem word documents.  Results were Exceptions no longer being thrown and text extracted successfully.  Other documents that successfully had their text extracted previously continued to do so.",
        "Issue Links": []
    },
    "NUTCH-327": {
        "Key": "NUTCH-327",
        "Summary": "bin/nutch setting of log path problems on cygwin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "23/Jul/06 18:29",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "23/Jul/06 18:44",
        "Description": "logs are written to wrong directory.\ncure: preprocess NUTCH_LOG_DIR the same way as the CLASSPATH",
        "Issue Links": []
    },
    "NUTCH-328": {
        "Key": "NUTCH-328",
        "Summary": "commons-cli-2.0-SNAPSHOT.jar provided with nutch is not compatible with jdk 1.4",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "23/Jul/06 18:55",
        "Updated": "24/Oct/06 16:14",
        "Resolved": "23/Jul/06 18:58",
        "Description": "We need to commit the library from issue http://issues.apache.org/jira/browse/HADOOP-350\nto nutch also",
        "Issue Links": []
    },
    "NUTCH-329": {
        "Key": "NUTCH-329",
        "Summary": "CrawlDbReader processTopNJob does not set jobNames",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "24/Jul/06 01:09",
        "Updated": "24/Jul/06 08:38",
        "Resolved": "24/Jul/06 08:38",
        "Description": "processTopNJob runs two job and both have no jobname setted.",
        "Issue Links": []
    },
    "NUTCH-330": {
        "Key": "NUTCH-330",
        "Summary": "command line tool to search a Lucene index",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Renaud Richardet",
        "Created": "25/Jul/06 20:19",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "22/Sep/08 15:21",
        "Description": "Tool to allow to search a Lucene index from the command line, makes development and testing faster\nusage:   bin/nutch searchindex [index dir] [searchkeyword]\nexample: bin/nutch searchindex crawl/index flowers",
        "Issue Links": []
    },
    "NUTCH-331": {
        "Key": "NUTCH-331",
        "Summary": "Fetcher incorrectly reports task progress to tasktracker resulting in skipped URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.8,                                            0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "27/Jul/06 11:10",
        "Updated": "23/Nov/06 10:56",
        "Resolved": "23/Nov/06 10:56",
        "Description": "Each Fetcher task starts multiple FetcherThreads, which consume the input fetchlist. These threads may block for a long time after being started and after reading their input fetchlist entries, due to \"politeness\" settings. However, the map-reduce framework considers the task as complete when all input data is read.\nThis causes the tasktracker to incorreclty assume that task processing is complete (because the task progress is 1.0, since all input has been consumed), whereas many URLs from the fetchlist may still be waiting for fetching, in blocked threads. The more threads is used the more apparent is this problem, because the final number of fetched pages may be short of the target number by as many as (numThreads * numMapTasks) entries.\nThe final result of this is that only a part of the fetchlist is fetched, because Fetcher map tasks are stopped when their progress is 1.0.",
        "Issue Links": []
    },
    "NUTCH-332": {
        "Key": "NUTCH-332",
        "Summary": "doubling score causes by page internal anchors.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "28/Jul/06 06:12",
        "Updated": "22/Sep/06 21:45",
        "Resolved": "22/Sep/06 21:45",
        "Description": "When a page has no outlinks but several links to itself e.g. it has a set of anchors the scores of the page are distributed to its outlinks. But all this outlinks pointing to the page back. This causes that the page score is doubled. \nI'm not sure but may be this causes also a never ending fetching loop of this page, since outlinks with the status of CrawlDatum.STATUS_LINKED are set CrawlDatum.STATUS_DB_UNFETCHED in CrawlDBReducer line: 107. \nSo may be the status fetched will be overwritten with unfetched. \nIn such a case we fetch the page every-time again and also every-time double the score of this page what causes very high scores without any reasons.",
        "Issue Links": []
    },
    "NUTCH-333": {
        "Key": "NUTCH-333",
        "Summary": "SegmentMerger and SegmentReader should use NutchJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael Stack",
        "Created": "29/Jul/06 04:20",
        "Updated": "03/Apr/07 01:20",
        "Resolved": "03/Apr/07 01:19",
        "Description": "I have a job jar that is nutch with additions. I can launch this job jar on a pure hadoop platform usually without issue.  I can run nutch jobs \u2013 update db, invert links, etc. \u2013 without issue.  Recently I tried to do the same with SegmentMerg'ing only it would fail complaining about ClassNotFound:\n2006-07-28 20:43:54,371 WARN org.apache.hadoop.mapred.JobTracker: job init failed\njava.io.IOException: java.lang.ClassNotFoundException: org.apache.nutch.segment.SegmentMerger$ObjectInputFormat\n        at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:130)\n        at org.apache.hadoop.mapred.JobTracker$JobInitThread.run(JobTracker.java:310)\n        at java.lang.Thread.run(Thread.java:595)\njava.io.IOException: Job failed!\nAfter digging and chatting today with Stefan, the SegmentMerger and SegmentReader classes are not like the others.   Others make a new JobConf inside in their job setup by doing a 'new NutchJob' whereas Segment* does 'new JobConf'.  Sure enough, if I make the change, all works. \nNutchJob triggers the setting of the job jar into the configuration (JobConf.findContainingJar is run).  This doesn't happen for 'new JobConf'.",
        "Issue Links": []
    },
    "NUTCH-334": {
        "Key": "NUTCH-334",
        "Summary": "I am using the search technique",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Siddharudh nadgeri",
        "Created": "31/Jul/06 14:31",
        "Updated": "05/Aug/06 16:10",
        "Resolved": "05/Aug/06 16:10",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-335": {
        "Key": "NUTCH-335",
        "Summary": "Pdf summary corrupt issue",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Siddharudh nadgeri",
        "Created": "31/Jul/06 14:35",
        "Updated": "09/Oct/09 15:47",
        "Resolved": "09/Oct/09 15:47",
        "Description": "I am using the Nutch search but for pdf it is giving summary as some garbage like\n\"!Unable to render embedded object: File (\"#\"#\"#\"#\"#\"#\"#\") not found.$%$%$#&##'$$$$$$$$$$$$$$$$$$ (\"$$$$$$$$$$$$$$$$$$$\nplease provide the solution",
        "Issue Links": [
            "/jira/browse/NUTCH-290",
            "/jira/browse/NUTCH-338"
        ]
    },
    "NUTCH-336": {
        "Key": "NUTCH-336",
        "Summary": "Harvested links shouldn't get db.score.injected in addition to inbound contributions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Chris Schneider",
        "Created": "01/Aug/06 17:20",
        "Updated": "23/Sep/06 17:28",
        "Resolved": "23/Sep/06 17:28",
        "Description": "Currently (even with Stefan's fix for NUTCH-324), harvested links have their initial scores set to db.score.injected + (sum of inbound contributions * db.score.link.[internal | external]), but this will place (at least external) harvested links even higher than injected URLs on the fetch list. Perhaps more importantly, this effect cascades.\nAs a simple example, if each page in A->B->C->D has exactly one external link and only A is injected, then D will receive an initial score of at least (4*db.score.injected) with the default db.score.link.external of 1.0. Higher values of db.score.injected and db.score.link.external obviously exacerbate this problem.",
        "Issue Links": []
    },
    "NUTCH-337": {
        "Key": "NUTCH-337",
        "Summary": "Fetcher ignores the fetcher.parse value configured in config file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.9.0",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Jeremy Huylebroeck",
        "Created": "02/Aug/06 00:04",
        "Updated": "23/Sep/06 18:56",
        "Resolved": "23/Sep/06 18:56",
        "Description": "using the command line call to Fetcher, if the noParsing parameter is given, everything is fine.\nif the noParsing is not given, the value in the nutch-site.xml (or nutch-default.xml) should be taken but it is \"true\" that is always given to the call to fetch.\nit should be the value from the conf.",
        "Issue Links": []
    },
    "NUTCH-338": {
        "Key": "NUTCH-338",
        "Summary": "Remove the text parser as an option for parsing PDF files in parse-plugins.xml",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "03/Aug/06 15:32",
        "Updated": "24/Sep/06 15:30",
        "Resolved": "18/Aug/06 15:11",
        "Description": "After some discussion on the mailing list, it was decided that parse-text should not really be an option to parse PDF content. So, this issue includes a trivial patch to remove the parse text plugin from being mapped to PDF content in parse-pugins.xml.",
        "Issue Links": [
            "/jira/browse/NUTCH-362",
            "/jira/browse/NUTCH-290",
            "/jira/browse/NUTCH-335"
        ]
    },
    "NUTCH-339": {
        "Key": "NUTCH-339",
        "Summary": "Refactor nutch to allow fetcher improvements",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Sami Siren",
        "Created": "04/Aug/06 14:17",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "06/Feb/08 12:29",
        "Description": "As I (and Stefan?) see it there are two major areas the current fetcher could be\nimproved (as in speed)\n1. Politeness code and how it is implemented is the biggest\nproblem of current fetcher(together with robots.txt handling).\nWith a simple code changes like replacing it with a PriorityQueue\nbased solution showed very promising results in increased IO.\n2. Changing fetcher to use non blocking io (this requires great amount\nof work as we need to implement the protocols from scratch again).\nI would like to start with working towards #1 by first refactoring\nthe current code (plugins actually) in following way:\n1. Move robots.txt handling away from (lib-http)plugin.\nEven if this is related only to http, leaving it to lib-http\ndoes not allow other kinds of scheduling strategies to be implemented\n(it is hardcoded to fetch robots.txt from the same thread when requesting\na page from a site from witch it hasn't tried to load robots.txt)\n2. Move code for politeness away from (lib-http)plugin\nIt is really usable outside http and also the current design limits\nchanging of the implementation (to queue based)\nWhere to move these, well my suggestion is the nutch core, does anybody\nsee problems with this?\nThese code refactoring activities are to be done in a way that none\nof the current functionality is (at least deliberately) changed leaving\ncurrent functionality as is thus leaving room and possibility to build\nthe next generation fetcher(s) without destroying the old one at same time.",
        "Issue Links": []
    },
    "NUTCH-340": {
        "Key": "NUTCH-340",
        "Summary": "Bug(s) in 0.8 tutorial",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "documentation",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "04/Aug/06 16:50",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "05/Aug/06 13:50",
        "Description": "There seems to be error(s) in whole web crawling section. This generates constantly (unneccessary) traffic to users list.",
        "Issue Links": []
    },
    "NUTCH-341": {
        "Key": "NUTCH-341",
        "Summary": "IndexMerger now deletes entire <workingdir> after completing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Chris Schneider",
        "Created": "05/Aug/06 04:17",
        "Updated": "18/Aug/06 18:46",
        "Resolved": "18/Aug/06 18:46",
        "Description": "Change 383304 deleted the following line near Line 117 (see <http://svn.apache.org/viewvc/lucene/nutch/trunk/src/java/org/apache/nutch/indexer/IndexMerger.java?r1=383304&r2=405204&diff_format=h> for details):\nworkDir = new File(workDir, \"indexmerger-workingdir\");\nPreviously, if no -workingdir <workingdir> parameter was specified, IndexMerger.main() would place an \"indexmerger-workingdir\" directory into the default directory and then delete the former after completing. Now, IndexMerger.main() defaults the value of its workDir to \"indexmerger\" within the default directory, and deletes this workDir afterward.\nHowever, if -workingdir <workingdir> is specified, IndexMerger.main() will now set workDir to this path and delete the entire <workingdir> afterward. Previously, IndexMerger.main() would only delete <workingDir>/\"indexmerger-workingdir\", without deleting <workingdir> itself. This is because the line mentioned above always appended \"indexmerger-workingdir\" to workDir.\nOur hardware configuration on the jobtracker/namenode box attempts to keep all large datasets on a separate, large hard drive. Accordingly, we were keeping dfs.name.dir, dfs.data.dir, mapred.system.dir, and mapred.local.dir on this drive. Unfortunately, we were passing the folder containing these folders in the <workingdir> parameter to the IndexMerger. As a result, the first time we ran the IndexMerger, we ended up trashing our entire DFS!\nPerhaps the way that the IndexMerger handles its <workingdir> parmaeter now is an acceptable design. However, given the way it handled this parameter in the past, I feel that the current implementation is unacceptably dangerous.\nMore importantly, perhaps there's some way that we could make hadoop more robust in handling its critical data files. I plan to place a directory owned by root with \"dr--------\" permissions into each of these critical directories in order to prevent any of them from suffering the fate of our DFS. This could become part of a standard hadoop installation.",
        "Issue Links": []
    },
    "NUTCH-342": {
        "Key": "NUTCH-342",
        "Summary": "Nutch commands log to nutch/logs/hadoop.logs by default",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chris Schneider",
        "Created": "05/Aug/06 15:03",
        "Updated": "30/Apr/13 08:52",
        "Resolved": "30/Apr/13 08:52",
        "Description": "If (by default) Nutch commands are going to send their output to a file named \"hadoop.log\", then it seems like the default location for this file should be the same location where Hadoop is putting its hadoop.log file (i.e., $HADOOP_LOG_DIR). Currently, if I set HADOOP_LOG_DIR to a special location (via hadoop-env.sh), this has no effect on where Nutch commands send their output.\nSome would probably suggest that I could just set NUTCH_LOG_DIR to $HADOOP_LOG_DIR myself. I still think that it should be defaulted this way in the nutch script. However, I'm unaware of an elegant way to modify such Nutch environment variables anyway. The hadoop-env.sh file provides a convenient place to modify Hadoop environment variables, but doing the same for Nutch environment variables presumably requires you to modify .bash_profile or a similar user script file (which is the way I used to accomplish this kind of thing with Nutch 0.7).",
        "Issue Links": []
    },
    "NUTCH-343": {
        "Key": "NUTCH-343",
        "Summary": "Index MP3 SHA1 hashes",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Hasan Diwan",
        "Created": "06/Aug/06 20:57",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "Add indexing of the mp3s sha1 hash.",
        "Issue Links": []
    },
    "NUTCH-344": {
        "Key": "NUTCH-344",
        "Summary": "Fetcher threads blocked on synchronized block in cleanExpiredServerBlocks",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Greg Kim",
        "Created": "07/Aug/06 23:54",
        "Updated": "24/Sep/06 15:30",
        "Resolved": "08/Aug/06 19:09",
        "Description": "With the recent change to the following code in HttpBase.java has tendencies to block fetcher threads while one thread busy waits... \n  private static void cleanExpiredServerBlocks() {\n    synchronized (BLOCKED_ADDR_TO_TIME) {\n      while (!BLOCKED_ADDR_QUEUE.isEmpty()) {   <===== LINE 3:   \n        String host = (String) BLOCKED_ADDR_QUEUE.getLast();\n        long time = ((Long) BLOCKED_ADDR_TO_TIME.get(host)).longValue();\n        if (time <= System.currentTimeMillis()) \n{   \n          BLOCKED_ADDR_TO_TIME.remove(host);\n          BLOCKED_ADDR_QUEUE.removeLast();\n        }\n      }\n    }\n  }\nLINE3:  As long as there are any entries in the BLOCKED_ADDR_QUEUE, the thread that first enters this block busy-waits until it becomes empty while all other threads block on the synchronized block.  This leads to extremely poor fetcher performance.  \nSince the checkin to respect crawlDelay in robots.txt, we are no longer guranteed that BLOCKED_ADDR_TO_TIME queue is a fifo list. The simple fix is to iterate the queue once rather than busy waiting...",
        "Issue Links": []
    },
    "NUTCH-345": {
        "Key": "NUTCH-345",
        "Summary": "Add support for Content-Encoding: deflated",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Pascal Beis",
        "Created": "08/Aug/06 11:03",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Dec/08 13:48",
        "Description": "Add support for the \"deflated\" content-encoding, next to the already\nimplemented GZIP content-encoding. Patch attached. See also the\n\"Patch: deflate encoding\" thread on nutch-dev on August 7/8 2006.",
        "Issue Links": []
    },
    "NUTCH-346": {
        "Key": "NUTCH-346",
        "Summary": "Improve readability of logs/hadoop.log",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Renaud Richardet",
        "Created": "09/Aug/06 20:12",
        "Updated": "29/Apr/13 00:26",
        "Resolved": "28/Apr/13 23:36",
        "Description": "adding\nlog4j.logger.org.apache.nutch.plugin.PluginRepository=WARN\nto conf/log4j.properties\ndramatically improves the readability of the logs in logs/hadoop.log (removes all INFO)",
        "Issue Links": []
    },
    "NUTCH-347": {
        "Key": "NUTCH-347",
        "Summary": "Build: plugins' Jars not found",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Otis Gospodnetic",
        "Created": "12/Aug/06 03:49",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "18/Aug/06 14:54",
        "Description": "While building Nutch, I noticed several places where various Jars from plugins' lib directories could not be found, for example:\n$ ant package\n...\ndeploy:\n[copy] Warning: Could not find file /home/otis/dev/repos/lucene/nutch/trunk/build/lib-log4j/lib-log4j.jar to copy.\ninit:\ninit-plugin:\ncompile:\njar:\ndeps-test:\ndeploy:\n[copy] Warning: Could not find file /home/otis/dev/repos/lucene/nutch/trunk/build/lib-nekohtml/lib-nekohtml.jar to copy.\n...\nThe problem is, these \"lib-XXXX.jar\" files do not exist.  Instead, those Jars are typically named with a version in the name, like log4j-1.2.11.jar.  I could not find where this \"lib-\" prefix comes from, nor where the version is dropped from the name.  Anyone knows?\nIn order to avoid these errors I had to make symbolic links and fake things:\ne.g.\n  ln -s log4j-1.2.11.jar lib-log4j.jar\nBut this should really be fixed somewhere, I just can't see where... \nNote that this doesn't completely break the build, but missing Jars can't be a good thing.",
        "Issue Links": []
    },
    "NUTCH-348": {
        "Key": "NUTCH-348",
        "Summary": "Generator is building fetch list using *lowest* scoring URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Chris Schneider",
        "Created": "16/Aug/06 07:26",
        "Updated": "17/Aug/06 16:33",
        "Resolved": "17/Aug/06 16:33",
        "Description": "Ever since revision 391271, when the CrawlDatum key was replaced by a FloatWritable key, the Generator.Selector.reduce method has been outputting the lowest scoring URLs! The CrawlDatum class has a Comparator that essentially treats higher scoring CrawlDatum objects as \"less than\" lower scoring CrawlDatum objects, so the higher scoring ones would appear first in a sequence file sorted using this as the key.\nWhen a FloatWritable based on the score itself (as returned from scfilters.generatorSortValue) became the sort key, it should have been negated in Generator.Selector.map to have the same result. Curiously, there is a comment to this effect immediately before the FloatWritable is set:\n      // sort by decreasing score\n      sortValue.set(sort);\nIt seems like the simplest way to fix this is to just negate the score, and this seems to work for me:\n      // sort by decreasing score\n      // 2006-08-15 CSc REALLY sort by decreasing score\n      sortValue.set(-sort);\nUnfortunately, this means that any crawls that have been done using Generator.java after revision 391271 should be discarded, as they were focused on fetching the lowest scoring unfetched URLs in the crawldb, essentially pointing the crawler 180 degrees from its intended direction.",
        "Issue Links": []
    },
    "NUTCH-349": {
        "Key": "NUTCH-349",
        "Summary": "Port Nutch to use Hadoop Text instead of UTF8",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "16/Aug/06 11:34",
        "Updated": "21/Nov/06 17:38",
        "Resolved": "21/Nov/06 17:38",
        "Description": "Currently Nutch uses org.apache.hadoop.io.UTF8 class to store/read Strings. This class has been deprecated in Hadoop 0.5.0, and Text class should be used instead. Sooner or later we will need to move Nutch to use this class instead of UTF8.\nThis raises numerous issues regarding the compatibility of existing data in CrawlDB, LinkDB and segments. I can see two ways to solve this:\n\nadd code in readers of respective formats to convert UTF8->Text on the fly. New writers would only use Text. This is less than ideal, because it complicates the code, and also at some point in time the UTF8 class will be removed.\n\n\ncreate a converter (to be maintaines as long as UTF8 exists), which converts existing data in bulk from UTF8 to Text. This requires an additional processing step when upgrading to convert all existing data to the new format.",
        "Issue Links": []
    },
    "NUTCH-350": {
        "Key": "NUTCH-350",
        "Summary": "urls blocked db.fetch.retry.max * http.max.delays times during fetching are marked as STATUS_DB_GONE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "17/Aug/06 19:29",
        "Updated": "23/Sep/06 19:44",
        "Resolved": "23/Sep/06 19:44",
        "Description": "Intranet crawls or focused crawls will fetch many pages from the same host. This causes that a thread will be blocked since a other thread already fetch from the same host. It is very likely that threads are more often blocked than http.max.delays. In such a case the HttpBase.blockAddr method throws a HttpException. This will be handled in the fetcher by increment the crawlDatum retries and set the status to STATUS_FETCH_RETRY. That means that at least you have only db.fetch.retry.max * http.max.delays chances to fetch a url. But in intranet or focused crawls it is very likely that this is not enough. Increaing one of the involved properties dramatically slow down the fetch. \nI suggest to not increase the CrawlDatum RetriesSinceFetch in case the problem was caused by a blocked thread.",
        "Issue Links": []
    },
    "NUTCH-351": {
        "Key": "NUTCH-351",
        "Summary": "Protocol forward proxy",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "17/Aug/06 19:38",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "03/Apr/14 15:15",
        "Description": "Protocol proxy adapter takes advantage of protocols known to http forward proxy. Usually there's atleast http, https and ftp.\nYou must configure nutch to use this plugin and to use http proxy before use.",
        "Issue Links": []
    },
    "NUTCH-352": {
        "Key": "NUTCH-352",
        "Summary": "Add jar command to bin/nutch to allow launching hadoop job jars",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Cathcart",
        "Created": "17/Aug/06 21:51",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "Add the ability to run hadoop job jars via bin/nutch jar jobjar.jar. See attachment for patch.",
        "Issue Links": []
    },
    "NUTCH-353": {
        "Key": "NUTCH-353",
        "Summary": "pages that serverside forwards will be refetched every time",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1,                                            0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "18/Aug/06 04:50",
        "Updated": "02/May/13 02:28",
        "Resolved": "03/Feb/09 13:19",
        "Description": "Pages that do a serverside forward are not written with a status change back into the crawlDb. Also the nextFetchTime is not changed. \nThis causes a refetch of the same page again and again. The result is nutch is not polite and refetching the forwarding and target page in each segment iteration. Also it effects the scoring since the forward page contribute it's score to all outlinks.",
        "Issue Links": [
            "/jira/browse/NUTCH-273"
        ]
    },
    "NUTCH-354": {
        "Key": "NUTCH-354",
        "Summary": "MapWritable,  nextEntry is not reset when Entries are recycled",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.8.1,                                            0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Groschupf",
        "Created": "19/Aug/06 22:48",
        "Updated": "21/Aug/06 19:08",
        "Resolved": "19/Aug/06 23:28",
        "Description": "MapWritables recycle entries from it internal linked-List for performance reasons. The nextEntry of a entry is not reseted in case a recyclable entry is found. This can cause wrong data in a MapWritable.",
        "Issue Links": []
    },
    "NUTCH-355": {
        "Key": "NUTCH-355",
        "Summary": "The title of query result  could like the summary have the highlight??",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8,                                            1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "King Kong",
        "Created": "20/Aug/06 14:12",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:36",
        "Description": "I'd like to make the title hightlight, but i can't found how to do it .\nwhen i query \"Nutch\" , the result must like this:\n<a href=\"http://lucene.apache.org/nutch/\" >Welcome to <b>Nutch</b>!  </a>  \nThis is the first <b>Nutch</b> release as an Apache Lucene sub-project. See CHANGES.txt for details. The release is available here. ... <b>Nutch</b>has now graduated from the Apache incubator, and is now a Subproject of Lucene. ...\n....",
        "Issue Links": []
    },
    "NUTCH-356": {
        "Key": "NUTCH-356",
        "Summary": "Plugin repository cache can lead to memory leak",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Enrico Triolo",
        "Created": "21/Aug/06 11:59",
        "Updated": "01/May/14 06:23",
        "Resolved": "24/Jan/14 13:22",
        "Description": "While I was trying to solve a problem I reported a while ago (see Nutch-314), I found out that actually the problem was related to the plugin cache used in class PluginRepository.java.\nAs  I said in Nutch-314, I think I somehow 'force' the way nutch is meant to work, since I need to frequently submit new urls and append their contents to the index; I don't (and I can't) have an urls.txt file with all urls I'm going to fetch, but I recreate it each time a new url is submitted.\nThus,  I think in the majority of times you won't have problems using nutch as-is, since the problem I found occours only if nutch is used in a way similar to the one I use.\nTo simplify your test I'm attaching a class that performs something similar to what I need. It fetches and index some sample urls; to avoid webmasters complaints I left the sample urls list empty, so you should modify the source code and add some urls.\nThen you only have to run it and watch your memory consumption with top. In my experience I get an OutOfMemoryException after a couple of minutes, but it clearly depends on your heap settings and on the plugins you are using (I'm using 'protocol-file|protocol-http|parse-(rss|html|msword|pdf|text)|language-identifier|index-(basic|more)|query-(basic|more|site|url)|urlfilter-regex|summary-basic|scoring-opic').\nThe problem is bound to the PluginRepository 'singleton' instance, since it never get released. It seems that some class maintains a reference to it and this class is never released since it is cached somewhere in the configuration.\nSo I modified the PluginRepository's 'get' method so that it never uses the cache and always returns a new instance (you can find the patch in attachment). This way the memory consumption is always stable and I get no OOM anymore.\nClearly this is not the solution, since I guess there are many performance issues involved, but for the moment it works.",
        "Issue Links": [
            "/jira/browse/NUTCH-844"
        ]
    },
    "NUTCH-357": {
        "Key": "NUTCH-357",
        "Summary": "crawling simulation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8.1,                                            0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Stefan Groschupf",
        "Created": "22/Aug/06 05:39",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "06/Feb/09 13:45",
        "Description": "We recently discovered  some serious issue related to crawling and scoring. Reproducing these problems is a kind of difficult, since first of all it is not polite to re-crawl a set of pages again and again, secondly it is difficult to catch the page that cause a problem. \nTherefore it would be very useful to have a testbed to simulate crawls where  we can control the response of  \"web servers\". \nFor the very beginning simulate very basic situation like a page points to it self,  link chains or internal links would already be very usefully. \nHowever later on simulate crawls against existing data collections like TREC or a webgraph would be much more interesting, for instance to caculate the quality of the nutch OPIC implementation against page rank scores of the webgraph or evaluaing crawling strategies.",
        "Issue Links": []
    },
    "NUTCH-358": {
        "Key": "NUTCH-358",
        "Summary": "Language Switching PROBLEM FIXED",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "David Podunavac",
        "Created": "22/Aug/06 13:02",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "Language selection on bottom of page does not affect the result page.\nSo if browser language config is set to e.g. \"en\" result page(search.jsp) will be displayed in EN\nbrowsers language. NO matter what language has been selected (the locale links of the bottom of page).\nrequest.getParameter=\"lang\" is useless as far as i can see\nSo the links on bottom of the page does not translate the reslutpages keywords.\nThis must be a BUG \nand shall be reported what i did now for that reason.",
        "Issue Links": []
    },
    "NUTCH-359": {
        "Key": "NUTCH-359",
        "Summary": "extraction of links will fail for whole page if one single link cannot be parsed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Renaud Richardet",
        "Created": "23/Aug/06 23:23",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 16:07",
        "Description": "When Nutch parses the outlinks of a fetched page, the process will fail if a single link cannot be parsed (e.g. java.net.MalformedURLException: unknown protocol). The attached patch will keep indexing the remaining links on that page even if one fails.",
        "Issue Links": []
    },
    "NUTCH-360": {
        "Key": "NUTCH-360",
        "Summary": "Switch nutch to use java 5 source format",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "01/Sep/06 15:18",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "02/Sep/06 05:14",
        "Description": "Since hadoop now requires java 5 there's no reason to hold back in Nutch source code.",
        "Issue Links": []
    },
    "NUTCH-361": {
        "Key": "NUTCH-361",
        "Summary": "generator create fetchlist randomly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.8.2,                                            0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Uros Gruber",
        "Created": "02/Sep/06 18:14",
        "Updated": "31/Oct/06 22:07",
        "Resolved": "31/Oct/06 22:07",
        "Description": "I noticed problems during generating fetchlist. I already post some info at the users list. Today I check release 0.8 and I'm certain that problem is only in version later than this. I've do testnig only on 0.8 and svn from today.\nThe problem is that generator generate fetchlist from crawldb but everytime i run there is different number of urls in fetchlist.\nFor example I put 6 test urls we have for testing and only 5 of 20 test there were all urls listed in fetchlist, sometimes onyl one. Config was always the same also when testing at version 0.8.\nI try to debug what might go wrong but I only end up that in /tmp there were all urls but somehow missed in crawl_generate\nI also se some of \n2006-09-02 20:14:20,147 DEBUG conf.Configuration - java.io.IOException: config(config)\n        at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:76)\n        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:87)\n        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:98)\n        at org.apache.nutch.util.NutchJob.<init>(NutchJob.java:26)\n        at org.apache.nutch.crawl.Generator.generate(Generator.java:330)\n        at org.apache.nutch.crawl.Generator.run(Generator.java:405)\n        at org.apache.nutch.util.ToolBase.doMain(ToolBase.java:145)\n        at org.apache.nutch.crawl.Generator.main(Generator.java:372)\nif I enable DEBUG loging but I doubt that this has anything to do with this.",
        "Issue Links": [
            "/jira/browse/NUTCH-370"
        ]
    },
    "NUTCH-362": {
        "Key": "NUTCH-362",
        "Summary": "Remove parse-text from unsupported filetypes in parse-plugins.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "07/Sep/06 18:02",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "21/Nov/06 17:52",
        "Description": "Remove parse-text from following mime types:\n\n(default)\napplication/rss+xml\napplication/vnd.wap.wbxml\napplication/vnd.wap.wmlc\napplication/vnd.wap.wmlscriptc\napplication/xhtml+xml\napplication/x-latex\napplication/x-netcdf\napplication/x-tex\napplication/x-texinfo\napplication/x-troff\napplication/x-troff-man\napplication/x-troff-me\napplication/x-troff-ms\nmessage/news\nmessage/rfc822\ntext/css\ntext/sgml\ntext/vnd.wap.wml\ntext/xml\ntext/x-setext\n\nAdd parse-html to application/xhtml+xml",
        "Issue Links": [
            "/jira/browse/NUTCH-338"
        ]
    },
    "NUTCH-363": {
        "Key": "NUTCH-363",
        "Summary": "Fetcher normalizes everything at least twice",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "0.8",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Doug Cook",
        "Created": "08/Sep/06 18:47",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "26/Jun/10 05:57",
        "Description": "New links are normalized twice by the fetcher: \nFirst in DOMContentUtils.getOutlinks, where the constructor Outlink(url.toString(), linkText.toString().trim(), conf)  normalizes the URL.\nThe second time is in ParseOutputFormat.write().\nFor some URLs (e.g. those repeated on a page) a given URL may be normalized a number of times, but it is always normalized at least twice.\nFor those of us with expensive normalizations, this is probably burning some CPU. \nI'd gladly fix this, but I'm not yet familiar enough with the code to know if there are some hidden assumptions which rely on this behavior.\n[A related note is that URLs are normalized *before* filtering; this is causing a lot of extra normalization as well. In general, filters may not be safe to run before normalization, but there is likely a class of them which are (filtering out .gif/.jpg etc). Perhaps the notion of a \"pre-normalizer filter\" would be a useful one?]",
        "Issue Links": []
    },
    "NUTCH-364": {
        "Key": "NUTCH-364",
        "Summary": "Javascript parser creates some fairly bogus URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Doug Cook",
        "Created": "09/Sep/06 00:22",
        "Updated": "01/Apr/11 15:26",
        "Resolved": "01/Apr/11 15:26",
        "Description": "If one crawls, say, \nhttp://www.metropoleparis.com/2000/501/\nwith the Javascript parser enabled, one gets outlinks of the form:\n2006-09-08 16:55:06,301 DEBUG js.JSParseFilter -  - outlink from JS: 'http://www.metropoleparis.com/2000/501/</IFRAME>'\n2006-09-08 16:55:06,302 DEBUG js.JSParseFilter -  - outlink from JS: 'http://www.metropoleparis.com/2000/501/</SCR'\n2006-09-08 16:55:06,302 DEBUG js.JSParseFilter -  - outlink from JS: 'http://www.metropoleparis.com/2000/501/</DIV>'\nAnother example would be:\nhttp://www.wein-plus.de/glossar/G.htm\nwhich yields the URL (among others):\n2006-09-08 16:55:10,499 DEBUG js.JSParseFilter -  - outlink from JS: 'http://www.wein-plus.de/glossar/<\\/a>'\nI have seen these form \"crawler traps\" and make small sites explode to many, many URLs. For the moment, I have the worst offenders plugged with specific filter rules, but it would be nice to see if there is a way to improve the JSParseFilter's heuristics to avoid these.",
        "Issue Links": []
    },
    "NUTCH-365": {
        "Key": "NUTCH-365",
        "Summary": "Flexible URL normalization",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "09/Sep/06 13:21",
        "Updated": "22/Sep/06 21:02",
        "Resolved": "22/Sep/06 21:02",
        "Description": "This patch is a heavily restructured version of the patch in NUTCH-253, so much that I decided to create a separate issue. It changes the URL normalization from a selectable single class to a flexible and context-aware chain of normalization filters.\nHighlights:\n\nrename all UrlNormalizer to URLNormalizer for consistency.\n\n\nuse a \"chained filter\" pattern for running several normalizers in sequence\n\n\nthe order in which normalizers are executed is defined by \"urlnormalizer.order\" property, which lists space-separated implementation classes. If there are more normalizers active than explicitly named on this list, they will be run in random order after the ones specified on the list are executed.\n\n\ndefine a set of contexts (or scopes) in which normalizers may be called. Each scope can have its own list of normalizers (via \"urlnormalizer.scope.<scope_name>\" property) and its own order (via \"urlnormalizer.order.<scope_name>\" property). If any of these properties are missing, default settings are used.\n\n\neach normalizer may further select among many configurations, depending on the context in which it is called, using a modified API:\n\n   URLNormalizer.normalize(String url, String scope);\n\nif a config for a given scope is not defined, then the default config will be used.\n\n\nseveral standard contexts / scopes have been defined, and various applications have been modified to attempt using appropriate normalizer in their context.\n\n\nall JUnit tests have been modified, and run successfully.\n\nNUTCH-363 suggests to me that further changes may be required in this area, perhaps we should combine urlfilters and urlnormalizers into a single subsystem of url munging - now that we have support for scopes and flexible combinations of normalizers we could turn URLFilters into a special case of normalizers (or vice versa, depending on the point of view) ...",
        "Issue Links": []
    },
    "NUTCH-366": {
        "Key": "NUTCH-366",
        "Summary": "Merge URLFilters and URLNormalizers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "12/Sep/06 14:28",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Currently Nutch uses two subsystems related to url validation and normalization:\n\nURLFilter: this interface checks if URLs are valid for further processing. Input URL is not changed in any way. The output is a boolean value.\n\n\nURLNormalizer: this interface brings URLs to their base (\"normal\") form, or removes unneeded URL components, or performs any other URL mangling as necessary. Input URLs are changed, and are returned as result.\n\nHowever, various Nutch tools run filters and normalizers in pre-determined order, i.e. normalizers first, and then filters. In some cases, where normalizers are complex and running them is costly (e.g. numerous regex rules, DNS lookups) it would make sense to run some of the filters first (e.g. prefix-based filters that select only certain protocols, or suffix-based filters that select only known \"extensions\"). This is currently not possible - we always have to run normalizers, only to later throw away urls because they failed to pass through filters.\nI would like to solicit comments on the following two solutions, and work on implementation of one of them:\n1) we could make URLFilters and URLNormalizers implement the same interface, and basically make them interchangeable. This way users could configure their order arbitrarily, even mixing filters and normalizers out of order. This is more complicated, but gives much more flexibility - and NUTCH-365 already provides sufficient framework to implement this, including the ability to define different sequences for different steps in the workflow.\n2) we could use a property \"url.mangling.order\"  to define whether normalizers or filters should run first. This is simple to implement, but provides only limited improvement - because either all filters or all normalizers would run, they couldn't be mixed in arbitrary order.\nAny comments?",
        "Issue Links": []
    },
    "NUTCH-367": {
        "Key": "NUTCH-367",
        "Summary": "DistributedSearch thown ClassCastException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "emanihc",
        "Created": "12/Sep/06 14:58",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "19/Sep/06 19:34",
        "Description": "I check out directly from SVN to get the latest sources (revision 442599),\nrun \n\"bin/nutch server 12345 ../crawl/\"\n\"bin/nutch 'org.apache.nutch.searcher.DistributedSearch$Client' 'www' localhost 12345\"\nand then it thrown Exception below:\n========================================================\n$ bin/nutch 'org.apache.nutch.searcher.DistributedSearch$Client' \"www\" localhost 12345\nSTATS: 1 servers, 5 segments.\nTotal hits: 9\nException in thread \"main\" java.lang.ClassCastException: $Proxy0\n       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:241)\n       at org.apache.nutch.searcher.DistributedSearch$Client.getRemote\n(DistributedSearch.java:284)\n       at org.apache.nutch.searcher.DistributedSearch$Client.getDetails\n(DistributedSearch.java:299)\n       at org.apache.nutch.searcher.DistributedSearch$Client.main\n(DistributedSearch.java:377)\n=========================================================",
        "Issue Links": []
    },
    "NUTCH-368": {
        "Key": "NUTCH-368",
        "Summary": "Message queueing system",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "15/Sep/06 20:36",
        "Updated": "22/Jan/08 14:48",
        "Resolved": "22/Jan/08 14:48",
        "Description": "This is an implementation of a filesystem-based message queueing system. The motivation for this functionality is explained in HADOOP-490 - there is nothing Nutch-specific in this implementation, so if it's considered generally useful it could be moved there.\nBelow are excerpts from the included javadocs.\nThe model of the system is as follows:\n\napplications (including map-reduce jobs) may create their own separate message queueing area. Alternatively, they can specifically ask for a named message queue, belonging to a different application or existing as a system-wide queue. Message queues are created under \"/mq\" and then the message queue id (for map-reduce jobs this is a job id, or it can be any other name passed as job id to the constructor).\n      Please see the example for more information.\n\n\na single unit of information passing through queues is a Msg, which has a unique identifier (consisting of creation time and publisher name), string subject, and content (Writable).\n\n\nsingle MsgQueue in fact consists of any number of topics. There are four predefined ones: in, out, err, and ctrl.\n\n\nmessages are published to topics, which present a sequential view of messages, sorted by msgId (which corresponds to their order of arrival).\n\n\neach message queue may periodically poll for changes (MsgQueue.startPolling()), using a separate thread. Polling updates the list of topics and messages. Poll interval is configurable, and defaults to 5 sec.\n\n\neach detected change in the queue (add/remove topic, add/remove message) may be communicated to registered listeners. Out-of-band messages are not supported in this version, but it's not too complicated to add them. Applications can create listeners watching queues for newly added messages, or deleted messages, added topics or deleted topics, etc.\n\n\neach instance of MsgQueue using the same physical queue maintains its own view of the queue, keeping track of topics and messages that it considers \"processed and discarded\". In other words, multiple readers and creators may modify queues, and each knows which messages it already processed and which ones are new. In a similar fashion, instances may willfully \"remove\" certain topics from their view, even though these topics still physically exist and are available for other instances (and later on they can \"add\" them to their view again).\n      This somewhat complicated feature was implemented in order to support multiple readers for the same message (e.g. many tasks per one mapred job). Each task needs to register for the same queue, and if they didn't have their own views of the queue, messages would be consumed by the first task that got to them. As it is implemented now, each task may consume messages at its own pace. At the end of the job applications may elect to keep the queue around or to destroy it (and thus remove all topics and messages in it).\n\n\nmessages, topics and queues may be destroyed by any user, at which point they are physically removed from the filesystem. All users will gradually update their views, during the next poll operation.\n\n\nthere is a command-line tool to examine and modify queues, and also to retrieve and send simple text messages. You can run it like this:\n\n         bin/nutch org.apache.nutch.util.msg.MsgQueueTool ...many options...",
        "Issue Links": [
            "/jira/browse/MAPREDUCE-205"
        ]
    },
    "NUTCH-369": {
        "Key": "NUTCH-369",
        "Summary": "StringUtil.resolveEncodingAlias  is unuseful.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "King Kong",
        "Created": "18/Sep/06 10:23",
        "Updated": "27/Sep/07 17:38",
        "Resolved": "26/Sep/07 14:06",
        "Description": "After we defined encoding alias map in StringUtil , but parse html use orginal encoding also.\nI found it is reading charset from  meta in nekohtml which HtmlParser  used .\nwe can set it's feature \"http://cyberneko.org/html/features/scanner/ignore-specified-charset\" to true \nthat nekohtml will use encoding we set;\nconcretely,\n  private DocumentFragment parseNeko(InputSource input) throws Exception {\n    DOMFragmentParser parser = new DOMFragmentParser();\n    // some plugins, e.g., creativecommons, need to examine html comments\n    try {\n   + parser.setFeature(\"http://cyberneko.org/html/features/scanner/ignore-specified-charset\",true);\n      parser.setFeature(\"http://apache.org/xml/features/include-comments\", \n              true);\n      ....\nBTW, It must be add on front of try block,because the following sentence  (parser.setFeature(\"http://apache.org/xml/features/include-comments\", \n              true) will throw exception.",
        "Issue Links": []
    },
    "NUTCH-370": {
        "Key": "NUTCH-370",
        "Summary": "Generator looses urls when run with LocalJobRunner",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "22/Sep/06 17:01",
        "Updated": "24/Sep/06 16:41",
        "Resolved": "24/Sep/06 16:41",
        "Description": "When generator is run with LocalJobRunner part of generated urls get lost. This is because two map outputs are created and only one of them is processed in reduce phase.\nWhen -numFetchers 1 is provided as command line parameters problem goes away.",
        "Issue Links": [
            "/jira/browse/NUTCH-361"
        ]
    },
    "NUTCH-371": {
        "Key": "NUTCH-371",
        "Summary": "DeleteDuplicates should remove documents with duplicate URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Chris Schneider",
        "Created": "25/Sep/06 16:14",
        "Updated": "16/Oct/06 21:16",
        "Resolved": "16/Oct/06 21:16",
        "Description": "DeleteDuplicates is supposed to delete documents with duplicate URLs (after deleting documents with identical MD5 hashes), but this part is apparently not yet implemented. Here's the comment from DeleteDuplicates.java:\n// 2. map indexes -> <<url, fetchdate>, <index,doc>>\n// partition by url\n// reduce, deleting all but most recent.\n//\n// Part 2 is not yet implemented, but the Indexer currently only indexes one\n// URL per page, so this is not a critical problem.\nIt is apparently also known that re-fetching the same URL (e.g., one month later) will result in more than one document with the same URL (this is alluded to in NUTCH-95), but the comment above suggests that the indexer will solve the problem before DeleteDuplicates, because it will only index one document per URL.\nThis is not necessarily the case if the segments are to be divided among search servers, as each server will have its own index built from its own portion of the segments. Thus, if the URL in question was fetched in different segments, and these segments end up assigned to different search servers, then the indexer can't be relied on to eliminate the duplicates.\nThus, it seems like the second part of the DeleteDuplicates algorithm (i.e., deleting documents with duplicate URLs) needs to be implemented. I agree with Byron and Andrzej that the most recently fetched document (rather than the one with the highest score) should be preserved.\nFinally, it's also possible to get duplicate URLs in the segments without re-fetching an expired URL in the crawldb. This can happen if 3 different URLs all redirect to the target URL. This is yet another consequence of handling redirections immediately, rather than adding the target URL to the crawldb for fetching in some subsequent segment (see NUTCH-273).",
        "Issue Links": [
            "/jira/browse/NUTCH-273",
            "/jira/browse/NUTCH-95",
            "/jira/browse/NUTCH-380"
        ]
    },
    "NUTCH-372": {
        "Key": "NUTCH-372",
        "Summary": "Fetcher halting and throttling",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "26/Sep/06 09:47",
        "Updated": "26/Sep/06 09:57",
        "Resolved": "26/Sep/06 09:57",
        "Description": "This patch uses the message queueing framework to implement the following functionality:\n\nability to gracefully stop fetching the current segment. This is different from simply killing the job in that the partial results (partially fetched segment) are available and can be further processed. This is especially useful for fetching large segments with long \"tails\", i.e. pages which are fetched very slowly, either because of politeness settings or the target site's bandwidth limitations.\n\n\nability to dynamicaly adjust the number of fetcher threads. For a long-running fetch job it makes sense to decrease the number of fetcher threads during the day, and increase it during the night. This can be done now with a cron script, using the MsgQueueTool command-line.\n\nIt's worthwhile to note that the patch itself is trivial, and most of the work is done by the MQ framework.",
        "Issue Links": []
    },
    "NUTCH-373": {
        "Key": "NUTCH-373",
        "Summary": "Fetcher halting and throttling",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "26/Sep/06 09:48",
        "Updated": "26/Sep/06 09:57",
        "Resolved": "26/Sep/06 09:57",
        "Description": "This patch uses the message queueing framework to implement the following functionality:\n\nability to gracefully stop fetching the current segment. This is different from simply killing the job in that the partial results (partially fetched segment) are available and can be further processed. This is especially useful for fetching large segments with long \"tails\", i.e. pages which are fetched very slowly, either because of politeness settings or the target site's bandwidth limitations.\n\n\nability to dynamicaly adjust the number of fetcher threads. For a long-running fetch job it makes sense to decrease the number of fetcher threads during the day, and increase it during the night. This can be done now with a cron script, using the MsgQueueTool command-line.\n\nIt's worthwhile to note that the patch itself is trivial, and most of the work is done by the MQ framework.",
        "Issue Links": []
    },
    "NUTCH-374": {
        "Key": "NUTCH-374",
        "Summary": "when http.content.limit be set to -1 and  Response.CONTENT_ENCODING  is gzip or x-gzip  , it can not fetch any thing.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Piotr Kosiorowski",
        "Reporter": "King Kong",
        "Created": "27/Sep/06 17:32",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "30/Sep/06 19:38",
        "Description": "I set \"http.content.limit\"  to -1 to not truncate content being fetched.\nHowever , if  response used gzip or x-gzip , then it was not able to uncompress.\nI found the problem is in HttpBase.processGzipEncoded  (plugin lib-http) \n  ...\n   byte[] content = GZIPUtils.unzipBestEffort(compressed, getMaxContent());\n   ...\nbecause it is not  deal with -1 to no limit , so must modify code to solve it;\n    byte[] content;\n    if (getMaxContent()>=0)\n{\n        content = GZIPUtils.unzipBestEffort(compressed, getMaxContent());\n    }\nelse\n{\n    \tcontent = GZIPUtils.unzipBestEffort(compressed);\n    }",
        "Issue Links": []
    },
    "NUTCH-375": {
        "Key": "NUTCH-375",
        "Summary": "Link to 0.8.x apidocs broken on website",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "29/Sep/06 03:24",
        "Updated": "23/Sep/08 04:18",
        "Resolved": "29/Sep/06 03:26",
        "Description": ">It looks like the link from the Nutch Project Homepage to the API Docs\n>for 0.8.x is broken.",
        "Issue Links": []
    },
    "NUTCH-376": {
        "Key": "NUTCH-376",
        "Summary": "Add methods to control runtime behaviour of NutchBean",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "30/Sep/06 19:44",
        "Updated": "13/Apr/11 23:36",
        "Resolved": "13/Apr/11 23:36",
        "Description": "Currently in order to activate new index on SearchServer one needs to stop + start the searchServer (or tomcat/webapp if running in non distributed mode). To ease up this maintenance(scriting) burden we should add new methods to api so that we can control the behaviour while searchserver / nutchbean is \"online\".\nI propose we add atleast the following method to api:\npublic void switchIndex(String dir)\nThis command will close current index and open new one from specified location\nWe could even create a small admin web-ui to control this behavior via browser.",
        "Issue Links": []
    },
    "NUTCH-377": {
        "Key": "NUTCH-377",
        "Summary": "Add possibility to search for multiple values",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stefan Neufeind",
        "Created": "01/Oct/06 14:01",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "Searches with boolean operators (AND or OR) are not (yet) possible. All search-items are always searched with AND.\nBut it would be nice to have the possibility to allow multiple values for a certain field. Maybe that could done using a separator?\nAs an example you might want to search for:\nsomeword    site:www.example.org|www.apache.org\nWhich (to my understand) would allow to search for one or more words with a restriction to those two sites. It would prevent having to implement AND and OR fully (maybe even including brackets) but would allow to cover a few often used cases imho.\nEasy/hard to do? To my understanding Lucene itself allows AND/OR-searches. So might basically be a problem of string-parsing and query-building towards Lucene?",
        "Issue Links": []
    },
    "NUTCH-378": {
        "Key": "NUTCH-378",
        "Summary": "MetaWrapper decorator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "03/Oct/06 19:46",
        "Updated": "14/Nov/06 19:47",
        "Resolved": "14/Nov/06 19:47",
        "Description": "First, a bit of background.\nCurrently some tools (Indexer, SegmentMerger, CrawlDbReducer) use ObjectWritable to pass data from different parts of segment(s) to map-reduce methods. However, there is a high risk that this data is processed incorrectly, because map-reduce methods no longer know the exact source of any given data item.\nExample: Indexer may process many segments at the same time. In its reduce() method it receives a set of values coming from different parts of the segment, but found at the same key (url). However, if the same page is fetched multiple times, Indexer will receive multiple sets of values from different segments (e.g. multiple fetchDatum, parseData, etc). It may happen that some of this data items it picks up for further processing belong to one set, and some other data to another, resulting in the final set that is a hodge-podge of partial data coming from different segments. This could be avoided if each value had metadata to mark it as belonging to a particular segment. Indexer could then collect all complete multiple sets, and then select the most recent one for further processing.\nSimilar situation occurs in SegmentMerger, where data coming from different segments is tagged with its source. However, ParseText class doesn't support any metadata, so its text has to be changed to contain the tag. This is unwieldy and far from elegant.\nA different problem occurs in CrawlDbReducer - we have instances of the same class, but it's sometimes difficult to determine where they originally came from. This also limits us to update CrawlDb from 1 segment at a time, otherwise CrawlDatum instances from earlier segments would be indistinguishable from those from newer segments... In short, the functionality and internal logic here could be vastly improved if we knew where any CrawlDatum instance came from.\nThe attached class provides this functionality - instead of using ObjectWritable (or plain CrawlDatum) we can wrap instances of input data in MetaWritable, and add necessary metadata that will support the processing at hand. Then in map-reduce methods we can unpack original values, and use additional metadata.\nNote: this wrapping/unwrapping is aplied only during map-reduce jobs - data stored in DBs and segments would remain the same.",
        "Issue Links": []
    },
    "NUTCH-379": {
        "Key": "NUTCH-379",
        "Summary": "ParseUtil does not pass through the content's URL to the ParserFactory",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "0.8.2,                                            0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "04/Oct/06 16:01",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "24/Oct/06 15:28",
        "Description": "Currently the ParseUtil class that is called by the Fetcher to actually perform the parsing of content does not forward thorugh the content's url for use in the ParserFactory. A bigger issue, however, is that the url (and for that matter, the pathSuffix) is no longer used to determine which parsing plugin should be called. My colleague at JPL discovered that more major bug and will soon input a JIRA issue for it. However, in the meantime, this small patch at least sets up the forwarding of the content's URL to the ParserFactory.",
        "Issue Links": []
    },
    "NUTCH-380": {
        "Key": "NUTCH-380",
        "Summary": "Nutch does not run/build against Hadoop 0.6",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8.1,                                            0.8.2,                                            0.9.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Jim Kellerman",
        "Created": "04/Oct/06 19:12",
        "Updated": "21/Nov/06 17:32",
        "Resolved": "21/Nov/06 17:32",
        "Description": "org.apache.nutch.indexer.DeleteDuplicates fails to build against Hadoop-0.6.2\nSpecifically, the interface org.apache.hadoop.mapred.RecordReader has two new methods:\nWritableComparable createKey();\nWritable createValue();\nWhich are not implemented by the anonymous inner class declared starting on line 155 of DeleteDuplicates.java 438670:\n149:    public RecordReader getRecordReader(final FileSystem fs,\n150:                                        final FileSplit split,\n151:                                        final JobConf job,\n152:                                        Reporter reporter) throws IOException {\n153:      final UTF8 index = new UTF8(split.getPath().toString());\n154:      reporter.setStatus(index.toString());\n155:      return new RecordReader() {",
        "Issue Links": [
            "/jira/browse/NUTCH-371"
        ]
    },
    "NUTCH-381": {
        "Key": "NUTCH-381",
        "Summary": "Ignore external link not work as expected",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Uros Gruber",
        "Created": "05/Oct/06 19:34",
        "Updated": "19/Mar/07 23:43",
        "Resolved": "19/Mar/07 23:43",
        "Description": "Currently there is no way to properly limit fetcher without regexp rules we use ignore.external.link option but It seams that It doesn't work in all cases.\nHere is example urls I'm seeing but\ncat urls1 urls2 urls3 urls/urls |grep yahoo.com doesn't return any hit. \nfetching http://help.yahoo.com/help/sports\nfetching http://www.turkish-xxx.com/adult-traffic-trade.php\nfetching http://help.yahoo.com/help/us/astr/\nfetching http://www.polish-xxx.com/de-index.html\nfetching http://www.driversplanet.com/Articles/Software/SpareBackup2.4.aspx\nfetching http://help.yahoo.com/help/groups\nfetching http://help.yahoo.com/help/fin/\nfetching http://www.driversplanet.com/Articles/Software/WindowsStorageServer2003R2.aspx\nfetching http://help.yahoo.com/help/us/edit/\nfetching http://www.polish-xxx.com/es-index.html\nAnyone notice this?\nI assume that there must be something with expired domains where pages generates randomly. But still why urls from other domain was added. Maybe urlregexp filter +* exclude.",
        "Issue Links": []
    },
    "NUTCH-382": {
        "Key": "NUTCH-382",
        "Summary": "Fix for NUTCH-365 introduced a bug if generate.max.per.host.by.ip is enabled",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Jim Kellerman",
        "Created": "10/Oct/06 02:12",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "06/Feb/08 12:53",
        "Description": "The fix for NUTCH-365 in org.apache.nutch.crawl.Generator.java (revision 449088) introduced a bug in which if generate.max.per.host.by.ip is enabled, the error message \"WARN  crawl.Generator (Generator.java:reduce(181)) - Malformed URL: '38.99.15.82', skipping\". The message varies according to the host IP.\nThis is because the hostname has already been converted to its IP address, and the code:\n              host = normalizers.normalize(host, URLNormalizers.SCOPE_GENERATE_HOST_COUNT);\nwill not normalize an IP address. What is needed to fix this this problem is to include the code inserted in revision 449088 inside an else block so that this code is not executed if generate.max.per.host.by.ip is enabled.",
        "Issue Links": []
    },
    "NUTCH-383": {
        "Key": "NUTCH-383",
        "Summary": "Upgrade Nutch to Hadoop 0.7",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "10/Oct/06 19:26",
        "Updated": "16/Oct/06 21:17",
        "Resolved": "16/Oct/06 21:17",
        "Description": "Upgrade Nutch to Hadoop 0.7, and replace all occurences of UTF8 with Text. UTF8 is deprecated and its use is discouraged due to its limitations.\nThis change will break API, in the sense that all third-party additions will have to be updated to use new APIs that use Text instead of UTF8 in method parameters.\nThis change also breaks backward compatibility of data in CrawlDb, LinkDb and segments. A tool to upgrade CrawlDb, LinkDb and segments can be created to facilitate the upgrade path.",
        "Issue Links": []
    },
    "NUTCH-384": {
        "Key": "NUTCH-384",
        "Summary": "Protocol-file plugin does not allow the parse plugins framework to operate properly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Paul Ramirez",
        "Created": "11/Oct/06 16:54",
        "Updated": "10/Mar/07 06:53",
        "Resolved": "10/Mar/07 06:49",
        "Description": "When using the file protocol one can not map a parse plugin to a content type. The only way to get the plugin called is through the default plugin. The issue is that the content type never gets mapped. Currently the content type does not get set by the file protocol.",
        "Issue Links": []
    },
    "NUTCH-385": {
        "Key": "NUTCH-385",
        "Summary": "Improve description of thread related configuration for Fetcher",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "documentation,                                            fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Chris Schneider",
        "Created": "11/Oct/06 18:39",
        "Updated": "27/Jun/14 08:45",
        "Resolved": "27/Jun/14 07:49",
        "Description": "For some time I've been puzzled by the interaction between two paramters that control how often the fetcher can access a particular host:\n1) The server delay, which comes back from the remote server during our processing of the robots.txt file, and which can be limited by fetcher.max.crawl.delay.\n2) The fetcher.threads.per.host value, particularly when this is greater than the default of 1.\nAccording to my (limited) understanding of the code in HttpBase.java:\nSuppose that fetcher.threads.per.host is 2, and that (by chance) the fetcher ends up keeping either 1 or 2 fetcher threads pointing at a particular host continuously. In other words, it never tries to point 3 at the host, and it always points a second thread at the host before the first thread finishes accessing it. Since HttpBase.unblockAddr never gets called with (((Integer)THREADS_PER_HOST_COUNT.get(host)).intValue() == 1), it never puts System.currentTimeMillis() + crawlDelay into BLOCKED_ADDR_TO_TIME for the host. Thus, the server delay will never be used at all. The fetcher will be continuously retrieving pages from the host, often with 2 fetchers accessing the host simultaneously.\nSuppose instead that the fetcher finally does allow the last thread to complete before it gets around to pointing another thread at the target host. When the last fetcher thread calls HttpBase.unblockAddr, it will now put System.currentTimeMillis() + crawlDelay into BLOCKED_ADDR_TO_TIME for the host. This, in turn, will prevent any threads from accessing this host until the delay is complete, even though zero threads are currently accessing the host.\nI see this behavior as inconsistent. More importantly, the current implementation certainly doesn't seem to answer my original question about appropriate definitions for what appear to be conflicting parameters. \nIn a nutshell, how could we possibly honor the server delay if we allow more than one fetcher thread to simultaneously access the host?\nIt would be one thing if whenever (fetcher.threads.per.host > 1), this trumped the server delay, causing the latter to be ignored completely. That is certainly not the case in the current implementation, as it will wait for server delay whenever the number of threads accessing a given host drops to zero.",
        "Issue Links": []
    },
    "NUTCH-386": {
        "Key": "NUTCH-386",
        "Summary": "Plugin to index categories by url rules",
        "Type": "New Feature",
        "Status": "Reopened",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Ernesto De Santis",
        "Created": "14/Oct/06 20:49",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The compressed zip has a install_notes.txt file with instructions.",
        "Issue Links": []
    },
    "NUTCH-387": {
        "Key": "NUTCH-387",
        "Summary": "host normalization in Generator$Selector",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "generator",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Johannes Zillmann",
        "Created": "18/Oct/06 09:40",
        "Updated": "03/Nov/06 11:55",
        "Resolved": "03/Nov/06 11:55",
        "Description": "the host normalization in Generator$Selector#reduce at line 177 seems broken:\nString host = new URL(url.toString()).getHost();\n...\ntry \n{\n            host = normalizers.normalize(host, URLNormalizers.SCOPE_GENERATE_HOST_COUNT);\n            host = new URL(host).getHost().toLowerCase();\n }\n catch (Exception e) \n{\n       LOG.warn(\"Malformed URL: '\" + host + \"', skipping\");\n }\n\nWith default configuration the basic nomalizer will be called, which is doing 'new URL(host)'.\nAlso in line below 'new URL(host)' will be called.\nSince url.getHost() always return the host without protocol, there will be a MalformedUrlException be thrown, always.\nThe job will continue as usual though, cause the exception is catched.",
        "Issue Links": []
    },
    "NUTCH-388": {
        "Key": "NUTCH-388",
        "Summary": "nutch-default.xml has outdated example for urlfilter.order",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1",
        "Fix Version/s": "0.9.0",
        "Component/s": "documentation,                                            fetcher",
        "Assignee": null,
        "Reporter": "Jared Dunne",
        "Created": "18/Oct/06 19:50",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "18/Nov/06 21:58",
        "Description": "The description for the nutch-default.xml entry for urlfilter.order is misleading/outdated.  In the example it refers to org.apache.nutch.net.RegexURLFilter & org.apache.nutch.net.PrefixURLFilter, when it should refer to org.apache.nutch.urlfilter.regex.RegexURLFilter & org.apache.nutch.urlfilter.prefix.PrefixURLFilter.\n<property>\n  <name>urlfilter.order</name>\n  <value></value>\n  <description>The order by which url filters are applied.\n  If empty, all available url filters (as dictated by properties\n  plugin-includes and plugin-excludes above) are loaded and applied in system\n  defined order. If not empty, only named filters are loaded and applied\n  in given order. For example, if this property has value:\n  org.apache.nutch.net.RegexURLFilter org.apache.nutch.net.PrefixURLFilter\n  then RegexURLFilter is applied first, and PrefixURLFilter second.\n  Since all filters are AND'ed, filter ordering does not have impact\n  on end result, but it may have performance implication, depending\n  on relative expensiveness of filters.\n  </description>\n</property>\nWe wanted to run prefix before regex so we copied the example from the description and reversed it.  Since these package names are incorrect, it did not work and the following warnings appeared in our logs for each of the URLs in our fetchlist:\n2006-10-17 15:55:46,533 WARN  crawl.Injector - Skipping http://bar.foo.com/:java.lang.NullPointerException \n2006-10-17 15:55:46,533 WARN  crawl.Injector - Skipping http://baz.foo.com/:java.lang.NullPointerException",
        "Issue Links": []
    },
    "NUTCH-389": {
        "Key": "NUTCH-389",
        "Summary": "a url tokenizer implementation for tokenizing index fields : url and host",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Enis Soztutar",
        "Created": "20/Oct/06 08:45",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "NutchAnalysis.jj tokenizes the input by threating & and _ as non token seperators, which is in the case of the urls not appropriate. So i have written a url tokenizer which the tokens that match the regular exp [a-zA-Z0-9]. As stated in http://www.gbiv.com/protocols/uri/rfc/rfc3986.html which describes the grammer for URIs, URL's can be tokenized with the above expression. \nNutchDocumentAnalyzer code is modified to use the UrlTokenizer with the \"url\", \"site\" and \"host\" fields.\nsee : http://www.mail-archive.com/nutch-user@lucene.apache.org/msg06247.html",
        "Issue Links": []
    },
    "NUTCH-390": {
        "Key": "NUTCH-390",
        "Summary": "Javadoc warnings",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "nutch.newbie",
        "Created": "24/Oct/06 13:58",
        "Updated": "30/Jan/07 05:58",
        "Resolved": "30/Jan/07 05:57",
        "Description": "Warnings aren't so good even if it build's successfully.\n [javadoc] Building tree for all the packages and classes...\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/net/URLNormalizers.java:74: warning - Tag @link: reference not found: org.apache.nutch.net.urlnormalizer.pass.PassURLNormalizer\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/net/URLNormalizers.java:273: warning - @return tag has no arguments.\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/plugin/PluginManifestParser.java:73: warning - @return tag has no arguments.\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/protocol/ProtocolFactory.java:37: warning - Missing closing '}' character for inline tag: \"{@link Protocol#X_POINT_ID).\"\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/protocol/ProtocolFactory.java:64: warning - @return tag has no arguments.\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/scoring/ScoringFilter.java:139: warning - Tag @param cannot be used in inline documentation.  It can only be used in the following types of documentation: class/interface, constructor, method.\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/scoring/ScoringFilter.java:139: warning - Tag @param cannot be used in inline documentation.  It can only be used in the following types of documentation: class/interface, constructor, method.\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/util/DomUtil.java:51: warning - @return tag has no arguments.\n[javadoc] /usr/local/src/nutch-0.9/src/java/org/apache/nutch/scoring/ScoringFilter.java:139: warning - Tag @param cannot be used in inline documentation.  It can only be used in the following types of documentation: class/interface, constructor, method.\n[javadoc] Building index for all the packages and classes...\n[javadoc] Building index for all classes...\n[javadoc] Generating /usr/local/src/nutch-0.9/build/docs/api/stylesheet.css...\n[javadoc] 10 warnings\n[copy] Copying 1 file to /usr/local/src/nutch-0.9/build/docs/api/org/apache/nutch/plugin/doc-files",
        "Issue Links": []
    },
    "NUTCH-391": {
        "Key": "NUTCH-391",
        "Summary": "ParseUtil logs file contents to log file when it cannot find parser",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "0.8.2,                                            0.9.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "24/Oct/06 14:20",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "24/Oct/06 14:28",
        "Description": "ParseUtil currently logs file contents to a log file when it cannot find suitable parser. This sould be changed to prevent log file to be polluted.",
        "Issue Links": []
    },
    "NUTCH-392": {
        "Key": "NUTCH-392",
        "Summary": "OutputFormat implementations should pass on Progressable",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Doug Cutting",
        "Created": "25/Oct/06 17:42",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "31/May/07 21:24",
        "Description": "OutputFormat implementations should pass the Progressable they are passed to underlying SequenceFile implementations.  This will keep reduce tasks from timing out when block writes are slow.  This issue depends on http://issues.apache.org/jira/browse/HADOOP-636.",
        "Issue Links": []
    },
    "NUTCH-393": {
        "Key": "NUTCH-393",
        "Summary": "Indexer doesn't handle null documents returned by filters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1,                                            0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Eelco Lempsink",
        "Created": "27/Oct/06 07:16",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "09/May/07 19:37",
        "Description": "Plugins (like IndexingFilter) may return a null value, but this isn't handled by the Indexer.  A trivial adjustment is all it takes:\n@@ -237,6 +237,7 @@\n       if (LOG.isWarnEnabled()) \n{ LOG.warn(\"Error indexing \"+key+\": \"+e); }\n       return;\n     }\n+    if (doc == null) return;\n     float boost = 1.0f;\n     // run scoring filters",
        "Issue Links": []
    },
    "NUTCH-394": {
        "Key": "NUTCH-394",
        "Summary": "Searching via Tomcat / nutch-0.9-dev.war raises exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.8.2,                                            0.9.0",
        "Component/s": "web gui",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Jason Trost",
        "Created": "27/Oct/06 20:01",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "28/Oct/06 10:33",
        "Description": "When you run a search, an exception is raised.  This is the line that is causing it:\nin NutchBean.java\nthis.linkDb = new LinkDbInlinks(fs, linkDb, this.conf);\nSetting that line to\nthis.linkDb = null;\nallows the search to continue.\n2006-10-26 17:43:06,422 INFO  NutchBean - creating new bean\n2006-10-26 17:43:06,454 INFO  NutchBean - opening indexes in crawl/indexes\n2006-10-26 17:43:06,586 INFO  Configuration - found resource common-terms.utf8 at file:/home/nutch/downloads/apache-tomcat-5.5.20/webapps/nutch-0.9-dev/WEB-INF/classes/common-terms.utf8\n2006-10-26 17:43:06,610 INFO  NutchBean - opening segments in crawl/segments\n2006-10-26 17:43:06,647 INFO  SummarizerFactory - Using the first summarizer extension found: Basic Summarizer\n2006-10-26 17:43:06,647 INFO  NutchBean - opening linkdb in crawl/linkdb\n2006-10-26 17:43:06,662 ERROR [jsp] - Servlet.service() for servlet jsp threw exception\njava.lang.NoClassDefFoundError: org/apache/commons/cli/ParseException\n        at org.apache.nutch.searcher.LinkDbInlinks.<init>(LinkDbInlinks.java:26)\n        at org.apache.nutch.searcher.NutchBean.init(NutchBean.java:155)\n        at org.apache.nutch.searcher.NutchBean.<init>(NutchBean.java:105)\n        at org.apache.nutch.searcher.NutchBean.<init>(NutchBean.java:83)\n        at org.apache.nutch.searcher.NutchBean.get(NutchBean.java:70)\n        at org.apache.jsp.search_jsp._jspService(search_jsp.java:104)\n        at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:334)\n        at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)\n        at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)\n        at javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n        at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:252)\n        at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:173)\n        at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:213)\n        at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:178)\n        at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:126)\n        at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:105)\n        at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:107)\n        at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:148)\n        at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:869)\n        at org.apache.coyote.http11.Http11BaseProtocol$Http11ConnectionHandler.processConnection(Http11BaseProtocol.java:664)\n        at org.apache.tomcat.util.net.PoolTcpEndpoint.processSocket(PoolTcpEndpoint.java:527)\n        at org.apache.tomcat.util.net.LeaderFollowerWorkerThread.runIt(LeaderFollowerWorkerThread.java:80)\n        at org.apache.tomcat.util.threads.ThreadPool$ControlRunnable.run(ThreadPool.java:684)\n        at java.lang.Thread.run(Thread.java:595)",
        "Issue Links": []
    },
    "NUTCH-395": {
        "Key": "NUTCH-395",
        "Summary": "Increase fetching speed",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1,                                            0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "29/Oct/06 20:40",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "13/Nov/06 19:48",
        "Description": "There have been some discussion on nutch mailing lists about fetcher being slow, this patch tried to address that. the patch is just a quich hack and needs some cleaning up, it also currently applies to 0.8 branch and not trunk and it has also not been tested in large. What it changes?\nMetadata - the original metadata uses spellchecking, new version does not (a decorator is provided that can do it and it should perhaps be used where http headers are handled but in most of the cases the functionality is not required)\nReading/writing various data structures - patch tries to do io more efficiently see the patch for details.\nInitial benchmark:\nA small benchmark was done to measure the performance of changes with a script that basically does the following:\n-inject a list of urls into a fresh crawldb\n-create fetchlist (10k urls pointing to local filesystem)\n-fetch\n-updatedb\noriginal code from 0.8-branch:\nreal    10m51.907s\nuser    10m9.914s\nsys     0m21.285s\nafter applying the patch\nreal    4m15.313s\nuser    3m42.598s\nsys     0m18.485s",
        "Issue Links": [
            "/jira/browse/NUTCH-398"
        ]
    },
    "NUTCH-396": {
        "Key": "NUTCH-396",
        "Summary": "mergesegs sorts URLs, making segments useless for subsequent fetch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Doug Cook",
        "Created": "03/Nov/06 23:07",
        "Updated": "01/Apr/11 14:40",
        "Resolved": "01/Apr/11 14:40",
        "Description": "Mergesegs leaves the output segment in URL-sorted order.\nThis is a problem if the segment was just generated and not yet fetched - the fetcher likes the URLs to be in essentially random order (sort by URL hash or similar). If I fetch a segment created by mergesegs, my performance is extremely poor since all URLs from a given host will be grouped together and the per-host delays kill me.\nI have a local fix which I am using: map using a key of MD5(URL) + URL, then, during the reduce phase, chop the MD5 off the front to get the original URL. This is simple, has essentially random order, no problems with collisions, and seems to work nicely.\nThe only thing I don't know is whether or not there is some other tool expecting the sorted order (I would expect not, since generate does not produce this). Right now I have my fix as an option (-randomize), but if there is no other tool requiring sorted order, it's probably cleaner to just make this non-optional.\nThoughts?",
        "Issue Links": []
    },
    "NUTCH-397": {
        "Key": "NUTCH-397",
        "Summary": "porting clustering-carrot2 plugin to carrot2 v2.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "07/Nov/06 16:19",
        "Updated": "22/Aug/07 14:27",
        "Resolved": "22/Aug/07 14:27",
        "Description": "A rather trivial port of clustering-carrot2 to new carrot2. I also added the necessary jars for Polish, so that nutch will not give the annoying exceptions when it is initializing clustering-carrot2. \nThere is a small problem, though. AFAICS, a small patch has to be applied to carrot2, otherwise nutch can not start the plugin. (I am also attaching that here.)",
        "Issue Links": []
    },
    "NUTCH-398": {
        "Key": "NUTCH-398",
        "Summary": "map-reduce very slow when crawling on single server",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "AJ Chen",
        "Created": "08/Nov/06 00:27",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Dec/08 13:48",
        "Description": "This seems a bug and so I create a ticket here. I'm using nutch 0.9-dev to crawl web on one linux server. With default hadoop\nconfiguration (local file system, no distributed crawling), the Generator and Fetcher spend unproportional amount of time on map-reduce opearations. For example:\n2006-11-01 20:32:44,074 INFO  crawl.Generator - Generator: segment:\ncrawl/segments/20061101203244\n... (doing map and reduce for 2 hours )\n2006-11-01 22:28:11,102 INFO  fetcher.Fetcher - Fetcher: segment:\ncrawl/segments/20061101203244\n... (fetching 12 hours )\n2006-11-02 11:15:10,590 INFO  mapred.LocalJobRunner - 175383 pages, 16583\nerrors, 3.8 pages/s, 687 kb/s,\n2006-11-02 11:17:24,039 INFO  mapred.LocalJobRunner - reduce > sort\n... (but doing reduce>sort and reduce>duce for 8 hours )\n2006-11-02 19:13:38,882 INFO  crawl.CrawlDb - CrawlDb update: segment:\ncrawl/segments/20061101203244\nSince it's crawling on a single machine, such slow map-reduce opearation is not expected.",
        "Issue Links": [
            "/jira/browse/NUTCH-395"
        ]
    },
    "NUTCH-399": {
        "Key": "NUTCH-399",
        "Summary": "Change CommandRunner to use concurrent api from jdk",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "11/Nov/06 15:23",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "11/Nov/06 15:28",
        "Description": "Change CommandRunner to use java.util.concurrent instead of EDU.oswego.cs.dl.util.concurrent. Remove file lib/concurrent-1.3.4.jar.",
        "Issue Links": []
    },
    "NUTCH-400": {
        "Key": "NUTCH-400",
        "Summary": "Update & add missing license headers",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.2,                                            0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "12/Nov/06 00:09",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "04/Mar/07 07:35",
        "Description": "All Apache releases after November 1st must conform to the new header guidelines, documented at:\nhttp://www.apache.org/legal/src-headers.html",
        "Issue Links": []
    },
    "NUTCH-401": {
        "Key": "NUTCH-401",
        "Summary": "Hardcoded /tmp directory in SegmentReader",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.2,                                            0.9.0",
        "Fix Version/s": "0.8.2,                                            0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rod Taylor",
        "Created": "13/Nov/06 19:35",
        "Updated": "14/Nov/06 12:25",
        "Resolved": "14/Nov/06 12:25",
        "Description": "Shouldn't this directory be configurable? I found it because of permission issues (/tmp isn't globally writable to catch stuff like this) but it seems that the Nutch Temp directory should be used instead.\n    Path tempDir = new Path(\"/tmp/segread-\" + new java.util.Random().nextInt());\n    fs.delete(tempDir);",
        "Issue Links": []
    },
    "NUTCH-402": {
        "Key": "NUTCH-402",
        "Summary": "Incrementalcrawling and indexing",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.7.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Arun Kumar",
        "Created": "16/Nov/06 04:53",
        "Updated": "22/Sep/08 16:11",
        "Resolved": "22/Sep/08 16:11",
        "Description": "Incrementalcrawling and indexing.... Write in details about later.....................",
        "Issue Links": []
    },
    "NUTCH-403": {
        "Key": "NUTCH-403",
        "Summary": "Make URL filtering optional in Generator",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "generator",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "18/Nov/06 21:35",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "19/Nov/06 18:49",
        "Description": "As of revision 384219 Generator has used url filtering to filter out urls when generating fetchlists. For a usecase where unwanted urls are filtered out before they enter crawldb filtering in Generator is not required and just consumes resources unneccessarily.",
        "Issue Links": []
    },
    "NUTCH-404": {
        "Key": "NUTCH-404",
        "Summary": "Fix LinkDB Usage - implementation mismatch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "linkdb",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "19/Nov/06 12:53",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "19/Nov/06 12:56",
        "Description": "LinkDB usage text does not match implementation: -noFiltering vs -noFilter, -noNormalizing vs -noNormalize",
        "Issue Links": []
    },
    "NUTCH-405": {
        "Key": "NUTCH-405",
        "Summary": "Content object is not properly initialized in map method of ParseSegment",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "21/Nov/06 17:17",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "21/Nov/06 17:20",
        "Description": "When unparsed segments are parsed the Content object is not properly initialized in map method. (This was a result of recent modifications to Content Object in NUTCH-395).",
        "Issue Links": []
    },
    "NUTCH-406": {
        "Key": "NUTCH-406",
        "Summary": "Metadata tries to write null values",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Dogacan Guney",
        "Created": "23/Nov/06 13:25",
        "Updated": "23/Nov/06 17:19",
        "Resolved": "23/Nov/06 17:17",
        "Description": "During parsing, some urls (especially pdfs, it seems) may create <some_key, null> pairs in ParseData's parseMeta. \nWhen Metadata.write() tries to write such a pair, it causes an NPE.\nStack trace will be something like this:\n        at org.apache.hadoop.io.Text.encode(Text.java:373)\n        at org.apache.hadoop.io.Text.encode(Text.java:354)\n        at org.apache.hadoop.io.Text.writeString(Text.java:394)\n        at org.apache.nutch.metadata.Metadata.write(Metadata.java:214)\nI can consistently reproduce this using the following url:\nhttp://www.efesbev.com/corporate_governance/pdf/MergerAgreement.pdf",
        "Issue Links": []
    },
    "NUTCH-407": {
        "Key": "NUTCH-407",
        "Summary": "Make Nutch crawling parent directories for file protocol configurable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Thorsten Scherler",
        "Created": "24/Nov/06 13:23",
        "Updated": "11/Sep/10 03:49",
        "Resolved": "27/Nov/06 09:38",
        "Description": "http://www.mail-archive.com/nutch-user@lucene.apache.org/msg06698.html\nI am looking into fixing some very weird behavior of the file protocol.\nI am using 0.8.\nResearching this topic I found \nhttp://www.mail-archive.com/nutch-user@lucene.apache.org/msg06536.html\nand\nhttp://www.folge2.de/tp/search/1/crawling-the-local-filesystem-with-nutch\nI am on Ubuntu but I have the same problem that nutch is going down the\ntree (including parents) and not up (including children from the root\nurl).\nFurther I would vote to make the fetch-parents optional and defined per\na property whether I would like this not very intuitive \"feature\".",
        "Issue Links": [
            "/jira/browse/NUTCH-905"
        ]
    },
    "NUTCH-408": {
        "Key": "NUTCH-408",
        "Summary": "Plugin development documentation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "nutch.newbie",
        "Created": "25/Nov/06 03:44",
        "Updated": "24/Aug/11 17:00",
        "Resolved": "24/Aug/11 17:00",
        "Description": "Documentation is rare! But very vital for extending current (0.9) nutch. Current docs on the wiki for 0.7 plugin development was good but it doesn't apply to 0.9 and new developers who are joining directly 0.9 find the 0.7 documentation not enough. A more practical plugin writing documentation for 0.9 is desired also exposing the plugin principals in practical terms i.e. extension points and libs etc. furthermore it would be good to provide some best practice example i.e. \nlook for the lib you are planning to use if its already in lib folder and maybe that version of the external lib is good for the plugin dev rather then using another version things like that..",
        "Issue Links": [
            "/jira/browse/NUTCH-1056"
        ]
    },
    "NUTCH-409": {
        "Key": "NUTCH-409",
        "Summary": "Add \"short circuit\" notion to filters to speedup mixed site/subsite crawling",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Doug Cook",
        "Created": "26/Nov/06 00:17",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "In the case where one is crawling a mixture of sites and sub-sites, the prefix matcher can match the sites quite quickly, but either the regex or automaton filters are considerably slower matching the sub-sites. In the current model of AND-ing all the filters together, the pattern-matching filter will be run on every site that matches the prefix matcher \u2013 even if that entire site is to be crawled and there are no sub-site patterns. If only a small portion of the sites actually need sub-site pattern matching, this is much slower than it should be.\nI propose (and attach) a simple modification allowing considerable speedup for this usage pattern. I define the notion of a \"short circuit\" match that means \"accept this URL and don't run any of the remaining filters in the filter chain.\" \nThough with this change, any filter plugin can in theory return a short-circuit match, I have only implemented the short-circuit match for the PrefixURLFilter. The configuration file format is backwards-compatible; shortcircuit matches just have SHORTCIRCUIT: in front of them.\nOne minor \"gotcha\":\n\nBecause the shortcircuit matches will avoid running any later filters, all of the site-independent filters need to be BEFORE the PrefixURLFilter in the chain.\n\nI get my best performance using the following filter chain:\n1) The SuffixURLFilter  to throw away anything with unwanted extensions\n2) The RegexURLFilter to do site-independent cleanup (ad removal, skipping , bulletin-board pages, etc.)\n3) The PrefixURLFilter, with SHORTCIRCUIT: in front of every site name EXCEPT the sites needing subsite matching\n4) The AutomatonURLFilter to match those sites needing subsite pattern matching.\nI have tens of thousands of sites and an order of magnitude fewer subsites, so skipping step #4 90% of the time speeds things up considerably (my reduce time on a round of crawling is down from some 26 hours to less than 10).\nThere are only two drawbacks to the implementation, and I think they're pretty minor:\n1) Because I pass a special token (PASS) in the place of the URL to implement the short circuit, if for some reason someone wanted to crawl a URL named \"PASS\", there would be problems. I find this highly unlikely, since that's an invalid URL.\n2) The correct behavior of steps #3 and #4 above depends upon coordination of the config files between the prefix and automaton filters, making an opportunity for user screwup. I thought about creating a \"new kind of filter\" which essentially combined prefix & automaton's behaviors, took one config file, and internally handled the short-circuiting. But I think the approach I took is simpler, cleaner, more flexible, and avoids creating yet another kind of filter. Coordinating the config files is pretty easy (I generate them programmatically).\nAs this is my first contribution to Nutch I'm sure that there are things I've missed, whether in coding style or desired patch format. I welcome any feedback, suggestions, etc.\nDoug",
        "Issue Links": []
    },
    "NUTCH-410": {
        "Key": "NUTCH-410",
        "Summary": "Faster RegexNormalize with more features",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Doug Cook",
        "Created": "29/Nov/06 19:44",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The patch associated with this is backwards-compatible and has several improvements over the stock 0.8 RegexURLNormalizer:\n1) About a 34% performance improvement, from only executing the superclass (BasicURLNormalizer) once in most cases, instead of twice as the stock version did. \n2) Support for expensive host-specific normalizations with good performance. Each <regex> block optionally takes a list of hosts to which to apply the associated regex. If supplied, the regex will only be applied to these hosts. This should have scalable performance; the comparison is O(1) regardless of the number of hosts. The format is:\n    <regex>\n        <host>www.host1.com</host>\n        <host>host2.site2.com</host>\n        <pattern> my pattern here </pattern>\n        <substitution> my substitution here </substitution>\n   </regex>\n3)  Support for decoding URLs with escaped character encodings (e.g. %20, etc.). This is useful, for example, to decode \"jump redirects\" which have the target URL encoded within the source, as on Yahoo. I tried to create an extensible notion of \"options,\" the first of which is \"unescape.\" The unescape function is applied after the substitution and only if the substitution pattern matches. A simple pattern to unescape Yahoo directory redirects would be something like:\n<regex>\n  <pattern>http://[a-z\\.]*\\.yahoo\\.com/.*/&#42;+(http[&]+)</pattern>\n  <substitution>$1</substitution>\n  <options>unescape</options>\n</regex>\n4) Added the notion of iterating the pattern chain. This is useful when the result of a normalization can itself be normalized. While some of this can be handled in the stock version by repeating patterns, or by careful ordering of patterns, the notion of iterating is cleaner and more powerful. The chain is defined to iterate only when the previous iteration changes the input, up to a configurable maxium number of iterations. The config parameter to change is: urlnormalizer.regex.maxiterations, which defaults to 1 (previous behavior). The change is performance-neutral when disabled, and has a relatively small performance cost when enabled.\nPardon any potentially unconventional Java on my part. I've got lots of C/C++ search engine experience, but Nutch is my first large Java app. I welcome any feedback, and hope this is useful.\nDoug",
        "Issue Links": []
    },
    "NUTCH-411": {
        "Key": "NUTCH-411",
        "Summary": "Parse ignores meta refresh redirection",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "30/Nov/06 14:35",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Nov/07 15:03",
        "Description": "If fetching and parsing are run as seperate jobs, then redirection coming from meta refresh tag (i.e. <meta http-equiv=\"refresh\" content=\"0;url=foo/\">) is ignored, resulting in the loss of that (\"foo/\") url.",
        "Issue Links": [
            "/jira/browse/NUTCH-572"
        ]
    },
    "NUTCH-412": {
        "Key": "NUTCH-412",
        "Summary": "plugin to parse the feed-url (rss/atom) of a blog",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Renaud Richardet",
        "Created": "03/Dec/06 07:21",
        "Updated": "10/Dec/12 16:49",
        "Resolved": "10/Dec/12 16:49",
        "Description": "A plugin that extracts the feed-url (rss/atom) of a blog by retrieving the href from the <head><link> element (if found), and stores it in metadata. \nThe meta can be accessed with \nparse.getData().getMeta(\"feedUrl\");\nyou can test this plugin with the main method of HtmlParser.\nThanks for a feedback.",
        "Issue Links": []
    },
    "NUTCH-413": {
        "Key": "NUTCH-413",
        "Summary": "Fetcher ignores -noParsing command line option",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Jonathan Amir",
        "Created": "07/Dec/06 23:10",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 16:20",
        "Description": "I believe that the patch applied in NUTCH-337 broke the fetcher. Now the fetcher ignores the -noParsing command-line option - the parsing occurs anyway.\nTo the best of my understanding of nutch, I managed to trace the problem as follows in the code:\nIn fetcher class, in line 473, -noParsing is evaluted properly and placed into a Configuration created by NutchConfiguartion.create(). So far so good.\nIn the same file, in line 280, the decision whether to parse or not depends on local field \"parsing\". During execution, this fields value is true, instead of false. This field is set to true by method \"configure\", in line 357. The problem is that method \"configure\" accepts a JobConf as a parameter, but the actual JobConf object that is passed to it is not the one used previously in line 473.\nThe one that is actually passed to configure is a different object. I think it is created in line 422, but I am not sure about it.",
        "Issue Links": []
    },
    "NUTCH-414": {
        "Key": "NUTCH-414",
        "Summary": "parse-mp3 plugin concatenating previous tags for text field",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Brian Whitman",
        "Created": "12/Dec/06 15:29",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "The parse-mp3 plugin seems to be saving a state of the previous parse's text content. For every new mp3 file parsed, it is putting the contents of all the previous text fields in the plain text field for that file.\nYou can see this by fetching a set of mp3s in one segment, then viewing their plain text in the nutch webapp. The plaintext will include the contents of all files fetched in that round, which makes searching fruitless.\nI made a tiny band-aid change to MP3Parser.java and MetadataCollector.java against the nightly. It seems to fix the problem.\n\u2014 MP3Parser.java      2006-12-10 09:43:26.000000000 -0500\n+++ MP3Parser.java.new  2006-12-10 16:37:03.000000000 -0500\n@@ -67,7 +67,7 @@\n       fos.write(raw);\n       fos.close();\n       MP3File mp3 = new MP3File(tmp);\n-\n+         metadataCollector.clearText();\n       if (mp3.hasID3v2Tag()) \n{\n         parse = getID3v2Parse(mp3, content.getMetadata());\n       }\n else if (mp3.hasID3v1Tag()) \n{\n\n--- MetadataCollector.java      2006-12-10 09:43:26.000000000 -0500\n+++ MetadataCollector.java.new  2006-12-10 16:37:28.000000000 -0500\n@@ -42,6 +42,10 @@\n       this.conf = conf;\n   }\n\n+  public void clearText() \n{\n+       text = \"\";\n+  }\n+\n   public void notifyProperty(String name, String value) throws\nMalformedURLException {\n     if (name.equals(\"TIT2-Text\"))\n       setTitle(value);",
        "Issue Links": []
    },
    "NUTCH-415": {
        "Key": "NUTCH-415",
        "Summary": "Generate should mark selected records in crawlDB",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.8.2,                                            0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "15/Dec/06 12:32",
        "Updated": "28/Dec/06 00:09",
        "Resolved": "28/Dec/06 00:09",
        "Description": "In Nutch 0.7.x, if user ran \"generate\" twice without intervening \"updatedb\", each fetchlist would be different, because \"generate\" would mark selected entries as \"being fetched\" (by moving their fetch time one week forward).\nIn Nutch 0.8 and later, crawldb is not modified at all during \"generate\". This means that two \"generate\"-s run without intervening \"updatedb\" will create exactly the same fetchlists, which is undesirable.\nI propose to re-implement this feature, using the same mechanism. CrawlDB update would be performed simultaneously with the first mapred job in Generator, and a modified crawldb content would be produced together with an (unsorted) fetchlist in Selector, using a custom OutputFormat (patches to follow  ). Additionally, to ensure that correct version of modified crawldb is installed, I propose to add a locking mechanism, which prevents from running two processes that modify crawldb simultaneously.",
        "Issue Links": []
    },
    "NUTCH-416": {
        "Key": "NUTCH-416",
        "Summary": "CrawlDatum status and CrawlDbReducer refactoring",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "15/Dec/06 12:45",
        "Updated": "28/Dec/06 00:13",
        "Resolved": "28/Dec/06 00:13",
        "Description": "CrawlDatum needs more status codes, e.g. to reflect redirected pages. However, current values of status codes are linear, which prevents us from adding new codes in proper places. This is also related to the logic in CrawlDbReducer, which makes decisions based on arithmetic ordering of status code values.\nI propose to change the codes so that they are grouped into related values, with significant gaps between groups for adding new codes without causing significant reordering. I also propose to change the logic in CrawlDbReducer so that its operation is not so dependent on actual code values.\nA mapping should also be added between old and new codes to facilitate backward-compatibility of existing data. This mapping should be applied on the fly, without requiring explicit data conversion.",
        "Issue Links": []
    },
    "NUTCH-417": {
        "Key": "NUTCH-417",
        "Summary": "After upgrade to hadoop-0.9.1, parsing and indexing doesn't work.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dogacan Guney",
        "Created": "15/Dec/06 13:26",
        "Updated": "09/May/07 18:43",
        "Resolved": "09/May/07 18:43",
        "Description": "If you parse while fetching then it is fine, but if you run parse as a different job, it creates an essentially empty parse_data directory(which has index files, but doesn't have data files). I am not sure why this is happening.\nAlso, indexing fails at Indexer.OutputFormat.getRecordWriter. The parameter fs seems to be an instance of PhasedFileSystem which throws exceptions on delete and \n{start,complete}\nLocalOutput.",
        "Issue Links": []
    },
    "NUTCH-418": {
        "Key": "NUTCH-418",
        "Summary": "Fixes parsing of XHTML (e.g. title)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.2",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael Wechner",
        "Created": "21/Dec/06 12:56",
        "Updated": "09/May/07 18:39",
        "Resolved": "09/May/07 18:39",
        "Description": "Fixes parsing of XHTML (e.g. title)",
        "Issue Links": []
    },
    "NUTCH-419": {
        "Key": "NUTCH-419",
        "Summary": "unavailable robots.txt kills fetch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Carsten Lehmann",
        "Created": "24/Dec/06 12:45",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "02/Mar/09 09:12",
        "Description": "I think there is another robots.txt-related problem which is not\nadressed by NUTCH-344,\nbut also results in an aborted fetch.\nI am sure that in my last fetch all 17 fetcher threads died\nwhile they were waiting for a robots.txt-file to be delivered by a not\nproperly responding web server.\nI looked at the squid access log, which is used by all fetch threads.\nIt ends with many  HTTP-504-errors (\"gateway timeout\") caused by a\ncertain robots.txt url:\n<....>\n1166652253.332 899427 127.0.0.1 TCP_MISS/504 1450 GET\nhttp://gso.gbv.de/robots.txt - DIRECT/193.174.240.8 text/html\n1166652343.350 899664 127.0.0.1 TCP_MISS/504 1450 GET\nhttp://gso.gbv.de/robots.txt - DIRECT/193.174.240.8 text/html\n1166652353.560 899871 127.0.0.1 TCP_MISS/504 1450 GET\nhttp://gso.gbv.de/robots.txt - DIRECT/193.174.240.8 text/html\nThese entries mean that it takes 15 minutes before the request ends\nwith a timeout.\nThis can be calculated from the squid log, the first column is the\nrequest  time (in UTC seconds), the second column is the duration of\nthe request (in ms):\n900000/1000/60=15 minutes.\nAs far as I understand it, every time a fetch thread tries to get this\nrobots.txt-file the thread busy waits for the duration of the request\n(15 minutes).\nIf this is right, then all 17 fetcher threads were caught in this trap\nat the time when  fetching was aborted, as there are 17 requests in\nthe squid log which did not timeout before the message  \"aborting with\n17 threads\" was written to the nutch-logfile.\nSetting fetcher.max.crawl.delay can not help here.\nI see 296 access attempts in total concerning this robots.txt-url in\nthe squid log of this crawl, but fetcher.max.crawl.delay is set to 30.",
        "Issue Links": []
    },
    "NUTCH-420": {
        "Key": "NUTCH-420",
        "Summary": "DeleteDuplicates.HashPartitioner depends on the order of IndexDocs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dogacan Guney",
        "Created": "26/Dec/06 11:30",
        "Updated": "11/Jan/07 22:02",
        "Resolved": "11/Jan/07 22:02",
        "Description": "DeleteDuplicates.HashPartitioner.reduce():\n// byScore case\nif (value.score > highest.score) {\n  highest.keep = false;\n  LOG.debug(\"-discard \" + highest + \", keep \" + value);\n  output.collect(highest.url, highest);     // delete highest\n  highest = value;\n}\n// !byScore is also similar\nSo assume two docs with same hash are in values.If the first has higher score than the second than second doc will be deleted. But if the first has lower score than the second then none will be deleted. AFAICS, there should be an else condition to delete value and keep highest as it is.",
        "Issue Links": []
    },
    "NUTCH-421": {
        "Key": "NUTCH-421",
        "Summary": "Allow predeterminate running order of index filters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "0.9.0",
        "Component/s": "indexer",
        "Assignee": "Sami Siren",
        "Reporter": "Alan Tanaman",
        "Created": "27/Dec/06 13:56",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "06/Jan/07 19:59",
        "Description": "I've tested a patch for org.apache.nutch.indexer.IndexingFilters, allowing the user to state in which order the indexing filters are to be run based on a new\nindexingfilter.order property. This is needed when a filter needs to rely on previously generated document fields as a source of input to generate further fields.\nAs suggested elsewhere, I based this on the urlfilter.order functionality:\n<property>\n  <name>indexingfilter.order</name>\n  <value>org.apache.nutch.indexer.basic.BasicIndexingFilter org.apache.nutch.indexer.more.MoreIndexingFilter</value>\n  <description>The order by which index filters are applied.\n  If empty, all available index filters (as dictated by properties\n  plugin-includes and plugin-excludes above) are loaded and applied in system\n  defined order. If not empty, only named filters are loaded and applied\n  in given order. For example, if this property has value:\n  org.apache.nutch.indexer.basic.BasicIndexingFilter org.apache.nutch.indexer.more.MoreIndexingFilter\n  then BasicIndexingFilter is applied first, and MoreIndexingFilter second.\n  Since all filters are AND'ed, filter ordering does not have impact\n  on end result, but it may have performance implication, depending\n  on relative expensiveness of filters.\n  </description>\n</property>",
        "Issue Links": []
    },
    "NUTCH-422": {
        "Key": "NUTCH-422",
        "Summary": "index-extra plugin creates additional fields in the index, based on configurable logic",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "1.5",
        "Component/s": "indexer",
        "Assignee": "Sami Siren",
        "Reporter": "Alan Tanaman",
        "Created": "28/Dec/06 19:22",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Feb/12 16:32",
        "Description": "Extract from the Readme file:\nA.  Introduction\n    The index-extra plugin allows you to configure additional fields that you wish to be added to the index, based on one of the following sources:\n\nThe parsed text\nMeta data fields\nPreviously created document-to-be-indexed fields\nPlain constant string\nJava expression combining one or more of the above, and resolving to a string\n    A regex can also be applied to any of the above, allowing fields to be created based on patterns extracted from the source.\n\nB.  Installation\n    1)  Binaries only:  Copy the 'index-extra' folder within index-extra-v1.0-bin-java1.5.zip to NUTCHDIR/build\n                        Copy the 'index-extra-conf.xml' file to NUTCHDIR/conf, and configure\n                        Enable the plugin by updating the nutch-site.xml file\n    2)  Source code:    Always refer to the Nutch wiki for detailed instructions on building Nutch.  In short:\n                        Copy the 'index-extra' folder within index-extra-v1.0-source.zip to NUTCHDIR/src/plugin\n                        Update the build.xml in NUTCHDIR/src/plugin to include plugin\n                        Update the NUTCHDIR/default.properties file to include plugin\n                        run ant to build\n                        Copy the 'index-extra-conf.xml' file to NUTCHDIR/conf, and configure\n                        Enable the plugin by updating the nutch-site.xml file\nC.  Known Issues\n    1)  For this plugin to work correctly on any document field, it is necessary to run the other index filters\n    first, so that all basic document fields are generated first.  To do this, configure the indexingfilter.order\n    property.  (Please see patch NUTCH-421 to enable indexingfilter.order property. If this patch is not applied,\n    the plugin will still work, but will not be able to use document fields created by other index filter plugins.)\n    2)  At this stage, field boost can not be used as Nutch scoring overrides the field boost with its own\n    document-level boost calculation.  This occurs at the end of org.apache.nutch.indexer.Indexer's reduce method.",
        "Issue Links": [
            "/jira/browse/NUTCH-809",
            "/jira/browse/NUTCH-809"
        ]
    },
    "NUTCH-423": {
        "Key": "NUTCH-423",
        "Summary": "Add other index-basic fields as query plugins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael Stack",
        "Created": "29/Dec/06 00:45",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:36",
        "Description": "The basic indexer plugin adds 'host', 'site', 'url', 'content', 'title', and 'anchor'.  The query-basic plugin expands queries against the 'default' field to run against all basic indexer plugin fields.  The query-url pluging adds query filtering on the 'url' field and query-site' on 'site'.  This patch adds plugins to filter on the remainder: 'host', 'content', 'title', and 'anchor'.",
        "Issue Links": []
    },
    "NUTCH-424": {
        "Key": "NUTCH-424",
        "Summary": "NekoHTML's DOMFragmentParser hangs on certain URLs (CLONE: Problem persists with Nutch 0.9 and 0.8.1 (Nekohtml 0.9.4))",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8.1,                                            0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Karsten Dello",
        "Created": "01/Jan/07 21:26",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "I've tracked down occasional fetcher hangs to NekoHTML's DOMFragmentParser hanging certain HTML documents, for example, http://www.inlandrevenue.gov.uk/charities/chapter_3.htm.\nThe thread dump on the hung parser is:\n\"CompilerThread0\" daemon prio=1 tid=0x080c4c18 nid=0x47da waiting on condition [0x00000000..0x8a3daf68]\n\"Signal Dispatcher\" daemon prio=1 tid=0x080c3d60 nid=0x47d9 waiting on condition [0x00000000..0x00000000]\n\"Finalizer\" daemon prio=1 tid=0x080b8818 nid=0x47d8 in Object.wait() [0x8a2a0000..0x8a2a0680]\n        at java.lang.Object.wait(Native Method)\n\nwaiting on <0x4a60d058> (a java.lang.ref.ReferenceQueue$Lock)\n        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:116)\nlocked <0x4a60d058> (a java.lang.ref.ReferenceQueue$Lock)\n        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:132)\n        at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)\n\n\"Reference Handler\" daemon prio=1 tid=0x080b7b50 nid=0x47d7 in Object.wait() [0x8a21f000..0x8a21f800]\n        at java.lang.Object.wait(Native Method)\n\nwaiting on <0x4a60d0d8> (a java.lang.ref.Reference$Lock)\n        at java.lang.Object.wait(Object.java:474)\n        at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)\nlocked <0x4a60d0d8> (a java.lang.ref.Reference$Lock)\n\n\"main\" prio=1 tid=0x0805c170 nid=0x47d1 waiting on condition [0xbfffc000..0xbfffcec8]\n        at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:99)\n        at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:393)\n        at java.lang.StringBuffer.append(StringBuffer.java:225)\n\nlocked <0x45910118> (a java.lang.StringBuffer)\n        at org.apache.xerces.dom.CharacterDataImpl.appendData(Unknown Source)\n        at org.cyberneko.html.parsers.DOMFragmentParser.characters(Unknown Source)\n        at org.cyberneko.html.filters.DefaultFilter.characters(Unknown Source)\n        at org.cyberneko.html.HTMLTagBalancer.characters(Unknown Source)\n        at org.cyberneko.html.HTMLScanner$ContentScanner.scanCharacters(Unknown Source)\n        at org.cyberneko.html.HTMLScanner$ContentScanner.scan(Unknown Source)\n        at org.cyberneko.html.HTMLScanner.scanDocument(Unknown Source)\n        at org.cyberneko.html.HTMLConfiguration.parse(Unknown Source)\n        at org.cyberneko.html.HTMLConfiguration.parse(Unknown Source)\n        at org.cyberneko.html.parsers.DOMFragmentParser.parse(Unknown Source)\n        at net.nutch.parse.html.HtmlParser.getParse(HtmlParser.java:157)\n        at net.nutch.parse.ParserChecker.main(ParserChecker.java:74)\n\n\"VM Thread\" prio=1 tid=0x080b4f30 nid=0x47d6 runnable\n\"VM Periodic Task Thread\" prio=1 tid=0x080c75f8 nid=0x47dc waiting on condition\nUsing the URL mentioned above, I was able to successfully parse the file using a normal NekoHTML DocumentParser.",
        "Issue Links": [
            "/jira/browse/NUTCH-17"
        ]
    },
    "NUTCH-425": {
        "Key": "NUTCH-425",
        "Summary": "parse-js pollutes anchor text with base URL of source page",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Michael Stack",
        "Created": "04/Jan/07 17:19",
        "Updated": "05/Jan/07 16:59",
        "Resolved": "05/Jan/07 16:59",
        "Description": "Parse-js plugin always adds URL \u2013 usually page base URL \u2013 as anchor text for any link discovered parsing javascript.  Anchor text is tokenized when indexed and by default gets a heavy weighting.  The upshot is often pages show high in search results for no reason other than query term appears in (URL) anchors.  \nSee http://www.mail-archive.com/nutch-user%40lucene.apache.org/msg06935.html for related user list postings.\nHere is extract from linkdb exhibiting the problem:\nhttps://www2.westpac.com.au/emarket/check_merch.cfm?id=900030 Inlinks: \n fromUrl: http://premier.ticketek.com.au/content/buyers/buyers_step1.aspx anchor: http://premier.ticketek.com.au/content/buyers/buyers_step1.aspx\n fromUrl: http://premier.ticketek.com.au/content/outlets/agencies_qld.aspx anchor: http://premier.ticketek.com.au/content/outlets/agencies_qld.aspx\n fromUrl: http://premier.ticketek.com.au/shows/show.aspx?sh=TSSWANS05 anchor: http://premier.ticketek.com.au/shows/show.aspx?sh=TSSWANS05\n fromUrl: http://premier.ticketek.com.au/content/outlets/agencies_vic.aspx anchor: http://premier.ticketek.com.au/content/outlets/agencies_vic.aspx\n fromUrl: http://premier.ticketek.com.au/Venues/VenueDetails.aspx?v=NMO&s=6547 anchor: http://premier.ticketek.com.au/Venues/VenueDetails.aspx?v=NMO&s=6547\n fromUrl: http://premier.ticketek.com.au/content/buyers/buyers_step5.aspx anchor: http://premier.ticketek.com.au/content/buyers/buyers_step5.aspx\n fromUrl: http://premier.ticketek.com.au/content/outlets/agencies_nsw.aspx anchor: http://premier.ticketek.com.au/content/outlets/agencies_nsw.aspx",
        "Issue Links": []
    },
    "NUTCH-426": {
        "Key": "NUTCH-426",
        "Summary": "parse-js skips parsing if found URL fails java.net.URL parse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Michael Stack",
        "Created": "04/Jan/07 20:12",
        "Updated": "05/Jan/07 17:00",
        "Resolved": "05/Jan/07 17:00",
        "Description": "The parse-js plugin in getJSLinks tries a regex looking for likely URLs against a string of javascript.  Any matches that do not begin 'www' are given to java.net.URL with base URL to test 'URLness'.  Often this test will fail.  Currently, when it fails, nutch skips processing any more of the javascript snippet logs an error.",
        "Issue Links": []
    },
    "NUTCH-427": {
        "Key": "NUTCH-427",
        "Summary": "protocol-smb: plugin protocol implementing the CIFS/SMB protocol. This protocol allows Nutch to crawl Microsoft Windows Shares remotely using the CIFS/SMB protocol implmentation.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8.1,                                            0.9.0,                                            1.0.0",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Armel Nene",
        "Created": "05/Jan/07 14:42",
        "Updated": "30/Dec/21 06:59",
        "Resolved": "09/May/13 05:00",
        "Description": "Title:    protocol-smb - Nutch protocol plugin for crawling Microsoft Windows shares\nAuthor:   Armel T. Nene \nUpdate:   Vadim Bauer\nEmail:    armel.nene NOSPAM-AT-NOSPAM idna-solutions.com, V a d i m B a u e r <AT> g m x . d e\nA.  Introduction\n    The protocol-smb plugins allows you to crawl Microsoft Windows shares. It implements\n    the CIFS/SMB protocol which is commonly used on Microsoft OS. The plugin replicate the\n    behaviour of the protocol-file over CIFS/SMB protocol. This plugin uses the JCifs library and also\n    support all the properties from the JCifs library.\n    You can find more information on the following site: http://jcifs.samba.org/\n    The smb protocol syntax for crawling is as follow: smb://xxxxx (i.e. smb://server/share).\nB.  Installation\n    1) Binaries only:   The protocol-smb files can be found in the ../plugins directory.\n\t\t\t\tCopy the \"protocol-smb\" to NUTCHHOME/build/plugins directory.\n                        Put the \"smb.properties\" file in the NUTCHHOME/conf directory.\n                        Configure the properties in \"smb.properties\" file\n                        Enable the plugin by updating \"nutch-site.xml\" file found in NUTCHHOME/conf directory\n\t\t\t\te.g. <property>\n    \t\t\t\t     \t<name>plugin.includes</name>\n    \t\t\t\t     \t<value>protocol-smb| other plugins...</value>\n    \t\t\t\t     \t<description>\n \t \t\t\t     \t</description>\n \t \t\t\t     </property>\n    2)  Source code:    The protocol-smb sources can be found in the ../src directory.\n\t\t\t\tAlways refer to the Nutch wiki for detailed instructions on building Nutch.  In short:\n                        Copy the 'protocol-smb' folder to NUTCHHOME/src/plugin\n                        Update the build.xml in NUTCHHOME/src/plugin to include plugin\n                        Update the NUTCHHOME/default.properties file to include plugin\n                        run ant to build\n                        Copy the 'smb.properties' file to NUTCHHOME/conf, and configure the properties\n                        Enable the plugin by updating the nutch-site.xml file\nC: Known Issues\n    1) URLMalformedException: unkown protocol: smb\n       The SMB URL protocol handler is not being successfully installed. \n       In short, the jCIFS jar must be loaded by the System class loader.\n       Workaround: a) a short term solutions will be to installed the JCIFS jar \n                      library found in protocol-smb folder in \n                      JDKHOME/jre/lib/ext and (or) JREHOME/lib/ext\n                   b) After completing step a), if the exeception is still thrown\n                      set the System properties by passing the following arguments\n                      to the JVM: \n                      -Djava.protocol.handler.pkgs=jcifs\n\t\t\t c) You can set the property also in your Code for example if \n\t\t\t    you start Crawling with org.apache.nutch.crawl.Crawl\n\t\t\t    Add the following two lines. This will be the Same like in b)\n\t\t\t    public static void main(String args[]) throws Exception {\n\t  \t\t    \tSystem.setProperty(\"java.protocol.handler.pkgs\", \"jcifs\");\n\t\t\t\tnew java.util.PropertyPermission(\"java.protocol.handler.pkgs\",\"read, write\")\n\t\t\t\t//and so on\n       Also you can visit the FAQ page: http://jcifs.samba.org/src/docs/faq.html\n    2) FATAL smb.SMB - Could not read content of protocol: smb://xxxxxx\n       This problem usually occurs if the following properties are not set correctly in\n       the \"smb.properties\" file:\n\nusername\npassword\ndomain\n\n       Also refer to the following resources for more information on the list of\n       available properties and how to set them:\n http://jcifs.samba.org/src/docs/api/overview-summary.html#scp\n       Also you can visit the FAQ page: http://jcifs.samba.org/src/docs/faq.html\n       N.B. All properties should set in the \"smb.properties\" file. You can set \n            all supported JCIFS properties in the \"smb.properties\" file.\n    3) Only tested on Windows XP and Windows Server 2003. Please report any tests \n       conclusion on other OS.",
        "Issue Links": [
            "/jira/browse/NUTCH-2856",
            "/jira/browse/NUTCH-2429"
        ]
    },
    "NUTCH-428": {
        "Key": "NUTCH-428",
        "Summary": "NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Piyush",
        "Created": "10/Jan/07 14:56",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "12/Jan/07 22:14",
        "Description": "I am using the NUTCH.Bat provided in one one of the thread. (i am not using CYGWIN) Whenever I try to fetch the Item, I am getting fetching failed \"nullpointerexception\" \nI have a URL Directory. which has urls.txt file. there is only one entry in the file which is http://www.winzip.com/land_about.htm. \nI have updated the crawl-urlfilter.txt with +^http://www.winzip.com/. \nIs there any other settings I am missing?? Any help is greatly appreciated. \nThe command i used to  start the crawl is \nnutch  crawl urls -dir crawlResults -depth 1\nHere is my log \ncrawl started in: crawlResult\nrootUrlDir = urls\nthreads = 10\ndepth = 1\nInjector: starting\nInjector: crawlDb: crawlResult/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: done\nGenerator: starting\nGenerator: segment: crawlResult/segments/20070110085314\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: Partitioning selected urls by host, for politeness.\nGenerator: done.\nFetcher: starting\nFetcher: segment: crawlResult/segments/20070110085314\nFetcher: threads: 10\nfetching http://www.winzip.com/land_about.htm\nfetch of http://www.winzip.com/land_about.htm failed with: java.lang.NullPointerException\nFetcher: done\nCrawlDb update: starting\nCrawlDb update: db: crawlResult/crawldb\nCrawlDb update: segment: crawlResult/segments/20070110085314\nCrawlDb update: Merging segment data into db.\nCrawlDb update: done\nLinkDb: starting\nLinkDb: linkdb: crawlResult/linkdb\nLinkDb: adding segment: crawlResult/segments/20070110085314\nLinkDb: done\nIndexer: starting\nIndexer: linkdb: crawlResult/linkdb\nIndexer: adding segment: crawlResult/segments/20070110085314\nOptimizing index.\nIndexer: done\nDedup: starting\nDedup: adding indexes in: crawlResult/indexes\nDedup: done\nAdding crawlResult/indexes/part-00000\ncrawl finished: crawlResult",
        "Issue Links": []
    },
    "NUTCH-429": {
        "Key": "NUTCH-429",
        "Summary": "Secured Searches",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Piyush",
        "Created": "11/Jan/07 20:08",
        "Updated": "11/Jan/07 20:46",
        "Resolved": "11/Jan/07 20:46",
        "Description": "Does NUTCH Support secured Searches? If yes, Could you please forward me to a appropriate documentation",
        "Issue Links": []
    },
    "NUTCH-430": {
        "Key": "NUTCH-430",
        "Summary": "integer overflow in HashComparator.compare",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1,                                            0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "generator",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "13/Jan/07 23:06",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "15/Jan/07 15:04",
        "Description": "There's a integer overflow problem in HashComparator wich leads to fetchlist not to be sorted properly by hash of url. This leads to slower fetching speeds if there are many urls from same host as they are not evenly distributed.",
        "Issue Links": []
    },
    "NUTCH-431": {
        "Key": "NUTCH-431",
        "Summary": "Move plugin specific properties out of nutch-site.xml and into specific conf files for plugins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "20/Jan/07 22:02",
        "Updated": "22/May/13 03:53",
        "Resolved": "23/Mar/13 04:16",
        "Description": "Currently, there are many plugin-specific properties that live in the global nutch properties files, nutch-site.xml and nutch-default.xml. These would be things like the protocol-ftp properties, even the protocol-http properties. It would be nice to refactor these properties out, into plugin specific configuration files, that ship with the plugins themselves. Thoughts? Comments? Tomatoes?",
        "Issue Links": [
            "/jira/browse/NUTCH-921"
        ]
    },
    "NUTCH-432": {
        "Key": "NUTCH-432",
        "Summary": "JAVA_PLATFORM with spaces (i.e. Mac OS X-ppc-32) breaks bin/nutch script",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Brian Whitman",
        "Created": "24/Jan/07 17:39",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "27/Mar/07 16:59",
        "Description": "In some later nightly in the past few weeks (not sure when) the bin/nutch script stopped working on my Macs with \nException in thread \"main\" java.lang.NoClassDefFoundError: OS\nOn any command. I tracked it down to the JAVA_PLATFORM env variable that is used to try to find a native hadoop library. The line\nJAVA_PLATFORM=`CLASSPATH=${CLASSPATH} ${JAVA} org.apache.hadoop.util.PlatformName`\nin bin/nutch returns \"Mac OS X-ppc-32\", which then appears as\n-Djava.library.path=/Users/bwhitman/Desktop/nn/lib/native/Mac OS X-ppc-32\nin the java command line to start a nutch tool. \nNot sure the best way to fix this, but I manually put \nJAVA_PLATFORM='MacOSX/PPC'\nand the error went away.",
        "Issue Links": [
            "/jira/browse/HADOOP-1081"
        ]
    },
    "NUTCH-433": {
        "Key": "NUTCH-433",
        "Summary": "java.io.EOFException in newer nightlies in mergesegs or indexing from hadoop.io.DataOutputBuffer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "generator,                                            indexer",
        "Assignee": "Sami Siren",
        "Reporter": "Brian Whitman",
        "Created": "24/Jan/07 17:53",
        "Updated": "18/Apr/07 15:44",
        "Resolved": "24/Jan/07 19:51",
        "Description": "The nightly builds have not been working at all for the past couple of weeks. Sami Siren has narrowed it down to HADOOP-331.\nTo replicate: download the nightly, then:\nbin/nutch inject crawl/crawldb urls/  # a single URL is in urls/urls \u2013 http://apache.org\nbin/nutch generate crawl/crawldb crawl/segments\nbin/nutch fetch crawl/segments/2007...\nbin/nutch updatedb crawl/crawldb crawl/segments/2007...\n\ngenerate a new segment with 5 URIs\nbin/nutch generate crawl/crawldb crawl/segments -topN 5\nbin/nutch fetch crawl/segments/2007... # new segment\nbin/nutch updatedb crawl/crawldb crawl/segments/2007... # new segment\n\n\nmerge the segments and index\nbin/nutch mergesegs crawl/merged -dir crawl/segments\n..\n\nWe get a crash in the mergesegs. This crash, with the exact same script and start URI, configuration and plugins, does not happen on a nightly from early January.\n2007-01-18 14:57:11,411 INFO  segment.SegmentMerger - Merging 2 segments to crawl/merged_07_01_18_14_56_22/20070118145711\n2007-01-18 14:57:11,482 INFO  segment.SegmentMerger - SegmentMerger:   adding crawl/segments/20070118145628\n2007-01-18 14:57:11,489 INFO  segment.SegmentMerger - SegmentMerger:   adding crawl/segments/20070118145641\n2007-01-18 14:57:11,495 INFO  segment.SegmentMerger - SegmentMerger: using segment data from: content crawl_generate crawl_fetch crawl_parse parse_data parse_text\n2007-01-18 14:57:11,594 INFO  mapred.InputFormatBase - Total input paths to process : 12\n2007-01-18 14:57:11,819 INFO  mapred.JobClient - Running job: job_5ug2ip\n2007-01-18 14:57:12,073 WARN  mapred.LocalJobRunner - job_5ug2ip\njava.io.EOFException\n        at java.io.DataInputStream.readFully(DataInputStream.java:178)\n        at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:57)\n        at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:91)\n        at org.apache.hadoop.io.UTF8.readChars(UTF8.java:212)\n        at org.apache.hadoop.io.UTF8.readString(UTF8.java:204)\n        at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:173)\n        at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:61)\n        at org.apache.nutch.metadata.MetaWrapper.readFields(MetaWrapper.java:100)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.spill(MapTask.java:427)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpillToDisk(MapTask.java:385)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$200(MapTask.java:239)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:188)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:109)",
        "Issue Links": []
    },
    "NUTCH-434": {
        "Key": "NUTCH-434",
        "Summary": "Replace usage of ObjectWritable with something based on GenericWritable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Sami Siren",
        "Created": "24/Jan/07 20:21",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Jun/07 07:06",
        "Description": "We should replace the usage of ObjectWritable and classes extending it with class extending GenericWritable. Classes based on GenericWritable have smaller footprint on disc and the baseclass also does not contain any classes that are Deprecated.\nThere is one problem though: the ParseData currently needs Configuration object before it can deserialize itself and GenericWritable\ndoesn't provide a way to inject configuration in. We could either a) remove the need for Configuration, or b) write a class similar to GenericWritable that does conf injecting.",
        "Issue Links": []
    },
    "NUTCH-435": {
        "Key": "NUTCH-435",
        "Summary": "Synonym-Editor that creates OWL for the ontology plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Urs Krebs",
        "Created": "26/Jan/07 12:31",
        "Updated": "13/Apr/11 23:36",
        "Resolved": "13/Apr/11 23:36",
        "Description": "Attached is a synonym-editor which helps create OWL for the ontology plugin. It is a separate Swing application. If of value, please include it to the distribution. I am happy to receive comments on what to do before the application can be included.",
        "Issue Links": []
    },
    "NUTCH-436": {
        "Key": "NUTCH-436",
        "Summary": "Incorrect handling of relative paths when the embedded URL path is empty",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Dennis Kubes",
        "Reporter": "Andrew Groh",
        "Created": "26/Jan/07 14:09",
        "Updated": "22/Sep/11 13:50",
        "Resolved": "10/Mar/07 02:37",
        "Description": "If you have a base URL of the form:\nhttp://a/b/c/d;p?q#f\nEmbedded URL: ?y\nCorrect Absolute URL: http://a/b/c/d;p?y \nNutch Generated URL: http://a/b/c/?y\nEmbedded URL: ;x\nCorrect Absolute URL: http://a/b/c/d;x \nNutch Generated URL: http://a/b/c/;x\nSee section 4, steps 5-7 of RFC 1808 for the definition of the correct set of steps, and section 5.1 for example\nhttp://www.ietf.org/rfc/rfc1808.txt",
        "Issue Links": [
            "/jira/browse/NUTCH-566",
            "/jira/browse/NUTCH-1115"
        ]
    },
    "NUTCH-437": {
        "Key": "NUTCH-437",
        "Summary": "MapFile in Hadoop Trunk has changed, must update references",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.2,                                            0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dennis Kubes",
        "Created": "02/Feb/07 18:59",
        "Updated": "07/Mar/07 22:10",
        "Resolved": "07/Mar/07 22:10",
        "Description": "The MapFile.Writer signature has changed in hadoop trunk (version 10.x +) to include a Configuration object.  Object in the Nutch codebase that reference MapFile.Writer will need to be updated.",
        "Issue Links": []
    },
    "NUTCH-438": {
        "Key": "NUTCH-438",
        "Summary": "Add -noAdditions to updatedb",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1",
        "Fix Version/s": "0.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nicol\u00e1s Lichtmaier",
        "Created": "02/Feb/07 19:58",
        "Updated": "22/Jan/08 14:37",
        "Resolved": "22/Jan/08 14:37",
        "Description": "It would be great for me to have -noAdditions  support (which is implemented in trunk) in the 0.8.x branch.",
        "Issue Links": []
    },
    "NUTCH-439": {
        "Key": "NUTCH-439",
        "Summary": "Top Level Domains Indexing / Scoring",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Enis Soztutar",
        "Reporter": "Enis Soztutar",
        "Created": "06/Feb/07 13:33",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "21/Aug/07 10:50",
        "Description": "Top Level Domains (tlds) are the last part(s) of the host name in a DNS system. TLDs are managed by the Internet Assigned Numbers Authority. IANA divides tlds into three. infrastructure, generic(such as \"com\", \"edu\") and country code tlds(such as \"en\", \"de\" , \"tr\", ). Indexing the top level domain and optionally boosting is needed for improving the search results and enhancing locality.",
        "Issue Links": []
    },
    "NUTCH-440": {
        "Key": "NUTCH-440",
        "Summary": "Command line utilities should exit with an error message when given wrong arguments",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nicol\u00e1s Lichtmaier",
        "Created": "06/Feb/07 20:28",
        "Updated": "22/Jan/08 14:38",
        "Resolved": "22/Jan/08 14:38",
        "Description": "I've lost a lot of time because I wasn't realizing an option was only implemented in trunk and not in 0.8.x. Should it have been reported as an error I would realize my mistake immediately. So please consider exiting with System.exit(1) if a nutch command line tool is given an incorrect option.",
        "Issue Links": []
    },
    "NUTCH-441": {
        "Key": "NUTCH-441",
        "Summary": "Thai Analyzer Plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Vee Satayamas",
        "Created": "07/Feb/07 08:59",
        "Updated": "01/Apr/11 14:58",
        "Resolved": "01/Apr/11 14:58",
        "Description": "This Thai analyzer plugin was created by coping and modifying the French analyzer plugin. However, there is no Thai analyzer in lucene-analyzers-2.0.0.jar (in lib-lucene-analyzers). Thus lucene-analyzers-nightly.jar was used instead.",
        "Issue Links": []
    },
    "NUTCH-442": {
        "Key": "NUTCH-442",
        "Summary": "Integrate Solr/Nutch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "rubdabadub",
        "Created": "07/Feb/07 18:37",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "12/Jan/09 13:27",
        "Description": "Hi:\nAfter trying out Sami's patch regarding Solr/Nutch. Can be found here (http://blog.foofactory.fi/2007/02/online-indexing-integrating-nutch-with.html) and I can confirm it worked  And that lead me to request the following :\nI would be very very great full if this could be included in nutch 0.9 as I am trying to eliminate my python based crawler which post documents to solr. As I am in the corporate enviornment I can't install trunk version in the production enviornment thus I am asking this to be included in 0.9 release. I hope my wish would be granted.\nI look forward to get some feedback.\nThank you.",
        "Issue Links": []
    },
    "NUTCH-443": {
        "Key": "NUTCH-443",
        "Summary": "allow parsers to return multiple Parse object, this will speed up the rss parser",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Renaud Richardet",
        "Created": "07/Feb/07 18:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Jun/07 17:20",
        "Description": "allow Parser#parse to return a Map<String,Parse>. This way, the RSS parser can return multiple parse objects, that will all be indexed separately. Advantage: no need to fetch all feed-items separately.\nsee the discussion at http://www.nabble.com/RSS-fecter-and-index-individul-how-can-i-realize-this-function-tf3146271.html",
        "Issue Links": []
    },
    "NUTCH-444": {
        "Key": "NUTCH-444",
        "Summary": "Possibly use a different library to parse RSS feed for improved performance and compatibility",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Renaud Richardet",
        "Created": "09/Feb/07 23:41",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Jun/07 14:02",
        "Description": "As discussed by Nutch Newbie, Gal, and Chris on NUTCH-443, the current library (feedparser) has the following issues:\n\nOutOfMemory when parsing > 100k feeds, since it has to convert the feed to jdom first\nno support for Atom 1.0\nthere has been no development in the last year\n\nAlternatives are:\n\nRome\nInforma\ncustom implementation based on Stax\n??",
        "Issue Links": []
    },
    "NUTCH-445": {
        "Key": "NUTCH-445",
        "Summary": "Domain \u0130ndexing / Query Filter",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Enis Soztutar",
        "Created": "15/Feb/07 10:34",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "Hostname's contain information about the domain of th host, and all of the subdomains. Indexing and Searching the domains are important for intuitive behavior. \nFrom DomainIndexingFilter javadoc : \nAdds the domain(hostname) and all super domains to the index. \n\n<br> For http://lucene.apache.org/nutch/ the\nfollowing will be added to the index : <br>\n<ul>\n<li>lucene.apache.org </li>\n<li>apache</li>\n<li>org </li>\n</ul>\nAll hostnames are domain names, but not all the domain names are\nhostnames. In the above example hostname lucene is a\nsubdomain of apache.org, which is itself a subdomain of\norg <br>\n\n\nCurrently Basic indexing filter indexes the hostname in the site field, and query-site plugin \nallows to search in the site field. However site:apache.org will not return http://lucene.apache.org\n By indexing the domain, we can be able to search domains. Unlike \n the site field (indexed by BasicIndexingFilter) search, searching the \n domain field allows us to retrieve lucene.apache.org to the query \n apache.org.",
        "Issue Links": []
    },
    "NUTCH-446": {
        "Key": "NUTCH-446",
        "Summary": "RobotRulesParser should ignore Crawl-delay values of other bots in robots.txt",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Sami Siren",
        "Reporter": "Dogacan Guney",
        "Created": "15/Feb/07 15:45",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "10/May/07 16:30",
        "Description": "RobotRulesParser doesn't check for addRules when reading the crawl-delay value, so the nutch bot will get the crawl-delay value of another robot's crawl-delay in robots.txt. \nLet me try to be more clear:\nUser-agent: foobot\nCrawl-delay: 3600\nUser-agent: *\nDisallow: /baz\nIn such a robots.txt file, nutch bot will get 3600 as its crawl-delay\nvalue, no matter what nutch bot's name actually is.",
        "Issue Links": []
    },
    "NUTCH-447": {
        "Key": "NUTCH-447",
        "Summary": "Dmoz Structure Parser Tool",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "20/Feb/07 21:02",
        "Updated": "31/Mar/08 05:17",
        "Resolved": "31/Mar/08 05:17",
        "Description": "This is a tool that will take the dmoz structure RDF file and return a listing of the categories.  The categories return can be limited by depth or by regular expression pattern.  This tool borrows heavily from the DmozParser.",
        "Issue Links": []
    },
    "NUTCH-448": {
        "Key": "NUTCH-448",
        "Summary": "Allow Plugin Includes and Excludes from File",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "22/Feb/07 07:17",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "09/Dec/08 17:57",
        "Description": "This functionality allows the plugin.includes and plugin.excludes values to be moved out of the nutch-default.xml and nutch-site.xml files and loaded from one or more text configurtion files found in the classpath.  This is a cleaner implementation then having one big long regular expression in the configuration file as plugin.includes or plugin.excludes.\nLoads plugin configuration from files defined by the plugin.files configurtion variable.  Files must be available to be found in the classpath.  The plugin files consist of one regex per line.  Plugins starting with a - will be excluded while lines starting with a # will be ignored.  All other non-blank lines will be included as plugins, one per line. Any plugins configured through plugin.includes and plugin.excludes in the configuration are also added.  Any plugins that are excluded are removed from the includes.",
        "Issue Links": []
    },
    "NUTCH-449": {
        "Key": "NUTCH-449",
        "Summary": "Format of junit output should be configurable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nigel Daley",
        "Created": "23/Feb/07 22:47",
        "Updated": "30/Apr/13 09:07",
        "Resolved": "30/Apr/13 09:07",
        "Description": "Allow the junit output format to be set by a system property.",
        "Issue Links": []
    },
    "NUTCH-450": {
        "Key": "NUTCH-450",
        "Summary": "How to set up nutch",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "administration gui",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Sandya S Murthy",
        "Created": "26/Feb/07 07:10",
        "Updated": "19/Mar/07 23:53",
        "Resolved": "19/Mar/07 23:53",
        "Description": "I have followed the instruction given in nutch tutorial to set up the nutch,\nI installed J2sdk , tomcat 4.x, and cygwin after that i download the nutch version 0.7.2. but i did'nt understand how to connect this nutch to the root directory, how to set up this downloaded nutch folder.\npls help",
        "Issue Links": []
    },
    "NUTCH-451": {
        "Key": "NUTCH-451",
        "Summary": "Tool to recover partial fetcher output",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "26/Feb/07 19:42",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 16:23",
        "Description": "This class may help you to recover partial data from a failed Fetcher run. \nNOTE 1: this works ONLY if you ran Fetcher using \"local\" file system, i.e. you didn't use DFS - partial output to DFS is permanently lost if a process fails to properly close the output streams.\nNOTE 2: if Fetcher was stopped abruptly (killed or crashed), then partial SequenceFile-s will be corrupted at the end. This means that it won't be possible to recover all data from them - most likely only the data up to the last sync marker can be recovered.\nThe recovery proces requires some preparation: \n\ndetermine the map directories corresponding to the map task outputs of the failed job. These map directories contain SequenceFile-s consisting of pairs of <Text, FetcherOutput>, named e.g. part-0.out, or file.out, or spill0.out.\n\n\ncreate the new input directory, let's say input/. Copy all SequenceFile-s into this directory, renaming them sequentially like this:\n  input/part-00000\n  input/part-00001\n  input/part-00002\n  input/part-00003\n  ...\n\n\nspecify the \"input\" directory as the input to this tool.\n\nIf all goes well, a new segment will be created as a subdirectory of the output dir.",
        "Issue Links": []
    },
    "NUTCH-452": {
        "Key": "NUTCH-452",
        "Summary": "Nutch JSF/My Faces Search Frontend",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "web gui",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Zaheed Haque",
        "Created": "01/Mar/07 08:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Dec/08 13:47",
        "Description": "As per Doug's suggestion a ticket is now open. Over the weekend I will write up a small instruction plus upload all the files necessary for the ticket. (I need to remove all the libs and list them so one could download the libs directly this way the patch will probably make the 10 MB limit) If you have questions, comments just let me know.\nCheers",
        "Issue Links": []
    },
    "NUTCH-453": {
        "Key": "NUTCH-453",
        "Summary": "Move stop words to a config file",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Steve Severance",
        "Created": "02/Mar/07 00:32",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:36",
        "Description": "Move the stop words from the code to a config file. This will allow the stop words to be modified without recompiling the code. The format could be the same as the regex-urlfilter where regexs are used to define the words or a plain text file of words could be used.",
        "Issue Links": []
    },
    "NUTCH-454": {
        "Key": "NUTCH-454",
        "Summary": "Review Debug Level Log Guards",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8.1,                                            0.9.0",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Dennis Kubes",
        "Created": "04/Mar/07 05:59",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "14/Jul/10 17:51",
        "Description": "There are currently log guards (i.e. is*Enabled type code) in many different places in the code.  NUTCH-309 is related to removing those log guards.  The caveat is that debug level log guards should be reviewed and should be kept only when in performance critical inner loops.  The instances where this occurs should be few and far between.  This issue request is for reviewing the debug level code guards and keeping only the performance critical ones.",
        "Issue Links": [
            "/jira/browse/NUTCH-309"
        ]
    },
    "NUTCH-455": {
        "Key": "NUTCH-455",
        "Summary": "dedup on tokenized fields is faulty",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Enis Soztutar",
        "Created": "07/Mar/07 10:07",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "(From LUCENE-252) \nnutch uses several index servers, and the search results from these servers are merged using a dedup field for for deleting duplicates. The values from this field is cached by Lucene's FieldCachImpl. The default is the site field, which is indexed and tokenized. However for a Tokenized Field (for example \"url\" in nutch), FieldCacheImpl returns an array of Terms rather that array of field values, so dedup'ing becomes faulty. Current FieldCache implementation does not respect tokenized fields , and as described above caches only terms. \nSo in the situation that we are searching using \"url\" as the dedup field, when a Hit is constructed in IndexSearcher, the dedupValue becomes a token of the url (such as \"www\" or \"com\") rather that the whole url. This prevents using tokenized fields in the dedup field. \nI have written a patch for lucene and attached it in http://issues.apache.org/jira/browse/LUCENE-252, this patch fixes the aforementioned issue about tokenized field caching. However building such a cache for about 1.5M documents takes 20+ secs. The code in IndexSearcher.translateHits() starts with\nif (dedupField != null) \n      dedupValues = FieldCache.DEFAULT.getStrings(reader, dedupField);\nand for the first call of search in IndexSearcher, cache is built. \nLong story short, i have written a patch against IndexSearcher, which in constructor warms-up the caches of wanted fields(configurable). I think we should vote for LUCENE-252, and then commit the above patch with the last version of lucene.",
        "Issue Links": []
    },
    "NUTCH-456": {
        "Key": "NUTCH-456",
        "Summary": "parse msexcel plugin speedup",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Heiko Dietze",
        "Created": "08/Mar/07 09:19",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "10/May/07 16:15",
        "Description": "The parse-msexcel plugin appends all table cells to one String. Currently this is done with += operation. Replace this with the StringBuffer. This will improve the speed of this parse plugin, because the number new objects and internal copy operations is reduced drastically.",
        "Issue Links": [
            "/jira/browse/NUTCH-473"
        ]
    },
    "NUTCH-457": {
        "Key": "NUTCH-457",
        "Summary": "Create top level dist directory and checkin KEYS file to subversion be standard with Lucene Java and Hadoop",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "08/Mar/07 20:10",
        "Updated": "22/May/13 03:54",
        "Resolved": "14/May/07 15:16",
        "Description": "The KEYS file contains public keys of committers and is used to sign releases.  According to a recent conversation with Doug, \"the convention we're moving towards in the Lucene project is that each sub-project should maintain this at https://svn.apache.org/repos/asf/lucene/SUB_PROJECT/dist/KEYS, then check this out in people.apache.org:/www/www.apache.org/dist/lucene/SUB_PROJECT.  Lucene Java and Hadoop currently implement this, Nutch does not yet.  So currently the KEYS file for Nutch simply lives in the above named directory on people.apache.org but is not backed up in subversion.\"\nI recommend that a top level dist directory be created and the KEYS file for Nutch be checked out into it to both implement the convention and to have the KEYS file backed up in source control.  \nI can go ahead and make these changes if there are no objections.",
        "Issue Links": []
    },
    "NUTCH-458": {
        "Key": "NUTCH-458",
        "Summary": "Proxy forwarding to nutch.war does not work. Need to add some code...",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "My Nutch",
        "Created": "12/Mar/07 16:48",
        "Updated": "01/Apr/11 15:26",
        "Resolved": "01/Apr/11 15:26",
        "Description": "In nutch.war in all jsp files that have this:\nString base = requestURI.substring(0, requestURI.lastIndexOf('/'));\nYou need to change to this:\nString forwardedHost = request.getHeader(\"X-Forwarded-Host\");\nString base;\nif(forwardedHost == null)\n{\nbase = requestURI.substring(0, requestURI.lastIndexOf('/'));\n}\nelse\n{\nbase = request.getScheme() + \"://\" + forwardedHost;\n}\ni.e.: You need to see if the request URI [header] has been modified by a proxy server and if so then use the forwarded host header of that proxy. Otherwise the HTML page will have a base of an internal computer that outsiders can not see. This is only a problem because your URLs are not relative but rather you are using a \"base\" tag, which is also not the best thing to do. Anyways, I searched the maillist archive and other people had this problem and hard-wired their base, but that is not the right way to do it. The code above will fix this problem, i.e.: I verified it works for Apache 2 proxy server in front of a tomcat app server that runs nutch.",
        "Issue Links": []
    },
    "NUTCH-459": {
        "Key": "NUTCH-459",
        "Summary": "Upgrade Nutch to Hadoop 0.12.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "15/Mar/07 14:21",
        "Updated": "19/Mar/07 23:51",
        "Resolved": "19/Mar/07 23:51",
        "Description": "This JIRA contains the new hadoop-0.12.1-dev-core.jar as of revision 518636.  I far as I can tell this jar doesn't break any of the current Nutch trunk code as of revision 517382.",
        "Issue Links": []
    },
    "NUTCH-460": {
        "Key": "NUTCH-460",
        "Summary": "RDF parser plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Ricardo Rocha",
        "Created": "17/Mar/07 04:42",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "I've written a couple plugins that I'd like to contribute.  RDFLinkParseFilter looks for links on the pages that point towards RDF information, and tags the pages with metadata about the type of links they hold. RDFLinkIndexingFilter indexes said metadata.  RDFParser parses RDF information from several possible formats using Jena, and extracts the links that the file points to as Outlinks so that they can be fetched as well.",
        "Issue Links": []
    },
    "NUTCH-461": {
        "Key": "NUTCH-461",
        "Summary": "microformats-reltag plugin and relative links",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.7.2,                                            0.8.1,                                            0.8.2,                                            0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jerome Charron",
        "Created": "19/Mar/07 22:08",
        "Updated": "01/Apr/11 15:26",
        "Resolved": "01/Apr/11 15:26",
        "Description": "The microformats-reltag plugin doesn't extract tags from relative URLs.\nIn fact, the code tries to construct a valid URL from the href. If the href is relative, the URL construction crash and then the tag is not extracted.\nSolution: Simply use a fake base for URL construction.",
        "Issue Links": []
    },
    "NUTCH-462": {
        "Key": "NUTCH-462",
        "Summary": "Noarchive urls are available via the cache link",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "0.8.1",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Steve Severance",
        "Created": "20/Mar/07 02:39",
        "Updated": "20/Mar/07 18:40",
        "Resolved": "20/Mar/07 18:40",
        "Description": "If a robots.txt file specifies a Noarchive statement then urls that or contained as part of that path should not be available via the cached link.\nFor example Noarchive:/ means that no pages should be available via the cached link.",
        "Issue Links": []
    },
    "NUTCH-463": {
        "Key": "NUTCH-463",
        "Summary": "Nutch powerpoint parser plugin fails to parse ppt with images",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "W Fong",
        "Created": "20/Mar/07 22:25",
        "Updated": "09/Aug/11 15:19",
        "Resolved": "09/Aug/11 15:19",
        "Description": "With powerpoint presentations that have images, the parser seems to treat images as if they are text and tries to index it resulting in maxFieldLength being reached.\nThe lines from the crawl log file for the powerpoint in question:\n Indexing http://127.0.0.1/ with analyzer org.apache.nutch.analysis.NutchDocumentAnalyzer@1ce85c4 (null)\n Indexing http://127.0.0.1/wiki/images/0/01/Customer.ppt with analyzer org.apache.nutch.analysis.NutchDocumentAnalyzer@1ce85c4 (null)\nmaxFieldLength 10000 reached, ignoring following tokens\nThe parser should extract only the text and skip the images.",
        "Issue Links": []
    },
    "NUTCH-464": {
        "Key": "NUTCH-464",
        "Summary": "Commandline Search",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "chsanthoshkumar",
        "Created": "27/Mar/07 05:08",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "28/Mar/07 06:03",
        "Description": "we have gone through this http://issues.apache.org/jira/browse/NUTCH-330.\nWith script given in the above issue was fetching the results like title and url , In our case we need to display the Summary also.\nIs there any way to obtain it from the given script name(CommandLineSearch.java.)\nPlease help me in achieve this.\nthanks & regards,\nSanthosh.Ch",
        "Issue Links": []
    },
    "NUTCH-465": {
        "Key": "NUTCH-465",
        "Summary": "I download nutch 0.9 used tar zxvf nutch-0.9.tar.gz   at last  A lone zero block",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "qiuwenbin",
        "Created": "27/Mar/07 10:53",
        "Updated": "08/Nov/07 19:10",
        "Resolved": "08/Nov/07 19:10",
        "Description": "I download nutch 0.9 used tar zxvf nutch-0.9.tar.gz   at last  A lone zero block",
        "Issue Links": []
    },
    "NUTCH-466": {
        "Key": "NUTCH-466",
        "Summary": "Flexible segment format",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "01/Apr/07 20:42",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "In many situations it is necessary to store more data associated with pages than it's possible now with the current segment format. Quite often it's a binary data. There are two common workarounds for this: one is to use per-page metadata, either in Content or ParseData, the other is to use an external independent database using page ID-s as foreign keys.\nCurrently segments can consist of the following predefined parts: content, crawl_fetch, crawl_generate, crawl_parse, parse_text and parse_data. I propose a third option, which is a natural extension of this existing segment format, i.e. to introduce the ability to add arbitrarily named segment \"parts\", with the only requirement that they should be MapFile-s that store Writable keys and values. Alternatively, we could define a SegmentPart.Writer/Reader to accommodate even more sophisticated scenarios.\nExisting segment API and searcher API (NutchBean, DistributedSearch Client/Server) should be extended to handle such arbitrary parts.\nExample applications:\n\nstoring HTML previews of non-HTML pages, such as PDF, PS and Office documents\nstoring pre-tokenized version of plain text for faster snippet generation\nstoring linguistically tagged text for sophisticated data mining\nstoring image thumbnails\n\netc, etc ...\nI'm going to prepare a patchset shortly. Any comments and suggestions are welcome.",
        "Issue Links": []
    },
    "NUTCH-467": {
        "Key": "NUTCH-467",
        "Summary": "DeleteDuplicate fails if Segment index directory has 0 documents",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dennis Kubes",
        "Created": "04/Apr/07 14:21",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "09/May/07 18:04",
        "Description": "If any of the segment indexes have 0 documents, then the DDRecordReader in DeleteDuplicates throws an IndexOutOfBoundsException.  The record reader needs to check for empty document segment indexes.",
        "Issue Links": []
    },
    "NUTCH-468": {
        "Key": "NUTCH-468",
        "Summary": "Scoring filter should distribute score to all outlinks at once",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "09/Apr/07 18:58",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Jun/07 09:29",
        "Description": "Currently ScoringFilter.distributeScoreToOutlink, as its name implies, takes only a single outlink and works on that. I would suggest that we change it to distributeScoreToOutlink_s_ so that it would take all the outlinks of a page at once. This has several advantages:\n1) A ScoringFilter plugin returns a single adjust datum to set its score instead of returning several.\n2) A ScoringFilter plugin can change the score of the original page (via adjust datum) even if there are no outlinks. This is useful if you have a ScoringFilter plugin that, say, scores pages based on content instead of outlinks.\n3) Since the ScoringFilter plugin recieves all outlinks at once, it can make better decisions on how to distribute the score. For example, right now it is not possible to create a plugin that always distributes exactly a page's 'cash' to outlinks(that is, if a page has score 5, it will always distribute exactly 5 points to its outlinks no matter what the internal/external factors are) if internal / external score factors are not 1.",
        "Issue Links": []
    },
    "NUTCH-469": {
        "Key": "NUTCH-469",
        "Summary": "changes to geoPosition plugin to make it work on nutch 0.9",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Mike Schwartz",
        "Created": "23/Apr/07 22:29",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "I have modified the geoPosition plugin (http://wiki.apache.org/nutch/GeoPosition) code to work with nutch 0.9.  (The code was built originally using nutch 0.7.)  I'd like to contribute my changes back to the nutch project.  I already communicated with the code's author (Matthias Jaekle), and he agrees with my mods.",
        "Issue Links": []
    },
    "NUTCH-470": {
        "Key": "NUTCH-470",
        "Summary": "Adding optional terms to a query",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trond Andersen",
        "Created": "24/Apr/07 07:11",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:36",
        "Description": "I'm missing API to add optional terms in the query class. Made a small adjustment to the API to support this.",
        "Issue Links": []
    },
    "NUTCH-471": {
        "Key": "NUTCH-471",
        "Summary": "Fix synchronization in NutchBean creation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Enis Soztutar",
        "Created": "24/Apr/07 08:09",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "14/Jul/07 13:03",
        "Description": "NutchBean is created and then cached in servlet context. But NutchBean.get(ServletContext app, Configuration conf) is not syncronized, which causes more than one instance of the bean (and DistributedSearch$Client) if servlet container is accessed rapidly during startup.",
        "Issue Links": []
    },
    "NUTCH-472": {
        "Key": "NUTCH-472",
        "Summary": "NullPointerException in ZipTextExtractor if no MIME type for zipped file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Antony Bowesman",
        "Created": "24/Apr/07 11:55",
        "Updated": "01/Apr/11 14:58",
        "Resolved": "01/Apr/11 14:58",
        "Description": "extractText throws a NPE in\n          String contentType = MIME.getMimeType(fname).getName();\nif the file in the zip has no configured mime type which breaks the parsing of the zip.\nCode should do:\n  public String extractText(InputStream input, String url, List outLinksList) throws IOException {\n    String resultText = \"\";\n    byte temp;\n    ZipInputStream zin = new ZipInputStream(input);\n    ZipEntry entry;\n    while ((entry = zin.getNextEntry()) != null) {\n      if (!entry.isDirectory()) {\n        int size = (int) entry.getSize();\n        byte[] b = new byte[size];\n        for(int x = 0; x < size; x++) {\n          int err = zin.read();\n          if(err != -1) \n{\n            b[x] = (byte)err;\n          }\n        }\n        String newurl = url + \"/\";\n        String fname = entry.getName();\n        newurl += fname;\n        URL aURL = new URL(newurl);\n        String base = aURL.toString();\n        int i = fname.lastIndexOf('.');\n        if (i != -1) {\n          // Trying to resolve the Mime-Type\n          MimeType mt = MIME.getMimeType(fname);\n          if (mt != null) {\n            String contentType = mt.getName();\n            try {\n              Metadata metadata = new Metadata();\n              metadata.set(Response.CONTENT_LENGTH, Long.toString(entry.getSize()));\n              metadata.set(Response.CONTENT_TYPE, contentType);\n              Content content = new Content(newurl, base, b, contentType, metadata, this.conf);\n              Parse parse = new ParseUtil(this.conf).parse(content);\n              ParseData theParseData = parse.getData();\n              Outlink[] theOutlinks = theParseData.getOutlinks();\n              for(int count = 0; count < theOutlinks.length; count++) \n{\n                outLinksList.add(new Outlink(theOutlinks[count].getToUrl(), theOutlinks[count].getAnchor(), this.conf));\n              }\n\n              resultText += entry.getName() + \" \" + parse.getText() + \" \";\n            } catch (ParseException e) {\n              if (LOG.isInfoEnabled()) \n{ \n               LOG.info(\"fetch okay, but can't parse \" + fname + \", reason: \" + e.getMessage());\n              }\n            }\n          } else \n{\n              resultText += entry.getName();\n          }\n        }\n      }\n    }\n    return resultText;\n  }",
        "Issue Links": []
    },
    "NUTCH-473": {
        "Key": "NUTCH-473",
        "Summary": "ExcelExtractor performance bad due to String concatenation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Antony Bowesman",
        "Created": "24/Apr/07 12:03",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "24/Apr/07 14:45",
        "Description": "Using 0.9 version of ExcelExtractor was still running after 4 hours at 100% CPU trying to extract the text from a 3MB Excel file containing 26 sheets, half with a matrix of approx 1100 rows x P columns and the others with approx 1000 rows x E columns.\nAfter changing ExcelExtractor to use StringBuffer the same extraction process took 3 seconds under Java 1.5.  Code changes below - example uses a 4K buffer per sheet - this was a completely arbitrary choice but keeps the number of StringBuffer expansions low for large files without using too much space for small files.\n  protected String extractText(InputStream input) throws Exception {\n    String resultText = \"\";\n    HSSFWorkbook wb = new HSSFWorkbook(input);\n    if (wb == null) \n{\n      return resultText;\n    }\n\n    HSSFSheet sheet;\n    HSSFRow row;\n    HSSFCell cell;\n    int sNum = 0;\n    int rNum = 0;\n    int cNum = 0;\n    sNum = wb.getNumberOfSheets();\n    //  Allow 4K per sheet - seems a reasonable start \n    StringBuffer sb = new StringBuffer(4096 * sNum);\n    for (int i=0; i<sNum; i++) {\n      if ((sheet = wb.getSheetAt) == null) \n{\n        continue;\n      }\n      rNum = sheet.getLastRowNum();\n      for (int j=0; j<=rNum; j++) {\n        if ((row = sheet.getRow(j)) == null)\n{\n          continue;\n        }\n        cNum = row.getLastCellNum();\n        for (int k=0; k<cNum; k++) {\n          if ((cell = row.getCell((short) k)) != null) {\n            /*if(HSSFDateUtil.isCellDateFormatted(cell) == true) \n{\n                resultText += cell.getDateCellValue().toString() + \" \";\n              }\n else\n             */\n            if (cell.getCellType() == HSSFCell.CELL_TYPE_STRING) \n{\n                sb.append(cell.getStringCellValue());\n                sb.append(' ');\n//              resultText += cell.getStringCellValue() + \" \";\n            }\n else if (cell.getCellType() == HSSFCell.CELL_TYPE_NUMERIC) \n{\n              Double d = new Double(cell.getNumericCellValue());\n              sb.append(d.toString());\n              sb.append(' ');\n//              resultText += d.toString() + \" \";\n            }\n            /* else if(cell.getCellType() == HSSFCell.CELL_TYPE_FORMULA)\n{\n                 resultText += cell.getCellFormula() + \" \";\n               }\n \n             */\n          }\n        }\n      }\n    }\n    return sb.toString();\n  }",
        "Issue Links": [
            "/jira/browse/NUTCH-456"
        ]
    },
    "NUTCH-474": {
        "Key": "NUTCH-474",
        "Summary": "Fetcher2 sets server-delay and blocking checks incorrectly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dogacan Guney",
        "Created": "24/Apr/07 14:09",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Apr/07 21:33",
        "Description": "1) Fetcher2 sets server delay incorrectly. It sets the delay to minCrawlDelay if maxThreads == 1 and to crawlDelay otherwise. Correct behaviour should be the opposite.\n2) Fetcher2 sets wrong configuration options so host blocking is still handled by the lib-http plugin (Fetcher2 is designed to handle blocking internally).",
        "Issue Links": []
    },
    "NUTCH-475": {
        "Key": "NUTCH-475",
        "Summary": "Adaptive crawl delay",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dogacan Guney",
        "Created": "25/Apr/07 12:07",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Current fetcher implementation waits a default interval before making another request to the same server (if crawl-delay is not specified in robots.txt). IMHO, an adaptive implementation will be better. If the server is under little load and can server requests fast, then fetcher can ask for more pages in a given interval. Similarly, if the server is suffering from heavy load, fetcher can slow down(w.r.t that host), easing the load on the server.",
        "Issue Links": []
    },
    "NUTCH-476": {
        "Key": "NUTCH-476",
        "Summary": "Would like to add a field to the document class for its MD5 signature",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Linh Pham",
        "Created": "27/Apr/07 21:25",
        "Updated": "17/Jun/07 09:17",
        "Resolved": "17/Jun/07 09:17",
        "Description": "During indexing a file, if an MD5 signature was calculated and stored along with the document  as a default,\nit could then be used to remove duplicates from the results on retrieval.",
        "Issue Links": []
    },
    "NUTCH-477": {
        "Key": "NUTCH-477",
        "Summary": "Extend URLFilters to support different filtering chains",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "03/May/07 21:52",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "I propose to make the following changes to URLFilters:\n\nextend URLFilters so that they support different filtering rules depending on the context where they are executed. This functionality mirrors the one that URLNormalizers already support.\n\n\nchange their return value to an int code, in order to support early termination of long filtering chains.",
        "Issue Links": []
    },
    "NUTCH-478": {
        "Key": "NUTCH-478",
        "Summary": "Add function for stopping FetherThread gracefully",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "chee.wu",
        "Created": "05/May/07 06:27",
        "Updated": "13/Apr/11 22:58",
        "Resolved": "13/Apr/11 22:58",
        "Description": "Now the fetch process will be  stopped only when  time out occurred during the fetch:\n\"System.currentTimeMillis() - lastRequestStart.get()) > timeout \"\nWe don't have method to let fetch process to stop.Some times we may have strict time requirement for fetch process, for example from 11pm to 7am.I want to shutdown fetch process at 7am every day even there  still have pages remained unfeched in the segments generated.\nA possible solution to implement this might be:\n1. User create a file named \"FetchStop\" in nutch home.\n2. Check the existence of the file every minute in the main thread,and set the boolean variable like \"stopFetch\" to true;\n3. FetchThread will check  the status of \"stopFetch\" before fetching next URL. If changed to true, FetcherThread will stop right now,also the value of activeThreads will be reduced.\n4. Finally, the main thread will end if  activeThreads=0",
        "Issue Links": []
    },
    "NUTCH-479": {
        "Key": "NUTCH-479",
        "Summary": "Support for OR queries",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "07/May/07 19:14",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "There have been many requests from users to extend Nutch query syntax to add support for OR queries, in addition to the implicit AND and NOT queries supported now.",
        "Issue Links": []
    },
    "NUTCH-480": {
        "Key": "NUTCH-480",
        "Summary": "Searching multiple indexes with a single nutch instance",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.8",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Ravi Chintakunta",
        "Created": "08/May/07 01:10",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "Searching across multiple indexes with a single instance of Nutch is a cool feature improvement. I had this requirement for my production site, where we wanted to list the available categories (indexes) to search as check boxes and the user could select any combination of indexes to search.  The results page also displays the number of hits in each index.\nTo do this:\n\nI modified web.xml to include the paths to various search indexes\nModified Nutch.java to read all the indexes and create IndexReaders\nModified IndexSearcher.java to handle multiple IndexReaders\n\nIn the attached file you will find the patch to the Nutch 0.8 code base and also the newly added files:\n\nSearchServlet - a servlet that is the web interface for search. This is simplified version of jsp versions (without the i18n) and outputs the results in text, xml or json format.\nSearchConstants - an interface for messages and constants\n\nPlease note that the patch includes the functionality for spell check - aka \"Did you mean?\"",
        "Issue Links": []
    },
    "NUTCH-481": {
        "Key": "NUTCH-481",
        "Summary": "http.content.limit is broken in the protocol-httpclient plugin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "charlie wanek",
        "Created": "11/May/07 18:23",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "04/Jan/08 19:51",
        "Description": "When using the protocol-httpclient plugin, the entire contents of the request URL is retrieved, regardless of the http.content.limit configuration setting.  (The issue does not affect the protocol-http plugin.)\nFor very large documents, this leads the Fetcher to believe that the FetcherThread is hung, and the Fetcher aborts its run, logging a warning about hung threads (Fetcher.java:433).\norg.apache.nutch.protocol.httpclient.HttpResponse is properly counting the content length, and is breaking its read loop at the proper point.\nHowever, when HttpResponse closes the InputStream from which it is reading, the InputStream object (an org.apache.commons.httpclient.AutoCloseInputStream) continues to read all of the content of the document from the webserver.\nThough I'm not certain this is the correct solution, a quick test shows that if HttpResponse is changed to abort the GET, the InputStream correctly aborts the read from the webserver, and the FetcherThread can continue.",
        "Issue Links": [
            "/jira/browse/NUTCH-559"
        ]
    },
    "NUTCH-482": {
        "Key": "NUTCH-482",
        "Summary": "Remove redundant plugin lib-log4j",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "12/May/07 07:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "14/May/07 14:38",
        "Description": "I think it's safe to remove since compatible log4j library is already available in top level",
        "Issue Links": []
    },
    "NUTCH-483": {
        "Key": "NUTCH-483",
        "Summary": "remove redundant commons-logging jar from ontology plugin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "12/May/07 07:55",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "14/May/07 14:52",
        "Description": "I think it's safe to remove since compatible commons-logging library is already available in top level",
        "Issue Links": []
    },
    "NUTCH-484": {
        "Key": "NUTCH-484",
        "Summary": "Nutch Nightly API link is broken in site",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Gal Nitzan",
        "Created": "12/May/07 08:59",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "13/May/07 14:56",
        "Description": "The Nightly API link is broken",
        "Issue Links": []
    },
    "NUTCH-485": {
        "Key": "NUTCH-485",
        "Summary": "Change HtmlParseFilter 's to return ParseResult object instead of Parse object",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Gal Nitzan",
        "Created": "12/May/07 19:50",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Jun/07 20:29",
        "Description": "The current implementation of HtmlParseFilters.java doesn't allow a filter to add parse objects to the ParseResult object.\nA change to the HtmlParseFilter is needed which allows the filter to return ParseResult . and ofcourse a change to  HtmlParseFilters .",
        "Issue Links": []
    },
    "NUTCH-486": {
        "Key": "NUTCH-486",
        "Summary": "Break searcher dependency on commons-cli",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Mark Woon",
        "Created": "14/May/07 23:34",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "31/May/07 18:59",
        "Description": "0.9 introduced what seems like an unnecessary dependency on commons-cli:\njava.lang.NoClassDefFoundError: org/apache/commons/cli/ParseException\n        at org.apache.nutch.searcher.LinkDbInlinks.<init>(LinkDbInlinks.java:42)\n        at org.apache.nutch.searcher.NutchBean.init(NutchBean.java:156)\n        at org.apache.nutch.searcher.NutchBean.<init>(NutchBean.java:106)\n        at org.apache.nutch.searcher.NutchBean.<init>(NutchBean.java:84)\n        at org.apache.nutch.searcher.NutchBean.get(NutchBean.java:71)\nCan we please break this dependency?",
        "Issue Links": []
    },
    "NUTCH-487": {
        "Key": "NUTCH-487",
        "Summary": "Neko HTML parser goes on default settings.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Marcin Okraszewski",
        "Created": "21/May/07 14:04",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "26/Sep/07 14:06",
        "Description": "The Neko HTML parser set up is done in silent try / catch statement (Nutch 0.9: HtmlParser.java:248-259). The problem is that the first feature being set thrown an exception. So, the whole setup block is skipped. The catch statement does nothing, so probably nobody noticed this.\nI attach a patch which fixes this. It was done on Nutch 0.9, but SVN trunk contains the same code.\nThe patch does:\n1. Fixes augmentations feature.\n2. Removes include-comments feature, because I couldn't find anything similar at http://people.apache.org/~andyc/neko/doc/html/settings.html\n3. Prints warn message when exception is caught.\nPlease note that now there goes a lot for messages to console (not log4j log), because \"report-errors\" feature is being set. Shouldn't it be removed?",
        "Issue Links": []
    },
    "NUTCH-488": {
        "Key": "NUTCH-488",
        "Summary": "Avoid parsing uneccessary links and get a more relevant outlink list",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Emmanuel Joke",
        "Reporter": "Emmanuel Joke",
        "Created": "22/May/07 07:38",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "18/Oct/07 16:55",
        "Description": "NekoHTML parser use a method to extract all outlinks from the HTML page. It will extracts them from the HTML content based on the list of param defined in the method setConf(). Then this list of links will be truncated to be limit to the the maximum number of outlinks that we'll process for a page defined in nutch-default.xml (db.max.outlinks.per.page = 100 by default ) and finally it will be go through all urlfilter defined.\nUnfortunetly it can happen that the list of outlinks is more than 100, so it will truncated the list and could remove some relevant links.\nSo I've added few options in the nutch-default.xml in order to enable/disable the extraction of specific HTML Tag links in this parser (SCRIPT, IMG, FORM, LINK).",
        "Issue Links": []
    },
    "NUTCH-489": {
        "Key": "NUTCH-489",
        "Summary": "URLFilter-suffix management of the url path when the url contains some query parameters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Emmanuel Joke",
        "Created": "22/May/07 08:33",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "18/Jun/07 18:14",
        "Description": "The current filter compares only on string level. It try to apply the filter to the full URL (path + query parameters).\nSo, even if we have define in our filter to exclude all js extension, it won't exclude this URL http://www.toto.com/from.js?id=5.\nI've added a new parameter in the filter which can be use to configure the filter to exclude URL based on the url path.",
        "Issue Links": []
    },
    "NUTCH-490": {
        "Key": "NUTCH-490",
        "Summary": "Extension point with filters for Neko HTML parser (with patch)",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Marcin Okraszewski",
        "Created": "22/May/07 12:16",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "In my project I need to set filters for Neko HTML parser. So instead of adding it hard coded, I made an extension point to define filters for Neko. I was fallowing the code for HtmlParser filters. In fact the method to get filters I think could be generalized to handle both cases. But I didn't want to make too big mess.\nThe attached patch is for Nutch 0.9. This part of code wasn't changed in trunk, so should be applicable easily.\nBTW. I wonder if it wouldn't be best to have HTML DOM Parsing defined by extension point itself. Now there are options for Neko and TagSoap. But if someone would like to use something else or set give different settings for the parser, he would need to modify HtmlParser class, instead of replacing a plugin.",
        "Issue Links": []
    },
    "NUTCH-491": {
        "Key": "NUTCH-491",
        "Summary": "dedup fails with ArrayIndexOutOfBoundsException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Nicol\u00e1s Lichtmaier",
        "Created": "23/May/07 16:51",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "10/Sep/07 19:48",
        "Description": "When trying to run dedup on an index I'm getting:\njava.lang.ArrayIndexOutOfBoundsException: Array index out of range: 426065\n        at org.apache.lucene.util.BitVector.get(BitVector.java:72)\n        at org.apache.lucene.index.SegmentReader.isDeleted(SegmentReader.java:346)\n        at org.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReader.next(DeleteDuplicates.java:176)\n        at org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)\n2007-05-23 09:45:04,620 FATAL indexer.DeleteDuplicates - DeleteDuplicates: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:604)\n        at org.apache.nutch.indexer.DeleteDuplicates.dedup(DeleteDuplicates.java:439)\n        at org.apache.nutch.indexer.DeleteDuplicates.run(DeleteDuplicates.java:506)\n        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)\n        at org.apache.nutch.indexer.DeleteDuplicates.main(DeleteDuplicates.java:490)",
        "Issue Links": []
    },
    "NUTCH-492": {
        "Key": "NUTCH-492",
        "Summary": "java.lang.OutOfMemoryError while indexing.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Nicol\u00e1s Lichtmaier",
        "Created": "26/May/07 23:41",
        "Updated": "18/Jun/07 08:57",
        "Resolved": "18/Jun/07 08:57",
        "Description": "I'm getting this:\njava.lang.OutOfMemoryError: Java heap space\n        at java.util.Arrays.copyOf(Arrays.java:2786)\n        at java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:94)\n        at java.io.DataOutputStream.write(DataOutputStream.java:90)\n        at org.apache.hadoop.io.Text.writeString(Text.java:399)\n        at org.apache.nutch.metadata.Metadata.write(Metadata.java:225)\n        at org.apache.nutch.parse.ParseData.write(ParseData.java:165)\n        at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:154)\n        at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65)\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:315)\n        at org.apache.nutch.indexer.Indexer.map(Indexer.java:306)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:126)\n2007-05-26 11:07:22,517 FATAL indexer.Indexer - Indexer: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:604)\n        at org.apache.nutch.indexer.Indexer.index(Indexer.java:273)\n        at org.apache.nutch.indexer.Indexer.run(Indexer.java:295)\n        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)\n        at org.apache.nutch.indexer.Indexer.main(Indexer.java:278)\nSomething weird I'm seeing in hadoop.log is that the plugins are loaded again and again. I've created a custom plugin (if that can be causing something). According to the code a nre plugin repository is created for each \"configuration object\". I'm sure I'm not modifying the configuration object in any part of my code (I've checked).\nWhy are the plugins loaded again and again and again until the heap is full?",
        "Issue Links": []
    },
    "NUTCH-493": {
        "Key": "NUTCH-493",
        "Summary": "contentType parse not correctly,,,,got empty content using readseg -get",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "wangxu",
        "Created": "30/May/07 00:05",
        "Updated": "18/Jun/07 09:00",
        "Resolved": "18/Jun/07 09:00",
        "Description": "I am using nutch0.9.\nI found lots of my crawled pages's contents are empty.\nthen I checked the log,and find the warnning accordingly:the ContentType is said to be \"url=http://......\",and cannot \nfind a suitable parser for the page:\nparser not found for contentType=\nurl=http://product.dangdang.com/product.aspx?product_id=490321\nthen most of this kind of pages's contents are empty.\nbut I didnot find any warn or error other than \"timeout\" from the fetcher log.\nCan somebody explain me why?\nmany thanks!",
        "Issue Links": []
    },
    "NUTCH-494": {
        "Key": "NUTCH-494",
        "Summary": "FindBugs: CrawlDbReader and DeleteDuplicates",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "31/May/07 08:50",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Nov/07 19:14",
        "Description": "Since FindBugs is so hot and everything right now, I thought I would run it against Nutch. I have found a couple of correctness bugs in CrawlDbReader and DeleteDuplicates that (even though FindBugs marked them as medium priority) looked important to me.",
        "Issue Links": []
    },
    "NUTCH-495": {
        "Key": "NUTCH-495",
        "Summary": "Unnecessary delays in Fetcher2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "31/May/07 15:49",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "16/Jun/07 10:35",
        "Description": "Even if a url is blocked by robots.txt (or has a crawl delay larger that max.crawl.delay), Fetcher2 still waits fetcher.server.delay before fetching another url from same host, which is not necessary, considering that Fetcher2 didn't make a request to server anyway.",
        "Issue Links": []
    },
    "NUTCH-496": {
        "Key": "NUTCH-496",
        "Summary": "ConcurrentModificationException can be thrown when getSorted() is called.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Briggs",
        "Created": "04/Jun/07 16:33",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "NGramProfile (within the org.apache.nutch.analysis.lang) package is not thread-safe due to a ConcurrentModificationException that can occur if during iteration of the resultant List from getSorted() and another call to getSorted() is invoked from within another thread.",
        "Issue Links": []
    },
    "NUTCH-497": {
        "Key": "NUTCH-497",
        "Summary": "Extreme Nested Tags causes StackOverflowException in DomContentUtils...Spider Trap",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1,                                            0.9.0,                                            1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "06/Jun/07 23:33",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "26/Jun/07 03:34",
        "Description": "Some webpages have a form of a spider trap that causes a StackOverflowException in DomContentUtils by having nested tags with thousands of layers deep.  DomContentUtils when trying to get outlinks uses a recursive method to parse the html.  With this type of nesting it errors out.",
        "Issue Links": [
            "/jira/browse/NUTCH-555"
        ]
    },
    "NUTCH-498": {
        "Key": "NUTCH-498",
        "Summary": "Use Combiner in LinkDb to increase speed of linkdb generation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "linkdb",
        "Assignee": "Dogacan Guney",
        "Reporter": "Espen Amble Kolstad",
        "Created": "14/Jun/07 08:05",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Jun/07 12:46",
        "Description": "I tried to add the follwing combiner to LinkDb\n   public static enum Counters \n{COMBINED}\n\n   public static class LinkDbCombiner extends MapReduceBase implements Reducer {\n      private int _maxInlinks;\n      @Override\n      public void configure(JobConf job) \n{\n         super.configure(job);\n         _maxInlinks = job.getInt(\"db.max.inlinks\", 10000);\n      }\n\n      public void reduce(WritableComparable key, Iterator values, OutputCollector output, Reporter reporter) throws IOException {\n            final Inlinks inlinks = (Inlinks) values.next();\n            int combined = 0;\n            while (values.hasNext()) {\n               Inlinks val = (Inlinks) values.next();\n               for (Iterator it = val.iterator(); it.hasNext() {\n                  if (inlinks.size() >= _maxInlinks) {\n                     if (combined > 0) \n{\n                        reporter.incrCounter(Counters.COMBINED, combined);\n                     }\n                     output.collect(key, inlinks);\n                     return;\n                  }\n                  Inlink in = (Inlink) it.next();\n                  inlinks.add(in);\n               }\n               combined++;\n            }\n            if (inlinks.size() == 0) \n{\n               return;\n            }\n            if (combined > 0) \n{\n               reporter.incrCounter(Counters.COMBINED, combined);\n            }\n            output.collect(key, inlinks);\n      }\n   }\nThis greatly reduced the time it took to generate a new linkdb. In my case it reduced the time by half.\nMap output records    8717810541\nCombined                  7632541507\nResulting output rec 1085269034\nThat's a 87% reduction of output records from the map phase",
        "Issue Links": []
    },
    "NUTCH-499": {
        "Key": "NUTCH-499",
        "Summary": "Refactor LinkDb and LinkDbMerger to reuse code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "linkdb",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "16/Jun/07 11:01",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Jun/07 08:39",
        "Description": "LinkDb.Merger.reduce and LinkDb.reduce works the same way. Refactor Nutch so that we can use the same code for both.",
        "Issue Links": []
    },
    "NUTCH-500": {
        "Key": "NUTCH-500",
        "Summary": "Add hadoop masters configuration file into conf folder",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Emmanuel Joke",
        "Created": "18/Jun/07 06:48",
        "Updated": "08/Jun/11 21:32",
        "Resolved": "10/Apr/08 15:24",
        "Description": "Hadoop scripts read a configuration file named masters to know how many namenode should be started.\nThis file is not in the repository for the moment, thus it generate some errors message (error which is not really important)  when we start the cluster.\nAnyway it could be a good idea to add a template file in the conf directory.",
        "Issue Links": []
    },
    "NUTCH-501": {
        "Key": "NUTCH-501",
        "Summary": "Implement a different caching mechanism for objects cached in configuration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "18/Jun/07 12:03",
        "Updated": "02/May/13 02:29",
        "Resolved": "29/Oct/07 14:58",
        "Description": "As per HADOOP-1343, Configuration.setObject and Configuration.getObject (which are used by Nutch to cache arbitrary objects) are deprecated and will be removed soon. We have to implement an alternative caching mechanism and replace all usages of Configuration.\n{getObject,setObject}\n with the new mechanism.",
        "Issue Links": [
            "/jira/browse/NUTCH-552"
        ]
    },
    "NUTCH-502": {
        "Key": "NUTCH-502",
        "Summary": "Bug in SegmentReader causes infinite loop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "19/Jun/07 06:00",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Jun/07 09:22",
        "Description": "A small bug in SegmentReader.get() may lead to an infinite loop.\n...\n    int cnt = 0;\n    do {\n      try \n{\n        Thread.sleep(5000);\n      }\n catch (Exception e) {};\n      it = threads.iterator();\n      while (it.hasNext()) \n{\n        if (((Thread)it.next()).isAlive()) cnt++;\n      }\n      if ((cnt > 0) && (LOG.isDebugEnabled())) \n{\n        LOG.debug(\"(\" + cnt + \" to retrieve)\");\n....\n\n      }\n    } while (cnt > 0); \nIf cnt ever becomes non-zero, SegmentReader gets stuck in that loop.\nThis bug is discovered by Ilya Vishnevsky.",
        "Issue Links": []
    },
    "NUTCH-503": {
        "Key": "NUTCH-503",
        "Summary": "Generator exits incorrectly for small fetchlists",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "generator",
        "Assignee": "Dogacan Guney",
        "Reporter": "Vishal Shah",
        "Created": "21/Jun/07 07:37",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "09/Jul/07 06:47",
        "Description": "I think I found the reason why the generator returns with an empty fetchlist for small fetchsizes. \n   After the first job finishes running, the generator checks the following condition to see if it got an empty list:\n    if (readers == null || readers.length == 0 || !readers[0].next(new\nFloatWritable())) {\n  The third condition is incorrect here. In some cases, esp. for small fetchlists, the first partition might be empty, but some other partition(s) might contain urls. In this case, the Generator is incorrectly assuming that all partitions are empty by just looking at the first. This problem could also occur when all URLs in the fetchlist are from the same host (or from a very small number of hosts, or from a number of hosts that all map to a small number of partitions).",
        "Issue Links": []
    },
    "NUTCH-504": {
        "Key": "NUTCH-504",
        "Summary": "NUTCH-443 broke parsing during fetching",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "22/Jun/07 08:29",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Jun/07 10:04",
        "Description": "After NUTCH-443, if one is parsing during fetching and parsing for a url fails, that url doesn't get segment name or similar properties in its metadata. Because of this, indexer fails (because, index expects to see segment name for all parses, even those that failed).",
        "Issue Links": []
    },
    "NUTCH-505": {
        "Key": "NUTCH-505",
        "Summary": "Outlink urls should be validated",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "23/Jun/07 20:14",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "11/Jul/07 10:54",
        "Description": "See discussion here:\nhttp://www.nabble.com/fetching-http%3A--www.variety.com-%3C-div%3E%3C-a%3E-tf3961692.html\nParse plugins may extract garbage urls from pages. We need a url validation system that tests these urls and filters out garbage.",
        "Issue Links": []
    },
    "NUTCH-506": {
        "Key": "NUTCH-506",
        "Summary": "Nutch should delegate compression to Hadoop",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "29/Jun/07 12:44",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Jul/07 15:18",
        "Description": "Some data structures within nutch (such as Content, ParseText) handle their own compression. We should delegate all compressions to Hadoop. \nAlso, nutch should respect io.seqfile.compression.type setting. Currently even if io.seqfile.compression.type is BLOCK or RECORD, nutch overrides it for some structures and sets it to NONE (However, IMO, ParseText should always be compressed as RECORD because of performance reasons).",
        "Issue Links": []
    },
    "NUTCH-507": {
        "Key": "NUTCH-507",
        "Summary": "lib-lucene-analyzers jar defintion is wrong in plugin.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Emmanuel Joke",
        "Created": "07/Jul/07 17:17",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "09/Jul/07 06:16",
        "Description": "lucene-analyzers-2.2.0.jar has been recently committed but the plugin.xml is still pointing to the old jar which doesn't exist in the lib folder(lucene-analyzers-2.1.0.jar)\nIt is defined as below:\n<plugin\n   id=\"lib-lucene-analyzers\"\n   name=\"Lucene Analysers\"\n   version=\"2.0.0\"\n   provider-name=\"org.apache.lucene\">\n   <runtime>\n     <library name=\"lucene-analyzers-2.1.0.jar\">\n        <export name=\"*\"/>\n     </library>\n   </runtime>\n</plugin>\nIt should be:\n<plugin\n   id=\"lib-lucene-analyzers\"\n   name=\"Lucene Analysers\"\n   version=\"2.2.0\"\n   provider-name=\"org.apache.lucene\">\n   <runtime>\n     <library name=\"lucene-analyzers-2.2.0.jar\">\n        <export name=\"*\"/>\n     </library>\n   </runtime>\n</plugin>",
        "Issue Links": []
    },
    "NUTCH-508": {
        "Key": "NUTCH-508",
        "Summary": "${hadoop.log.dir} and ${hadoop.log.file} are not propagated to the tasktracker",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Emmanuel Joke",
        "Created": "07/Jul/07 17:26",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Oct/07 10:58",
        "Description": "As described in http://www.nabble.com/Crawl-error-with-hadoop-t3994217.html\nthe log4j config file is missing some parameters.\nhadoop.log.dir=.\nhadoop.log.file=hadoop.log\nThanks for the help of Mathijs",
        "Issue Links": []
    },
    "NUTCH-509": {
        "Key": "NUTCH-509",
        "Summary": "Update Crawldb: avoid to start a job if there is no valid segment",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Emmanuel Joke",
        "Created": "08/Jul/07 08:02",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "09/Jul/07 06:16",
        "Description": "UpdateDb is configured to check if there is a valid segment and add it to the job.\nIn the current version, even if there isn't any valid segment, we start a job.",
        "Issue Links": []
    },
    "NUTCH-510": {
        "Key": "NUTCH-510",
        "Summary": "IndexMerger delete working dir",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Enis Soztutar",
        "Created": "09/Jul/07 06:35",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "11/Jul/07 15:31",
        "Description": "IndexMerger does not delete the working dir when an IOException is thrown such as No space left on device. Local temporary directories should be deleted.",
        "Issue Links": []
    },
    "NUTCH-511": {
        "Key": "NUTCH-511",
        "Summary": "Recrawling",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "anuradha",
        "Created": "12/Jul/07 11:38",
        "Updated": "12/Jul/07 15:24",
        "Resolved": "12/Jul/07 15:24",
        "Description": "Hi,\nFirst I have crawled one website.\nI added one page to crawled site. After that I have recrawled the same website.\nI have copied the recrawling the code  from \"http://wiki.apache.org/nutch/IntranetRecrawl#head-e58e25a0b9530bb6fcdfb282fd27a207fc0aff03\"\nBut I didn't get the results from the newly added page.\nI am using nutch 0.9.0 and jvm/java-1.5.0-sun\nPlease guide me how to recrawl the site.\nThanks in advance,\nRegards,\nAnuradha",
        "Issue Links": []
    },
    "NUTCH-512": {
        "Key": "NUTCH-512",
        "Summary": "Search on date range",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "anuradha",
        "Created": "12/Jul/07 11:39",
        "Updated": "12/Jul/07 15:32",
        "Resolved": "12/Jul/07 15:32",
        "Description": "Hi,\nI need to search on date range.\nI am getting 0 hits when tring the url like\nhttp://IPADDRESS:8080/search.jsp?query=date:[20070601-20071204]apache&hitsPerSite=100\u2329=en&hitsPerPage=100\nI am using NUTCH 0.9\nI have been changed on conf/nutch-default.xml like :\n<property>\n  <name>plugin.includes</name>  <value>protocol-http|urlfilter-regex|parse-(text|html|js)|index-(basic|more)|query-(basic|site|url|more)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)</value>\n  <description>Regular expression naming plugin directory names to\n  include.  Any plugin not matching this expression is excluded.\n  In any case you need at least include the nutch-extensionpoints plugin. By\n  default Nutch includes crawling just HTML and plain text via HTTP,\n  and basic indexing and search plugins. In order to use HTTPS please enable\n  protocol-httpclient, but be aware of possible intermittent problems with the\n  underlying commons-httpclient library.\n  </description>\n</property>\nI have been changed on conf/nutch-site.xml like:\n<property>\n    <name>plugin.includes</name> <value>nutch-extensionpoints|protocol-http|language-identifier|urlfilter-regex|parse-(text|html|pdf|msword)|index-(basic|more)|query-(basic|site|url|more)</value>\n    <description>Regular expression naming plugin directory names to include.Any plugin not matching this expression is excluded.\n    </description>\n</property>\nPlease guide me how to search on date range.\nThanks in advance.\nRegards,\nAnuradha",
        "Issue Links": []
    },
    "NUTCH-513": {
        "Key": "NUTCH-513",
        "Summary": "suffix-urlfilter.txt does not have a template",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "12/Jul/07 17:12",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "13/Jul/07 17:20",
        "Description": "Unlike other urlfilter config files, suffix-urlfilter.txt is not generated from a template (suffix-urlfilter.txt.template) file. \nI can just do a \"svn mv\" to rename the file, but I am not sure about the consequences. If I have a locally modified suffix-urlfilter.txt and do a \"svn up\" will my suffix-urlfilter.txt file be deleted too?",
        "Issue Links": []
    },
    "NUTCH-514": {
        "Key": "NUTCH-514",
        "Summary": "Indexer should only index pages with fetch status SUCCESS",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "14/Jul/07 12:09",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "30/Jul/07 19:02",
        "Description": "Currently if you parse during fetch, nutch only parses pages which are successfully (i.e, have a status STATUS_FETCH_SUCCESS). But, if you run parse as a seperate job, nutch parses pages like \"404 not found\"s or \"301 moved\"s. Since most of these can be successfully parsed these are indexed and show up in search results. \nIMO, we should either somehow mark contents so that a separate parse doesn't output non-STATUS_FETCH_SUCCESS pages or we should filter them out in Indexer.",
        "Issue Links": []
    },
    "NUTCH-515": {
        "Key": "NUTCH-515",
        "Summary": "Next fetch time is set incorrectly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "16/Jul/07 12:14",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Jul/07 06:19",
        "Description": "After NUTCH-61 , db.default.fetch.interval option is deprecated and superceded by db.fetch.interval.default. However, various parts in nutch still use the old option. Since old option is in days (with default being 30) and new option in seconds (default is ~250000), when nutch fetches a url, its next fetch time is set as **30 SECONDS** later. This means that nutch keeps refetching same urls over and over and over and over.",
        "Issue Links": []
    },
    "NUTCH-516": {
        "Key": "NUTCH-516",
        "Summary": "Next fetch time is not set when it is a CrawlDatum.STATUS_FETCH_GONE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Emmanuel Joke",
        "Created": "17/Jul/07 12:08",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "26/Jul/07 08:36",
        "Description": "We can not crawl some page due to a robots restriction. In this case we update the db with the Metada: pst:robots_denied(18) , we add the status code 3 and we change the fecth interval to 67.5 days.\nUnfortunetely the Fetch time is never change, so it keeps generating this page and fetching it every time.\nWe should update the schedule fetch in crawldb to reflect to the fetch interval.\nWe should add in crawldbreducer:\ncase CrawlDatum.STATUS_FETCH_GONE:            // permanent failure\n      if (old != null)\n        result.setSignature(old.getSignature());  // use old signature\n      result.setStatus(CrawlDatum.STATUS_DB_GONE);\n      result = schedule.setPageGoneSchedule((Text)key, result, prevFetchTime,\n          prevModifiedTime, fetch.getFetchTime());\n     // set the schedule\n      result = schedule.setFetchSchedule((Text)key, result, prevFetchTime,\n          prevModifiedTime, fetch.getFetchTime(), fetch.getModifiedTime(), modified);\n      break;",
        "Issue Links": []
    },
    "NUTCH-517": {
        "Key": "NUTCH-517",
        "Summary": "build encoding should be UTF-8",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Enis Soztutar",
        "Created": "18/Jul/07 08:08",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "18/Jul/07 18:00",
        "Description": "build encoding send to javac should be UTF-8 so that non-ascii characters can be used in the source code. This issue has emerged from NUTCH-439",
        "Issue Links": []
    },
    "NUTCH-518": {
        "Key": "NUTCH-518",
        "Summary": "Fix OpicScoringFilter to respect scoring filter chaining",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Enis Soztutar",
        "Created": "18/Jul/07 08:14",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "03/Feb/09 13:17",
        "Description": "Opic Scoring returns the score that it calculates, rather than returning previous_score * calculated_score. This prevents using another scoring filter along with Opic scoring.",
        "Issue Links": []
    },
    "NUTCH-519": {
        "Key": "NUTCH-519",
        "Summary": "&nbsp; prased incorrectly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Chris Hane",
        "Created": "18/Jul/07 21:53",
        "Updated": "01/Apr/11 15:26",
        "Resolved": "01/Apr/11 15:26",
        "Description": "I have deployed nutch in a standard configuration without any modifications.\nOn all of the pages that it is crawling on my website, during the parse phase it convertes \u00a0 html entity into \u00c2.\nThe charset is set on the page to be: \n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=windows-1252\">\nWhen I issue the command\nbin/nutch readseg -get demo.crawl/segments/20070718174552/ http://demo.itsolut.com/mr.com/bookstore/maintenancemanagement/wiremanlibrary.htm\nThe HTML portion contains:\n    <tr>\n      <td align=\"center\">\n      <b><font face=\"Arial\" size=\"0\">Address: 120 South\n7th Street\u00a0 -\u00a0 Terre Haute, IN 47807</font></b>\n      </td>\n    </tr>\nand the parsed content is:\n Address: 120 South 7th Street\u00c2  -\u00c2  Terre Haute, IN 47807\nAlso, the output contains the following:\nParse Metadata: OriginalCharEncoding=windows-1252 CharEncodingForConversion=windows-1252",
        "Issue Links": []
    },
    "NUTCH-520": {
        "Key": "NUTCH-520",
        "Summary": "A common infrastructure for different index backends",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Dogacan Guney",
        "Created": "19/Jul/07 08:47",
        "Updated": "31/Jul/07 13:20",
        "Resolved": "31/Jul/07 13:20",
        "Description": "With the discussion of solr as a possible index and search backend, I think we need a new indexing architecture (that doesn't depend on lucene) that can use multiple backends to index.",
        "Issue Links": []
    },
    "NUTCH-521": {
        "Key": "NUTCH-521",
        "Summary": "Modified injector to allow newly injected CrawlDatum to overwrite original",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Rob Young",
        "Created": "19/Jul/07 09:49",
        "Updated": "01/Apr/11 15:26",
        "Resolved": "01/Apr/11 15:26",
        "Description": "Before this patch if a CrawlDatum is already in the crawldb then it will be used in preference to the CrawlDatum created by the newly injected url. This patch gives the user the ability to force the injected CrawlDatum to be used instead. The use case for this patch was the requirement for injected urls to jump to the top of the TopN list so that we can garuntee they will be crawled immediately (usefull for intranet crawling where changes can trigger injects).",
        "Issue Links": []
    },
    "NUTCH-522": {
        "Key": "NUTCH-522",
        "Summary": "Use URLValidator in the Injector",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "injector",
        "Assignee": "Emmanuel Joke",
        "Reporter": "Emmanuel Joke",
        "Created": "19/Jul/07 11:44",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Aug/07 10:58",
        "Description": "Same as NUTCH-505, we should use the UrlValidator to check url in the Injector",
        "Issue Links": []
    },
    "NUTCH-523": {
        "Key": "NUTCH-523",
        "Summary": "web2 searchform problems with patch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Hal Finkel",
        "Created": "21/Jul/07 23:56",
        "Updated": "01/Apr/11 15:26",
        "Resolved": "01/Apr/11 15:26",
        "Description": "In the current trunk, SearchForm.clone() does not make a deep copy of the KeyValue objects which can be then modified by the caller. This causes incorrect behavior when using the spell-check plugin. Also, SearchForm.setValue() can cause the old key-value pair to appear in active along with the new pair, which is also incorrect. This patch fixes both of these issues.",
        "Issue Links": []
    },
    "NUTCH-524": {
        "Key": "NUTCH-524",
        "Summary": "Generate Problem with Single Node",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Daniel Clark",
        "Created": "23/Jul/07 21:26",
        "Updated": "22/Sep/08 16:30",
        "Resolved": "22/Sep/08 16:30",
        "Description": "Nutch with Hadoop has problems with a single node in URL list when there is a cluster of two or more machines.  I will provide a fix for this.",
        "Issue Links": []
    },
    "NUTCH-525": {
        "Key": "NUTCH-525",
        "Summary": "DeleteDuplicates generates ArrayIndexOutOfBoundsException when trying to rerun dedup on a segment",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Vishal Shah",
        "Created": "24/Jul/07 07:41",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "26/Jul/07 08:53",
        "Description": "When trying to rerun dedup on a segment, we get the following Exception:\njava.lang.ArrayIndexOutOfBoundsException: Array index out of range: 261883\n\tat org.apache.lucene.util.BitVector.get(BitVector.java:72)\n\tat org.apache.lucene.index.SegmentReader.isDeleted(SegmentReader.java:346)\n\tat org.apache.nutch.indexer.DeleteDuplicates1$InputFormat$DDRecordReader.next(DeleteDuplicates1.java:167)\n\tat org.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:175)\n\tat org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:1445)\nTo reproduce the error, try creating two segments with identical urls - fetch, parse, index and dedup the 2 segments. Then rerun dedup.\nThe error comes from the DDRecordReader.next() method:\n//skip past deleted documents\nwhile (indexReader.isDeleted(doc) && doc < maxDoc) doc++;\nIf the last document in the index is deleted, then this loop will skip past the last document and call indexReader.isDeleted(doc) again.\nThe conditions should be inverted in order to fix the problem.\nI've attached a patch here.\nOn a related note, why should we skip past deleted documents? The only time when this will happen is when we are rerunning dedup on a segment. If documents are not deleted for any reason other than dedup, then they should be given a chance to compete again, isn't it? We could fix this by putting an indexReader.undeleteAll() in the constructor for DDRecordReader. Any thoughts on this?",
        "Issue Links": []
    },
    "NUTCH-526": {
        "Key": "NUTCH-526",
        "Summary": "Use a combiner in LinDbMerger to improve the performance as in LinkDb",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "linkdb",
        "Assignee": "Emmanuel Joke",
        "Reporter": "Emmanuel Joke",
        "Created": "25/Jul/07 02:23",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "04/Sep/07 03:35",
        "Description": "We have defined a Combiner in LinkDb to improve the performance, we should do the same thing in LinkDbMerger.",
        "Issue Links": []
    },
    "NUTCH-527": {
        "Key": "NUTCH-527",
        "Summary": "MapWritable doesn't support all hadoops writable types",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Rob Young",
        "Created": "25/Jul/07 11:03",
        "Updated": "27/Nov/08 17:04",
        "Resolved": "27/Nov/08 17:04",
        "Description": "The map of classes which implement org.apache.hadoop.io.Writable is not complete. It does not, for example, include org.apache.hadoop.io.BooleanWritable. I would happily provide a patch if someone would explain what the Byte parameter is.",
        "Issue Links": []
    },
    "NUTCH-528": {
        "Key": "NUTCH-528",
        "Summary": "CrawlDbReader: add some new stats + dump into a csv format",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Emmanuel Joke",
        "Created": "26/Jul/07 07:55",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "15/Jan/08 22:03",
        "Description": "I've added improve the stats to list the number of urls by status and by hosts. This is an option which is not mandatory.\n\nFor instance if you set sortByHost option, it will show:\nbin/nutch readdb crawl/crawldb -stats sortByHost\nCrawlDb statistics start: crawl/crawldb\nStatistics for CrawlDb: crawl/crawldb\nTOTAL urls:\t36\nretry 0:\t36\nmin score:\t0.0020\navg score:\t0.059\nmax score:\t1.0\nstatus 1 (db_unfetched):\t33\n   www.yahoo.com :\t33\nstatus 2 (db_fetched):\t3\n   www.yahoo.com :\t3\nCrawlDb statistics: done\nOf course without this option the stats are unchanged.\n\nI've add a new option to dump the crawldb into a CSV format. It will then be easy to integrate the file in Excel and make some more complex statistics.\nbin/nutch readdb crawl/crawldb -dump FOLDER toCsv\nExtract of the file:\nUrl;Status code;Status name;Fetch Time;Modified Time;Retries since fetch;Retry interval;Score;Signature;Metadata\n\"http://www.yahoo.com/\";1;\"db_unfetched\";Wed Jul 25 14:59:59 CST 2007;Thu Jan 01 08:00:00 CST 1970;0;2592000.0;30.0;0.04151206;\"null\";\"null\"\n\"http://www.yahoo.com/help.html\";1;\"db_unfetched\";Wed Jul 25 15:08:09 CST 2007;Thu Jan 01 08:00:00 CST 1970;0;2592000.0;30.0;0.0032467535;\"null\";\"null\"\n\"http://www.yahoo.com/contacts.html\";1;\"db_unfetched\";Wed Jul 25 15:08:12 CST 2007;Thu Jan 01 08:00:00 CST 1970;0;2592000.0;30.0;0.0032467535;\"null\";\"null\"\n\n\nI've removed some unused code ( CrawlDbDumpReducer ) as confirmed by Andrzej.",
        "Issue Links": []
    },
    "NUTCH-529": {
        "Key": "NUTCH-529",
        "Summary": "NodeWalker.skipChildren doesn't work for more than 1 child.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Emmanuel Joke",
        "Created": "27/Jul/07 03:28",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Sep/07 08:27",
        "Description": "I used NodeWalker to parse an HTML page and skip element like \"SELECT\" and their children. I noticed that it didn't skip the \"OPTION\" element which was the children of the parent SELECT element. It skipt it if I have only one element but if I have 8 children elements it keep it.",
        "Issue Links": []
    },
    "NUTCH-530": {
        "Key": "NUTCH-530",
        "Summary": "Add a combiner to improve performance on updatedb",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Emmanuel Joke",
        "Reporter": "Emmanuel Joke",
        "Created": "29/Jul/07 08:32",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 16:32",
        "Description": "We have a lot of similar links with status \"linked\" generated at the ouput of the map task when we try to update the crawldb based on the segment fetched.\nWe can use a combiner to improve the performance.",
        "Issue Links": []
    },
    "NUTCH-531": {
        "Key": "NUTCH-531",
        "Summary": "Pages with no ContentType cause a Null Pointer exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.9.0,                                            1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Carl Cerecke",
        "Created": "29/Jul/07 20:55",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Dec/08 13:54",
        "Description": "Some pages cause a null pointer exception because the contentType is missing (e.g. http://www.absoluteit.co.nz and http://defence.allmedia.co.nz)\nThe solution that I used was to change line 165 (trunk) of Content.java to:\nText.writeString(out, contentType != null?contentType:\"\");\nrfc2616 states this should be application/octet-stream if we don't know the content type, and can't figure it out.\nBut perhaps the problem is in getContentType() at line 281 (trunk). I don't yet know enough of how it is connected together to determine where the best place for fixing this bug is.",
        "Issue Links": []
    },
    "NUTCH-532": {
        "Key": "NUTCH-532",
        "Summary": "CrawlDbMerger: wrong computation of last fetch time",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Emmanuel Joke",
        "Reporter": "Emmanuel Joke",
        "Created": "30/Jul/07 08:59",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "03/Sep/07 13:37",
        "Description": "CrawlDbMerger.reduce analyse the last fetch time of each record and keep the more recent record.\nThis comparison is based on a FetchInterval in days : resTime = res.getFetchTime() - Math.round(res.getFetchInterval() * 3600 * 24 * 1000);\nIt was not really a noticeable as the Math.Round method return the INTEGER.MAX_VALUE i.e 25 days.",
        "Issue Links": []
    },
    "NUTCH-533": {
        "Key": "NUTCH-533",
        "Summary": "LinkDbMerger: url normalized is not updated in the key and inlinks list",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "linkdb",
        "Assignee": "Emmanuel Joke",
        "Reporter": "Emmanuel Joke",
        "Created": "30/Jul/07 09:52",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "31/Jul/07 12:07",
        "Description": "The key url and inlinks url are passed through a normalizer. The url return are not updated and we keep collecting the old key url and inlinks related.",
        "Issue Links": []
    },
    "NUTCH-534": {
        "Key": "NUTCH-534",
        "Summary": "SegmentMerger: add -normalize option",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Emmanuel Joke",
        "Created": "31/Jul/07 10:34",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "15/Jan/08 17:55",
        "Description": "We should add the option to normalize the URL in the segment (similar as MergeLinkdb and MergeDb).",
        "Issue Links": []
    },
    "NUTCH-535": {
        "Key": "NUTCH-535",
        "Summary": "ParseData's contentMeta accumulates unnecessary values during parse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "03/Aug/07 12:50",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Aug/07 07:34",
        "Description": "After NUTCH-506, if you run parse on a segment, parseData's contentMeta accumulates metadata of every content parsed so far. This is because NUTCH-506 changed constructor to create a new metadata (before NUTCH-506, a new metadata was created for every call to readFields). It seems hadoop somehow caches Content instance so each new call to Content.readFields during ParseSegment increases size of metadata. Because of this, one can end up with huge parse_data directory (something like 10 times larger than content directory)",
        "Issue Links": []
    },
    "NUTCH-536": {
        "Key": "NUTCH-536",
        "Summary": "Reduce number of warnings in nutch core",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "03/Aug/07 15:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Aug/07 14:23",
        "Description": "Nutch core (code under src/java) gives around 600 warnings. Most of them are unused variables/imports and Java5 generics style warnings. This issue is to track changes to reduce number of warnings.",
        "Issue Links": []
    },
    "NUTCH-537": {
        "Key": "NUTCH-537",
        "Summary": "TestMP3Parser.java, TestRTFParser.java, TestMSWordParser.java compile",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Hasan Diwan",
        "Created": "07/Aug/07 18:57",
        "Updated": "09/Aug/11 15:20",
        "Resolved": "09/Aug/11 15:20",
        "Description": "add \".get(content.getUrl());\" to parse = new ParseUtil(conf).parseByExtensionId(\"foo\", content), per the working TestRSSParser.java",
        "Issue Links": []
    },
    "NUTCH-538": {
        "Key": "NUTCH-538",
        "Summary": "Delete unused classes under o.a.n.util",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "08/Aug/07 08:22",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Nov/07 19:09",
        "Description": "ThreadPool and FibonacciHeap is not used by anything else in nutch source so, IMHO, there is no point keeping them. Java 5 has really nice alternatives to ThreadPool under java.util.concurrent. I don't know if FibonacciHeap is faster than java's priority queue but we don't make huge priority queues anyway so even if it is faster it is probably not noticable.",
        "Issue Links": []
    },
    "NUTCH-539": {
        "Key": "NUTCH-539",
        "Summary": "HttpClient plugin does not work with BasicAuthentication",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Ravi Chintakunta",
        "Created": "08/Aug/07 16:44",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "04/Jan/08 19:51",
        "Description": "For Nutch to fetch pages with basic authentication, the HttpClient should be configured with the username and password credentials. \nFor this to work:\n1. Add the username and password credentials to nutch-site.xml as below:\n<property>\n  <name>http.auth.basic.username</name>\n  <value>myusername</value>\n  <description>\n\tusername for http basic auth\n  </description>\n</property>\n<property>\n  <name>http.auth.basic.password</name>\n  <value>mypassword</value>\n  <description>\n\tpassword for http basic auth\n  </description>\n</property>\n2. Configure httpclient with these credentials by applying the attached patch to nutch/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/Http.java",
        "Issue Links": [
            "/jira/browse/NUTCH-559",
            "/jira/browse/NUTCH-557"
        ]
    },
    "NUTCH-540": {
        "Key": "NUTCH-540",
        "Summary": "some problem about the Nutch cache",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "crossany",
        "Created": "09/Aug/07 07:11",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "I'am a chinese.\nI just test to search chinese word in nutch. I install nutch0.9 in tomcat5 on linux.and the Tomcat charset it's UTF-8 and I use nutch to Crawl the website it a chinese website the web charset it's also UTF-8. when Use the nutch on tomcat for search chinese word , I find the search result' Title and description was right to display. but when I click the cache, the cache web was display a error charset code, I see the cache\nweb' charset also utf-8. I find a website use Nutch http://www.synoo.com:8080/zh/ I just test to search chinese word . It's also error.\nI use Luke to see the segments It's can display chinese word, I think maybe it's a Bug.",
        "Issue Links": [
            "/jira/browse/NUTCH-543"
        ]
    },
    "NUTCH-541": {
        "Key": "NUTCH-541",
        "Summary": "Index url field untokenized",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Enis Soztutar",
        "Reporter": "Enis Soztutar",
        "Created": "09/Aug/07 14:26",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "Url field is indexed as Strore.YES , Index.TOKENIZED. We also need the untokenized version of the url field in some contexts : \n1. For deleting duplicates by url (at search time). see NUTCH-455\n2. For restricting the search to a certain url (may be used in the case of RSS search where each entry in the Rss is added as a distinct document with (possibly) same url ) \n   query-url extends FieldQueryFilter so: \n    Query: url:http://www.apache.org/\n    Parsed: url:\"http http-www http-www-apache www www-apache apache org\"\n    Translated: +url:\"http-http-www http-www-http-www-apache http-www-apache-www www-www-apache www-apache apache org\"\n3. for accessing a document(s) in the search servers in the search servers. (using query plugin)\nI suggest we add url as in index-basic and implement a query-url-untoken plugin. \ndoc.add(new Field(\"url\", url.toString(), Field.Store.YES, Field.Index.TOKENIZED));\ndoc.add(new Field(\"url_untoken\", url.toString(), Field.Store.NO, Field.Index.UN_TOKENIZED));",
        "Issue Links": []
    },
    "NUTCH-542": {
        "Key": "NUTCH-542",
        "Summary": "Null Pointer Exception on getSummary when segment no longer exists",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jeff V.",
        "Created": "20/Aug/07 20:44",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:36",
        "Description": "If the index refers to a search result in a given segment, but that segment directory does not exist (has been deleted for some reason) the search.jsp will return a completely blank page because a Null Pointer Exception is being thrown from getSummary. At the very least it would be nice to get a more friendly log message such as \"segment doesn't exist\". But ideally the search should continue with just omitting the non-existent results.",
        "Issue Links": []
    },
    "NUTCH-543": {
        "Key": "NUTCH-543",
        "Summary": "CLONE -some problem about the Nutch cache",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "crossany",
        "Created": "21/Aug/07 07:46",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "21/Aug/07 09:01",
        "Description": "I'am a chinese.\nI just test to search chinese word in nutch. I install nutch0.9 in tomcat5 on linux.and the Tomcat charset it's UTF-8 and I use nutch to Crawl the website it a chinese website the web charset it's also UTF-8. when Use the nutch on tomcat for search chinese word , I find the search result' Title and description was right to display. but when I click the cache, the cache web was display a error charset code, I see the cache\nweb' charset also utf-8. I find a website use Nutch http://www.synoo.com:8080/zh/ I just test to search chinese word . It's also error.\nI use Luke to see the segments It's can display chinese word, I think maybe it's a Bug.",
        "Issue Links": [
            "/jira/browse/NUTCH-540"
        ]
    },
    "NUTCH-544": {
        "Key": "NUTCH-544",
        "Summary": "Upgrade Carrot2 clustering plugin to the newest stable release (2.1)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dawid Weiss",
        "Created": "22/Aug/07 13:41",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "28/Aug/07 06:27",
        "Description": "This issue upgrades Carrot2 search results clustering plugin to the newest stable version.",
        "Issue Links": []
    },
    "NUTCH-545": {
        "Key": "NUTCH-545",
        "Summary": "Configuration and OnlineClusterer get initialized in every request.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "web gui",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dawid Weiss",
        "Created": "22/Aug/07 19:04",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "28/Aug/07 06:34",
        "Description": "The initialization code block in search.jsp is invoked in every request (it's part of the request block). This is unnecessary and actually slows down the request cycle \u2013 Configuration and OnlineClusterer can (and should) be reused.\nThe attached patch moved initialization code to  jspInit().",
        "Issue Links": []
    },
    "NUTCH-546": {
        "Key": "NUTCH-546",
        "Summary": "file URL are filtered out by the crawler",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Marc Brette",
        "Created": "23/Aug/07 14:55",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "10/Sep/07 19:45",
        "Description": "I tried to index file system using the file:/ protocol, which worked fine in version 0.9\nThe file URL are being filtered out and not fetched at all.\nI investigated the code and saw that there are 2 issues:\n1) One is with the class UrlValidator: when validating an URL, it check the 'authority', a combination of host and port. As it is null for file, the URL is rejected.\n2) Once this check is removed, files that contain space characters (and maybe other characters to be URL encoded) are also filtered out. It maybe be because the file protocol plugin doesn't URL encode space characters and/or UrlValidator is enforce the rule to encode such character.\nTo workaround these issues, I just commented out UrlValidator checks and it works fine.",
        "Issue Links": []
    },
    "NUTCH-547": {
        "Key": "NUTCH-547",
        "Summary": "Redirection handling: YahooSlurp's algorithm",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "03/Sep/07 07:46",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Nov/07 13:18",
        "Description": "After reading Yahoo's algorithm (then one Andrzej linked to:\nhttp://help.yahoo.com/l/nz/yahooxtra/search/webcrawler/slurp-11.html )\nin the redirect/alias handling discussion, I had a bit of a spare\ntime, so I implemented it.\nNote that the patch I am attaching is for the 'choosing' algorithm described in\nYahoo's help page. It makes no attempt to handle aliases in any way. (See http://www.nabble.com/Redirects-and-alias-handling-%28LONG%29-tf4270371.html#a12154362 for the discussion about alias handling).\nE.g,\ngenerate \"http://www.milliyet.com.tr/\"\nfetch \"http:/www.milliyet.com.tr/\" which redirects to\n\"http://www.milliyet.com.tr/2007/08/29/index.html?ver=39\".\nUpdate second page's datum's metadata to indicate that\n\"http://www.milliyet.com.tr/\" is the representative form.\nUpdatedb, invertlinks, etc...\nWhile indexing second page, change its \"url\" field to\n\"http://www.milliyet.com.tr/\".",
        "Issue Links": [
            "/jira/browse/NUTCH-572"
        ]
    },
    "NUTCH-548": {
        "Key": "NUTCH-548",
        "Summary": "Move URLNormalizer from Outlink to ParseOutputFormat",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Emmanuel Joke",
        "Reporter": "Emmanuel Joke",
        "Created": "04/Sep/07 10:33",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Nov/07 15:08",
        "Description": "The idea is to avoid instantiating a new URLNormalizer for every OutLink. \nSo I move this operation to the ParseOutputFormat object.",
        "Issue Links": []
    },
    "NUTCH-549": {
        "Key": "NUTCH-549",
        "Summary": "Bug",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "crossany",
        "Created": "07/Sep/07 02:34",
        "Updated": "10/Sep/07 19:41",
        "Resolved": "10/Sep/07 19:41",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-550": {
        "Key": "NUTCH-550",
        "Summary": "Parse fails if db.max.outlinks.per.page is -1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "07/Sep/07 08:28",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "10/Sep/07 19:40",
        "Description": "See here http://www.nabble.com/nutch-nightly%3A-IllegalArgumentException%3A-Illegal-Capacity%3A--1-tf4245360.html#a12511661\nOne of (my|the) earlier commits broke ParseOutputFormat such that if db.max.outlinks.per.page is -1, ParseOutputFormat tries to create an ArrayList of size -1. The solution is to simply make maxOutlinks variable Integer.MAX_VALUE if db.max.outlinks.per.page is -1 .",
        "Issue Links": []
    },
    "NUTCH-551": {
        "Key": "NUTCH-551",
        "Summary": "performance for generate is often really bad",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Jim",
        "Created": "07/Sep/07 23:43",
        "Updated": "06/Feb/08 16:34",
        "Resolved": "06/Feb/08 16:34",
        "Description": "Generate often takes many hours to finish (6+), where I would expect it to be done in minutes.\n        This behavior has been observed for topN of small (~100) and large (~1000000) values.  Other configuration values are\ngenerate.max.per.host: -1\ngenerate.max.per.host.by.ip: false\n        I added debug code to Generator->Selector.map to see when map is called and returns, and observed interesting behavior, described here:\n        1. Most of the time, when generate is run urls are processed in chunky batches, usually about 40 at a time, followed by a 1 second delay.  I timed the delay, and it really is a 1 second delay (ie- 30 batches was 30 seconds.)  When this happens it takes hours to complete.\n        2. Sometimes (randomly as far as I can tell) when I run nutch, the urls are processed without delays.  It is an all or nothing event, either I run and all urls process quickly without delay (in minutes), or more likely I get the chunky processing with many 1 second delays and the program takes hours to end.  The one exception is....\n        3. When the processing runs quickly I've seen the main thread end (I have some profiling going, so I know when a thread ends), and then more likely than not a second thread begins where the first starts, chunky like usual.  Although I sometimes can get fast processing in one thread, it is almost impossible for me te get it in all threads and therefore general processing is very slow (hours).\n        4. I tried to put in more debug code to find the line where the delays occured, but the last line printed to the log at a delay seemed random, leading me to believe that the log is not being flushed uniformly.  The timestamps in the log always indicate that the delay is wither right before or after the first log item in the map function.\n        5. The profiler I used seemed to imply that about 100% of the time was spent in javallang.Thread.sleep.  I am not completely familiar with the profiler I used so I am not completely sure I inturpreted this correctly.",
        "Issue Links": []
    },
    "NUTCH-552": {
        "Key": "NUTCH-552",
        "Summary": "Upgrade Nutch to Hadoop 0.15.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Andrzej Bialecki",
        "Created": "13/Sep/07 16:09",
        "Updated": "02/May/13 02:29",
        "Resolved": "15/Nov/07 18:10",
        "Description": "Upgrade Nutch to Hadoop 0.15.x .",
        "Issue Links": [
            "/jira/browse/NUTCH-501"
        ]
    },
    "NUTCH-553": {
        "Key": "NUTCH-553",
        "Summary": "Add more normalization rules to regex-normalize file.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "13/Sep/07 16:40",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "06/Feb/08 12:27",
        "Description": "Add more normalization rules to the urlnormalizer-regex configuration file. Some useful rules can be glimpsed from the GoogleMini documentation.",
        "Issue Links": []
    },
    "NUTCH-554": {
        "Key": "NUTCH-554",
        "Summary": "Generator throws java.io.IOException and dies on injected urls with no protocol",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "generator",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Brian Whitman",
        "Created": "15/Sep/07 15:16",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "18/Sep/07 19:08",
        "Description": "On trunk nutch, injecting URLs with no protocol (like issues.apache.org/jira/ vs. https://issues.apache.org/jira/) causes the generator to fail with an IOException:\njava.net.MalformedURLException: no protocol: www.variogr.am\n        at java.net.URL.<init>(URL.java:567)\n        at java.net.URL.<init>(URL.java:464)\n        at java.net.URL.<init>(URL.java:413)\n        at org.apache.nutch.crawl.Generator$Selector.reduce(Generator.java:187)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:326)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:155)\n2007-09-15 11:11:26,986 FATAL crawl.Generator - Generator: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:604)\n        at org.apache.nutch.crawl.Generator.generate(Generator.java:416)\n        at org.apache.nutch.crawl.Generator.run(Generator.java:557)\n        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:189)\n        at org.apache.nutch.crawl.Generator.main(Generator.java:520)\nTo test:\n\ncat test/urls.txt\nwww.variogr.am\nhttp://www.variogr.am/\n\n\nbin/nutch inject testcrawl/crawldb test/\n(this goes fine)\n\n\nbin/nutch generate testcrawl/crawldb testcrawl/segments -topN 10\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: starting\nGenerator: segment: testcrawl/segments/20070915111125\nGenerator: filtering: true\nGenerator: topN: 10\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: java.io.IOException: Job failed!\n\nThis issue did not exist in earlier versions of nutch \u2013 it would ignore the malformed URL without crashing.",
        "Issue Links": []
    },
    "NUTCH-555": {
        "Key": "NUTCH-555",
        "Summary": "StackOverflowError in DomContentUtils",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Karsten Dello",
        "Created": "16/Sep/07 18:05",
        "Updated": "31/Mar/08 05:14",
        "Resolved": "31/Mar/08 05:14",
        "Description": "Parsing certain pages (which expose very bad html) causes an stackoverflow error, as the recursion depth is too high (more then 1000).\nBut parsing should be stable, it is probably better to just skip pages like this. \nAttached it\na) the stacktrace\nb) the segmentreader-get output for the url where the exception is thrown\nPossible fixes:\nparseOutlinks in DomContentUtils is implemented recursive. \nAn iterative implementation would fix this, but maybe it is easier to simply  limit the recursion to a reasonable depth.",
        "Issue Links": [
            "/jira/browse/NUTCH-497"
        ]
    },
    "NUTCH-556": {
        "Key": "NUTCH-556",
        "Summary": "automatic adjust the CrawlDatum.fetchInterval according to the number of newly outlinks",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "King Kong",
        "Created": "17/Sep/07 06:32",
        "Updated": "22/Sep/08 16:33",
        "Resolved": "22/Sep/08 16:33",
        "Description": "The spider must could  find the new urls  in time.  and the new urls usually are included in some url like index page,list page.\nbut  the score of url can not reflect it Adequately.\nCould we adjust the CrawlDatum.fetchInterval according to the number of newly outlinks.",
        "Issue Links": []
    },
    "NUTCH-557": {
        "Key": "NUTCH-557",
        "Summary": "protocol-http11 for HTTP 1.1, HTTPS, NTLM, Basic and Digest Authentication",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Susam Pal",
        "Created": "18/Sep/07 18:11",
        "Updated": "24/Sep/07 18:53",
        "Resolved": "24/Sep/07 18:53",
        "Description": "'protocol-http11' is a protocol plugin which supports retrieving documents via the HTTP 1.0, HTTP 1.1 and HTTPS protocols, optionally with Basic, Digest and NTLM authentication schemes for web server as well as proxy server.\nThe user guide and other information can be found here:- http://wiki.apache.org/nutch/protocol-http11",
        "Issue Links": [
            "/jira/browse/NUTCH-539"
        ]
    },
    "NUTCH-558": {
        "Key": "NUTCH-558",
        "Summary": "Need tool to retrieve domain statistics",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chris Schneider",
        "Reporter": "Chris Schneider",
        "Created": "19/Sep/07 23:52",
        "Updated": "13/Apr/11 23:45",
        "Resolved": "13/Apr/11 23:45",
        "Description": "Several developers have expressed interest in a tool to retrieve statistics from a crawl on a domain basis (e.g., how many pages were successfully fetched from www.apache.org vs. apache.org, where the latter total would include the former).",
        "Issue Links": []
    },
    "NUTCH-559": {
        "Key": "NUTCH-559",
        "Summary": "NTLM, Basic and Digest Authentication schemes for web/proxy server",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Susam Pal",
        "Created": "24/Sep/07 18:28",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "04/Jan/08 19:50",
        "Description": "Added basic, digest and NTLM authentication schemes to protocol-httpclient. The authentication schemes can be configured for proxy server as well as web servers of a domain. HTTP authentication can take place over HTTP/1.0, HTTP/1.1 and HTTPS.\nThe authentication guide can be found here: http://wiki.apache.org/nutch/HttpAuthenticationSchemes.",
        "Issue Links": [
            "/jira/browse/NUTCH-481",
            "/jira/browse/NUTCH-560",
            "/jira/browse/NUTCH-561",
            "/jira/browse/NUTCH-539"
        ]
    },
    "NUTCH-560": {
        "Key": "NUTCH-560",
        "Summary": "protocol-httpclient reading more bytes than http.content.limit",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0,                                            1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Joseph M.",
        "Created": "25/Sep/07 10:34",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "04/Jan/08 19:51",
        "Description": "I modified protocol-httpclient HttpResponse.java to download files to file system. If I set http.content.limit to 5000... it fetches around 5500 to 6000 bytes instead and downloads it to file system. There is calculation mistake in calculateTryToRead() function.\n\n        int tryAndRead = calculateTryToRead(totalRead);\n        while ((bufferFilled = in.read(buffer, 0, buffer.length)) != -1 && tryAndRead > 0) {\n          totalRead += bufferFilled;\n          out.write(buffer, 0, bufferFilled);\n          tryAndRead = calculateTryToRead(totalRead);\n        }\n\nwhile loop stops when calculateTryToRead() returns -ve or 0.\n\nprivate int calculateTryToRead(int totalRead) {\n    int tryToRead = Http.BUFFER_SIZE;\n    if (http.getMaxContent() <= 0) {\n      return http.BUFFER_SIZE;\n    } else if (http.getMaxContent() - totalRead < http.BUFFER_SIZE) {\n      tryToRead = http.getMaxContent() - totalRead;\n    }\n    return tryToRead;\n  }\n\nIt is returning -ve when totalRead > http.getMaxContent(). So more bytes than http.content.limit is read before breaking while loop.",
        "Issue Links": [
            "/jira/browse/NUTCH-559"
        ]
    },
    "NUTCH-561": {
        "Key": "NUTCH-561",
        "Summary": "HttpClient plugin does not work with NTLM authentication",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.9.0,                                            1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Alexis Votta",
        "Created": "25/Sep/07 17:27",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "04/Jan/08 19:51",
        "Description": "NTLM authentication is not working even after properly configuring. Details are present in these posts :\n1. http://www.nabble.com/Re%3A-crawling-sites-which-require-authentication-p7069617.html\n2. http://www.nabble.com/protocol-httpclient-NTLM-authentication-fails-p12743584.html\n3. http://www.nabble.com/Does-authentication-work--p12884045.html\nFrom the above posts it seems that there is some bug present since Nutch 0.7.x",
        "Issue Links": [
            "/jira/browse/NUTCH-559"
        ]
    },
    "NUTCH-562": {
        "Key": "NUTCH-562",
        "Summary": "Port mime type framework to use Tika mime detection framework",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "29/Sep/07 04:36",
        "Updated": "08/Jun/11 21:35",
        "Resolved": "09/Oct/07 00:24",
        "Description": "With Tika (http://incubator.apache.org/tika/) nearing  a stable 0.1 release candidate, I think it would be a good time to patch Nutch to use Tika's mime detection system (an improvement over the existing Nutch one written primarily by Jerome). Tika's mime system is based on the mime system from Freedesktop.org and includes several improvements over the existing Nutch mime system such as:\n1. reliable XML-based content detection (a clear issue plaguing Nutch for some time now), ability to delineate between RSS, XML, ATOM, etc.\n2. mime magic pattern matching, including support for multiple patterns\n3. glob pattern matches (ability to support > 1)\nI'll get together a patch and then attach it to the list once it's relatively stable.",
        "Issue Links": [
            "/jira/browse/NUTCH-185"
        ]
    },
    "NUTCH-563": {
        "Key": "NUTCH-563",
        "Summary": "Include custom fields in BasicQueryFilter",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Julien Nioche",
        "Created": "01/Oct/07 16:23",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "18/Feb/09 12:54",
        "Description": "This patch allows to include additional fields in the BasicQueryFilter by specifying runtime parameters.  Any parameter matching the regular expression (query\\\\.basic\\\\.(.+).boost\") will be added to the list of fields to be used by the BQF and the specified float value will be used as boost.",
        "Issue Links": []
    },
    "NUTCH-564": {
        "Key": "NUTCH-564",
        "Summary": "External parser supports encoding attribute",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Antony Bowesman",
        "Created": "03/Oct/07 21:42",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "08/Aug/10 19:04",
        "Description": "When an external component generates text, which is returned to the external parser, it always converts the text using the default character set.  (os.toString()).  For example, the returned text may be utf-8, but will not be converted to a String correctly.\nI added the attribute <encoding> to the <implementation> XML in plugin.xml and this is then used to convert the text.\nI have tested my original fix on my local 0.9 and include a patch, but have also made an untested patch for trunk.",
        "Issue Links": []
    },
    "NUTCH-565": {
        "Key": "NUTCH-565",
        "Summary": "Arc File to Nutch Segments Converter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "09/Oct/07 05:07",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "04/Nov/07 16:06",
        "Description": "Functionality that allows arc files, such as those produced by the internet archive project or by the Grub distributed crawler to be parsed into Nutch segments.",
        "Issue Links": []
    },
    "NUTCH-566": {
        "Key": "NUTCH-566",
        "Summary": "Sun's URL class has bug in creation of relative query URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Doug Cook",
        "Created": "10/Oct/07 15:55",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "26/Apr/14 21:36",
        "Description": "I'm using 0.81, but this will affect all other versions as well.\nRelative links of the form \"?blah\" are resolved incorrectly. For example, with a base URL of http://www.fleurie.org/entreprise.asp, and a relative link of \"?id_entrep=111\", Nutch will resolve this pair to the link\n\"http://www.fleurie.org/?id_entrep=111\". No such URL exists, and all browsers I tried will resolve the pair to \"http://www.fleurie.org/entreprise.asp?id_entrep=111\".\nI tracked this down to what could be called a bug in Sun's URL class. According to Sun's spec, they parse the relative URL according to RFC 2396. But the original RFC for relative links was RFC 1808, and the two RFCs differ in how they handle relative links beginning with \"?\". Most browsers (Netscape/Mozilla, IE, Safari) implemented RFC 1808, and stuck with it (for compatibility and also because the behavior makes more sense). Apparently even the people that wrote RFC 2396 recognized that this was a mistake, and the specified behavior was changed in RFC 3986 to match what browsers do. \nFor a discussion of this, see  http://gbiv.com/protocols/uri/rev-2002/issues.html#003-relative-query\nSun's URL implementation, however, still implements RFC2396, as far as I can tell, and is out of step with the rest of the world.\nThis breaks link extraction on a number of sites.\nI implemented a simple workaround, which I'm attaching. It is a static method to create URLs which behaves exactly as new URL(URL base, String relativePath), and I use it as a drop-in replacement for that in DOMContentUtils, Javascript link extraction, etc. Obviously, it really only matters wherever links are extracted. I haven't included the calling code from DOMContentUtils, etc. because my local versions are largely rewritten, but it should be pretty obvious.\nI put it in the org.apache.nutch.net directory, but obviously feel free to move it to another place if you feel it belongs there!",
        "Issue Links": [
            "/jira/browse/NUTCH-952",
            "/jira/browse/NUTCH-797",
            "/jira/browse/NUTCH-952",
            "/jira/browse/NUTCH-436",
            "/jira/browse/NUTCH-952",
            "/jira/browse/NUTCH-797"
        ]
    },
    "NUTCH-567": {
        "Key": "NUTCH-567",
        "Summary": "Proper (?) handling of URIs in TagSoup.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dawid Weiss",
        "Created": "17/Oct/07 12:06",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "25/Feb/08 09:39",
        "Description": "Doug Cook reported that TagSoup incorrectly handles some URI parameters. More discussion on the list and at TagSoup's mailing list.\nhttp://tech.groups.yahoo.com/group/tagsoup-friends/message/838\nI looked at the sources of TagSoup because I'm using it myself (although the URIs are not relevant for me). It seems like you can implement a naive workaround by remembering the parsing state and just avoiding entity resolution. Attached is the patch that does this.",
        "Issue Links": []
    },
    "NUTCH-568": {
        "Key": "NUTCH-568",
        "Summary": "Indexer does not update the Lucene \"TITLE\" field",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "smorales",
        "Created": "19/Oct/07 19:29",
        "Updated": "01/Apr/11 14:58",
        "Resolved": "01/Apr/11 14:58",
        "Description": "Hi,\nThe indexer is unable to update the field \"TITLE\" of the Lucene index when processing specific html documents.\nThis issue has been reproduced using Nutch-Nightly Build #241 (Oct 19, 2007 4:01:28 AM)\nThe problem does not occurs using NUTCH 9.0.\nWorkflow:\n1.- Extracted package and copy across the following configuration files from NUTCH 9.0\n\n{nutch_home_9.0}/bin/url folder, containing the urls\n- {nutch_home_9.0}\n/conf/nutch-site.xml\n{nutch_home_9.0}\n/conf/crawl-urlfilter.txt\n\n2.- To reproduce the issue, you need to copy the attached html document to your webserver/filesytem.\n3.- Run the crawl.\nFor example: ./nutch crawl urls -dir crawl -depth 22\n4.- Open the index using Luke.  For this test, I used lukeall-0.7.1.jar\n5.- Select the window select the \"document\" tab, move thru the docs until you find our html document.\nYou will see that the TITLE field is empty  --> INCORRECT because this html document contains a title.\n6.- Now, open the html document, add a space anywhere then save it again.\n7.- Repeat step 3 and 4.\nYou will notice that this time the field \"TITLE\" field contains the correct information\nPlease advice,\nMany thanks in advance for your support.\nSergio",
        "Issue Links": []
    },
    "NUTCH-569": {
        "Key": "NUTCH-569",
        "Summary": "Protocol plugins should report progress to the fetcher",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "23/Oct/07 12:25",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "When downloading very large files over slow connections, protocol plugins spend long time in Protocol.getProtocolOutput(...). This sometimes leads to a timeout in Fetcher / Fetcher2, with the message \"aborting with hung threads\". Protocol plugins should periodically notify their caller about progress. In a situation when the call to getProtocolOutput takes very long time to return, this will help the caller to determine whether the wait is justified.\nPreferably, the callback interface should allow the monitoring of not only the binary progress / no-progress, but also the download speed, so that the caller could terminate slow connections. E.g.\n\ninterface ProtocolReporter {\n  void progress(long bytesDownloaded);\n}",
        "Issue Links": [
            "/jira/browse/NUTCH-1182"
        ]
    },
    "NUTCH-570": {
        "Key": "NUTCH-570",
        "Summary": "Improvement of URL Ordering in Generator.java",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": "Otis Gospodnetic",
        "Reporter": "Ned Rockson",
        "Created": "26/Oct/07 00:00",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "12/Apr/10 17:23",
        "Description": "[Copied directly from my email to nutch-dev list]\nRecently I switched to Fetcher2 over Fetcher for larger whole web fetches (50-100M at a time).  I found that the URLs generated are not optimal because they are simply randomized by a hash comparator.  In one crawl on 24 machines it took about 3 days to crawl 30M URLs.  In comparison with old benchmarks I had set with regular Fetcher.java this was at least 3 fold more time.\nAnyway, I realized that the best situation for ordering can be approached by randomization, but in order to get optimal ordering, urls from the same host should be as far apart in the list as possible.  So I wrote a series of 2 map/reduces to optimize the ordering and for a list of 25M documents it takes about 10 minutes on our cluster.  Right now I have it in its own class, but I figured it can go in Generator.java and just add a flag in nutch-default.xml determining if the user wants to use it.",
        "Issue Links": [
            "/jira/browse/NUTCH-776"
        ]
    },
    "NUTCH-571": {
        "Key": "NUTCH-571",
        "Summary": "parse-mp3 plugin doesn't always index album of mp3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Joseph Chen",
        "Created": "03/Nov/07 02:25",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "28/Jan/09 11:33",
        "Description": "The parse-mp3 plugin does not always index the album of an mp3.  The reason for this bug is a simple typo in MetadataCollector.java:\n  public String getTitle() {\n    String text = \"\";\n    if (title != null) \n{\n      text = title;\n    }\n   if (album != null) {\n      if (!text.equals(\"\")) \n{\n        text += \" - \" + album;\n      }\n else \n{\n        text = title;\n      }\n    }\n...\nChanging line 79 from \"text = title\" to \"text = album\" fixes the problem.",
        "Issue Links": []
    },
    "NUTCH-572": {
        "Key": "NUTCH-572",
        "Summary": "Scoring and redirected Urls",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "04/Nov/07 21:41",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "20/Jan/09 15:58",
        "Description": "When a redirect is found for a given url, the new or end url is stored as the content page and the old CrawlDatum get one of a few redirect codes.  The page that gets indexed in Nutch is the end page and it gets indexed under the end url.  Many times a site will have a significant number of links pointing to start page and very few pointing to the redirected end page.  This is especially true for external links.  Opic scores do not get transfered to the end page but stay with the start page (the one doing the redirecting).  But the start page doesn't get indexed.  Hence the end page will show up in the index but under a usually much reduced score.  A good example of this is cnn.com:\nURL: http://www.cnn.com/\nVersion: 6\nStatus: 5 (db_redir_perm)\nFetch time: Tue Dec 04 11:02:09 CST 2007\nModified time: Wed Dec 31 18:00:00 CST 1969\nRetries since fetch: 0\nRetry interval: 2592000 seconds (30 days)\nScore: 51.19438\nSignature: b5baaf80e9e10aa6205fc39051c362ff\nMetadata: pst:success(1), lastModified=0\nwhich redirects to http://www.cnn.com/?refresh=1\nURL: http://www.cnn.com/?refresh=1\nVersion: 6\nStatus: 2 (db_fetched)\nFetch time: Tue Dec 04 11:02:11 CST 2007\nModified time: Wed Dec 31 18:00:00 CST 1969\nRetries since fetch: 0\nRetry interval: 2592000 seconds (30 days)\nScore: 1.0\nSignature: b5baaf80e9e10aa6205fc39051c362ff\nMetadata: pst:success(1), lastModified=0\nNow, cnn which should be one of the highest, if not the highest ranking site in the index for keywords such as news in fact doesn't show up in the index and it's redirected end page appears much farther down in search results.  My proposal is we somehow make OPIC scores follow redirects.  To do this we would most likely need to store a start and end url for redirected urls.",
        "Issue Links": [
            "/jira/browse/NUTCH-411",
            "/jira/browse/NUTCH-547"
        ]
    },
    "NUTCH-573": {
        "Key": "NUTCH-573",
        "Summary": "Multiple Domains - Query Search",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Enis Soztutar",
        "Reporter": "Rajasekar Karthik",
        "Created": "07/Nov/07 18:58",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "Searching multiple domains can be done on Lucene - nut not that efficiently on nutch.\nQuery:\n+content:\"abc\" +(site\"www.aaa.com\" site:\"www.bbb.com\")\nworks on lucene but the same concept does not work on nutch.\nIn Lucene, it works with \norg.apache.lucene.analysis.KeywordAnalyzer\norg.apache.lucene.analysis.standard.StandardAnalyzer \nbut NOT on\norg.apache.lucene.analysis.SimpleAnalyzer \nIs Nutch analyzer based on SimpleAnalyzer? In this case, is there a workaround to make this work? Is there an option to change what analyzer nutch is using? \nJust FYI, another solution (inefficient I believe) which seems to be working on nutch\n<query> -site:\"ccc.com\" -site:\"ddd.com\"",
        "Issue Links": []
    },
    "NUTCH-574": {
        "Key": "NUTCH-574",
        "Summary": "Including inlink anchor text in index can create irrelevant search results.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "07/Nov/07 19:44",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "13/Nov/07 17:46",
        "Description": "Currently the basic indexing filter includes inbound anchor text for a given URL in the index.  This sometimes allows pages to show up in search results where they may not be relevant.  An example of this is a search for \"dallas hotels\" in our production index (www.visvo.com).  Google would show up first in this example although there is no text matching either dallas or hotels on the google home page.  What is happening here is there are inlinks into google with the words dallas and hotels which get included in the index for google.com and because google would have a very high boost due to inlinks, google shows up first for these search terms.  I propose we add an option to allow/prevent inlink anchor text from being included in the index and set the default for this option to NOT include inbound link anchor text.",
        "Issue Links": []
    },
    "NUTCH-575": {
        "Key": "NUTCH-575",
        "Summary": "NPE in OpenSearchServlet when summary is null",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0,                                            1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "John H. Lee",
        "Created": "08/Nov/07 22:43",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "14/Mar/08 15:10",
        "Description": "summaries[i].toHtml() is called without checking if summaries[i] is not null, causing an unhandled NullPointerException and a failed OpenSearchServlet query.",
        "Issue Links": [
            "/jira/browse/NUTCH-613"
        ]
    },
    "NUTCH-576": {
        "Key": "NUTCH-576",
        "Summary": "Different Analyzers Support",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.7.2,                                            0.8,                                            0.8.1,                                            0.8.2,                                            0.7.3,                                            0.9.0,                                            1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rajasekar Karthik",
        "Created": "14/Nov/07 15:10",
        "Updated": "13/Apr/11 23:36",
        "Resolved": "13/Apr/11 23:36",
        "Description": "For changing existing lucene analyzers  - can it be made more intuitive? EG: In nutch-site.xml, another property could be added that one can specify what analyzer to use. \nCurrently, I believe one has to modify this file -  NutchDocumentAnalysis.java & compile to get it working - which might be uneasy for many beginners.",
        "Issue Links": []
    },
    "NUTCH-577": {
        "Key": "NUTCH-577",
        "Summary": "Use explicit tika-config.xml file to enable mime magic detection to be turned on and off",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "17/Nov/07 23:27",
        "Updated": "22/May/13 03:54",
        "Resolved": "27/Sep/10 02:42",
        "Description": "Currently, there is a configuration file for Tika (which the trunk in Nutch uses for its mime type detection) called \"tika-config.xml\" left unexposed (a default one lives in the tika-0.1-dev.jar file). Tika's mime system has two config files it relies on: tika-mimetypes.xml (which Nutch has its own version of, that overrides the version that comes with the tika jar file), and tika-config.xml (to turn on or off magic char detection). We should probably have a nutch version of tika-config.xml, so that Nutch users can employ magic char mime detection. I'll get going on this in the next day or so.",
        "Issue Links": []
    },
    "NUTCH-578": {
        "Key": "NUTCH-578",
        "Summary": "URL fetched with 403 is generated over and over again",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.9",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Nathaniel Powell",
        "Created": "20/Nov/07 21:39",
        "Updated": "07/Jul/14 13:50",
        "Resolved": "07/Jul/14 12:39",
        "Description": "I have not changed the following parameter in the nutch-default.xml:\n<property>\n  <name>db.fetch.retry.max</name>\n  <value>3</value>\n  <description>The maximum number of times a url that has encountered\n  recoverable errors is generated for fetch.</description>\n</property>\nHowever, there is a URL which is on the site that I'm crawling, www.teachertube.com, which keeps being generated over and over again for almost every segment (many more times than 3):\nfetch of http://www.teachertube.com/images/ failed with: Http code=403, url=http://www.teachertube.com/images/\nThis is a bug, right?\nThanks.",
        "Issue Links": [
            "/jira/browse/NUTCH-813",
            "/jira/browse/NUTCH-1245",
            "/jira/browse/NUTCH-1247"
        ]
    },
    "NUTCH-579": {
        "Key": "NUTCH-579",
        "Summary": "Feed plugin only indexes one post per feed due to identical digest",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Joseph Chen",
        "Created": "21/Nov/07 07:41",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "21/Jan/09 19:43",
        "Description": "When parsing an rss feed, only one post will be indexed per feed.  The reason for this is that the digest, which is calculated for based on the content (or the url if the content is null) is always the same for each post in a feed.\nI noticed this when I was examining my lucene indexes using Luke.  All of the individual feed entries were being indexed properly but then when the dedup step ran, my merged index ended up with only one document.\nAs a quick fix, I simply overrode the digest in the FeedIndexingFilter.java, by adding the following code to the filter function:\nbyte[] signature = MD5Hash.digest(url.toString()).getDigest();\ndoc.removeField(\"digest\");\ndoc.add(new Field(\"digest\", StringUtil.toHexString(signature), Field.Store.YES, Field.Index.NO));\nThis seems to fix the issue as the index now contains the proper number of documents.\nAnyone have any comments on whether this is a good solution or if there is a better solution?",
        "Issue Links": []
    },
    "NUTCH-580": {
        "Key": "NUTCH-580",
        "Summary": "Remove deprecated hadoop api calls (FS)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "21/Nov/07 16:47",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Jan/08 09:01",
        "Description": "There are quite a lot of calls to deprecated hadoop api functionality. Following patch will take care of fs related ones.",
        "Issue Links": []
    },
    "NUTCH-581": {
        "Key": "NUTCH-581",
        "Summary": "DistributedSearch does not update search servers added to search-servers.txt on the fly",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rohan Mehta",
        "Created": "21/Nov/07 16:57",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "04/Dec/07 19:15",
        "Description": "DistributedSearch client updates the search servers added to the search-servers.txt file on the fly. \nThis patch will updates the search servers on the fly and the client does not need a restart.",
        "Issue Links": []
    },
    "NUTCH-582": {
        "Key": "NUTCH-582",
        "Summary": "Add missing type parameters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "21/Nov/07 18:47",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Feb/09 18:44",
        "Description": "Hadoop 0.15 added possibility to use type parameters with several interfaces and makes it easier to use correct types in Mappers, Reducers et al. in addition to improved readability. Following patch will add type parameters to Mappers, Reducers, OutputCollectors, MapRunnables, InputFormats and OutputFormats.",
        "Issue Links": []
    },
    "NUTCH-583": {
        "Key": "NUTCH-583",
        "Summary": "FeedParser empty links for items",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": "Enis Soztutar",
        "Reporter": "Enis Soztutar",
        "Created": "27/Nov/07 14:59",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "FeedParser in feed plugin just discards the item if it does not have <link> element. However Rss 2.0 does not necessitate the <link> element for each <item>. \nMoreover sometimes the link is given in the <guid> element which is a globally unique identifier for the item. I think we can search the url for an item first, then if it is still not found, we can use the feed's url, but with merging all the parse texts into one Parse object.",
        "Issue Links": []
    },
    "NUTCH-584": {
        "Key": "NUTCH-584",
        "Summary": "urls missing from fetchlist",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0,                                            1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "generator",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Ruslan Ermilov",
        "Created": "28/Nov/07 15:57",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "16/Jan/08 16:53",
        "Description": "When generating an initial set of ~100k URLs for fetching, I've noticed that some URLs are missing from the fetchlist.\nThe test case below has only 2 URLs, and I've used the FreeGenerator tool instead of the standard inject/generate\nthat saves me time when experimenting. It doesn't matter if I run it in clustered or local mode.\nSomehow only one of two URLs ends up in the fetchlist:\n$ rm -rf segments\n$ cat urls/x\nhttp://tkd.ru/\nhttp://t-f.ru/\n$ nutch org.apache.nutch.tools.FreeGenerator urls segments\n$ nutch readseg -dump segments/* xxx -nocontent -noparse -noparsedata -noparsetext -nofetch\nSegmentReader: dump segment: segments/20071128195720\nSegmentReader: done\n$ cat xxx/dump\nRecno:: 0\nURL:: http://tkd.ru/\nCrawlDatum::\nVersion: 5\nStatus: 0 (unknown)\nFetch time: Wed Nov 28 19:57:20 GMT 2007\nModified time: Thu Jan 01 00:00:00 GMT 1970\nRetries since fetch: 0\nRetry interval: 0.0 days\nScore: 1.0\nSignature: null\nMetadata: null\n$",
        "Issue Links": []
    },
    "NUTCH-585": {
        "Key": "NUTCH-585",
        "Summary": "[PARSE-HTML plugin] Block certain parts of HTML code from being indexed",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Andrea Spinelli",
        "Created": "29/Nov/07 11:13",
        "Updated": "12/Nov/20 14:40",
        "Resolved": null,
        "Description": "We are using nutch to index our own web sites; we would like not to index certain parts of our pages, because we know they are not relevant (for instance, there are several links to change the background color) and generate spurious matches.\nWe have modified the plugin so that it ignores HTML code between certain HTML comments, like\n<!-- START-IGNORE -->\n... ignored part ...\n<!-- STOP-IGNORE -->\nWe feel this might be useful to someone else, maybe factorizing the comment strings as constants in the configuration files (say parser.html.ignore.start and parser.html.ignore.stop in nutch-site.xml).\nWe are almost ready to contribute our code snippet.  Looking forward for any expression of  interest - or for an explanation why waht we are doing is plain wrong!",
        "Issue Links": []
    },
    "NUTCH-586": {
        "Key": "NUTCH-586",
        "Summary": "Add option to run compiled classes w/o job file",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Enis Soztutar",
        "Created": "30/Nov/07 10:35",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Dec/07 18:23",
        "Description": "bin/nutch adds nutch-*.job files under build and base directory to the classpath. However building the job file takes a long time. We have a target compile-core which builds only the core classes w/o plugins, but we need a way to run the compiled core class files. An option to bin/nutch to run the classes compiled with ant compile-core seems enough.",
        "Issue Links": []
    },
    "NUTCH-587": {
        "Key": "NUTCH-587",
        "Summary": "Upgrade Nutch to use Hadoop 0.15.3 release",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "03/Dec/07 23:34",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "28/Jan/08 22:38",
        "Description": "Upgrade Nutch to use the recently released hadoop 0.15.3.  There were some critical changes in hadoop between 0.15.0 and 0.15.1.  The biggest as applies to Nutch is the removal of deprecated method for default and final resources.",
        "Issue Links": []
    },
    "NUTCH-588": {
        "Key": "NUTCH-588",
        "Summary": "Help Need",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.7.2",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Teccon Ingenieros",
        "Created": "04/Dec/07 16:39",
        "Updated": "25/Jan/09 11:40",
        "Resolved": "07/Dec/07 10:46",
        "Description": "Hello,\nWe are trying to index a word file, if we put the static url like (/servlet/jsp/documento.doc) it works ok, put if we try to do the same with an dinamic url that generates that file (/servlet/jsp/leerFichero.jsp&id=112) it does\u00b4t work, it does\u00b4t index our url.\nWhat can we do?\nRegards,",
        "Issue Links": []
    },
    "NUTCH-589": {
        "Key": "NUTCH-589",
        "Summary": "Hierarchical Classloaders",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ryan Levering",
        "Created": "05/Dec/07 00:04",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Currently the Nutch plugin classloader flattens all the jars from a plugins' dependencies and instantiates a new classloader for each plugin.  I think it would be better to create a hierarchical classloader chain.  Currently plugins can't pass objects from a common plugin to one another because the objects are created using different classloaders.  Nutch currently avoids this by only using interfaces from a common classloader to pass objects between plugins, but I can't see the harm in improving the plugin classloader.  It would require a change to PluginDescription and PluginClassLoader in order to override ClassLoader to maintain the export filter functionality that currently exists.",
        "Issue Links": []
    },
    "NUTCH-590": {
        "Key": "NUTCH-590",
        "Summary": "Index multiple docs per call using IndexingFilter extension point",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Nathaniel Powell",
        "Created": "06/Dec/07 00:59",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "14/Mar/08 15:00",
        "Description": "There are many applications where extracting and indexing multiple documents from a single HTML web file or other object would be useful. Therefore, it would help a lot if the IndexingFilter extension point were modified to pass in a list of documents as an argument and return a list (or collection) of documents.",
        "Issue Links": []
    },
    "NUTCH-591": {
        "Key": "NUTCH-591",
        "Summary": "StringIndexOutOfBoundsException when extracting text from a Word document.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "frank ling",
        "Created": "14/Dec/07 00:47",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "18/Feb/09 08:37",
        "Description": "see \nhttp://issues.apache.org/bugzilla/show_bug.cgi?id=41076+",
        "Issue Links": [
            "/jira/browse/NUTCH-691"
        ]
    },
    "NUTCH-592": {
        "Key": "NUTCH-592",
        "Summary": "Fetcher2 : NPE for page with status ProtocolStatus.TEMP_MOVED",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Emmanuel Joke",
        "Created": "16/Dec/07 15:22",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "14/Mar/08 14:58",
        "Description": "I have a NPE for page when ProtocolStatus.TEMP_MOVED. It seems handleRedirect function can return NULL for few case and it has not been managed in the function as it has been done for the case ProtocolStatus.SUCCESS.",
        "Issue Links": []
    },
    "NUTCH-593": {
        "Key": "NUTCH-593",
        "Summary": "Nutch crawl problem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "sudarat",
        "Created": "19/Dec/07 02:47",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "06/Feb/08 16:39",
        "Description": "i use nutch-0.9, hadoop-0.12.2 and i use this command \"bin/nutch crawl \nurls -dir crawled -depth 3\" have error : \n\ncrawl started in: crawled\nrootUrlDir = input\nthreads = 10\ndepth = 3\nInjector: starting\nInjector: crawlDb: crawled/crawldb\nInjector: urlDir: input\nInjector: Converting injected urls to crawl db entries.\nTotal input paths to process : 1\nRunning job: job_0001\nmap 0% reduce 0%\nmap 100% reduce 0%\nmap 100% reduce 100%\nJob complete: job_0001\nCounters: 6\nMap-Reduce Framework\nMap input records=3\nMap output records=1\nMap input bytes=22\nMap output bytes=52\nReduce input records=1\nReduce output records=1\nInjector: Merging injected urls into crawl db.\nTotal input paths to process : 2\nRunning job: job_0002\nmap 0% reduce 0%\nmap 100% reduce 0%\nmap 100% reduce 58%\nmap 100% reduce 100%\nJob complete: job_0002\nCounters: 6\nMap-Reduce Framework\nMap input records=3\nMap output records=1\nMap input bytes=60\nMap output bytes=52\nReduce input records=1\nReduce output records=1\nInjector: done\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: starting\nGenerator: segment: crawled/segments/25501213164325\nGenerator: filtering: false\nGenerator: topN: 2147483647\nTotal input paths to process : 2\nRunning job: job_0003\nmap 0% reduce 0%\nmap 100% reduce 0%\nmap 100% reduce 100%\nJob complete: job_0003\nCounters: 6\nMap-Reduce Framework\nMap input records=3\nMap output records=1\nMap input bytes=59\nMap output bytes=77\nReduce input records=1\nReduce output records=1\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=0 - no more URLs to fetch.\nNo URLs to fetch - check your seed list and URL filters.\ncrawl finished: crawled\n\nbut sometime i crawl some url it has error indexes time that \n\nIndexer: done\nDedup: starting\nDedup: adding indexes in: crawled/indexes\nTotal input paths to process : 2\nRunning job: job_0025\nmap 0% reduce 0%\nTask Id : task_0025_m_000001_0, Status : FAILED\ntask_0025_m_000001_0: - Error running child \ntask_0025_m_000001_0: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000001_0: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000001_0: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000001_0: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000001_0: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000001_0: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000001_0: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nTask Id : task_0025_m_000000_0, Status : FAILED\ntask_0025_m_000000_0: - Error running child \ntask_0025_m_000000_0: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000000_0: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000000_0: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000000_0: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000000_0: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000000_0: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000000_0: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nTask Id : task_0025_m_000000_1, Status : FAILED\ntask_0025_m_000000_1: - Error running child \ntask_0025_m_000000_1: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000000_1: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000000_1: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000000_1: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000000_1: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000000_1: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000000_1: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nTask Id : task_0025_m_000001_1, Status : FAILED\ntask_0025_m_000001_1: - Error running child \ntask_0025_m_000001_1: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000001_1: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000001_1: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000001_1: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000001_1: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000001_1: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000001_1: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nTask Id : task_0025_m_000001_2, Status : FAILED\ntask_0025_m_000001_2: - Error running child \ntask_0025_m_000001_2: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000001_2: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000001_2: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000001_2: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000001_2: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000001_2: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000001_2: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nTask Id : task_0025_m_000000_2, Status : FAILED\ntask_0025_m_000000_2: - Error running child \ntask_0025_m_000000_2: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000000_2: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000000_2: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000000_2: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000000_2: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000000_2: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000000_2: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nmap 100% reduce 100%\nTask Id : task_0025_m_000001_3, Status : FAILED\ntask_0025_m_000001_3: - Error running child \ntask_0025_m_000001_3: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000001_3: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000001_3: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000001_3: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000001_3: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000001_3: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000001_3: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nTask Id : task_0025_m_000000_3, Status : FAILED\ntask_0025_m_000000_3: - Error running child \ntask_0025_m_000000_3: java.lang.ArrayIndexOutOfBoundsException: -1 \ntask_0025_m_000000_3: at \norg.apache.lucene.index.MultiReader.isDeleted(MultiReader.java:113) \ntask_0025_m_000000_3: at \norg.apache.nutch.indexer.DeleteDuplicates$InputFormat$DDRecordReade \nr.next(DeleteDuplicates.java:176) \ntask_0025_m_000000_3: at \norg.apache.hadoop.mapred.MapTask$1.next(MapTask.java:157) \ntask_0025_m_000000_3: at \norg.apache.hadoop.mapred.MapRunner.run(MapRunner.java:46) \ntask_0025_m_000000_3: at org.apache.hadoop.mapred.MapTask.run \n(MapTask.java:175) \ntask_0025_m_000000_3: at \norg.apache.hadoop.mapred.TaskTracker$Child.main \n(TaskTracker.java:1445) \nException in thread \"main\" java.io.IOException: Job failed! \nat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:604) \nat org.apache.nutch.indexer.DeleteDuplicates.dedup \n(DeleteDuplicates.java:439) \nat org.apache.nutch.crawl.Crawl.main(Crawl.java:135) \n\nhow i solve it?",
        "Issue Links": []
    },
    "NUTCH-594": {
        "Key": "NUTCH-594",
        "Summary": "Serve Nutch search results in multiple formats including XML and JSON",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "21/Dec/07 17:10",
        "Updated": "13/Jan/09 04:16",
        "Resolved": "02/Jan/09 21:39",
        "Description": "Allow search results to be served in XML, JSON, and other configurable formats.  Right now there is an OpenSearch servlet that returns returns in RSS. I would like something that has more flexibility in terms of the XML being served and also supports other formats such as JSON or plain text.",
        "Issue Links": []
    },
    "NUTCH-595": {
        "Key": "NUTCH-595",
        "Summary": "\"Target file:/.... already exists\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "27/Dec/07 13:08",
        "Updated": "01/Apr/11 15:26",
        "Resolved": "01/Apr/11 15:26",
        "Description": "This is related to the upgrade to Hadoop 0.15.0. I'm unable to run any Hadoop jobs in local mode under Cygwin:\n\n2007-12-27 13:54:24,468 WARN  mapred.LocalJobRunner - job_local_1\njava.io.IOException: Target file:/c:/tmp/hadoop-abial/mapred/temp/inject-temp-19350068/_reduce_kmsua5/part-00000 already exists\n        at org.apache.hadoop.fs.FileUtil.checkDest(FileUtil.java:246)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:125)\n        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:116)\n        at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:180)\n        at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:394)\n        at org.apache.hadoop.mapred.Task.moveTaskOutputs(Task.java:452)\n        at org.apache.hadoop.mapred.Task.moveTaskOutputs(Task.java:469)\n        at org.apache.hadoop.mapred.Task.saveTaskOutput(Task.java:426)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:165)\n2007-12-27 13:54:24,843 FATAL crawl.Injector - Injector: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:831)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:162)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:192)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.hadoop.util.ToolBase.doMain(ToolBase.java:54)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:182)\n\n\nAFAIK this should be fixed in HADOOP-2228, which is a part of 0.15.2.",
        "Issue Links": []
    },
    "NUTCH-596": {
        "Key": "NUTCH-596",
        "Summary": "ParseSegments parse content even if its not CrawlDatum.STATUS_FETCH_SUCCESS",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Emmanuel Joke",
        "Created": "30/Dec/07 09:51",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "18/Apr/08 18:50",
        "Description": "We have 2 choices to parse the content either within the Fetcher class or with the ParseSegment class\nFetcher(1 or 2) will check first if the CrawlDatum == STATUS_FETCH_SUCCESS nad if its true it will parse the content.\nHowever we don't have this check in ParseSegment, thus we parse every content store on the disk without checking the Status.\nSo i think we should implement this check, i can see only 3 solutions:\n\nread the status code in the Metadata of the Content object\ndon't store content for fetch with a crawldatun <>  STATUS_FETCH_SUCCESS\nload the crawldatum object in ParseSegement\n\nWhat are your thoughts ?",
        "Issue Links": []
    },
    "NUTCH-597": {
        "Key": "NUTCH-597",
        "Summary": "Fetcher2 - java.lang.NullPointerException when host does not exist and fetcher.threads.per.host.by.ip is set to true causes threads to finish.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Remco Verhoef",
        "Created": "30/Dec/07 16:29",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "15/Jan/08 22:39",
        "Description": "When fetcher.threads.per.host.by.ip is set to true the following exception is thrown when the host does not exist. FetchItem.create returns null when it is not able to resolve the host address when it is redirecting.\n2007-12-30 15:34:42,720 WARN  fetcher.Fetcher2 - Unable to resolve: \n{url}\n  , skipping.\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - java.lang.NullPointerException\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetchItemQueues.finishFetchItem(Fetcher2.java:327)\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetchItemQueues.finishFetchItem(Fetcher2.java:323)\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - at org.apache.nutch.fetcher.Fetcher2$FetcherThread.run(Fetcher2.java:632)\n2007-12-30 15:34:42,721 FATAL fetcher.Fetcher2 - fetcher caught:java.lang..NullPointerException\n2007-12-30 15:34:42,721 INFO  fetcher.Fetcher2 - -finishing thread FetcherThread, activeThreads=49",
        "Issue Links": []
    },
    "NUTCH-598": {
        "Key": "NUTCH-598",
        "Summary": "Remove deprecated use of ToolBase, Migration to the new implementation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Emmanuel Joke",
        "Created": "02/Jan/08 08:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Mar/08 10:33",
        "Description": "All major nutch command are using the deprecated ToolBase class. We need to upgrade the code to extend Configured, implements the interface Tool and use ToolRunner.run().",
        "Issue Links": []
    },
    "NUTCH-599": {
        "Key": "NUTCH-599",
        "Summary": "nutch crawl and index problem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "sudarat",
        "Created": "08/Jan/08 01:46",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "08/Jan/08 07:44",
        "Description": "first i set \n\nskip file:, ftp:, & mailto: urls\n-^(file|ftp|mailto):\n\n\nskip image and other suffixes we can't yet parse\n#-\\.(png|PNG|ico|ICO|css|sit|eps|wmf|zip|mpg|gz|rpm|tgz|mov|MOV|exe|bmp|BMP)$\n\n\nskip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n\nskip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.(/.+?)/.?\\1/.*?\\1/\n\n\nskip everything else\n+.\n\n in conf/crawl-urlfilter.txt and use this command \"bin/nutch crawl urls -dir crawled -depth 3\"  i can crawl http://guide.kanook.com but i can't crawl http://www.kapook.com , some webpage can't crawl all why? and index file after crawl don't have segments file for nutch search it have only\nrw-rr- 1 nutch users   365 \u0e21.\u0e04.  7 16:47 _0.fdt\nrw-rr- 1 nutch users     8 \u0e21.\u0e04.  7 16:47 _0.fdx\nrw-rr- 1 nutch users    66 \u0e21.\u0e04.  7 16:47 _0.fnm\nrw-rr- 1 nutch users   370 \u0e21.\u0e04.  7 16:47 _0.frq\nrw-rr- 1 nutch users     9 \u0e21.\u0e04.  7 16:47 _0.nrm\nrw-rr- 1 nutch users   611 \u0e21.\u0e04.  7 16:47 _0.prx\nrw-rr- 1 nutch users   135 \u0e21.\u0e04.  7 16:47 _0.tii\nrw-rr- 1 nutch users 10553 \u0e21.\u0e04.  7 16:47 _0.tis\nrw-rr- 1 nutch users     0 \u0e21.\u0e04.  7 16:47 index.done\nrw-rr- 1 nutch users    41 \u0e21.\u0e04.  7 16:47 segments_2\nrw-rr- 1 nutch users    20 \u0e21.\u0e04.  7 16:47 segments.gen\nhow to solve it?",
        "Issue Links": []
    },
    "NUTCH-600": {
        "Key": "NUTCH-600",
        "Summary": "Nutch index problem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "sudarat",
        "Created": "09/Jan/08 04:53",
        "Updated": "11/Jan/08 18:03",
        "Resolved": "11/Jan/08 18:03",
        "Description": "after crawl wtth this command \"bin/nutch crawl urls -dir crawled -depth 3\" index file not complete it's not have segments file \nit have only \n/user/nutch/crawld/indexes/part-00000/index.done        <r 1>   0\n/user/nutch/crawld/indexes/part-00000/segments.gen      <r 1>   20\n/user/nutch/crawld/indexes/part-00000/segments_1        <r 1>   20\n/user/nutch/crawld/indexes/part-00001/index.done        <r 1>   0\n/user/nutch/crawld/indexes/part-00001/segments.gen      <r 1>   20\n/user/nutch/crawld/indexes/part-00001/segments_2        <r 1>   20\nwhich nutch search cannot work it try to find segments file.  can i solve it?",
        "Issue Links": []
    },
    "NUTCH-601": {
        "Key": "NUTCH-601",
        "Summary": "Recrawling on existing crawl directory using force option",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Susam Pal",
        "Created": "04/Feb/08 18:08",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "14/Mar/08 14:54",
        "Description": "Added a '-force' option to the 'bin/nutch crawl' command line. With this option, one can crawl and recrawl in the following manner:\n\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5 -force\n\n\nThis option can be used for the first crawl too:\n\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5 -force\nbin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5 -force\n\n\nIf one tries to crawl without the -force option when the crawl directory already exists, he/she finds a small warning along with the error message:\n\n# bin/nutch crawl urls -dir crawl -depth 2 -topN 10 -threads 5\nException in thread \"main\" java.lang.RuntimeException: crawl already\nexists. Add -force option to recrawl.\n       at org.apache.nutch.crawl.Crawl.main(Crawl.java:89)",
        "Issue Links": []
    },
    "NUTCH-602": {
        "Key": "NUTCH-602",
        "Summary": "Allow configurable number of handlers for search servers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "05/Feb/08 16:49",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "07/Feb/08 22:25",
        "Description": "This improvement changes the distributed search server to allow a configurable number of RPC handlers.  Before the number was hardcoded at 10 handlers.  For high volume environments that limit will be quickly reached and the overall search will slowdown.  The patch changes nutch-default.xml with the configuration parameter searchers.num.handlers and changes DistributedSearch to pull the number of handlers from the configuration.",
        "Issue Links": []
    },
    "NUTCH-603": {
        "Key": "NUTCH-603",
        "Summary": "Add more default url normalizations",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "05/Feb/08 16:58",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "14/Feb/08 22:17",
        "Description": "By default the regex-urlnormalizers only remove PHPSESSID strings.  I propose adding in more default url normalizers including expressions for removing different types of session ids, removing default pages, remvoing interpage links, and cleaning up url strings.  The point of these expressions is to decrease the number of duplicate urls that are being stored and scored in the crawl database and being fetched.",
        "Issue Links": []
    },
    "NUTCH-604": {
        "Key": "NUTCH-604",
        "Summary": "Upgrade Nutch to Lucene 2.3.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "05/Feb/08 22:32",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "06/Feb/08 12:09",
        "Description": "Upgrade Nutch to Lucene 2.3.0 release jars.",
        "Issue Links": []
    },
    "NUTCH-605": {
        "Key": "NUTCH-605",
        "Summary": "Change deprecated configuration methods for Hadoop",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "08/Feb/08 01:07",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "12/Feb/08 14:54",
        "Description": "Changes use of the now deprecated addFinalResource and addDefaultResource methods to just use addResouce",
        "Issue Links": []
    },
    "NUTCH-606": {
        "Key": "NUTCH-606",
        "Summary": "Refactoring of Generator, run all urls through checks",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "generator",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "08/Feb/08 22:10",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "12/Feb/08 14:51",
        "Description": "Refactor the generator to make sure all host run through checks such as host and protocol checks, ip checks if necessary.  Currently the generator only does this for urls if generate.max.per.host > 0 which by default is -1.  So by default all urls will get collected without checks.",
        "Issue Links": []
    },
    "NUTCH-607": {
        "Key": "NUTCH-607",
        "Summary": "Update build.xml to include tika jar in war file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "08/Feb/08 22:23",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "09/Feb/08 18:42",
        "Description": "Update the build.xml to include the tika jar in the war file.  Currently the jar is not included and the cached.jsp page errors out.",
        "Issue Links": []
    },
    "NUTCH-608": {
        "Key": "NUTCH-608",
        "Summary": "Upgrade nutch to use released apache-tika-0.1-incubating",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "09/Feb/08 01:47",
        "Updated": "08/Jun/11 21:35",
        "Resolved": "12/Feb/08 14:33",
        "Description": "This patch will upgrade Nutch to use the released tika-0.1-incubating jar containing stable APIs and code, as opposed to the -dev version of the jar file that's currently in place in SVN.",
        "Issue Links": []
    },
    "NUTCH-609": {
        "Key": "NUTCH-609",
        "Summary": "Allow Plugins to be Loaded from Jar File(s)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "09/Feb/08 18:55",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Currently plugins cannot be loaded from a jar file.  Plugins must be unzipped in one or more directories specified by the plugin.folders config.  I have been thinking about an extension to PluginRepository or PluginManifestParser (or both) that would allow plugins to packaged into multiple independent jar files and placed on the classpath.  The system would search the classpath for resources with the correct folder name and would load any plugins in those jars.\nThis functionality would be very useful in making the nutch core more flexible in terms of packaging.  It would also help with web applications where we don't want to have a plugins directory included in the webapp.\nThoughts so far are unzipping those plugin jars into a common temp directory before loading.  Another option is using something like commons vfs to interact with the jar files.  VFS essential uses a disk based temporary cache for jar files, so it is pretty much the same solution.   What are everyone else's thoughts on this?",
        "Issue Links": []
    },
    "NUTCH-610": {
        "Key": "NUTCH-610",
        "Summary": "Can't Update or modify an index while web gui is running",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Ciminera Frederic",
        "Created": "11/Feb/08 18:11",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "17/Mar/08 16:50",
        "Description": "When the search web application is started a NutchBean is created and initializes its searcher on the index files (and also a FetchedSegment on segments).\nThis index searcher (and also FetchedSegment) is holding a lock on the files on disk that prevent the index to be updated or modified.\nIt would be nice to be able to update an index without having to restart the web server.",
        "Issue Links": []
    },
    "NUTCH-611": {
        "Key": "NUTCH-611",
        "Summary": "Upgrade Nutch to use Hadoop 0.16",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "13/Feb/08 02:01",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "14/Feb/08 22:22",
        "Description": "Upgrade Nutch to use the recently released Hadoop 0.16 libraries.  This change removes the deprecated methods addDefaultResource and addFinalResource for configuration.  All configuration is now done through addResource.",
        "Issue Links": []
    },
    "NUTCH-612": {
        "Key": "NUTCH-612",
        "Summary": "URL filtering is always disabled in Generator when invoked by Crawl",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "generator",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Susam Pal",
        "Created": "15/Feb/08 19:49",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "14/Mar/08 14:37",
        "Description": "When a crawl is done using the 'bin/nutch crawl' command, no filtering is done in Generator even if 'crawl.generate.filter' is set to true in the configuration file.\nThe problem is that in the Generator's generate method, the following code unconditionally sets the filter value of the job to whatever is passed to it:-\n\njob.setBoolean(CRAWL_GENERATE_FILTER, filter);\n\nThe code in Crawl.java always passes this as false. \nThis has been fixed by exposing an overloaded generate method which takes only the 5 arguments that Crawl needs to set. This overloaded method reads the configuration and sets the filter value appropriately.",
        "Issue Links": []
    },
    "NUTCH-613": {
        "Key": "NUTCH-613",
        "Summary": "Empty Summaries and Cached Pages",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher,                                            web gui",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dennis Kubes",
        "Created": "19/Feb/08 06:27",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "14/Mar/08 14:22",
        "Description": "There is a bug where some search results do not have summaries and viewing their cached pages causes a NullPointer.  This bug is due to redirects getting stored under the new url and the getURL method of FetchedSegments getting the wrong (old) url which is stored in crawldb but has no content or parse objects.",
        "Issue Links": [
            "/jira/browse/NUTCH-575"
        ]
    },
    "NUTCH-614": {
        "Key": "NUTCH-614",
        "Summary": "Order Inlinks by OPIC score of parent page",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0,                                            1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "20/Feb/08 04:37",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "28/Jun/08 20:15",
        "Description": "Currently when saving inlinks there is a max number of inlinks (configurable) which get saved and very little logic goes into deciding which inlinks get saved.  This patch uses the OPIC score of the encompassing page to set a score for each inlink.  Inlinks are then reverse sorted according to score and the best inlinks are saved first.  The logic behind this is that pages with higher OPIC scores should have better links which they are pointing to.",
        "Issue Links": []
    },
    "NUTCH-615": {
        "Key": "NUTCH-615",
        "Summary": "Redirected URL are fetched wihtout setting any FetchInterval",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Emmanuel Joke",
        "Created": "26/Feb/08 14:29",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Mar/08 12:33",
        "Description": "An url which is redirected result to a new URL. We create a new CrawlDatum for the new URL within the Fetcher but the FetchInterval was not initialized.\nThe new url was recorded in the DB with a FetchInterval = 0 and the FetchTime is never correctly updated to be fetch later in the future. Thus we keep crawling those URL at each generation.\nThis patch fix this issue.",
        "Issue Links": []
    },
    "NUTCH-616": {
        "Key": "NUTCH-616",
        "Summary": "Reset Fetch Retry counter when fetch is successful",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Emmanuel Joke",
        "Created": "26/Feb/08 17:25",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Mar/08 12:42",
        "Description": "We manage a counter to check how many time the URL has been consecutively in state Retry following some trouble to get the page.\nHere is a sample of the code:\ncase ProtocolStatus.RETRY:          // retry\n                 fit.datum.setRetriesSinceFetch(fit.datum.getRetriesSinceFetch()+1);\n However i notice that we don't reinitialize this counter at 0 in the case of successful fetch.",
        "Issue Links": []
    },
    "NUTCH-617": {
        "Key": "NUTCH-617",
        "Summary": "Cached Text Only",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Siddharth Jha",
        "Created": "04/Mar/08 08:45",
        "Updated": "04/Mar/08 19:22",
        "Resolved": "04/Mar/08 19:22",
        "Description": "Hello All\nI would like to know if it is possible to do Cached Text implementation of webpages in Nutch. By Cached Text , I mean that this should store only the text part of the webpage without any images??\nThanks\nSiddharth",
        "Issue Links": []
    },
    "NUTCH-618": {
        "Key": "NUTCH-618",
        "Summary": "Tika error \"Media type alias already exists\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Andrzej Bialecki",
        "Created": "06/Mar/08 07:16",
        "Updated": "08/Jun/11 21:35",
        "Resolved": "04/Jun/08 13:41",
        "Description": "After the upgrade to the latest Tika jar we see a lot of errors like this:\n2008-03-06 08:07:20,659 WARN org.apache.tika.mime.MimeTypesReader: Invalid media type alias: text/xml\norg.apache.tika.mime.MimeTypeException: Media type alias already exists: text/xml\n\tat org.apache.tika.mime.MimeTypes.addAlias(MimeTypes.java:312)\n\tat org.apache.tika.mime.MimeType.addAlias(MimeType.java:238)\n\tat org.apache.tika.mime.MimeTypesReader.readMimeType(MimeTypesReader.java:168)\n\tat org.apache.tika.mime.MimeTypesReader.read(MimeTypesReader.java:138)\n\tat org.apache.tika.mime.MimeTypesReader.read(MimeTypesReader.java:121)\n\tat org.apache.tika.mime.MimeTypesFactory.create(MimeTypesFactory.java:56)\n\tat org.apache.nutch.util.MimeUtil.(MimeUtil.java:58)\n\tat org.apache.nutch.protocol.Content.(Content.java:85)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:226)\n\tat org.apache.nutch.fetcher.Fetcher2$FetcherThread.run(Fetcher2.java:523)\nThis is caused most likely by the duplicate tika-mimetypes.xml file - one copy is embedded inside the Tika jar, the other is found in Nutch conf/ directory. The one inside the jar seems to be more recent, so I propose to simply remove the one we have in conf.",
        "Issue Links": []
    },
    "NUTCH-619": {
        "Key": "NUTCH-619",
        "Summary": "Another Language Identifier Plugin using Unicode code point range",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Vinci",
        "Created": "15/Mar/08 15:40",
        "Updated": "28/Nov/11 13:11",
        "Resolved": "28/Nov/11 13:11",
        "Description": "After I checked the language-identifier plugin, I found the internal implementation is inefficient for language that can be clear identify based on their unicode codepoint  (e.g. CJK Language)\nIf Nutch work under unicode, can anybody write a language identifier based on unicode  code point range? The map is here:\nhttp://en.wikipedia.org/wiki/Basic_Multilingual_Plane\nalso you can refer to NutchAnalysis.jj for some of language code range \n\nSome late developed language or rare character - include some CJK character, are moved to SIP\nMay be a special property should be set if multiple language character detected (languages that are other than English alphabet) - my suggestion here is, let CJK locale be the default case as they need bi-gram or other analyzer for better indexing\n\t\nCJK character is very difficult to further divide as they are share han characters - if you really want to identify the specific  member of CJK, you need to use the language identifier plugin",
        "Issue Links": []
    },
    "NUTCH-620": {
        "Key": "NUTCH-620",
        "Summary": "BasicURLNormalizer should collapse runs of slashes with a single slash",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Mark DeSpain",
        "Created": "16/Mar/08 07:22",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Mar/08 10:44",
        "Description": "The BasicURLNormalizer should collapse runs of slash characters '/' with a single slash.  \nFor example,  the following URLs should be normalized to http://lucene.apache.org/nutch/about.html\n\nhttp://lucene.apache.org/nutch//about.html\n\n\nhttp://lucene.apache.org//nutch/about.html\n\n\nhttp://lucene.apache.org/////nutch////about.html (an exaggerated example)",
        "Issue Links": []
    },
    "NUTCH-621": {
        "Key": "NUTCH-621",
        "Summary": "Nutch needs to declare it's crypto usage",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "0.7,                                            0.7.1,                                            0.7.2,                                            0.8,                                            0.8.1,                                            0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Grant Ingersoll",
        "Created": "18/Mar/08 13:00",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "29/Sep/08 13:05",
        "Description": "Per the ASF board direction outlined at http://www.apache.org/dev/crypto.html, Nutch needs to declare it's use of crypto libraries (i.e. BouncyCastle, via PDFBox/Tika).\nSee TIKA-118.",
        "Issue Links": []
    },
    "NUTCH-622": {
        "Key": "NUTCH-622",
        "Summary": "Support for application/x-suggestions+json",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Bobby Hubbard",
        "Created": "26/Mar/08 15:09",
        "Updated": "28/Nov/11 13:12",
        "Resolved": "28/Nov/11 13:12",
        "Description": "Would be really cool to expose an application/x-suggestions+json search reply to support OpenSearch [1] suggestions.\n[1] http://developer.mozilla.org/en/docs/Supporting_search_suggestions_in_search_plugins",
        "Issue Links": []
    },
    "NUTCH-623": {
        "Key": "NUTCH-623",
        "Summary": "Change plugin source directory \"languageidentifier\" to \"language-identifier\"",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Ignacio J. Ortega",
        "Created": "29/Mar/08 18:33",
        "Updated": "14/Oct/11 09:06",
        "Resolved": "11/Oct/11 22:08",
        "Description": "When trying to develop and debug Nutch  in eclipse, following the instructions at http://wiki.apache.org/nutch/RunNutchInEclipse0%2e9, you cant run with languageidentifier is rename to language-identifier, when later issue an svn update, you end having two languageidentifier src dirs, one with the dash and another without it, it's an annoyance only, i know, but it stucks me for 2 weeks..so if can be corrected...",
        "Issue Links": []
    },
    "NUTCH-624": {
        "Key": "NUTCH-624",
        "Summary": "Better parsed text by default parser",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Vinci",
        "Created": "30/Mar/08 11:55",
        "Updated": "09/Jan/09 18:58",
        "Resolved": "09/Jan/09 18:58",
        "Description": "I found the parsed text by default parser, Neko in 1.0 nightly is not easy to process - it just add a space to the end of the tag. \nFor easier analysis,  neko (or other parser) should change the behaviour to \n1.adding tab for inline element\n2.add a tab+newline for block level element end\ninstead of  space\nThat will help another application to use the parsed text.",
        "Issue Links": []
    },
    "NUTCH-625": {
        "Key": "NUTCH-625",
        "Summary": "Non-ascii character broken in dumped content for mixed encoding (utf-8 and multi-byte)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Vinci",
        "Created": "30/Mar/08 12:05",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 20:05",
        "Description": "If the crawl db contains both utf-8 non-ascii character and non-utf-8 non-ascii character(i.e. multi-byte character), the dumped contents by readseg utility will have garbled character appear in all of the non-utf8 non-ascii text, and those texts are unable to repair by encoding reload.\nAt the same time, the utf-8 text is normal, only the non-utf8 text broken.\nAny possible solution available for repairing the broken text?",
        "Issue Links": []
    },
    "NUTCH-626": {
        "Key": "NUTCH-626",
        "Summary": "fetcher2 breaks out the domain with db.ignore.external.links set at cross domain redirects",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Sami Siren",
        "Reporter": "Remco Verhoef",
        "Created": "06/Apr/08 21:16",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Feb/09 09:18",
        "Description": "Fetcher2 breaks out of the db.ignore.external.links directive when encounterin a cross domain redirect. The redirected url is followed without checking for db.ignore.external.links and cross domain.",
        "Issue Links": []
    },
    "NUTCH-627": {
        "Key": "NUTCH-627",
        "Summary": "Minimize host address lookup",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": "Otis Gospodnetic",
        "Reporter": "Otis Gospodnetic",
        "Created": "10/Apr/08 04:10",
        "Updated": "25/Jan/09 11:40",
        "Resolved": "13/Jan/09 22:17",
        "Description": "The simple patch that I'm about to attach keeps track of hosts whose \"max URLs per host\" limit we already reached, as well as hosts whose hostname->IP lookup already failed.  For such hosts, further DNS lookups are skipped:\n\nthere is no point in looking up a hostname yet again if we already have the max number of URLs for that host\nthere is little point in attempting to look up a hostname yet again if the previous lookup already failed\n\nIn a simple test, this saved a few hundred thousand lookups for the first case and a few hundred lookups for the second case.\nIf nobody complains, I'll commit by the end of the week.",
        "Issue Links": []
    },
    "NUTCH-628": {
        "Key": "NUTCH-628",
        "Summary": "Host database to keep track of host-level information",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            generator",
        "Assignee": null,
        "Reporter": "Otis Gospodnetic",
        "Created": "12/Apr/08 04:19",
        "Updated": "09/Jul/12 13:54",
        "Resolved": "09/Jul/12 13:54",
        "Description": "Nutch would benefit from having a DB with per-host/domain/TLD information.  For instance, Nutch could detect hosts that are timing out, store information about that in this DB.  Segment/fetchlist Generator could then skip such hosts, so they don't slow down the fetch job.  Another good use for such a DB is keeping track of various host scores, e.g. spam score.\nFrom the recent thread on nutch-user@lucene:\nOtis asked:\n> While we are at it, how would one go about implementing this DB, as far as its structures go?\nAndrzej said:\nThe easiest I can imagine is to use something like <Text, MapWritable>.\nThis way you could store arbitrary information under arbitrary keys.\nI.e. a single database then could keep track of aggregate statistics at\ndifferent levels, e.g. TLD, domain, host, ip range, etc. The basic set\nof statistics could consist of a few predefined gauges, totals and averages.",
        "Issue Links": [
            "/jira/browse/NUTCH-882",
            "/jira/browse/NUTCH-655"
        ]
    },
    "NUTCH-629": {
        "Key": "NUTCH-629",
        "Summary": "Detect slow and timeout servers and drop their URLs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Otis Gospodnetic",
        "Reporter": "Otis Gospodnetic",
        "Created": "12/Apr/08 07:06",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Oct/11 22:21",
        "Description": "Fetch jobs will finish faster if we find a way to prevent servers that are either slow or time out from slowing down the whole process.\nI'll attach a patch that counts per-server exceptions and timeouts and tracks download speed per server.\nQueues/sservers that exceed timeout or download thresholds are marked as \"tooManyErrors\" or \"tooSlow\".  Once they get marked as such, all of their subsequent URLs get dropped (i.e. they do not fetched) and marked GONE.\nAt the end of the fetch task, stats for each server processed are printed.\nAlso, I believe the per-host/domain/TLD/etc. DB from NUTCH-628 would be the right place to add server data collected by this patch.",
        "Issue Links": []
    },
    "NUTCH-630": {
        "Key": "NUTCH-630",
        "Summary": "Error caused by index-more plugin  in the latest svn revision - 652259",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "taknev ivrok",
        "Created": "30/Apr/08 15:21",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "02/Oct/08 09:37",
        "Description": "This problem is reported in the user mailng list: http://www.nabble.com/index-more-problem--td16757538.html\nUpon running bin/nutch  crawl urls -dir crawl  in the latest svn version the following error occurs. \nNote: This error does not happen after I remove index-more plugin from plugin.includes in the conf/nutch-site.xml file. \nFetcher: done\nCrawlDb update: starting\nCrawlDb update: db: crawlfs/crawldb\nCrawlDb update: segments: [crawlfs/segments/20080430051112]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: Merging segment data into db.\nCrawlDb update: done\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: starting\nGenerator: segment: crawlfs/segments/20080430051126\nGenerator: filtering: true\nGenerator: topN: 100000\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=2 - no more URLs to fetch.\nLinkDb: starting\nLinkDb: linkdb: crawlfs/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: adding segment:\nfile:/home/admin/nutch-2008-04-30_04-01-32/crawlfs/segments/20080430051112\nLinkDb: adding segment:\nfile:/home/admin/nutch-2008-04-30_04-01-32/crawlfs/segments/20080430051053\nLinkDb: done\nIndexer: starting\nIndexer: linkdb: crawlfs/linkdb\nIndexer: adding segment:\nfile:/home/admin/nutch-2008-04-30_04-01-32/crawlfs/segments/20080430051112\nIndexer: adding segment:\nfile:/home/admin/nutch-2008-04-30_04-01-32/crawlfs/segments/20080430051053\nIFD [Thread-102]: setInfoStream\ndeletionPolicy=org.apache.lucene.index.KeepOnlyLastCommitDeletionPolicy@1cfd3b2\nIW 0 [Thread-102]: setInfoStream:\ndir=org.apache.lucene.store.FSDirectory@/tmp/hadoop-admin/mapred/local/index/_1406110510\nautoCommit=true\nmergePolicy=org.apache.lucene.index.LogByteSizeMergePolicy@1536eec\nmergeScheduler=org.apache.lucene.index.ConcurrentMergeScheduler@9770a3\nramBufferSizeMB=16.0 maxBuffereDocs=50 maxBuffereDeleteTerms=-1\nmaxFieldLength=10000 index=\nException in thread \"main\" java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:894)\n        at org.apache.nutch.indexer.Indexer.index(Indexer.java:311)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:144)",
        "Issue Links": []
    },
    "NUTCH-631": {
        "Key": "NUTCH-631",
        "Summary": "MoreIndexingFilter fails with NoSuchElementException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Sami Siren",
        "Reporter": "Stefan Will",
        "Created": "09/May/08 20:55",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "17/Feb/09 14:29",
        "Description": "I did a simple crawl and started the indexer with the index-more plugin activated. The index job fails with the following stack trace in the task log:\njava.util.NoSuchElementException\n        at java.util.TreeMap.key(TreeMap.java:433)\n        at java.util.TreeMap.firstKey(TreeMap.java:287)\n        at java.util.TreeSet.first(TreeSet.java:407)\n        at java.util.Collections$UnmodifiableSortedSet.first(Collections.java:1114)\n        at org.apache.nutch.indexer.more.MoreIndexingFilter.addType(MoreIndexingFilter.java:207)\n        at org.apache.nutch.indexer.more.MoreIndexingFilter.filter(MoreIndexingFilter.java:90)\n        at org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:111)\n        at org.apache.nutch.indexer.Indexer.reduce(Indexer.java:249)\n        at org.apache.nutch.indexer.Indexer.reduce(Indexer.java:52)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:333)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:164)\nI traced this down to the part in MoreIndexingFilter where the mime type is split into primary type and subtype for indexing:\n    contentType = mimeType.getName();\n    String primaryType = mimeType.getSuperType().getName();\n    String subType = mimeType.getSubTypes().first().getName();\nApparently Tika does not have a subtype for text/html. Furthermore, the supertype for text/html is set as application/octet-stream, which I doubt is what we want indexed. Don't we want primaryType to be \"text\" and subType to be \"html\" ?\nSo I changed the code to:\n    contentType = mimeType.getName();\n    String[] split = contentType.split(\"/\");\n    String primaryType = split[0];\n    String subType = (split.length>1)?split[1]:null;\nThis does what I think it should do, but perhaps I'm missing something ?",
        "Issue Links": []
    },
    "NUTCH-632": {
        "Key": "NUTCH-632",
        "Summary": "Bug in TextParser with encoding",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Antony Bowesman",
        "Created": "20/May/08 02:58",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "02/Oct/08 09:32",
        "Description": "If a Content object is created with the following Content-Type: text/plain; charset=\"windows-1251\"\nthe Content object discards the charset parameter.  As a result, when the TextParser calls\nString encoding = StringUtil.parseCharacterEncoding(content.getContentType());\nit always gets null because the contentType stored in the Content object no longer contains the charset string.  The code has changed a lot from 0.9, so I am not sure if this is still a problem, but I made a fix that simply saves charset in Content with\n    if (this.contentType.startsWith(\"text/\"))\n        this.charset = StringUtil.parseCharacterEncoding(contentType);\nand TextParser just calls\n    String encoding = content.getCharset();",
        "Issue Links": []
    },
    "NUTCH-633": {
        "Key": "NUTCH-633",
        "Summary": "ParseSegment no longer allow reparsing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Xue Yong Zhi",
        "Created": "26/May/08 07:27",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 16:43",
        "Description": "ParseSegment used to allow reparsing even if parsing has been enabled in Fetcher. But now it throws a NumberFormatException as 'content.getMetadata().get(Nutch.FETCH_STATUS_KEY)' is null.\nThis patch will fix the problem:\n\u2014 a/src/java/org/apache/nutch/parse/ParseSegment.java\n+++ b/src/java/org/apache/nutch/parse/ParseSegment.java\n@@ -70,8 +70,10 @@ public class ParseSegment extends Configured implements Tool, Mapper<WritableCom\n       key = newKey;\n     }\n+    //status_key is only available when parsing is not done in fetcher\n+    String status_key = content.getMetadata().get(Nutch.FETCH_STATUS_KEY);\n     int status =\n\nInteger.parseInt(content.getMetadata().get(Nutch.FETCH_STATUS_KEY));\n+      (null == status_key) ? CrawlDatum.STATUS_FETCH_SUCCESS : Integer.parseInt(status_key);\n     if (status != CrawlDatum.STATUS_FETCH_SUCCESS) {\n       // content not fetched successfully, skip document\n       LOG.debug(\"Skipping \" + key + \" as content is not fetched successfully\");",
        "Issue Links": []
    },
    "NUTCH-634": {
        "Key": "NUTCH-634",
        "Summary": "Patch - Nutch - Hadoop 0.17.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Michael Gottesman",
        "Created": "10/Jun/08 04:41",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "05/Aug/08 17:30",
        "Description": "This is a patch so that Nutch can be used with Hadoop 0.17.0. The patch is located at http://pastie.org/212001\nThe patch compiles and passes all current Nutch unit tests.\nI have tested that the crawler side of Nutch (i.e. inject, generate, fetch, parse, merge w/crawldb) definetly works, but have not tested the lucene indexing part. It might work, but it might not. \nNOTE - the two main bugs that had to be overcome were not noticed by any of the unit tests. The bugs only came up during actual testing. The bugs were:\n1. Changes to the Hadoop Iterator\n2. Addition of Serialization to MapReduce Framework",
        "Issue Links": []
    },
    "NUTCH-635": {
        "Key": "NUTCH-635",
        "Summary": "LinkAnalysis Tool for Nutch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "12/Jun/08 18:11",
        "Updated": "02/May/13 02:29",
        "Resolved": "04/Dec/08 21:18",
        "Description": "This is a basic pagerank type link analysis tool for nutch which simulates a sparse matrix using inlinks and outlinks and converges after a given number of iterations.  This tool is mean to replace the current scoring system in nutch with a system that converges instead of exponentially increasing scores.  Also includes a tool to create an outlinkdb.",
        "Issue Links": [
            "/jira/browse/NUTCH-646",
            "/jira/browse/NUTCH-647"
        ]
    },
    "NUTCH-636": {
        "Key": "NUTCH-636",
        "Summary": "Http client plug-in https doesn't work on IBM JRE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Curtis d'Entremont",
        "Created": "23/Jun/08 19:10",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "06/Feb/09 13:17",
        "Description": "I want to crawl my site, which is https, using the protocol-httpclient plug-in. However it throws exceptions each request, something about an unknown algorithm \"SunX509\" for SSL. I don't recall the exact message. I don't have permission to change the JRE on our production server.\nI had to modify DummyX509TrustManager to hardcode the string to \"IbmX509\" instead of \"SunX509\" in order to work. It would be better if the plug-in could automatically figure out which one to use. At the very least, try the major ones until you don't hit any exception and take that one.",
        "Issue Links": []
    },
    "NUTCH-637": {
        "Key": "NUTCH-637",
        "Summary": "Add method to nutch and tika system(Code written)",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Michael Bostwick",
        "Created": "16/Jul/08 18:32",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Nov/08 16:50",
        "Description": "public MimeType getMimeType(byte[] content)\n{\n\t  return this.mimeTypes.getMimeType(content);\n  }\n paste this code into nutch/src/java/org/apache/nutch/util/MimeUtil.java which will allow byte content to be used to determine file type",
        "Issue Links": []
    },
    "NUTCH-638": {
        "Key": "NUTCH-638",
        "Summary": "Launching Distributed Searchers with URI indicating filesystem to use rather than relying on hadoop config files.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Aaron Nall",
        "Created": "28/Jul/08 19:32",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:36",
        "Description": "I wanted to conduct all index creation operations in hdfs but search from the local file system using the same cluster of machines.  I believe that this is a common use case.  \nThis required either a parallel nutch install or edits (scripted or manual) to hadoop-site.xml to change the file system from hdfs to local when starting a distributed searcher service.  This minor patch makes IndexSearcher and NutchBean honor URIs as supported by hadoop 0.17 without altering existing functionality if simple paths are entered.",
        "Issue Links": []
    },
    "NUTCH-639": {
        "Key": "NUTCH-639",
        "Summary": "Change LuceneDocumentWrapper visibility from private to protected",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Guillaume Smet",
        "Created": "06/Aug/08 08:20",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "20/Sep/08 17:05",
        "Description": "Hi,\nI'm currently working on a nutch-solr integration based on http://wiki.apache.org/nutch/RunningNutchAndSolr .\nI changed what is proposed in this page a bit so that I don't have to patch Nutch anymore - I just have to add a jar containing a custom Indexer in nutch/lib.\nThe only remaining problem which forced me to patch nutch is the following step:\n9. Edit nutch-trunk/src/java/org/apache/nutch/indexer/Indexer.java changing scope on LuceneDocumentWrapper from private to protected\nCould we consider changing the visibility of LuceneDocumentWrapper to protected directly in Nutch trunk so that we could inherit from Indexer to create our own ones without patching nutch?\nI attached the (trivial...) patch to do so.\nThanks.",
        "Issue Links": []
    },
    "NUTCH-640": {
        "Key": "NUTCH-640",
        "Summary": "confusing description \"set it to Integer.MAX_VALUE\"",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": "Dogacan Guney",
        "Reporter": "Stijn Vermeeren",
        "Created": "06/Aug/08 12:47",
        "Updated": "03/Oct/08 04:18",
        "Resolved": "02/Oct/08 09:17",
        "Description": "This property \"indexer.max.tokens\" has the following description in nutch-default.xml :\n\" The maximum number of tokens that will be indexed for a single field\n  in a document. This limits the amount of memory required for\n  indexing, so that collections with very large files will not crash\n  the indexing process by running out of memory.\n  Note that this effectively truncates large documents, excluding\n  from the index tokens that occur further in the document. If you\n  know your source documents are large, be sure to set this value\n  high enough to accomodate the expected size. If you set it to\n  Integer.MAX_VALUE, then the only limit is your memory, but you\n  should anticipate an OutOfMemoryError.\"\nApparently, \"set it to Integer.MAX_VALUE\" here means <<substitute the integer value of Integer.MAX_VALUE>>, and not <<put the text \"Integer.MAX_VALUE\" between the value tags>>. I think this is very confusing and the description should be improved.\nI first put <value>Integer.MAX_VALUE</value> in my configuration, and it took a long time to figure out what was wrong, especially since Nutch rolled back on the default value of 10000 instead of giving an error.",
        "Issue Links": []
    },
    "NUTCH-641": {
        "Key": "NUTCH-641",
        "Summary": "IndexSorter incorrectly copies stored fields",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "06/Aug/08 21:57",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Aug/08 00:44",
        "Description": "Recent versions of Lucene introduced IndexReader.document(int, FieldSelector) method. When using IndexWriter.addIndexes(IndexReader[]) Lucene now uses that method from IndexReader instead of the old one IndexReader.document(int).\nUnfortunately, this new method is not overriden in IndexSorter, which leads to a subtle corruption of sorted indexes - while the indexed fields are sorted properly, the values from stored fields are not sorted and remain in the sorted index in the original order. This means that in a sorted index the values of indexed fields and stored fields are completely out of sync, which later results in incorrect documents being retrieved from segments.",
        "Issue Links": []
    },
    "NUTCH-642": {
        "Key": "NUTCH-642",
        "Summary": "Unit tests fail when run in non-local mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "build",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "06/Aug/08 23:07",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Aug/08 00:50",
        "Description": "Unit tests work correctly only when run in Hadoop \"local\" mode. In distributed mode the classpath that JUnit uses doesn't contain the job jar, so Hadoop doesn't know where to find the implementing classes, and consequently all map-reduce jobs fail.",
        "Issue Links": []
    },
    "NUTCH-643": {
        "Key": "NUTCH-643",
        "Summary": "ClassCastException in PdfParser on encrypted PDF with empty password",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Guillaume Smet",
        "Created": "08/Aug/08 13:48",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "06/Feb/09 13:12",
        "Description": "Hi,\nIf a PDF document is encrypted with an empty password, the PdfParser should decrypt it using the empty password.\nThis behaviour is implemented with the following code:\n      if (pdf.isEncrypted()) \n{\n        DocumentEncryption decryptor = new DocumentEncryption(pdf);\n        //Just try using the default password and move on\n        decryptor.decryptDocument(\"\");\n      }\nIt uses a deprecated API and moreover it seems there is a bug in PDFBox in this deprecated API (we have a ClassCastException in PDFBox) as we have the following error:\n2008-08-07 19:15:56,860 WARN  parse.pdf - General exception in PDF parser: org.pdfbox.pdmodel.encryption.PDEncryptionDictionary cannot be cast to org.pdfbox.pdmodel.encryption.PDStandardEncryption\n2008-08-07 19:15:56,862 WARN  parse.pdf - java.lang.ClassCastException: org.pdfbox.pdmodel.encryption.PDEncryptionDictionary cannot be cast to org.pdfbox.pdmodel.encryption.PDStandardEncryption\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.pdfbox.encryption.DocumentEncryption.decryptDocument(DocumentEncryption.java:197)\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.apache.nutch.parse.pdf.PdfParser.getParse(PdfParser.java:98)\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:82)\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.apache.nutch.fetcher.Fetcher$FetcherThread.output(Fetcher.java:336)\n2008-08-07 19:15:56,862 WARN  parse.pdf - at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:178)\n2008-08-07 19:15:56,874 WARN  fetcher.Fetcher - Error parsing: http://www2.culture.gouv.fr/deps/fr/stateurope071.pdf: failed(2,0): Can't be handled as pdf document. java.lang.ClassCastException: org.pdfbox.pdmodel.encryption.PDEncryptionDictionary cannot be cast to org.pdfbox.pdmodel.encryption.PDStandardEncryption\nUsing the new security API, we don't have any error parsing this document and we can get its content:\n\t\t\tif (pdf.isEncrypted()) \n{\n\t\t\t\t// Just try using the default password and move on\n\t\t\t\tpdf.openProtection(new StandardDecryptionMaterial(\"\"));\n\t\t\t}\n\nI attached the patch fixing this problem: it works perfectly with the above document and get rids of the deprecated API.\nRegards,\n\u2013 \nGuillaume",
        "Issue Links": []
    },
    "NUTCH-644": {
        "Key": "NUTCH-644",
        "Summary": "RTF parser doesn't compile anymore",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Guillaume Smet",
        "Created": "08/Aug/08 14:26",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "18/Feb/10 10:49",
        "Description": "Due to API changes, the RTF parser (which is not compiled by default due to licensing problem) doesn't compile anymore.\nThe build.xml script doesn't work anymore too as http://www.cobase.cs.ucla.edu/pub/javacc/rtf_parser_src.jar doesn't exist anymore (404). I didn't fix the build.xml as I don't know from where we want to get the jar file but only the compilations issues.\nRegards,\n\u2013 \nGuillaume",
        "Issue Links": [
            "/jira/browse/NUTCH-705"
        ]
    },
    "NUTCH-645": {
        "Key": "NUTCH-645",
        "Summary": "Parse-swf unit test failing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "build",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "19/Aug/08 00:29",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Aug/08 00:44",
        "Description": "Parse-swf unit tests fail under Java 1.6, but run successfuly under Java 1.5",
        "Issue Links": []
    },
    "NUTCH-646": {
        "Key": "NUTCH-646",
        "Summary": "New Indexing Framework for Nutch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0,                                            1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "19/Aug/08 04:03",
        "Updated": "02/May/13 02:29",
        "Resolved": "04/Dec/08 21:29",
        "Description": "New indexing framework for Nutch that provides a more generic field abstraction consistent with Lucene index semantics.  Allows multiple MR jobs to be created for different fields and those fields to be aggregated and indexed in the end.  Overcomes limitations of the current indexer that limits what databases are passed into the indexer.  Creates a new extension point as well for field-filters for manipulation of fields during the indexing process.",
        "Issue Links": [
            "/jira/browse/NUTCH-635"
        ]
    },
    "NUTCH-647": {
        "Key": "NUTCH-647",
        "Summary": "Resolve URLs tool",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "19/Aug/08 04:05",
        "Updated": "02/May/13 02:29",
        "Resolved": "02/Dec/08 14:51",
        "Description": "A tool that takes a listing of urls and attempts to resolve their IP addresses.  Useful for running after the fetcher has run to determine if DNS problems exist.",
        "Issue Links": [
            "/jira/browse/NUTCH-635"
        ]
    },
    "NUTCH-648": {
        "Key": "NUTCH-648",
        "Summary": "debian style autocomplete",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jim",
        "Created": "28/Aug/08 00:24",
        "Updated": "22/May/13 03:54",
        "Resolved": "10/Dec/12 16:59",
        "Description": "Here is a suggested improvement:  At the end of this file is a debian style bash autocomplete script, just place into /etc/bash_complete.d/ with filename nutch, and you can tab complete at the command prompt, ie\nbash> nutch [tab][tab]\n   crawl readdb convdb mergedb readlinkdb inject generate freegen fetch fetch2 parse\n   readseg mergesegs updatedb invertlinks mergelinkdb index merge dedup plugin server\nbash> nutch c[tab][tab]\n   crawl convdb\netc.\n   This also includes optional parameters, and filename completion where it can be used.  I really like having this when typing in long nutch commands, and think it would be a great addition to the project.\n   The file is heavily taken from the corresponding svn file that does the same thing.\nFile begins here:\nshopt -s extglob\n_nutch()\n{\n       local cur cmds cmdOpts optsParam opt\n       local i\n       COMPREPLY=()\n       cur=${COMP_WORDS[COMP_CWORD]}\n\nPossible expansions\n       cmds='crawl readdb convdb mergedb readlinkdb inject generate freegen fetch fetch2 parse readseg mergesegs updatedb invertlinks \\\nmergelinkdb index merge dedup plugin server'\n\n       if [[ $COMP_CWORD -eq 1 ]] ; then\n               COMPREPLY=( $( compgen -W \"$cmds\" \u2013 $cur ) )\n               return 0\n       fi\n\noptions that require a parameter\nThis needs to be filled in better\n       optsParam=\"-topN|-depth\"\n\n\nif not typing an option, or if the previous option required a\nparameter, then fallback on ordinary filename expansion\n       if [[ \"$cur\" != -* ]] || \\\n          [[ ${COMP_WORDS[COMP_CWORD-1]} == @($optsParam) ]] ; then\n               return 0\n       fi\n\n\npossible options for the command\n       cmdOpts=\n       case ${COMP_WORDS[1]} in\n       crawl)\n               cmdOpts=\"-dir -threads -depth -topN\"\n               ;;\n       readdb)\n               cmdOpts=\"-stats -dump -topN -url\"\n               ;;\n       convdb)\n               cmdOpts=\"-withMetadata\"\n               ;;\n       mergedb)\n               cmdOpts=\"-normalize -filter\"\n               ;;\n       readlinkdb)\n               cmdOpts=\"-dump -url\"\n               ;;\n       inject)\n               cmdOpts=\"\"\n               ;;\n       generate)\n               cmdOpts=\"-force -topN -numFetchers -adddays -noFilter\"\n               ;;\n       freegen)\n               cmdOpts=\"-filter -normalize\"\n               ;;\n       fetch)\n               cmdOpts=\"-threads -noParsing\"\n               ;;\n       fetch2)\n               cmdOpts=\"-threads -noParsing\"\n               ;;\n       parse)\n               cmdOpts=\"\"\n               ;;\n       readseg)\n               cmdOpts=\"-dump -list -get -nocontent -nofetch -nogenerate -noparse -noparsedata -noparsetext -dir\"\n               ;;\n       mergesegs)\n               cmdOpts=\"-dir -filter -slice\"\n               ;;\n       updatedb)\n               cmdOpts=\"-dir -force -normalize -filter -noAdditions\"\n               ;;\n       invertlinks)\n               cmdOpts=\"-dir -force -noNormalize -noFilter\"\n               ;;\n       mergelinkdb)\n               cmdOpts=\"-normalize -filter\"\n               ;;\n       index)\n               cmdOpts=\"\"\n               ;;\n       merge)\n               cmdOpts=\"-workingdir\"\n               ;;\n       dedup)\n               cmdOpts=\"\"\n               ;;\n       plugin)\n               cmdOpts=\"\"\n               ;;\n       server)\n               cmdOpts=\"\"\n               ;;\n       *)\n               ;;\n       esac\n\n\ntake out options already given\n       for (( i=2; i<=$COMP_CWORD-1; ++i )) ; do\n               opt=${COMP_WORDS[$i]}\n\n               cmdOpts=\" $cmdOpts \"\n               cmdOpts=${cmdOpts/ ${opt} / }\n\nskip next option if this one requires a parameter\n               if [[ $opt == @($optsParam) ]] ; then\n                       ((++i))\n               fi\n       done\n\n       COMPREPLY=( $( compgen -W \"$cmdOpts\" \u2013 $cur ) )\n       return 0\n}\ncomplete -F _nutch -o default nutch",
        "Issue Links": []
    },
    "NUTCH-649": {
        "Key": "NUTCH-649",
        "Summary": "Log list of files found but not crawled.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Jim",
        "Created": "28/Aug/08 00:33",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "I use Nutch to find the location of executables on the web, but we do not download the executables with Nutch.  In order to get nutch to give the location of files without downloading the files, I had to make a very small patch to the code, but I think this change might be useful to others also.  The patch just logs files that are being filtered at the info level, although perhaps it should be at the debug level.\n   I have included a svn diff with this change.  Use cases would be to both use as a diagnostic tool (let's see what we are skipping) as well as a way to find content and links pointed to by a page or site without having to actually download that content.\nIndex: ParseOutputFormat.java\n===================================================================\n\u2014 ParseOutputFormat.java      (revision 593619)\n+++ ParseOutputFormat.java      (working copy)\n@@ -193,17 +193,20 @@\n                toHost = null;\n              }\n              if (toHost == null || !toHost.equals(fromHost)) \n{ // external links\n+               LOG.info(\"filtering externalLink \" + toUrl + \" linked to by \" + fromUrl);\n+\n                continue; // skip it\n              }\n            }\n            try {\n              toUrl = normalizers.normalize(toUrl,\n                          URLNormalizers.SCOPE_OUTLINK); // normalize the url\n\ntoUrl = filters.filter(toUrl);   // filter the url\nif (toUrl == null) \n{\n-                continue;\n-              }\n} catch (Exception e) \nUnknown macro: {++             if (filters.filter(toUrl) == null) {   // filter the url\n+                     LOG.info(\"filtering content \" + toUrl + \" linked to by \" + fromUrl);\n+                     continue;\n+                 }+           } \n catch (Exception e) \n{\n              continue;\n            }\n            CrawlDatum target = new CrawlDatum(CrawlDatum.STATUS_LINKED, interval);",
        "Issue Links": []
    },
    "NUTCH-650": {
        "Key": "NUTCH-650",
        "Summary": "Hbase Integration",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "18/Sep/08 12:03",
        "Updated": "02/May/13 02:29",
        "Resolved": "10/Aug/10 16:43",
        "Description": "This issue will track nutch/hbase integration",
        "Issue Links": [
            "/jira/browse/NUTCH-664",
            "/jira/browse/NUTCH-808"
        ]
    },
    "NUTCH-651": {
        "Key": "NUTCH-651",
        "Summary": "Remove bin/{start|stop}-balancer.sh from svn tracking",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "19/Sep/08 12:04",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "22/Sep/08 11:08",
        "Description": "Files bin/\n{start|stop}\n-balancer.sh are version controlled. I don't see any reason for why they should be tracked since ant generates them anyway. So, if no one objects I will remove them from version control.",
        "Issue Links": []
    },
    "NUTCH-652": {
        "Key": "NUTCH-652",
        "Summary": "AdaptiveFetchSchedule#setFetchSchedule doesn't calculate fetch interval correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "19/Sep/08 13:01",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "12/Jan/09 13:37",
        "Description": "Fetch interval is stored in seconds, but in AdaptiveFetchSchedule#setFetchSchedule it is treated as if in milliseconds. Also, fetch interval bound checking is done after interval value is stored in CrawlDatum, thus, has no effect.",
        "Issue Links": []
    },
    "NUTCH-653": {
        "Key": "NUTCH-653",
        "Summary": "Upgrade to hadoop 0.18",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "19/Sep/08 13:04",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Sep/08 08:51",
        "Description": "This issue will track nutch's update to hadoop 0.18 version.",
        "Issue Links": []
    },
    "NUTCH-654": {
        "Key": "NUTCH-654",
        "Summary": "urlfilter-regex's main does not work",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "01/Oct/08 07:47",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "02/Oct/08 09:05",
        "Description": "As you know, nutch allows you to run plugin with \"bin/nutch plugin ...\". But, RegexURLFilter's main is broken because it is initialized without a conf, so it can't read any of the rules and fails with exception if you try to do something with it.",
        "Issue Links": []
    },
    "NUTCH-655": {
        "Key": "NUTCH-655",
        "Summary": "Injecting Crawl metadata",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "injector",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "01/Oct/08 12:46",
        "Updated": "05/Aug/10 23:04",
        "Resolved": "06/Jan/10 17:02",
        "Description": "the patch attached allows to inject metadata into the crawlDB. The input file has to contain fields separated by tabs, with the URL being on the first column. The metadata names and values are separated by '='. A input line might look like this:\nhttp://www.myurl.com  \\t  categ=value1 \\t categ2=value2\nThis functionality can be useful to store external knowledge and index it with a custom plugin",
        "Issue Links": [
            "/jira/browse/NUTCH-628"
        ]
    },
    "NUTCH-656": {
        "Key": "NUTCH-656",
        "Summary": "DeleteDuplicates based on crawlDB only",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "09/Oct/08 08:35",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "14/Nov/13 11:56",
        "Description": "The existing dedup functionality relies on Lucene indices and can't be used when the indexing is delegated to SOLR.\nI was wondering whether we could use the information from the crawlDB instead to detect URLs to delete then do the deletions in an indexer-neutral way. As far as I understand the content of the crawlDB contains all the elements we need for dedup, namely :\n\nURL\nsignature\nfetch time\nscore\n\nIn map-reduce terms we would have two different jobs : \n\nread crawlDB and compare on URLs : keep only most recent element - oldest are stored in a file and will be deleted later\n\n\nread crawlDB and have a map function generating signatures as keys and URL + fetch time +score as value\nreduce function would depend on which parameter is set (i.e. use signature or score) and would output as list of URLs to delete\n\nThis assumes that we can then use the URLs to identify documents in the indices.\nAny thoughts on this? Am I missing something?\nJulien",
        "Issue Links": [
            "/jira/browse/NUTCH-1324",
            "/jira/browse/NUTCH-1047",
            "/jira/browse/NUTCH-1688"
        ]
    },
    "NUTCH-657": {
        "Key": "NUTCH-657",
        "Summary": "Estonian N-gram profile has wrong name",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "0.8.1,                                            0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jonathan Young",
        "Created": "14/Oct/08 17:11",
        "Updated": "28/Nov/11 13:20",
        "Resolved": "28/Nov/11 13:20",
        "Description": "The Nutch language identifier plugin contains an ngram profile, ee.ngp, in src/plugin/languageidentifier/src/java/org/apache/nutch/analysis/lang .  \"ee\" is the ISO-3166-1-alpha-2 code for Estonia (see http://www.iso.org/iso/country_codes/iso_3166_code_lists/english_country_names_and_code_elements.htm), but it is the ISO-639-2 code for Ewe (see \nhttp://www.loc.gov/standards/iso639-2/php/English_list.php).  \"et\" is the ISO-639-2 code for Estonian, and the language profile in ee.ngp is clearly Estonian.\nProposed solution: rename ee.ngp to et.ngp .",
        "Issue Links": []
    },
    "NUTCH-658": {
        "Key": "NUTCH-658",
        "Summary": "Add Counter for # of doc fetched in Reporter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "31/Oct/08 16:31",
        "Updated": "05/Jan/10 10:16",
        "Resolved": "05/Jan/10 10:16",
        "Description": "Having a Counter for the number of documents fetched duplicates the information in the status of each Map but should be summed across all the Map instances and displayed along with the standard Counters. The same mechanism could be used for the other possible status of a URL (redir / gone etc...)",
        "Issue Links": []
    },
    "NUTCH-659": {
        "Key": "NUTCH-659",
        "Summary": "Help! No urls fetched for internal repository website",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Bryan",
        "Created": "09/Nov/08 23:33",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "12/Nov/08 05:02",
        "Description": "I am new to Nutch, and implemented Nutch for my internal company websites search. The version is nutch-2008-11-02_04-01-26.tar.\nMy internal company websites includes several HTTP websites. \nAnother one is SVN repository HTTPS websites in XML structure, using <dir> and <file> tag.\nThe search in HTTP websites is good. \nThe HTTPS is ok. We have some links in those HTTP websites which point to Word files under SVN website. They can be indexed.\nBut the Nutch does not search my SVN website. If I only search the SVN website, it is always: 0 urls fetched.\nMy nutch-site.xml is as following:\n<property>\n  <name>plugin.includes</name>\n  <value>protocol-httpclient|urlfilter-regex|parse-(text|html|js|msexcel|msword|mspowerpoint|pdf|zip|swf|rss)|index-(basic|anchor)|query-(basic|site|url)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)</value>\n\nskip file:, ftp:, & mailto: urls\n\n-^(ftp|mailto):\n\naccept hosts in MY.DOMAIN.NAME\n\n+^http://([a-z0-9]*\\.)*smartlabs.com.au/\nAny help would be much appreciated. Thanks in advnce.",
        "Issue Links": []
    },
    "NUTCH-660": {
        "Key": "NUTCH-660",
        "Summary": "Does anybody know how to let nutch crawl this kind of website?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Invalid",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Bryan",
        "Created": "11/Nov/08 05:26",
        "Updated": "25/Jan/09 11:40",
        "Resolved": "12/Nov/08 05:00",
        "Description": "My company intranet website is a svn repository, similar to : http://svn.apache.org/repos/asf/lucene/nutch/ .\nDoes anybody have an idea on how to let nutch do search on it?\nThanks.\nBryan",
        "Issue Links": []
    },
    "NUTCH-661": {
        "Key": "NUTCH-661",
        "Summary": "errors when the uri contains space characters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Christos LAIOS",
        "Created": "12/Nov/08 17:15",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "20/Jan/09 20:48",
        "Description": "While spidering our intranet, i get the following errors when the uri contains space characters\nfetch of http://intranet-rtd.rtd.cec.eu.int/services/docs/AAR_2007 - FINAL.doc failed with: java.lang.IllegalArgumentException: Invalid uri 'http://intranet-rtd.rtd.cec.eu.int/services/docs/AAR_2007 - FINAL.doc': escaped absolute path not valid",
        "Issue Links": []
    },
    "NUTCH-662": {
        "Key": "NUTCH-662",
        "Summary": "Upgrade Nutch to use Lucene 2.4",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "21/Nov/08 10:15",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "02/Dec/08 14:41",
        "Description": "Upgrade nutch to use Lucene 2.4.  This release changes the lucene file format.  New indexes created by this lucene version will NOT be readable by older versions.  Lucene 2.4 can read and update older index formats although updating an older format will convert it to the new format.  There are also some performance and functionality improvments.",
        "Issue Links": []
    },
    "NUTCH-663": {
        "Key": "NUTCH-663",
        "Summary": "Upgrade Nutch to use Hadoop 0.19",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "21/Nov/08 10:17",
        "Updated": "02/Jun/09 05:37",
        "Resolved": "02/Dec/08 14:47",
        "Description": "Upgrade Nutch to use a newer hadoop, version 0.18.2.  This includes performance improvements, bug fixes, and new functionality.  Changes some current APIs.",
        "Issue Links": []
    },
    "NUTCH-664": {
        "Key": "NUTCH-664",
        "Summary": "Possibility to update already stored documents.",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sergey Khilkov",
        "Created": "26/Nov/08 06:30",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "We have huge index of stored documents. It is high cost procedure to fetch page, merge indexes any time we update some information about page. The information can be changed 1-3 times per day. At this moment we have to store changed info in database, but in this case we have lots of problems with sorting, search restricions and so on. Lucene itself allows delete single document and add new one into existing index. But there is a problem with hadoop... As I understand hadoop filesystem has no possibility to write in random positions. But it will be great feature if nutch will be able to update created index.",
        "Issue Links": [
            "/jira/browse/NUTCH-650"
        ]
    },
    "NUTCH-665": {
        "Key": "NUTCH-665",
        "Summary": "Search Load Testing Tool",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "26/Nov/08 14:40",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "02/Dec/08 14:56",
        "Description": "A tool which spawn a number of threads and executes searches against configured search servers.  This is used for light load testing of search servers.",
        "Issue Links": []
    },
    "NUTCH-666": {
        "Key": "NUTCH-666",
        "Summary": "Analysis plugins for multiple language and new Language Identifier Tool",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "26/Nov/08 14:54",
        "Updated": "09/Aug/11 16:51",
        "Resolved": "09/Aug/11 16:28",
        "Description": "Add analysis plugins for czech, greek, japanese, chinese, korean, dutch, russian, and thai.  Also includes a new Language Identifier tool that used the new indexing framework in NUTCH-646.",
        "Issue Links": [
            "/jira/browse/TIKA-369"
        ]
    },
    "NUTCH-667": {
        "Key": "NUTCH-667",
        "Summary": "Input Format for working with Content in Hadoop Streaming",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "26/Nov/08 15:57",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "02/Dec/08 14:59",
        "Description": "This is a ContextAsText input format that removes line endings with spaces that allow Nutch content to be used more effectively inside of Hadoop streaming jobs that allow MapReduce jobs to be written in any language that can communicate with stdin and stdout.",
        "Issue Links": []
    },
    "NUTCH-668": {
        "Key": "NUTCH-668",
        "Summary": "Domain URL Filter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "03/Dec/08 01:41",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "29/Dec/08 17:58",
        "Description": "A URLFilter that adds the ability to filter out URLs by top level domain or by hostname.  A configuration file with a listing of URLs is used to denote accepted urls.",
        "Issue Links": []
    },
    "NUTCH-669": {
        "Key": "NUTCH-669",
        "Summary": "Consolidate code for Fetcher and Fetcher2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Sami Siren",
        "Reporter": "Todd Lipcon",
        "Created": "04/Dec/08 21:12",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "02/Mar/09 12:30",
        "Description": "I'd like to consolidate a lot of the common code between Fetcher and Fetcher2.java.\nIt seems to me like there are the following differences:\n\nFetcher relies on the Protocol to obey robots.txt and crawl delay settings whereas Fetcher2 implements them itself\nFetcher2 uses a different queueing model (queue per crawl host) to accomplish the per-host limiting without making the Protocol do it.\n\nI've begun work on this but want to check with people on the following:\n\nWhat reason is there for Fetcher existing at all since Fetcher2 seems to be a superset of functionality?\n\n\nIs it on the road map to remove the robots/delay logic from the Http protocol and make Fetcher2's delegation of duties the standard?\n\n\nAny other improvements wanted for Fetcher while I am in and around the code?",
        "Issue Links": []
    },
    "NUTCH-670": {
        "Key": "NUTCH-670",
        "Summary": "feed plugin does not parse RSS2 enclosures",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Todd Lipcon",
        "Created": "10/Dec/08 04:28",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The feed parse in plugins/feed does not get count links found in RSS2 \"enclosure\" tags as Outlinks.\nIt's a pretty simple patch - SyndEntry has a getEnclosures call. I'll submit the patch tomorrow.",
        "Issue Links": []
    },
    "NUTCH-671": {
        "Key": "NUTCH-671",
        "Summary": "JSP errors in Nutch searcher webapp running with Tomcat 6",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Edwin Chu",
        "Created": "10/Dec/08 07:13",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "03/Feb/09 15:44",
        "Description": "Tomcat throws an exception like the following when accessing search.jsp, cached.jsp, explain.jsp and anchors.jsp.\norg.apache.jasper.JasperException: /anchors.jsp(63,22) Attribute value  language + \"/include/header.html\" is quoted with \" which must be escaped when used within the value\nIt is caused by the stricter rules about attribute value quoting implemented in Tomcat 6.",
        "Issue Links": []
    },
    "NUTCH-672": {
        "Key": "NUTCH-672",
        "Summary": "allow unit tests to be run from bin/nutch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Todd Lipcon",
        "Created": "11/Dec/08 03:54",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "29/Sep/11 11:41",
        "Description": "In development it's handy to be able to run a single test case easily. You can do it with ant -Dtestcase=foo test, but that's slow since it still checks all the plugins for changes, rebuilds jars, etc.\nThis patch adds a command to bin/nutch to run a junit against what's already compiled. It's much faster than using ant. Recommended for use with nutch -core",
        "Issue Links": []
    },
    "NUTCH-673": {
        "Key": "NUTCH-673",
        "Summary": "Upgrade the Carrot2 plug-in to release 3.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Sean Dean",
        "Created": "15/Dec/08 23:01",
        "Updated": "13/Apr/11 22:50",
        "Resolved": "13/Apr/11 22:50",
        "Description": "Release 3.0 of the Carrot2 plug-in was released recently.\nWe currently have version 2.1 in the source tree and upgrading it to the latest version before 1.0-release might make sence.\nDetails on the release can be found here: http://project.carrot2.org/release-3.0-notes.html\nOne major change in requirements is for JDK 1.5 to be used, but this is also now required for Hadoop 0.19 so this wouldnt be the only reason for the switch.",
        "Issue Links": [
            "/jira/browse/NUTCH-787"
        ]
    },
    "NUTCH-674": {
        "Key": "NUTCH-674",
        "Summary": "NutchBean doesn't check for searcher.dir existance.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kuba Ko\u0144czyk",
        "Created": "18/Dec/08 13:12",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "If searcher.dir doesn't exists or it's not accessible, searcher will just continue and report that there is 0 hits found.It should throw an exception or log an error instead.As an starting point, there was a patch proposed some time ago on Nuch-dev: http://www.mail-archive.com/nutch-developers@lists.sourceforge.net/msg09422.html to solve this problem.",
        "Issue Links": []
    },
    "NUTCH-675": {
        "Key": "NUTCH-675",
        "Summary": "Reduce tasks do not report their status and are killed by jobtracker",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "0.9.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "sha feng",
        "Created": "22/Dec/08 03:08",
        "Updated": "25/Jan/09 11:39",
        "Resolved": "23/Dec/08 18:55",
        "Description": "We choose Fetcher2 as our fetcher. Map tasks of Fetcher2 fetches about 2,000,000 urls, but at reduce stage, all reduce tasks can not report their status and be killed by jobtracker. Although we change mapred.task.timeout from 60,000 to 1,800,000, it does not work. So, who can tell us why? By the way, the version of Nutch we use is 0.9 and the version of Hadoop is 0.12. \nThanks for your help!",
        "Issue Links": []
    },
    "NUTCH-676": {
        "Key": "NUTCH-676",
        "Summary": "MapWritable is written inefficiently and confusingly",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Todd Lipcon",
        "Created": "30/Dec/08 20:56",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "21/Jan/09 19:26",
        "Description": "The MapWritable implemention in o.a.n.crawl is written confusingly - it maintains its own internal linked list which I think may have a bug somewhere (I'm getting an NPE in certain cases in the code, though it's hard to track down)\nCan anyone comment as to why MapWritable is written the way it is, rather than just using a HashMap or a LinkedHashMap if consistent ordering is important? I imagine that would improve performance.\nWhat about just using the Hadoop MapWritable? Obviously that would break some backwards compatibility but it may be a good idea at some point to reduce confusion (I didn't realize that Nutch had its own impl until a few minutes ago)",
        "Issue Links": []
    },
    "NUTCH-677": {
        "Key": "NUTCH-677",
        "Summary": "Segment merge filering based on segment content",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.2",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Marcin Okraszewski",
        "Created": "08/Jan/09 22:03",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "25/Jul/10 06:56",
        "Description": "I needed a segment filtering based on meta data detected during parse phase. Unfortunately current URL based filtering does not allow for this. So I have created a new SegmentMergeFilter extension which receives segment entry which is being merged and decides if it should be included or not. Even though I needed only ParseData for my purpose I have done it a bit more general purpose, so the filter receives all merged data.\nThe attached patch is for version 0.9 which I use. Unfortunately I didn't have time to check how it fits to trunk version. Sorry",
        "Issue Links": []
    },
    "NUTCH-678": {
        "Key": "NUTCH-678",
        "Summary": "Hadoop 0.19 requires an update of jets3t",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Julien Nioche",
        "Created": "14/Jan/09 21:23",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Jan/09 17:10",
        "Description": "jets3t-0.6.0.jar is currently in the lib directory. Hadoop 0.19 relies on the version 0.6.1\nI am getting java.lang.NoSuchMethodError: org.jets3t.service.S3Service.moveObject(Ljava/lang/String;Ljava/lang/String;Ljava/lang/String;Lorg/jets3t/service/model/S3Object;Z)Ljava/util/Map;\n\tat org.apache.hadoop.fs.s3native.Jets3tNativeFileSystemStore.rename(Jets3tNativeFileSystemStore.java:228)\nwhen using distcp with jets3t-0.6.0.jar. I haven't tried with 0.6.1 yet but I suspect this is the cause. It won't hurt to upgrade jets3t anyway",
        "Issue Links": []
    },
    "NUTCH-679": {
        "Key": "NUTCH-679",
        "Summary": "Fetcher2 implementing Tool",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "15/Jan/09 13:33",
        "Updated": "10/Oct/09 04:45",
        "Resolved": "09/Oct/09 15:57",
        "Description": "The patch attached makes Fetcher2 implement Tool. As a result we should be able to override parameters on the command line e.g. \nbin/nutch fetch2 -Dfetcher.server.min.delay=1.0 -Dmapred.reduce.tasks=4 segments/20090115072836\ninstead of having to modify the *-site.xml files in conf/",
        "Issue Links": []
    },
    "NUTCH-680": {
        "Key": "NUTCH-680",
        "Summary": "Update external jars to latest versions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "19/Jan/09 14:00",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "28/Jan/09 14:13",
        "Description": "This issue will be used to update external libraries nutch uses. \nThese are the libraries that are outdated (upon a quick glance):\nnekohtml (1.9.9)\nlucene-highlighter (2.4.0)\njdom (1.1)\ncarrot2 - as mentioned in another issue\njets3t - above\nicu4j (4.0.1)\njakarta-oro (2.0.8)\nWe should probably update tika to whatever the latest is as well before 1.0.\nPlease add ones  I missed in comments.\nAlso what exactly is pmd-ext? There is an extra jakarta-oro and jaxen there.....",
        "Issue Links": [
            "/jira/browse/NUTCH-700"
        ]
    },
    "NUTCH-681": {
        "Key": "NUTCH-681",
        "Summary": "parse-mp3 compilation problem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Wildan Maulana",
        "Created": "20/Jan/09 08:17",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "21/Jan/09 13:10",
        "Description": "Due to API changes, the MP3 parser (which is not compiled by default due to licensing problem) doesn't compile anymore.\ncompile:\n[echo] Compiling plugin: parse-mp3\n[javac] Compiling 2 source files to /home/wildan/jobstuff/LIPI/Ngoprek/nutch/build/parse-mp3/classes\n[javac] /home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/parse-mp3/src/java/org/apache/nutch/parse/mp3/MP3Parser.java:53: org.apache.nutch.parse.mp3.MP3Parser is not abstract and does not override abstract method getParse(org.apache.nutch.protocol.Content) in org.apache.nutch.parse.Parser\n[javac] public class MP3Parser implements Parser {\n[javac]        ^\n[javac] /home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/parse-mp3/src/java/org/apache/nutch/parse/mp3/MP3Parser.java:58: getParse(org.apache.nutch.protocol.Content) in org.apache.nutch.parse.mp3.MP3Parser cannot implement getParse(org.apache.nutch.protocol.Content) in org.apache.nutch.parse.Parser; attempting to use incompatible return type\n[javac] found   : org.apache.nutch.parse.Parse\n[javac] required: org.apache.nutch.parse.ParseResult\n[javac]   public Parse getParse(Content content) {\n[javac]                ^\n[javac] /home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/parse-mp3/src/java/org/apache/nutch/parse/mp3/MetadataCollector.java:54: cannot find symbol\n[javac] symbol  : constructor Outlink(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)\n[javac] location: class org.apache.nutch.parse.Outlink\n[javac]       links.add(new Outlink(value, \"\", this.conf));\n[javac]                 ^\n[javac] Note: /home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/parse-mp3/src/java/org/apache/nutch/parse/mp3/MetadataCollector.java uses unchecked or unsafe operations.\n[javac] Note: Recompile with -Xlint:unchecked for details.\n[javac] 3 errors\nBUILD FAILED\n/home/wildan/jobstuff/LIPI/Ngoprek/nutch/build.xml:113: The following error occurred while executing this line:\n/home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/build.xml:55: The following error occurred while executing this line:\n/home/wildan/jobstuff/LIPI/Ngoprek/nutch/src/plugin/build-plugin.xml:111: Compile failed; see the compiler error output for details.",
        "Issue Links": []
    },
    "NUTCH-682": {
        "Key": "NUTCH-682",
        "Summary": "SOLR indexer does not set boost on the document",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "29/Jan/09 18:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "29/Jan/09 19:12",
        "Description": "the method org.apache.nutch.indexer.solr.SolrWriter.write(NutchDocument doc) should call the method setDocumentBoost as illustrated below :  \nfor(final Entry<String, List<String>> e : doc) {\n      for (final String val : e.getValue()) \n{\n        inputDoc.addField(e.getKey(), val);\n      }\n    }\n    inputDoc.setDocumentBoost(doc.getScore());\nas done by the LuceneWriter.",
        "Issue Links": []
    },
    "NUTCH-683": {
        "Key": "NUTCH-683",
        "Summary": "NUTCH-676 broke CrawlDbMerger",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "29/Jan/09 19:45",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "11/Feb/09 09:14",
        "Description": "Switch to hadoop's MapWritable broke CrawlDbMerger. Part of the reason is that we reuse the same MapWritable instance during reduce\nwhich apparently is a big no-no for hadoop's MapWritable. Also, hadoop's MapWritable#putAll doesn't work (I think.... see HADOOP-5142),\nso we should also work around that.",
        "Issue Links": []
    },
    "NUTCH-684": {
        "Key": "NUTCH-684",
        "Summary": "Dedup support for Solr",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "30/Jan/09 16:34",
        "Updated": "25/Sep/13 10:29",
        "Resolved": "09/Mar/09 17:35",
        "Description": "After NUTCH-442, nutch now can index to both solr and lucene. However, duplicate deletion feature (based on digests) is only available in lucene. It should also be available for solr.",
        "Issue Links": []
    },
    "NUTCH-685": {
        "Key": "NUTCH-685",
        "Summary": "Content-level redirect status lost in ParseSegment",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "06/Feb/09 10:11",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "When Fetcher runs in parsing mode, content-level redirects (HTML meta tag \"Refresh\") are properly discovered and recorded in crawl_fetch under source URL and target URL. If Fetcher runs in non-parsing mode, and ParseSegment is run as a separate step, the content-level redirection data is used only to add the new (target) URL, but the status of the original URL is not reset to indicate a redirect. Consequently, status of the original URL will be different depending on the way you run Fetcher, whereas it should be the same.",
        "Issue Links": [
            "/jira/browse/NUTCH-2261"
        ]
    },
    "NUTCH-686": {
        "Key": "NUTCH-686",
        "Summary": "Russian Analysis Plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "OpenTeam.ru",
        "Created": "10/Feb/09 05:19",
        "Updated": "10/Feb/09 05:29",
        "Resolved": "10/Feb/09 05:29",
        "Description": "This patch creates Russian Analysis plugin",
        "Issue Links": []
    },
    "NUTCH-687": {
        "Key": "NUTCH-687",
        "Summary": "Add RAT",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "17/Feb/09 13:59",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "18/Feb/09 08:12",
        "Description": "Add apache rat so we can easily see the situation with required headers",
        "Issue Links": []
    },
    "NUTCH-688": {
        "Key": "NUTCH-688",
        "Summary": "Fix missing/wrong headers in source files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "17/Feb/09 14:03",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "18/Feb/09 09:15",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-689": {
        "Key": "NUTCH-689",
        "Summary": "Swf parser doesn't seem to handle relative links",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Peter Sparks",
        "Created": "17/Feb/09 20:54",
        "Updated": "01/Apr/11 15:26",
        "Resolved": "01/Apr/11 15:26",
        "Description": "I was using the swf parser to extract links from flash files on the site www.arnoldworldwide.com and I was getting an malformed url exception because an outlink was found and it was a relative link that wasn't being resolved. I was able to fix it by resolving all links as they are added to the list of outlinks.",
        "Issue Links": []
    },
    "NUTCH-690": {
        "Key": "NUTCH-690",
        "Summary": "bug in DomContentUtils.shouldThrowAwayLink?",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Peter Sparks",
        "Created": "17/Feb/09 21:08",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "I found a potential bug in DomContentUtils.shouldThrowAwayLink. It returns true for the 5 links at the top of the home page for www.aksteel.com. Here are the links in the source:\n            <a id=\"Search\" href=\"/search/default.aspx\"></a>\n            <a id=\"Investor\" style=\"height: 15px;\" href=\"/investor_information/\"></a>\n            <a id=\"Markets\" href=\"/markets_products/\"></a>\n            <a id=\"Production\" href=\"/production_facilities/\"></a>\n            <a id=\"News\" href=\"/news/\"></a>\nPerhaps I am just ignorant of what this function is supposed to do but returning true for these 5 links on that site make that site impossible to crawl.",
        "Issue Links": []
    },
    "NUTCH-691": {
        "Key": "NUTCH-691",
        "Summary": "Update jakarta poi jars to the most relevant version",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dmitry Lihachev",
        "Created": "18/Feb/09 04:31",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "18/Feb/09 12:43",
        "Description": "Update  jakarta poi jars to the most relevant version closes bug NUTCH-591.",
        "Issue Links": [
            "/jira/browse/NUTCH-591"
        ]
    },
    "NUTCH-692": {
        "Key": "NUTCH-692",
        "Summary": "AlreadyBeingCreatedException with Hadoop 0.19",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "18/Feb/09 12:30",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "15/Mar/10 15:53",
        "Description": "I have been using the SVN version of Nutch on an EC2 cluster and got some AlreadyBeingCreatedException during the reduce phase of a parse. For some reason one of my tasks crashed and then I ran into this AlreadyBeingCreatedException when other nodes tried to pick it up.\nThere was recently a discussion on the Hadoop user list on similar issues with Hadoop 0.19 (see http://markmail.org/search/after+upgrade+to+0%2E19%2E0). I have not tried using 0.18.2 yet but will do if the problems persist with 0.19\nI was wondering whether anyone else had experienced the same problem. Do you think 0.19 is stable enough to use it for Nutch 1.0?\nI will be running a crawl on a super large cluster in the next couple of weeks and I will confirm this issue  \nJ.",
        "Issue Links": []
    },
    "NUTCH-693": {
        "Key": "NUTCH-693",
        "Summary": "Add configurable option for treating nofollow behaviour.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrew McCall",
        "Created": "18/Feb/09 20:58",
        "Updated": "23/Aug/13 20:21",
        "Resolved": "12/Jan/13 20:41",
        "Description": "For my purposes I'd like to follow links even if they're marked nofollow- Ideally I'd like to follow them, but not pass the link juice between them. \nI've attached a patch that adds a configuration element parser.html.outlinks.ignore_nofollow which allows the parser to ignore the nofollow elements on a page.",
        "Issue Links": [
            "/jira/browse/NUTCH-795"
        ]
    },
    "NUTCH-694": {
        "Key": "NUTCH-694",
        "Summary": "Distributed Search Server fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Dr. Nadine Hochstotter",
        "Created": "19/Feb/09 08:37",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "23/Feb/09 07:03",
        "Description": "I run Nutch on a single server, I have two crawl directories, that's why I use Nutch  in distributed search server mode as described in the hadoop manual.\nBut since I have a new Trunk Version (04.02.2009) it fails. Local search on one index works fine. But distributed search throws following exception:\nIn catalina.out (server)\n2009-02-18 17:08:14,906 ERROR NutchBean - org.apache.hadoop.ipc.RemoteException: java.io.IOException: Unknown Protocol classname:org.apache.nutch.searcher.RPCSegmentBean\n       at org.apache.nutch.searcher.NutchBean.getProtocolVersion(NutchBean.java:403)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)\n       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)\n       at org.apache.hadoop.ipc.Client.call(Client.java:696)\n       at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:216)\n       at $Proxy4.getProtocolVersion(Unknown Source)\n       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:319)\n       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:306)\n       at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:343)\n       at org.apache.nutch.searcher.DistributedSegmentBean.<init>(DistributedSegmentBean.java:103)\n       at org.apache.nutch.searcher.NutchBean.<init>(NutchBean.java:111)\n       at org.apache.nutch.searcher.NutchBean.<init>(NutchBean.java:80)\n       at org.apache.nutch.searcher.NutchBean$NutchBeanConstructor.contextInitialized(NutchBean.java:422)\n       at org.apache.catalina.core.StandardContext.listenerStart(StandardContext.java:3843)\n       at org.apache.catalina.core.StandardContext.start(StandardContext.java:4350)\n       at org.apache.catalina.core.StandardContext.reload(StandardContext.java:3099)\n       at org.apache.catalina.manager.ManagerServlet.reload(ManagerServlet.java:913)\n       at org.apache.catalina.manager.HTMLManagerServlet.reload(HTMLManagerServlet.java:536)\n       at org.apache.catalina.manager.HTMLManagerServlet.doGet(HTMLManagerServlet.java:114)\n       at javax.servlet.http.HttpServlet.service(HttpServlet.java:690)\n       at javax.servlet.http.HttpServlet.service(HttpServlet.java:803)\n       at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)\n       at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)\n       at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)\n       at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:175)\n       at org.apache.catalina.authenticator.AuthenticatorBase.invoke(AuthenticatorBase.java:525)\n       at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:128)\n       at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)\n       at org.apache.catalina.valves.RequestFilterValve.process(RequestFilterValve.java:269)\n       at org.apache.catalina.valves.RemoteAddrValve.invoke(RemoteAddrValve.java:81)\n       at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)\n       at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:286)\n       at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:844)\n       at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:583)\n       at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:447)\n       at java.lang.Thread.run(Thread.java:619)\nAnd in Hadoop.log:\n2009-02-18 17:07:52,847 INFO  ipc.Server - IPC Server handler 48 on 13001: starting\n2009-02-18 17:07:52,847 INFO  ipc.Server - IPC Server handler 49 on 13001: starting\n2009-02-18 17:07:52,847 INFO  ipc.Server - IPC Server handler 40 on 13001: starting\n2009-02-18 17:08:14,675 INFO  ipc.RPC - Call: getProtocolVersion(org.apache.nutch.searcher.RPCS...\n2009-02-18 17:08:14,857 INFO  ipc.RPC - Return: 1\n2009-02-18 17:08:14,878 INFO  ipc.RPC - Call: getProtocolVersion(org.apache.nutch.searcher.RPCS...\n2009-02-18 17:08:14,879 INFO  ipc.Server - IPC Server handler 0 on 13001, call getProtocolVersion(org.apache.nutch.searcher.RPCSegmentBean, 1) from 78.46.86.99:40851: error: java.io.IOException: Unknown Protocol classname:org.apache.nutch.searcher.RPCSegmentBean\njava.io.IOException: Unknown Protocol classname:org.apache.nutch.searcher.RPCSegmentBean\n       at org.apache.nutch.searcher.NutchBean.getProtocolVersion(NutchBean.java:403)\n       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n       at java.lang.reflect.Method.invoke(Method.java:597)\n       at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:452)\n       at org.apache.hadoop.ipc.Server$Handler.run(Server.java:892)\n2009-02-18 17:08:14,879 INFO  ipc.RPC - Call: ping()\n2009-02-18 17:08:14,938 INFO  ipc.RPC - Return: true\n2009-02-18 17:08:24,876 INFO  ipc.RPC - Call: ping()\nWe do not run Nutch in PseudoDistributedMode. We only use the distributed search mode. With Nutch-0.9 this was working properly.",
        "Issue Links": []
    },
    "NUTCH-695": {
        "Key": "NUTCH-695",
        "Summary": "incorrect mime type detection by MoreIndexingFilter plugin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Sami Siren",
        "Reporter": "Dmitry Lihachev",
        "Created": "19/Feb/09 10:04",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Feb/09 10:26",
        "Description": "When server sends Content-Type header with optional params like Content-Type: text/html; charset=UTF-8 MoreIndexingFilter returns null in type field.",
        "Issue Links": []
    },
    "NUTCH-696": {
        "Key": "NUTCH-696",
        "Summary": "Timeout for Parser",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchbase,                                            1.2,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "19/Feb/09 16:56",
        "Updated": "22/May/13 03:54",
        "Resolved": "03/Aug/10 12:30",
        "Description": "I found that the parsing sometimes crashes due to a problem on a specific document, which is a bit of a shame as this blocks the rest of the segment and Hadoop ends up finding that the node does not respond. I was wondering about whether it would make sense to have a timeout mechanism for the parsing so that if a document is not parsed after a time t, it is simply treated as an exception and we can get on with the rest of the process.\nDoes that make sense? Where do you think we should implement that, in ParseUtil?",
        "Issue Links": [
            "/jira/browse/NUTCH-700"
        ]
    },
    "NUTCH-697": {
        "Key": "NUTCH-697",
        "Summary": "Generate log output for solr indexer and dedup",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Dmitry Lihachev",
        "Created": "20/Feb/09 08:09",
        "Updated": "22/May/13 03:54",
        "Resolved": "14/Jul/10 18:12",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-838"
        ]
    },
    "NUTCH-698": {
        "Key": "NUTCH-698",
        "Summary": "CrawlDb is corrupted after a few crawl cycles",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "20/Feb/09 08:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Feb/09 10:10",
        "Description": "After change to hadoop's MapWritable, crawldb becomes corrupted after some fetch cycles. For more details see this discussion thread:\nhttp://www.nabble.com/Fetcher2-crashes-with-current-trunk-td21978049.html",
        "Issue Links": []
    },
    "NUTCH-699": {
        "Key": "NUTCH-699",
        "Summary": "Add an \"official\" solr schema for solr integration",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "20/Feb/09 10:32",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Feb/09 06:21",
        "Description": "See Andrzej's comments on NUTCH-684 for more info.",
        "Issue Links": []
    },
    "NUTCH-700": {
        "Key": "NUTCH-700",
        "Summary": "Neko1.9.11 goes into a loop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Julien Nioche",
        "Created": "20/Feb/09 10:44",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "02/Mar/09 10:21",
        "Description": "Neko1.9.11 goes into a loop on some documents e.g. \nhttp://mediacet.com/Archive/FourYorkshiremen/bb/post.htm\nhttp://cizel.co.kr/main.php\nreverting to 0.9.4 seems to fix the problem\nThe approach mentioned in https://issues.apache.org/jira/browse/NUTCH-696 could be a way to alleviate similar issues\nPS: haven't had time to report to the Neko people yet, will do at some stage",
        "Issue Links": [
            "/jira/browse/NUTCH-680",
            "/jira/browse/NUTCH-696"
        ]
    },
    "NUTCH-701": {
        "Key": "NUTCH-701",
        "Summary": "Replace Fetcher with Fetcher2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "fetcher",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "24/Feb/09 10:06",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "24/Feb/09 10:54",
        "Description": "Currently there are two fetcher implementation within nutch, one too many. This task tracks the process of promoting Fetcher2.\nmy plan is basically to\n-remove Fetcher all together and rename Fetcher2 to Fetcher\n-fix crawl class so it works with F2 api.\nIf there are no objections I will proceed with this soon.",
        "Issue Links": []
    },
    "NUTCH-702": {
        "Key": "NUTCH-702",
        "Summary": "Lazy Instanciation of Metadata in CrawlDatum",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "25/Feb/09 13:05",
        "Updated": "19/Sep/09 07:20",
        "Resolved": "08/Sep/09 13:15",
        "Description": "CrawlDatum systematically instanciates its metadata, which is quite wasteful especially in the case of CrawlDBReducer when it generates a new CrawlDatum for each incoming link before storing it in a List.\nInitial testing on the lazy instanciation shows an improvement in both speed and memory consumption. I will generate a patch for it ASAP",
        "Issue Links": []
    },
    "NUTCH-703": {
        "Key": "NUTCH-703",
        "Summary": "Upgrade to Hadoop 0.19.1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "25/Feb/09 16:45",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "27/Feb/09 18:54",
        "Description": "From release notes: \"Release 0.19.1 fixes many critical bugs in 0.19.0, including **some data loss issues**.\".",
        "Issue Links": []
    },
    "NUTCH-704": {
        "Key": "NUTCH-704",
        "Summary": "ensure that more important pages are crawled first",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "kr",
        "Created": "26/Feb/09 06:50",
        "Updated": "26/Feb/09 08:29",
        "Resolved": "26/Feb/09 08:29",
        "Description": "To implement url ordering algorithms mentioned in the paper \"Efficient crawling through url ordering\" by Lawrence Page et,al.. for crawling to ensure that more \"important\" pages are crawled first.This is important as even the most powerful and successful search engines have crawled only 15% of the WWW.",
        "Issue Links": []
    },
    "NUTCH-705": {
        "Key": "NUTCH-705",
        "Summary": "parse-rtf plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dmitry Lihachev",
        "Created": "27/Feb/09 04:16",
        "Updated": "22/May/13 03:54",
        "Resolved": "18/Feb/10 10:48",
        "Description": "Demoting this issue and moving to 1.1 - current patch is not suitable due to LGPL licensed parts.",
        "Issue Links": [
            "/jira/browse/NUTCH-644",
            "/jira/browse/NUTCH-766"
        ]
    },
    "NUTCH-706": {
        "Key": "NUTCH-706",
        "Summary": "Url regex normalizer: default pattern for session id removal not to match \"newsId\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Meghna Kukreja",
        "Created": "27/Feb/09 18:47",
        "Updated": "22/May/13 03:54",
        "Resolved": "10/Oct/12 21:14",
        "Description": "Hey,\nI encountered the following problem while trying to crawl a site using\nnutch-trunk. In the file regex-normalize.xml, the following regex is\nused to remove session ids:\n<pattern>([;_]?((?i)l|j|bv_)?((?i)sid|phpsessid|sessionid)=.*?)(?|&|#|$)</pattern>.\nThis pattern also transforms a url, such as,\n\"&newsId=2000484784794&newsLang=en\" into \"&new&newsLang=en\" (since it\nmatches 'sId' in the 'newsId'), which is incorrect and hence does not\nget fetched. This expression needs to be changed to prevent this.\nThanks,\nMeghna",
        "Issue Links": [
            "/jira/browse/NUTCH-1328"
        ]
    },
    "NUTCH-707": {
        "Key": "NUTCH-707",
        "Summary": "Generation of multiple segments in multiple runs returns only 1 segment",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "1.1",
        "Component/s": "generator",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Michael Chan",
        "Created": "28/Feb/09 17:40",
        "Updated": "10/Oct/09 04:45",
        "Resolved": "09/Oct/09 12:44",
        "Description": "To generate multiple segments, generator.update.crawldb is set to true and -topN is defined to be the size of the segments. However, only one segment of size N is generated.\nFor example, I've tried it with a db containing 10,000+ links according to dump. When generator.update.crawldb is set to true and -topN is set to 5, only 1 segment of size 5 is produced.\nIt seems to me the problem is due to an incorrect recording of generation time. Selector.map assigns the generation time to each URL, even reduce only collects N many. It's perfectly fine if the generator was run once and that the db isn't updated. In the situation where the generator is run again within genDelay, all the remaining URLs will be excluded. So, I suggest the generation time should be assigned in reduce rather than map.",
        "Issue Links": []
    },
    "NUTCH-708": {
        "Key": "NUTCH-708",
        "Summary": "NutchBean: OOM due to searcher.max.hits and dedup.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Aaron Binns",
        "Created": "01/Mar/09 20:11",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "When searching an index we built for the National Archives, this one in particular: http://webharvest.gov/collections/congress110th/\nWe ran into an interesting situation.\nWe were using searcher.max.hits=1000 in order to get faster searches.  Since our index is sorted, the \"best\" documents are \"at the front\" and setting searcher.max.hits=1000 would give us a nice trade-off of search quality vs. response time.\nWhat I discovered was that with dedup (on site) enabled, we would get into this loop where the searcher.max.hits would limit the raw hits to 1000 and the deduplication code would get to the end of those 1000 results and still need more as it hadn't found enough de-dup'd results to satisfy the query.\nThe first 6 pages of results would be fine, but when we got to page 7, the NutchBean would need more than 1000 raw results in order to get 60 de-duped results.\nThe code:\n    for (int rawHitNum = 0; rawHitNum < hits.getTotal(); rawHitNum++) {\n      // get the next raw hit                                                                                                                                                                                    \n      if (rawHitNum >= hits.getLength())\n        {\n        // optimize query by prohibiting more matches on some excluded values                                                                                                                                    \n        Query optQuery = (Query)query.clone();\n        for (int i = 0; i < excludedValues.size(); i++) \n{\n          if (i == MAX_PROHIBITED_TERMS)\n            break;\n          optQuery.addProhibitedTerm(((String)excludedValues.get(i)),\n                                     dedupField);\n        }\n        numHitsRaw = (int)(numHitsRaw * rawHitsFactor);\n        if (LOG.isInfoEnabled()) \n{\n          LOG.info(\"re-searching for \"+numHitsRaw+\" raw hits, query: \"+optQuery);\n        }\n        hits = searcher.search(optQuery, numHitsRaw,\n                               dedupField, sortField, reverse);\n        if (LOG.isInfoEnabled()) \n{\n          LOG.info(\"found \"+hits.getTotal()+\" raw hits\");\n        }\n        rawHitNum = -1;\n        continue;\n      }\nThe loop constraints were never satisfied as rawHitNum and hits.getLength() are capped by searcher.max.hits (1000).  The numHitsRaw keeps increasing by a factor of 2 (rawHitsFactor) until it gets to 2^31 or so and deep down in the search library code an array is allocated using that value as the size and you get an OOM.\nWe worked around the problem by abandoning the use of searcher.max.hits.  I suppose we could have increased the value, but the index was small enough (~10GB) that disabling searcher.max.hits didn't degrade the response time too much.",
        "Issue Links": []
    },
    "NUTCH-709": {
        "Key": "NUTCH-709",
        "Summary": "JSParseFilter gets into an infinate loop and ets all the stack",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Tim Hawkins",
        "Created": "03/Mar/09 13:26",
        "Updated": "01/Apr/11 15:03",
        "Resolved": "01/Apr/11 15:03",
        "Description": "When crawling pages with seperate fetch and parse, I see processes die becuase of stack overflow. \nOutput is generaly.\njava.lang.StackOverflowError\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:146)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\n\tat org.apache.nutch.parse.js.JSParseFilter.walk(JSParseFilter.java:148)\nInspection of the code shows that this is a recursive call to walk(.....)",
        "Issue Links": []
    },
    "NUTCH-710": {
        "Key": "NUTCH-710",
        "Summary": "Support for rel=\"canonical\" attribute",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Frank McCown",
        "Created": "03/Mar/09 14:49",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "There is a the new rel=\"canonical\" attribute which is\nnow being supported by Google, Yahoo, and Live:\nhttp://googlewebmastercentral.blogspot.com/2009/02/specify-your-canonical.html\nAdding support for this attribute value will potentially reduce the number of URLs crawled and indexed and reduce duplicate page content.",
        "Issue Links": [
            "/jira/browse/NUTCH-1753"
        ]
    },
    "NUTCH-711": {
        "Key": "NUTCH-711",
        "Summary": "Indexer failing after upgrade to Hadoop 0.19.1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "04/Mar/09 10:52",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "04/Mar/09 15:11",
        "Description": "After upgrade to Hadoop 0.19.1 Reducer is initialized in a different order than before (see http://svn.apache.org/viewvc?view=rev&revision=736239). IndexingFilters populate current JobConf with field options that are required for IndexerOutputFormat to function properly. However, the filters are instantiated in Reducer.configure(), which is now called after the OutputFormat is initialized, and not before as previously.\nThe workaround for now is to instantiate IndexinigFilters once again inside IndexerOutputFormat.  This issue should be revisited before 1.1 in order to find a better solution.\nSee this thread for more information: http://www.lucidimagination.com/search/document/7c62c625c7ea17fe/problem_with_crawling_using_the_latest_1_0_trunk",
        "Issue Links": []
    },
    "NUTCH-712": {
        "Key": "NUTCH-712",
        "Summary": "ParseOutputFormat should catch java.net.MalformedURLException coming from normalizers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "06/Mar/09 11:57",
        "Updated": "29/Nov/09 04:08",
        "Resolved": "28/Nov/09 22:42",
        "Description": "ParseOutputFormat should catch java.net.MalformedURLException coming from normalizers otherwise the whole parsing step crashes instead of simply ignoring dodgy outlinks",
        "Issue Links": []
    },
    "NUTCH-713": {
        "Key": "NUTCH-713",
        "Summary": "Config options for webgraph Scoring not documented",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Eric J. Christeson",
        "Created": "09/Mar/09 15:41",
        "Updated": "24/Apr/12 14:11",
        "Resolved": "24/Apr/12 14:11",
        "Description": "There are a number of properties for webgraph scoring that are only documented in code.  I have found these:\nlink.ignore.internal.host\nlink.ignore.internal.domain\nlink.ignore.limit.domain\nlink.ignore.limit.host\nlink.ignore.limit.page\nlink.loops.depth\nlink.analyze.initial.score\nlink.analyze.damping.factor\nlink.analyze.rank.one\nlink.analyze.iteration\nlink.analyze.num.iterations\nI have a patch to add these to conf/nutch-default.xml with the best description I could find.",
        "Issue Links": []
    },
    "NUTCH-714": {
        "Key": "NUTCH-714",
        "Summary": "Need a SFTP and SCP Protocol Handler",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sanjoy Ghosh",
        "Created": "10/Mar/09 00:39",
        "Updated": "22/Sep/17 11:26",
        "Resolved": "24/Oct/10 01:09",
        "Description": "An SFTP and SCP Protocol handler is needed to fetch intranet content on an SFTP or SCP server.",
        "Issue Links": [
            "/jira/browse/NUTCH-1103",
            "/jira/browse/NUTCH-2429"
        ]
    },
    "NUTCH-715": {
        "Key": "NUTCH-715",
        "Summary": "Subcollection plugin doesn't work with default subcollections.xml file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "indexer",
        "Assignee": "Sami Siren",
        "Reporter": "Dmitry Lihachev",
        "Created": "10/Mar/09 05:54",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "10/Mar/09 07:07",
        "Description": "Subcollection plugin cann't parse his configuration file because it contatins top level comment (ASF notice) and DomUtil doesn't carry about of top-level comments",
        "Issue Links": []
    },
    "NUTCH-716": {
        "Key": "NUTCH-716",
        "Summary": "Make subcollection index filed multivalued",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Dmitry Lihachev",
        "Created": "10/Mar/09 08:42",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "26/Aug/10 14:57",
        "Description": "Looks like a reasonable thing to do. Marking as 1.2 and will commit if no one objects",
        "Issue Links": [
            "/jira/browse/NUTCH-898"
        ]
    },
    "NUTCH-717": {
        "Key": "NUTCH-717",
        "Summary": "Make Nutch Solr integration easier",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "10/Mar/09 09:56",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Erik Hatcher proposed we should provide a full solr config dir to be used with Nutch-Solr. Now we only provide index schema. It would be considerably easier to setup nutch-solr if we provided the whole conf dir that you could use with solr like:\njava -Dsolr.solr.home=<Nutch's Solr Home> -jar start.jar",
        "Issue Links": [
            "/jira/browse/NUTCH-1034",
            "/jira/browse/NUTCH-1035"
        ]
    },
    "NUTCH-718": {
        "Key": "NUTCH-718",
        "Summary": "urlfilter-subnets plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitry Lihachev",
        "Created": "12/Mar/09 09:49",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "This plugin filter urls by netmasks in CIDR-notation",
        "Issue Links": []
    },
    "NUTCH-719": {
        "Key": "NUTCH-719",
        "Summary": "fetchQueues.totalSize incorrect in Fetcher2",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "12/Mar/09 14:41",
        "Updated": "23/Feb/10 08:49",
        "Resolved": "19/Feb/10 18:51",
        "Description": "I had a look at the logs generated by Fetcher2 and found cases where there were no active fetchQueues but fetchQueues.totalSize was != 0\nfetcher.Fetcher2 - -activeThreads=200, spinWaiting=200, fetchQueues.totalSize=1, fetchQueues=0\nsince the code relies on fetchQueues.totalSize to determine whether the work is finished or not the task is blocked until the abortion mechanism kicks in\n2009-03-12 09:27:38,977 WARN  fetcher.Fetcher2 - Aborting with 200 hung threads.\ncould that be a synchronisation issue? any ideas?",
        "Issue Links": []
    },
    "NUTCH-720": {
        "Key": "NUTCH-720",
        "Summary": "site: search operator with no query term",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Frank McCown",
        "Created": "12/Mar/09 18:53",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "25/Mar/09 13:20",
        "Description": "Google, Yahoo, and Live list all pages they have indexed for the \"site:www.example.com\" query.  But Nutch returns back 0 results unless a query term is also supplied (e.g., \"site:www.example.com term\"). It would be helpful to make Nutch support the site: operator in a consistent manor.",
        "Issue Links": []
    },
    "NUTCH-721": {
        "Key": "NUTCH-721",
        "Summary": "Fetcher2 Slow",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Roger Dunk",
        "Created": "17/Mar/09 20:29",
        "Updated": "25/Aug/09 05:46",
        "Resolved": "25/Aug/09 05:46",
        "Description": "Fetcher2 fetches far more slowly than Fetcher1.\nConfig options:\nfetcher.threads.fetch = 80\nfetcher.threads.per.host = 80\nfetcher.server.delay = 0\ngenerate.max.per.host = 1\nWith a queue size of ~40,000, the result is:\nactiveThreads=80, spinWaiting=79, fetchQueues.totalSize=0\nwith maybe a download of 1 page per second.\nRuning with -noParse makes little difference.\nCPU load average is around 0.2. With Fetcher1 CPU load is around 2.0 - 3.0\nHosts already cached by local caching NS appear to download quickly upon a re-fetch, so possible issue relating to NS lookups, however all things being equal Fetcher1 runs fast without pre-caching hosts.",
        "Issue Links": []
    },
    "NUTCH-722": {
        "Key": "NUTCH-722",
        "Summary": "Nutch contains jars that we cannot redistribute",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "19/Mar/09 13:18",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "23/Mar/09 06:41",
        "Description": "It seems that we have some jars (as part of pdf parser) that we cannot redistribute.\nJukkas comment from email:\n\"\nThe release contains the Java Advanced Imaging libraries (jai_core.jar and jai_codec.jar) which are licensed under Sun's Binary Code License. We can't redistribute those libraries.\n\"",
        "Issue Links": [
            "/jira/browse/NUTCH-724"
        ]
    },
    "NUTCH-723": {
        "Key": "NUTCH-723",
        "Summary": "LICENCE.txt is lacking info that should be there",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "19/Mar/09 13:23",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Mar/09 21:11",
        "Description": "Jukkas comment from email:\n\nThe LICENSE.txt file should have at least references to the licenses of the bundled libraries.",
        "Issue Links": []
    },
    "NUTCH-724": {
        "Key": "NUTCH-724",
        "Summary": "Drop the JAI libraries",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jukka Zitting",
        "Created": "19/Mar/09 13:23",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Mar/09 13:50",
        "Description": "The PDF parser plugin contains Java Advanced Imaging (JAI) libraries (jai_core.jar and jai_codec.jar) that are licensed under the Sun Binary Code License. The license is incompatible with Apache policies, so we need to drop those libraries.\nAFAIK (see PDFBOX-381) PDFBox only uses the JAI libraries for handling page rotations and tiff images, so simply dropping the JAI jars shouldn't have too much impact. A better solution would be to switch to using Apache PDFBox that has a proper workaround for this issue, but the first Apache PDFBox release has not yet been made.",
        "Issue Links": [
            "/jira/browse/NUTCH-722"
        ]
    },
    "NUTCH-725": {
        "Key": "NUTCH-725",
        "Summary": "NOTICE.txt is lacking info that should be there",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "19/Mar/09 13:25",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Mar/09 19:49",
        "Description": "Jukkas comment from email:\n\nThe NOTICE.txt file should start with the the following lines:\n\n          Apache Nutch\n          Copyright 2009 The Apache Software Foundation\n\nThe NOTICE.txt file should contain the required copyright notices\nfrom all bundled libraries.",
        "Issue Links": []
    },
    "NUTCH-726": {
        "Key": "NUTCH-726",
        "Summary": "README.txt is lacking info that should be there",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "19/Mar/09 13:26",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Mar/09 13:48",
        "Description": "from Jukkas email:\n\nThe README.txt should start with \"Apache Nutch\" instead of \"Nutch\"",
        "Issue Links": []
    },
    "NUTCH-727": {
        "Key": "NUTCH-727",
        "Summary": "Add KEYS file to release artifact",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "19/Mar/09 13:37",
        "Updated": "10/Apr/09 12:29",
        "Resolved": "19/Mar/09 21:35",
        "Description": "comment from Grant:\n>> Where's the KEYS file for Nutch?\n>\n> hi,\n>\n> the keys file is at the top level nutch directory (eg: http://www.nic.funet.fi/pub/mirrors/apache.org/lucene/nutch/KEYS)\nOK, I think it should be in the tarball, too., at the top",
        "Issue Links": []
    },
    "NUTCH-728": {
        "Key": "NUTCH-728",
        "Summary": "Improve nutch release packaging",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "19/Mar/09 17:13",
        "Updated": "22/May/13 03:53",
        "Resolved": "09/Mar/12 11:18",
        "Description": "see the discussion from http://www.lucidimagination.com/search/document/aa4d52cbd9af026a/discuss_contents_of_nutch_release_artifact",
        "Issue Links": []
    },
    "NUTCH-729": {
        "Key": "NUTCH-729",
        "Summary": "NPE in FieldIndexer when BasicFields url doesn't exist",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "0.9.0,                                            1.0.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "25/Mar/09 14:53",
        "Updated": "19/Jul/11 13:18",
        "Resolved": "19/Jul/11 13:18",
        "Description": "There is a NullPointerException during a logging call in FieldIndexer when there isn't a url for a document.  Documents shouldn't be without urls but since the FieldIndexer doesn't validate fields it is possible for it to occur.  Most often this happens when BasicFields is run with the wrong segments directory and doesn't complain.  It could also occur if using the FieldIndexer to index things other than basic fields.",
        "Issue Links": []
    },
    "NUTCH-730": {
        "Key": "NUTCH-730",
        "Summary": "NPE in LinkRank if no nodes with which to create the WebGraph",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dennis Kubes",
        "Created": "26/Mar/09 03:13",
        "Updated": "10/Oct/09 04:45",
        "Resolved": "09/Oct/09 12:54",
        "Description": "For LinkRank, if there are no nodes to process, then a NullPointerException is thrown when trying to count number of nodes.",
        "Issue Links": []
    },
    "NUTCH-731": {
        "Key": "NUTCH-731",
        "Summary": "Redirection of robots.txt in RobotRulesParser",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "03/Apr/09 17:53",
        "Updated": "17/Sep/18 15:59",
        "Resolved": "09/Oct/09 13:13",
        "Description": "The patch attached allows to follow one level of redirection for robots.txt files. A similar issue was mentioned in NUTCH-124 and has been marked as fixed a long time ago but the problem remained, at least when using Fetcher2 . Mathijs Homminga pointed to the problem in a mail to the nutch-dev list in March.\nI have been using this patch for a while now on a large cluster and noticed that the ratio of robots_denied per fetchlist went up, meaning that at least we are now getting restrictions we would not have had before (and getting less complaints from webmasters at the same time)",
        "Issue Links": [
            "/jira/browse/NUTCH-2581",
            "/jira/browse/NUTCH-2646"
        ]
    },
    "NUTCH-732": {
        "Key": "NUTCH-732",
        "Summary": "Subcollection plugin not working on Nutch-1.0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Filipe Antunes",
        "Created": "07/Apr/09 12:07",
        "Updated": "27/Apr/10 18:17",
        "Resolved": "27/Apr/10 18:17",
        "Description": "I am trying to get subcollections working, using Nutch-1.0 !\nI configured subcolections.xml then I added the plugin on nutch-site.xml.\nWhen the index finishes, I opened lucene luke to check if the database was working properly.\nThe field subcollection is populated as it should, but searching for any subcollection, on the search tab of luke, returns no results.\nIf I do a search on the url field, I can see that every record has a subcollection associated, yet i can't search for using the  subcollection field.\nsearch examples on luke:\nsubcollection:sub1 -> no results\nurl:sub1 -> results with field subcollection populated -> sub1\nSame results using:\n./bin/nutch org.apache.nutch.searcher.NutchBean \"subcollection:sub1 sub\"\nIf i use the \"explain\", subcollection field is there with the correct word.\nIt makes no sense so i beleive it's a bug.",
        "Issue Links": []
    },
    "NUTCH-733": {
        "Key": "NUTCH-733",
        "Summary": "plain text view of cached files ignores HTML encoding",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "web gui",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Ilguiz Latypov",
        "Created": "30/Apr/09 19:53",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "14/Jul/10 20:16",
        "Description": "The plain text view of cached HitDetails is sent as raw text under the Content-Type: text/html header.\nEither the content type should be changed to text/plain (patch attached) or the text should be HTML-encoded (perhaps, using http://commons.apache.org/lang/api/org/apache/commons/lang/StringEscapeUtils.html).",
        "Issue Links": []
    },
    "NUTCH-734": {
        "Key": "NUTCH-734",
        "Summary": "option to filter \"a\" tag text",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "ron",
        "Created": "02/May/09 11:47",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jan/13 19:29",
        "Description": "Motivation:\nWhen fetching pages with \"menue links\" the menues (for example search) appear on all pages of the site. Searching for the word \"search\" then returns all pages of the site, instead of just returning the the search page.\nChange request:\nAdd options to filter texts of \"a\" tags, or more generally add filters to avoid texts within specific tags.\nI have worked around this by changing DOMContentUtils.getTextHelper : \n     if (nodeType == Node.TEXT_NODE && !(currentNode.getParentNode() != null && \"a\".equalsIgnoreCase(currentNode.getParentNode().getNodeName()))) \n\nRon",
        "Issue Links": []
    },
    "NUTCH-735": {
        "Key": "NUTCH-735",
        "Summary": "crawl-tool.xml must be read before nutch-site.xml when invoked using crawl command",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "web gui",
        "Assignee": "Dogacan Guney",
        "Reporter": "Susam Pal",
        "Created": "09/May/09 06:55",
        "Updated": "08/Jun/09 04:14",
        "Resolved": "07/Jun/09 17:12",
        "Description": "The inline documentation of 'conf/crawl-tool.xml' mentions:\n\n<!-- Do not modify this file directly.  Instead, copy entries that you -->\n<!-- wish to modify from this file into nutch-site.xml and change them -->\n<!-- there.  If nutch-site.xml does not already exist, create it.      -->\n\n\nHowever, I don't see any way of overriding the properties defined in 'conf/crawl-tool.xml' as 'conf/nutch-site.xml' is added to the configuration before 'conf/crawl-tool.xml' in the code. Here are the relevant code snippets:\nsrc/org/apache/nutch/crawl/Crawl.java:\n\nConfiguration conf = NutchConfiguration.create();\nconf.addResource(\"crawl-tool.xml\");\nJobConf job = new NutchJob(conf);\n\n\nsrc/org/apache/nutch/tool/NutchConfiguration.java:\n\nconf.addResource(\"nutch-default.xml\");\nconf.addResource(\"nutch-site.xml\");\n\n\nI have fixed this in the attached patch. 'crawl-tool.xml' is now added to the configuration before 'nutch-site.xml' only if crawl is invoked using the 'bin/nutch crawl' command.",
        "Issue Links": []
    },
    "NUTCH-736": {
        "Key": "NUTCH-736",
        "Summary": "how long it takes nutch 1.0 to fetch",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Otis Gospodnetic",
        "Reporter": "Filipe Antunes",
        "Created": "14/May/09 09:38",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "24/May/09 02:41",
        "Description": "I need an opinion about how long it takes nuch 1.0 to fetch a web site.\nAt the moment I'm indexing 3000 sites (medical area). university's, clinics, hospitals, associations, journals (html, docs, PDF, txt, xls).\nSo far I have 5 segments (64Gb) and the its fetching the 6th.\nUsing an Intel 2.8 Core2Duo OS X 10.5.6 with 4Mbit internet connection (the machine is throttled to 64Kbytes during the day (8 hours)) and this fetching started one month ago.\nDoes anyone have statistics of how long a site (# of pages) nutch 1.0 takes?",
        "Issue Links": []
    },
    "NUTCH-737": {
        "Key": "NUTCH-737",
        "Summary": "urlnormalizer-unalias plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dmitry Lihachev",
        "Created": "26/May/09 04:15",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 20:26",
        "Description": "I tried to search any whole site duplication detection tools without success. This plugin allows to do domain name transformation (for example www.google.com -> google.com). It is very stupid, but can be useful when fighting with site aliases. For detect site aliases I use my own ugly class (based on SolrDeleteDuplicates).",
        "Issue Links": [
            "/jira/browse/NUTCH-1319"
        ]
    },
    "NUTCH-738": {
        "Key": "NUTCH-738",
        "Summary": "Close SegmentUpdater when FetchedSegments is closed",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Martina Koch",
        "Created": "26/May/09 06:40",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "28/Nov/09 21:27",
        "Description": "Currently FetchedSegments starts a SegmentUpdater, but never closes it when FetchedSegments is closed.\n(The problem was described in this mailing: http://www.mail-archive.com/nutch-user@lucene.apache.org/msg13823.html)",
        "Issue Links": []
    },
    "NUTCH-739": {
        "Key": "NUTCH-739",
        "Summary": "SolrDeleteDuplications too slow when using hadoop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dmitry Lihachev",
        "Created": "28/May/09 04:32",
        "Updated": "29/Nov/09 04:08",
        "Resolved": "28/Nov/09 21:36",
        "Description": "in my environment i always have many warnings like this on the dedup step\n\nTask attempt_200905270022_0212_r_000003_0 failed to report status for 600 seconds. Killing!\n\n\nsolr logs:\n\nINFO: [] webapp=/solr path=/update params={wt=javabin&waitFlush=true&optimize=true&waitSearcher=true&maxSegments=1&version=2.2} status=0 QTime=173741\nMay 27, 2009 10:29:27 AM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: {optimize=} 0 173599\nMay 27, 2009 10:29:27 AM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/solr path=/update params={wt=javabin&waitFlush=true&optimize=true&waitSearcher=true&maxSegments=1&version=2.2} status=0 QTime=173599\nMay 27, 2009 10:29:27 AM org.apache.solr.search.SolrIndexSearcher close\nINFO: Closing Searcher@2ad9ac58 main\nMay 27, 2009 10:29:27 AM org.apache.solr.core.JmxMonitoredMap$SolrDynamicMBean getMBeanInfo\nWARNING: Could not getStatistics on info bean org.apache.solr.search.SolrIndexSearcher\norg.apache.lucene.store.AlreadyClosedException: this IndexReader is closed\n....\n\n\nSo I think the problem in the piece of code on line 301 of SolrDeleteDuplications ( solr.optimize() ). Because we have few job tasks each of ones tries to optimize solr indexes before closing.\nThe simplest way to avoid this bug - removing this line and sending \"<optimize/>\" message directly to solr server after dedup step",
        "Issue Links": []
    },
    "NUTCH-740": {
        "Key": "NUTCH-740",
        "Summary": "Configuration option to override default language for fetched pages.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Marcin Okraszewski",
        "Created": "28/May/09 21:12",
        "Updated": "23/Mar/10 04:11",
        "Resolved": "22/Mar/10 09:00",
        "Description": "By default \"Accept-Language\" HTTP request header is set to English. Unfortunately this value is hard coded and seems there is no way to override it. As a result you may index English version of pages even though you would prefer it in different language.",
        "Issue Links": []
    },
    "NUTCH-741": {
        "Key": "NUTCH-741",
        "Summary": "Job file includes multiple copies of nutch config files.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "build",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Kirby Bohling",
        "Created": "29/May/09 19:59",
        "Updated": "29/Nov/09 04:08",
        "Resolved": "28/Nov/09 22:30",
        "Description": "From a clean checkout, running \"ant tar\" will create a .job file.  The .job file includes two copies of the nutch-site.xml and nutch-default.xml file.",
        "Issue Links": []
    },
    "NUTCH-742": {
        "Key": "NUTCH-742",
        "Summary": "Checksum Error",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "mawanqiang",
        "Created": "20/Jun/09 09:17",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "21/Jun/09 03:36",
        "Description": "Approximately 1 million data used to create index when nutch1.0 error.\nThe error is:\njava.lang.RuntimeException: problem advancing post rec#6758513\nat org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:883)\nat org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.moveToNext(ReduceTask.java:237)\nat org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.next(ReduceTask.java:233)\nat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:79)\nat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:50)\nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)\nat org.apache.hadoop.mapred.Child.main(Child.java:158)\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum Error\nat org.apache.hadoop.mapred.IFileInputStream.doRead(IFileInputStream.java:153)\nat org.apache.hadoop.mapred.IFileInputStream.read(IFileInputStream.java:90)\nat org.apache.hadoop.mapred.IFile$Reader.readData(IFile.java:301)\nat org.apache.hadoop.mapred.IFile$Reader.rejigData(IFile.java:331)\nat org.apache.hadoop.mapred.IFile$Reader.readNextBlock(IFile.java:315)\nat org.apache.hadoop.mapred.IFile$Reader.next(IFile.java:377)\nat org.apache.hadoop.mapred.Merger$Segment.next(Merger.java:174)\nat org.apache.hadoop.mapred.Merger$MergeQueue.adjustPriorityQueue(Merger.java:277)\nat org.apache.hadoop.mapred.Merger$MergeQueue.next(Merger.java:297)\nat org.apache.hadoop.mapred.Task$ValuesIterator.readNextKey(Task.java:922)\nat org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:881)\n... 6 more",
        "Issue Links": []
    },
    "NUTCH-743": {
        "Key": "NUTCH-743",
        "Summary": "Site search powered by Lucene/Solr",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "23/Jun/09 16:26",
        "Updated": "22/May/13 03:54",
        "Resolved": "03/Jul/09 05:09",
        "Description": "Replace current Nutch site search with Lucene/Solr powered search hosted by Lucid Imagination (http://www.lucidimagination.com/search).  It allows one to search all of the Nutch (content from other parts of the Lucene ecosystem is also available) content from a single place, including web, wiki, JIRA and mail archives. Lucid has a fault tolerant setup with replication and fail over as well as monitoring services in place. \nA preview of the site with the new search enabled is available at http://people.apache.org/~siren/site/",
        "Issue Links": []
    },
    "NUTCH-744": {
        "Key": "NUTCH-744",
        "Summary": "indexing items in rss-feed in seperate page",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Invalid",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tarun",
        "Created": "09/Jul/09 14:29",
        "Updated": "09/Jul/09 14:40",
        "Resolved": "09/Jul/09 14:37",
        "Description": "the rss feed have many items.\ni want to index each item seperately. So that when we search the query should take place in each item. and the content displayed as search result should show the result from that item only.\nNot the whole rss page that is crawled.",
        "Issue Links": []
    },
    "NUTCH-745": {
        "Key": "NUTCH-745",
        "Summary": "MyHtmlParser getParse return not null\uff0cso all Analyzer-(zh|fr) cannot run",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jcore_XiaTian",
        "Created": "10/Jul/09 05:38",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jan/13 19:30",
        "Description": "MyHtmlParser getParse return not null\uff0cso all Analyzer-(zh|fr) cannot run\n\tpublic ParseResult getParse(Content content) \n{\n    \treturn ParseResult.createParseResult(content.getUrl(), new ParseStatus(ParseStatus.FAILED, \n                ParseStatus.FAILED_MISSING_CONTENT, \n        \"No textual content available\").getEmptyParse(conf)); \n\t\t\n\t\t// return null;\n\t}\n\n========nutch-site.xml=======\n<property>\n  <name>plugin.includes</name>\n  <value>protocol-http|urlfilter-regex|parse-(myHtml|html|text|js)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|language-identifier|analysis-(zh)</value>\n  <description><![CDATA[\n  ]]>  </description>\n</property>\n==========parse-plugins.xml============\n<mimeType name=\"text/html\">\n\t\t<plugin id=\"parse-myHtml\" />\n\t\t<plugin id=\"parse-html\" />\n\t</mimeType>\n<alias name=\"parse-myHtml\"\n\t\t\textension-id=\"org.apache.nutch.parse.html.MyHtmlParser\" />\n===src/plugin/parse-html/src/java/org/apache/nutch/parse/html/HtmlParser.java========\n public ParseResult getParse(Content content) {\n.....\n// cannot run the code:\n  ParseResult filteredParse = this.htmlParseFilters.filter(content, parseResult, \n                                                             metaTags, root);\n.......",
        "Issue Links": []
    },
    "NUTCH-746": {
        "Key": "NUTCH-746",
        "Summary": "NutchBeanConstructor does not close NutchBean upon contextDestroyed, causing resource leak in the container.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Kirby Bohling",
        "Created": "26/Jul/09 23:13",
        "Updated": "29/Nov/09 04:08",
        "Resolved": "28/Nov/09 21:17",
        "Description": "NutchBeanConstructor is not cleaning up upon application shutdown (contextDestroyed()).   It leaves open the SegmentUpdater, and potentially other resources.  This causes the WebApp's classloader to not be able to GC'ed in Tomcat, which after repeated restarts will lead to a PermGen error.",
        "Issue Links": []
    },
    "NUTCH-747": {
        "Key": "NUTCH-747",
        "Summary": "inject&Index metadatas and inherit these metadatas to all matching suburls",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer,                                            injector",
        "Assignee": null,
        "Reporter": "Marko Bauhardt",
        "Created": "06/Aug/09 10:35",
        "Updated": "22/May/13 03:53",
        "Resolved": "05/Nov/12 14:54",
        "Description": "Hi.\nthe following two patches supports\n+ inject metadatas to url's into a metadatadb\nurl.com <TAB> <METAKEY> : <TAB> <METAVALUE> <TAB> <METAVALUE> <METAKEY> : <METAVALUE> ...\n...\n+ updates the parse_data metadata from a shard and write the metadatas to all fetched urls that starts with an url from the metadatadb\n+ this patch support's metadata to all matching suburls inheritance\nthe second patch implements a index-metadata plugin.\n+ this plugin extract all metadats from the parse_data of a shard and index it. which metadats you can configure in the plugin.properties.\n+ to index for example the lang you have to configure the plugin.properties: lang=STORE,UNTOKENIZED\n+ that means that the index plugin exract metadata values with key \"lang\". if exists, all values are indexed stored and untokenized\nExample\ncreate start url's in \"/tmp/urls/start/urls.txt\"\nhttp://lucene.apache.org/nutch/apidocs-1.0/index.html\nhttp://lucene.apache.org/nutch/apidocs-0.9/index.html\ncreate metadata url's in \"/tmp/urls/metadata/urls.txt\"\nhttp://lucene.apache.org/nutch/apidocs-1.0/     version:        1.0\nhttp://lucene.apache.org/nutch/apidocs-0.9/     version:        0.9\nInject Urls\nbin/nutch inject crawldb /tmp/urls/start/\nbin/nutch org.apache.nutch.crawl.metadata.MetadataInjector metadatadb /tmp/urls/metadata/\nFetch & Parse & Update\nbin/nutch generate crawldb segments\nbin/nutch fetch segments/20090806105717/\nbin/nutch org.apache.nutch.crawl.metadata.ParseDataUpdater metadatadb segments/20090806105717\nbin/nutch updatedb crawldb/ segments/20090806105717/\nFetch & Parse & Update Again\n...\nIndex\nbin/nutch invertlinks linkdb -dir segments/\nbin/nutch index index crawldb/ linkdb/ segments/20090806105717 segments/20090806110127\nCheck your Index\nAll urls starting with \"http://lucene.apache.org/nutch/apidocs-1.0/ \" are indexed with \"version:1.0\".\nAll urls starting with \"http://lucene.apache.org/nutch/apidocs-0.9/ \" are indexed with \"version:0.9\".\nThis issue is some related to NUTCH-655",
        "Issue Links": []
    },
    "NUTCH-748": {
        "Key": "NUTCH-748",
        "Summary": "DiskChecker  Could not find",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.0.0",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "mawanqiang",
        "Created": "18/Aug/09 06:28",
        "Updated": "09/Oct/09 13:57",
        "Resolved": "09/Oct/09 13:57",
        "Description": "2009-08-17 19:08:17,286 WARN  mapred.TaskTracker - getMapOutput(attempt_200908171832_0012_m_000013_0,55) failed :\norg.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/jobcache/job_200908171832_0012/attempt_200908171832_0012_m_000013_0/output/file.out.index in any of the configured local directories\n\tat org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:381)\n\tat org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:138)\n\tat org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:2840)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:689)\n\tat javax.servlet.http.HttpServlet.service(HttpServlet.java:802)\n\tat org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:427)\n\tat org.mortbay.jetty.servlet.WebApplicationHandler.dispatch(WebApplicationHandler.java:475)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:567)\n\tat org.mortbay.http.HttpContext.handle(HttpContext.java:1565)\n\tat org.mortbay.jetty.servlet.WebApplicationContext.handle(WebApplicationContext.java:635)\n\tat org.mortbay.http.HttpContext.handle(HttpContext.java:1517)\n\tat org.mortbay.http.HttpServer.service(HttpServer.java:954)\n\tat org.mortbay.http.HttpConnection.service(HttpConnection.java:814)\n\tat org.mortbay.http.HttpConnection.handleNext(HttpConnection.java:981)\n\tat org.mortbay.http.HttpConnection.handle(HttpConnection.java:831)\n\tat org.mortbay.http.SocketListener.handleConnection(SocketListener.java:244)\n\tat org.mortbay.util.ThreadedServer.handle(ThreadedServer.java:357)\n\tat org.mortbay.util.ThreadPool$PoolThread.run(ThreadPool.java:534)",
        "Issue Links": []
    },
    "NUTCH-749": {
        "Key": "NUTCH-749",
        "Summary": "Fetching the url from crawldb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "salima abdulsalam",
        "Created": "21/Aug/09 13:36",
        "Updated": "21/Aug/09 15:37",
        "Resolved": "21/Aug/09 15:37",
        "Description": "Hi,\n Iam new to using the nutch with solr.I followed the link  http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/  for integration.Iam getting an error while fetching the url from crawldb.\nI used the below command\n  bin/nutch fetch $SEGMENT -noParsing and i set the SEGMENT as  export SEGMENT=crawl/segments/`ls -tr crawl/segments|tail -1`\nafter running the command, iam getting the error as\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting\nFetcher: segment: crawl/segments/20090821062021\nException in thread \"main\" java.io.IOException: Illegal file pattern: Expecting set closure character or end of range, or } for glob 20090821062021 at 30\n        at org.apache.hadoop.fs.FileSystem$GlobFilter.error(FileSystem.java:1086)\n        at org.apache.hadoop.fs.FileSystem$GlobFilter.setRegex(FileSystem.java:1071)\n        at org.apache.hadoop.fs.FileSystem$GlobFilter.<init>(FileSystem.java:989)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:955)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globPathsLevel(FileSystem.java:964)\n        at org.apache.hadoop.fs.FileSystem.globStatusInternal(FileSystem.java:904)\n        at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:868)\n        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:159)\n        at org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:39)\n        at org.apache.nutch.fetcher.Fetcher$InputFormat.getSplits(Fetcher.java:101)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:797)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1142)\n        at org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:969)\n        at org.apache.nutch.fetcher.Fetcher.main(Fetcher.java:1003)\nCan anyone help in this.\nThanks,\nSalima",
        "Issue Links": []
    },
    "NUTCH-750": {
        "Key": "NUTCH-750",
        "Summary": "HtmlParser plugin - page title extraction",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Alexey Torochkov",
        "Created": "29/Aug/09 09:20",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "A little improvement to trying to extract <title> tag in body if it doesn't exist in head.\nIn current version DOMContentUtils just skip all after <body> in getTitle() method.\nAttached patch allows to change this behavior (for default it doesn't change anything) and can cope with webmasters mistakes",
        "Issue Links": []
    },
    "NUTCH-751": {
        "Key": "NUTCH-751",
        "Summary": "Upgrade version of HttpClient",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "04/Sep/09 19:00",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "11/Jan/10 16:19",
        "Description": "The existing version of commons http-client (3.01) should be replaced with the latest version from http://hc.apache.org/.\nCurrently the only way of using the https protocol is to enable http-client. The version 3.01 is bugged and causes a lot of issues which have been reported before. Apparently the new version has been redesigned and should fix them. The old v3.01 is too unstable to be used on a large scale.\nI will try to send a patch in the next couple of weeks but would love to hear your thoughts on this.\nJ.",
        "Issue Links": []
    },
    "NUTCH-752": {
        "Key": "NUTCH-752",
        "Summary": "how to index data from databse(ect oracle)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "zhengfang",
        "Created": "07/Sep/09 09:46",
        "Updated": "10/Sep/09 07:56",
        "Resolved": "10/Sep/09 07:56",
        "Description": "I am new user for nutch.I need some advice about the databases used by Nutch-1.0.\nmy application has 3 datasource:\n1. website\n2. oracle\n3. word.\nNow, I know nutch can solve problem 1 and 3, but I didn't find solution for problem 2, anyone can help me? thanks!!",
        "Issue Links": []
    },
    "NUTCH-753": {
        "Key": "NUTCH-753",
        "Summary": "Prevent new Fetcher to retrieve the robots twice",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "07/Sep/09 18:32",
        "Updated": "28/Nov/09 14:08",
        "Resolved": "25/Nov/09 17:21",
        "Description": "The new Fetcher which is now used by default handles the robots file directly instead of relying on the protocol. The options Protocol.CHECK_BLOCKING and Protocol.CHECK_ROBOTS are set to false to prevent fetching the robots.txt twice (in Fetcher + in protocol), which avoids calling robots.isAllowed. However in practice the robots file is still fetched as there is a call to robots.getCrawlDelay() a bit further which is not covered by the if (Protocol.CHECK_ROBOTS).",
        "Issue Links": []
    },
    "NUTCH-754": {
        "Key": "NUTCH-754",
        "Summary": "Use GenericOptionsParser instead of FileSystem.parseArgs()",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "16/Sep/09 13:46",
        "Updated": "10/Oct/09 04:45",
        "Resolved": "09/Oct/09 13:55",
        "Description": "FileSystem.parseArgs() should be replaced with GenericOptionsParser. Doing this allows to compile Nutch with the 0.20.* branch of Hadoop",
        "Issue Links": []
    },
    "NUTCH-755": {
        "Key": "NUTCH-755",
        "Summary": "DomainURLFilter crashes on malformed URL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Mike Baranczak",
        "Created": "17/Sep/09 02:39",
        "Updated": "29/Dec/09 22:30",
        "Resolved": "28/Nov/09 22:19",
        "Description": "2009-09-16 21:54:17,001 ERROR [Thread-156] DomainURLFilter - Could not apply filter on url: http:/comments.php\njava.lang.NullPointerException\n        at org.apache.nutch.urlfilter.domain.DomainURLFilter.filter(DomainURLFilter.java:173)\n        at org.apache.nutch.net.URLFilters.filter(URLFilters.java:88)\n        at org.apache.nutch.parse.ParseOutputFormat$1.write(ParseOutputFormat.java:200)\n        at org.apache.nutch.parse.ParseOutputFormat$1.write(ParseOutputFormat.java:113)\n        at org.apache.nutch.fetcher.FetcherOutputFormat$1.write(FetcherOutputFormat.java:96)\n        at org.apache.nutch.fetcher.FetcherOutputFormat$1.write(FetcherOutputFormat.java:70)\n        at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:410)\n        at org.apache.hadoop.mapred.lib.IdentityReducer.reduce(IdentityReducer.java:39)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:436)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:170)\nExpected behavior would be to recognize the URL as malformed, and reject it.",
        "Issue Links": []
    },
    "NUTCH-756": {
        "Key": "NUTCH-756",
        "Summary": "CrawlDatum.set() does not reset Metadata if it is null",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "29/Sep/09 10:40",
        "Updated": "10/Oct/09 04:45",
        "Resolved": "09/Oct/09 14:05",
        "Description": "The patch Nutch-702 implemented the lazy instanciation of CrawlDatum objects, but when using the method set(CrawlDatum) to copy the content of an instance into the current object we did not reset the metadata of the current object if it was null in the instance passed as argument. As a result, the metadata of the current object might be kept from a previous CrawlDatum and won't correspond to the other fields of the instance.\nThe patch attached fixes this issue.",
        "Issue Links": []
    },
    "NUTCH-757": {
        "Key": "NUTCH-757",
        "Summary": "RequestUtils getBooleanParameter() always returns false",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Niall Pemberton",
        "Created": "30/Sep/09 14:40",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "09/Oct/09 13:31",
        "Description": "RequestUtils getBooleanParameter() always returns false because it tests the name of the parameter rather than the value!\nYou can see this problem with the Nutch webapp, if you try adding a \"summary\" parameter the summary is never included, whatever value you specify (either \"1\", \"true\" or \"yes\"",
        "Issue Links": []
    },
    "NUTCH-758": {
        "Key": "NUTCH-758",
        "Summary": "Set subversion eol-style to \"native\"",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Niall Pemberton",
        "Created": "30/Sep/09 18:44",
        "Updated": "10/Oct/09 04:45",
        "Resolved": "09/Oct/09 17:03",
        "Description": "It would be really nice to set the subversion eol-style (end-of-line style) to \"native\" - makes it much easier for different contributors on different OS's to contribute patches.",
        "Issue Links": []
    },
    "NUTCH-759": {
        "Key": "NUTCH-759",
        "Summary": "Removal of deprecated APIs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Incomplete",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Stephen Norman",
        "Created": "14/Oct/09 01:28",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "14/Jul/10 17:49",
        "Description": "The Nutch project is using a number of deprecated APIs.\nIt would be nice to clean this up in to maintain compatibility in the future.",
        "Issue Links": []
    },
    "NUTCH-760": {
        "Key": "NUTCH-760",
        "Summary": "Allow field mapping from nutch to solr index",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "David Stuart",
        "Created": "15/Oct/09 10:43",
        "Updated": "28/Nov/09 14:08",
        "Resolved": "25/Nov/09 21:00",
        "Description": "I am using nutch to crawl sites and have combined it\nwith solr pushing the nutch index using the solrindex command. I have\nset it up as specified on the wiki using the copyField url to id in the\nschema. Whilst this works fine it is stuff's up my inputs from other\nsources in solr (e.g. using the solr data import handler) as they have\nboth id's and url's. I have patch that implements a nutch xml schema\ndefining what basic nutch fields map to in your solr push.",
        "Issue Links": []
    },
    "NUTCH-761": {
        "Key": "NUTCH-761",
        "Summary": "Avoid cloningCrawlDatum in CrawlDbReducer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "03/Nov/09 14:39",
        "Updated": "28/Nov/09 14:08",
        "Resolved": "25/Nov/09 18:09",
        "Description": "In the huge majority of cases the CrawlDbReducer gets unique CrawlData in its reduce phase and these will be the entries coming from the crawlDB and not present in the segments.\nThe patch attached optimizes the reduce step by avoid an unnecessary cloning of the CrawlDatum fields when there is only one CrawlDatum in the values. This has more impact has the crawlDB gets larger,  we noticed an improvement of around 25-30% in the time spent in the reduce phase.",
        "Issue Links": []
    },
    "NUTCH-762": {
        "Key": "NUTCH-762",
        "Summary": "Alternative Generator which can generate several segments in one parse of the crawlDB",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "generator",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "03/Nov/09 15:03",
        "Updated": "07/Sep/11 09:53",
        "Resolved": "22/Mar/10 16:23",
        "Description": "When using Nutch on a large scale (e.g. billions of URLs), the operations related to the crawlDB (generate - update) tend to take the biggest part of the time. One solution is to limit such operations to a minimum by generating several fetchlists in one parse of the crawlDB then update the Db only once on several segments. The existing Generator allows several successive runs by generating a copy of the crawlDB and marking the URLs to be fetched. In practice this approach does not work well as we need to read the whole crawlDB as many time as we generate a segment.\nThe patch attached contains an implementation of a MultiGenerator  which can generate several fetchlists by reading the crawlDB only once. The MultiGenerator differs from the Generator in other aspects: \n\ncan filter the URLs by score\nnormalisation is optional\nIP resolution is done ONLY on the entries which have been selected for  fetching (during the partitioning). Running the IP resolution on the whole crawlDb is too slow to be usable on a large scale\ncan max the number of URLs per host or domain (but not by IP)\ncan choose to partition by host, domain or IP\n\nTypically the same unit (e.g. domain) would be used for maxing the URLs and for partitioning; however as we can't count the max number of URLs by IP another unit must be chosen while partitioning by IP. \nWe found that using a filter on the score can dramatically improve the performance as this reduces the amount of data being sent to the reducers.\nThe MultiGenerator is called via : nutch org.apache.nutch.crawl.MultiGenerator ...\nwith the following options :\nMultiGenerator <crawldb> <segments_dir> [-force] [-topN N] [-numFetchers numFetchers] [-adddays numDays] [-noFilter] [-noNorm] [-maxNumSegments num]\nwhere most parameters are similar to the default Generator - apart from : \n-noNorm (explicit)\n-topN : max number of URLs per segment\n-maxNumSegments : the actual number of segments generated could be less than the max value select e.g. not enough URLs are available for fetching and fit in less segments\nPlease give it a try and less me know what you think of it\nJulien Nioche\nhttp://www.digitalpebble.com",
        "Issue Links": [
            "/jira/browse/NUTCH-1074"
        ]
    },
    "NUTCH-763": {
        "Key": "NUTCH-763",
        "Summary": "Separate configuration files from resources to be included in the job file",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "05/Nov/09 18:33",
        "Updated": "12/Jul/10 09:19",
        "Resolved": "12/Jul/10 09:19",
        "Description": "One of the things I found confusing when I was learning Nutch was the fact that the conf/ directory contains at the same time : \n\nconfiguration files for Hadoop / Nutch which are put in the jar files but not used there\nresource files (e.g. filtering rules) which MUST be up to date in the job file\n\nI would separate the conf/ directory from say a resources/ directory which would contain the rule files and other things to put in the job file. Unless I am mistaken none of the configuration files need to be in the job file. I know it is a very minor point, but that would probably simplify things and make it easier for beginners to understand what has to be modified where.",
        "Issue Links": []
    },
    "NUTCH-764": {
        "Key": "NUTCH-764",
        "Summary": "Add support for vfsfile:// loading of plugins for JBoss",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "tcurran@approachingpi.com",
        "Created": "10/Nov/09 01:23",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:36",
        "Description": "In the file:\n/src/java/org/apache/nutch/plugin/PluginManifestParser.java\nThere is a check to make sure that the plugin file location is a url formatted like \"file://path/plugins\".\nWhen deployed on Jboss, the file protocol will sometimes be: \"vfsfile://path/plugins\".  The code with vfsfile can operate the same so I propose a change to the check to also allow this protocol.  This would allow Nutch to be deployed on the newer versions of JBoss without any modification.\nHere is a simple patch:\nIndex: src/java/org/apache/nutch/plugin/PluginManifestParser.java\n===================================================================\n\u2014 src/java/org/apache/nutch/plugin/PluginManifestParser.java\tMon Nov 09 20:20:51 EST 2009\n+++ src/java/org/apache/nutch/plugin/PluginManifestParser.java\tMon Nov 09 20:20:51 EST 2009\n@@ -121,7 +121,8 @@\n       } else if (url == null) \n{\n         LOG.warn(\"Plugins: directory not found: \" + name);\n         return null;\n-      }\n else if (!\"file\".equals(url.getProtocol())) \n{\n+      }\n else if (!\"file\".equals(url.getProtocol()) &&\n+        !\"vfsfile\".equals(url.getProtocol())) \n{\n         LOG.warn(\"Plugins: not a file: url. Can't load plugins from: \" + url);\n         return null;\n       }",
        "Issue Links": []
    },
    "NUTCH-765": {
        "Key": "NUTCH-765",
        "Summary": "Allow Crawl class to call Either Solr or Lucene Indexer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.0.0,                                            1.1",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "12/Nov/09 20:59",
        "Updated": "28/Nov/09 14:08",
        "Resolved": "21/Nov/09 23:35",
        "Description": "Change to the crawl class to have a -solr option which will call the solr indexer instead of the lucene indexer.  This also allows it to ignore dedup and merge for solr indexing and to point to a specific solr instance.",
        "Issue Links": []
    },
    "NUTCH-766": {
        "Key": "NUTCH-766",
        "Summary": "Tika parser",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Julien Nioche",
        "Created": "18/Nov/09 14:49",
        "Updated": "28/Sep/10 00:48",
        "Resolved": "12/Feb/10 06:52",
        "Description": "Tika handles a lot of different formats under the bonnet and exposes them nicely via SAX events. What is described here is a tika-parser plugin which delegates the pasring mechanism of Tika but can still coexist with the existing parsing plugins which is useful for formats partially handled by Tika (or not at all). Some of the elements below have already been discussed on the mailing lists. Note that this is work in progress, your feedback is welcome.\nTika is already used by Nutch for its MimeType implementations. Tika comes as different jar files (core and parsers), in the work described here we decided to put the libs in 2 different places\nNUTCH_HOME/lib : tika-core.jar\nNUTCH_HOME/tika-plugin/lib : tika-parsers.jar\nTika being used by the core only for its Mimetype functionalities we only need to put tika-core at the main lib level whereas the tika plugin obviously needs the tika-parsers.jar + all the jars used internally by Tika\nDue to limitations in the way Tika loads its classes, we had to duplicate the TikaConfig class in the tika-plugin. This might be fixed in the future in Tika itself or avoided by refactoring the mimetype part of Nutch using extension points.\nUnlike most other parsers, Tika handles more than one Mime-type which is why we are using \"*\" as its mimetype value in the plugin descriptor and have modified ParserFactory.java so that it considers the tika parser as potentially suitable for all mime-types. In practice this means that the associations between a mime type and a parser plugin as defined in parse-plugins.xml are useful only for the cases where we want to handle a mime type with a different parser than Tika. \nThe general approach I chose was to convert the SAX events returned by the Tika parsers into DOM objects and reuse the utilities that come with the current HTML parser i.e. link detection,  metatag handling but also means that we can use the HTMLParseFilters in exactly the same way. The main difference though is that HTMLParseFilters are not limited to HTML documents anymore as the XHTML tags returned by Tika can correspond to a different format for the original document. There is a duplication of code with the html-plugin which will be resolved by either a) getting rid of the html-plugin altogether or b) exporting its jar and make the tika parser depend on it.\nThe following libraries are required in the lib/ directory of the tika-parser : \n      <library name=\"asm-3.1.jar\"/>\n      <library name=\"bcmail-jdk15-144.jar\"/>\n      <library name=\"commons-compress-1.0.jar\"/>\n      <library name=\"commons-logging-1.1.1.jar\"/>\n      <library name=\"dom4j-1.6.1.jar\"/>\n      <library name=\"fontbox-0.8.0-incubator.jar\"/>\n      <library name=\"geronimo-stax-api_1.0_spec-1.0.1.jar\"/>\n      <library name=\"hamcrest-core-1.1.jar\"/>\n      <library name=\"jce-jdk13-144.jar\"/>\n      <library name=\"jempbox-0.8.0-incubator.jar\"/>\n      <library name=\"metadata-extractor-2.4.0-beta-1.jar\"/>\n      <library name=\"mockito-core-1.7.jar\"/>\n      <library name=\"objenesis-1.0.jar\"/>\n      <library name=\"ooxml-schemas-1.0.jar\"/>\n      <library name=\"pdfbox-0.8.0-incubating.jar\"/>\n      <library name=\"poi-3.5-FINAL.jar\"/>\n      <library name=\"poi-ooxml-3.5-FINAL.jar\"/>\n      <library name=\"poi-scratchpad-3.5-FINAL.jar\"/>\n      <library name=\"tagsoup-1.2.jar\"/>\n      <library name=\"tika-parsers-0.5-SNAPSHOT.jar\"/>\n      <library name=\"xml-apis-1.0.b2.jar\"/>\n      <library name=\"xmlbeans-2.3.0.jar\"/>\nThere is a small test suite which needs to be improved. We will need to have a look at each individual format and check that it is covered by Tika and if so to the same extent; the Wiki is probably the right place for this. The language identifier (which is a HTMLParseFilter) seemed to work fine.\nAgain, your comments are welcome. Please bear in mind that this is just a first step. \nJulien\nhttp://www.digitalpebble.com",
        "Issue Links": [
            "/jira/browse/NUTCH-789",
            "/jira/browse/NUTCH-767",
            "/jira/browse/NUTCH-705"
        ]
    },
    "NUTCH-767": {
        "Key": "NUTCH-767",
        "Summary": "Update Tika to v0.5  for the MimeType detection",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "18/Nov/09 14:56",
        "Updated": "12/Jan/10 04:47",
        "Resolved": "11/Jan/10 10:13",
        "Description": "The version 0.5 of TIka requires a few changes to the MimeType implementation. Tika is now split in several jars, we need to place the tika-core.jar in the main nutch lib.",
        "Issue Links": [
            "/jira/browse/NUTCH-185",
            "/jira/browse/NUTCH-766"
        ]
    },
    "NUTCH-768": {
        "Key": "NUTCH-768",
        "Summary": "Upgrade Nutch 1.0 to use Hadoop 0.20",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "21/Nov/09 23:38",
        "Updated": "19/Dec/09 05:14",
        "Resolved": "01/Dec/09 14:58",
        "Description": "Upgrade Nutch 1.0 to use the Hadoop 0.20 release.",
        "Issue Links": []
    },
    "NUTCH-769": {
        "Key": "NUTCH-769",
        "Summary": "Fetcher to skip queues for URLS getting repeated exceptions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "23/Nov/09 11:05",
        "Updated": "01/Dec/09 15:15",
        "Resolved": "01/Dec/09 15:15",
        "Description": "As discussed on the mailing list (see http://www.mail-archive.com/nutch-user@lucene.apache.org/msg15360.html) this patch allows to clear URLs queues in the Fetcher when more than a set number of exceptions have been encountered in a row. This can speed up the fetching substantially in cases where target hosts are not responsive (as a TimeoutException would be thrown) and limits cases where a whole Fetch step is slowed down because of a few queues.\nby default the parameter fetcher.max.exceptions.per.queue has a value of -1 and is deactivated.",
        "Issue Links": [
            "/jira/browse/NUTCH-770",
            "/jira/browse/NUTCH-770"
        ]
    },
    "NUTCH-770": {
        "Key": "NUTCH-770",
        "Summary": "Timebomb for Fetcher",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "23/Nov/09 11:23",
        "Updated": "05/Dec/09 16:50",
        "Resolved": "01/Dec/09 14:50",
        "Description": "This patch provides the Fetcher with a timebomb mechanism. By default the timebomb is not activated; it can be set using the parameter fetcher.timebomb.mins. The number of minutes is relative to the start of the Fetch job. When the number of minutes is reached, the QueueFeeder skips all remaining entries then all active queues are purged. This allows to keep the Fetch step under comtrol and works well in combination with NUTCH-769",
        "Issue Links": [
            "/jira/browse/NUTCH-769",
            "/jira/browse/NUTCH-769"
        ]
    },
    "NUTCH-771": {
        "Key": "NUTCH-771",
        "Summary": "Add WebGraph classes to the bin/nutch script",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Dennis Kubes",
        "Created": "24/Nov/09 20:47",
        "Updated": "13/Sep/11 21:26",
        "Resolved": "18/Aug/11 13:26",
        "Description": "Currently the webgraph jobs are called on the command line by calling main methods on their classes.  I propose to upgrade the bin/nutch shell script to allow calling these jobs as well.  This would include the webgraphdb, linkrank, scoreupdater, and nodedumper jobs.",
        "Issue Links": [
            "/jira/browse/NUTCH-1049",
            "/jira/browse/NUTCH-875"
        ]
    },
    "NUTCH-772": {
        "Key": "NUTCH-772",
        "Summary": "Upgrade Nutch to use Lucene 2.9.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "25/Nov/09 12:36",
        "Updated": "28/Nov/09 14:08",
        "Resolved": "25/Nov/09 21:17",
        "Description": "Upgrade Nutch to the latest Lucene release.",
        "Issue Links": []
    },
    "NUTCH-773": {
        "Key": "NUTCH-773",
        "Summary": "some minor bugs in AbstractFetchSchedule.java",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "fetcher,                                            generator",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Reinhard P\u00f6tz",
        "Created": "25/Nov/09 14:15",
        "Updated": "28/Nov/09 14:08",
        "Resolved": "25/Nov/09 17:12",
        "Description": "fixes some minor trivial bugs in AbstractFetchSchedule.java",
        "Issue Links": []
    },
    "NUTCH-774": {
        "Key": "NUTCH-774",
        "Summary": "Retry interval in crawl date is set to 0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Reinhard P\u00f6tz",
        "Created": "02/Dec/09 12:04",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "14/Jul/10 20:13",
        "Description": "When i fetch and parse a feed with the feed plugin,\nhttp://www.wachauclimbing.net/home/impressum-disclaimer/feed/\nanother crawl date is generated\nhttp://www.wachauclimbing.net/home/impressum-disclaimer/comment-page-1/\nafter fetching a second round\nthe dump in the crawl db still shows a retry interval with value 0.\nhttp://www.wachauclimbing.net/home/impressum-disclaimer/comment-page-1/ Version: 7\nStatus: 2 (db_fetched)\nFetch time: Wed Dec 02 12:48:22 CET 2009\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 0 seconds (0 days)\nScore: 1.0833334\nSignature: db9ab2193924cd2d0b53113a500ca604\nMetadata: pst: success(1), lastModified=0\na check should be done in DefaultFetchSchedule (or AbstractFetchSchedule) in the\nmethod \nsetFetchSchedule",
        "Issue Links": []
    },
    "NUTCH-775": {
        "Key": "NUTCH-775",
        "Summary": "Enhance Searcher interface",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "16/Dec/09 07:04",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Feb/10 20:48",
        "Description": "Current Searcher interface is too limited for many purposes:\nHits search(Query query, int numHits, String dedupField, String sortField,\n      boolean reverse) throws IOException;\nIt would be nice that we had an interface that allowed adding different features without changing the interface. I am proposing that we deprecate the current search method and introduce something like:\nHits search(Query query, Metadata context) throws IOException;\nAlso at the same time we should enhance the QueryFilter interface to look something like:\nBooleanQuery filter(Query input, BooleanQuery translation, Metadata context)\n    throws QueryException;\nI would like to hear your comments before proceeding with a patch.",
        "Issue Links": []
    },
    "NUTCH-776": {
        "Key": "NUTCH-776",
        "Summary": "Configurable queue depth",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "MilleBii",
        "Created": "17/Dec/09 08:05",
        "Updated": "15/Nov/11 11:48",
        "Resolved": "15/Nov/11 11:48",
        "Description": "I propose that we create a configurable item for the queuedepth in Fetcher.java instead of the hard-coded value of 50.\nkey name : fetcher.queues.depth\nDefault value : remains 50 (of course)",
        "Issue Links": [
            "/jira/browse/NUTCH-1141",
            "/jira/browse/NUTCH-570"
        ]
    },
    "NUTCH-777": {
        "Key": "NUTCH-777",
        "Summary": "Upgrading to jetty6 broke unit tests",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "build",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "18/Dec/09 18:29",
        "Updated": "22/May/13 03:53",
        "Resolved": "18/Dec/09 19:01",
        "Description": "It seems that somewhere down the line, there was an upgrade to jetty6, which broke unit tests, specifically TestFetcher and CrawlDBTestUtil.",
        "Issue Links": []
    },
    "NUTCH-778": {
        "Key": "NUTCH-778",
        "Summary": "Running Nutch On linux having whoami exception?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Prakash Panjwani",
        "Created": "09/Jan/10 11:16",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "22/Jan/10 09:47",
        "Description": "I want to run nutch on the linux kernel,I have loged in as a root user, I have setted all the environment variable and nutch file setting. I have created a url.txt file which content the url to crawl, When i am trying to run nutch using following command\nbin/nutch crawl urls -dir pra\nit generates following exception.\ncrawl started in: pra\nrootUrlDir = urls\nthreads = 10\ndepth = 5\nInjector: starting\nInjector: crawlDb: pra/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nException in thread \"main\" java.io.IOException: Failed to get the current user's information.\n        at org.apache.hadoop.mapred.JobClient.getUGI(JobClient.java:717)\n        at org.apache.hadoop.mapred.JobClient.configureCommandLineOptions(JobClient.java:592)\n        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:788)\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1142)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:160)\n        at org.apache.nutch.crawl.Crawl.main(Crawl.java:113)\nCaused by: javax.security.auth.login.LoginException: Login failed: Cannot run program \"whoami\": java.io.IOException: error=12, Cannot allocate memory\n        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:250)\n        at org.apache.hadoop.security.UnixUserGroupInformation.login(UnixUserGroupInformation.java:275)\n        at org.apache.hadoop.mapred.JobClient.getUGI(JobClient.java:715)\n        ... 5 more\nServer has enough space to run any java application.I have attached the statics..\n total       used       free  \nMem:        524320     194632     329688 \n-/+ buffers/cache:     194632     329688\nSwap:      2475680          0    2475680\nTotal:     3000000     194632    2805368\nIs it sufficient memory space for nutch? Please some one help me ,I am new with linux kernel and nutch. \nThanks in Advance.",
        "Issue Links": []
    },
    "NUTCH-779": {
        "Key": "NUTCH-779",
        "Summary": "Mechanism for passing metadata from parse to crawldb",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "18/Jan/10 16:53",
        "Updated": "22/May/13 03:53",
        "Resolved": "30/Mar/10 08:30",
        "Description": "The patch attached allows to pass parse metadata to the corresponding entry of the crawldb.  \nComments are welcome",
        "Issue Links": [
            "/jira/browse/NUTCH-1024"
        ]
    },
    "NUTCH-780": {
        "Key": "NUTCH-780",
        "Summary": "Nutch crawler did not read configuration files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Vu Hoang",
        "Created": "21/Jan/10 04:00",
        "Updated": "14/Jul/10 18:59",
        "Resolved": "14/Jul/10 18:29",
        "Description": "Nutch searcher can read properties at the constructor ...\nNutchSearcher.java\nNutchBean bean = new NutchBean(getFilesystem().getConf(), fs);\n... // put search engine code here\n\n\n... but Nutch crawler is not, it only reads data from arguments.\nNutchCrawler.java\nStringBuilder builder = new StringBuilder();\nbuilder.append(domainlist + SPACE);\nbuilder.append(ARGUMENT_CRAWL_DIR);\nbuilder.append(domainlist + SUBFIX_CRAWLED + SPACE);\nbuilder.append(ARGUMENT_CRAWL_THREADS);\nbuilder.append(threads + SPACE);\nbuilder.append(ARGUMENT_CRAWL_DEPTH);\nbuilder.append(depth + SPACE);\nbuilder.append(ARGUMENT_CRAWL_TOPN);\nbuilder.append(topN + SPACE);\nCrawl.main(builder.toString().split(SPACE));",
        "Issue Links": []
    },
    "NUTCH-781": {
        "Key": "NUTCH-781",
        "Summary": "Update Tika to v0.6  for the MimeType detection",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "01/Feb/10 09:23",
        "Updated": "03/Feb/10 06:00",
        "Resolved": "01/Feb/10 10:00",
        "Description": "[from annoucement]\nApache Tika, a subproject of Apache Lucene, is a toolkit for detecting and\nextracting metadata and structured text content from various documents using\nexisting parser libraries.\nApache Tika 0.6 contains a number of improvements and bug fixes. Details can\nbe found in the changes file:\nhttp://www.apache.org/dist/lucene/tika/CHANGES-0.6.txt",
        "Issue Links": []
    },
    "NUTCH-782": {
        "Key": "NUTCH-782",
        "Summary": "Ability to order htmlparsefilters",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "01/Feb/10 11:41",
        "Updated": "02/Mar/10 04:09",
        "Resolved": "01/Mar/10 15:08",
        "Description": "Patch which adds a new parameter 'htmlparsefilter.order' which specifies the order in which HTMLParse filters are applied. HTMLParse filter ordering MAY have an impact on end result, as some filters could rely on the metadata generated by a previous filter.",
        "Issue Links": []
    },
    "NUTCH-783": {
        "Key": "NUTCH-783",
        "Summary": "IndexingFiltersChecker Utility",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Julien Nioche",
        "Created": "01/Feb/10 12:01",
        "Updated": "11/Jul/11 10:56",
        "Resolved": "11/Jul/11 10:47",
        "Description": "This patch contains a new utility which allows to check the configuration of the indexing filters. The IndexingFiltersChecker reads and parses a URL and run the indexers on it. Displays the fields obtained and the first\n 100 characters of their value.\nCan be used e.g. ./nutch org.apache.nutch.indexer.IndexingFiltersChecker http://www.lemonde.fr/",
        "Issue Links": [
            "/jira/browse/NUTCH-1038"
        ]
    },
    "NUTCH-784": {
        "Key": "NUTCH-784",
        "Summary": "CrawlDBScanner",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "01/Feb/10 14:32",
        "Updated": "30/Mar/10 04:15",
        "Resolved": "29/Mar/10 12:12",
        "Description": "The patch file contains a utility which dumps all the entries matching a regular expression on their URL. The dump mechanism of the crawldb reader is not  very useful on large crawldbs as the ouput can be extremely large and the -url  function can't help if we don't know what url we want to have a look at.\nThe CrawlDBScanner can either generate a text representation of the CrawlDatum-s or binary objects which can then be used as a new CrawlDB. \nUsage: CrawlDBScanner <crawldb> <output> <regex> [-s <status>] <-text>\nregex: regular expression on the crawldb key\n-s status : constraint on the status of the crawldb entries e.g. db_fetched, db_unfetched\n-text : if this parameter is used, the output will be of TextOutputFormat; otherwise it generates a 'normal' crawldb with the MapFileOutputFormat\nfor instance the command below : \n./nutch com.ant.CrawlDBScanner crawl/crawldb /tmp/amazon-dump .+amazon.com.* -s db_fetched -text\nwill generate a text file /tmp/amazon-dump containing all the entries of the crawldb matching the regexp  .+amazon.com.* and having a status of db_fetched",
        "Issue Links": [
            "/jira/browse/NUTCH-806"
        ]
    },
    "NUTCH-785": {
        "Key": "NUTCH-785",
        "Summary": "Fetcher : copy metadata from origin URL when redirecting + call scfilters.initialScore on newly created URL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "01/Feb/10 16:51",
        "Updated": "30/Mar/10 08:36",
        "Resolved": "30/Mar/10 08:36",
        "Description": "When following the redirections, the Fetcher does not copy the metadata from the original URL to the new one or calls the method scfilters.initialScore",
        "Issue Links": []
    },
    "NUTCH-786": {
        "Key": "NUTCH-786",
        "Summary": "Better list of suffix domains",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "05/Feb/10 11:49",
        "Updated": "05/Feb/10 14:05",
        "Resolved": "05/Feb/10 11:53",
        "Description": "Small improvement to the content of domain-suffixes.xml : added compound TLD for .ar, .co, .id, .il, .mx, .nz and .za",
        "Issue Links": []
    },
    "NUTCH-787": {
        "Key": "NUTCH-787",
        "Summary": "Upgrade Lucene to 3.0.1.",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "build",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dawid Weiss",
        "Created": "05/Feb/10 12:09",
        "Updated": "20/Mar/10 04:11",
        "Resolved": "19/Mar/10 11:37",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-673"
        ]
    },
    "NUTCH-788": {
        "Key": "NUTCH-788",
        "Summary": "search.jsp typo causing searches to fail",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.1",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "Sammy Yu",
        "Created": "11/Feb/10 05:40",
        "Updated": "22/May/13 03:53",
        "Resolved": "15/Feb/10 08:19",
        "Description": "Call to initialize the servlet parameter is missing parentheses.",
        "Issue Links": [
            "/jira/browse/NUTCH-793"
        ]
    },
    "NUTCH-789": {
        "Key": "NUTCH-789",
        "Summary": "Improvements to Tika parser",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "12/Feb/10 06:53",
        "Updated": "22/May/13 03:54",
        "Resolved": "23/Mar/13 04:15",
        "Description": "As reported by Sami in NUTCH-766, Sami has a few improvements he made to the Tika parser. We'll track that progress here.",
        "Issue Links": [
            "/jira/browse/NUTCH-766"
        ]
    },
    "NUTCH-790": {
        "Key": "NUTCH-790",
        "Summary": "Some external javadoc links are broken",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "build",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "14/Feb/10 16:41",
        "Updated": "22/May/13 03:54",
        "Resolved": "14/Feb/10 17:03",
        "Description": "Nutch javadoc links for lucene and hadoop are broken.",
        "Issue Links": []
    },
    "NUTCH-791": {
        "Key": "NUTCH-791",
        "Summary": "External links for published javadocs are partially broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Sami Siren",
        "Created": "14/Feb/10 16:49",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "05/Jul/10 11:03",
        "Description": "Lucene and Hadoop links point to non existing urls. For some versions of apidocs the links are just broken and for some they do not exist at all. Basically what is required is that the javadocs are generated again with proper urls for external packages.",
        "Issue Links": []
    },
    "NUTCH-792": {
        "Key": "NUTCH-792",
        "Summary": "Nutch version still contains 1.0",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "14/Feb/10 17:09",
        "Updated": "22/May/13 03:53",
        "Resolved": "14/Feb/10 17:14",
        "Description": "Should be 1.1-dev now in trunk.",
        "Issue Links": []
    },
    "NUTCH-793": {
        "Key": "NUTCH-793",
        "Summary": "search.jsp compile errors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "web gui",
        "Assignee": "Sami Siren",
        "Reporter": "Sami Siren",
        "Created": "15/Feb/10 08:09",
        "Updated": "22/May/13 03:53",
        "Resolved": "15/Feb/10 08:10",
        "Description": "Related to the searcher interface changes recently committed I broke search.jsp which does not currently compile.",
        "Issue Links": [
            "/jira/browse/NUTCH-788"
        ]
    },
    "NUTCH-794": {
        "Key": "NUTCH-794",
        "Summary": "Language Identification must use check the parse metadata for language values",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "16/Feb/10 09:30",
        "Updated": "22/May/13 03:53",
        "Resolved": "31/Mar/10 17:08",
        "Description": "The following HTML document : \n<html lang=\"fi\"><head>document 1 title</head><body>jotain suomeksi</body></html>\nis rendered as the following xhtml by Tika : \n<?xml version=\"1.0\" encoding=\"UTF-8\"?><html xmlns=\"http://www.w3.org/1999/xhtml\"><head><title/></head><body>document 1 titlejotain suomeksi</body></html>\nwith the lang attribute getting lost.  The lang is not stored in the metadata either.\nI will open an issue on Tika and modify TestHTMLLanguageParser so that the tests don't break anymore",
        "Issue Links": [
            "/jira/browse/TIKA-379"
        ]
    },
    "NUTCH-795": {
        "Key": "NUTCH-795",
        "Summary": "Add ability to maintain nofollow attribute in linkdb",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "linkdb",
        "Assignee": null,
        "Reporter": "Sammy Yu",
        "Created": "18/Feb/10 19:46",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-693"
        ]
    },
    "NUTCH-796": {
        "Key": "NUTCH-796",
        "Summary": "Zero results problems difficult to troubleshoot due to lack of logging",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0,                                            1.1",
        "Fix Version/s": "1.1",
        "Component/s": "web gui",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Jesse Hires",
        "Created": "20/Feb/10 00:34",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "18/Mar/10 18:46",
        "Description": "There are a few places where search can fail in a distributed environment, but when configuration is not quite right, there are no indications of errors or logging.\nIncreased logging of failures would help troubleshoot such problems, as well as lower the \"I get 0 results, why?\" questions that come across the mailing lists. \nAreas where logging would be helpful:\nsearch app cannot locate search-servers.txt\nsearch app cannot find searcher node listed in search-server.txt\nsearch app cannot connect to port on searcher specified in search-server.txt\nsearcher (bin/nutch server...) cannot find index\nsearcher cannot find segments\nAccess denied in any of the above scenarios.\nThere are probably more that would be helpful, but I am not yet familiar to know all the points of possible failure between the webpage and a search node.",
        "Issue Links": []
    },
    "NUTCH-797": {
        "Key": "NUTCH-797",
        "Summary": "URL not properly constructed when link target begins with a \"?\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1,                                            nutchgora",
        "Fix Version/s": "2.3",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Robert Hohman",
        "Created": "25/Feb/10 20:47",
        "Updated": "01/May/14 06:23",
        "Resolved": "28/Apr/14 21:10",
        "Description": "This is my first bug and patch on nutch, so apologies if I have not provided enough detail.\nIn crawling the page at http://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0 there are links in the page that look like this:\n<a href=\"?co=0&sk=0&p=2&pi=1\">2</a></td><td><a href=\"?co=0&sk=0&p=3&pi=1\">3</a>\nin org.apache.nutch.parse.tika.DOMContentUtils rev 916362 (trunk), as getOutlinks looks for links, it comes across this link, and constucts a new url with a base URL class built from \"http://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0\", and a target of \"?co=0&sk=0&p=2&pi=1\"\nThe URL class, per RFC 3986 at http://labs.apache.org/webarch/uri/rfc/rfc3986.html#relative-merge, defines how to merge these two, and per the RFC, the URL class merges these to: http://careers3.accenture.com/Careers/ASPX/?co=0&sk=0&p=2&pi=1\nbecause the RFC explicitly states that the rightmost url segment (the Search.aspx in this case) should be ripped off before combining.\nWhile this is compliant with the RFC, it means the URLs which are created for the next round of fetching are incorrect.  Modern browsers seem to handle this case (I checked IE8 and Firefox 3.5), so I'm guessing this is an obscure exception or handling of what is a poorly formed url on accenture's part.\nI have fixed this by modifying DOMContentUtils to look for the case where a ? begins the target, and then pulling the rightmost component out of the base and inserting it into the target before the ?, so the target in this example becomes:\nSearch.aspx?co=0&sk=0&p=2&pi=1\nThe URL class then properly constructs the new url as:\nhttp://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0&p=2&pi=1\nIf it is agreed that this solution works, I believe the other html parsers in nutch would need to be modified in a similar way.\nCan I get feedback on this proposed solution?  Specifically I'm worried about unforeseen side effects.\nMuch thanks\nHere is the patch info:\nIndex: src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\n===================================================================\n\u2014 src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\t(revision 916362)\n+++ src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/DOMContentUtils.java\t(working copy)\n@@ -299,6 +299,50 @@\n     return false;\n   }\n+  private URL fixURL(URL base, String target) throws MalformedURLException\n+  {\n+\t  // handle params that are embedded into the base url - move them to target\n+\t  // so URL class constructs the new url class properly\n+\t  if  (base.toString().indexOf(';') > 0)  \n+          return fixEmbeddedParams(base, target);\n+\t  \n+\t  // handle the case that there is a target that is a pure query.\n+\t  // Strictly speaking this is a violation of RFC 2396 section 5.2.2 on how to assemble\n+\t  // URLs but I've seen this in numerous places, for example at\n+\t  // http://careers3.accenture.com/Careers/ASPX/Search.aspx?co=0&sk=0\n+\t  // It has urls in the page of the form href=\"?co=0&sk=0&pg=1\", and by default\n+\t  // URL constructs the base+target combo as \n+\t  // http://careers3.accenture.com/Careers/ASPX/?co=0&sk=0&pg=1, incorrectly\n+\t  // dropping the Search.aspx target\n+\t  //\n+\t  // Browsers handle these just fine, they must have an exception similar to this\n+\t  if (target.startsWith(\"?\"))\n+\t  \n{\n+\t\t  return fixPureQueryTargets(base, target);\n+\t  }\n+\t  \n+\t  return new URL(base, target);\n+  }\n+  \n+  private URL fixPureQueryTargets(URL base, String target) throws MalformedURLException\n+  {\n+\tif (!target.startsWith(\"?\"))\n+\t\treturn new URL(base, target);\n+\n+\tString basePath = base.getPath();\n+\tString baseRightMost=\"\";\n+\tint baseRightMostIdx = basePath.lastIndexOf(\"/\");\n+\tif (baseRightMostIdx != -1)\n+\t\n{\n+\t\tbaseRightMost = basePath.substring(baseRightMostIdx+1);\n+\t}\n+\t\n+\tif (target.startsWith(\"?\"))\n+\t\ttarget = baseRightMost+target;\n+\t\n+\treturn new URL(base, target);\n+  }\n+\n   /**\n\nHandles cases where the url param information is encoded into the base\nurl as opposed to the target.\n@@ -400,8 +444,7 @@\n             if (target != null && !noFollow && !post)\n               try \n{\n                 \n-                URL url = (base.toString().indexOf(';') > 0) ? \n-                  fixEmbeddedParams(base, target) :  new URL(base, target);\n+                URL url = fixURL(base, target);\n                 outlinks.add(new Outlink(url.toString(),\n                                          linkText.toString().trim()));\n               }\n catch (MalformedURLException e) {",
        "Issue Links": [
            "/jira/browse/NUTCH-1115",
            "/jira/browse/NUTCH-952",
            "/jira/browse/NUTCH-566",
            "/jira/browse/NUTCH-952",
            "/jira/browse/NUTCH-566"
        ]
    },
    "NUTCH-798": {
        "Key": "NUTCH-798",
        "Summary": "Upgrade to SOLR1.4",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "26/Feb/10 15:23",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Mar/10 13:06",
        "Description": "in particular SOLR1.4 has a StreamingUpdateSolrServer which would simplify the way we buffer the docs before sending them to the SOLR instance",
        "Issue Links": []
    },
    "NUTCH-799": {
        "Key": "NUTCH-799",
        "Summary": "SOLRIndexer to commit once all reducers have finished",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "indexer",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "01/Mar/10 15:03",
        "Updated": "06/Mar/10 04:09",
        "Resolved": "05/Mar/10 10:10",
        "Description": "What about doing only one SOLR commit after the MR job has finished in SOLRIndexer instead of doing that at the end of every Reducer? \nI ran into timeout exceptions in some of my reducers and I suspect that this was due to the fact that other reducers had already finished and called commit.",
        "Issue Links": []
    },
    "NUTCH-800": {
        "Key": "NUTCH-800",
        "Summary": "Generator builds a URL list that is not encoded",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "0.6,                                            0.7,                                            0.7.1,                                            0.7.2,                                            0.8,                                            0.8.1,                                            0.8.2,                                            0.7.3,                                            0.9.0,                                            1.0.0,                                            1.1",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Jesse Campbell",
        "Created": "05/Mar/10 22:00",
        "Updated": "29/Mar/10 20:59",
        "Resolved": null,
        "Description": "The URL string that is grabbed by the generator when creating the fetch list does not get encoded, could potentially allow unsafe excecution, and breaks reading improperly encoded URLs from the scraped pages.\nSince we a) cannot guarantee that any site we scrape is not malitious, and b) likely do not have control over all content providers, we are currently forced to use a regex normalizer to perform the same function as a built-in java class (it would be unsafe to leave alone)\nA quick solution would be to update Generator.java to utilize the java.net.URLEncoder static class:\nline 187: \nold: String urlString = url.toString();\nnew: String urlString = URLEncoder.encode(url.toString(),\"UTF-8\");\nline 192:\nold: u = new URL(url.toString());\nnew: u = new URL(urlString);\nThe use of URLEncoder.encode could also be at the updatedb stage.",
        "Issue Links": []
    },
    "NUTCH-801": {
        "Key": "NUTCH-801",
        "Summary": "Remove RTF and MP3 parse plugins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "10/Mar/10 14:45",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Mar/10 13:25",
        "Description": "Parse-rtf and parse-mp3 are not built by default  due to licensing issues. Since we now have parse-tika to handle these formats I would be in favour of removing these 2 plugins altogether to keep things nice and simple. The other plugins will probably be phased out only after the release of 1.1  when parse-tika will have been tested a lot more.\nAny reasons not to?\nJulien",
        "Issue Links": []
    },
    "NUTCH-802": {
        "Key": "NUTCH-802",
        "Summary": "Problems managing outlinks with large url length",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7",
        "Component/s": "parser",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Pablo Arag\u00f3n",
        "Created": "18/Mar/10 10:39",
        "Updated": "22/May/13 03:53",
        "Resolved": "30/Apr/13 09:27",
        "Description": "Nutch can get idle during the collection of outlinks if  the URL address of the outlink is too large.\nThe maximum sizes of an URL for the main web servers are:\n\nApache: 4,000 bytes\nMicrosoft Internet Information Server (IIS): 16, 384 bytes\nPerl HTTP::Daemon: 8.000 bytes\n\nURL adress sizes bigger than 4000 bytes are problematic, so the limit should be set in the nutch-default.xml configuration file.\nI attached a patch",
        "Issue Links": []
    },
    "NUTCH-803": {
        "Key": "NUTCH-803",
        "Summary": "Upgrade Hadoop to 0.20.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "19/Mar/10 11:40",
        "Updated": "20/Mar/10 04:11",
        "Resolved": "19/Mar/10 11:51",
        "Description": "Per subject. We are currently using 0.20.1, so there are no API changes.",
        "Issue Links": []
    },
    "NUTCH-804": {
        "Key": "NUTCH-804",
        "Summary": "CrawlDatum.statNames can be modified",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Mike Baranczak",
        "Created": "25/Mar/10 21:06",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "public static final HashMap<Byte, String> statNames\nIt's possible to modify the contents of this hash map from anywhere in the application, which could cause problems in unrelated places. Unless I'm missing something, there's no good reason to modify this map after it's initialized. So, it should either not be declared public, or be made read-only.",
        "Issue Links": []
    },
    "NUTCH-805": {
        "Key": "NUTCH-805",
        "Summary": "Unable to resolve the url-blah-blah, skipping",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "0.9.0",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "P Kaustubh",
        "Created": "26/Mar/10 11:20",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "26/Jun/10 05:59",
        "Description": "I configured the nutch-0.9 as well as nutch-1.0 to crawl intranet website. The machine access the internet/intranet using proxy i had made this setup in nutch-default.xml\neverything works well untill i run script, when fetcher tries to access the urls from seed gives error as \nunable to resolve www.urladdres.com , skipping\nQueueFeeder finished: total 1 records.\n-finishing thread FetcherThread, activeThreads=0\n-finishing thread FetcherThread, activeThreads=0\n-finishing thread FetcherThread, activeThreads=0\n-finishing thread FetcherThread, activeThreads=0\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: done",
        "Issue Links": []
    },
    "NUTCH-806": {
        "Key": "NUTCH-806",
        "Summary": "Merge CrawlDBScanner with CrawlDBReader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "29/Mar/10 13:46",
        "Updated": "29/Jul/13 13:40",
        "Resolved": "29/Jul/13 13:40",
        "Description": "The CrawlDBScanner NUTCH-784 should be merged with the CrawlDBReader. Will do that after the 1.1 release",
        "Issue Links": [
            "/jira/browse/NUTCH-1244",
            "/jira/browse/NUTCH-784",
            "/jira/browse/NUTCH-1241"
        ]
    },
    "NUTCH-807": {
        "Key": "NUTCH-807",
        "Summary": "JSParseFilter produces malformed URL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Minyao Zhu",
        "Created": "02/Apr/10 07:31",
        "Updated": "09/Dec/12 07:39",
        "Resolved": "09/Dec/12 07:39",
        "Description": "This is found when crawling site: http://zhidao.baidu.com/    ( a Chinese language site )\nIt appears this page contains javascripts which confused JSParseFilter, which produced URL like this:\nhttp://zhidao.baidu.com/){if(A===46){baidu.hide(\nNot sure the impact/scope of this issue in general.  The observation for this specific site is, much less pages got crawled.\nThanks.",
        "Issue Links": []
    },
    "NUTCH-808": {
        "Key": "NUTCH-808",
        "Summary": "Evaluate ORM Frameworks which support non-relational column-oriented datastores and RDBMs",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Enis Soztutar",
        "Reporter": "Enis Soztutar",
        "Created": "02/Apr/10 12:02",
        "Updated": "26/Apr/10 17:17",
        "Resolved": "26/Apr/10 17:17",
        "Description": "We have an ORM layer in the NutchBase branch, which uses Avro Specific Compiler to compile class definitions given in JSON. Before moving on with this, we might benefit from evaluating other frameworks, whether they suit our needs. \nWe want at least the following capabilities:\n\nUsing POJOs\nAble to persist objects to at least HBase, Cassandra, and RDBMs\nAble to efficiently serialize objects as task outputs from Hadoop jobs\nAllow native queries, along with standard queries\n\nAny comments, suggestions for other frameworks are welcome.",
        "Issue Links": [
            "/jira/browse/NUTCH-650"
        ]
    },
    "NUTCH-809": {
        "Key": "NUTCH-809",
        "Summary": "Parse-metatags plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.5",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "02/Apr/10 14:15",
        "Updated": "02/May/13 02:30",
        "Resolved": "04/Apr/12 14:49",
        "Description": "Parse-metatags plugin\nThe parse-metatags plugin consists of a HTMLParserFilter which takes as parameter a list of metatag names with '*' as default value. The values are separated by ';'.\nIn order to extract the values of the metatags description and keywords, you must specify in nutch-site.xml\n\n<property>\n  <name>metatags.names</name>\n  <value>description;keywords</value>\n</property>\n\n\nThe MetatagIndexer uses the output of the parsing above to create two fields 'keywords' and 'description'. Note that keywords is multivalued.\nThe query-basic plugin is used to include these fields in the search e.g. in nutch-site.xml\n\n<property>\n  <name>query.basic.description.boost</name>\n  <value>2.0</value>\n</property>\n\n<property>\n  <name>query.basic.keywords.boost</name>\n  <value>2.0</value>\n</property>\n\n\nThis code has been developed by DigitalPebble Ltd and offered to the community by ANT.com",
        "Issue Links": [
            "/jira/browse/NUTCH-422",
            "/jira/browse/NUTCH-1005",
            "/jira/browse/NUTCH-422",
            "/jira/browse/NUTCH-1005",
            "/jira/browse/NUTCH-1406"
        ]
    },
    "NUTCH-810": {
        "Key": "NUTCH-810",
        "Summary": "Upgrade to Tika 0.7",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "06/Apr/10 08:57",
        "Updated": "07/Apr/10 04:24",
        "Resolved": "06/Apr/10 11:42",
        "Description": "Upgrading to Tika 0.7 before 1.1 release\nThe TikaConfig mechanism has changed and does not rely on a default XML config file anymore. Am working on it.",
        "Issue Links": []
    },
    "NUTCH-811": {
        "Key": "NUTCH-811",
        "Summary": "Develop an ORM framework",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Enis Soztutar",
        "Reporter": "Enis Soztutar",
        "Created": "13/Apr/10 18:34",
        "Updated": "11/Aug/10 21:00",
        "Resolved": "11/Aug/10 21:00",
        "Description": "By Nutch-808, it is clear that we need an ORM layer on top of the datastore, so that different backends can be used to store data. \nThis issue will track the development of the ORM layer. Initially full support for HBase is planned, with RDBM, Hadoop MapFile and Cassandra support scheduled for later.",
        "Issue Links": []
    },
    "NUTCH-812": {
        "Key": "NUTCH-812",
        "Summary": "Crawl.java incorrectly uses the Generator API resulting in NPE",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Andrzej Bialecki",
        "Created": "17/Apr/10 00:17",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/Apr/10 05:39",
        "Description": "As reported by Phil Barnett on nutch-user:\n\nThe Fix.\nIn line 131 of Crawl.java\nGenerate no longer returns segments like it used to. Now it returns segs.\nline 131 needs to read\n If (segs == null)\n Instead of the current\nIf (segments == null)\nAfter that change and a recompile, crawl is working just fine.",
        "Issue Links": []
    },
    "NUTCH-813": {
        "Key": "NUTCH-813",
        "Summary": "Repetitive crawl 403 status page",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "18/Apr/10 04:08",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 20:01",
        "Description": "When we crawl a page the return a 403 status. It will be crawl repetitively each days with default schedule.\nEven when we restrict by paramter db.fetch.retry.max",
        "Issue Links": [
            "/jira/browse/NUTCH-578"
        ]
    },
    "NUTCH-814": {
        "Key": "NUTCH-814",
        "Summary": "SegmentMerger bug",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Dennis Kubes",
        "Created": "27/Apr/10 10:45",
        "Updated": "27/Apr/10 15:24",
        "Resolved": "27/Apr/10 15:24",
        "Description": "Dennis reported:\n\nIn the SegmentMerger.java file about line 150 we have this:\n       final SequenceFile.Reader reader =\n         new SequenceFile.Reader(FileSystem.get(job), fSplit.getPath(),\njob);\nThen about line 166 in the record reader we have this:\nboolean res = reader.next(key, w);\nIf I am reading that right, that would mean that the map tap would loop\nover all records for a given file and not just a given split.\nRight, this should instead use SequenceFileRecordReader that already has the logic to handle splits. Patch coming shortly - thanks for spotting this! This could be the reason for \"out of disk space\" errors that many users reported.",
        "Issue Links": []
    },
    "NUTCH-815": {
        "Key": "NUTCH-815",
        "Summary": "Invalid blank line before If-Modified-Since HTTP header",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Pascal Dimassimo",
        "Created": "27/Apr/10 16:02",
        "Updated": "27/Apr/10 18:07",
        "Resolved": "27/Apr/10 18:07",
        "Description": "If there is a Modified time stored in the crawldb for a link, the class org.apache.nutch.protocol.http.HttpResponse will use it as the value for the If-Modified-Since header. \nLine 131:\nreqStr.append(\"\\r\\n\");\nif (datum.getModifiedTime() > 0) {\n        reqStr.append(\"If-Modified-Since: \" + HttpDateFormat.toString(datum.getModifiedTime()));\n        reqStr.append(\"\\r\\n\");\n}\nThe problem is that an extra blank line is insert before this header. This make the header invalid:\n----------------------------------------------------------------------------------\nGET /tinysite/second.html HTTP/1.0\nHost: localhost:8080\nAccept-Encoding: x-gzip, gzip, deflate\nUser-Agent: nutch/Nutch-1.0\nAccept-Language: en-us,en-gb,en;q=0.7,*;q=0.3\nIf-Modified-Since: Tue, 27 Apr 2010 13:51:50 GMT\n----------------------------------------------------------------------------------\nI'm using the AdaptiveFetchSchedule to set the Modified time in the crawldb. \nI've made a test by moving the line 131 after the if block and it works. I think this is where that line should go.",
        "Issue Links": []
    },
    "NUTCH-816": {
        "Key": "NUTCH-816",
        "Summary": "Add zip target to build.xml",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.1",
        "Component/s": "build",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "27/Apr/10 19:53",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/May/10 22:48",
        "Description": "Just like we have an ant tar target (pun intended) we should have an ant zip target. I'd like to have this ready for the release and future releases.",
        "Issue Links": []
    },
    "NUTCH-817": {
        "Key": "NUTCH-817",
        "Summary": "parse-(html)does follow links of full html page, parse-(tika) does follow any links and stops at level 1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "matthew a. grisius",
        "Created": "02/May/10 02:58",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "27/Jun/14 09:38",
        "Description": "submitted per Julien Nioche. I did not see where to attach a file so I pasted it here. btw: Tika command line returns empty html body for this file.\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Frameset//EN\" \"http://www.w3.org/TR/html4/frameset.dtd\">\n<!-NewPage->\n<HTML>\n<HEAD>\n<!-- Generated by javadoc on Fri Mar 28 17:23:42 EDT 2008-->\n<TITLE>\nMatrix Application Development Kit\n</TITLE>\n<SCRIPT type=\"text/javascript\">\n    targetPage = \"\" + window.location.search;\n    if (targetPage != \"\" && targetPage != \"undefined\")\n       targetPage = targetPage.substring(1);\n    function loadFrames() \n{\n\n        if (targetPage != \"\" && targetPage != \"undefined\")\n\n             top.classFrame.location = top.targetPage;\n\n    }\n\n</SCRIPT>\n<NOSCRIPT>\n</NOSCRIPT>\n</HEAD>\n<FRAMESET cols=\"20%,80%\" title=\"\" onLoad=\"top.loadFrames()\">\n<FRAMESET rows=\"30%,70%\" title=\"\" onLoad=\"top.loadFrames()\">\n<FRAME src=\"overview-frame.html\" name=\"packageListFrame\" title=\"All Packages\">\n<FRAME src=\"allclasses-frame.html\" name=\"packageFrame\" title=\"All classes and interfaces (except non-static nested types)\">\n</FRAMESET>\n<FRAME src=\"overview-summary.html\" name=\"classFrame\" title=\"Package, class and interface descriptions\" scrolling=\"yes\">\n<NOFRAMES>\n<H2>\nFrame Alert</H2>\n<P>\nThis document is designed to be viewed using the frames feature. If you see this message, you are using a non-frame-capable web client.\n<BR>\nLink to<A HREF=\"overview-summary.html\">Non-frame version.</A>\n</NOFRAMES>\n</FRAMESET>\n</HTML>",
        "Issue Links": [
            "/jira/browse/TIKA-379"
        ]
    },
    "NUTCH-818": {
        "Key": "NUTCH-818",
        "Summary": "Parse-tika uses minorCodes instead of majorCodes in ParseStatus",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "11/May/10 14:26",
        "Updated": "11/May/10 14:39",
        "Resolved": "11/May/10 14:39",
        "Description": "Parse-tika uses minorCodes instead of majorCodes in ParseStatus which results in a IAOOB Exception in ParseSegment as the values are outside the range of majorCodes.\nThis happens for instance when no parser implementation can be found for a given mimetype.",
        "Issue Links": []
    },
    "NUTCH-819": {
        "Key": "NUTCH-819",
        "Summary": "Included Solr schema.xml and solrindex-mapping.xml don't play together",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.1",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "11/May/10 14:54",
        "Updated": "11/May/10 15:05",
        "Resolved": "11/May/10 15:05",
        "Description": "conf/solrindex-mapping.xml already fills in \"id\" field, but conf/schema.xml uses copyField to create \"id\" from \"url\". This results in multiple values of the \"id\" field which is not allowed, because id is defined as uniqueKey in the schema.",
        "Issue Links": []
    },
    "NUTCH-820": {
        "Key": "NUTCH-820",
        "Summary": "Infinite loop when hitspersite is set",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Xiao Yang",
        "Created": "13/May/10 09:00",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "NutchBean will re-search over and over, when the page number become large and the excluded sites exceed MAX_PROHIBITED_TERMS.",
        "Issue Links": []
    },
    "NUTCH-821": {
        "Key": "NUTCH-821",
        "Summary": "Use ivy in nutch builds",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": "Enis Soztutar",
        "Reporter": "Enis Soztutar",
        "Created": "13/May/10 18:05",
        "Updated": "07/Jul/10 09:26",
        "Resolved": "07/Jul/10 09:26",
        "Description": "Ivy is the de-facto dependency management tool used in conjunction with Ant. It would be nice if we switch to using Ivy in Nutch builds. \nMaven is also an alternative, but I think Nutch will benefit more with an Ant+Ivy architecture.",
        "Issue Links": []
    },
    "NUTCH-822": {
        "Key": "NUTCH-822",
        "Summary": "DOAP file still refers to Lucene",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "documentation",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Sebb",
        "Created": "13/May/10 22:29",
        "Updated": "22/May/13 03:54",
        "Resolved": "24/Jun/10 13:27",
        "Description": "The Nutch DOAP file still refers to Lucene, so Nutch is still listed as a Lucene sub-project in the http://projects.apache.org/ listings",
        "Issue Links": []
    },
    "NUTCH-823": {
        "Key": "NUTCH-823",
        "Summary": "Download page should not have pointer to nightly builds",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sebb",
        "Created": "14/May/10 15:37",
        "Updated": "08/Aug/10 12:29",
        "Resolved": "25/Jul/10 06:43",
        "Description": "The download page\nhttp://www.apache.org/dist/lucene/nutch/\nhas a pointer to nightly builds. These are not supposed to be advertised to the general public, see:\nhttp://www.apache.org/dev/release.html#what\n\"Do not include any links on the project website that might encourage non-developers to download and use nightly builds, snapshots, release candidates, or any other similar package. \"",
        "Issue Links": []
    },
    "NUTCH-824": {
        "Key": "NUTCH-824",
        "Summary": "Crawling - File Error 404 when fetching file with an hexadecimal character in the file name.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0,                                            1.2,                                            1.3,                                            nutchgora",
        "Fix Version/s": "1.0.0,                                            1.3,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Michela Becchi",
        "Created": "20/May/10 20:26",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "07/Jan/11 17:18",
        "Description": "Hello,\nI am performing a local file system crawling.\nMy problem is the following: all files that contain some hexadecimal characters in the name do not get crawled.\nFor example, I will see the following error:\nfetching file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.html\norg.apache.nutch.protocol.file.FileError: File Error: 404\n        at org.apache.nutch.protocol.file.File.getProtocolOutput(File.java:92)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:535)\nfetch of file:/nutch-1.0/wikidump/wiki-en/en/articles/a/2E/m/A.M._%28album%29_8a09.html failed with: org.apache.nutch.protocol.file.FileError: File Error: 404\nI am using nutch-1.0.\nAmong other standard settings, I configured nutch-site.conf as follows:\n<property>\n  <name>plugin.includes</name>\n  <value>protocol-file|protocol-http|urlfilter-regex|parse-(text|html|js|pdf)|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)</value>\n  <description>Regular expression naming plugin directory names to\n  include.  Any plugin not matching this expression is excluded.\n  In any case you need at least include the nutch-extensionpoints plugin. By\n  default Nutch includes crawling just HTML and plain text via HTTP,\n  and basic indexing and search plugins. In order to use HTTPS please enable\n  protocol-httpclient, but be aware of possible intermittent problems with the\n  underlying commons-httpclient library.\n  </description>\n</property>\n<property>\n  <name>file.content.limit</name>\n  <value>-1</value>\n</property>\nMoreover, crawl-urlfilter.txt   looks like:\n\nskip http:, ftp:, & mailto: urls\n-^(http|ftp|mailto):\n\n\nskip image and other suffixes we can't yet parse\n-\\.(gif|GIF|jpg|JPG|png|PNG|ico|ICO|css|sit|eps|wmf|zip|ppt|mpg|xls|gz|rpm|tgz|mov|MOV|exe|jpeg|JPEG|bmp|BMP)$\n\n\nskip URLs containing certain characters as probable queries, etc.\n-[?*!@=]\n\n\nskip URLs with slash-delimited segment that repeats 3+ times, to break loops\n-.*(/[^/])/[^/]\\1/[^/]+\\1/\n\n\naccept hosts in MY.DOMAIN.NAME\n#+^http://([a-z0-9]*\\.)*MY.DOMAIN.NAME/\n\n\naccept everything else\n+.*\n~    \n\n\u2014\nThanks,\nMichela",
        "Issue Links": []
    },
    "NUTCH-825": {
        "Key": "NUTCH-825",
        "Summary": "Publish nutch artifacts to central maven repository",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Enis Soztutar",
        "Created": "21/May/10 09:09",
        "Updated": "30/Aug/18 12:27",
        "Resolved": "28/Oct/10 13:54",
        "Description": "As per the discussion at NUTCH-821, publishing nutch artifacts to maven will be nice. NUTCH-821 already introduces dependency management with ivy. As for the remaining, ant task for generating pom files should be developed, and artifacts should be published to maven repo by a committer after a release.",
        "Issue Links": []
    },
    "NUTCH-826": {
        "Key": "NUTCH-826",
        "Summary": "Mailing list is broken.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "John Sherwood",
        "Created": "24/May/10 06:29",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "24/May/10 09:07",
        "Description": "All of the following addresses are failing:\nnutch-user@nutch.apache.org\nnutch-user-subscribe@nutch.apache.org\nnutch-user-subscribe@lucene.apache.org\nFor the last one, the mailer daemon said \n\"This mailing list has moved to user at nutch.apache.org.\"\nBelow is the message I tried to send:\nHi people,\nI've been banging my head against this problem for two days now.\nSimply, I want to add a field with the value of a given meta tag.\nI've been trying the parse-xml plugin, but that seems that it doesn't\nwork with version 1.0.  I've tried the code at\nhttp://sujitpal.blogspot.com/2009/07/nutch-getting-my-feet-wet.html\nand it hasn't worked.  I don't even know why.  I don't even know if my\nplugin is being used... or even looked for!  Nutch seems to have a\ninfuriating \"Fail silently\" policy for plugins.  I put a\nSystem.exit(1) in my filters just to see if my code is even being\nencountered.  It has not in spite of my config telling it to.\nHere's my config:\nnutch-site.xml\n...\n<property>\n <name>plugin.includes</name>\n <value>protocol-http|urlfilter-regex|parse-html|index-(basic|anchor)|query-(basic|site|url)|response-(json|xml)|summary-basic|scoring-opic|urlnormalizer-(pass|regex|basic)|metadata</value>\n</property>\n...\nparse-plugins.xml\n...\n<mimeType name=\"application/xhtml+xml\">\n   <plugin id=\"parse-html\" />\n   <plugin id=\"metadata\" />\n</mimeType>\n<mimeType name=\"text/html\">\n      <plugin id=\"parse-html\" />\n      <plugin id=\"metadata\" />\n</mimeType>\n<mimeType name=\"text/sgml\">\n      <plugin id=\"parse-html\" />\n      <plugin id=\"metadata\" />\n</mimeType>\n<mimeType name=\"text/xml\">\n         <plugin id=\"parse-html\" />\n         <plugin id=\"parse-rss\" />\n        <plugin id=\"metadata\" />\n        <plugin id=\"feed\" />\n</mimeType>\n...\n<alias name=\"metadata\"\nextension-id=\"com.example.website.nutch.parsing.MetaTagExtractorParseFilter\"\n/>\n...\nI've also copied the plugin.xml and jar from my build/metadata to the\nplugins root dir.\nNonetheless, Nutch runs and puts data in solr for me.  Afaik, Nutch is\ncompletely unaware of my plugin despite my config options.  Is the\nsome other place I need to tell Nutch to use my plugin?  Is there some\nother approach to do this without having to write a plugin?  This does\nseem like a lot of work to simply get a meta tag into a field.  Any\nhelp would be appreciated.\nSincerely,\nJohn Sherwood",
        "Issue Links": []
    },
    "NUTCH-827": {
        "Key": "NUTCH-827",
        "Summary": "HTTP POST Authentication",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1,                                            nutchgora",
        "Fix Version/s": "1.10",
        "Component/s": "protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Jasper van Veghel",
        "Created": "27/May/10 12:08",
        "Updated": "15/Jun/16 06:48",
        "Resolved": "13/Feb/15 22:06",
        "Description": "I've created a patch against the trunk which adds support for very rudimentary POST-based authentication support. It takes a link from nutch-site.xml with a site to POST to and its respective parameters (username, password, etc.). It then checks upon every request whether any cookies have been initialized, and if none have, it fetches them from the given link.\nThis isn't perfect but Works For Me (TM) as I generally only need to retrieve results from a single domain and so have no cookie overlap (i.e. if the domain cookies expire, all cookies disappear from the HttpClient and I can simply re-fetch them). A natural improvement would be to be able to specify one particular cookie to check the expiration-date against. If anyone is interested in this beside me I'd be glad to put some more effort into making this more universally applicable.",
        "Issue Links": [
            "/jira/browse/NUTCH-1518",
            "/jira/browse/NUTCH-1940",
            "/jira/browse/NUTCH-1929",
            "/jira/browse/NUTCH-1613",
            "/jira/browse/NUTCH-1943"
        ]
    },
    "NUTCH-828": {
        "Key": "NUTCH-828",
        "Summary": "Fetch Filter",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "fetcher",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "08/Jun/10 05:34",
        "Updated": "15/Nov/13 17:38",
        "Resolved": "15/Nov/13 17:38",
        "Description": "Adds a Nutch extension point for a fetch filter.  The fetch filter allows filtering content and parse data/text after it is fetched but before it is written to segments.  The fliter can return true if content is to be written or false if it is not.  \nSome use cases for this filter would be topical search engines that only want to fetch/index certain types of content, for example a news or sports only search engine.  In these types of situations the only way to determine if content belongs to a particular set is to fetch the page and then analyze the content.  If the content passes, meaning belongs to the set of say sports pages, then we want to include it.  If it doesn't then we want to ignore it, never fetch that same page in the future, and ignore any urls on that page.  If content is rejected due to a fetch filter then its status is written to the CrawlDb as gone and its content is ignored and not written to segments.  This effectively stop crawling along the crawl path of that page and the urls from that page.  An example filter, fetch-safe, is provided that allows fetching content that does not contain a list of bad words.",
        "Issue Links": []
    },
    "NUTCH-829": {
        "Key": "NUTCH-829",
        "Summary": "duplicate hadoop temp files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0,                                            1.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Mike Baranczak",
        "Created": "13/Jun/10 23:16",
        "Updated": "22/May/13 03:54",
        "Resolved": "28/Apr/13 01:14",
        "Description": "When two crawls are started at exactly the same time, I see the following error: \n\norg.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/tmp/hadoop-mike/mapred/temp/generate-temp-1276463469075 already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:111)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:793)\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1142)\n\tat org.apache.nutch.crawl.Generator.generate(Generator.java:472)\n\tat org.apache.nutch.crawl.Generator.generate(Generator.java:409)\n[...]\nI traced it down to this code in Generator (I'm using Nutch 1.0, but this is still in the trunk):\n\nPath tempDir =\n      new Path(getConf().get(\"mapred.temp.dir\", \".\") +\n               \"/generate-temp-\"+ System.currentTimeMillis());\nI admit that this is an unlikely scenario for most users, but it just so happens that I ran into it. To absolutely guarantee that the temp directory doesn't already exist, I suggest changing System.currentTimeMillis() to java.util.UUID.randomUUID().toString().",
        "Issue Links": []
    },
    "NUTCH-830": {
        "Key": "NUTCH-830",
        "Summary": "ScoringFilter to restrict the crawl to the hosts/domains listed in the seeds",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.1",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "23/Jun/10 13:03",
        "Updated": "17/Aug/10 08:57",
        "Resolved": "17/Aug/10 08:55",
        "Description": "The DomainURLFilter allows to specify the domains to consider for a crawl. This works fine but requires to edit a list of domain / hosts manually. The patch presented here offers the same functionality but uses a different mechanism as we use a custom scoring filter to filter the outlinks. \n1. add a metadata to your seed list e.g. 'origin' with as values the seed URL\ne.g. http://www.cnn.com/ origin=http://www.cnn.com/\n2. The custom scoring filter would take care of :\n\ntransmitting the origin metadata to its outlinks\nremove from the outlinks the ones which do not have the same host / domain as the origin\n\nThe parameter scoring.insite.mode allows to specify whether to restrict on the host or domain. The parameter scoring.insite.addOriginOnInject allows to addition of the metadata during the injection step and reuses the URL automatically.",
        "Issue Links": []
    },
    "NUTCH-831": {
        "Key": "NUTCH-831",
        "Summary": "Allow configuration of how fields crawled by Nutch are stored / indexed / tokenized",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.2",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Jeroen van Vianen",
        "Created": "23/Jun/10 13:16",
        "Updated": "22/May/13 03:54",
        "Resolved": "29/Jun/10 05:40",
        "Description": "Currently, it is impossible to change the way Nutch stores / indexes / tokenizes the fields it creates while crawling and indexing URLs.\nI wanted to be able to store the content field so I could use my own Lucene code and hightlighting code to work on the stored content field. Currently, content is only tokenized.\nSee nutch-trunk/src/plugin/index-basic/src/java/org/apache/nutch/indexer/basic/BasicIndexer.addIndexBackendOptions(Configuration conf) for the current settings.\nThere's already code in Nutch to configure how fields are stored / indexed / tokenized from conf/nutch-site.xml:\n<property>\n  <name>lucene.field.store.content</name>\n  <value>YES</value>\n</property>\n(content is the name of the field)\nHowever, the BasicIndexer overrides these settings with its own. Attached is a patch which will make sure the above settings are only applied when none have been specified in nutch-site.xml",
        "Issue Links": []
    },
    "NUTCH-832": {
        "Key": "NUTCH-832",
        "Summary": "Website menu has lots of broken links - in particular the API docs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "nutchgora",
        "Component/s": "documentation",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Alexander Lachlan Mclintock",
        "Created": "24/Jun/10 11:11",
        "Updated": "22/May/13 03:53",
        "Resolved": "25/Jun/10 22:00",
        "Description": "The website seems to have lots of broken links. eg the menu on the left points to various URLs of the form \nhttp://nutch.apache.org/apidocs-1.0/index.html\nbut these don't seem to exist on the server. \nAlso \nhttp://nutch.apache.org/release/",
        "Issue Links": []
    },
    "NUTCH-833": {
        "Key": "NUTCH-833",
        "Summary": "Website is still Lucene branded",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "nutchgora",
        "Component/s": "documentation",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Alexander Lachlan Mclintock",
        "Created": "24/Jun/10 11:18",
        "Updated": "22/May/13 03:54",
        "Resolved": "26/Jun/10 05:36",
        "Description": "The Nutch website still has a lot of Lucene branding and links which are confusing. eg the breadcrumbs\nApache > Lucene > Nutch  > \nappear at the top of most pages, along with the lucene logo and link to their home page.",
        "Issue Links": []
    },
    "NUTCH-834": {
        "Key": "NUTCH-834",
        "Summary": "Separate the Nutch web site from trunk",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "nutchgora",
        "Component/s": "documentation",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "24/Jun/10 13:58",
        "Updated": "01/Jul/10 05:45",
        "Resolved": "30/Jun/10 08:34",
        "Description": "As discussed on dev@, it would be useful to move the PDFBox Nutch web site sources from .../asf/nutch/trunk to .../asf/nutch/site and to use the svnpubsub mechanism for instant deployment of site changes.\nThe related issue for infra is https://issues.apache.org/jira/browse/INFRA-2822\nSee also https://issues.apache.org/jira/browse/PDFBOX-623",
        "Issue Links": []
    },
    "NUTCH-835": {
        "Key": "NUTCH-835",
        "Summary": "document deduplication (exact duplicates) failed using MD5Signature",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0,                                            1.1",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Sebastian Nagel",
        "Created": "25/Jun/10 11:01",
        "Updated": "22/May/13 03:53",
        "Resolved": "01/Jul/10 12:11",
        "Description": "The MD5Signature class calculates different signatures for identical documents.\nThe reason is that\n  byte[] data = content.getContent();\n  ... StringBuilder().append(data) ...\nuses java.lang.Object.toString() to get a string representation of the (binary) content\nwhich results in unique hash codes (e.g., [B@30dc9065) even for two byte arrays\nwith identical content.\nA solution would be to take the MD5 sum of the binary content as first part of the\nfinal signature calculation (the parsed content is the second part):\n  ... .append(StringUtil.toHexString(MD5Hash.digest(data).getDigest())).append(parse.getText());\nOf course, there are many other solutions...",
        "Issue Links": []
    },
    "NUTCH-836": {
        "Key": "NUTCH-836",
        "Summary": "Remove deprecated parse plugins",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "30/Jun/10 11:30",
        "Updated": "04/Jul/10 04:23",
        "Resolved": "02/Jul/10 10:55",
        "Description": "Some of the parser plugins in 1.1 are covered by the parse-tika plugin. These plugins have been kept in 1.1 but should be removed from 2.0 where we'll rely on parse-tika almost exclusively. Some existing plugins might be kept when there is no equivalent in Tika (to be discussed). The following plugins are removed : \n\nparse-html\nparse-msexcel\nparse-mspowerpoint\nparse-msword\nparse-pdf\nparse-oo\nparse-text\nlib-jakarta-poi\nlib-parsems\n\nThe patch does not (yet) remove :\n\nparse-ext\nparse-js\nparse-rss\nparse-swf\nparse-zip\nfeed\n\nPlease review the patch and vote for its inclusion in the trunk.",
        "Issue Links": []
    },
    "NUTCH-837": {
        "Key": "NUTCH-837",
        "Summary": "Remove search servers and Lucene dependencies",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "nutchgora",
        "Component/s": "web gui",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "30/Jun/10 15:42",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "02/Jul/10 17:29",
        "Description": "One of the main aspects of 2.0 is the delegation of the indexing and search to external resources like SOLR. We can simplify the code a lot by getting rid of the : \n\nsearch servers\nindexing and analysis with Lucene\nsearch side functionalities : ontologies / clustering etc...\nIn the short term only SOLR / SOLRCloud will be supported but the plan would be to add other systems as well.",
        "Issue Links": []
    },
    "NUTCH-838": {
        "Key": "NUTCH-838",
        "Summary": "Add timing information to all Tool classes",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "fetcher,                                            generator,                                            indexer,                                            linkdb,                                            parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Jeroen van Vianen",
        "Created": "30/Jun/10 21:30",
        "Updated": "22/May/13 03:53",
        "Resolved": "03/Jul/10 18:01",
        "Description": "Am happily trying to crawl a few hundred URLs incrementally. Performance is degrading suddenly after the index reaches approximately 25000 URLs.\nAt first each inject (generate, fetch, parse, updatedb) * 3, invertlinks, solrindex, solrdedup batch takes approximately half an hour with topN 500, but elapsed times now increase to 00h45m,  01h15m, 01h30m with every batch. As I'm uncertain which of the phases takes so much time I decided to add start and finish times to al classes that implement Tool so I at least have a feeling and can review them in a log file.\nAm using pretty old hardware, but I am planning to recrawl these URLs on a regular basis and if every iteration is going to take more and more time, index updates will be few and far between \nI added timing information to all Tool classes for consistency whereas there are only 10 or so Tools that are really interesting.",
        "Issue Links": [
            "/jira/browse/NUTCH-697"
        ]
    },
    "NUTCH-839": {
        "Key": "NUTCH-839",
        "Summary": "nutch doesnt run under 0.20.2+228-1~karmic-cdh3b1 version of hadoop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Robert Gonzalez",
        "Created": "01/Jul/10 19:27",
        "Updated": "28/Nov/11 13:13",
        "Resolved": "28/Nov/11 13:13",
        "Description": "new versions of hadoop appear to put jars in a different format now, instead of file:/a/b/c/d/job.jar, its now jar:file:/a/b/c/d/job.jar!, which breaks nutch when its trying to load its plugins.  Specifically, the stack trace looks like:\nCaused by: java.lang.RuntimeException: x point org.apache.nutch.net.URLNormalizer not found.\n\tat org.apache.nutch.net.URLNormalizers.<init>(URLNormalizers.java:124)\n\tat org.apache.nutch.crawl.Injector$InjectMapper.configure(Injector.java:57)\nA simple test class was written the used the URLFilters class, and the following stack trace resulted:\n10/07/01 14:25:25 INFO mapred.JobClient: Task Id : attempt_201006171624_46525_m_000000_1, Status : FAILED\njava.lang.RuntimeException: org.apache.nutch.net.URLFilter not found.\n\tat org.apache.nutch.net.URLFilters.<init>(URLFilters.java:52)\n\tat com.maxpoint.crawl.BidSampler$BIdSMapper.setup(BidSampler.java:42)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:170)\nRunning this on an older version of hadoop works.",
        "Issue Links": []
    },
    "NUTCH-840": {
        "Key": "NUTCH-840",
        "Summary": "Port tests from parse-html to parse-tika",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "1.1,                                            1.6",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "02/Jul/10 10:27",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "We don't have test for HTML in parse-tika so I'll copy them from the old parse-html plugin",
        "Issue Links": []
    },
    "NUTCH-841": {
        "Key": "NUTCH-841",
        "Summary": "Create a Wicket-based Web Application for Nutch",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.3",
        "Component/s": "web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Chris A. Mattmann",
        "Created": "02/Jul/10 15:56",
        "Updated": "06/Apr/15 16:58",
        "Resolved": "22/Sep/14 14:24",
        "Description": "In light of the conversation on NUTCH-837, we are removing the old Nutch webapp and will replace it with a 2.0 one that works with GORA + Solr. \nApache Nutch versions prior to 1.3 used to ship with a web application that allowed basic search, and browse of the information captured in the Nutch index. Since 1.3, we deprecated and removed the webapp mainly due to the fact that the segment API changed (we moved to Solr), and also due to the fact that we didn't want to maintain a webapp b/c those JSPs were a pain.\nI am going to propose having a Nutch web application using Apache Wicket http://wicket.apache.org/. This would be very cool and since I know Wicket, I'm willing to help maintain it. \nThe webapp should implement all of the old web pages and functionality, and also should support the basic views, and connection to Solr instead of to Lucene, and of should also consider both the trunk branch, and the 2.0 branch (Gora based).\nI'm putting this out there as a GSoC project for 2013.",
        "Issue Links": [
            "/jira/browse/NUTCH-929",
            "/jira/browse/NUTCH-929",
            "/jira/browse/NUTCH-1286",
            "/jira/browse/NUTCH-1769"
        ]
    },
    "NUTCH-842": {
        "Key": "NUTCH-842",
        "Summary": "AutoGenerate WebPage code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.2",
        "Component/s": "None",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "03/Jul/10 07:45",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Mar/13 22:17",
        "Description": "This issue will track the addition of an ant task that will automatically generate o.a.n.storage.WebPage (and ProtocolStatus and ParseStatus) from src/gora/webpage.avsc.",
        "Issue Links": [
            "/jira/browse/NUTCH-1477"
        ]
    },
    "NUTCH-843": {
        "Key": "NUTCH-843",
        "Summary": "Separate the build and runtime environments",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "07/Jul/10 15:38",
        "Updated": "22/May/13 03:53",
        "Resolved": "07/Jul/10 20:31",
        "Description": "Currently there is no clean separation of source, build and runtime artifacts. On one hand, it makes it easier to get started in local mode, but on the other hand it makes the distributed (or pseudo-distributed) setup much more challenging and tricky. Also, some resources (config files and classes) are included several times on the classpath, they are loaded under different classloaders, and in the end it's not obvious what copy and why takes precedence.\nHere's an example of a harmful unintended behavior caused by this mess: Hadoop daemons (jobtracker and tasktracker) will get conf/ and build/ on their classpath. This means that a task running on this cluster will have two copies of resources from these locations - one from the inherited classpath from tasktracker, and the other one from the just unpacked nutch.job file. If these two versions differ, only the first one will be loaded, which in this case is the one taken from the (unpacked) conf/ and build/ - the other one, from within the nutch.job file, will be ignored.\nIt's even worse when you add more nodes to the cluster - the nutch.job will be shipped to the new nodes as a part of each task setup, but now the remote tasktracker child processes will use resources from nutch.job - so some tasks will use different versions of resources than other tasks. This usually leads to a host of very difficult to debug issues.\nThis issue proposes then to separate these environments into the following areas:\n\nsource area - i.e. our current sources. Note that bin/ scripts will belong to this category too, so there will be no top-level bin/. nutch-default.xml belongs to this category too. Other customizable files can be moved to src/conf too, or they could stay in top-level conf/ as today, with a README that explains that changes made there take effect only after you rebuild the job jar.\n\n\nbuild area - contains build artifacts, among them the nutch.job jar.\n\n\nruntime (or deploy) area - this area contains all artifacts needed to run Nutch jobs. For a distributed setup that uses an existing Hadoop cluster (installed from plain vanilla Hadoop release) this will be a /deploy directory, where we put the following:\n\nbin/nutch\nnutch.job\n\n\nThat's it - nothing else should be needed, because all other resources are already included in the job jar. These resources can be copied directly to the master Hadoop node.\n\nFor a local setup (using LocalJobTracker) this will be a /runtime directory, where we put the following:\n\nbin/nutch\nlib/hadoop-libs\nplugins/\nnutch.job\n\n\nDue to limitations in the PluginClassLoader the local runtime requires that the plugins/ directory be unpacked from the job jar. And we need the hadoop libs to run in the local mode. We may later on refine this local setup to something like this:\n\nbin/nutch\nconf/\nlib/hadoop-libs\nlib/nutch-libs\nplugins/\nnutch.jar\n\n\nso that it's easier to modify the config without rebuilding the job jar (which actually would not be used in this case).",
        "Issue Links": []
    },
    "NUTCH-844": {
        "Key": "NUTCH-844",
        "Summary": "Improve NutchConfiguration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "08/Jul/10 12:59",
        "Updated": "22/May/13 03:53",
        "Resolved": "14/Jul/10 14:41",
        "Description": "This patch cleans up NutchConfiguration from servlet dependency, and modifies the API to allow bootstrapping via API from Properties. This is important for use cases where Nutch is embedded in a larger application.\nAlso, while I'm at it, remove the support for alternative \"crawl\" configuration when running Crawl tool, which has always been a source of confusion.",
        "Issue Links": [
            "/jira/browse/NUTCH-356",
            "/jira/browse/NUTCH-925"
        ]
    },
    "NUTCH-845": {
        "Key": "NUTCH-845",
        "Summary": "Native hadoop libs not available through maven",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "08/Jul/10 13:34",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/Jul/10 14:14",
        "Description": "There are no maven artifacts for the native libs (I verified this on Hadoop ML). I think it's better to delete the libs, after all we don't want to keep bits and pieces of dependencies in our svn, but let's leave a placeholder and a README that explains how to get them.",
        "Issue Links": []
    },
    "NUTCH-846": {
        "Key": "NUTCH-846",
        "Summary": "Remove Hadoop related scripts in /bin",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "09/Jul/10 08:06",
        "Updated": "09/Jul/10 15:17",
        "Resolved": "09/Jul/10 09:39",
        "Description": "The build and runtime environments have been separated in https://issues.apache.org/jira/browse/NUTCH-843 and we now have two separate folders for local and deployed configurations. The Hadoop scripts in /bin are of no use in local mode. As for the deployed configuration it is meant to be used in a separate Hadoop setup which will have these scripts already. I don't think we have any use for /bin, do we?",
        "Issue Links": []
    },
    "NUTCH-847": {
        "Key": "NUTCH-847",
        "Summary": "Wrong version of SOLR in Ivy.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "09/Jul/10 12:04",
        "Updated": "09/Jul/10 15:20",
        "Resolved": "09/Jul/10 12:08",
        "Description": "We'd upgraded to SOLR 4.0 before releasing 1.1 but the Ivy config currently still mentions 1.3.0",
        "Issue Links": []
    },
    "NUTCH-848": {
        "Key": "NUTCH-848",
        "Summary": "Error when calling 'nutch solrindex' in deployed configuration",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "09/Jul/10 15:37",
        "Updated": "14/Jul/10 13:57",
        "Resolved": "14/Jul/10 13:57",
        "Description": "See https://issues.apache.org/jira/browse/NUTCH-843\nIn a deployed environment with just bin/nutch and the job file on an independent Hadoop configuration, a call to 'nutch solrindex' yields : \n\nException in thread \"main\" java.lang.NoClassDefFoundError: org/apache/solr/client/solrj/SolrServer\nCaused by: java.lang.ClassNotFoundException: org.apache.solr.client.solrj.SolrServer\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:200)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:188)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:307)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:252)\n\tat java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)\nCould not find the main class: org.apache.nutch.indexer.solr.SolrIndexer.  Program will exit.\n\n\nSurprisingly some tasks work fine, but not this one.",
        "Issue Links": []
    },
    "NUTCH-849": {
        "Key": "NUTCH-849",
        "Summary": "different versions of the same library in nutch-2.0-dev.job and local\\lib directory",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Pham Tuan Minh",
        "Created": "12/Jul/10 10:01",
        "Updated": "08/Jan/23 19:49",
        "Resolved": "08/Jan/23 19:49",
        "Description": "Hi,\nI found that after building runtime, In nutch-2.0-dev.job and local\\lib directory contains different versions of the same library\nant-1.7.1.jar\nant-1.6.5.jar\nservlet-api-2.5-20081211.jar\nservlet-api-2.5-6.1.14.jar\nI predict these libraries come from different dependencies branch. Anyone help me to fix it?\nThanks,",
        "Issue Links": []
    },
    "NUTCH-850": {
        "Key": "NUTCH-850",
        "Summary": "SolrDeleteDuplicates needs to clone the SolrRecord objects",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "12/Jul/10 15:57",
        "Updated": "12/Jul/10 16:11",
        "Resolved": "12/Jul/10 16:11",
        "Description": "The reduce() method of SolrDeleteDuplicates deduplicates SOLRRecords given their signature. The first SOLRRecord is stored in a variable recordToKeep and is compared to the following SOLRRecords found with the same signature. The only trouble being that the first instance is reused by Hadoop when calling values.next() and hence  recordToKeep gets the same values as the latest call to values.next(). \nThe patch attached clones the SOLRRecord before assigning them to recordToKeep in order to avoid the problem.",
        "Issue Links": []
    },
    "NUTCH-851": {
        "Key": "NUTCH-851",
        "Summary": "Port logging to slf4j",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "13/Jul/10 16:10",
        "Updated": "10/Aug/10 08:46",
        "Resolved": "10/Aug/10 08:46",
        "Description": "We are already inheriting a dependency on slf4j from Solr so we might as well use it \nAny thoughts on this?",
        "Issue Links": []
    },
    "NUTCH-852": {
        "Key": "NUTCH-852",
        "Summary": "parser not found for contentType=application/xhtml+xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Pham Tuan Minh",
        "Created": "13/Jul/10 19:29",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "14/Jul/10 09:15",
        "Description": "I config nutch trunk to crawl sample site (http://www.lucidimagination.com/), then it post to solr server for indexing, however, I got following error. It seems tika parser is not working properly or tika libraries is not recognized!\n----------------------\n$ bin/nutch-local crawl urls -solr http://127.0.0.1:8983/solr/ -dir crawl -depth 3 -topN 50\ncrawl started in: crawl\nrootUrlDir = urls\nthreads = 10\ndepth = 3\nsolrUrl=http://127.0.0.1:8983/solr/\ntopN = 50\nInjector: starting at 2010-07-14 02:08:20\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2010-07-14 02:08:31, elapsed: 00:00:11\nGenerator: starting at 2010-07-14 02:08:32\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 50\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl/segments/20100714020838\nGenerator: finished at 2010-07-14 02:08:42, elapsed: 00:00:10\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.age\nnts' property.\nFetcher: starting at 2010-07-14 02:08:42\nFetcher: segment: crawl/segments/20100714020838\nFetcher: threads: 10\nQueueFeeder finished: total 1 records + hit by time limit :0\nfetching http://www.lucidimagination.com/\n-finishing thread FetcherThread, activeThreads=4\n-finishing thread FetcherThread, activeThreads=1\n-finishing thread FetcherThread, activeThreads=2\n-finishing thread FetcherThread, activeThreads=3\n-finishing thread FetcherThread, activeThreads=5\n-finishing thread FetcherThread, activeThreads=7\n-finishing thread FetcherThread, activeThreads=8\n-finishing thread FetcherThread, activeThreads=9\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0\nError parsing: http://www.lucidimagination.com/: org.apache.nutch.parse.ParseException: parser not found for contentType=application/xhtml+xml url=http://www.lucidimagination.com/\n        at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:74)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.output(Fetcher.java:879)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:647)\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2010-07-14 02:08:54, elapsed: 00:00:12\nCrawlDb update: starting at 2010-07-14 02:08:54\nCrawlDb update: db: crawl/crawldb\nCrawlDb update: segments: [crawl/segments/20100714020838]\nCrawlDb update: additions allowed: true\n$\nCrawlDb update: URL filtering: true\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2010-07-14 02:09:01, elapsed: 00:00:07\n$\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: topN: 50\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=1 - no more URLs to fetch.\nLinkDb: starting at 2010-07-14 02:09:06\nLinkDb: linkdb: crawl/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714014136\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714015544\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714020206\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714020232\nLinkDb: adding segment: file:/D:/work/workspace/nutch/runtime/local/crawl/segments/20100714020838\nLinkDb: merging with existing linkdb: crawl/linkdb\nLinkDb: finished at 2010-07-14 02:09:19, elapsed: 00:00:12\nSolrIndexer: starting at 2010-07-14 02:09:19\nSolrIndexer: finished at 2010-07-14 02:09:36, elapsed: 00:00:17\nSolrDeleteDuplicates: starting at 2010-07-14 02:09:41\nSolrDeleteDuplicates: Solr url: http://127.0.0.1:8983/solr/\nSolrDeleteDuplicates: finished at 2010-07-14 02:09:45, elapsed: 00:00:04\ncrawl finished: crawl\n----------------------\nThanks",
        "Issue Links": []
    },
    "NUTCH-853": {
        "Key": "NUTCH-853",
        "Summary": "Remove unused parameter files from conf/",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "14/Jul/10 15:05",
        "Updated": "14/Jul/10 16:19",
        "Resolved": "14/Jul/10 15:39",
        "Description": "Since we separated the build and runtime environments (NUTCH-843) some of the resources in conf/ are now useless. This is the case for instance of slave,master, hadoop-policy.xml, hdfs-site.xml etc... which are generated automatically from the templates. In local mode the hadoop related resources are not used and in deployed mode we already have them in the hadoop setup proper. Others like context.xsl were relevant for the servlets which have now been removed",
        "Issue Links": []
    },
    "NUTCH-854": {
        "Key": "NUTCH-854",
        "Summary": "Define standard attributes with values and explaination to configuration files in conf directory",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Pham Tuan Minh",
        "Created": "14/Jul/10 18:00",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "08/Aug/10 07:39",
        "Description": "It would make nutch easier to use if all configuration file in conf directory is defined standard attributes with values and explanation. For example, currently nutch-site.xml.template contains no attributes and no explanation, we should define them.\n-------------\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<!-- site-specific property overrides in this file. -->\n<configuration>\n<!-- Agent name-->\n<property>\n<name>http.agent.name</name>\n<value>nutch-solr-integration</value>\n</property>\n<!---->\n<property>\n<name>generate.max.per.host</name>\n<value>100</value>\n</property>\n<property>\n<!-- plug-in using in this site -->\n<name>plugin.includes</name>\n<value>protocol-http|urlfilter-regex|parse-tika|scoring-opic|urlnormalizer-(pass|regex|basic)</value>\n</property>\n</configuration>\n-------------\nThanks,",
        "Issue Links": []
    },
    "NUTCH-855": {
        "Key": "NUTCH-855",
        "Summary": "ScoringFilter and IndexingFilter: To allow for the propagation of URL Metatags and their subsequent indexing.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.2",
        "Component/s": "generator,                                            indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Scott Gonyea",
        "Created": "15/Jul/10 01:48",
        "Updated": "22/May/13 03:53",
        "Resolved": "07/Aug/10 15:33",
        "Description": "This plugin is designed to enhance the NUTCH-655 patch, by doing two things:\n1. Meta Tags that are supplied with your Crawl URLs, during injection, will be propagated throughout the outlinks of those Crawl URLs.\n2. When you index your URLs, the meta tags that you specified with your URLs will be indexed alongside those URLs--and can be directly queried, assuming you have done everything else correctly.\nThe flat-file of URLs you are injecting should, per NUTCH-655, be tab-delimited in the form of:\nwww.url.com\\tkey1=value1\\tkey2=value2\\t...\\tkeyN=valueN\nor:\nhttp://slashdot.org/\tcorp_owner=Geeknet\twill_it_blend=indubitably\nhttp://engadget.com/\tcorp_owner=Weblogs\tgenre=geeksquad_thriller\nTo activate this plugin, you must modify two properties in your nutch-sites.xml:\n1. plugin.includes\n   add: urlmeta\n   to:   <value>...</value>\n   ie: <value>urlmeta|parse-tika|scoring-opic|...</value>\n2. urlmeta.tags\n   Insert a comma-delimited list of metatags. Using the above example:\n   <value>corp_owner, will_it_blend, genre</value>\n   Note that you do not need to include the tag with every URL. However, you must specify each tag if you want it to be propagated and later indexed.",
        "Issue Links": []
    },
    "NUTCH-856": {
        "Key": "NUTCH-856",
        "Summary": "Use Tika for parsing feed",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "16/Jul/10 15:17",
        "Updated": "22/May/13 03:53",
        "Resolved": "20/Jul/10 18:35",
        "Description": "We currently have 2 plugins for dealing with feeds : \n\nfeeds\nparse-rss\n\nI have proposed https://issues.apache.org/jira/browse/TIKA-466 which would at least cover the functionalities of parse-rss. If/when this is added to Tika then we should be able to remove parse-rss and rely on Tika instead",
        "Issue Links": []
    },
    "NUTCH-857": {
        "Key": "NUTCH-857",
        "Summary": "DistributedBeans should not close their RPC counterparts",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.2",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "19/Jul/10 20:59",
        "Updated": "26/Jul/10 11:57",
        "Resolved": "26/Jul/10 11:57",
        "Description": "DistributedSearch and Segment Beans currently call close on their RPC counterparts from their own close methods.  This results in killing (closing) all distributed servers when the main bean (website, application, etc) is shutdown.  DistributedSearchServer (SegmentServer) are run independent from the main NutchBean or website calling those servers in shard type environments.  With the current code the distributed servers are closed and any further search requests throw IndexAlreadyClosed exceptions.  The distributed servers have to be restarted before searching can resume.  Obviously this doesn't work in a large distributed search where multiple beans could be called the distributed servers and where distributed servers could be coming up and down frequently.\nThe solution is simple though.  The Distributed beans shouldn't call close on their RPC counterparts.  Patch is attached.",
        "Issue Links": []
    },
    "NUTCH-858": {
        "Key": "NUTCH-858",
        "Summary": "No longer able to set per-field boosts on lucene documents",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.2",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Edward Drapkin",
        "Created": "21/Jul/10 19:41",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Aug/10 14:06",
        "Description": "I'm working on upgrading from Nutch 0.9 to Nutch 1.1 and I've noticed that it no longer seems possible to set boosts on specific fields in lucene documents.  This is, in my opinion, a major feature regression and removes a huge component to fine tuning search.  Can this be added?",
        "Issue Links": []
    },
    "NUTCH-859": {
        "Key": "NUTCH-859",
        "Summary": "Diff trunk and NutchBase",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "23/Jul/10 13:17",
        "Updated": "09/Aug/10 08:39",
        "Resolved": "09/Aug/10 08:37",
        "Description": "Before we turn NutchBase into trunk we need to make sure that all (more or less) recent changes in the trunk have been ported to NutchBase. I have done that recently but given that there is a very large number of changes I might have missed a few things here and there.",
        "Issue Links": []
    },
    "NUTCH-860": {
        "Key": "NUTCH-860",
        "Summary": "package task fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchbase,                                            nutchgora",
        "Fix Version/s": "nutchbase,                                            nutchgora",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "23/Jul/10 14:01",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "26/Jul/10 10:26",
        "Description": "The ant 'package' tasks fails since we reorganised the code and removed the bin directory. The patch attached fixed the issue and adds the /runtime directory to the package.\nAn objections?",
        "Issue Links": []
    },
    "NUTCH-861": {
        "Key": "NUTCH-861",
        "Summary": "Rename HTMLParserFilter",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "23/Jul/10 15:43",
        "Updated": "11/Aug/10 09:58",
        "Resolved": "11/Aug/10 09:38",
        "Description": "The name 'HTMLParserFilter' is slightly confusing as it gives the impression that the implementations of this endpoint are getting only HTML documents. \nThe plugin parse-tika calls the HTMLParserFilters and passes them a DOM representation of the XHTML-like documents it got from the underlying Tika parsers. This means that we are getting a DOM representation for documents in any format recognised by Tika and not only HTML.\nWhat about renaming HTMLParserFilter into ParserFilter? Any other suggestions?",
        "Issue Links": []
    },
    "NUTCH-862": {
        "Key": "NUTCH-862",
        "Summary": "HttpClient null pointer exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Jul/10 11:54",
        "Updated": "22/May/13 03:54",
        "Resolved": "17/Sep/10 14:54",
        "Description": "When re-fetching a document (a continued crawl) HttpClient throws an null pointer exception causing the document to be emptied:\n2010-07-27 12:45:09,199 INFO  fetcher.Fetcher - fetching http://localhost/doc/selfhtml/html/index.htm\n2010-07-27 12:45:09,203 ERROR httpclient.Http - java.lang.NullPointerException\n2010-07-27 12:45:09,204 ERROR httpclient.Http - at org.apache.nutch.protocol.httpclient.HttpResponse.<init>(HttpResponse.java:138)\n2010-07-27 12:45:09,204 ERROR httpclient.Http - at org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:154)\n2010-07-27 12:45:09,204 ERROR httpclient.Http - at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:220)\n2010-07-27 12:45:09,204 ERROR httpclient.Http - at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:537)\n2010-07-27 12:45:09,204 INFO  fetcher.Fetcher - fetch of http://localhost/doc/selfhtml/html/index.htm failed with: java.lang.NullPointerException\nBecause the document is re-fetched the server answers \"304\" (not modified):\n127.0.0.1 - - [27/Jul/2010:12:45:09 +0200] \"GET /doc/selfhtml/html/index.htm HTTP/1.0\" 304 174 \"-\" \"Nutch-1.0\"\nNo content is sent in this case (empty http body).\nIndex: trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpResponse.java\n===================================================================\n\u2014 trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpResponse.java        (revision 979647)\n+++ trunk/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpResponse.java        (working copy)\n@@ -134,7 +134,8 @@\n         if (code == 200) throw new IOException(e.toString());\n         // for codes other than 200 OK, we are fine with empty content\n       } finally \n{\n-        in.close();\n+        if (in != null)\n+          in.close();\n         get.abort();\n       }",
        "Issue Links": []
    },
    "NUTCH-863": {
        "Key": "NUTCH-863",
        "Summary": "Benchmark and a testbed proxy server",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "29/Jul/10 13:07",
        "Updated": "22/May/13 03:53",
        "Resolved": "30/Jul/10 19:52",
        "Description": "This issue adds two components:\n\na testbed proxy server that can serve various content: pre-fetched Nutch segments, forward requests to original URLs, or create a lot of unique but predictable fake content (with outlinks) on the fly.\na simple Benchmark class to measure the time taken to complete several crawl cycles using fake content.\n'ant proxy' and 'ant benchmark' targets to execute a benchmark run.\n\nTogether these tools should provide a more or less objective method to measure the end-to-end crawl performance. This initial version can be further instrumented to collect statistics about various stages of data processing.",
        "Issue Links": []
    },
    "NUTCH-864": {
        "Key": "NUTCH-864",
        "Summary": "Fetcher generates entries with status 0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher",
        "Assignee": "Dogacan Guney",
        "Reporter": "Julien Nioche",
        "Created": "30/Jul/10 13:59",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "After a round of fetching which got the following protocol status :\n10/07/30 15:11:39 INFO mapred.JobClient:     ACCESS_DENIED=2\n10/07/30 15:11:39 INFO mapred.JobClient:     SUCCESS=1177\n10/07/30 15:11:39 INFO mapred.JobClient:     GONE=3\n10/07/30 15:11:39 INFO mapred.JobClient:     TEMP_MOVED=138\n10/07/30 15:11:39 INFO mapred.JobClient:     EXCEPTION=93\n10/07/30 15:11:39 INFO mapred.JobClient:     MOVED=521\n10/07/30 15:11:39 INFO mapred.JobClient:     NOTFOUND=62\nI ran : ./nutch org.apache.nutch.crawl.WebTableReader -stats\n10/07/30 15:12:37 INFO crawl.WebTableReader: Statistics for WebTable: \n10/07/30 15:12:37 INFO crawl.WebTableReader: TOTAL urls:\t2690\n10/07/30 15:12:37 INFO crawl.WebTableReader: retry 0:\t2690\n10/07/30 15:12:37 INFO crawl.WebTableReader: min score:\t0.0\n10/07/30 15:12:37 INFO crawl.WebTableReader: avg score:\t0.7587361\n10/07/30 15:12:37 INFO crawl.WebTableReader: max score:\t1.0\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 0 (null):\t649\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 2 (status_fetched):\t1177 (SUCCESS=1177)\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 3 (status_gone):\t112 \n10/07/30 15:12:37 INFO crawl.WebTableReader: status 34 (status_retry):\t93 (EXCEPTION=93)\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 4 (status_redir_temp):\t138  (TEMP_MOVED=138)\n10/07/30 15:12:37 INFO crawl.WebTableReader: status 5 (status_redir_perm):\t521 (MOVED=521)\n10/07/30 15:12:37 INFO crawl.WebTableReader: WebTable statistics: done\nThere should not be any entries with status 0 (null)\nI will investigate a bit more...",
        "Issue Links": []
    },
    "NUTCH-865": {
        "Key": "NUTCH-865",
        "Summary": "Format source code in unique style",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Pham Tuan Minh",
        "Created": "30/Jul/10 15:38",
        "Updated": "29/Jan/15 05:39",
        "Resolved": "29/Jan/15 05:39",
        "Description": "We should define a standard format rules for source code/comments, then using eclipse tool to format the whole source code in the same style.",
        "Issue Links": []
    },
    "NUTCH-866": {
        "Key": "NUTCH-866",
        "Summary": "STOP Nutch without breaking the crawled data",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Pham Tuan Minh",
        "Created": "30/Jul/10 16:44",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "14/Aug/10 07:50",
        "Description": "How we can stop running nutch instance in local mode and in reducer mode without breaking the crawled data? \nFor example, you push a list of site that take a long time to complete crawl; then you want to stop nutch instance suddenly ...\n\nFor local mode, I suggest as below\n\nWe create a stop.txt file in specific directory, then for a piece of time, nutch instance will check whether this file existed or not; if existed, nutch instance will stop itself normally\n\nFor reducer mode, may we use zookeper to keep state of each instance?\n\nAny other suggestion?\nThanks,",
        "Issue Links": []
    },
    "NUTCH-867": {
        "Key": "NUTCH-867",
        "Summary": "Port Nutch benchmark to Nutchbase",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchbase",
        "Fix Version/s": "nutchbase",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "31/Jul/10 18:06",
        "Updated": "22/May/13 03:53",
        "Resolved": "05/Aug/10 12:53",
        "Description": "Bring tools from NUTCH-863 to Nutchbase, and measure the performance of the Nutchbase branch vs. trunk.",
        "Issue Links": []
    },
    "NUTCH-868": {
        "Key": "NUTCH-868",
        "Summary": "ParseSegment NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchbase,                                            1.2,                                            nutchgora",
        "Fix Version/s": "1.1,                                            nutchbase,                                            1.2,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Max Lynch",
        "Created": "02/Aug/10 02:51",
        "Updated": "22/May/13 03:53",
        "Resolved": "02/Aug/10 10:02",
        "Description": "The Nutch parser step will fail if ParseUtil.parse returns null.  Perhaps the issue is because I applied this patch: https://issues.apache.org/jira/browse/NUTCH-696 \nThe exception happens in ParseSegment.map at line 91.  The attached patch fixes the issue.",
        "Issue Links": []
    },
    "NUTCH-869": {
        "Key": "NUTCH-869",
        "Summary": "Add back parse-html",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchbase,                                            1.2,                                            nutchgora",
        "Fix Version/s": "nutchbase,                                            1.2,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Andrzej Bialecki",
        "Created": "02/Aug/10 10:31",
        "Updated": "04/Aug/10 10:29",
        "Resolved": "04/Aug/10 10:27",
        "Description": "We need to add back parse-html. There are a few serious problems with HTML parsing in Tika 0.7, so it's not possible to do a quality crawl using parse-tika alone. The necessary improvements to Tika are on the way, so if a future version of Tika > 0.7 has a chance of passing our tests we can again remove this plugin and use parse-tika alone.",
        "Issue Links": []
    },
    "NUTCH-870": {
        "Key": "NUTCH-870",
        "Summary": "Injector should add the metadata before calling injectedScore",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchbase,                                            1.2",
        "Fix Version/s": "nutchbase,                                            1.2,                                            nutchgora",
        "Component/s": "injector",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Julien Nioche",
        "Created": "02/Aug/10 12:26",
        "Updated": "10/Aug/10 08:28",
        "Resolved": "10/Aug/10 08:28",
        "Description": "The injector should add the metadata to the CrawlDatum before calling scfilters.injectedScore(value, datum) as the metadata could be used by the ScoringFilters in this method.\nThe method scfilters.injectedScore() should also be called regardless of whether the score is the one specified in the seed list or the one set by default but we should then modify the scoringOPICFilter so that it does not override it. \nAny thoughts?",
        "Issue Links": []
    },
    "NUTCH-871": {
        "Key": "NUTCH-871",
        "Summary": "MoreIndexingFilter missing date format",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchbase",
        "Fix Version/s": "nutchbase,                                            1.2,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Max Lynch",
        "Created": "02/Aug/10 16:42",
        "Updated": "22/May/13 03:54",
        "Resolved": "03/Aug/10 17:20",
        "Description": "Added another date format to MoreIndexingFilter.java which is part of the indexer-more plugin.  I had a date that wasn't being properly parsed so I fixed it.",
        "Issue Links": []
    },
    "NUTCH-872": {
        "Key": "NUTCH-872",
        "Summary": "Change the default fetcher.parse to FALSE",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            1.3,                                            nutchgora",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "05/Aug/10 13:54",
        "Updated": "10/Sep/12 11:52",
        "Resolved": "01/Jul/11 15:30",
        "Description": "I propose to change this property to false. The reason is that it's a safer default - parsing issues don't lead to a loss of the downloaded content. For larger crawls this is the recommended way to run Fetcher. Users that run smaller crawls can still override it.",
        "Issue Links": []
    },
    "NUTCH-873": {
        "Key": "NUTCH-873",
        "Summary": "Ivy configuration settings don't include Gora",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "07/Aug/10 23:18",
        "Updated": "22/May/13 03:53",
        "Resolved": "07/Aug/10 23:20",
        "Description": "The Nutch 2.0 trunk now requires Gora, and even though it's not available in any repository, we should still configure Ivy to depend on it so that the build will work provided you follow the Gora instructions here:\nhttp://github.com/enis/gora\nI've fixed it locally and will commit an update shortly that takes care of it. In order to compile Nutch trunk now (before we get Gora into a repo), here are the steps (copied from http://github.com/enis/gora):\n\n$ git clone git://github.com/enis/gora.git\n$ cd gora \n$ ant\n\n\nThis will install Gora into your local Ivy repo. Then from there on out, just update your Ivy resolver (or alternatively just the Nutch build post this issue being resolved) and you're good.",
        "Issue Links": []
    },
    "NUTCH-874": {
        "Key": "NUTCH-874",
        "Summary": "Make sure all plugins in src/plugin are compatible with Nutch 2.0 and Gora",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "08/Aug/10 19:07",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "I just noticed while fixing NUTCH-564 that the ExtParser hasn't been brought up to date with Nutch 2.0 trunk. We should review the plugins in src/plugin to make sure they all work with Gora/Nutchbase now.",
        "Issue Links": [
            "/jira/browse/NUTCH-1515"
        ]
    },
    "NUTCH-875": {
        "Key": "NUTCH-875",
        "Summary": "Port Webgraph to Nutch 2.0",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "linkdb",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "09/Aug/10 08:42",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The webgraph has not yet been ported to the GORA-based API.",
        "Issue Links": [
            "/jira/browse/NUTCH-1030",
            "/jira/browse/NUTCH-771",
            "/jira/browse/NUTCH-1051"
        ]
    },
    "NUTCH-876": {
        "Key": "NUTCH-876",
        "Summary": "Remove remaining robots/IP blocking code in lib-http",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "09/Aug/10 14:09",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Aug/10 08:34",
        "Description": "There are remains of the (very old) blocking code in lib-http/.../HttpBase.java. This code was used with the OldFetcher to manage politeness limits. New trunk doesn't have OldFetcher anymore, so this code is useless. Furthermore, there is an actual bug here - FetcherJob forgets to set Protocol.CHECK_BLOCKING and Protocol.CHECK_ROBOTS to false, and the defaults in lib-http are set to true.",
        "Issue Links": []
    },
    "NUTCH-877": {
        "Key": "NUTCH-877",
        "Summary": "Allow setting of slop values for non-quote phrase queries on query-basic plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2",
        "Fix Version/s": "1.2",
        "Component/s": "None",
        "Assignee": "Dennis Kubes",
        "Reporter": "Dennis Kubes",
        "Created": "09/Aug/10 18:47",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "26/Aug/10 15:01",
        "Description": "Patch adds a configuration variable for setting slop values on phrase queries.  The default slop value, which currently can't be changed through configuration, is Integer.MAX_VALUE.  It produces something like this, which doesn't seem right to me.  If you are searching for a phrase you usually want it within a certain distance:\n2.9141337E-4 = weight(content:\"my phrase\"~2147483647 in 1029), product of:\n\n0.07163286 = queryWeight(content:\"my phrase\"~2147483647), product of:\n          o 9.657982 = idf(content: my=13470 phrase=534)\n          o 0.0074169594 = queryNorm\n\nThis patch adds the query.phrase.slop configuration value to the nutch-default.xml file.  It has a default setting of 5.",
        "Issue Links": []
    },
    "NUTCH-878": {
        "Key": "NUTCH-878",
        "Summary": "ScoringFilters should not override the injected score",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "injector",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "10/Aug/10 08:27",
        "Updated": "22/May/13 03:53",
        "Resolved": "10/Aug/10 15:24",
        "Description": "the method injectedScore in OPICScoringFilter (and also LinkAnalysisScoringFilter) should be modified in order not to override the score specified in the seed list metadata",
        "Issue Links": []
    },
    "NUTCH-879": {
        "Key": "NUTCH-879",
        "Summary": "URL-s getting lost",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "10/Aug/10 20:23",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "I ran the Benchmark using branch-1.3 and trunk (formerly nutchbase). With the same Benchmark parameters and the same plugins branch-1.3 collects ~1.5mln urls, while trunk collects ~20,000 urls. Clearly something is wrong.",
        "Issue Links": []
    },
    "NUTCH-880": {
        "Key": "NUTCH-880",
        "Summary": "REST API for Nutch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "11/Aug/10 09:49",
        "Updated": "22/May/13 03:53",
        "Resolved": "28/Oct/10 09:24",
        "Description": "This issue is for discussing a REST-style API for accessing Nutch.\nHere's an initial idea:\n\nI propose to use org.restlet for handling requests and returning JSON/XML/whatever responses.\nhook up all regular tools so that they can be driven via this API. This would have to be an async API, since all Nutch operations take long time to execute. It follows then that we need to be able also to list running operations, retrieve their current status, and possibly abort/cancel/stop/suspend/resume/...? This also means that we would have to potentially create & manage many threads in a servlet - AFAIK this is frowned upon by J2EE purists...\npackage this in a webapp (that includes all deps, essentially nutch.job content), with the restlet servlet as an entry point.\n\nOpen issues:\n\nhow to implement the reading of crawl results via this API\nshould we manage only crawls that use a single configuration per webapp, or should we have a notion of crawl contexts (sets of crawl configs) with CRUD ops on them? this would be nice, because it would allow managing of several different crawls, with different configs, in a single webapp - but it complicates the implementation a lot.",
        "Issue Links": [
            "/jira/browse/NUTCH-929"
        ]
    },
    "NUTCH-881": {
        "Key": "NUTCH-881",
        "Summary": "Good quality documentation for Nutch",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "1.10",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Andrzej Bialecki",
        "Created": "11/Aug/10 10:05",
        "Updated": "09/Jan/15 06:07",
        "Resolved": "09/Jan/15 06:07",
        "Description": "This is, and has been, a long standing request from Nutch users. This becomes an acute need as we redesign Nutch 2.0, because the collective knowledge and the Wiki will no longer be useful without massive amount of editing.\nIMHO the reference documentation should be in SVN, and not on the Wiki - the Wiki is good for casual information and recipes but I think it's too messy and not reliable enough as a reference.\nI propose to start with the following:\n 1. let's decide on the format of the docs. Each format has its own pros and cons:\n\nHTML: easy to work with, but formatting may be messy unless we edit it by hand, at which point it's no longer so easy... Good toolchains to convert to other formats, but limited expressiveness of larger structures (e.g. book, chapters, TOC, multi-column layouts, etc).\nDocbook: learning curve is higher, but not insurmountable... Naturally yields very good structure. Figures/diagrams may be problematic - different renderers (html, pdf) like to treat the scaling and placing somewhat differently.\nWiki-style (Confluence or TWiki): easy to use, but limited control over larger structures. Maven Doxia can format cwiki, twiki, and a host of other formats to e.g. html and pdf.\nother?\n\n 2. start documenting the main tools and the main APIs (e.g. the plugins and all the extension points). We can of course reuse material from the Wiki and from various presentations (e.g. the ApacheCon slides).",
        "Issue Links": []
    },
    "NUTCH-882": {
        "Key": "NUTCH-882",
        "Summary": "Design a Host table in GORA",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "11/Aug/10 11:26",
        "Updated": "27/Apr/12 05:37",
        "Resolved": "26/Apr/12 09:19",
        "Description": "Having a separate GORA table for storing information about hosts (and domains?) would be very useful for : \n\ncustomising the behaviour of the fetching on a host basis e.g. number of threads, min time between threads etc...\nstoring stats\nkeeping metadata and possibly propagate them to the webpages\nkeeping a copy of the robots.txt and possibly use that later to filter the webtable\nstore sitemaps files and update the webtable accordingly\n\nI'll try to come up with a GORA schema for such a host table but any comments are of course already welcome",
        "Issue Links": [
            "/jira/browse/GORA-105",
            "/jira/browse/NUTCH-628",
            "/jira/browse/NUTCH-1290"
        ]
    },
    "NUTCH-883": {
        "Key": "NUTCH-883",
        "Summary": "Remove unused parameters from nutch-default.xml",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "11/Aug/10 12:22",
        "Updated": "12/Aug/10 18:24",
        "Resolved": "12/Aug/10 18:24",
        "Description": "The parameter file nutch-default.xml contains entries which are not used in 2.0 (e.g. configuration of the indexing with Lucene). I will submit a patch for this",
        "Issue Links": []
    },
    "NUTCH-884": {
        "Key": "NUTCH-884",
        "Summary": "FetcherJob should run more reduce tasks than default",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "11/Aug/10 13:25",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/Aug/10 12:05",
        "Description": "FetcherJob now performs fetching in the reduce phase. This means that in a typical Hadoop setup there will be many fewer reduce tasks than map tasks, and consequently the max. total throughput of Fetcher will be proportionally reduced. I propose that FetcherJob should set the number of reduce tasks to the number of map tasks. This way the fetching will be more granular.",
        "Issue Links": []
    },
    "NUTCH-885": {
        "Key": "NUTCH-885",
        "Summary": "Error JAVA_HOME is Not Set",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "nutchbase",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "venkata hanuman choudari",
        "Created": "12/Aug/10 06:32",
        "Updated": "12/Aug/10 07:49",
        "Resolved": "12/Aug/10 07:49",
        "Description": "After the Following the steps for Crawl the db in local system and run the ./start-all.sh i am getting following error\n./start-all.sh \nstarting namenode, logging to /var/www/nutch-0.9/bin/../logs/hadoop-chow-namenode-chow-desktop.out\nchow@localhost's password: \nlocalhost: starting datanode, logging to /var/www/nutch-0.9/bin/../logs/hadoop-chow-datanode-chow-desktop.out\nlocalhost: Error: JAVA_HOME is not set.\ncat: /var/www/nutch-0.9/bin/../conf/masters: No such file or directory\nstarting jobtracker, logging to /var/www/nutch-0.9/bin/../logs/hadoop-chow-jobtracker-chow-desktop.out\nchow@localhost's password: \nlocalhost: starting tasktracker, logging to /var/www/nutch-0.9/bin/../logs/hadoop-chow-tasktracker-chow-desktop.out\nlocalhost: Error: JAVA_HOME is not set.",
        "Issue Links": []
    },
    "NUTCH-886": {
        "Key": "NUTCH-886",
        "Summary": "A .gitignore file for Nutch",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "12/Aug/10 07:52",
        "Updated": "12/Aug/10 09:32",
        "Resolved": "12/Aug/10 09:32",
        "Description": "We need a .gitignore file under nutch/ so git does not try to track many unnecessary files.",
        "Issue Links": []
    },
    "NUTCH-887": {
        "Key": "NUTCH-887",
        "Summary": "Delegate parsing of feeds to Tika",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "14/Aug/10 08:30",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "[Starting a new thread from https://issues.apache.org/jira/browse/NUTCH-874]\nOne of the plugins which hasn't been ported yet is the feed parser. We could rely on the one we recently added to Tika, knowing that there is a substantial difference in the sense that the Tika feed parser generates a simple XHTML representation of the document where the feeds are simply represented as anchors whereas the Nutch version created new documents for each feed.\nThere is also the parse-rss plugin in Nutch which is quite similar - what's the difference with the feed one again? Since the Tika parser would handle all sorts of feed formats why not simply rely on it? \nAny thoughts on this?",
        "Issue Links": []
    },
    "NUTCH-888": {
        "Key": "NUTCH-888",
        "Summary": "Remove parse-rss",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "16/Aug/10 08:47",
        "Updated": "29/Jun/11 04:01",
        "Resolved": "04/May/11 20:17",
        "Description": "See https://issues.apache.org/jira/browse/NUTCH-887\n\nCM : I wrote parse-rss back in 2005, and used commons-feedparser from Kevin Burton and his crew. At the time it was well developed, and a little more flexible and easier for me to pick up than Rome. Since then however, its development has really become stagnant and it is no longer maintained.\nIn terms of real differences in terms of functionality, they are roughly equivalent so there isn't much difference.\nAlready +1 from Andrzej and Chris. Will remove it tomorrow if there aren't any objections in the meantime",
        "Issue Links": [
            "/jira/browse/NUTCH-934",
            "/jira/browse/NUTCH-967"
        ]
    },
    "NUTCH-889": {
        "Key": "NUTCH-889",
        "Summary": "remove gora jars from lib dir",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "17/Aug/10 17:03",
        "Updated": "18/Aug/10 09:05",
        "Resolved": "18/Aug/10 09:05",
        "Description": "Gora does not yet have any published jars we could retrieve with Ivy, instead we currently need to install it and build it locally so that Nutch Ivy can resolve it.\nThe lib dir contains some gora jars which should be removed as they are not necessarily up to date.\nNote that depending on the Gora backend used for Nutch you might need to add some jars in the lib dir e.g. specific SQL drivers\nI will remove these jars to prevent any confusion",
        "Issue Links": []
    },
    "NUTCH-890": {
        "Key": "NUTCH-890",
        "Summary": "SqlStore doesn't work with nested types in Avro schema",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "19/Aug/10 10:40",
        "Updated": "22/May/13 03:53",
        "Resolved": "24/Aug/10 12:43",
        "Description": "ParseStatus and ProtocolStatus are not properly serialized and stored when using SqlStore. This may indicate a broader issue in Gora with processing of nested types in Avro schemas.\nHBaseStore works properly, i.e. both types can be correctly stored and retrieved. SqlStore produces either NULL or '\\0\\0' value. This happens both when using HSQLDB and MySQL.",
        "Issue Links": []
    },
    "NUTCH-891": {
        "Key": "NUTCH-891",
        "Summary": "Nutch build should not depend on unversioned local deps",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "19/Aug/10 10:54",
        "Updated": "22/May/13 03:53",
        "Resolved": "29/Apr/13 19:07",
        "Description": "The fix in NUTCH-873 introduces an unknown variable to the build process. Since local ivy artifacts are unversioned, different people that install Gora jars at different points in time will use the same artifact id but in fact the artifacts (jars) will differ because they will come from different revisions of Gora sources. Therefore Nutch builds based on the same svn rev. won't be repeatable across different environments.\nAs much as it pains the ivy purists  until Gora publishes versioned artifacts I'd like to revert the fix in NUTCH-873 and add again Gora jars built from a known external rev. We can add a README that contains commit id from Gora.",
        "Issue Links": []
    },
    "NUTCH-892": {
        "Key": "NUTCH-892",
        "Summary": "nutch maven build support",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Marius Cristian Vulpe",
        "Created": "19/Aug/10 17:16",
        "Updated": "22/May/11 10:24",
        "Resolved": "19/Aug/10 20:28",
        "Description": "I use nutch search mechanism form a standalone java application. I use maven to configure my dependencies and I have seen that nutch doesn't publish any artifacts to the public repositories.\nPlease let me know if somebody is working towards this direction.\nIf not, I think I can spent some time to \"mavenize\" the project and I can send you a version of that (I plan to do that for version 1.1).\nI would need feedback on this.",
        "Issue Links": []
    },
    "NUTCH-893": {
        "Key": "NUTCH-893",
        "Summary": "DataStore.put() silently loses records when executed from multiple processes",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Invalid",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "25/Aug/10 21:59",
        "Updated": "13/Sep/10 14:18",
        "Resolved": "13/Sep/10 14:18",
        "Description": "In order to debug the issue described in NUTCH-879 I created a test to simulate multiple clients appending to webtable (please see the patch), which is the situation that we have in distributed map-reduce jobs.\nThere are two tests there: one that uses multiple threads within the same JVM, and another that uses single thread in multiple JVMs. Each test first clears webtable (be careful!), and then puts a bunch of pages, and finally counts that all are present and their values correspond to keys. To make things more interesting each execution context (thread or process) closes and reopens its instance of DataStore a few times.\nThe multithreaded test passes just fine. However, the multi-process test fails with missing keys, as many as 30%.",
        "Issue Links": []
    },
    "NUTCH-894": {
        "Key": "NUTCH-894",
        "Summary": "Move statistical language identification from indexing to parsing step",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "parser",
        "Assignee": "Dogacan Guney",
        "Reporter": "Julien Nioche",
        "Created": "27/Aug/10 08:46",
        "Updated": "01/Oct/10 18:30",
        "Resolved": "01/Oct/10 18:30",
        "Description": "The statistical identification of language is currently done part in the indexing step, whereas the detection based on HTTP header and HTML code is done during the parsing.\nWe could keep the same logic i.e. do the statistical detection only if nothing has been found with the previous methods but as part of the parsing. This would be useful for ParseFilters which need the language information or to use with ScoringFilters e.g. to focus the crawl on a set of languages.\nSince the statistical models have been ported to Tika we should probably rely on them instead of maintaining our own.\nAny thoughts on this?",
        "Issue Links": []
    },
    "NUTCH-895": {
        "Key": "NUTCH-895",
        "Summary": "Urls with characters like [? = ] getting filtered out.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jitendra",
        "Created": "31/Aug/10 06:03",
        "Updated": "01/Sep/10 08:09",
        "Resolved": "01/Sep/10 08:09",
        "Description": "Hi,\nI am trying to write XpathBasedLinkExtractor which extracts links out of html page using xpaths.\nBut all the extracted links which contains characters like [? , = ] are being filtered out. I am not able to nail it down where it is happening. They are not going into segments.\nI have also commented out regular expression -[?*!@=] in regex-urlfilter.txt. Still It is showing same behaviour.\nCan any one give me idea about this. Where am I going wrong. I am stuck at this for last day.\nThanks\nJitendra",
        "Issue Links": []
    },
    "NUTCH-896": {
        "Key": "NUTCH-896",
        "Summary": "Gora-based tests need to have their own config files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "31/Aug/10 11:26",
        "Updated": "03/May/12 15:30",
        "Resolved": "03/May/12 15:30",
        "Description": "The tests extending AbstractNutchTest (Injector, Generator, Fetcher) have hard-coded properties for GORA. It would be better to be able to rely on a file gora.properties used only for the tests, just as we do with the nutch-*.xml config files (see CrawlTestUtil). This way we wouldn't use the configs set in the main /conf file as they could be specific to a given GORA backend e.g. Mysql vs hsqldb. This would also help running the tests with a non-default GORA backend. \nWe need to modify GORA and make the method DataStoreFactory.setProperties public.",
        "Issue Links": [
            "/jira/browse/NUTCH-1205",
            "/jira/browse/NUTCH-1135",
            "/jira/browse/NUTCH-1081"
        ]
    },
    "NUTCH-897": {
        "Key": "NUTCH-897",
        "Summary": "Subcollection requires blacklist element",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Sep/10 11:35",
        "Updated": "08/May/11 22:34",
        "Resolved": "12/Apr/11 12:51",
        "Description": "This is a very minor issue with in Subcollection.java. It throws an error if the (empty) blacklist element was omitted. I think it should either not silently fail in case of an omitted blacklist element or throw a decent error message that the blacklist element is required. The following exception gets thrown if the blacklist element is omitted in a subcollection block:\n2010-09-06 13:32:30,438 INFO  collection.CollectionManager - Instantiating CollectionManager                                                            \n2010-09-06 13:32:30,438 INFO  collection.CollectionManager - initializing CollectionManager                                                             \n2010-09-06 13:32:30,451 INFO  collection.CollectionManager - file has1 elements                                                                         \n2010-09-06 13:32:30,456 WARN  collection.CollectionManager - Error occured:java.lang.NullPointerException                                               \n2010-09-06 13:32:30,469 WARN  collection.CollectionManager - java.lang.NullPointerException                                                             \n2010-09-06 13:32:30,470 WARN  collection.CollectionManager - at org.apache.nutch.collection.Subcollection.initialize(Subcollection.java:173)            \n2010-09-06 13:32:30,470 WARN  collection.CollectionManager - at org.apache.nutch.collection.CollectionManager.parse(CollectionManager.java:98)          \n2010-09-06 13:32:30,470 WARN  collection.CollectionManager - at org.apache.nutch.collection.CollectionManager.init(CollectionManager.java:75)           \n2010-09-06 13:32:30,470 WARN  collection.CollectionManager - at org.apache.nutch.collection.CollectionManager.<init>(CollectionManager.java:56)         \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.collection.CollectionManager.getCollectionManager(CollectionManager.java:115)                                                                                                                                                  \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.addSubCollectionField(SubcollectionIndexingFilter.java:65)                                                                                                                   \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.indexer.subcollection.SubcollectionIndexingFilter.filter(SubcollectionIndexingFilter.java:71)                                                                                                                                  \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:109)               \n2010-09-06 13:32:30,471 WARN  collection.CollectionManager - at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:134)             \n2010-09-06 13:32:30,472 WARN  collection.CollectionManager - at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:50)              \n2010-09-06 13:32:30,472 WARN  collection.CollectionManager - at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)                  \n2010-09-06 13:32:30,472 WARN  collection.CollectionManager - at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)                            \n2010-09-06 13:32:30,472 WARN  collection.CollectionManager - at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)",
        "Issue Links": []
    },
    "NUTCH-898": {
        "Key": "NUTCH-898",
        "Summary": "Multi valued subcollection is not multi valued",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.2",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "06/Sep/10 16:43",
        "Updated": "07/Sep/10 11:15",
        "Resolved": "07/Sep/10 11:15",
        "Description": "NUTCH-716 concatenates multiple values in a single string instead of adding single values to a multi valued field. For a test crawl i have defined the following two subcollection definitions:\n<subcollection>\n<name>asdf</name>\n<id>asdf-site</id>\n<whitelist>http://asdf/</whitelist>\n<blacklist/>\n</subcollection>\n<subcollection>\n<name>news</name>\n<id>asdf-news</id>\n<whitelist>http://asdf/news/</whitelist>\n<blacklist/>\n</subcollection>\nReindexing the segments by sending them to Solr will yield the following results for a news URL:\n<doc>\n<arr name=\"subcollection\">\n<str>asdf</str>\n</arr>\n<str name=\"url\">http://asdf/home/</str>\n</doc>\n<doc>\n<arr name=\"subcollection\">\n<str>asdf news</str>\n</arr>\n<str name=\"url\">http://asdf/news/</str>\n</doc>\nInstead, i expected the following result for the second document:\n<doc>\n<arr name=\"subcollection\">\n<str>asdf</str>\n<str>news</str>\n</arr>\n<str name=\"url\">http://asdf/news/</str>\n</doc>\nMy Solr schema.xml has the following declaration for the subcollection field:\n<field name=\"subcollection\" type=\"string\" stored=\"true\" indexed=\"true\" multiValued=\"true\" />",
        "Issue Links": [
            "/jira/browse/NUTCH-716"
        ]
    },
    "NUTCH-899": {
        "Key": "NUTCH-899",
        "Summary": "java.sql.BatchUpdateException: Data truncation: Data too long for column 'content' at row 1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "Faruk Berks\u00f6z",
        "Created": "07/Sep/10 13:40",
        "Updated": "18/Dec/10 19:30",
        "Resolved": "07/Sep/10 15:31",
        "Description": "wenn i try to fetch a web page (e.g. http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html ) with mysql storage definition,\nI am seeing the following error in my hadoop logs. ,  (no error with hbase ) ;\njava.io.IOException: java.sql.BatchUpdateException: Data truncation: Data too long for column 'content' at row 1\n    at org.gora.sql.store.SqlStore.flush(SqlStore.java:316)\n    at org.gora.sql.store.SqlStore.close(SqlStore.java:163)\n    at org.gora.mapreduce.GoraOutputFormat$1.close(GoraOutputFormat.java:72)\n    at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:567)\n    at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408)\n    at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\nThe type of the column 'content' is BLOB.\nIt may be important for the next developments of Gora.",
        "Issue Links": []
    },
    "NUTCH-900": {
        "Key": "NUTCH-900",
        "Summary": "Confusion in nutch-default between http.content.limit and file.content.limit",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            nutchgora",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Markus Jelsma",
        "Created": "08/Sep/10 10:06",
        "Updated": "27/Oct/10 10:39",
        "Resolved": "08/Sep/10 11:05",
        "Description": "The http.content.limit and file.content.limit settings can be confusing and have fooled at least several users. The description element for these settings should be changed to reflect the difference between them so users won't be fooled that easy.\nSee also: http://lucene.472066.n3.nabble.com/ERROR-tika-TikaParser-org-apache-pdfbox-io-PushBackInputStream-td964353.html for a discussion.",
        "Issue Links": []
    },
    "NUTCH-901": {
        "Key": "NUTCH-901",
        "Summary": "Make index-more plug-in configurable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            nutchgora",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Markus Jelsma",
        "Created": "08/Sep/10 10:42",
        "Updated": "04/Jan/11 20:32",
        "Resolved": "21/Sep/10 04:06",
        "Description": "In my case, i don't want the index-more plug-in to split content-types on slash. Tokenization is something a Solr instance should take care of. Instead of removing the code (which would break compatibility for users that rely on it), we need a way to configure the plug-in not to split the content-type.",
        "Issue Links": []
    },
    "NUTCH-902": {
        "Key": "NUTCH-902",
        "Summary": "Add all necessary files and configuration so that nutch can be used with different backends out-of-the-box",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchbase",
        "Fix Version/s": "nutchgora",
        "Component/s": "documentation,                                            storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Enis Soztutar",
        "Created": "08/Sep/10 13:38",
        "Updated": "05/May/12 00:59",
        "Resolved": "05/May/12 00:59",
        "Description": "As per the discussion in the mailing list and http://wiki.apache.org/nutch/GORA_HBase, it will be good to include all the necessary files and configuration. I propose that we maintain configuration for at least SQL, HBase and Cassandra. \nThe following changes are needed:\nconf/gora-sql-mapping.xml\nconf/gora-hbase-mapping.xml\nconf/gora-cassandra-mapping.xml\ncomments on nutch-default and ivy.xml \nShall we also include jars from gora-hbase, gora-cassandra and their dependencies ?",
        "Issue Links": [
            "/jira/browse/NUTCH-1205",
            "/jira/browse/NUTCH-1175"
        ]
    },
    "NUTCH-903": {
        "Key": "NUTCH-903",
        "Summary": "RESUME_KEY field in FetcherJob.Java has not been get correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Faruk Berks\u00f6z",
        "Created": "08/Sep/10 14:45",
        "Updated": "08/Sep/10 15:01",
        "Resolved": "08/Sep/10 15:01",
        "Description": "Source modification request for nutch 2.0 .\nFetcherJob.Java\n\t...\n\tFetcherMapper\n\t....\n\t\tprotected void setup(Context context) \n{\n\t\t\t  Configuration conf = context.getConfiguration();\n\t\t\t  shouldContinue = conf.getBoolean(\"job.continue\", false);\n\t\t>>>\t  \n\t\t>>>  job.continue has not beeen set anywhere\n\t\t>>> \"job.continue\" should be RESUME_KEY which is set before for this purpose\n\t\t>>>\t\t\t  \n\t\t\t  crawlId = new Utf8(conf.get(GeneratorJob.CRAWL_ID, Nutch.ALL_CRAWL_ID_STR));\n\t\t\t}\n\t...",
        "Issue Links": []
    },
    "NUTCH-904": {
        "Key": "NUTCH-904",
        "Summary": "\"-resume\" option is always processed  as \"false\" in FetcherJob.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Faruk Berks\u00f6z",
        "Created": "08/Sep/10 15:52",
        "Updated": "14/Sep/10 13:45",
        "Resolved": "14/Sep/10 13:45",
        "Description": "job.continue has not beeen set anywhere.\n\"job.continue\" should be RESUME_KEY which is set before for this purpose.\n\n\nFetcherJob.java\n   ...\n   FetcherMapper\n   ....\n      protected void setup(Context context) {\n         Configuration conf = context.getConfiguration();\n         shouldContinue = conf.getBoolean(\"job.continue\", false);       \n         crawlId = new Utf8(conf.get(GeneratorJob.CRAWL_ID, Nutch.ALL_CRAWL_ID_STR));\n      }\n\t...",
        "Issue Links": []
    },
    "NUTCH-905": {
        "Key": "NUTCH-905",
        "Summary": "Configurable file protocol parent directory crawling",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0,                                            1.1",
        "Fix Version/s": "1.2,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "11/Sep/10 01:57",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Sep/10 03:50",
        "Description": "See the discussion on NUTCH-407: apply the patch and backport to 1.2 and port to 2.0.",
        "Issue Links": [
            "/jira/browse/NUTCH-407"
        ]
    },
    "NUTCH-906": {
        "Key": "NUTCH-906",
        "Summary": "Nutch OpenSearch sometimes raises DOMExceptions due to Lucene column names not being valid XML tag names",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "1.2",
        "Component/s": "web gui",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Asheesh Laroia",
        "Created": "13/Sep/10 19:13",
        "Updated": "22/May/13 03:53",
        "Resolved": "17/Sep/10 19:06",
        "Description": "The Nutch FAQ explains that OpenSearch includes \"all fields that are available at search result time.\" However, some Lucene column names can start with numbers. Valid XML tags cannot. If Nutch is generating OpenSearch results for a document with a Lucene document column whose name starts with numbers, the underlying Xerces library throws this exception: \norg.w3c.dom.DOMException: INVALID_CHARACTER_ERR: An invalid or illegal XML character is specified. \nSo I have written a patch that tests strings before they are used to generate tags within OpenSearch.\nI hope you merge this, or a better version of the patch!",
        "Issue Links": []
    },
    "NUTCH-907": {
        "Key": "NUTCH-907",
        "Summary": "DataStore API doesn't support multiple storage areas for multiple disjoint crawls",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "15/Sep/10 15:00",
        "Updated": "22/May/13 03:53",
        "Resolved": "21/Oct/10 12:02",
        "Description": "In Nutch 1.x it was possible to easily select a set of crawl data (crawldb, page data, linkdb, etc) by specifying a path where the data was stored. This enabled users to run several disjoint crawls with different configs, but still using the same storage medium, just under different paths.\nThis is not possible now because there is a 1:1 mapping between a specific DataStore instance and a set of crawl data.\nIn order to support this functionality the Gora API should be extended so that it can create stores (and data tables in the underlying storage) that use arbitrary prefixes to identify the particular crawl dataset. Then the Nutch API should be extended to allow passing this \"crawlId\" value to select one of possibly many existing crawl datasets.",
        "Issue Links": []
    },
    "NUTCH-908": {
        "Key": "NUTCH-908",
        "Summary": "Infinite Loop and Null Pointer Bugs in Searching",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.0.0,                                            1.1",
        "Fix Version/s": "1.2",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Dennis Kubes",
        "Created": "16/Sep/10 20:10",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/Sep/10 04:49",
        "Description": "It is possible for the NutchBean to drop into an infinite loop while trying to optimize a query to re-search for more results.  There are also two Null Pointer bugs in the search process.  One in NutchBean where there was an incorrect loop assignment and a second in DistributedSegementsBean when a segment is null (shouldn't happen but still should be handled.)  A patch is available for both.",
        "Issue Links": []
    },
    "NUTCH-909": {
        "Key": "NUTCH-909",
        "Summary": "Add alternative search-provider to Nutch site",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "documentation",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Alex Baranau",
        "Created": "20/Sep/10 13:56",
        "Updated": "22/May/13 03:54",
        "Resolved": "27/Sep/10 02:21",
        "Description": "Add additional search provider (to existed Lucid Find) search-lucene.com. \nInitiated in discussion: http://search-lucene.com/m/2suCr1UnDfF1\nAccording to Andrzej's suggestion, \"when preparing the patch let's follow the same rationales as those in TIKA-488, since they are applicable here too\", so please refer to that issue for more insight on implementation details.",
        "Issue Links": []
    },
    "NUTCH-910": {
        "Key": "NUTCH-910",
        "Summary": "Cached.jsp has a bug with encoding",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.0.0",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Attila Pados",
        "Created": "27/Sep/10 15:03",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 20:03",
        "Description": "cached.jsp\nPages that has a non default encoding, or not utf-8 etc, the cached content is displayed screwed. This is quite annoying, but doesn't harm critically functionality.\nadd       :   Metadata parseData = bean.getParseData(details).getParseMeta();\noriginal :  Metadata metaData = bean.getParseData(details).getContentMeta();\nreplace: String encoding = (String) parseData.get(\"CharEncodingForConversion\");\nIn the cached jsp, the encoding variable is tried to retrieved from the wrong metadata source, contentMeta, which doesn't include this value.\nIt resides in the parseMetadata instead. \nFirst line is not a replacement above, it has to be added.  Original metadata is needed there for other things.\nThen below, the encoding value line has to be changed, that is a replacement.\nThis fix is for 1.0 nutch version, i didn't found an issue in the list that would cover this, just a mail found with google, on a mailing list that refered to it.",
        "Issue Links": []
    },
    "NUTCH-911": {
        "Key": "NUTCH-911",
        "Summary": "recrawls file protocol causes Errors/Exceptions when actually not modified or gone",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "fetcher,                                            protocol",
        "Assignee": null,
        "Reporter": "Peter Lundberg",
        "Created": "07/Oct/10 20:39",
        "Updated": "01/May/14 06:23",
        "Resolved": "07/Aug/13 21:11",
        "Description": "When recrawling file systems file are marked as error and logging occurs such as:\njava.net.MalformedURLException\n\tat java.net.URL.<init>(URL.java:601)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.net.URL.<init>(URL.java:413)\n\tat org.apache.nutch.protocol.file.File.getProtocolOutput(File.java:85)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:627)\nfetch of file:/Users/peter.lundberg/Documents/valtech/scan-test/Peter Lundberg 20090929.pdf failed with: java.net.MalformedURLException\nThis is due to FileResponse and File not working well together. The same is true for files that after a while disappear from the file system being crawled (ie error instead of GONE). I am too new with nutch to know the design rational behind this or any sideaffect. Below is a patch that I have used that cleans up the segment data and removevs false errors in the log file.\n\u2014 src/plugin/protocol-file/src/java/org/apache/nutch/protocol/file/File.java\t(revision 997976)\n+++ src/plugin/protocol-file/src/java/org/apache/nutch/protocol/file/File.java\t(working copy)\n@@ -79,6 +79,10 @@\n         if (code == 200) \n{                          // got a good response\n           return new ProtocolOutput(response.toContent());              // return it\n   \n+        }\n else if (code == 404) \n{                   // handle no such file\n+          return new ProtocolOutput(response.toContent(), ProtocolStatus.STATUS_GONE );  \n+        }\n else if (code == 304) \n{                   // handle not modified\n+          return new ProtocolOutput(response.toContent(), ProtocolStatus.STATUS_NOTMODIFIED );  \n         }\n else if (code >= 300 && code < 400) {     // handle redirect\n           if (redirects == MAX_REDIRECTS)\n             throw new FileException(\"Too many redirects: \" + url);",
        "Issue Links": []
    },
    "NUTCH-912": {
        "Key": "NUTCH-912",
        "Summary": "MoreIndexingFilter does not parse docx and xlsx date formats",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Erlend Gar\u00e5sen",
        "Created": "12/Oct/10 10:56",
        "Updated": "08/May/11 22:34",
        "Resolved": "05/Jan/11 16:45",
        "Description": "The following error occurs in hadoop.log when MoreIndexingFilter tries to parse dates from MS Office formats:\n2010-10-08 13:56:32,555 WARN  more.MoreIndexingFilter - http://ridder.uio.no/test1.xlsx: can't parse erroneous date: 2010-10-08T13:55:54Z\nThis problem affects docx and xlsx formats, but probably the other XML-based MS Office formats as well.",
        "Issue Links": []
    },
    "NUTCH-913": {
        "Key": "NUTCH-913",
        "Summary": "Nutch should use new namespace for Gora",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "storage",
        "Assignee": "Dogacan Guney",
        "Reporter": "Dogacan Guney",
        "Created": "13/Oct/10 14:13",
        "Updated": "26/Oct/10 11:18",
        "Resolved": "26/Oct/10 11:18",
        "Description": "Gora is in Apache Incubator now (Yey!). We recently changed Gora's namespace from org.gora to org.apache.gora. This means nutch should use the new namespace otherwise it won't compile with newer builds of Gora.",
        "Issue Links": []
    },
    "NUTCH-914": {
        "Key": "NUTCH-914",
        "Summary": "Implement Apache Project Branding Requirements",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "19/Oct/10 10:49",
        "Updated": "11/Sep/11 07:54",
        "Resolved": "11/Sep/11 07:54",
        "Description": "We should implement the requirements from http://www.apache.org/foundation/marks/pmcs.html",
        "Issue Links": []
    },
    "NUTCH-915": {
        "Key": "NUTCH-914 Implement Apache Project Branding Requirements",
        "Summary": "project website basics",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "19/Oct/10 10:52",
        "Updated": "16/Jul/11 11:54",
        "Resolved": "19/Oct/10 11:31",
        "Description": "http://www.apache.org/foundation/marks/pmcs#websites",
        "Issue Links": []
    },
    "NUTCH-916": {
        "Key": "NUTCH-914 Implement Apache Project Branding Requirements",
        "Summary": "Project Naming And Descriptions",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "19/Oct/10 10:52",
        "Updated": "16/Jul/11 11:53",
        "Resolved": "16/Jul/11 11:52",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-917": {
        "Key": "NUTCH-914 Implement Apache Project Branding Requirements",
        "Summary": "Website Navigation Links",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "19/Oct/10 10:52",
        "Updated": "10/Aug/11 12:08",
        "Resolved": "10/Aug/11 12:08",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-918": {
        "Key": "NUTCH-914 Implement Apache Project Branding Requirements",
        "Summary": "Trademark Attributions",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "19/Oct/10 10:52",
        "Updated": "16/Jul/11 11:55",
        "Resolved": "16/Jul/11 11:55",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-919": {
        "Key": "NUTCH-914 Implement Apache Project Branding Requirements",
        "Summary": "Logos and Graphics",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "19/Oct/10 10:52",
        "Updated": "28/Jul/11 18:20",
        "Resolved": "28/Jul/11 18:20",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-920": {
        "Key": "NUTCH-914 Implement Apache Project Branding Requirements",
        "Summary": "Project Metadata",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "19/Oct/10 10:53",
        "Updated": "11/Aug/11 04:01",
        "Resolved": "28/Jul/11 18:22",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-921": {
        "Key": "NUTCH-921",
        "Summary": "Reduce dependency of Nutch on config files",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "19/Oct/10 16:51",
        "Updated": "22/May/13 03:53",
        "Resolved": "21/Oct/10 11:44",
        "Description": "Currently many components in Nutch rely on reading their configuration from files. These files need to be on the classpath (or packed into a job jar). This is inconvenient if you want to manage configuration via API, e.g. when embedding Nutch, or running many jobs with slightly different configurations.\nThis issue tracks the improvement to make various components read their config directly from Configuration properties.",
        "Issue Links": [
            "/jira/browse/NUTCH-431"
        ]
    },
    "NUTCH-922": {
        "Key": "NUTCH-922",
        "Summary": "SolrWriter should log source fields that are not mapped",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "20/Oct/10 14:29",
        "Updated": "14/Apr/11 10:43",
        "Resolved": "14/Apr/11 10:43",
        "Description": "Currently the SolrWriter::write() method silently ignores source fields that have no mapping to a Solr field. Fields that are ignored should be logged. Any more thoughts on this one?",
        "Issue Links": []
    },
    "NUTCH-923": {
        "Key": "NUTCH-923",
        "Summary": "Multilingual support for Solr-index-mapping",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Matthias Agethle",
        "Created": "20/Oct/10 16:04",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "It would be useful to extend the mapping-possibilites when indexing to solr.\nOne useful feature would be to use the detected language of the html page (for example via the language-identifier plugin) and send the content to corresponding language-aware solr-fields.\nThe mapping file could be as follows:\n<field dest=\"lang\" source=\"lang\"/>\n<field dest=\"title_${lang}\" source=\"title\" />\nso that the title-field gets mapped to title_en for English-pages and tilte_fr for French pages.\nWhat do you think? Could this be useful also to others?\nOr are there already other solutions out there?",
        "Issue Links": []
    },
    "NUTCH-924": {
        "Key": "NUTCH-924",
        "Summary": "Static field in solr mapping",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "David Stuart",
        "Created": "22/Oct/10 10:26",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Sep/11 12:57",
        "Description": "Provide the facility to pass static data defined in solrindex-mapping.xml to solr during the mapping process.",
        "Issue Links": [
            "/jira/browse/NUTCH-940"
        ]
    },
    "NUTCH-925": {
        "Key": "NUTCH-925",
        "Summary": "plugins stored in weakhashmap lead memory leak",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.2",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "congliu",
        "Created": "23/Oct/10 09:35",
        "Updated": "20/Dec/11 13:25",
        "Resolved": "07/Jul/11 13:36",
        "Description": "I suffer serious memory leak using Nutch 1.2 though a very deep crawl. I get the error like this:\nException in thread \"Thread-113544\" java.lang.OutOfMemoryError: PermGen space\n\tat java.lang.Throwable.getStackTraceElement(Native Method)\n\tat java.lang.Throwable.getOurStackTrace(Throwable.java:591)\n\tat java.lang.Throwable.printStackTrace(Throwable.java:510)\n\tat org.apache.log4j.spi.ThrowableInformation.getThrowableStrRep(ThrowableInformation.java:76)\n\tat org.apache.log4j.spi.LoggingEvent.getThrowableStrRep(LoggingEvent.java:407)\n\tat org.apache.log4j.WriterAppender.subAppend(WriterAppender.java:305)\n\tat org.apache.log4j.DailyRollingFileAppender.subAppend(DailyRollingFileAppender.java:359)\n\tat org.apache.log4j.WriterAppender.append(WriterAppender.java:160)\n\tat org.apache.log4j.AppenderSkeleton.doAppend(AppenderSkeleton.java:251)\n\tat org.apache.log4j.helpers.AppenderAttachableImpl.appendLoopOnAppenders(AppenderAttachableImpl.java:66)\n\tat org.apache.log4j.Category.callAppenders(Category.java:206)\n\tat org.apache.log4j.Category.forcedLog(Category.java:391)\n\tat org.apache.log4j.Category.log(Category.java:856)\n\tat org.slf4j.impl.Log4jLoggerAdapter.log(Log4jLoggerAdapter.java:509)\n\tat org.apache.commons.logging.impl.SLF4JLocationAwareLog.warn(SLF4JLocationAwareLog.java:173)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:256)\nException in thread \"main\" java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1252)\n\tat org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:1107)\n\tat org.apache.nutch.crawl.Crawl.main(Crawl.java:133)\nI guess Plugin repository cache lead to memory leak.\nAs u know plugins is stored in weakhashmap <conf, plugins>, and new class classload\ncreate when u need plugins.\nUsually,WeakHashMap object can been gc, but class and classload is stored in Perm NOT stack and gc can't perform in Perm, SO (java.lang.OutOfMemoryError: PermGen space) occured..., is any nutch-issues have concerned this promble? or there is any solution? \nnutch-356 may help?",
        "Issue Links": [
            "/jira/browse/NUTCH-844"
        ]
    },
    "NUTCH-926": {
        "Key": "NUTCH-926",
        "Summary": "Redirections from META tag don't get filtered",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2",
        "Fix Version/s": "1.9",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Marco Novo",
        "Created": "27/Oct/10 14:45",
        "Updated": "15/Jul/14 11:51",
        "Resolved": "15/Jul/14 11:32",
        "Description": "We have nutch set to crawl a domain urllist and we want to fetch only passed domains (hosts) not subdomains.\nSo\nWWW.DOMAIN1.COM\n..\n..\n..\nWWW.RIGHTDOMAIN.COM\n..\n..\n..\n..\nWWW.DOMAIN.COM\nWe sets nutch to:\nNOT FOLLOW EXERNAL LINKS\nDuring crawling of WWW.RIGHTDOMAIN.COM\nif a page contains\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n<html>\n<head>\n<title></title>\n    <META http-equiv=\"refresh\" content=\"0;\n    url=http://WRONG.RIGHTDOMAIN.COM\">\n</head>\n<body>\n</body>\n</html>\nNutch continues to crawl the WRONG subdomains! But it should not do this!!\nDuring crawling of WWW.RIGHTDOMAIN.COM\nif a page contains\n<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 4.01 Transitional//EN\">\n<html>\n<head>\n<title></title>\n    <META http-equiv=\"refresh\" content=\"0;\n    url=http://WWW.WRONGDOMAIN.COM\">\n</head>\n<body>\n</body>\n</html>\nNutch continues to crawl the WRONG domain! But it should not do this! If that we will spider all the web....\nWe think the problem is in org.apache.nutch.parse ParseOutputFormat. We have done a patch so we will attach it",
        "Issue Links": []
    },
    "NUTCH-927": {
        "Key": "NUTCH-927",
        "Summary": "Sub pages are not getting crawled",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Rameez Raja",
        "Created": "28/Oct/10 03:58",
        "Updated": "28/Oct/10 08:10",
        "Resolved": "28/Oct/10 08:10",
        "Description": "In my program the objective is to crawl all the pages and fetch the contents from it. The category wise fetching the information is done perfectly but the sub pages are not getting crawled. In the sense, the nextpages are in the form of links at the bottom of the webpage as shown below - \n<a href=\"http://reviews.logitech.com/7061/224/reviews.htm?page=2\" title=\"Next Page >\" name=\"BV_TrackingTag_Review_Display_NextPage\">More Reviews for Z-5500 Digital 5.1 Speaker System</a>.\nI am using the below script to crawl the site.\n$NUTCH_HOME/search/scripts/crawl.sh testcrawlreviews 5 & > crawl.log\nwhere 5 is the depth\nShown below is the snapshot\ncd $NUTCH_HOME\nbin/nutch inject $BASEDIR/crawldb urls\nbin/nutch generate $BASEDIR/crawldb $BASEDIR/segments\nSEGMENT=`ls $BASEDIR/segments/ | tail -1`\necho processing segment $SEGMENT\nbin/nutch fetch $BASEDIR/segments/$SEGMENT -threads 10\nbin/nutch updatedb $BASEDIR/crawldb $BASEDIR/segments/$SEGMENT -filter\ndone",
        "Issue Links": []
    },
    "NUTCH-928": {
        "Key": "NUTCH-928",
        "Summary": "Segmentation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Rameez Raja",
        "Created": "28/Oct/10 05:06",
        "Updated": "28/Oct/10 08:09",
        "Resolved": "28/Oct/10 08:09",
        "Description": "Is there any configuration needed to create segments for each URL rather than for each depth?\nCan anyone suggest a way to do?",
        "Issue Links": []
    },
    "NUTCH-929": {
        "Key": "NUTCH-929",
        "Summary": "Create a REST-based admin UI for Nutch",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.3",
        "Component/s": "administration gui",
        "Assignee": "Fjodor Vershinin",
        "Reporter": "Andrzej Bialecki",
        "Created": "28/Oct/10 09:17",
        "Updated": "23/Sep/14 00:05",
        "Resolved": "23/Sep/14 00:05",
        "Description": "This is a follow up to NUTCH-880 - we need to expose the functionality of REST API in a user-friendly admin UI. Thanks to the nature of the API the UI can be implemented in any UI framework that speaks REST/JSON, so it could be a simple webapp (we already have jetty) or a Swing / Pivot / etc standalone application.",
        "Issue Links": [
            "/jira/browse/NUTCH-880",
            "/jira/browse/NUTCH-841",
            "/jira/browse/NUTCH-841"
        ]
    },
    "NUTCH-930": {
        "Key": "NUTCH-930",
        "Summary": "Remove remaining dependencies on Lucene API",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "28/Oct/10 20:39",
        "Updated": "22/May/13 03:53",
        "Resolved": "28/Oct/10 20:57",
        "Description": "Nutch doesn't use Lucene API anymore, all indexing happens via Lucene-agnostic SolrJ API. The only place where we still use a minor part of Lucene is in index-basic, and that use (DateTools) can be easily replaced.",
        "Issue Links": []
    },
    "NUTCH-931": {
        "Key": "NUTCH-931",
        "Summary": "Simple admin API to fetch status and stop the service",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "REST_api",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "28/Oct/10 21:03",
        "Updated": "22/May/13 03:53",
        "Resolved": "29/Oct/10 13:48",
        "Description": "REST API needs a simple info / stats service and the ability to shutdown the server.",
        "Issue Links": []
    },
    "NUTCH-932": {
        "Key": "NUTCH-932",
        "Summary": "Bulk REST API to retrieve crawl results as JSON",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "REST_api",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "29/Oct/10 17:00",
        "Updated": "22/May/13 03:53",
        "Resolved": "25/Nov/10 12:18",
        "Description": "It would be useful to be able to retrieve results of a crawl as JSON. There are a few things that need to be discussed:\n\nhow to return bulk results using Restlet (WritableRepresentation subclass?)\n\n\nwhat should be the format of results?\n\nI think it would make sense to provide a single record retrieval (by primary key), all records, and records within a range. This incidentally matches well the capabilities of the Gora Query class",
        "Issue Links": []
    },
    "NUTCH-933": {
        "Key": "NUTCH-933",
        "Summary": "Fetcher does not save a pages Last-Modified value in CrawlDatum",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Joe Kemp",
        "Created": "31/Oct/10 03:14",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "07/Jul/14 13:03",
        "Description": "I added the following code in the output method just after the If (content !=null) statement.\n        String lastModified = metadata.get(\"Last-Modified\");\n        if (lastModified !=null && !lastModified.equals(\"\")) {\n        \ttry \n{\n\t\t\t\tDate lastModifiedDate = DateUtil.parseDate(lastModified);\n\t\t\t\tdatum.setModifiedTime(lastModifiedDate.getTime());\n\t\t\t}\n catch (DateParseException e) {\n\t\t\t}\n        }\nI now get 304 for pages that haven't changed when I recrawl.  Need to do further testing.  Might also need a configuration parameter to turn off this behavior, allowing pages to be forced to be refreshed.",
        "Issue Links": []
    },
    "NUTCH-934": {
        "Key": "NUTCH-934",
        "Summary": "Upgrade to Tika 0.8",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "15/Nov/10 11:51",
        "Updated": "13/Apr/11 23:48",
        "Resolved": "17/Feb/11 16:08",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-888",
            "/jira/browse/NUTCH-967"
        ]
    },
    "NUTCH-935": {
        "Key": "NUTCH-935",
        "Summary": "remove unnecessary /./ in basic urlnormalizer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Stondet",
        "Created": "17/Nov/10 10:00",
        "Updated": "08/May/11 22:34",
        "Resolved": "05/Jan/11 16:54",
        "Description": "remove unnecessary /./ in basic urlnormalizer, because this is a rather a sign of bad webserver configuration than of a wanted link.",
        "Issue Links": []
    },
    "NUTCH-936": {
        "Key": "NUTCH-936",
        "Summary": "LanguageIdentifier should not set empty lang field on NutchDocument",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Nov/10 17:55",
        "Updated": "22/Dec/10 16:59",
        "Resolved": "22/Dec/10 16:59",
        "Description": "For some reason the language identifier plugin sometimes sets an empty value for the lang field. It is confirmed to occur in 1.2 when parsing a scanned PDF file which cannot be OCR'd to proper text, resulting in an empty content field. Anyway, whether it's a problem with the parser or not, the plugin itself should not add an empty value because the content field can always be empty. The plugin already checks for a null value and then sets the lang field to `unknown`, which is fine. But when the lang string is empty, it should also be set to `unknown`.\nThis might break clients that have conditional logic on the empty value, but not on the `unknown` value because it may never have occurred in their set up and therefore they might not have added `unknown` to their logic. However, it might seem a little bit overkill to put this proposal behind a configuration option and let Nutch by default continue to behave as it currently does. Any thoughts on this one?\nHere's the troublesome URL : http://www.nrc.nl/redactie/binnenland/memo_buza_irak.pdf that returns an empty content field and an empty lang string in 1.2 and presumably in trunk and other versions as well.",
        "Issue Links": []
    },
    "NUTCH-937": {
        "Key": "NUTCH-937",
        "Summary": "When nutch is run on hadoop > 0.20.2 (or cdh) it will not find plugins because MapReduce will not unpack plugin/ directory from the job's pack (due to MAPREDUCE-967)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Claudio Martella",
        "Created": "23/Nov/10 15:56",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "28/Sep/11 11:18",
        "Description": "Jobs running in on hadoop 0.21 or cloudera cdh 0.20.2+737 will fail because of missing plugins (i.e.):\n10/10/28 12:22:21 WARN mapred.JobClient: Use GenericOptionsParser for\nparsing the arguments. Applications should implement Tool for the same.\n10/10/28 12:22:22 INFO mapred.FileInputFormat: Total input paths to\nprocess : 1\n10/10/28 12:22:23 INFO mapred.JobClient: Running job: job_201010271826_0002\n10/10/28 12:22:24 INFO mapred.JobClient:  map 0% reduce 0%\n10/10/28 12:22:39 INFO mapred.JobClient: Task Id :\nattempt_201010271826_0002_m_000000_0, Status : FAILED\njava.lang.RuntimeException: Error in configuring object\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at\norg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at\norg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:379)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:217)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)\n    at org.apache.hadoop.mapred.Child.main(Child.java:211)\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 9 more\nCaused by: java.lang.RuntimeException: Error in configuring object\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at\norg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at\norg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)\n    ... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 17 more\nCaused by: java.lang.RuntimeException: x point\norg.apache.nutch.net.URLNormalizer not found.\n    at org.apache.nutch.net.URLNormalizers.<init>(URLNormalizers.java:122)\n    at\norg.apache.nutch.crawl.Injector$InjectMapper.configure(Injector.java:70)\n    ... 22 more\n10/10/28 12:22:40 INFO mapred.JobClient: Task Id :\nattempt_201010271826_0002_m_000001_0, Status : FAILED\njava.lang.RuntimeException: Error in configuring object\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at\norg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at\norg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:379)\n    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:317)\n    at org.apache.hadoop.mapred.Child$4.run(Child.java:217)\n    at java.security.AccessController.doPrivileged(Native Method)\n    at javax.security.auth.Subject.doAs(Subject.java:396)\n    at\norg.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1063)\n    at org.apache.hadoop.mapred.Child.main(Child.java:211)\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 9 more\nCaused by: java.lang.RuntimeException: Error in configuring object\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:93)\n    at\norg.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:64)\n    at\norg.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n    at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:34)\n    ... 14 more\nCaused by: java.lang.reflect.InvocationTargetException\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at\nsun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n    at\nsun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n    at java.lang.reflect.Method.invoke(Method.java:597)\n    at\norg.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:88)\n    ... 17 more\nCaused by: java.lang.RuntimeException: x point\norg.apache.nutch.net.URLNormalizer not found.\n    at org.apache.nutch.net.URLNormalizers.<init>(URLNormalizers.java:122)\n    at\norg.apache.nutch.crawl.Injector$InjectMapper.configure(Injector.java:70)\n    ... 22 more\nThe bug is due to MAPREDUCE-967 (part of hadoop 0.21 and cdh 0.20.2+737) which modifies the way MapReduce unpacks the job's jar. The old way was to unpack the whole of it, now only classes/ and lib/ are unpacked. This way nutch is missing the plugins/ directory.\nA workaround is to force unpacking of the plugin/ directory by setting 'mapreduce.job.jar.unpack.pattern' configuration to \"(?:classes/|lib/|plugins/).*\"",
        "Issue Links": [
            "/jira/browse/NUTCH-993"
        ]
    },
    "NUTCH-938": {
        "Key": "NUTCH-938",
        "Summary": "Imposible to fetch sites with robots.txt",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Enrique Berlanga",
        "Created": "23/Nov/10 17:50",
        "Updated": "30/Nov/10 09:34",
        "Resolved": "30/Nov/10 09:34",
        "Description": "Crawling a site with a robots.txt file like this:  (e.g: http://www.melilla.es)\n-------------------\nUser-agent: *\nDisallow: /\n-------------------\nNo links are followed. \nIt doesn't matters the value set at \"protocol.plugin.check.blocking\" or \"protocol.plugin.check.robots\" properties, because they are overloaded in class org.apache.nutch.fetcher.Fetcher:\n// set non-blocking & no-robots mode for HTTP protocol plugins.\n    getConf().setBoolean(Protocol.CHECK_BLOCKING, false);\n    getConf().setBoolean(Protocol.CHECK_ROBOTS, false);\nFalse is the desired value, but in FetcherThread inner class, robot rules are checket ignoring the configuration:\n----------------\nRobotRules rules = protocol.getRobotRules(fit.url, fit.datum);\nif (!rules.isAllowed(fit.u)) {\n ...\nLOG.debug(\"Denied by robots.txt: \" + fit.url);\n...\ncontinue;\n}\n-----------------------\nI suposse there is no problem in disabling that part of the code directly for HTTP protocol. If so, I could submit a patch as soon as posible to get over this.\nThanks in advance",
        "Issue Links": []
    },
    "NUTCH-939": {
        "Key": "NUTCH-939",
        "Summary": "Added -dir command line option to Indexer and SolrIndexer,  allowing to specify directory containing segments",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.3",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Claudio Martella",
        "Created": "26/Nov/10 13:47",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "21/Dec/10 14:15",
        "Description": "The patches add -dir option, so the user can specify the directory in which the segments are to be found. The actual mode is to specify the list of segments, which is not very easy with hdfs. Also, the -dir option is already implemented in LinkDB and SegmentMerger, for example.",
        "Issue Links": [
            "/jira/browse/NUTCH-1001"
        ]
    },
    "NUTCH-940": {
        "Key": "NUTCH-940",
        "Summary": "static field plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Claudio Martella",
        "Created": "26/Nov/10 15:17",
        "Updated": "30/Jun/15 17:29",
        "Resolved": "19/Sep/11 15:28",
        "Description": "A simple plugin called at indexing that adds fields with static data. You can specify a list of <fieldname>:<fieldcontent> per nutch job.\nIt can be useful when collections can't be created by urlpatterns, like in subcollection, but on a job-basis.",
        "Issue Links": [
            "/jira/browse/NUTCH-924",
            "/jira/browse/NUTCH-2052"
        ]
    },
    "NUTCH-941": {
        "Key": "NUTCH-941",
        "Summary": "Search returns blank page, when there is more than one SOLR server configured",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Charan Malemarpuram",
        "Created": "02/Dec/10 19:54",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "Search returns a blank page throwing the following exception in the log file.\njava.lang.RuntimeException: Missing hit details! Found: 7, expecting: 8\n        at org.apache.nutch.searcher.SolrSearchBean.getDetails(SolrSearchBean.java:175)\n        at org.apache.nutch.searcher.DistributedSearchBean$DetailTask.call(DistributedSearchBean.java:92)\nThis happens, when there is more than one SOLR server configured for search.\nRoot cause of this issue is the \n NutchBean dedup logic does a \"contains\" check on a Map of Hit objects . \n Hit objects do not have hashcode and equals implemented. It is matching by reference, \n When NutchBean requests for more hits to process site based result grouping,  it gets a new object every time from SOLR result and the whole logic breaks.",
        "Issue Links": []
    },
    "NUTCH-942": {
        "Key": "NUTCH-942",
        "Summary": "Add user uid from drupal or other cms to the author field of Nutch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "bronco",
        "Created": "02/Dec/10 20:05",
        "Updated": "13/Apr/11 23:24",
        "Resolved": "13/Apr/11 23:24",
        "Description": "I wrote a modul which offers the user to add his domain to the solr search index of a website. The user website gets crawled with Nutch.\nTo search specially the sides of the user it is necessary to add the uid of drupal to the author field or to the uid field of solr. So the problem is how can I add this informations to nutch that he adds this extra info before he submits the crawler results for indexing to solr?\nMy problem is I am not a java programmer but willing to learn. So is someone here who could give me a hint  or some snippets or can describe what I have to do?",
        "Issue Links": []
    },
    "NUTCH-943": {
        "Key": "NUTCH-943",
        "Summary": "Search Results default dedup field \"site\" should be stored in index.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Charan Malemarpuram",
        "Created": "02/Dec/10 21:35",
        "Updated": "08/Jun/11 21:34",
        "Resolved": "01/Apr/11 14:35",
        "Description": "\"site\" is not configured as a stored field in SOLR schema.\nSearch returns only two results always and had \"See More Hits\" button, even if the results are from different sites.\n\"See More\"\nAttached patch changes the default schema.xml config to store site field.",
        "Issue Links": []
    },
    "NUTCH-944": {
        "Key": "NUTCH-944",
        "Summary": "Increase the number of elements to look for URLs and add the ability to specify multiple attributes by elements",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Jean-Francois Gingras",
        "Created": "03/Dec/10 17:24",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Here a patch for DOMContentUtils.java that increase the number of elements to look for URLs. It also add the ability to specify multiple attributes by elements, for example:\nlinkParams.put(\"frame\", new LinkParams(\"frame\", \"longdesc,src\", 0));\nlinkParams.put(\"object\", new LinkParams(\"object\", \"classid,codebase,data,usemap\", 0));\nlinkParams.put(\"video\", new LinkParams(\"video\", \"poster,src\", 0)); // HTML 5\nI have a patch for release-1.0 and branch-1.3\nI would love to hear your comments about this.",
        "Issue Links": []
    },
    "NUTCH-945": {
        "Key": "NUTCH-945",
        "Summary": "Indexing to multiple SOLR Servers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.2",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Charan Malemarpuram",
        "Created": "03/Dec/10 20:22",
        "Updated": "07/Aug/13 07:34",
        "Resolved": "07/Aug/13 07:34",
        "Description": "It would be nice to have a default Indexer in Nutch, which can submit docs to multiple SOLR Servers.\n> Partitioning is always the question, when writing to multiple SOLR Servers.\n> Default partitioning can be a simple hashcode based distribution with addition hooks to customization.",
        "Issue Links": [
            "/jira/browse/NUTCH-1480",
            "/jira/browse/NUTCH-1480"
        ]
    },
    "NUTCH-946": {
        "Key": "NUTCH-946",
        "Summary": "cache.jsp does not recognize encoding conversion from content different to UTF-8",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Enrique Berlanga",
        "Created": "21/Dec/10 09:34",
        "Updated": "22/May/13 03:54",
        "Resolved": "25/Apr/12 08:51",
        "Description": "Cache view does not recognize encoding conversion needed to show properly page content stored in a segment.\nThe problem is that it searchs \"CharEncodingForConversion\" meta in content metadata, but it's stored in parse metadata.\nHere is the patch I've generated for the fixed version:\n\n\n\n\n\nEclipse Workspace Patch 1.0\n#P branch-1.2\nIndex: src/web/jsp/cached.jsp\n===================================================================\n\n\nsrc/web/jsp/cached.jsp\t(revision 1027060)\n+++ src/web/jsp/cached.jsp\t(working copy)\n@@ -39,17 +39,18 @@\n     ResourceBundle.getBundle(\"org.nutch.jsp.cached\", request.getLocale())\n     .getLocale().getLanguage();\n\n\n\n\n\n\nMetadata metaData = bean.getParseData(details).getContentMeta();\n+  Metadata contentMetaData = bean.getParseData(details).getContentMeta();\n+  Metadata parseMetaData = bean.getParseData(details).getParseMeta();\n\n   String content = null;\n\nString contentType = (String) metaData.get(Metadata.CONTENT_TYPE);\n+  String contentType = (String) contentMetaData.get(Metadata.CONTENT_TYPE);\n   if (contentType.startsWith(\"text/html\")) {\n     // FIXME : it's better to emit the original 'byte' sequence \n     // with 'charset' set to the value of 'CharEncoding',\n     // but I don't know how to emit 'byte sequence' in JSP.\n     // out.getOutputStream().write(bean.getContent(details)) may work, \n     // but I'm not sure.\nString encoding = (String) metaData.get(\"CharEncodingForConversion\");\n+    String encoding = (String) parseMetaData.get(\"CharEncodingForConversion\"); \n     if (encoding != null) {\n       try {\n         content = new String(bean.getContent(details), encoding);",
        "Issue Links": []
    },
    "NUTCH-947": {
        "Key": "NUTCH-947",
        "Summary": "text.jsp does not compile on Apache Tomcat, and charset is not specified",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Enrique Berlanga",
        "Created": "21/Dec/10 10:29",
        "Updated": "13/Apr/11 23:36",
        "Resolved": "13/Apr/11 23:36",
        "Description": "If you add index-more plugin, view as plain text option is shown in search result, but if you access the jsp a compilation error is shown:\norg.apache.jasper.JasperException: /text.jsp(60,29) Attribute value\ndetails.getValue(\"url\") is quoted with \" which must be escaped when used\nwithin the value\nTo solve this and the missing character encofing, i've generated the atached patch file.",
        "Issue Links": []
    },
    "NUTCH-948": {
        "Key": "NUTCH-948",
        "Summary": "Remove Lucene dependencies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.3",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "21/Dec/10 14:30",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "21/Dec/10 14:39",
        "Description": "Branch-1.3 still has Lucene libs, but uses Lucene only in one place, namely it uses DateTools in index-basic. DateTools should be replaced with Solr's DateUtil, as we did in trunk, and then we can remove Lucene libs as a dependency.",
        "Issue Links": []
    },
    "NUTCH-949": {
        "Key": "NUTCH-949",
        "Summary": "Conflicting ANT jars in classpath",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "22/Dec/10 14:50",
        "Updated": "22/Dec/10 16:46",
        "Resolved": "22/Dec/10 16:46",
        "Description": "When the locally installed version of ANT > 1.7.1 the test-plugins task crashes because of a conflict between the versions of  the ANT jars that can be found in the classpath. \nThis is due to Avro being referenced in the ivy.xml file despite the fact that it is not needed in 1.3\nWill commit the change shortly; will also check whether this is an issue for 2.0",
        "Issue Links": []
    },
    "NUTCH-950": {
        "Key": "NUTCH-950",
        "Summary": "Content-Length limit, URL filter and few minor issues",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Alexis",
        "Created": "01/Jan/11 08:06",
        "Updated": "22/May/13 03:53",
        "Resolved": "10/Jan/11 10:30",
        "Description": "1. crawl command (nutch1.patch)\nThe class was renamed to Crawler but the references to it were not updated.\n2. URL filter (nutch2.patch)\nThis avoids a NPE on bogus urls which host do not have a suffix.\n3. Content-Length limit (nutch3.patch)\nThis is related to NUTCH-899.\nThe patch avoids the entire flush operation on the Gora datastore to crash because the MySQL blob limit was exceeded by a few bytes. Both protocol-http and protocol-httpclient plugins were problematic.\n4. Ivy configuration (nutch4.patch)\n\nChange xercesImpl and restlet versions. These 2 version changes are required. The first one currently makes a JUnit test crash, the second one is missing in default Maven repository.\n\n\nAdd gora-hbase, zookeeper which is an HBase dependency. Add MySQL connector. These jars are necesary to run Gora with HBase or MySQL datastores. (more a suggestion that a requirement here)\n\n\nAdd com.jcraft/jsch, which is a protocol-sftp plugin dependency.",
        "Issue Links": []
    },
    "NUTCH-951": {
        "Key": "NUTCH-951",
        "Summary": "Backport changes from 2.0 into 1.3",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.3",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Julien Nioche",
        "Created": "05/Jan/11 10:24",
        "Updated": "10/Mar/11 21:56",
        "Resolved": "09/Mar/11 11:50",
        "Description": "I've compared the changes from 2.0 with 1.3 and found the following differences (excluding anything specific to 2.0/GORA)\n\nNUTCH-564 External parser supports encoding attribute (Antony Bowesman, mattmann)\nNUTCH-714 Need a SFTP and SCP Protocol Handler (Sanjoy Ghosh, mattmann)\nNUTCH-825 Publish nutch artifacts to central maven repository (mattmann)\nNUTCH-851 Port logging to slf4j (jnioche)\nNUTCH-861 Renamed HTMLParseFilter into ParseFilter\nNUTCH-872 Change the default fetcher.parse to FALSE (ab).\nNUTCH-876 Remove remaining robots/IP blocking code in lib-http (ab)\nNUTCH-880 REST API for Nutch (ab)\nNUTCH-883 Remove unused parameters from nutch-default.xml (jnioche)\nNUTCH-884 FetcherJob should run more reduce tasks than default (ab)\nNUTCH-886 A .gitignore file for Nutch (dogacan)\nNUTCH-894 Move statistical language identification from indexing to parsing step\nNUTCH-921 Reduce dependency of Nutch on config files (ab)\nNUTCH-930 Remove remaining dependencies on Lucene API (ab)\nNUTCH-931 Simple admin API to fetch status and stop the service (ab)\nNUTCH-932 Bulk REST API to retrieve crawl results as JSON (ab)\n\nLet's go through this and decide what to port to 1.3",
        "Issue Links": []
    },
    "NUTCH-952": {
        "Key": "NUTCH-952",
        "Summary": "fix outlink which started with '?' in html parser",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Stondet",
        "Created": "06/Jan/11 03:14",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "26/Apr/14 21:46",
        "Description": "<a href=\"?w=ruby%20on%20rails&ty=c&sd=0\" >ruby on rails</a>(a snippet from http://bbs.soso.com/search?ty=c&sd=0&w=rails)\noutlink parsed from above link: http://bbs.soso.com/?w=ruby%20on%20rails&ty=c&sd=0\nbut expected is http://bbs.soso.com/search?w=ruby%20on%20rails&ty=c&sd=0",
        "Issue Links": [
            "/jira/browse/NUTCH-566",
            "/jira/browse/NUTCH-797",
            "/jira/browse/NUTCH-566",
            "/jira/browse/NUTCH-566",
            "/jira/browse/NUTCH-797"
        ]
    },
    "NUTCH-953": {
        "Key": "NUTCH-950 Content-Length limit, URL filter and few minor issues",
        "Summary": "Fix crawl command",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "06/Jan/11 13:15",
        "Updated": "06/Jan/11 13:17",
        "Resolved": "06/Jan/11 13:17",
        "Description": "1. crawl command (nutch1.patch)\nThe class was renamed to Crawler but the references to it were not updated.",
        "Issue Links": []
    },
    "NUTCH-954": {
        "Key": "NUTCH-950 Content-Length limit, URL filter and few minor issues",
        "Summary": "Bugfix for Content-Length limit in http protocols",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "06/Jan/11 13:19",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "07/Jan/11 15:49",
        "Description": "3. Content-Length limit (nutch3.patch)\nThis is related to NUTCH-899.\nThe patch avoids the entire flush operation on the Gora datastore to crash because the MySQL blob limit was exceeded by a few bytes. Both protocol-http and protocol-httpclient plugins were problematic.",
        "Issue Links": []
    },
    "NUTCH-955": {
        "Key": "NUTCH-955",
        "Summary": "Ivy configuration",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Alexis",
        "Created": "10/Jan/11 10:18",
        "Updated": "22/May/13 03:53",
        "Resolved": "09/Mar/11 12:22",
        "Description": "As mentioned in NUTCH-950, we can slightly improve the Ivy configuration to help setup the Gora backend more easily.\nIf the user does not want to stick with default HSQL database, other alternatives exist, such as MySQL and HBase.\norg.restlet and xercesImpl versions should be changed as well.",
        "Issue Links": []
    },
    "NUTCH-956": {
        "Key": "NUTCH-956",
        "Summary": "solrindex issues",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Alexis",
        "Created": "13/Jan/11 21:31",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/May/13 22:06",
        "Description": "I ran into a few caveats with solrindex command trying to index documents.\nPlease refer to http://techvineyard.blogspot.com/2010/12/build-nutch-20.html#solrindex that describes my tests.",
        "Issue Links": [
            "/jira/browse/NUTCH-1552"
        ]
    },
    "NUTCH-957": {
        "Key": "NUTCH-957",
        "Summary": "fetcher.timelimit.mins is invalid when depth is greater than 1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2",
        "Fix Version/s": "1.3",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Wade Lau",
        "Created": "16/Jan/11 16:06",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "21/Jan/11 14:21",
        "Description": "The setting value of  fetcher.timelimit.mins will be invalid when runing ./bin/nutch crawl with depth=n (n>1).\nThe reason is that the value of fetcher.timelimit.mins has been reset in the following paragraph ( org.apache.nutch.fetcher.Fetcher.java ), \nlong timelimit = getConf().getLong(\"fetcher.timelimit.mins\", -1);\nif (timelimit != -1) {\n  timelimit = System.currentTimeMillis() + (timelimit * 60 * 1000);\n  LOG.info(\"Fetcher Timelimit set for : \" + timelimit);\n  getConf().setLong(\"fetcher.timelimit.mins\", timelimit);\n}\nwhen the crawler goes down to next depth, the value will be the time value of last one which is timelimit.mins + currentTimeMillis.\nSome logs look like:\ndepth=1 \nFetcher: starting at 2011-01-16 20:58:53\nFetcher: segment: crawl/segments/20110116205851\nFetcher Timelimit set for : 1295182793540 now is:[1295182733540] timelimit:[1] new.sum:[1295182793540]\ndepth=2\nFetcher: starting at 2011-01-16 21:00:20\nFetcher: segment: crawl/segments/20110116210018\nFetcher Timelimit set for : 77712262795220167 now is:[1295182820167] timelimit:[1295182793540] new.sum:[77712262795220167]\nThe solution is easy to go as below:\nlong timelimit = getConf().getLong(\"fetcher.timelimit.mins.init\", -1);\nif( timelimit == -1)\n{\n    timelimit = getConf().getLong(\"fetcher.timelimit.mins\", -1);\n    getConf().setLong(\"fetcher.timelimit.mins.init\", timelimit);\n}\nif (timelimit != -1) {\n  timelimit = System.currentTimeMillis() + (timelimit * 60 * 1000);\n  LOG.info(\"Fetcher Timelimit set for : \" + timelimit);\n  getConf().setLong(\"fetcher.timelimit.mins\", timelimit);\n}\nHope  this will be helpful for the next release, and save time for others.\nrefer:\nhttp://ufqi.com/exp/x1183.html?title=apache.nutch.timelimit.bug",
        "Issue Links": []
    },
    "NUTCH-958": {
        "Key": "NUTCH-958",
        "Summary": "Httpclient scheme priority order fix",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.3",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Claudio Martella",
        "Created": "17/Jan/11 10:56",
        "Updated": "01/Apr/11 15:07",
        "Resolved": "18/Mar/11 17:09",
        "Description": "Httpclient will try to authenticate in this order by default: ntlm, digest, basic.\nIf you set as default a scheme that comes in this list after a scheme that is negotiated by the server, and this authentication fails, the default scheme will not be tried.\nI.e. if you set digest as default scheme but the server negotiates ntlm, the client will still try ntlm and fail.\nThe fix sets the default scheme as the only possible scheme for authentication for the given realm by setting the authentication priorities of httpclient.",
        "Issue Links": []
    },
    "NUTCH-959": {
        "Key": "NUTCH-959",
        "Summary": "use of \"ROWS\"  destroys result-lists: first hit appears also als last hit on each \"page\"  (search via search?query... -> xml )",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Thomas",
        "Created": "19/Jan/11 10:27",
        "Updated": "13/Apr/11 23:43",
        "Resolved": "13/Apr/11 23:43",
        "Description": "http://localhost:8080/nutch-1.2/search?query=SEARCHTERM&start=0&dupes=10&rows=4\nin general I get the correct results, but as soon as i start using \"rows=\" the results become weird: \nthe first hit appears on the first \"page\" twice: as first hit, and as last entry of the page \nwhen I change the start-Value, the first hit remains as last entry of the page .... \n(sorry for my english, I hope the problem is understandable)",
        "Issue Links": []
    },
    "NUTCH-960": {
        "Key": "NUTCH-960",
        "Summary": "Language ID - confidence factor",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "M Alexander",
        "Created": "21/Jan/11 19:19",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jan/13 19:25",
        "Description": "Hi\nIn JAVA implementation, what is the best way to calculate the confidence of the outcome of the language id for a given text?\nFor example:\nn-gram matching / total n-gram * 100.\nwhen a text is passed. The outcome would be \"en\" with 89% confidence. What is the best way to implement this to the existig nutch language id code?\nThanks",
        "Issue Links": []
    },
    "NUTCH-961": {
        "Key": "NUTCH-961",
        "Summary": "Expose Tika's boilerpipe support",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Jan/11 13:23",
        "Updated": "20/Mar/16 00:47",
        "Resolved": "16/Feb/16 14:43",
        "Description": "Tika 0.8 comes with the Boilerpipe content handler which can be used to extract boilerplate content from HTML pages. We should see how we can expose Boilerplate in the Nutch cofiguration.\nUse the following properties to enable and control Boilerpipe.\n\n<property>\n  <name>tika.extractor</name>\n  <value>none</value>\n  <description>\n  Which text extraction algorithm to use. Valid values are: boilerpipe or none.\n  </description>\n</property>\n \n<property> \n  <name>tika.extractor.boilerpipe.algorithm</name>\n  <value>ArticleExtractor</value>\n  <description> \n  Which Boilerpipe algorithm to use. Valid values are: DefaultExtractor, ArticleExtractor\n  or CanolaExtractor.\n  </description>\n</property>",
        "Issue Links": [
            "/jira/browse/TIKA-676",
            "/jira/browse/NUTCH-967",
            "/jira/browse/NUTCH-1375",
            "/jira/browse/NUTCH-1233"
        ]
    },
    "NUTCH-962": {
        "Key": "NUTCH-962",
        "Summary": "max. redirects not handled correctly: fetcher stops at max-1 redirects",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Sebastian Nagel",
        "Created": "26/Jan/11 15:20",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "09/Mar/11 12:03",
        "Description": "The fetcher stops following redirects one redirect before the max. redirects is reached.\nThe description of http.redirect.max\n> The maximum number of redirects the fetcher will follow when\n> trying to fetch a page. If set to negative or 0, fetcher won't immediately\n> follow redirected URLs, instead it will record them for later fetching.\nsuggests that if set to 1 that one redirect will be followed.\nI tried to crawl two documents the first redirecting by\n <meta http-equiv=\"refresh\" content=\"0; URL=./to/meta_refresh_target.html\">\nto the second with http.redirect.max = 1\nThe second document is not fetched and the URL has state GONE in CrawlDb.\nfetching file:/test/redirects/meta_refresh.html\nredirectCount=0\n-finishing thread FetcherThread, activeThreads=1\n\ncontent redirect to file:/test/redirects/to/meta_refresh_target.html (fetching now)\nredirect count exceeded file:/test/redirects/to/meta_refresh_target.html\n\nThe attached patch would fix this: if http.redirect.max is 1 : one redirect is followed.\nOf course, this would mean there is no possibility to skip redirects at all since 0\n(as well as negative values) means \"treat redirects as ordinary links\".",
        "Issue Links": []
    },
    "NUTCH-963": {
        "Key": "NUTCH-963",
        "Summary": "Add support for deleting Solr documents with STATUS_DB_GONE in CrawlDB (404 urls)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "1.3",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Claudio Martella",
        "Created": "26/Jan/11 18:00",
        "Updated": "08/May/11 22:34",
        "Resolved": "08/Apr/11 11:25",
        "Description": "When issuing recrawls it can happen that certain urls have expired (i.e. URLs that don't exist anymore and return 404).\nThis patch creates a new command in the indexer that scans the crawldb looking for these urls and issues delete commands to SOLR.",
        "Issue Links": [
            "/jira/browse/NUTCH-979"
        ]
    },
    "NUTCH-964": {
        "Key": "NUTCH-964",
        "Summary": "ERROR conf.Configuration - Failed to set setXIncludeAware(true)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Jan/11 13:08",
        "Updated": "08/May/11 22:34",
        "Resolved": "27/Jan/11 16:04",
        "Description": "Each executed job results in a number of occurences of the exception below:\n2011-01-27 13:40:34,457 ERROR conf.Configuration - Failed to set setXIncludeAware(true) for parser org.apache.xerces.jaxp.DocumentBuilderFactoryImpl@3801318b:java.lang.UnsupportedOperationException: This parser does not support specification \"null\" version \"null\"\njava.lang.UnsupportedOperationException: This parser does not support specification \"null\" version \"null\"\n        at javax.xml.parsers.DocumentBuilderFactory.setXIncludeAware(DocumentBuilderFactory.java:590)\n        at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1054)\n        at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1040)\n        at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:980)\n        at org.apache.hadoop.conf.Configuration.get(Configuration.java:436)\n        at org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:103)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:95)\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:230)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:248)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:238)\nThis can be fixed by upgrading xercesImpl from 2.6.2 to 2.9.1. If modified ivy and lib-xml's ivy configuration and can commit it. The question is, is upgrading the correct method? I've tested Nutch with 2.9.1 and except the lack of the annoying exception everything works as expected.",
        "Issue Links": []
    },
    "NUTCH-965": {
        "Key": "NUTCH-965",
        "Summary": "Skip parsing for truncated documents",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Alexis",
        "Created": "08/Feb/11 17:37",
        "Updated": "25/Feb/12 04:29",
        "Resolved": "22/Feb/12 11:06",
        "Description": "The issue you're likely to run into when parsing truncated FLV files is described here:\nhttp://www.mail-archive.com/user@nutch.apache.org/msg01880.html\nThe parser library gets stuck in infinite loop as it encounters corrupted data due to for example truncating big binary files at fetch time.",
        "Issue Links": []
    },
    "NUTCH-966": {
        "Key": "NUTCH-966",
        "Summary": "Behavior of NOINDEX,FOLLOW is not intuitive",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "1.2",
        "Fix Version/s": "2.5",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Josh Pavel",
        "Created": "09/Feb/11 14:31",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "If a page has NOINDEX,FOLLOW for the ROBOTS metatag, Nutch will still create a document that can be found in the index via metatag or URL matching.  Instead, Nutch should rely on doc or parse metadata but nothing should be stored by the html parser. (thanks to Julien Nioche for helping me to understand the issue).",
        "Issue Links": []
    },
    "NUTCH-967": {
        "Key": "NUTCH-967",
        "Summary": "Upgrade to Tika 0.9",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Markus Jelsma",
        "Created": "17/Feb/11 16:02",
        "Updated": "02/May/13 02:29",
        "Resolved": "08/Apr/11 10:09",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-888",
            "/jira/browse/NUTCH-961",
            "/jira/browse/NUTCH-934"
        ]
    },
    "NUTCH-968": {
        "Key": "NUTCH-968",
        "Summary": "Crawling - File Error 404 when fetching file with an chinese word in the file name",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dominic Xu",
        "Created": "04/Mar/11 01:19",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "I am performing a local file system crawling.\nMy problem is the following: all files that contain some chinese words in the file name do not get crawled.\nexample:\nfetching  /mnt/\u4e2d\u6587.txt\nI will get the error :org.apache.nutch.protocol.file.FileError: File Error: 404.\nand I read ISSUE NUTCH-824 https://issues.apache.org/jira/browse/NUTCH-824\nand I patch with trunk : Committed revision 1056394.\nbut the bug no fix.\nI fix the problem by modifying  the file : src/plugin/protocol-file/src/java/org/apache/nutch/protocol/file/FileResponse.java \n262    for (int i=0; i<list.length; i++) {\n263      f = list[i];\n264      String name = f.getName();\n265 +try \n{\n266 +      // specify the encoding via the config later?\n267 +      name = java.net.URLEncoder.encode(name, \"UTF-8\");\n268 +    }\n catch (UnsupportedEncodingException ex) \n{\n269 +    }\n270 +\n271 String time = HttpDateFormat.toString(f.lastModified());\nThere is must encode by utf8.\nand I modify the content with meta tag.\n251- StringBuffer x = new StringBuffer(\"<html><head>\");\n251+ StringBuffer x = new StringBuffer(\"<html><head><meta http-equiv=\\\"Content-Type\\\" content=\\\"text/html; charset=utf-8\\\" />\");",
        "Issue Links": []
    },
    "NUTCH-969": {
        "Key": "NUTCH-969",
        "Summary": "protocol-ftp with configurable encoding",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Te mule",
        "Created": "19/Mar/11 13:36",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "I use Nutch fetching a chinease ftp sites.But fetch ftp url's encoding is wrong.FTP used gb2312 encoding.but nutch used UTF-8 encoding.I want to change encoding to gb2312.How should I do?I hope nutch next version can change FTP encoding.Thank you.",
        "Issue Links": []
    },
    "NUTCH-970": {
        "Key": "NUTCH-970",
        "Summary": "Injector job crashes with MySQL with table collation set to utf8_general_ci",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.3",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "22/Mar/11 09:51",
        "Updated": "16/Apr/14 22:28",
        "Resolved": "16/Apr/14 22:27",
        "Description": "Running the injector of trunk with an already existing database where the default collation is utf8_* or ucs2_* the following GoraException is thrown:\nInjectorJob: starting\nInjectorJob: urlDir: urls\nInjectorJob: org.apache.gora.util.GoraException: java.io.IOException: com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:110)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:93)\n        at org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:43)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:227)\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:266)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:276)\nCaused by: java.io.IOException: com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n        at org.apache.gora.sql.store.SqlStore.createSchema(SqlStore.java:226)\n        at org.apache.gora.sql.store.SqlStore.initialize(SqlStore.java:172)\n        at org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:81)\n        at org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:104)\n        ... 7 more\nCaused by: com.mysql.jdbc.exceptions.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:936)\n        at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:2985)\n        at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1631)\n        at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:1723)\n        at com.mysql.jdbc.Connection.execSQL(Connection.java:3283)\n        at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:1332)\n        at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:1604)\n        at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:1519)\n        at com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:1504)\n        at org.apache.gora.sql.store.SqlStore.createSchema(SqlStore.java:224)\n        ... 10 more",
        "Issue Links": [
            "/jira/browse/NUTCH-1473"
        ]
    },
    "NUTCH-971": {
        "Key": "NUTCH-971",
        "Summary": "IndexMerger produces indexes itself cannot merge anymore",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.2",
        "Fix Version/s": "1.3",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Gabriele Kahlout",
        "Created": "26/Mar/11 10:34",
        "Updated": "27/Mar/11 09:16",
        "Resolved": "26/Mar/11 12:55",
        "Description": "Here's what I do:\n1. index the fetched segs\n$ rm -r $new_indexes $temp_indexes\n$ bin/nutch index $new_indexes $it_crawldb crawl/linkdb crawl/segments/*\nI examine the index with luke and it's as expected.\n2. merge the new index with the previous\n$ bin/nutch merge $temp_indexes $new_indexes $indexes\nIndexMerger: starting at 2011-03-26 10:24:58\nIndexMerger: merging indexes to: crawl/temp_indexes\nAdding file:/Users/simpatico/nutch-1.2/crawl/new_indexes/part-00000\nIndexMerger: finished at 2011-03-26 10:24:59, elapsed: 00:00:01\nOn the first iteration, when $indexes is empty it works fine by essentially duplicating  $new_indexes into $temp_indexes.\nBut on the 2nd iteration, after I mv $temp_indexes $indexes[1] the merged index $temp_indexes contains only #new_indexes and nothing from $indexes, which indeed still contains the data from the previous iteration. That is, it doesn't merge.\nThis unexpected merge behavior is NOT symmetric, i.e.\n$ bin/nutch merge $temp_indexes $indexes $new_indexes\nIndexMerger: starting at 2011-03-26 10:32:15\nIndexMerger: merging indexes to: crawl/temp_indexes\nAdding file:/Users/simpatico/nutch-1.2/crawl/new_indexes/part-00000\nIndexMerger: finished at 2011-03-26 10:32:16, elapsed: 00:00:01\nThe morale of the story is that a merged index cannot be merged with another, i.e. bin/nutch merge is meant to  merge only 2 indeces generated with bin/nutch index (or solrindex, perhaps).\nThe difference between the 2 indeces I can tell is that the merged index doesn't contain file index_done (and a hidden companion), but adding those to the merged index before merging it again doesn't solve either.\nThe way/workaround to make the merged index equivalent to the bin/nutch index generated index seems to be putting it in a \"part\" subdirectory:\nbin/nutch merge crawl/temp_indexes/part-1 crawl/indexes crawl/new_indexes\nIndexMerger: starting at 2011-03-26 11:18:10\nIndexMerger: merging indexes to: crawl/temp_indexes/part-1\nAdding file:/Users/simpatico/nutch-1.2/crawl/indexes/part-1\nAdding file:/Users/simpatico/nutch-1.2/crawl/new_indexes/part-00000\nIndexMerger: finished at 2011-03-26 11:18:12, elapsed: 00:00:01\nWhere was this documented? I'd recommend rather not documenting but have IndexMerger handle it as in the attached patch.",
        "Issue Links": [
            "/jira/browse/NUTCH-972"
        ]
    },
    "NUTCH-972": {
        "Key": "NUTCH-972",
        "Summary": "Mergedb doesn't merge with empty directory, as is the case with merge (for indexes)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2",
        "Fix Version/s": "1.3",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "Gabriele Kahlout",
        "Created": "27/Mar/11 09:10",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "08/Apr/11 11:06",
        "Description": "Just an issue of unexpected behavior. This series of commands works with bin/nutch merge to merge indexes but not with crawldb.\nallcrawldb=\"crawl/allcrawldb\"\ntemp_crawldb=\"crawl/temp_crawldb\"\nmerge_dbs=\"$it_crawldb $allcrawldb\"\n\nif [[ ! -d $allcrawldb ]]\nthen\nmerge_dbs=\"$it_crawldb\"\nfi\nuncomment the above and mergedb will work fine.\nbin/nutch mergedb $temp_crawldb $merge_dbs\t\nrm -r $it_crawldb $allcrawldb crawl/segments crawl/linkdb\nmv $temp_crawldb $allcrawldb\n\nThis is the exception that occurs:\nbin/nutch mergedb crawl/temp_crawldb crawl/crawldb crawl/allcrawldb\nCrawlDb merge: starting at 2011-03-27 10:13:06\nAdding crawl/crawldb\nAdding crawl/allcrawldb\nCrawlDb merge: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/simpatico/nutch-1.2/crawl/allcrawldb/current\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:190)\n\tat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:44)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:201)\n\tat org.apache.hadoop.mapred.JobClient.writeOldSplits(JobClient.java:810)\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:781)\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:730)\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1249)\n\tat org.apache.nutch.crawl.CrawlDbMerger.merge(CrawlDbMerger.java:126)\n\tat org.apache.nutch.crawl.CrawlDbMerger.run(CrawlDbMerger.java:187)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.CrawlDbMerger.main(CrawlDbMerger.java:159)\nBeside the scripting workaround I've attached a patch which skips adding the empty folder to the collection of dbs to merge. I've also added it a log of which dbs actually get added, consistent with merge interface.",
        "Issue Links": [
            "/jira/browse/NUTCH-971"
        ]
    },
    "NUTCH-973": {
        "Key": "NUTCH-973",
        "Summary": "Remove Segment Merger in 1.3",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "1.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "27/Mar/11 17:07",
        "Updated": "01/Apr/11 14:20",
        "Resolved": "01/Apr/11 14:20",
        "Description": "The code for the segment merging is still in 1.3, as far as I understand its original function it was mostly useful for having a single data structure where the search app could get the cached data from. Now that we've delegated the indexing and search to SOLR we don't really need to worry about the cache anymore. Would it make sense to purge it or do you guys think it would still be useful?",
        "Issue Links": []
    },
    "NUTCH-974": {
        "Key": "NUTCH-974",
        "Summary": "Parsing Error in Nutch 1.2 on Windows7",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Niksa Jakovljevic",
        "Created": "05/Apr/11 08:20",
        "Updated": "22/May/13 03:53",
        "Resolved": "24/Apr/11 08:32",
        "Description": "Hello World example of crawling does not work with Nutch 1.2 libs, but works fine with Nutch 1.1 libs. Note that same configuration is used in both Nutch 1.2 and Nutch 1.1.\nNutch 1.2 always throws following exception:\n2011-04-01 16:33:45,177 WARN  parse.ParseUtil - Unable to successfully parse content http://www.test.com/ of type text/html\n2011-04-01 16:33:45,177 WARN  fetcher.Fetcher - Error parsing: http://www.test.com/: failed(2,200): org.apache.nutch.parse.ParseException: Unable to successfully parse content\nThanks,\nNiksa Jakovljevic",
        "Issue Links": []
    },
    "NUTCH-975": {
        "Key": "NUTCH-975",
        "Summary": "Fix missing/wrong headers in source files",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Apr/11 11:31",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "14/Apr/11 09:52",
        "Description": "It seems several source files still do not contain the proper ASL headers. This includes older core in 1.3 (indexer.NutchField etc) and recent code in 2.0 (API for instance). This should be fixed (yet again). So if you spot one",
        "Issue Links": []
    },
    "NUTCH-976": {
        "Key": "NUTCH-976",
        "Summary": "Rename properties solrindex.* to solr.*",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Apr/11 14:54",
        "Updated": "08/May/11 22:34",
        "Resolved": "14/Apr/11 10:02",
        "Description": "All Solr properties are now consistently using solr.* instead of solrindex.*. This has been changed for solrindex.mapping.file which was not configurable at all.\nWas: The shipped nutch-default.xml configuration file uses solrindex. as namespace for configuration parameters but the namespace (or prefix) in SolrConstants is solr instead. It should be solrindex.",
        "Issue Links": [
            "/jira/browse/NUTCH-977"
        ]
    },
    "NUTCH-977": {
        "Key": "NUTCH-977",
        "Summary": "SolrMappingReader uses hardcoded configuration parameter name for mapping file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Apr/11 14:56",
        "Updated": "08/May/11 22:34",
        "Resolved": "14/Apr/11 10:07",
        "Description": "Because the SolrMappingReader uses a hard coded value for the name of the mapping file configuration parameter it actually works. It should rely on SolrConstants instead of using a hard coded value.",
        "Issue Links": [
            "/jira/browse/NUTCH-976"
        ]
    },
    "NUTCH-978": {
        "Key": "NUTCH-978",
        "Summary": "A Plugin for extracting certain element of a web page on html page parsing.",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Ammar Shadiq",
        "Created": "06/Apr/11 15:09",
        "Updated": "04/Oct/16 17:13",
        "Resolved": null,
        "Description": "Nutch use parse-html plugin to parse web pages, it process the contents of the web page by removing html tags and component like javascript and css and leaving the extracted text to be stored on the index. Nutch by default doesn't have the capability to select certain atomic element on an html page, like certain tags, certain content, some part of the page, etc.\nA html page have a tree-like xml pattern with html tag as its branch and text as its node. This branch and node could be extracted using XPath. XPath allowing us to select a certain branch or node of an XML and therefore could be used to extract certain information and treat it differently based on its content and the user requirements. Furthermore a web domain like news website usually have a same html code structure for storing the information on its web pages. This same html code structure could be parsed using the same XPath query and retrieve the same content information element. All of the XPath query for selecting various content could be stored on a XPath Configuration File.\nThe purpose of nutch are for various web source, not all of the web page retrieved from those various source have the same html code structure, thus have to be threated differently using the correct XPath Configuration. The selection of the correct XPath configuration could be done automatically using regex by matching the url of the web page with valid url pattern for that xpath configuration.\nThis automatic mechanism allow the user of nutch to process various web page and get only certain information that user wants therefore making the index more accurate and its content more flexible.\nThe component for this idea have been tested on nutch 1.2 for selecting certain elements on various news website for the purpose of document clustering. This includes a Configuration Editor Application build using NetBeans 6.9 Application Framework. though its need a few debugging.\nhttp://dl.dropbox.com/u/2642087/For_GSoC/for_GSoc.zip",
        "Issue Links": []
    },
    "NUTCH-979": {
        "Key": "NUTCH-979",
        "Summary": "Add support for deleting Solr documents with ProtocolStatusCodes.NOTFOUND",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "08/Apr/11 11:39",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "When issuing recrawls it can happen that certain urls have expired (i.e. URLs that don't exist anymore and return 404).\nThis issue creates a new command in the indexer that scans for WebPages with ProtocolStatusCodes.NOTFOUND and issues delete commands to Solr.",
        "Issue Links": [
            "/jira/browse/NUTCH-987",
            "/jira/browse/NUTCH-1036",
            "/jira/browse/NUTCH-963",
            "/jira/browse/NUTCH-1000"
        ]
    },
    "NUTCH-980": {
        "Key": "NUTCH-980",
        "Summary": "Fix IllegalAccessError with slf4j used in Solrj.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "build",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Apr/11 12:36",
        "Updated": "08/May/11 22:34",
        "Resolved": "14/Apr/11 08:57",
        "Description": "Currently Solr commands fail because of:\n Exception in thread \"main\" java.lang.IllegalAccessError: tried to \n access field org.slf4j.impl.StaticLoggerBinder.SINGLETON from class \n org.slf4j.LoggerFactory\n         at \n org.slf4j.LoggerFactory.staticInitialize(LoggerFactory.java:83)\n         at org.slf4j.LoggerFactory.<clinit>(LoggerFactory.java:73)\n         at \n org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.<clinit>(CommonsHttpSolrServer.java:78)\nJulien looked it up http://www.slf4j.org/faq.html#IllegalAccessError , we need to change the versions in Ivy. I haven't yet come around to test it with trunk so we need to look for it there as well.",
        "Issue Links": []
    },
    "NUTCH-981": {
        "Key": "NUTCH-981",
        "Summary": "Add tests for solr* tasks",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "12/Apr/11 14:36",
        "Updated": "30/Aug/11 11:16",
        "Resolved": "30/Aug/11 11:16",
        "Description": "As demonstrated in NUTCH-980, we come up with some tests the Solr indexer.",
        "Issue Links": [
            "/jira/browse/NUTCH-1046"
        ]
    },
    "NUTCH-982": {
        "Key": "NUTCH-982",
        "Summary": "Remove copying of ID and URL field in solrmapping",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Apr/11 15:21",
        "Updated": "22/May/13 03:53",
        "Resolved": "13/Apr/11 19:35",
        "Description": "Guys, the Solrindexer seems to be broken in trunk. With current solrmapping and code you'll get an exception complaining about multiple values in a non-multivalued field; the ID field which must of course be single valued. This happens because of the current mapping code and mapping config copy the url and id fields. The old 1.3 NutchDocument does not contain an ID field but in trunk it does.\nI propose to change the current solrmapping configuration by simply removing:\n                <field dest=\"id\" source=\"url\"/>\n                <copyField source=\"url\" dest=\"url\"/>\nIf not, we need to do something about the solrmapping code.",
        "Issue Links": []
    },
    "NUTCH-983": {
        "Key": "NUTCH-983",
        "Summary": "Upgrade SolrJ",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "13/Apr/11 13:44",
        "Updated": "29/Jun/11 04:01",
        "Resolved": "05/May/11 17:57",
        "Description": "Solr 3.1 has been released a while ago. The Javabin format between 1.4.1 and 3.1 has been changed so our SolrJ 1.4.1 cannot send documents to 3.1. Since Nutch 2.0 won't be released within a short period i believe it would be a good idea to upgrade our SolrJ to 3.1. New Solr users are encouraged to use Solr 3.1 or upgrade so i expect more users wanting to use 3.1 as well. Any thoughts?",
        "Issue Links": [
            "/jira/browse/NUTCH-994"
        ]
    },
    "NUTCH-984": {
        "Key": "NUTCH-984",
        "Summary": "Parse-tika throws some URL's away",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "18/Apr/11 16:35",
        "Updated": "02/May/13 02:29",
        "Resolved": "22/Apr/11 06:09",
        "Description": "For some reason using parse-tika a crawl just wouldn't dive into some website news archive. The paging through the news archive is done with simple anchors:\n<div class=\"page active\">1</div> <a href=\"/nieuws/overzicht/1/\"><div class=\"page\">2</div> </a> <a href=\"/nieuws/overzicht/2/\"><div class=\"page\">3</div> </a>\nI added some logging to DOMContentUtils:\n2011-04-18 18:26:09,788 INFO  tika.DOMContentUtils - Throw away link:  http://www.site.nl/nieuws/overzicht/1/\n2011-04-18 18:26:09,788 INFO  tika.DOMContentUtils - Throw away link:  http://www.site.nl/nieuws/overzicht/2/\n2011-04-18 18:26:09,788 INFO  tika.DOMContentUtils - Throw away link:  http://www.site.nl/nieuws/overzicht/3/\n...\nNow, this is rather funky. The code for private boolean shouldThrowAwayLink(Node node, NodeList children, int childLen, LinkParams params) is the same for parse-html and parse-tika. I also tested the two parsers between versions 1.2 and 1.3 for the following URL.\nhttp://news.bbc.co.uk/2/hi/europe/country_profiles/1154019.stm\n 1.2 - parse-tika: 196\n 1.2 - parse-html: 296\n 1.3 - parse-tika: 279\n 1.3 - parse-html: 296\nSomething clearly improved in 1.3 but not generating the remaining URL's are a blocker for parse-tika in my case. Relevant configurations are the same parser.html.outlinks.ignore_tags is not being used. Testing has been done with ParserChecker only.",
        "Issue Links": [
            "/jira/browse/TIKA-648"
        ]
    },
    "NUTCH-985": {
        "Key": "NUTCH-985",
        "Summary": "MoreIndexingFilter doesn't use properly formatted date fields for Solr",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Dietrich Schmidt",
        "Created": "19/Apr/11 18:27",
        "Updated": "23/May/11 16:49",
        "Resolved": "18/May/11 12:05",
        "Description": "I am using the index-more plugin to parse the lastModified data in web\npages in order to store it in a Solr data field.\nIn solrindex-mapping.xml I am mapping lastModified to a field \"changed\" in Solr:\n                <field dest=\"changed\" source=\"lastModified\"/>\nHowever, when posting data to Solr the SolrIndexer posts it as a long,\nnot as a date:\n<add><doc boost=\"1.0\"><field\nname=\"changed\">1079326800000</field><field\nname=\"tstamp\">20110414144140188</field><field\nname=\"date\">20040315</field>\nSolr rejects the data because of the improper data type.",
        "Issue Links": [
            "/jira/browse/NUTCH-989",
            "/jira/browse/NUTCH-997",
            "/jira/browse/NUTCH-986"
        ]
    },
    "NUTCH-986": {
        "Key": "NUTCH-986",
        "Summary": "Dedup fails due to date format (long)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "26/Apr/11 09:51",
        "Updated": "29/Jun/11 04:01",
        "Resolved": "28/Apr/11 09:58",
        "Description": "As already mentioned on the list, dedup also failes because of invalid date formats.\nApr 19, 2011 10:34:50 AM org.apache.solr.request.BinaryResponseWriter$Resolver \ngetDoc\nWARNING: Error reading a field from document : \nSolrDocument[\n{digest=7ff92a31c58e43a34fd45bc6d87cda03}\n]\njava.lang.NumberFormatException: For input string: \"2011-04-19T08:16:31.675Z\"\n        at \njava.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n        at java.lang.Long.parseLong(Long.java:419)\n        at java.lang.Long.valueOf(Long.java:525)\n        at org.apache.solr.schema.LongField.toObject(LongField.java:82)\n....\nStrange enough, Solr seems to allow updates of long fields with a formatted \ndate. In Nutch 1.2 the tstamp field is actually a long but in 1.3 the field is \na valid Solr date format. This exception is only triggered using the javabin \nresponse writer so there's something weird in Solr too.\nWe need to either change the tstamp field back to a long or update the Solr \nexample schema and fix SolrDeleteDuplicates to use the formatted date instead \nof the long.",
        "Issue Links": [
            "/jira/browse/NUTCH-985",
            "/jira/browse/NUTCH-989"
        ]
    },
    "NUTCH-987": {
        "Key": "NUTCH-987",
        "Summary": "Support HTTP auth for Solr communication",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "26/Apr/11 15:50",
        "Updated": "13/Sep/11 21:26",
        "Resolved": "06/Sep/11 11:57",
        "Description": "At the moment we cannot send data directly to a public HTTP auth protected Solr instance. I've a WIP that passes a configured HTTPClient object to CommonsHttpSolrServer, it works. This issue should add this ability to indexing, dedup and clean and be configured from some configuration file.\nEnable Solr HTTP auth communication by setting the following parameters in your nutch-site config:\n\nsolr.auth=true\nsolr.auth.username=USERNAME\nsolr.auth.password=PASSWORD",
        "Issue Links": [
            "/jira/browse/NUTCH-1036",
            "/jira/browse/NUTCH-979"
        ]
    },
    "NUTCH-988": {
        "Key": "NUTCH-988",
        "Summary": "index-feed plugin also doesn't use proper date fields",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "27/Apr/11 10:59",
        "Updated": "23/May/11 11:22",
        "Resolved": "23/May/11 11:22",
        "Description": "Like some other fields, the date fields generated by the feed-plugin are not using the proper date format for Solr.",
        "Issue Links": []
    },
    "NUTCH-989": {
        "Key": "NUTCH-989",
        "Summary": "index-basic plugin doesn't use Solr date fieldType",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Apr/11 11:12",
        "Updated": "29/Jun/11 04:01",
        "Resolved": "05/May/11 13:51",
        "Description": "The index-basic plugin actually sends over a properly formatted date with millis but the schema isn't configured to use the dateField.",
        "Issue Links": [
            "/jira/browse/NUTCH-985",
            "/jira/browse/NUTCH-986"
        ]
    },
    "NUTCH-990": {
        "Key": "NUTCH-990",
        "Summary": "protocol-httpclient fails with short pages",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Gabriele Kahlout",
        "Created": "27/Apr/11 14:34",
        "Updated": "10/Nov/11 09:26",
        "Resolved": "26/Aug/11 09:28",
        "Description": "Using protocol-http with a few words html pages works fine. But with protocol-httpclient the same pages disappear from the index, although they are still fetched.\nThose small files are useful for quick testing. \nSteps to reproduce:\n$ svn co http://svn.apache.org/repos/asf/nutch/branches/branch-1.3 nutch-1.3\nChecked out revision 1097214.\n$ cd nutch-1.3\n$ xmlstarlet edit -L -u \"/configuration/property[name='http.agent.name']\"/value -v 'test' conf/nutch-default.xml\n$ ant\nDownload to runtime/local the following script and seeds list file. They assume a $HADOOP_HOME environment variable. It's a 1.3 adaptation of [1].\nhttp://dp4j.sf.net/debug/whole-web-crawling-incremental\nhttp://dp4j.sf.net/debug/urls\n$ cd runtime/local\nThis will empty your Solr index (-f) and crawl:\n$ ./whole-web-crawling-incremental -f .\nNow Check Solr index searching for artificial and you will find the page pointed to in urls.\nNow change plugin-includes in conf/nutch-default to use protocol-httpclient instead of protocol-http and re-run the script. No more results in solr. Try again with http and the results return.\n[1] http://wiki.apache.org/nutch/Whole-Web%20Crawling%20incremental%20script",
        "Issue Links": []
    },
    "NUTCH-991": {
        "Key": "NUTCH-991",
        "Summary": "SolrDedup must issue a commit",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Apr/11 15:19",
        "Updated": "29/Jun/11 04:01",
        "Resolved": "28/Apr/11 11:18",
        "Description": "Title says it all. SolrDedup job doesn't commit but it should.",
        "Issue Links": []
    },
    "NUTCH-992": {
        "Key": "NUTCH-992",
        "Summary": "SolrDedup is broken in 2.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.3",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "28/Apr/11 11:23",
        "Updated": "01/May/14 06:23",
        "Resolved": "25/Jun/13 20:21",
        "Description": "SolrDedup seems to have been broken for at least a few months, perhaps more. It does fetch the documents from Solr but when processing the rows we get the following exception:\nException in thread \"main\" java.lang.NullPointerException\n        at org.apache.hadoop.io.serializer.SerializationFactory.getSerializer(SerializationFactory.java:73)\n        at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:899)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:779)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:432)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:447)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:350)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.run(SolrDeleteDuplicates.java:360)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.main(SolrDeleteDuplicates.java:370)",
        "Issue Links": [
            "/jira/browse/NUTCH-1571"
        ]
    },
    "NUTCH-993": {
        "Key": "NUTCH-993",
        "Summary": "NullPointerException at FetcherOutputFormat.checkOutputSpecs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Christian Guegi",
        "Created": "03/May/11 10:10",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "06/Jul/11 13:47",
        "Description": "When running Nutch as a mapreduce job on an existing cluster I get an NullPointerException at org.apache.nutch.fetcher.FetcherOutputFormat.checkOutputSpecs.\nThe reason is that the passed in reference to the file system is null.\nThe attached patch ignores the parameter 'fs' and creates a new reference to the file system.",
        "Issue Links": [
            "/jira/browse/NUTCH-937"
        ]
    },
    "NUTCH-994": {
        "Key": "NUTCH-994",
        "Summary": "Fine tune Solr schema",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/May/11 13:54",
        "Updated": "29/Jun/11 04:01",
        "Resolved": "23/May/11 10:50",
        "Description": "The supplied schema is old and doesn't use more advanced fieldTypes such as Trie based (since Solr 1.4) and perhaps other improvements. We need to fine tune the schema.",
        "Issue Links": [
            "/jira/browse/NUTCH-983"
        ]
    },
    "NUTCH-995": {
        "Key": "NUTCH-995",
        "Summary": "Generate POM file using the Ivy makepom task",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.3",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Julien Nioche",
        "Created": "07/May/11 05:58",
        "Updated": "29/Jun/11 04:01",
        "Resolved": "04/Jun/11 18:42",
        "Description": "We currently have a pom.xml file in the SVN repository and use it for publishing our artefacts. The trouble with this is that we need to keep its content in sync with our ivy file. Instead we could use the makepom task (http://ant.apache.org/ivy/history/2.2.0/use/makepom.html) to generate the pom.xml automatically.\nThe existing pom.xml for 1.3 needs fixing anyway as it declares dependencies to GORA and has the wrong versions for some dependencies.",
        "Issue Links": []
    },
    "NUTCH-996": {
        "Key": "NUTCH-996",
        "Summary": "Indexer adds solr.commit.size+1 docs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "08/May/11 01:42",
        "Updated": "23/May/11 16:49",
        "Resolved": "10/May/11 00:47",
        "Description": "SolrIndexer adds one additional document. This issue can be spotted easily with Solr 3.1 which accurately reports the number of added docs in the log.",
        "Issue Links": []
    },
    "NUTCH-997": {
        "Key": "NUTCH-997",
        "Summary": "IndexingFitlers to store Date objects instead of Strings",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3",
        "Component/s": "indexer",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "17/May/11 13:40",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "18/May/11 11:57",
        "Description": "See Nutch-985.\nSeveral IndexingFilters generate fields containing Dates with String values. This patch changes this so that Date objects are stored then converted into whatever type and format are required during the indexing.",
        "Issue Links": [
            "/jira/browse/NUTCH-985"
        ]
    },
    "NUTCH-998": {
        "Key": "NUTCH-998",
        "Summary": "index-basic should use filename if title is empty",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "18/May/11 11:31",
        "Updated": "15/Aug/11 14:19",
        "Resolved": "15/Aug/11 14:19",
        "Description": "In some cases documents are indexed with empty title fields, this is not very user friendly. Although this can be remedied in Solr using a conditional copyField in a custom update request processor i'd rather see it fixed in Nutch itself.\nAny thoughts?",
        "Issue Links": []
    },
    "NUTCH-999": {
        "Key": "NUTCH-999",
        "Summary": "Normalise String representation for Dates in IndexingFilters",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "18/May/11 12:00",
        "Updated": "22/May/13 03:53",
        "Resolved": "22/Jun/11 13:09",
        "Description": "NUTCH-997 has been applied to Nutch-1.3 so that various indexing filters store Date objects as value for fields. However in trunk NutchDocuments can have only String values which means that we will have to convert the Dates to Strings in each indexing filter.",
        "Issue Links": []
    },
    "NUTCH-1000": {
        "Key": "NUTCH-1000",
        "Summary": "Add option not to commit to Solr",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            1.4,                                            nutchgora",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/May/11 11:42",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "30/Jun/11 16:19",
        "Description": "We need an option to prevent a job from sending a commit to Solr. A commit can take a lot of resources (cache warming) and it's not always necessary to commit after index, dedup or clean, especially if they are run immediately after the other.",
        "Issue Links": [
            "/jira/browse/NUTCH-979",
            "/jira/browse/NUTCH-1025"
        ]
    },
    "NUTCH-1001": {
        "Key": "NUTCH-1001",
        "Summary": "bin/nutch fetch/parse handle crawl/segments directory",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gabriele Kahlout",
        "Created": "01/Jun/11 20:03",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "I'm having issues porting scripts across different systems to support the step of extracting the latest/only segments resulting from the generate phase.\nVariants include:\n$ export SEGMENT=crawl/segments/`ls -tr crawl/segments|tail -1` #[1]\n$ s1=`ls -d crawl/segments/2* | tail -1` #[2]\n$ segment=`$HADOOP_HOME/bin/hadoop dfs -ls crawl/segments | tail -1 | grep -o [a-zA-Z0-9/\\-]* |tail -1`\n$ segment=`$HADOOP_HOME/bin/hdfs -ls crawl/segments | tail -1 | grep -o [a-zA-Z0-9/\\-]* |tail -1`\nAnd I'm not sure what windows users would have to do. Some users may also do with:\nbin/nutch fetch with crawl/segments/2*\nBut I don't see a need in having the user extract/worry-about the latest/only segment, and have it a described step in every nutch tutorial. More over only fetch and parse expect a segment while other commands are fine with the directory of segments.\nTherefore, I think it's beneficial if fetch and parse also handle directories of segments. \n[1] http://www.lucidimagination.com/blog/2009/03/09/nutch-solr/\n[2] http://wiki.apache.org/nutch/NutchTutorial#Command_Line_Searching",
        "Issue Links": [
            "/jira/browse/NUTCH-939"
        ]
    },
    "NUTCH-1002": {
        "Key": "NUTCH-1002",
        "Summary": "Want to be able to filter url's through code, rather than through configuration file - crawl-urlfilter.txt",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.2",
        "Fix Version/s": "None",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "shantanu sardal",
        "Created": "02/Jun/11 21:36",
        "Updated": "07/Jun/11 20:21",
        "Resolved": "04/Jun/11 08:12",
        "Description": "Want to be able to filter url's through code, rather than through configuration file - crawl-urlfilter.txt .",
        "Issue Links": []
    },
    "NUTCH-1003": {
        "Key": "NUTCH-1003",
        "Summary": "'package' task does not reflect the new organisation of the code",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.3,                                            nutchgora",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "04/Jun/11 08:17",
        "Updated": "25/Jun/11 12:53",
        "Resolved": "04/Jun/11 08:38",
        "Description": "The package task still generate the nutch job and jar files in the main directory as well as the compiled plugins and the dependencies in lib. This was useful when we ran Nutch from bin but now that we've moved the execution to runtime/ these are not needed. More annoyingly the task does not copy the /ivy dir. \nThe patch attached fixes these issues for 1.3. Patch for 2.0 to follow shortly",
        "Issue Links": []
    },
    "NUTCH-1004": {
        "Key": "NUTCH-1004",
        "Summary": "Do not index empty values for title field",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Jun/11 13:26",
        "Updated": "30/Sep/11 23:20",
        "Resolved": "16/Aug/11 11:59",
        "Description": "Tika can generate multiple values for the title field for some files such as certain PDF's. It seems parse-tika's DOMContentUtils.getTitle() and helper methods are responsible for this behaviour. We should add a check on this to prevent empty values for the title field.",
        "Issue Links": [
            "/jira/browse/NUTCH-1140"
        ]
    },
    "NUTCH-1005": {
        "Key": "NUTCH-1005",
        "Summary": "Parse headings plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "indexer,                                            parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Jun/11 13:39",
        "Updated": "22/May/13 03:54",
        "Resolved": "07/Feb/12 13:25",
        "Description": "Very simple plugin for extracting and indexing a comma separated list of headings via the headings configuration directive.",
        "Issue Links": [
            "/jira/browse/NUTCH-809",
            "/jira/browse/NUTCH-809"
        ]
    },
    "NUTCH-1006": {
        "Key": "NUTCH-1006",
        "Summary": "meta equiv with single quotes not accepted",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.2,                                            1.3,                                            1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Jun/11 20:05",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "24/Jun/11 14:39",
        "Description": "As posted by Alex F:\nthe regex metaPattern inside org.apache.nutch.parse.html.HtmlParser is not\nsuitable for sites using single quotes for <meta http-equiv....>\n  Example: <meta http-equiv='Content-Type' content='text/html;\ncharset=iso-8859-1'>\n  We experienced a couple of pages with that kind of quotes and Nutch-1.2\nwas not able to handle it.\nIs there any fallback or would it be good to use the following\nregex: \"<meta\\\\s+([^>]http-equiv=(\\\"|')?content-type(\\\"|')?[^>])>\" (single\nor regular quotes are accepted)?\nSee this thread:\nhttp://lucene.472066.n3.nabble.com/Character-encoding-on-Html-Pages-td3034850.html",
        "Issue Links": []
    },
    "NUTCH-1007": {
        "Key": "NUTCH-1007",
        "Summary": "Add readdb -host output",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "MilleBii",
        "Created": "13/Jun/11 17:30",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 19:52",
        "Description": "I have created an enhancement for the readdb feature, which computes a list of <host> <nbre of urls for that host>.\nI think it could be valuable for many people. This is to know what is in the crawldb.\nLike -dump or -topN the syntax proposed would be like this : readdb -host ouput",
        "Issue Links": []
    },
    "NUTCH-1008": {
        "Key": "NUTCH-1008",
        "Summary": "Switch to crawler-commons version of robots.txt parsing code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kenneth William Krugler",
        "Created": "17/Jun/11 18:11",
        "Updated": "21/Jun/12 11:12",
        "Resolved": "21/Jun/12 11:12",
        "Description": "The Bixo project has an improved version of Nutch's robots.txt parsing code.\nThis was recently contributed to crawler-commons, in a format that should be independent of Bixo, Cascading, and even Hadoop.\nNutch could switch to this, and benefit from more robust parsing, better compliance with ad hoc extensions to the robot exclusion protocol, and a wider community of users/developers for that code.",
        "Issue Links": [
            "/jira/browse/NUTCH-1031"
        ]
    },
    "NUTCH-1009": {
        "Key": "NUTCH-1009",
        "Summary": "Incorrect path when using readdb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "1.3",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gabriele Kahlout",
        "Created": "20/Jun/11 17:58",
        "Updated": "20/Jun/11 18:37",
        "Resolved": "20/Jun/11 18:37",
        "Description": "I've not looked into the code but the path to the data folder is in a part-00000 directory. This seems to occur for unfetched urls only.\n\n$ $NUTCH_HOME/runtime/deploy/bin/nutch readdb gabriele/crawl/crawldb -url http://www.winonapost.com/stock/functions/VDG_Pub/detail.php?choice=42376&home_page=1&archives=\n[1] 7193\n[2] 7194\ngkahlout@loocia-c1:~/MemoRead/gabriele$ Exception in thread \"main\" java.io.FileNotFoundException: File does not exist: hdfs://loocia-c1/user/gkahlout/gabriele/crawl/crawldb/current/crawldb-merge-1110533277/data\n        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:457)\n        at org.apache.hadoop.fs.FileSystem.getLength(FileSystem.java:676)\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1424)\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1419)\n        at org.apache.hadoop.io.MapFile$Reader.createDataFileReader(MapFile.java:302)\n        at org.apache.hadoop.io.MapFile$Reader.open(MapFile.java:284)\n        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:273)\n        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:260)\n        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:253)\n        at org.apache.hadoop.mapred.MapFileOutputFormat.getReaders(MapFileOutputFormat.java:93)\n        at org.apache.nutch.crawl.CrawlDbReader.openReaders(CrawlDbReader.java:81)\n        at org.apache.nutch.crawl.CrawlDbReader.get(CrawlDbReader.java:379)\n        at org.apache.nutch.crawl.CrawlDbReader.readUrl(CrawlDbReader.java:386)\n        at org.apache.nutch.crawl.CrawlDbReader.main(CrawlDbReader.java:511)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
        "Issue Links": []
    },
    "NUTCH-1010": {
        "Key": "NUTCH-1010",
        "Summary": "ContentLength not trimmed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            1.4",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "22/Jun/11 11:04",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "24/Jun/11 13:57",
        "Description": "Somewhere in some component the ContentLength field is not trimmed. This allows a seemingly numeric field to be treated as a string by the indexer in cases one or more leading or trailing whitespace is added. The result is a hard to debug exception with no way to identify the bad document (amongst thousands) or the bad field.\n\nJun 22, 2011 1:03:42 PM org.apache.solr.common.SolrException log\nSEVERE: java.lang.NumberFormatException: For input string: \"32717     \"\n        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)\n        at java.lang.Long.parseLong(Long.java:419)\n        at java.lang.Long.parseLong(Long.java:468)\n\n\nThis can be quickly fixed in the index-more plugin by simply using the trim() when adding the field.",
        "Issue Links": []
    },
    "NUTCH-1011": {
        "Key": "NUTCH-1011",
        "Summary": "Normalize duplicate slashes in URL's",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Jun/11 13:47",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "06/Jul/11 15:36",
        "Description": "Many websites produce faulty URL's with multiple slashes e.g. http://cocoon.apache.org///////////////////////1.x/dynamic.html\nThis can be really nasty if the number of slashes varies, resulting in many URL's actually pointing to the same page and generating new (unique) URL's to the same or other duplicate pages.",
        "Issue Links": [
            "/jira/browse/NUTCH-1013"
        ]
    },
    "NUTCH-1012": {
        "Key": "NUTCH-1012",
        "Summary": "Cannot handle illegal charset $charset",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Jun/11 23:29",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "28/Jun/11 16:03",
        "Description": "Pages returning:\n\nContent-Type: text/html; charset=$charset\n\n\ncause:\n\nError parsing: http://host/: failed(2,200): java.nio.charset.IllegalCharsetNameException: $charset\nFound a TextHeaderAtom not followed by a TextBytesAtom or TextCharsAtom: Followed by 3999\nParseSegment: finished at 2011-06-24 01:14:54, elapsed: 00:01:12\n\n\nStack trace:\n\n2011-06-24 01:14:23,442 WARN  parse.html - java.nio.charset.IllegalCharsetNameException: $charset\n2011-06-24 01:14:23,442 WARN  parse.html - at java.nio.charset.Charset.checkName(Charset.java:284)\n2011-06-24 01:14:23,442 WARN  parse.html - at java.nio.charset.Charset.lookup2(Charset.java:458)\n2011-06-24 01:14:23,442 WARN  parse.html - at java.nio.charset.Charset.lookup(Charset.java:437)\n2011-06-24 01:14:23,442 WARN  parse.html - at java.nio.charset.Charset.isSupported(Charset.java:479)\n2011-06-24 01:14:23,442 WARN  parse.html - at org.apache.nutch.util.EncodingDetector.resolveEncodingAlias(EncodingDetector.java:310)\n2011-06-24 01:14:23,442 WARN  parse.html - at org.apache.nutch.util.EncodingDetector.addClue(EncodingDetector.java:201)\n2011-06-24 01:14:23,442 WARN  parse.html - at org.apache.nutch.util.EncodingDetector.addClue(EncodingDetector.java:208)\n2011-06-24 01:14:23,442 WARN  parse.html - at org.apache.nutch.util.EncodingDetector.autoDetectClues(EncodingDetector.java:193)\n2011-06-24 01:14:23,442 WARN  parse.html - at org.apache.nutch.parse.html.HtmlParser.getParse(HtmlParser.java:138)\n2011-06-24 01:14:23,442 WARN  parse.html - at org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:35)\n2011-06-24 01:14:23,443 WARN  parse.html - at org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:24)\n2011-06-24 01:14:23,443 WARN  parse.html - at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n2011-06-24 01:14:23,443 WARN  parse.html - at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n2011-06-24 01:14:23,443 WARN  parse.html - at java.lang.Thread.run(Thread.java:662)\n2011-06-24 01:14:23,443 WARN  parse.ParseSegment - Error parsing: http://host/: failed(2,200): java.nio.charset.Ill\negalCharsetNameException: $charset",
        "Issue Links": []
    },
    "NUTCH-1013": {
        "Key": "NUTCH-1013",
        "Summary": "Migrate RegexURLNormalizer from Apache ORO to java.util.regex",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Jun/11 13:15",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "04/Jul/11 14:28",
        "Description": "Apache ORO uses old Perl 5-style regular expressions. Features such as the powerful lookbehind are not available. The project has become retired as well.",
        "Issue Links": [
            "/jira/browse/NUTCH-1014",
            "/jira/browse/NUTCH-1011"
        ]
    },
    "NUTCH-1014": {
        "Key": "NUTCH-1014",
        "Summary": "Migrate from Apache ORO to java.util.regex",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "24/Jun/11 15:44",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "13/Oct/18 10:17",
        "Description": "A separate issue tracking migration of all components from Apache ORO to java.util.regex. Components involved are:\n\nRegexURLNormalzier\nOutlinkExtractor\nJSParseFilter\nMoreIndexingFilter\nBasicURLNormalizer",
        "Issue Links": [
            "/jira/browse/NUTCH-1062",
            "/jira/browse/NUTCH-1013",
            "/jira/browse/NUTCH-1021",
            "/jira/browse/NUTCH-1061",
            "/jira/browse/NUTCH-2192"
        ]
    },
    "NUTCH-1015": {
        "Key": "NUTCH-1015",
        "Summary": "MoreIndexingFilter: can't parse erroneous date: 2006-05-24T20:03:42",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "27/Jun/11 10:30",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "MoreIndexingFilter must handle the following url's gracefully:\n\ncan't parse erroneous date: Sun, 27 Jun 2010 06:51:35 GMT+1\ncan't parse erroneous date: ma, 27 jun 2011 05:15:32 GMT\ncan't parse erroneous date: \"Mon, 23 May 2011 22:05:58 GMT\"\ncan't parse erroneous date: GMT",
        "Issue Links": [
            "/jira/browse/NUTCH-1190"
        ]
    },
    "NUTCH-1016": {
        "Key": "NUTCH-1016",
        "Summary": "Strip UTF-8 non-character codepoints",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Jun/11 15:45",
        "Updated": "09/May/12 14:06",
        "Resolved": "30/Jun/11 16:19",
        "Description": "During a very large crawl i found a few documents producing non-character codepoints. When indexing to Solr this will yield the following exception:\n\nSEVERE: java.lang.RuntimeException: [was class java.io.CharConversionException] Invalid UTF-8 character 0xffff at char #1142033, byte #1155068)\n        at com.ctc.wstx.util.ExceptionUtil.throwRuntimeException(ExceptionUtil.java:18)\n        at com.ctc.wstx.sr.StreamScanner.throwLazyError(StreamScanner.java:731)\n\n\nQuite annoying! Here's quick fix for SolrWriter that'll pass the value of the content field to a method to strip away non-characters. I'm not too sure about this implementation but the tests i've done locally with a huge dataset now passes correctly. Here's a list of codepoints to strip away: http://unicode.org/cldr/utility/list-unicodeset.jsp?a=[:Noncharacter_Code_Point=True:]\nPlease comment!",
        "Issue Links": [
            "/jira/browse/NUTCH-1026"
        ]
    },
    "NUTCH-1017": {
        "Key": "NUTCH-1017",
        "Summary": "Exception getting mime type by name",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Jun/11 19:29",
        "Updated": "22/May/13 03:53",
        "Resolved": "02/Jan/12 11:59",
        "Description": "Large crawls of `bad` websites tend to produce a lot of parsing errors. One of them is related to retrieving mime types, so it seems:\n\nWARNING: Exception getting mime type by name: [<WEBSITE_CONTENT>]: Message: Invalid media type name: <WEBSITE_CONTENT>\nJun 27, 2011 9:23:27 PM org.apache.nutch.util.MimeUtil forName\nWARNING: Exception getting mime type by name: [<WEBSITE_CONTENT>]: Message: Invalid media type name: <WEBSITE_CONTENT>\nJun 27, 2011 9:23:27 PM org.apache.nutch.util.MimeUtil forName\nWARNING: Exception getting mime type by name: [Mime-Type]: Message: Invalid media type name: Mime-Type\nJun 27, 2011 9:23:27 PM org.apache.nutch.util.MimeUtil forName\nWARNING: Exception getting mime type by name: [<WEBSITE_CONTENT>]: Message: Invalid media type name: <WEBSITE_CONTENT>\nJun 27, 2011 9:23:27 PM org.apache.nutch.util.MimeUtil forName\nWARNING: Exception getting mime type by name: [text/html charset=utf-8]: Message: Invalid media type name: text/html charset=utf-8",
        "Issue Links": [
            "/jira/browse/NUTCH-1230",
            "/jira/browse/NUTCH-1041",
            "/jira/browse/NUTCH-1064"
        ]
    },
    "NUTCH-1018": {
        "Key": "NUTCH-1018",
        "Summary": "Solr Document Size Limit",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Mark Achee",
        "Created": "27/Jun/11 20:01",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 19:49",
        "Description": "There should be an option, perhaps named solr.content.limit, that defines the max size of documents added to Solr.  I've had issues with large documents in Solr, so I set the file.content.limit to 2MB.  However, this causes many files to not be parsed (mostly PDFs) because of only retrieving parts of the document.  With this new option, I could still correctly parse them, but only index the first 2MB (or however large it is set) in Solr.",
        "Issue Links": []
    },
    "NUTCH-1019": {
        "Key": "NUTCH-1019",
        "Summary": "Edit comment in org.apache.nutch.crawl.Crawl to reflect removal of legacy",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Jun/11 02:20",
        "Updated": "17/Jul/11 20:43",
        "Resolved": "17/Jul/11 20:43",
        "Description": "When updating the wiki documentation for command line options, I noticed that the comment on line 51 of the above class is inaccurate and needs to be updated to reflect changes. Although this is a trivial task I won't be able to committ until 2nd week July. Can I ask someone else please?",
        "Issue Links": []
    },
    "NUTCH-1020": {
        "Key": "NUTCH-1020",
        "Summary": "Create or locate class for org.apache.nutch.tools.compat.CrawlDbConverter",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            1.4,                                            nutchgora",
        "Fix Version/s": "1.4",
        "Component/s": "linkdb",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Jun/11 06:44",
        "Updated": "18/Jul/11 11:58",
        "Resolved": "18/Jul/11 11:57",
        "Description": "Whilst updating the CommandLineOptions for release 1.3 on the wiki, I noticed that the above class does not exist in the expected location in /src folder. Having looked further afield, it appears that this class (which is meant to convert Nutch 0.9 WebDB to 1.3 format WebDB) does not exist.",
        "Issue Links": []
    },
    "NUTCH-1021": {
        "Key": "NUTCH-1021",
        "Summary": "Migrate OutlinkExtractor from Apache ORO to java.util.regex",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.16",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "28/Jun/11 12:38",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "13/Oct/18 10:16",
        "Description": "Migrate from deprecated ORO to Java util regex.",
        "Issue Links": [
            "/jira/browse/NUTCH-1014",
            "/jira/browse/NUTCH-1063"
        ]
    },
    "NUTCH-1022": {
        "Key": "NUTCH-1022",
        "Summary": "Upgrade version number of Nutch agent in conf",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "28/Jun/11 13:52",
        "Updated": "28/Jun/11 13:54",
        "Resolved": "28/Jun/11 13:54",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1023": {
        "Key": "NUTCH-1023",
        "Summary": "Trivial error in error message for org.apache.nutch.crawl.LinkDbReader",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "linkdb",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Jun/11 20:47",
        "Updated": "17/Jul/11 20:49",
        "Resolved": "17/Jul/11 20:49",
        "Description": "The following line in the above class has a trivial error in syntax before the -dump parameter. Instead of a curly bracket, it should be consistent with the round bracket.\n126       System.err.println(\"Usage: LinkDbReader <linkdb> {-dump <out_dir> | -url <url>)\");",
        "Issue Links": []
    },
    "NUTCH-1024": {
        "Key": "NUTCH-1024",
        "Summary": "Dynamically set fetchInterval by MIME-type",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "29/Jun/11 23:22",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jun/12 10:12",
        "Description": "Add facility to configure default or fixed fetchInterval values by MIME-type. This is useful for conserving resources for files that are known to change frequently or never and everything in between.\n\nsimple key\\tvalue\\n configuration file\nonly set fetchInterval for new documents\nkeep max fetchInterval fixed by current config",
        "Issue Links": [
            "/jira/browse/NUTCH-779"
        ]
    },
    "NUTCH-1025": {
        "Key": "NUTCH-1025",
        "Summary": "Add option not to commit to Solr",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "30/Jun/11 16:17",
        "Updated": "22/May/13 03:54",
        "Resolved": "09/Jul/12 11:56",
        "Description": "We need an option to prevent a job from sending a commit to Solr. A commit can take a lot of resources (cache warming) and it's not always necessary to commit after index, dedup or clean, especially if they are run immediately after the other.",
        "Issue Links": [
            "/jira/browse/NUTCH-1306",
            "/jira/browse/NUTCH-1000"
        ]
    },
    "NUTCH-1026": {
        "Key": "NUTCH-1026",
        "Summary": "Strip UTF-8 non-character codepoints",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "30/Jun/11 16:18",
        "Updated": "11/May/12 04:16",
        "Resolved": "10/May/12 12:46",
        "Description": "During a very large crawl i found a few documents producing non-character codepoints. When indexing to Solr this will yield the following exception:\n\nSEVERE: java.lang.RuntimeException: [was class java.io.CharConversionException] Invalid UTF-8 character 0xffff at char #1142033, byte #1155068)\n        at com.ctc.wstx.util.ExceptionUtil.throwRuntimeException(ExceptionUtil.java:18)\n        at com.ctc.wstx.sr.StreamScanner.throwLazyError(StreamScanner.java:731)\n\n\nQuite annoying! Here's quick fix for SolrWriter that'll pass the value of the content field to a method to strip away non-characters. I'm not too sure about this implementation but the tests i've done locally with a huge dataset now passes correctly. Here's a list of codepoints to strip away: http://unicode.org/cldr/utility/list-unicodeset.jsp?a=[:Noncharacter_Code_Point=True:]\nPlease comment!",
        "Issue Links": [
            "/jira/browse/NUTCH-1016"
        ]
    },
    "NUTCH-1027": {
        "Key": "NUTCH-1027",
        "Summary": "Degrade log level of `can't find rules for scope`",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Jul/11 00:17",
        "Updated": "13/Sep/11 21:26",
        "Resolved": "11/Jul/11 11:58",
        "Description": "The warning for regex.RegexURLNormalizer - can't find rules for scope '<SCOPE>', using default should be degraded to info because:\n\nnew users are unaware of the normalizer\nthe scoping of normalizer is not really documented (meaning wiki/tutorial, not just javadoc)\ni don't consider it a warning (i.e. this no scope is not bad)\n\nThougts?",
        "Issue Links": []
    },
    "NUTCH-1028": {
        "Key": "NUTCH-1028",
        "Summary": "Log parser keys",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Jul/11 12:11",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "19/Sep/11 15:27",
        "Description": "The parser can take ages (many hours) to complete. During this time the only output is an error or warning if it's unable to parse something (which is very common). Sometimes the parser can run for several hours without any output: this is scary. I propose to add a LOG.info to the mapper and write the key when parsing, similar to the fetcher.\nThoughts?",
        "Issue Links": []
    },
    "NUTCH-1029": {
        "Key": "NUTCH-1029",
        "Summary": "Readdb throws EOFException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "linkdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/Jul/11 17:13",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "17/Jul/11 14:02",
        "Description": "Readdb -stats on a crawldb with 1 record exits with EOFError on Hadoop-0.20.203.0.\n\nException in thread \"main\" java.io.EOFException\n        at java.io.DataInputStream.readFully(DataInputStream.java:180)\n        at java.io.DataInputStream.readFully(DataInputStream.java:152)\n        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1450)\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1428)\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1417)\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1412)\n        at org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders(SequenceFileOutputFormat.java:93)\n        at org.apache.nutch.crawl.CrawlDbReader.processStatJob(CrawlDbReader.java:320)\n        at org.apache.nutch.crawl.CrawlDbReader.main(CrawlDbReader.java:502)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
        "Issue Links": [
            "/jira/browse/NUTCH-1069",
            "/jira/browse/NUTCH-1110"
        ]
    },
    "NUTCH-1030": {
        "Key": "NUTCH-1030",
        "Summary": "WebgraphDB program requires manually added directories",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Jul/11 23:43",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "11/Jul/11 10:23",
        "Description": "The WebgraphDB program exists with only a fatal error in the log. Because it doesn't make all directories it required\nFirst run:\n\n2011-07-06 01:41:32,150 FATAL webgraph.WebGraph - WebGraph: java.io.IOException: No input paths specified in job\n\n\nsecond attempt:\n\n2011-07-06 01:23:20,626 FATAL webgraph.WebGraph - WebGraph: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/markus/projects/apache/nutch/branches/branch-1.4/runtime/local/crawl/webgraphdb/outlinks\n\n\nThe first run exists but makes the directory. The second attempt will never create the required directory.\n\nprogram must create the directory if it doesn't exist\nprogram must write such errors to stdout\nprogram must write success output to stdout",
        "Issue Links": [
            "/jira/browse/NUTCH-875"
        ]
    },
    "NUTCH-1031": {
        "Key": "NUTCH-1031",
        "Summary": "Delegate parsing of robots.txt to crawler-commons",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": "Tejas Patil",
        "Reporter": "Julien Nioche",
        "Created": "06/Jul/11 13:35",
        "Updated": "22/May/13 03:54",
        "Resolved": "29/Apr/13 20:29",
        "Description": "We're about to release the first version of Crawler-Commons http://code.google.com/p/crawler-commons/ which contains a parser for robots.txt files. This parser should also be better than the one we currently have in Nutch. I will delegate this functionality to CC as soon as it is available publicly",
        "Issue Links": [
            "/jira/browse/NUTCH-1008",
            "/jira/browse/NUTCH-1455"
        ]
    },
    "NUTCH-1032": {
        "Key": "NUTCH-1032",
        "Summary": "Delegate parsing of robots.txt to crawler-commons",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "06/Jul/11 13:35",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "06/Jul/11 20:34",
        "Description": "We're about to release the first version of Crawler-Commons http://code.google.com/p/crawler-commons/ which contains a parser for robots.txt files. This parser should also be better than the one we currently have in Nutch. I will delegate this functionality to CC as soon as it is available publicly",
        "Issue Links": []
    },
    "NUTCH-1033": {
        "Key": "NUTCH-1033",
        "Summary": "Backport FetcherJob should run more reduce tasks than default",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.3,                                            1.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "06/Jul/11 14:12",
        "Updated": "06/Jul/11 15:49",
        "Resolved": "06/Jul/11 15:48",
        "Description": "Andrzej wrote:\"FetcherJob now performs fetching in the reduce phase. This means that in a typical Hadoop setup there will be many fewer reduce tasks than map tasks, and consequently the max. total throughput of Fetcher will be proportionally reduced. I propose that FetcherJob should set the number of reduce tasks to the number of map tasks. This way the fetching will be more granular.\"\nThis issue covers the backport of NUTCH-884 to Nutch 1.4-dev.",
        "Issue Links": []
    },
    "NUTCH-1034": {
        "Key": "NUTCH-1034",
        "Summary": "Create Solr Velocity templates",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "06/Jul/11 20:39",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Solr has Velocity integration and provides an easy method for creating HTML based front-ends for the search engine. This issue tracks the development of Velocity templates specifically for Nutch users.",
        "Issue Links": [
            "/jira/browse/NUTCH-717",
            "/jira/browse/NUTCH-1035"
        ]
    },
    "NUTCH-1035": {
        "Key": "NUTCH-1035",
        "Summary": "Tune Solr config for Nutch users",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "06/Jul/11 20:41",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Oct/19 12:57",
        "Description": "To improve and ease integration with Solr we should provide a solrconfig.xml specifically for Nutch integration including a request handler with a Velocity response writer.",
        "Issue Links": [
            "/jira/browse/NUTCH-717",
            "/jira/browse/NUTCH-1034"
        ]
    },
    "NUTCH-1036": {
        "Key": "NUTCH-1036",
        "Summary": "Solr jobs should increment counters in Reporter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "09/Jul/11 17:16",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "06/Sep/11 12:00",
        "Description": "Some jobs report progress by incrementing counters in the Reporter object. Solr* jobs should also report their progress.",
        "Issue Links": [
            "/jira/browse/NUTCH-979",
            "/jira/browse/NUTCH-987"
        ]
    },
    "NUTCH-1037": {
        "Key": "NUTCH-1037",
        "Summary": "Deduplicate anchors before indexing",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "09/Jul/11 23:07",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "19/Jul/11 13:13",
        "Description": "Anchors are not deduplicated before indexing. This can result in a very high number of similar and identical anchors being indexed. Before indexing, anchors must be deduplicated at least on case.\nUse anchorIndexingFilter.deduplicate=true to deduplicate anchors case-insensitive.",
        "Issue Links": []
    },
    "NUTCH-1038": {
        "Key": "NUTCH-1038",
        "Summary": "Port IndexingFiltersChecker to 2.0",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "11/Jul/11 10:42",
        "Updated": "22/May/13 03:53",
        "Resolved": "27/Mar/13 20:27",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1501",
            "/jira/browse/NUTCH-783",
            "/jira/browse/NUTCH-1082"
        ]
    },
    "NUTCH-1039": {
        "Key": "NUTCH-1039",
        "Summary": "Fetcher fails for pages without content-length header",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.7",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Jul/11 12:52",
        "Updated": "22/May/13 03:53",
        "Resolved": "03/May/13 20:36",
        "Description": "Fetcher fails:\n2011-07-11 14:45:34,764 ERROR http.Http - org.apache.nutch.protocol.http.api.HttpException: bad content length:\n2011-07-11 14:45:34,765 ERROR http.Http - at org.apache.nutch.protocol.http.HttpResponse.readPlainContent(HttpResponse.java:218)\n2011-07-11 14:45:34,765 ERROR http.Http - at org.apache.nutch.protocol.http.HttpResponse.<init>(HttpResponse.java:158)\n2011-07-11 14:45:34,765 ERROR http.Http - at org.apache.nutch.protocol.http.Http.getResponse(Http.java:64)\n2011-07-11 14:45:34,765 ERROR http.Http - at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:138)\n2011-07-11 14:45:34,765 ERROR http.Http - at org.apache.nutch.parse.ParserChecker.main(ParserChecker.java:79)\nBoth fetcher and indexing filter checker fail sometimes. I'm unsure whether this is something in Nutch or whether the remote server only returns content-length incidentally.",
        "Issue Links": [
            "/jira/browse/NUTCH-1096"
        ]
    },
    "NUTCH-1040": {
        "Key": "NUTCH-1040",
        "Summary": "Backport REST-API from 2.0",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "11/Jul/11 14:52",
        "Updated": "28/Aug/15 20:34",
        "Resolved": "28/Aug/15 20:34",
        "Description": "See https://issues.apache.org/jira/browse/NUTCH-880",
        "Issue Links": []
    },
    "NUTCH-1041": {
        "Key": "NUTCH-1041",
        "Summary": "Not reading mime-type correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.5",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "11/Jul/11 16:54",
        "Updated": "22/May/13 03:53",
        "Resolved": "02/Jan/12 12:00",
        "Description": "Another issue with mime-types and test url's. Below are two logs lines from MimeUtil. Mime-type is still ok at the start of the autoResolveContentType method:\n\nJul 11, 2011 6:46:15 PM org.apache.nutch.util.MimeUtil autoResolveContentType\nINFO: Type: text/html; charset=ISO-8859-1 from: http://www.taxipoll.nl/taxipol.htm\nJul 11, 2011 6:46:15 PM org.apache.nutch.util.MimeUtil autoResolveContentType\nINFO: Type: text/html from: http://archief.hoofdklassehockey.nl/hschema2009.html\n\n\nmIME-TYpe correctness has been confirmed with Curl. The documents, however, do not end up in the index with the correct mime-type, here's output from IndexingFiltersChecker. ParserChecker does output the correct Content-Type.\n\nhttp://www.taxipoll.nl/taxipol.htm   -->  taxipoll/htm\nhttp://archief.hoofdklassehockey.nl/hschema2009.html  --> tet/html",
        "Issue Links": [
            "/jira/browse/NUTCH-1230",
            "/jira/browse/NUTCH-1064",
            "/jira/browse/NUTCH-1017"
        ]
    },
    "NUTCH-1042": {
        "Key": "NUTCH-1042",
        "Summary": "Fetcher.max.crawl.delay property not taken into account correctly when set to -1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "fetcher",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Nutch User - 1",
        "Created": "12/Jul/11 14:29",
        "Updated": "22/May/13 03:53",
        "Resolved": "28/Jan/13 08:05",
        "Description": "[Originally: (http://lucene.472066.n3.nabble.com/A-possible-bug-or-misleading-documentation-td3162397.html).]\nFrom nutch-default.xml:\n\"\n<property>\n <name>fetcher.max.crawl.delay</name>\n <value>30</value>\n <description>\n If the Crawl-Delay in robots.txt is set to greater than this value (in\n seconds) then the fetcher will skip this page, generating an error report.\n If set to -1 the fetcher will never skip such pages and will wait the\n amount of time retrieved from robots.txt Crawl-Delay, however long that\n might be.\n </description>\n</property>\n\"\nFetcher.java:\n(http://svn.apache.org/viewvc/nutch/branches/branch-1.3/src/java/org/apache/nutch/fetcher/Fetcher.java?view=markup).\nThe line 554 in Fetcher.java: \"this.maxCrawlDelay =\nconf.getInt(\"fetcher.max.crawl.delay\", 30) * 1000;\" .\nThe lines 615-616 in Fetcher.java:\n\"\nif (rules.getCrawlDelay() > 0) {\n  if (rules.getCrawlDelay() > maxCrawlDelay) {\n\"\nNow, the documentation states that, if fetcher.max.crawl.delay is set to\n-1, the crawler will always wait the amount of time the Crawl-Delay\nparameter specifies. However, as you can see, if it really is negative\nthe condition on the line 616 is always true, which leads to skipping\nthe page whose Crawl-Delay is set.",
        "Issue Links": [
            "/jira/browse/NUTCH-1284"
        ]
    },
    "NUTCH-1043": {
        "Key": "NUTCH-1043",
        "Summary": "Add pattern for filtering .js in default url filters",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "12/Jul/11 17:26",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "18/Jul/11 09:27",
        "Description": "The Javascript parser is not used by default as it is extremely noisy, however the default URL filters do not filter out URLs ending in .js and the default parser (Tika) can't parse them. In a nutshell we are fetching URLS that we know can't be parsed.\nI suggest that we add a regex to the default URL filters. If people are interested in fetching and parsing .js files they can activate the plugin in their conf and remove the regex in the URL filters.",
        "Issue Links": []
    },
    "NUTCH-1044": {
        "Key": "NUTCH-1044",
        "Summary": "Redirected URLs and possibly all of their outlinked URLs have invalid scores.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher,                                            parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Nutch User - 1",
        "Created": "13/Jul/11 08:59",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "10/Aug/11 19:55",
        "Description": "1.: http://lucene.472066.n3.nabble.com/URL-redirection-and-zero-scores-td3085311.html\n2.: http://lucene.472066.n3.nabble.com/A-possible-solution-to-my-URL-redirection-and-zero-scores-problem-td3162164.html\nPlease note that also URLs redirected by meta refresh redirection do have invalid scores. For such URLs a CrawlDatum is created on the lines 157-177 of ParseOutputFormat.java (http://svn.apache.org/viewvc/nutch/branches/branch-1.3/src/java/org/apache/nutch/parse/ParseOutputFormat.java?view=markup). The new CrawlDatum's score isn't set anywhere after the creation so it's 1.0f as can be seen on the line 122 of CrawlDatum.java (http://svn.apache.org/viewvc/nutch/branches/branch-1.3/src/java/org/apache/nutch/crawl/CrawlDatum.java?view=markup).\nIt's another question whether the redirected URL's score should be just passed to the new URL or should the redirection be considered as a link in which case the new URL's score would be 'originalScore' / ('numberOfOutlinks' + 1).",
        "Issue Links": []
    },
    "NUTCH-1045": {
        "Key": "NUTCH-1045",
        "Summary": "MimeUtil to rely on default config provided by Tika",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "13/Jul/11 10:10",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "25/Jul/11 12:40",
        "Description": "We currently provide conf/tika-mimetypes.xml despite the fact that it is absolutely similar to the one found in tika-core.jar\nHaving a mechanism for specifying a custom tika-mimetypes.xml is good though but if the user hasn't specified one or if it can't be loaded then we should rely on Tika's default. This way we won't need to provide conf/tika-mimetypes.xml anymore and keep it in sync with the default Tika one whenever we upgrade Tika.",
        "Issue Links": []
    },
    "NUTCH-1046": {
        "Key": "NUTCH-1046",
        "Summary": "Add tests for indexing to SOLR",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "13/Jul/11 14:19",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "We currently have no tests for checking that the indexing to SOLR works as expected. Running an embedded SOLR Server within the tests would be good.",
        "Issue Links": [
            "/jira/browse/NUTCH-981"
        ]
    },
    "NUTCH-1047": {
        "Key": "NUTCH-1047",
        "Summary": "Pluggable indexing backends",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7",
        "Component/s": "indexer",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "13/Jul/11 14:26",
        "Updated": "25/Sep/13 10:29",
        "Resolved": "07/Mar/13 11:23",
        "Description": "One possible feature would be to add a new endpoint for indexing-backends and make the indexing plugable. at the moment we are hardwired to SOLR - which is OK - but as other resources like ElasticSearch are becoming more popular it would be better to handle this as plugins. Not sure about the name of the endpoint though : we already have indexing-plugins (which are about generating fields sent to the backends) and moreover the backends are not necessarily for indexing / searching but could be just an external storage e.g. CouchDB. The term backend on its own would be confusing in 2.0 as this could be pertaining to the storage in GORA. 'indexing-backend' is the best name that came to my mind so far - please suggest better ones.\nWe should come up with generic map/reduce jobs for indexing, deduplicating and cleaning and maybe add a Nutch extension point there so we can easily hook up indexing, cleaning and deduplicating for various backends.",
        "Issue Links": [
            "/jira/browse/NUTCH-1527",
            "/jira/browse/NUTCH-1528",
            "/jira/browse/NUTCH-1088",
            "/jira/browse/NUTCH-1517",
            "/jira/browse/NUTCH-1446",
            "/jira/browse/NUTCH-1139",
            "/jira/browse/NUTCH-656",
            "/jira/browse/NUTCH-1568"
        ]
    },
    "NUTCH-1048": {
        "Key": "NUTCH-1048",
        "Summary": "Busted links on http://nutch.apache.org/mailing_lists.html",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Eric Pugh",
        "Created": "13/Jul/11 21:22",
        "Updated": "19/Jul/11 19:45",
        "Resolved": "19/Jul/11 19:38",
        "Description": "Just tried to follow the links for the Agents mailing list on and get broken links.  I think the links need updating.  I'd update mailing_lists.xml to reflect the mailing lists that do come up, but not sure if something about archiving them needs to be done!",
        "Issue Links": []
    },
    "NUTCH-1049": {
        "Key": "NUTCH-1049",
        "Summary": "Add classes to bin/nutch",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "14/Jul/11 11:37",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "18/Aug/11 13:25",
        "Description": "The following classes have been added to the bin/nutch script for convenience:\n\nparsechecker\nindexchecker\ndomainstats\nwebgraph\nlinkrank\nscoreupdater\nnodedumper",
        "Issue Links": [
            "/jira/browse/NUTCH-771"
        ]
    },
    "NUTCH-1050": {
        "Key": "NUTCH-1050",
        "Summary": "Add segmentDir option to WebGraph",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "14/Jul/11 15:08",
        "Updated": "13/Sep/11 21:26",
        "Resolved": "19/Jul/11 12:50",
        "Description": "One must either merge segments first or script around the lack of a segmentDir option when creating a WebGraph. This patch adds the -segmentDir /path/to/segments option. This issue also removes the surplus newline output by the LinkRank program when iterating and adds the WebGraph classes to the log4j.properties file.",
        "Issue Links": []
    },
    "NUTCH-1051": {
        "Key": "NUTCH-1051",
        "Summary": "Export WebGraph node scores for solr.ExternalFileField",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "14/Jul/11 16:07",
        "Updated": "13/Sep/11 21:26",
        "Resolved": "16/Aug/11 16:28",
        "Description": "The current webgraph.NodeDumper dumps a flat <url>\\t<float>\\n file, which is almost exactly what is needed for using ExternalFileField in Solr. This issue tracks the option to add to dump it in the proper format. Using EFF we can update scores without reindexing millions of documents. There's one caveat, Solr won't accept an equals-sign in the key but there's a small patch for this in SOLR-2545.",
        "Issue Links": [
            "/jira/browse/NUTCH-875",
            "/jira/browse/SOLR-2545"
        ]
    },
    "NUTCH-1052": {
        "Key": "NUTCH-1052",
        "Summary": "Multiple deletes of the same URL using SolrClean",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.3,                                            1.4",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Julien Nioche",
        "Reporter": "Tim Pease",
        "Created": "14/Jul/11 21:59",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "30/Sep/11 14:10",
        "Description": "The SolrClean class does not keep track of purged URLs, it only checks the URL status for \"db_gone\". When run multiple times the same list of URLs will be deleted from Solr. For small, stable crawl databases this is not a problem. For larger crawls this could be an issue. SolrClean will become an expensive operation.\nOne solution is to add a \"purged\" flag in the CrawlDatum metadata. SolrClean would then check this flag in addition to the \"db_gone\" status before adding the URL to the delete list.\nAnother solution is to add a new state to the status field \"db_gone_and_purged\".\nEither way, the crawl DB will need to be updated after the Solr delete has successfully occurred.",
        "Issue Links": [
            "/jira/browse/NUTCH-1139"
        ]
    },
    "NUTCH-1053": {
        "Key": "NUTCH-1053",
        "Summary": "Parsing of RSS feeds fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.7",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "15/Jul/11 09:29",
        "Updated": "22/May/13 03:53",
        "Resolved": "21/May/13 00:44",
        "Description": "See discussion on http://lucene.472066.n3.nabble.com/RSS-feed-parsing-on-Nutch-1-3-td3166487.html\nHave been able to reproduce the problem and will look into it",
        "Issue Links": [
            "/jira/browse/NUTCH-1494"
        ]
    },
    "NUTCH-1054": {
        "Key": "NUTCH-1054",
        "Summary": "Make linkDB optional during indexing",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "15/Jul/11 12:00",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "18/Jul/11 09:21",
        "Description": "Having a linkDB is currently mandatory for indexing, however not all users are interested in using the anchors. The linkDB should be optional while indexing",
        "Issue Links": []
    },
    "NUTCH-1055": {
        "Key": "NUTCH-1055",
        "Summary": "upgrade package.html file in language identifier plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jul/11 13:40",
        "Updated": "19/Jul/11 04:04",
        "Resolved": "18/Jul/11 11:44",
        "Description": "package.html within the language identifier plugin contains the following... however the link is broken.\n<html>\n<body>\n<p>Text document language identifier.</p><p>Language profiles are based on material from\n<a href=\"http://www.isi.edu/~koehn/europarl/\">http://www.isi.edu/~koehn/europarl/</a>.</p>\n</body>\n</html>\nThe correct link should be\nhttp://www.homepages.inf.ed.ac.uk/pkoehn/publications/europarl.ps\nI will submit a patch.",
        "Issue Links": []
    },
    "NUTCH-1056": {
        "Key": "NUTCH-1056",
        "Summary": "Write a new plugin example for inclusion on the wiki",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jul/11 17:19",
        "Updated": "24/Aug/11 17:02",
        "Resolved": "24/Aug/11 17:02",
        "Description": "It is important that we have a comprehensive plugin example for the current release of Nutch as packages and some classes have changed enough to create confusion when attempting to write new custom plugins.\nThis also effects the following file \n/trunk/src/java/org/apache/nutch/plugin/package.html\nwhere an out of date example is mentioned which has now been moved to the archive for exactly the reasons staed above\n@see <a href=\"http://wiki.apache.org/nutch/WritingPluginExample\">\n     Writing Plugin Example\nI don't think submitting a patch to simply remove this content from the above .html document is the solution. Rather a better solution is to create a wiki resource and edit the .html file accordingly.",
        "Issue Links": [
            "/jira/browse/NUTCH-408"
        ]
    },
    "NUTCH-1057": {
        "Key": "NUTCH-1057",
        "Summary": "Make fetcher thread time out configurable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Jul/11 17:31",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "06/Sep/11 11:58",
        "Description": "The fetcher sets a time out value based of half the mapred.task.timeout value. This is not a proper value for all cases. Add an option (fetcher.thread.timeout.divisor) to configure the divisor used and default it to two.",
        "Issue Links": []
    },
    "NUTCH-1058": {
        "Key": "NUTCH-1058",
        "Summary": "Upgrade Solr schema to version 1.4",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Jul/11 14:35",
        "Updated": "04/Oct/11 04:03",
        "Resolved": "03/Oct/11 13:25",
        "Description": "The version of our Solr schema should be updated from 1.3 to the current version. I propose to commit the change prior to 1.4 and 2.0 RC's, the Solr schema version may have incremented more than once at the time of an RC.",
        "Issue Links": []
    },
    "NUTCH-1059": {
        "Key": "NUTCH-1059",
        "Summary": "Remove convdb command from /bin/nutch",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Jul/11 16:33",
        "Updated": "18/Jul/11 11:23",
        "Resolved": "18/Jul/11 11:23",
        "Description": "There is no class shipped with >=Nutch 1.3 for the Convdb command therefore I'm assuming this command somehow slipped through the net undetected. I will attach a trivial patch simply removing it from the bin/nutch script",
        "Issue Links": []
    },
    "NUTCH-1060": {
        "Key": "NUTCH-1060",
        "Summary": "URL filters to produce regexes to be used by OutlinkExtractor.",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "19/Jul/11 09:53",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The problem:\nOutlinkExtractor produces many URL's from plain text using an advanced regular expression:\n\n([A-Za-z][A-Za-z0-9+.-]{1,120}:[A-Za-z0-9/](([A-Za-z0-9$_.+!*,;/?:@&~=-])|%[A-Fa-f0-9]{2}){1,333}(#([a-zA-Z0-9][a-zA-Z0-9$_.+!*,;/?:@&~=%-]{0,1000}))?)\n\n\nThis expression does not take into account the various non-regex-based URL filters such as prefix, domain and suffix and thus produces URL's that are going to be filtered out by some filter. This, however, becomes a problem when parsing millions of documents that are being processed by the OutlinkExtractor (when case parse-html|parse-tika do not produce any outlinks). Large bodies of full text usually contain a lot of sequences that are extracted as URL's. Many of which are thought to be part of an URI schema such as:\nid:123\nsays:what\nuser:doe\nupdate:tue-19-jul\nThe above examples can be easily remedied by using a configured prefix URL filter. It may, however, be an even better idea to prevent the extraction of these URL's at the first place. No extraction means filtering less URL's and potentially saving a lot of data.\nComments? I'll see if i can produce a patch.",
        "Issue Links": []
    },
    "NUTCH-1061": {
        "Key": "NUTCH-1061",
        "Summary": "Migrate MoreIndexingFilter from Apache ORO to java.util.regex",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Jul/11 12:01",
        "Updated": "22/May/13 03:53",
        "Resolved": "10/Nov/11 15:03",
        "Description": "Here's a migrating resetTitle method to use Apache ORO. There was no unit test for this method so i added it. The test passes with old Apache ORO impl. and with the new j.u.regex impl.\nPlease comment.",
        "Issue Links": [
            "/jira/browse/NUTCH-1014"
        ]
    },
    "NUTCH-1062": {
        "Key": "NUTCH-1062",
        "Summary": "Migrate BasicURLNormalizer from Apache ORO to java.util.regex",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Jul/11 12:10",
        "Updated": "17/Sep/15 07:04",
        "Resolved": "22/Apr/15 21:08",
        "Description": "Issue for migration from ORO to j.u.regex. There is a small problem here. I began the migration mostly because of the double slash issue using lookback which was not supported in ORO. This was to prevent the URL schema from being reduced to one slash. The current Basic URL Normalizer has this problem built-in!\n\n        // this pattern tries to find spots like \"xx//yy\" in the url,\n        // which could be replaced by a \"/\"\n        adjacentSlashRule = new Rule();\n        adjacentSlashRule.pattern = (Perl5Pattern)      \n          compiler.compile(\"/{2,}\", Perl5Compiler.READ_ONLY_MASK);     \n        adjacentSlashRule.substitution = new Perl5Substitution(\"/\");\n\n\nBut provides the wrong solution as it touches the schema as well. What to do? Migrate to j.u.regex and keep this `feature` intact? \nedit: reading more it looks like it is being fixed at a later stage. A slash is added for URI schema's http & ftp.",
        "Issue Links": [
            "/jira/browse/NUTCH-1014"
        ]
    },
    "NUTCH-1063": {
        "Key": "NUTCH-1063",
        "Summary": "OutlinkExtractor test generates an exception but does not fail",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "19/Jul/11 14:11",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "13/Oct/18 10:21",
        "Description": "Testsuite: org.apache.nutch.parse.TestOutlinkExtractor\nTests run: 4, Failures: 0, Errors: 0, Time elapsed: 0.043 sec\n------------- Standard Output ---------------\n2011-07-19 15:06:36,073 ERROR parse.OutlinkExtractor (OutlinkExtractor.java:getOutlinks(121)) - getOutlinks\njava.lang.NullPointerException\n\tat org.apache.oro.text.regex.PatternMatcherInput.<init>(Unknown Source)\n\tat org.apache.nutch.parse.OutlinkExtractor.getOutlinks(OutlinkExtractor.java:95)\n\tat org.apache.nutch.parse.OutlinkExtractor.getOutlinks(OutlinkExtractor.java:72)\n\tat org.apache.nutch.parse.TestOutlinkExtractor.testGetNoOutlinks(TestOutlinkExtractor.java:40)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat junit.framework.TestCase.runTest(TestCase.java:168)\n\tat junit.framework.TestCase.runBare(TestCase.java:134)\n\tat junit.framework.TestResult$1.protect(TestResult.java:110)\n\tat junit.framework.TestResult.runProtected(TestResult.java:128)\n\tat junit.framework.TestResult.run(TestResult.java:113)\n\tat junit.framework.TestCase.run(TestCase.java:124)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:232)\n\tat junit.framework.TestSuite.run(TestSuite.java:227)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:79)\n\tat junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)\n\tat org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:785)",
        "Issue Links": [
            "/jira/browse/NUTCH-1021",
            "/jira/browse/NUTCH-2192"
        ]
    },
    "NUTCH-1064": {
        "Key": "NUTCH-1064",
        "Summary": "o.a.n.util.MimeUtil uses deprecated Tika methods",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "19/Jul/11 15:07",
        "Updated": "22/May/13 03:53",
        "Resolved": "02/Jan/12 12:00",
        "Description": "this class is in serious need of refactoring as the underlying Tika API has changed a lot. The logic around what strategies to use e.g. trust the metadata returned by the server? trust Tika's detection? etc... should be reimplemented using the Detector implementations",
        "Issue Links": [
            "/jira/browse/NUTCH-1230",
            "/jira/browse/NUTCH-1017",
            "/jira/browse/NUTCH-1041"
        ]
    },
    "NUTCH-1065": {
        "Key": "NUTCH-1065",
        "Summary": "New mvn.template",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Jul/11 09:50",
        "Updated": "05/Aug/11 04:03",
        "Resolved": "04/Aug/11 10:26",
        "Description": "Removal of Otis from mvn.template file and addition of myself. This does not alter functionality of any mvn or ivy tasks or files.",
        "Issue Links": []
    },
    "NUTCH-1066": {
        "Key": "NUTCH-1066",
        "Summary": "trivial correction of domain-urlfilter.txt",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Jul/11 09:58",
        "Updated": "22/Jul/11 15:53",
        "Resolved": "22/Jul/11 15:50",
        "Description": "Trivial spelling correction in domain-urlfilter.txt in both trunk and branch-1.4\nThe attached patches simply correct this.",
        "Issue Links": []
    },
    "NUTCH-1067": {
        "Key": "NUTCH-1067",
        "Summary": "Configure minimum throughput for fetcher",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "22/Jul/11 14:31",
        "Updated": "06/Mar/12 10:59",
        "Resolved": "16/Sep/11 11:19",
        "Description": "Large fetches can contain a lot of url's for the same domain. These can be very slow to crawl due to politeness from robots.txt, e.g. 10s per url. If all other url's have been fetched, these queue's can stall the entire fetcher, 60 url's can then take 10 minutes or even more. This can usually be dealt with using the time bomb but the time bomb value is hard to determine.\nThis patch adds a fetcher.throughput.threshold setting meaning the minimum number of pages per second before the fetcher gives up. It doesn't use the global number of pages / running time but records the actual pages processed in the previous second. This value is compared with the configured threshold.\nBesides the check the fetcher's status is also updated with the actual number of pages per second and bytes per second.",
        "Issue Links": []
    },
    "NUTCH-1068": {
        "Key": "NUTCH-1068",
        "Summary": "Automaton performance improvements based on Lucene code base",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kirby Bohling",
        "Created": "25/Jul/11 16:32",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "19/May/14 20:37",
        "Description": "The Lucene team maintains a modified Automaton library cut down to precisely what they need.  It can have significant performance enhancements.\nI am attempting to backport and shepherd a patch for the original Automaton library.\nThe original Lucene code is here:\nhttp://svn.apache.org/viewvc/lucene/dev/trunk/lucene/src/java/org/apache/lucene/util/automaton/\nThe Lucene code is likely slightly faster, as it includes several micro optimizations I removed to avoid having to request re-license permission.  I would definitely performance test using the Lucene RegEx vs. the patched code.  The Lucene code also uses code points not characters, which might make a difference for UTF-16 vs. UTF-32 in obscure cases (I believe the Lucene code builds a UTF-32 clean DFA for accuracy, and then translates it to a UTF-8 DFA for performance but I'm not 100% sure.  I don't need/use any of that code, and currently really only worried about ASCII DFAs).\nWhen making heavy use of the NFA-to-DFA transformation, I see a 4x speed up.  It likely has a 1.5-2x speed up for regular expression execution from what I can tell.  The Nutch backend uses this code in a couple of places, and it likely would lead to performance benefits for those areas.\nI will attach my backported version for the Automaton 1.11-7 release.  While I don't own any of the copyright, all of the code is copyrighted under the BSD license, or the ASF 2.0 license.  It is pretty obviously approved for ASF usage.  I am not checking that the patch is usable as I'm not the copyright holder.  If that is an issue, I'll say \"yes\", I just don't believe I have any legal standing to do so.  I don't want to create licensing issues for the ASF.",
        "Issue Links": []
    },
    "NUTCH-1069": {
        "Key": "NUTCH-1069",
        "Summary": "Readlinkdb broken on Hadoop > 0.20",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "linkdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "25/Jul/11 20:56",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "11/Aug/11 16:39",
        "Description": "reading the linkdb doesn't work on Hadoop 0.20+. It believes data is to be read from the _SUCCESS file that is written by newer Hadoop version.\nQuick fix is to remove the _SUCCESS file",
        "Issue Links": [
            "/jira/browse/NUTCH-1029"
        ]
    },
    "NUTCH-1070": {
        "Key": "NUTCH-1070",
        "Summary": "Run nutch under native windows (no cygwin)",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.3",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Radim Kolar",
        "Created": "27/Jul/11 17:07",
        "Updated": "04/Nov/11 16:34",
        "Resolved": "03/Nov/11 22:36",
        "Description": "Its possible to run Nutch in windows without cygwin. \n1. Startup script needs to be ported from SH to BAT\n2. Because hadoop runs on unix only, we must emulate unix commands to make it work. Luckily only chmod, bash and df needs to be emulated",
        "Issue Links": []
    },
    "NUTCH-1071": {
        "Key": "NUTCH-1071",
        "Summary": "Crawldb update to total counts per status",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "28/Jul/11 13:52",
        "Updated": "28/Jul/11 14:04",
        "Resolved": "28/Jul/11 13:55",
        "Description": "The reduce phase of the crawldb update outputs all the entries that will be found in the updated crawldb. We can use the counters to summarise the number of URLs per status, which is a bit like the readdb -stats functionality except that it does not require an additional step. \nThis is a useful way of monitoring the progress of a crawl using the Hadoop JobTracker UI.",
        "Issue Links": []
    },
    "NUTCH-1072": {
        "Key": "NUTCH-1072",
        "Summary": "Display number and size of queues in Fetcher status",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "28/Jul/11 14:53",
        "Updated": "28/Jul/11 15:01",
        "Resolved": "28/Jul/11 15:01",
        "Description": "Knowing about the number of queues gives a better idea of the distribution of the fetch list per [host|domain|IP].\nAs for the size of the queues, it gets useful when the feeder has finished reading form the input (i.e. can see 100% in the column complete) to assess how soon the Fetch is going to finish.",
        "Issue Links": []
    },
    "NUTCH-1073": {
        "Key": "NUTCH-1073",
        "Summary": "Rename parameters 'fetcher.threads.per.host.by.ip' and 'fetcher.threads.per.host'",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "29/Jul/11 07:48",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "01/Sep/11 13:08",
        "Description": "The parameters 'fetcher.threads.per.host.by.ip' and 'fetcher.threads.per.host' should be renamed into 'fetcher.queue.mode' and 'fetcher.threads.per.queue' as it done in trunk.\nThis would better reflect the way the Fetcher works, would allow to group by domain and is easier to understand.",
        "Issue Links": []
    },
    "NUTCH-1074": {
        "Key": "NUTCH-1074",
        "Summary": "topN is ignored with maxNumSegments",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Aug/11 14:07",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "23/Sep/11 12:10",
        "Description": "When generating segments with topN and maxNumSegments, topN is not respected. It looks like the first generated segment contains topN * maxNumSegments of URLs's, at least the number of map input records roughly matches.",
        "Issue Links": [
            "/jira/browse/NUTCH-762"
        ]
    },
    "NUTCH-1075": {
        "Key": "NUTCH-1075",
        "Summary": "Delegate language identification to Tika",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "02/Aug/11 06:20",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "19/Aug/11 13:11",
        "Description": "In 2.0 the language identification is delegated to Tika and is done as part of the parsing step (and not during the indexing as done currently).\nThe patch attached is a backport from trunk which implements this and adds a new parameter to determine the strategy to use\n\n \n<property>\n  <name>lang.extraction.policy</name>\n  <value>detect,identify</value>\n  <description>This determines when the plugin uses detection and\n  statistical identification mechanisms. The order in which the\n  detect and identify are written will determine the extraction\n  policy. Default case (detect,identify)  means the plugin will\n  first try to extract language info from page headers and metadata,\n  if this is not successful it will try using tika language\n  identification. Possible values are:\n    detect\n    identify\n    detect,identify\n    identify,detect\n  </description>\n</property>",
        "Issue Links": []
    },
    "NUTCH-1076": {
        "Key": "NUTCH-1076",
        "Summary": "Solrindex has no documents following bin/nutch solrindex when using protocol-file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.3",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Seth Griffin",
        "Created": "05/Aug/11 22:01",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Oct/19 13:00",
        "Description": "Note: When using protocol-http I am able to update solr effortlessly.\nTo test this I have a single pdf file that I am trying to index in my urls directory.\nI execute:\nbin/nutch crawl urls\nOutput:\nsolrUrl is not set, indexing will be skipped...\ncrawl started in: crawl-20110805151045\nrootUrlDir = urls\nthreads = 10\ndepth = 5\nsolrUrl=null\nInjector: starting at 2011-08-05 15:10:45\nInjector: crawlDb: crawl-20110805151045/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2011-08-05 15:10:48, elapsed: 00:00:02\nGenerator: starting at 2011-08-05 15:10:48\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: crawl-20110805151045/segments/20110805151050\nGenerator: finished at 2011-08-05 15:10:51, elapsed: 00:00:03\nFetcher: Your 'http.agent.name' value should be listed first in 'http.robots.agents' property.\nFetcher: starting at 2011-08-05 15:10:51\nFetcher: segment: crawl-20110805151045/segments/20110805151050\nFetcher: threads: 10\nQueueFeeder finished: total 1 records + hit by time limit :0\nfetching file:///home/nutch/nutch-1.3/runtime/local/indexdir/Altec.pdf\n-finishing thread FetcherThread, activeThreads=9\n-finishing thread FetcherThread, activeThreads=8\n-finishing thread FetcherThread, activeThreads=7\n-finishing thread FetcherThread, activeThreads=6\n-finishing thread FetcherThread, activeThreads=5\n-finishing thread FetcherThread, activeThreads=4\n-finishing thread FetcherThread, activeThreads=3\n-finishing thread FetcherThread, activeThreads=2\n-finishing thread FetcherThread, activeThreads=1\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0\n-activeThreads=0\nFetcher: finished at 2011-08-05 15:10:53, elapsed: 00:00:02\nParseSegment: starting at 2011-08-05 15:10:53\nParseSegment: segment: crawl-20110805151045/segments/20110805151050\nParseSegment: finished at 2011-08-05 15:10:56, elapsed: 00:00:03\nCrawlDb update: starting at 2011-08-05 15:10:56\nCrawlDb update: db: crawl-20110805151045/crawldb\nCrawlDb update: segments: [crawl-20110805151045/segments/20110805151050]\nCrawlDb update: additions allowed: true\nCrawlDb update: URL normalizing: true\nCrawlDb update: URL filtering: true\nCrawlDb update: Merging segment data into db.\nCrawlDb update: finished at 2011-08-05 15:10:57, elapsed: 00:00:01\nGenerator: starting at 2011-08-05 15:10:57\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: 0 records selected for fetching, exiting ...\nStopping at depth=1 - no more URLs to fetch.\nLinkDb: starting at 2011-08-05 15:10:58\nLinkDb: linkdb: crawl-20110805151045/linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\nLinkDb: adding segment: file:/home/nutch/nutch-1.3/runtime/local/crawl-20110805151045/segments/20110805151050\nLinkDb: finished at 2011-08-05 15:10:59, elapsed: 00:00:01\ncrawl finished: crawl-20110805151045\nThen with a clean solr index (stats output from stats.jsp below):\nsearcherName : Searcher@14dd758 main\ncaching : true\nnumDocs : 0\nmaxDoc : 0\nreader : SolrIndexReader\n{this=1ee148b,r=ReadOnlyDirectoryReader@1ee148b,refCnt=1,segments=0}\nreaderDir : org.apache.lucene.store.NIOFSDirectory@/home/solr/apache-solr-3.1.0/example/solr/data/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@987197\nindexVersion : 1312575204101\nopenedAt : Fri Aug 05 15:13:24 CDT 2011\nregisteredAt : Fri Aug 05 15:13:24 CDT 2011\nwarmupTime : 0 \n\nI then execute:\n\nbin/nutch solrindex http://localhost:8983/solr/ crawl-20110805151045/crawldb/ crawl-20110805151045/linkdb/ crawl-20110805151045/segments/*\n\nbin/nutch output:\n\nSolrIndexer: starting at 2011-08-05 15:15:48\nSolrIndexer: finished at 2011-08-05 15:15:50, elapsed: 00:00:01\n\nsolr output:\n\nAug 5, 2011 3:15:50 PM org.apache.solr.update.DirectUpdateHandler2 commit\nINFO: start commit(optimize=false,waitFlush=true,waitSearcher=true,expungeDeletes=false)\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher <init>\nINFO: Opening Searcher@15f1f9c main\nAug 5, 2011 3:15:50 PM org.apache.solr.update.DirectUpdateHandler2 commit\nINFO: end_commit_flush\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@15f1f9c main from Searcher@14dd758 main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@15f1f9c main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@15f1f9c main from Searcher@14dd758 main\n\tfilterCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@15f1f9c main\n\tfilterCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@15f1f9c main from Searcher@14dd758 main\n\tqueryResultCache{lookups=0,hits=0,hitratio=0.00,inserts=1,evictions=0,size=1,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@15f1f9c main\n\tqueryResultCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming Searcher@15f1f9c main from Searcher@14dd758 main\n\tdocumentCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher warm\nINFO: autowarming result for Searcher@15f1f9c main\n\tdocumentCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.core.QuerySenderListener newSearcher\nINFO: QuerySenderListener sending requests to Searcher@15f1f9c main\nAug 5, 2011 3:15:50 PM org.apache.solr.core.QuerySenderListener newSearcher\nINFO: QuerySenderListener done.\nAug 5, 2011 3:15:50 PM org.apache.solr.core.SolrCore registerSearcher\nINFO: [] Registered new searcher Searcher@15f1f9c main\nAug 5, 2011 3:15:50 PM org.apache.solr.search.SolrIndexSearcher close\nINFO: Closing Searcher@14dd758 main\n\tfieldValueCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\n\tfilterCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\n\tqueryResultCache{lookups=0,hits=0,hitratio=0.00,inserts=1,evictions=0,size=1,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\n\tdocumentCache{lookups=0,hits=0,hitratio=0.00,inserts=0,evictions=0,size=0,warmupTime=0,cumulative_lookups=0,cumulative_hits=0,cumulative_hitratio=0.00,cumulative_inserts=0,cumulative_evictions=0}\nAug 5, 2011 3:15:50 PM org.apache.solr.update.processor.LogUpdateProcessor finish\nINFO: {commit=} 0 8\nAug 5, 2011 3:15:50 PM org.apache.solr.core.SolrCore execute\nINFO: [] webapp=/solr path=/update params={waitSearcher=true&waitFlush=true&wt=javabin&commit=true&version=2} status=0 QTime=8\n\noutput from stats.jsp:\n\nstats: \t\nsearcherName : Searcher@15f1f9c main\ncaching : true\nnumDocs : 0\nmaxDoc : 0\nreader : SolrIndexReader{this=1ee148b,r=ReadOnlyDirectoryReader@1ee148b,refCnt=1,segments=0}\nreaderDir : org.apache.lucene.store.NIOFSDirectory@/home/solr/apache-solr-3.1.0/example/solr/data/index lockFactory=org.apache.lucene.store.NativeFSLockFactory@987197\nindexVersion : 1312575204101\nopenedAt : Fri Aug 05 15:15:50 CDT 2011\nregisteredAt : Fri Aug 05 15:15:50 CDT 2011\nwarmupTime : 2",
        "Issue Links": [
            "/jira/browse/NUTCH-1483",
            "/jira/browse/NUTCH-1483"
        ]
    },
    "NUTCH-1077": {
        "Key": "NUTCH-1077",
        "Summary": "Nutch 2 DbUpdateMapper throws ArrayOutOfBoundsException when running update",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Tom Davidson",
        "Created": "09/Aug/11 17:18",
        "Updated": "11/May/12 09:12",
        "Resolved": "11/May/12 09:12",
        "Description": "I got this error when running a simple nutch update after doing  a small fetch and parse.\njava.lang.ArrayIndexOutOfBoundsException: 0\n        at org.apache.nutch.util.TableUtil.reverseAppendSplits(TableUtil.java:126)\n        at org.apache.nutch.util.TableUtil.reverseUrl(TableUtil.java:66)\n        at org.apache.nutch.util.TableUtil.reverseUrl(TableUtil.java:43)\n        at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:70)\n        at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:36)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:647)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:270)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1127)\n        at org.apache.hadoop.mapred.Child.main(Child.java:264)",
        "Issue Links": []
    },
    "NUTCH-1078": {
        "Key": "NUTCH-1078",
        "Summary": "Upgrade all instances of commons logging to slf4j (with log4j backend)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Aug/11 14:26",
        "Updated": "02/May/13 02:29",
        "Resolved": "29/Sep/11 12:49",
        "Description": "Whilst working on another issue, I noticed that some classes still import and use commons logging for example HttpBase.java\n\nimport java.util.*;\n\n// Commons Logging imports\nimport org.apache.commons.logging.Log;\nimport org.apache.commons.logging.LogFactory;\n\n// Nutch imports\nimport org.apache.nutch.crawl.CrawlDatum;\n\n\nAt this stage I am unsure how many (if any others) still import and reply upon commons logging, however they should be upgraded to slf4j for branch-1.4.",
        "Issue Links": [
            "/jira/browse/NUTCH-1091"
        ]
    },
    "NUTCH-1079": {
        "Key": "NUTCH-1079",
        "Summary": "StringBuffer converted to StringBuilder",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Karthik K",
        "Created": "11/Aug/11 09:15",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "18/Apr/14 08:28",
        "Description": "All across the codebase, it contains StringBuffer, when thread-safety is probably not intended. \nThis patch replaces StringBuffer to StringBuilder, as applicable.",
        "Issue Links": []
    },
    "NUTCH-1080": {
        "Key": "NUTCH-1080",
        "Summary": "Type safe members , arguments for better readability",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "fetcher",
        "Assignee": "Tejas Patil",
        "Reporter": "Karthik K",
        "Created": "11/Aug/11 10:28",
        "Updated": "12/Jan/15 21:09",
        "Resolved": "12/Jan/15 21:09",
        "Description": "Enable generics for some of the API, for better type safety and readability, in the process.",
        "Issue Links": []
    },
    "NUTCH-1081": {
        "Key": "NUTCH-1081",
        "Summary": "ant tests fail",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher,                                            generator,                                            injector,                                            storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Aug/11 10:39",
        "Updated": "15/Jun/12 15:58",
        "Resolved": "15/Jun/12 15:58",
        "Description": "The following tests fail when running ant test on trunk 2.0\n\n    [junit] Running org.apache.nutch.api.TestAPI\n    [junit] Tests run: 4, Failures: 1, Errors: 0, Time elapsed: 11.028 sec\n    [junit] Test org.apache.nutch.api.TestAPI FAILED\n    [junit] Running org.apache.nutch.crawl.TestGenerator\n    [junit] Tests run: 4, Failures: 0, Errors: 4, Time elapsed: 0.478 sec\n    [junit] Test org.apache.nutch.crawl.TestGenerator FAILED\n    [junit] Running org.apache.nutch.crawl.TestInjector\n    [junit] Tests run: 1, Failures: 0, Errors: 1, Time elapsed: 0.474 sec\n    [junit] Test org.apache.nutch.crawl.TestInjector FAILED\n    [junit] Running org.apache.nutch.fetcher.TestFetcher\n    [junit] Tests run: 2, Failures: 0, Errors: 2, Time elapsed: 0.526 sec\n    [junit] Test org.apache.nutch.fetcher.TestFetcher FAILED\n    [junit] Running org.apache.nutch.storage.TestGoraStorage\n    [junit] Tests run: 2, Failures: 0, Errors: 2, Time elapsed: 0.468 sec\n    [junit] Test org.apache.nutch.storage.TestGoraStorage FAILED",
        "Issue Links": [
            "/jira/browse/NUTCH-896"
        ]
    },
    "NUTCH-1082": {
        "Key": "NUTCH-1082",
        "Summary": "IndexingFiltersChecker utility does not list multi valued fields",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "15/Aug/11 13:43",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "16/Aug/11 12:03",
        "Description": "The indexing filters checker does not list all values for a multi valued field. It uses the NutchDocument.getFieldValue() method that always returns null or the first entry of the list of values.",
        "Issue Links": [
            "/jira/browse/NUTCH-1038"
        ]
    },
    "NUTCH-1083": {
        "Key": "NUTCH-1083",
        "Summary": "ParserChecker implements Tool",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "16/Aug/11 13:16",
        "Updated": "16/Aug/11 14:01",
        "Resolved": "16/Aug/11 14:01",
        "Description": "This will allow us to pass parameters on the command line when using ParserChecker instead of having to modify nutch-site.xml. Useful for debugging a parser e.g. -D parser.timeout=-1 etc...",
        "Issue Links": []
    },
    "NUTCH-1084": {
        "Key": "NUTCH-1084",
        "Summary": "ReadDB url throws exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.3",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Aug/11 16:56",
        "Updated": "18/Nov/16 10:10",
        "Resolved": "18/Nov/16 10:09",
        "Description": "Upgrade your Apache Hadoop cluster to 2.7.3 or higher to solve the problem.\nReaddb -url suffers from two problems:\n1. it trips over the _SUCCESS file generated by newer Hadoop version\n2. throws can't find class: org.apache.nutch.protocol.ProtocolStatus (???)\nThe first problem can be remedied by not allowing the injector or updater to write the _SUCCESS file. Until now that's the solution implemented for similar issues. I've not been successful as to make the Hadoop readers simply skip the file.\nThe second issue seems a bit strange and did not happen on a local check out. I'm not yet sure whether this is a Hadoop issue or something being corrupt in the CrawlDB. Here's the stack trace:\n\nException in thread \"main\" java.io.IOException: can't find class: org.apache.nutch.protocol.ProtocolStatus because org.apache.nutch.protocol.ProtocolStatus\n        at org.apache.hadoop.io.AbstractMapWritable.readFields(AbstractMapWritable.java:204)\n        at org.apache.hadoop.io.MapWritable.readFields(MapWritable.java:146)\n        at org.apache.nutch.crawl.CrawlDatum.readFields(CrawlDatum.java:278)\n        at org.apache.hadoop.io.SequenceFile$Reader.getCurrentValue(SequenceFile.java:1751)\n        at org.apache.hadoop.io.MapFile$Reader.get(MapFile.java:524)\n        at org.apache.hadoop.mapred.MapFileOutputFormat.getEntry(MapFileOutputFormat.java:105)\n        at org.apache.nutch.crawl.CrawlDbReader.get(CrawlDbReader.java:383)\n        at org.apache.nutch.crawl.CrawlDbReader.readUrl(CrawlDbReader.java:389)\n        at org.apache.nutch.crawl.CrawlDbReader.main(CrawlDbReader.java:514)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
        "Issue Links": [
            "/jira/browse/HADOOP-12406"
        ]
    },
    "NUTCH-1085": {
        "Key": "NUTCH-1085",
        "Summary": "Nutch script does not require HADOOP_HOME",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "22/Aug/11 11:41",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "23/Aug/11 15:19",
        "Description": "The Nutch script currently requires HADOOP_HOME to be set and point to a valid HADOOP setup in order to run in distributed mode. What is actually needs is not the location of the whole Hadoop setup but just to be able to call the executable 'hadoop'. This requires the users to add HADOOP_HOME/bin to their path. \nMoreover when using Nutch on Hadoop distribs such as Cloudera's CDH, the various parts of Hadoop are located in a separate directories so the concept of HADOOP_HOME is not necessarily relevant to find the hadoop executable.",
        "Issue Links": []
    },
    "NUTCH-1086": {
        "Key": "NUTCH-1086",
        "Summary": "Rewrite protocol-httpclient",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.20",
        "Component/s": "protocol",
        "Assignee": "Fabio Santagostino",
        "Reporter": "Markus Jelsma",
        "Created": "22/Aug/11 15:31",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "There are several issues about protocol-httpclient and several comments about rewriting the plugin with the new http client libraries. There is, however, not yet an issue for rewriting/reimplementing protocol-httpclient.\nhttp://hc.apache.org/httpcomponents-client-ga/",
        "Issue Links": [
            "/jira/browse/NUTCH-2059"
        ]
    },
    "NUTCH-1087": {
        "Key": "NUTCH-1087",
        "Summary": "Deprecate crawl command and replace with example script",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Markus Jelsma",
        "Created": "23/Aug/11 11:40",
        "Updated": "03/Oct/13 10:34",
        "Resolved": "20/Oct/12 08:51",
        "Description": "remove the crawl command\nadd basic crawl shell script\n\nSee thread:\nhttp://www.mail-archive.com/dev@nutch.apache.org/msg03848.html",
        "Issue Links": [
            "/jira/browse/NUTCH-1621"
        ]
    },
    "NUTCH-1088": {
        "Key": "NUTCH-1088",
        "Summary": "Write Solr XML documents",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Aug/11 12:15",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Documents need to be reindexed when index-time analysis is modified. Indexing individual segments from Nutch is tedious, especially for small segments. This issue should add a feature that can write XML batches.",
        "Issue Links": [
            "/jira/browse/NUTCH-1047"
        ]
    },
    "NUTCH-1089": {
        "Key": "NUTCH-1089",
        "Summary": "short compressed pages caused Exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "simone frenzel",
        "Created": "23/Aug/11 14:49",
        "Updated": "01/Oct/13 12:04",
        "Resolved": "23/Aug/11 15:52",
        "Description": "Hi,\ntested nutch on compressed pages, and on pages with Basic Auth and compression. On short compressed pages this Exception is thrown: \n2011-08-19 17:06:55,190 ERROR httpclient.Http - java.io.IOException: unzipBestEffort returned null\n2011-08-19 17:06:55,190 ERROR httpclient.Http - at org.apache.nutch.protocol.http.api.HttpBase.processGzipEncoded(HttpBase.java:310)\n2011-08-19 17:06:55,191 ERROR httpclient.Http - at org.apache.nutch.protocol.httpclient.HttpResponse.<init>(HttpResponse.java:163)\n2011-08-19 17:06:55,191 ERROR httpclient.Http - at org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:154)\n2011-08-19 17:06:55,191 ERROR httpclient.Http - at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:138)\n2011-08-19 17:06:55,191 ERROR httpclient.Http - at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:628)\nIn same cases Basic Auth failt also. \nWorks fine with the patch.",
        "Issue Links": [
            "/jira/browse/NUTCH-1647"
        ]
    },
    "NUTCH-1090": {
        "Key": "NUTCH-1090",
        "Summary": "LinkDb (invertlinks) should inform the user when it ignores internal links",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.5",
        "Component/s": "linkdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Marek Bachmann",
        "Created": "24/Aug/11 14:08",
        "Updated": "22/May/13 03:54",
        "Resolved": "15/Nov/11 11:56",
        "Description": "I used nutch to crawl sites on a single domain. After the crawl was complete I tried to build a LinkDb. The LinkDb was empty. \nIt comes up that this happens because the invertlinks command ignores internal links to the same domain by default. \nUnfortunately the LinkDb class doesn't tell anything about that. So it was hard to find out why the LinkDb was empty. \nI suggest to add an information for the user when the invertlinks command is ignoring internal links.",
        "Issue Links": []
    },
    "NUTCH-1091": {
        "Key": "NUTCH-1091",
        "Summary": "Remove commons logging dependency from Nutch branch and trunk",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Aug/11 17:56",
        "Updated": "02/May/13 02:29",
        "Resolved": "30/Sep/11 10:44",
        "Description": "Once all logging has been shifted to slf4j with log4j backend as per NUTCH-1078, we should deprecate the ivy dependency on common logging.",
        "Issue Links": [
            "/jira/browse/NUTCH-1078"
        ]
    },
    "NUTCH-1092": {
        "Key": "NUTCH-881 Good quality documentation for Nutch",
        "Summary": "overhaul FAQ's and publish to Nutch site",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Aug/11 18:36",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "22/Sep/11 15:05",
        "Description": "We require a complete overhaul of the FAQ's on the Wiki. Once this is accomplished they need to be pushed into the Nutch site.",
        "Issue Links": []
    },
    "NUTCH-1093": {
        "Key": "NUTCH-881 Good quality documentation for Nutch",
        "Summary": "create core documentation",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Aug/11 18:45",
        "Updated": "24/Sep/11 11:23",
        "Resolved": "24/Sep/11 11:23",
        "Description": "We should provide core documentation for Nutch 1.4 (main stream Nutch users), a good example of how this would be structured can be found here [1]. \nFurther to this, core documentation for Nutch trunk 2.0 should be kept separately shipped in a /docs directory and stored via svn. This issue and what this encompasses is covered in the next sub-task.   \n[1] http://incubator.apache.org/gora/docs/current/index.html",
        "Issue Links": []
    },
    "NUTCH-1094": {
        "Key": "NUTCH-881 Good quality documentation for Nutch",
        "Summary": "create comprehensive documentation for Nutchgora branch",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.2",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Aug/11 18:47",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jan/13 18:42",
        "Description": "This should shadow the core documentation for Nutch 1.4 (branch) and mainstream users, however it should include fundamentals specific to Nutch trunk. Until we release Nutch 2.0 this documentation should be stored in svn under a /docs directory.",
        "Issue Links": []
    },
    "NUTCH-1095": {
        "Key": "NUTCH-1095",
        "Summary": "remove i18n from Nutch site to archive and legacy secton of wiki",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "documentation,                                            web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Aug/11 18:52",
        "Updated": "08/Sep/11 17:33",
        "Resolved": "08/Sep/11 17:33",
        "Description": "This has been deprecated since release of Nutch 1.3 as we no longer use the Nutch webapp within Tomcat or another servlet container.",
        "Issue Links": []
    },
    "NUTCH-1096": {
        "Key": "NUTCH-1096",
        "Summary": "Empty (not null) ContentLength results in failure of fetch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "25/Aug/11 12:48",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "01/Sep/11 15:15",
        "Description": "In rare occasions, servers return an empty string ContentLength, which results in a fetch failure. One could argue whether the fetch is allowed to proceed in these cases. I for one believe it is. (Just like the cases where the header is null or not properly trimmed).\nPatch will be right up.",
        "Issue Links": [
            "/jira/browse/NUTCH-1039"
        ]
    },
    "NUTCH-1097": {
        "Key": "NUTCH-1097",
        "Summary": "application/xhtml+xml should be enabled in plugin.xml of parse-html; allow multiple mimetypes for plugin.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Ferdy",
        "Created": "25/Aug/11 14:10",
        "Updated": "31/Oct/11 13:00",
        "Resolved": "12/Oct/11 18:22",
        "Description": "The configuration in parse-plugins.xml expects the parse-html plugin to accept application/xhtml+xml, however the plugin.xml of this plugin does not list this type. Either change the entry in parse-plugins.xml or change the parse-html plugin.xml. I suggest the latter. See patch.",
        "Issue Links": []
    },
    "NUTCH-1098": {
        "Key": "NUTCH-1098",
        "Summary": "better url-normalizer basic",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.5",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Radim Kolar",
        "Created": "25/Aug/11 23:31",
        "Updated": "05/Nov/11 05:42",
        "Resolved": "04/Nov/11 13:32",
        "Description": "Basic URL normalizer lacks 2 important features\nEncode space in URL into %20 to unbreak httpclient and possibly others who do not expect space inside URL\nAbility to decode %33 encoding in URL. This is important for avoiding duplicates",
        "Issue Links": []
    },
    "NUTCH-1099": {
        "Key": "NUTCH-1099",
        "Summary": "Add HBase and Cassandra storage properties to nutch-default.xml",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "29/Aug/11 18:56",
        "Updated": "12/Sep/11 04:10",
        "Resolved": "11/Sep/11 16:40",
        "Description": "I was getting fed up manually adding the properties for HBase and Cassandra to nutch-site.xml manually and thought if we could at least add them to nutch-default.xml then comment them out then it would be a simply copy paste job rather than manually fetching the content from somewhere else I had it stored. N.B. this changes no functionality, just makes people lives a bit easier.",
        "Issue Links": []
    },
    "NUTCH-1100": {
        "Key": "NUTCH-1100",
        "Summary": "SolrDedup broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.9",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "30/Aug/11 12:50",
        "Updated": "12/Nov/13 00:32",
        "Resolved": "11/Nov/13 16:01",
        "Description": "Some Solr indices are unable to be deduped from Nutch. For unknown reasons Nutch will throw the exception below. There are no peculiarities to be found in the Solr logs, the queries are normal and seem to succeed.\n\njava.lang.NullPointerException\n        at org.apache.hadoop.io.Text.encode(Text.java:388)\n        at org.apache.hadoop.io.Text.set(Text.java:178)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:272)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:243)\n        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:192)\n        at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:176)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)",
        "Issue Links": [
            "/jira/browse/NUTCH-1368"
        ]
    },
    "NUTCH-1101": {
        "Key": "NUTCH-1101",
        "Summary": "Options to purge db_gone records in updatedb",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "linkdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/Aug/11 10:14",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "09/Sep/11 11:17",
        "Description": "Add option to updatedb to filter out records with status db_gone (http 404). This is especially useful in cases where a crawl db is targetted at only a specific site. If the site, for some reason, suddenly changes a lot of url's we'll get a crawl db filled with garbage. Since the targetted site is known (or controlled) it is safe to get rid of all these url's: reduce db size, reduce useless http requests.",
        "Issue Links": []
    },
    "NUTCH-1102": {
        "Key": "NUTCH-1102",
        "Summary": "Fetcher, rely on fetcher.parse directive only",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/Aug/11 12:42",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "14/Sep/11 11:00",
        "Description": "The fetcher in 1.3 still has the -noParse option but does not do anything. A -parse switch (NUTCH-872) is ignored, it seems my build wasn't messed up afterall. The fetcher.parse configuration directive is also ignored. In short, Nutch 1.3 cannot parse fetched data immediately regardless of configuration and options.\nHow to procede? It makes little sense to have both the command option and the configuration directive, it raises the question of authority and adds unnecessary confusion.\nI propose to get rid of the command option and rely on the configuration directive alone.\nPlease comment.",
        "Issue Links": []
    },
    "NUTCH-1103": {
        "Key": "NUTCH-1103",
        "Summary": "Port protocol-sftp to 1.4",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Markus Jelsma",
        "Created": "01/Sep/11 18:58",
        "Updated": "23/Apr/20 15:26",
        "Resolved": null,
        "Description": "Port protocol-sftp from trunk back to 1.4",
        "Issue Links": [
            "/jira/browse/NUTCH-714"
        ]
    },
    "NUTCH-1104": {
        "Key": "NUTCH-1104",
        "Summary": "Port issues from trunk NutchGora branch",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "06/Sep/11 11:58",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "31/Jan/15 21:43",
        "Description": "Umbrella issue for tracking issues that should be ported from 1.x trunk to the NutchGora branch. Please mark ported issues by modifying this description.\nNOT YET PORTED:\n\nNUTCH-809 Parse-metatags plugin\nNUTCH-987 Support HTTP auth for Solr communication\nNUTCH-1028 Log parser keys\nNUTCH-1036 Solr jobs should increment counters in Reporter\nNUTCH-1057 Make fetcher thread time out configurable\nNUTCH-1067 Configure minimum throughput for fetcher\nNUTCH-1101 Options to purge db_gone records in updatedb\nNUTCH-1102 Fetcher, rely on fetcher.parse directive only\nNUTCH-1105 MaxContentLength option for index-basic\nNUTCH-940 Statis field plugin\nNUTCH-1094 create comprehensive documentation for Nutch 2.0 trunk\nNUTCH-1207 ParserChecker to output signature\nNUTCH-1090 InvertLinks should inform when ignoring internal links\nNUTCH-1174 Outlinks are not properly normalized\nNUTCH-1203 ParseSegment to show number of milliseconds per parse\nNUTCH-1173 DomainStats doesn't count db_not_modified\nNUTCH-1155 Host/domain limit in generator is generate.max.count+1\nNUTCH-1061 Migrate MoreIndexingFilter from Apache ORO to java.util.regex\nNUTCH-1142 Normalization and filtering in WebGraph\nNUTCH-1153 LinkRank not to log all keys and not to write Hadoop _SUCCESS file\nNUTCH-1195 Add Solr 4x (trunk) example schema\nNUTCH-1141 Configurable Fetcher queue depth\nNUTCH-1214 DomainStats tool should be named for what it's doing\nNUTCH-1213 Pass additional SolrParams when indexing to Solr\nNUTCH-1211 URLFilterChecker command line help doesn't inform user of STDIN requirements\nNUTCH-1231 Upgrade to Tika 1.0\nNUTCH-1230 MimeType API deprecated and breaks with Tika 1.0\nNUTCH-1235 Upgrade to new Hadoop 0.20.205.0\nNUTCH-1184 Fetcher to parse and follow Nth degree outlinks\nNUTCH-1214 DomainStats tool should be named for what it's doing\nNUTCH-1207 ParserChecker to output signature\nNUTCH-1174 Outlinks are not properly normalized\nNUTCH-1173 DomainStats doesn't count db_not_modified\nNUTCH-1142 Normalization and filtering in WebGraph\n\nPORTED:\n\nNo issues yet\n\nNOT GOING TO BE PORTED:\n\nNo issues, explain why it should not be ported",
        "Issue Links": []
    },
    "NUTCH-1105": {
        "Key": "NUTCH-1105",
        "Summary": "MaxContentLength option for index-basic",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            1.4,                                            nutchgora",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Sep/11 12:25",
        "Updated": "13/Sep/11 21:26",
        "Resolved": "12/Sep/11 12:21",
        "Description": "Like the limit for title, the basic indexing filter should have an optional setting to limit and truncate the content length.",
        "Issue Links": []
    },
    "NUTCH-1106": {
        "Key": "NUTCH-1106",
        "Summary": "Options to skip url's based on length",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.15",
        "Component/s": "linkdb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Markus Jelsma",
        "Created": "09/Sep/11 12:06",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "17/Jul/18 11:34",
        "Description": "Adds option to skip URL's exceeding a certain length. At first we used regex to impose this limit but having this options configurable is more convenient. Comments?",
        "Issue Links": [
            "/jira/browse/NUTCH-1314",
            "https://github.com/apache/nutch/pull/359"
        ]
    },
    "NUTCH-1107": {
        "Key": "NUTCH-1107",
        "Summary": "Log slow parse entries",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "09/Sep/11 19:58",
        "Updated": "18/Jan/16 20:48",
        "Resolved": "18/Jan/16 20:48",
        "Description": "Parse mapper and outputformat should have a facility to log (configurable) slow entries. This is useful for debugging slow parses. Logging parser keys only is not good enough, especially in a distributed environment.\nSometimes the actual parse (mapper) is very slow and sometimes the normalization and filtering of an entry's outlinks is slow.",
        "Issue Links": []
    },
    "NUTCH-1108": {
        "Key": "NUTCH-1108",
        "Summary": "Index image and video format with nutch 1.3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "hadi",
        "Created": "10/Sep/11 05:43",
        "Updated": "12/Sep/11 09:30",
        "Resolved": "12/Sep/11 09:30",
        "Description": "when i want to index video file with nutch 1.3 i get the following error :\nError parsing: file:///D:/film.avi: failed(2,0): Can't retrieve Tika parser for\n   mime-type video/x-msvideo",
        "Issue Links": []
    },
    "NUTCH-1109": {
        "Key": "NUTCH-1109",
        "Summary": "Add Sonar targets to Ant build.xml",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Sep/11 12:30",
        "Updated": "14/Oct/11 10:56",
        "Resolved": "12/Oct/11 18:28",
        "Description": "Sonar [1] is an open platform to manage code quality. I was experimenting today with what kind of analysis it allows us to do on a given codebase and was pleasantly surprised with the results. For details on the documentation please see here [2]. It can be easily integrated into our ant build.xml and is an easy way to explicitly identify latent areas of code which we could possibly improve upon. \nAt this stage I wish to highlight some of my statistics in findings...\nRunning Sonar via the attached patch identifies (based upon the analysis rules from Sonar) that the Branch-1.4 codebase contains issues as follows\n\nCritical 28 \t\t\nMajor \t1,231 \t\t\nMinor \t356 \t\t\nInfo \t119\n\n\nThese range from a catch statement being identified in o.a.n.crawl.Generator which shouldn't be catching throwable since it includes errors, through to trivial issues such as nested statements which could be combined in the same class.\nAlthough on the face of it, this seems an excellent way to make code more consistent across the board, which may in turn lead to 'better' code, I am by no way saying that this is a step we should move towards without thinking it through and discussing at length. I also think that there needs to be a good deal of our own judgement to decide whether any issues flagged up by Sonar should be marked as false positives.\nTo conclude I would like to add that I onl decided to open this issue in an attempt to gauge peoples views on the direction it takes us in.\n[1] http://www.sonarsource.org/\n[2] http://docs.codehaus.org/display/SONAR/Documentation",
        "Issue Links": [
            "/jira/browse/NUTCH-1136"
        ]
    },
    "NUTCH-1110": {
        "Key": "NUTCH-1110",
        "Summary": "Updatedb must not write _SUCCESS file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "13/Sep/11 09:06",
        "Updated": "13/Sep/11 21:27",
        "Resolved": "13/Sep/11 18:16",
        "Description": "Readdb fails with the presence of a _SUCCESS file written by Hadoop mapred after update job.",
        "Issue Links": [
            "/jira/browse/NUTCH-1029"
        ]
    },
    "NUTCH-1111": {
        "Key": "NUTCH-1111",
        "Summary": "Cashed previous link",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "hadi",
        "Created": "13/Sep/11 10:04",
        "Updated": "13/Sep/11 10:10",
        "Resolved": "13/Sep/11 10:10",
        "Description": "When i add new links in urls/urls.txt and start crawling in command line the previous links that i have deleted and does not exit start to fetchting, i don't know where these link cached,i also delete the crawl folder but this problem still happen.",
        "Issue Links": []
    },
    "NUTCH-1112": {
        "Key": "NUTCH-1112",
        "Summary": "off-by-one error in protocol-httpclient; truncates up to HttpBase.BUFFER_SIZE content",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Edward Drapkin",
        "Created": "14/Sep/11 06:26",
        "Updated": "16/Sep/11 08:41",
        "Resolved": "16/Sep/11 08:41",
        "Description": "This line of code is in protocol-httpclient/src/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/HttpResponse.java:\nwhile ((bufferFilled = in.read(buffer, 0, buffer.length)) != -1 && totalRead + bufferFilled < contentLength) {\n   ...\n} \nWhen the entire content length is less than the size of the buffer, the entire content will be read into the buffer (and bufferFilled == contentLength) and the HttpResponse object here will have empty content; similarly, the last buffer (up to BUFFER_SIZE) will be skipped. This simply needs to be changed to `totalRead + bufferFilled <= contentLength`.\nThanks!",
        "Issue Links": []
    },
    "NUTCH-1113": {
        "Key": "NUTCH-1113",
        "Summary": "Merging segments causes URLs to vanish from crawldb/index?",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Edward Drapkin",
        "Created": "15/Sep/11 20:02",
        "Updated": "07/Mar/14 10:52",
        "Resolved": "07/Mar/14 10:05",
        "Description": "When I run Nutch, I use the following steps:\nnutch inject crawldb/ url.txt\nrepeated 3 times:\nnutch generate crawldb/ segments/ -normalize\nnutch fetch `ls -d segments/* | tail -1`\nnutch parse `ls -d segments/* | tail -1`\nnutch update crawldb `ls -d segments/* | tail -1`\nnutch mergesegs merged/ -dir segments/\nnutch invertlinks linkdb/ -dir merged/\nnutch index index/ crawldb/ linkdb/ -dir merged/ (I forward ported the lucene indexing code from Nutch 1.1).\nWhen I crawl with merging segments, I lose about 20% of the URLs that wind up in the index vs. when I crawl without merging the segments.  Somehow the segment merger causes me to lose ~20% of my crawl database!",
        "Issue Links": [
            "/jira/browse/NUTCH-1616",
            "/jira/browse/NUTCH-1616",
            "/jira/browse/NUTCH-1520"
        ]
    },
    "NUTCH-1114": {
        "Key": "NUTCH-1114",
        "Summary": "Attr file missing in domain filter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Sep/11 14:12",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "19/Sep/11 14:16",
        "Description": "WARN org.apache.nutch.urlfilter.domain.DomainURLFilter: Attribute \"file\" is not defined in plugin.xml for plugin urlfilter-domain\nFile element in plugin.xml is commented out but should not. Uncommenting results in an INFO message.",
        "Issue Links": []
    },
    "NUTCH-1115": {
        "Key": "NUTCH-1115",
        "Summary": "Option to disable fixing of embedded params in DomContentUtils",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Sep/11 16:53",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "22/Sep/11 14:04",
        "Description": "Add option to disable fixing of embedded params:\nhttp://lucene.472066.n3.nabble.com/Outlinks-with-embedded-params-td3332396.html\nWhen enabled, millions of crap url's are output as outlink. This results in many 404 in the DB and many very long URL's that actually lead to the same page.",
        "Issue Links": [
            "/jira/browse/NUTCH-797",
            "/jira/browse/NUTCH-436"
        ]
    },
    "NUTCH-1116": {
        "Key": "NUTCH-1116",
        "Summary": "Write JUnit tests for all plugins",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:35",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This issue is a step towards covering the parts of our plugin codebase which are currently missing JUnit test cases. Each plugin will have its own sub-issue meaning that this parent issue should not be deemed complete until all existing (and newly contributed) plugins have the appropriate test cases.",
        "Issue Links": []
    },
    "NUTCH-1117": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for index-anchor",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.6",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:37",
        "Updated": "22/May/13 03:54",
        "Resolved": "13/Nov/12 19:19",
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": [
            "/jira/browse/NUTCH-1151"
        ]
    },
    "NUTCH-1118": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for index-basic",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.7",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:38",
        "Updated": "22/May/13 03:54",
        "Resolved": "23/Dec/12 17:45",
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1119": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for index-static",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.7",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:39",
        "Updated": "22/May/13 03:54",
        "Resolved": "08/Jan/13 03:36",
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1120": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for microformats-reltag",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:41",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1121": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for parse-js",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.16",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:42",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "13/Oct/18 10:15",
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1122": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for protocol-ftp",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:43",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1123": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for scoring-link",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:44",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1124": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for scoring-opic",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "2.3",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:45",
        "Updated": "01/May/14 06:23",
        "Resolved": "27/Oct/13 11:57",
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1125": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for tld",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "2.3",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:46",
        "Updated": "01/May/14 06:23",
        "Resolved": "01/Nov/13 18:45",
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1126": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for urlfilter-prefix",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.8,                                            2.2.1",
        "Component/s": "build",
        "Assignee": "Markus Jelsma",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:47",
        "Updated": "11/Oct/19 15:36",
        "Resolved": "24/Jun/13 13:14",
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": [
            "/jira/browse/NUTCH-1169"
        ]
    },
    "NUTCH-1127": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for urlfilter-validator",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.7",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:48",
        "Updated": "22/May/13 03:54",
        "Resolved": "08/Jan/13 03:56",
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1128": {
        "Key": "NUTCH-1116 Write JUnit tests for all plugins",
        "Summary": "JUnit test for urlmeta",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 16:48",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This issue is part of the larger attempt to provide a Junit test case for every Nutch plugin.",
        "Issue Links": []
    },
    "NUTCH-1129": {
        "Key": "NUTCH-1129",
        "Summary": "Any23 Nutch plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 21:14",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Jan/18 21:21",
        "Description": "This plugin should build on the Any23 library to provide us with a plugin which extracts RDF data from HTTP and file resources. Although as of writing Any23 not part of the ASF, the project is working towards integration into the Apache Incubator. Once the project proves its value, this would be an excellent addition to the Nutch 1.X codebase.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/205"
        ]
    },
    "NUTCH-1130": {
        "Key": "NUTCH-1129 Any23 Nutch plugin",
        "Summary": "JUnit test for Any23 RDF plugin",
        "Type": "Sub-task",
        "Status": "Reopened",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/11 21:15",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The JUnit test should be written prior to the progression of the Any23 Nutch plugin",
        "Issue Links": []
    },
    "NUTCH-1131": {
        "Key": "NUTCH-1131",
        "Summary": "Rely on published artefacts for GORA",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "25/Sep/11 20:41",
        "Updated": "25/Sep/11 20:47",
        "Resolved": "25/Sep/11 20:47",
        "Description": "We had to build GORA locally prior to building Nutch 2.0 but can now rely on the published artefacts with version 0.1.1-incubation",
        "Issue Links": []
    },
    "NUTCH-1132": {
        "Key": "NUTCH-1081 ant tests fail",
        "Summary": "Fix TestGenerator for Nutchgora",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "generator",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Sep/11 17:19",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Oct/11 20:10",
        "Description": "This issue is part of a larger target which aims to fix broken JUnit tests for Nutchgora",
        "Issue Links": []
    },
    "NUTCH-1133": {
        "Key": "NUTCH-1081 ant tests fail",
        "Summary": "Fix TestInjector for Nutchgora",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "injector",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Sep/11 17:20",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Oct/11 20:10",
        "Description": "This issue is part of a larger target which aims to fix broken JUnit tests for Nutchgora",
        "Issue Links": []
    },
    "NUTCH-1134": {
        "Key": "NUTCH-1081 ant tests fail",
        "Summary": "Fix TestFetcher for Nutchgora",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Sep/11 17:21",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Oct/11 20:10",
        "Description": "This issue is part of a larger target which aims to fix broken JUnit tests for Nutchgora",
        "Issue Links": []
    },
    "NUTCH-1135": {
        "Key": "NUTCH-1081 ant tests fail",
        "Summary": "Fix TestGoraStorage for Nutchgora",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Sep/11 17:22",
        "Updated": "20/Oct/11 17:32",
        "Resolved": "20/Oct/11 12:47",
        "Description": "This issue is part of a larger target which aims to fix broken JUnit tests for Nutchgora",
        "Issue Links": [
            "/jira/browse/NUTCH-896"
        ]
    },
    "NUTCH-1136": {
        "Key": "NUTCH-1136",
        "Summary": "Ant pmd target is broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Sep/11 19:03",
        "Updated": "06/Oct/11 12:55",
        "Resolved": "06/Oct/11 12:55",
        "Description": "issuing an 'ant pmd' command results in a failure as follows\n\nBUILD FAILED\n/home/lewis/ASF/trunk/build.xml:327: taskdef class net.sourceforge.pmd.ant.PMDTask cannot be found\n using the classloader AntClassLoader[]\n\n\nThe resulting fix should address this.",
        "Issue Links": [
            "/jira/browse/NUTCH-1109"
        ]
    },
    "NUTCH-1137": {
        "Key": "NUTCH-1137",
        "Summary": "LinkDb / invertlinks: command line arguments ignored",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.4",
        "Component/s": "linkdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Sebastian Nagel",
        "Created": "28/Sep/11 21:49",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "03/Oct/11 10:57",
        "Description": "If the tool invertlinks is called with option -dir <segmentsDir> all remaining\narguments are ignored:\n\n% $NUTCH_HOME/bin/nutch invertlinks linkdb -dir segments -noNormalize -noFilter\nLinkDb: starting at 2011-09-28 23:24:07\nLinkDb: linkdb: linkdb\nLinkDb: URL normalize: true\nLinkDb: URL filter: true\n\n\n(URLs are normalized and filtered despite -noNormalize/-noFilter)\nThe patch also restricts the ordering of arguments according to the help text:\nUsage: LinkDb <linkdb> (-dir <segmentsDir> | <seg1> <seg2> ...) [-force] [-noNormalize] [-noFilter]\n(segments must be given before the optional flags)",
        "Issue Links": []
    },
    "NUTCH-1138": {
        "Key": "NUTCH-1138",
        "Summary": "remove LogUtil from trunk and nutch gora",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "29/Sep/11 12:31",
        "Updated": "11/Jan/12 17:37",
        "Resolved": "11/Jan/12 17:35",
        "Description": "This should move towards the removal of the LogUtil class from both codebases as per comments in NUTCH-1078.",
        "Issue Links": []
    },
    "NUTCH-1139": {
        "Key": "NUTCH-1139",
        "Summary": "Indexer to delete documents",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "30/Sep/11 14:08",
        "Updated": "22/May/13 03:53",
        "Resolved": "10/Jan/12 13:57",
        "Description": "Add an option -delete to the solrindex command. With this feature enabled documents of the currently processing segment with status FETCH_GONE or FETCH_REDIR_PERM are deleted, a following SolrClean is not required anymore.\nThis issue is a follow up of NUTCH-1052.",
        "Issue Links": [
            "/jira/browse/NUTCH-1047",
            "/jira/browse/NUTCH-1052"
        ]
    },
    "NUTCH-1140": {
        "Key": "NUTCH-1140",
        "Summary": "index-more plugin, resetTitle method creates multiple values in the Title field",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.3",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Joe Liedtke",
        "Created": "30/Sep/11 19:03",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "From the comments in MoreIndexingFilter.java, the index-more plugin is meant to reset the Title field of a document if it contains a Content-Disposition header. The current behavior is to add a Title regardless of whether one exists or not, which can cause issues down the line with the Solr Indexing process, and based on a thread in the nutch user list it appears that this is causing some users to mark the title as multi-valued in the schema:\nhttp://www.lucidimagination.com/search/document/9440ff6b5deb285b/multiple_values_encountered_for_non_multivalued_field_title#17736c5807826be8\nThe following patch removes the title field before adding a new one, which has resolved the issue for me:\n\u2014 MoreIndexingFilter.old\t2011-09-30 11:44:35.000000000 +0000\n+++ MoreIndexingFilter.java\t2011-09-30 09:58:48.000000000 +0000\n@@ -276,6 +276,7 @@\n     for (int i=0; i<patterns.length; i++) {\n       if (matcher.contains(contentDisposition,patterns[i])) \n{\n         result = matcher.getMatch();\n+        doc.removeField(\"title\");\n         doc.add(\"title\", result.group(1));\n         break;\n       }",
        "Issue Links": [
            "/jira/browse/NUTCH-1004"
        ]
    },
    "NUTCH-1141": {
        "Key": "NUTCH-1141",
        "Summary": "Configurable Fetcher queue depth",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "01/Oct/11 08:22",
        "Updated": "15/Nov/11 11:48",
        "Resolved": "01/Oct/11 08:24",
        "Description": "The Fetcher currently has a queue depth based on the number of threads, multiplied by 50. This issue is about having a parameter to specify a larger value which would give a deeper queue and improve the performance of the fetcher when the ordering of the URLs within the fetch list is not optimal.",
        "Issue Links": [
            "/jira/browse/NUTCH-776"
        ]
    },
    "NUTCH-1142": {
        "Key": "NUTCH-1142",
        "Summary": "Normalization and filtering in WebGraph",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Oct/11 11:54",
        "Updated": "22/May/13 03:53",
        "Resolved": "10/Nov/11 14:29",
        "Description": "The WebGraph programs performs URL normalization. Since normalization of outlinks is already performed during the parse it should become optional. There is also no URL filtering mechanism in the web graph program. When a CrawlDatum is removed from the CrawlDB by an URL filter is should be possible to remove it from the web graph as well.",
        "Issue Links": [
            "/jira/browse/NUTCH-1144",
            "/jira/browse/NUTCH-1171"
        ]
    },
    "NUTCH-1143": {
        "Key": "NUTCH-1143",
        "Summary": "Omit anchor in webgraph's LinkDatum",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Oct/11 11:56",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Anchors are stored  unchecked in the webgraph. it looks like for cosmetic reasons only. When dealing with hundreds of millions of records it takes up significant space and I/O time.\nThis issue should add an option to omit the anchor.",
        "Issue Links": []
    },
    "NUTCH-1144": {
        "Key": "NUTCH-1144",
        "Summary": "Filtering optional in WebGraph",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Oct/11 13:08",
        "Updated": "22/May/13 03:54",
        "Resolved": "03/Oct/11 12:49",
        "Description": "There is no URL filtering mechanism in the web graph program. When a CrawlDatum is removed from the CrawlDB by an URL filter is should be possible to remove it from the web graph as well.",
        "Issue Links": [
            "/jira/browse/NUTCH-1142"
        ]
    },
    "NUTCH-1145": {
        "Key": "NUTCH-1145",
        "Summary": "Add linkrank config directives to default conf",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Oct/11 13:42",
        "Updated": "22/May/13 03:54",
        "Resolved": "09/Feb/12 10:00",
        "Description": "All linkrank configuration directives need to be added to nutch-default with a proper description.",
        "Issue Links": [
            "/jira/browse/NUTCH-1256"
        ]
    },
    "NUTCH-1146": {
        "Key": "NUTCH-1146",
        "Summary": "Get rid of _success files in webgraph code",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/Oct/11 11:36",
        "Updated": "22/May/13 03:54",
        "Resolved": "05/Jan/12 11:07",
        "Description": "WebGraph tools here and there also suffer from reading a _SUCCESS file. All jobs there should disable this setting.",
        "Issue Links": []
    },
    "NUTCH-1147": {
        "Key": "NUTCH-1147",
        "Summary": "WebGraph nodeDumper uses only 1 reducer",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/Oct/11 14:06",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The noderDumper is restricted to only one reducer, making it slow and producing too large files.",
        "Issue Links": []
    },
    "NUTCH-1148": {
        "Key": "NUTCH-1148",
        "Summary": "Nutchgora job jar functionalilty is broken: PluginManifestParser cannot load plugins from system classloader.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "04/Oct/11 15:10",
        "Updated": "16/Nov/11 13:04",
        "Resolved": "16/Nov/11 13:04",
        "Description": "This affects running nutchgora using Hadoop it's RunJar mechanism (hadoop jar ...). The mr tasks are perfectly able to load the plugins (please note NUTCH-937). But, when the plugins are loaded from the job submitter process itself, loading plugins might fail due to classloading issues. This is caused by the fact that PluginManifestParser does not use the contextClassLoader that is set by RunJar. This classloader contains the plugins folder. At least the FetcherJob is affected by this, because the job submitter uses getFields of Protocol implementations, therefore loading the plugins.\nThe current 1.x is not affected because it does not load plugins at any point during the job submission. This might of course change so I propose to 'fix' the issue in the 1.x branch as well.\nThe solution is fairly simple, PluginManifestParser should use the contextClassLoader of the current thread instead of using the system classloader. I will attach patch right away. It currently works but it still needs some further testing.",
        "Issue Links": []
    },
    "NUTCH-1149": {
        "Key": "NUTCH-1149",
        "Summary": "DomainStats should process numeric CrawlDB metadata",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Oct/11 00:57",
        "Updated": "18/Jan/16 20:46",
        "Resolved": "18/Jan/16 20:46",
        "Description": "Right now the DomainStats program only outputs the sum of fetched records per domain or host. It should also be able to output processed numerics of meta data in order to get the average size (content length) for a given domain or host. This is also useful for generating a metric for adult material (by domain or host) when using a plugin that stores a propability factor of adult material per URL in the Crawl DB.",
        "Issue Links": [
            "/jira/browse/NUTCH-1325"
        ]
    },
    "NUTCH-1150": {
        "Key": "NUTCH-1150",
        "Summary": "http.redirect.max can lead to multiple parses of the same url",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.3,                                            1.4",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "05/Oct/11 11:44",
        "Updated": "21/Aug/20 15:09",
        "Resolved": "21/Aug/20 15:09",
        "Description": "With http.redirect.max > 0 it's possible that a document is parsed multiple times. This is the case when several url's from the same fetch redirect to a shared location.",
        "Issue Links": [
            "/jira/browse/NUTCH-2776",
            "/jira/browse/NUTCH-1184"
        ]
    },
    "NUTCH-1151": {
        "Key": "NUTCH-1151",
        "Summary": "Index-anchor to add numInlinks count",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Oct/11 14:19",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Issue to improve in index-anchor to add the number of inlinks per document. This count is useful for calculating some authority metric in the search server.T",
        "Issue Links": [
            "/jira/browse/NUTCH-1117"
        ]
    },
    "NUTCH-1152": {
        "Key": "NUTCH-1152",
        "Summary": "Upgrade to SolrJ 3.4.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "None",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "07/Oct/11 15:51",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "07/Oct/11 16:13",
        "Description": "Current release of Lucene/Solr is 3.4.0, but we're still using 3.1.0. The fix is trivial, just replace 3.1.0 with 3.4.0 in ivy.xml. If there are no objections I'll make the change shortly.",
        "Issue Links": []
    },
    "NUTCH-1153": {
        "Key": "NUTCH-1153",
        "Summary": "LinkRank must not log all hyperlinks",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Oct/11 16:04",
        "Updated": "22/May/13 03:53",
        "Resolved": "10/Nov/11 14:28",
        "Description": "The LinkRank reducer log's all hyperlinks with the associated statistics at INFO level. This results in much increased I/O-wait. Level should be reduced to DEBUG.",
        "Issue Links": []
    },
    "NUTCH-1154": {
        "Key": "NUTCH-1154",
        "Summary": "Upgrade to Tika 0.10",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.4",
        "Component/s": "parser",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "07/Oct/11 18:45",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "11/Oct/11 09:35",
        "Description": "There have been significant improvements in Tika 0.10 and it would be nice to use the latest Tika in 1.4.",
        "Issue Links": []
    },
    "NUTCH-1155": {
        "Key": "NUTCH-1155",
        "Summary": "Host/domain limit in generator is generate.max.count+1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Oct/11 22:20",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Nov/11 15:01",
        "Description": "Generator always outputs 1 additional URL per domain/host limit.",
        "Issue Links": []
    },
    "NUTCH-1156": {
        "Key": "NUTCH-1156",
        "Summary": "building errors with gora-hbase as a backend; update ivy.xml to use correct dependancies",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "11/Oct/11 11:14",
        "Updated": "02/Nov/11 11:19",
        "Resolved": "31/Oct/11 11:51",
        "Description": "This patch makes sure nutchgora can actually be built when gora-hbase is uncommented in ivy.xml. Note that is still commented though, so sql is still the default backend. However whenever one wishes to use hbase (as we do) simply uncommenting the section in ivy.xml won't do the trick. This patch fixes this. Changes in ivy.xml:\n-Set correct version for gora-hbase and config.\n-Add thrift to exclude as it is not available in the repos; it is not needed in most cases but please correct me if I'm wrong.\n-Additionally, it removes the comment that hbase library itself should be manually added, as this not needed anymore.",
        "Issue Links": []
    },
    "NUTCH-1157": {
        "Key": "NUTCH-1157",
        "Summary": "building errors with gora-hbase as a backend; update ivy.xml to use correct dependancies",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "11/Oct/11 11:14",
        "Updated": "11/Oct/11 11:15",
        "Resolved": "11/Oct/11 11:15",
        "Description": "This patch makes sure nutchgora can actually be built when gora-hbase is uncommented in ivy.xml. Note that is still commented though, so sql is still the default backend. However whenever one wishes to use hbase (as we do) simply uncommenting the section in ivy.xml won't do the trick. This patch fixes this. Changes in ivy.xml:\n-Set correct version for gora-hbase and config.\n-Add thrift to exclude as it is not available in the repos; it is not needed in most cases but please correct me if I'm wrong.\n-Additionally, it removes the comment that hbase library itself should be manually added, as this not needed anymore.",
        "Issue Links": []
    },
    "NUTCH-1158": {
        "Key": "NUTCH-1158",
        "Summary": "Write JUnit tests for all nutchgora plugins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:16",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "This issue should act as a parent issue to track the development and gradual integration and addition of JUnit tests to accompany all nutchgora plugins.",
        "Issue Links": []
    },
    "NUTCH-1159": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for index-anchor",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:18",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Aug/12 13:58",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1160": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for index-basic",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:18",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Aug/12 19:53",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1161": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for microformats-reltag plugin",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:20",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Aug/12 19:54",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1162": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for parse-js",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:21",
        "Updated": "22/May/13 03:53",
        "Resolved": "15/Sep/12 16:17",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1163": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for protocol-ftp",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:23",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1164": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for protocol-http",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "None",
        "Assignee": "Nima Falaki",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:24",
        "Updated": "09/Oct/14 19:51",
        "Resolved": "09/Oct/14 19:21",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1165": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for protocol-sftp",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:24",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1166": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for scoring-link",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "linkdb",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:26",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1167": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for scoring-opic",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:26",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1168": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for tld",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:27",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1169": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for urlfilter-prefix",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.3.1",
        "Component/s": "None",
        "Assignee": "Talat Uyarer",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:28",
        "Updated": "17/Sep/15 07:06",
        "Resolved": "24/May/15 12:06",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": [
            "/jira/browse/NUTCH-1126"
        ]
    },
    "NUTCH-1170": {
        "Key": "NUTCH-1158 Write JUnit tests for all nutchgora plugins",
        "Summary": "Write JUnit tests for urlfilter-validator",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/11 20:29",
        "Updated": "26/May/15 15:44",
        "Resolved": "26/May/15 15:44",
        "Description": "This issue should provide a single Junit test as part of an effort to provide JUnit tests for all nutchgora plugins",
        "Issue Links": []
    },
    "NUTCH-1171": {
        "Key": "NUTCH-1171",
        "Summary": "WebGraph to overwrite normalized input keys",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Oct/11 13:05",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Oct/11 13:47",
        "Description": "Right now the Outlinks program normalizes input keys but outputs the original input key. This prevents new directives to normalize existing outlinks. The input key should be overwritten with the normalized URL.",
        "Issue Links": [
            "/jira/browse/NUTCH-1142"
        ]
    },
    "NUTCH-1172": {
        "Key": "NUTCH-1172",
        "Summary": "AbstractNuchTest should have a generic testdir instead of specific 'inject' dir",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "14/Oct/11 08:54",
        "Updated": "03/Nov/11 04:15",
        "Resolved": "02/Nov/11 16:12",
        "Description": "This is a very trivial issue but nevertheless important for the goal to have clarified tests. This patch changes the testdir in AbstractNutchTest to \"build/test/working\" so that it is not specifically aimed towards the TestInjector test anymore. Also removes some unnecessary imports.\nI was actually unsure whether to move the testdir to TestInjector altogether, but then again perhaps it's nice to have a generic working dir available to any test that decides to use one.\nIf there aren't any objections, I will be committing this patch in a few days.",
        "Issue Links": []
    },
    "NUTCH-1173": {
        "Key": "NUTCH-1173",
        "Summary": "DomainStats doesn't count db_not_modified",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Oct/11 22:18",
        "Updated": "22/May/13 03:54",
        "Resolved": "10/Nov/11 15:24",
        "Description": "The DomainStats tool does not count records with a not_modified status. Domains therefore can disappear from the stats output while they shouldn't.",
        "Issue Links": []
    },
    "NUTCH-1174": {
        "Key": "NUTCH-1174",
        "Summary": "Outlinks are not properly normalized",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.5",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "18/Oct/11 09:43",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Nov/11 15:19",
        "Description": "In ParseOutputFormat, the toUrl is read from Outlink and is processed. This String object is filtered, normalized etc but the original Outlink object is actually added. The normalized url in toUrl is not written back to the Outlink object.\nThis issue adds a setUrl method to Outlink which is used in ParseOutputFormat to overwrite the unnormalized url.",
        "Issue Links": [
            "/jira/browse/NUTCH-1184"
        ]
    },
    "NUTCH-1175": {
        "Key": "NUTCH-1175",
        "Summary": "Update ivy.xml to use correct dependancies with gora-cassandra as a backend",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Oct/11 12:32",
        "Updated": "31/Oct/11 23:01",
        "Resolved": "31/Oct/11 23:01",
        "Description": "This issue should add the correct target for the gora 0.1.1-incubating dependency required to use Cassandra as storage mechanism.\nI will get a patch together and add in due course.",
        "Issue Links": [
            "/jira/browse/NUTCH-902"
        ]
    },
    "NUTCH-1176": {
        "Key": "NUTCH-1176",
        "Summary": "Fix all javadoc warnings from nightly builds",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Oct/11 22:24",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Oct/19 13:01",
        "Description": "The warnings can clearly be seen from the javadoc target (near bottom) of any successful nightly build. An example is provided below.\nhttps://builds.apache.org/job/nutch-trunk/1638/console",
        "Issue Links": []
    },
    "NUTCH-1177": {
        "Key": "NUTCH-1177",
        "Summary": "Generator to select on retry interval",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Oct/11 18:47",
        "Updated": "22/May/13 03:54",
        "Resolved": "13/Jan/12 14:31",
        "Description": "The generator already has a mechanism to select entries with a score larger than specified threshold but should also have a means to select entries with a retry interval lower than specified by a configuration option.\nSuch a feature is particulary useful when dealing with too large crawldb's where you still want a crawl to fetch rapid changing url's first.\nThis issue should also add the missing generate.min.score configuration to nutch-default.",
        "Issue Links": [
            "/jira/browse/NUTCH-1248"
        ]
    },
    "NUTCH-1178": {
        "Key": "NUTCH-1178",
        "Summary": "Incorrect CSV header CrawlDatumCsvOutputFormat",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Oct/11 15:48",
        "Updated": "22/May/13 03:54",
        "Resolved": "10/Nov/11 14:31",
        "Description": "The CSV header doesn't mention both retry interval fields (seconds + days). We should either add another field to the header to get rid of one retry interval field. I prefer the former as people may already rely on the current format.",
        "Issue Links": []
    },
    "NUTCH-1179": {
        "Key": "NUTCH-1179",
        "Summary": "Option to restrict generated records by metadata",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Oct/11 23:30",
        "Updated": "24/Feb/16 14:18",
        "Resolved": "24/Feb/16 14:18",
        "Description": "The generator should be able to select entries based on a metadata key/value pair.",
        "Issue Links": [
            "/jira/browse/NUTCH-2231"
        ]
    },
    "NUTCH-1180": {
        "Key": "NUTCH-1180",
        "Summary": "UpdateDB to backup previous CrawlDB",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Oct/11 23:36",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Nov/11 11:55",
        "Description": "Nutch currently replaces an existing CrawlDB with the new CrawlDB. By optionally keeping a previous version on HDFS users can easily revert in case of a mistake without relying on external backup mechanims.\nThis should be enabled by default.",
        "Issue Links": []
    },
    "NUTCH-1181": {
        "Key": "NUTCH-1181",
        "Summary": "Indexer to use webgraph inlinks",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "25/Oct/11 21:23",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Indexers currently rely on the LinkDB for anchor indexing while the WebGraph provides the same data as an inverted link DB. An inlinkDB created by the WebGraph program with non-zero LinkRank scores on the nodes also provide an improved set ordered by popularity.\nThis issue must:\n\nlet IndexerMapReduce understand the new format;\nallow for indexing only popular anchors.\n\nThe goal is todeprecate all code associated with invertlinks and ultimately remove it from the codebase.",
        "Issue Links": [
            "/jira/browse/NUTCH-1282"
        ]
    },
    "NUTCH-1182": {
        "Key": "NUTCH-1182",
        "Summary": "fetcher to log hung threads",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3,                                            1.4",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "26/Oct/11 19:17",
        "Updated": "04/May/14 22:29",
        "Resolved": "04/May/14 20:20",
        "Description": "While crawling a slow server with a couple of very large PDF documents (30 MB) on it\nafter some time and a bulk of successfully fetched documents the fetcher stops\nwith the message: Aborting with 10 hung threads.\nFrom now on every cycle ends with hung threads, almost no documents are fetched\nsuccessfully. In addition, strange hadoop errors are logged:\n\n   fetch of http://.../xyz.pdf failed with: java.lang.NullPointerException\n    at java.lang.System.arraycopy(Native Method)\n    at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$Buffer.write(MapTask.java:1108)\n    ...\n\n\nor\n\n   Exception in thread \"QueueFeeder\" java.lang.NullPointerException\n         at org.apache.hadoop.fs.BufferedFSInputStream.getPos(BufferedFSInputStream.java:48)\n         at org.apache.hadoop.fs.FSDataInputStream.getPos(FSDataInputStream.java:41)\n         at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:214)\n\n\nI've run the debugger and found:\n\nafter the \"hung threads\" are reported the fetcher stops but the threads are still alive and continue fetching a document. In consequence, this will\n\t\nlimit the small bandwidth of network/server even more\nafter the document is fetched the thread tries to write the content via output.collect() which must fail because the fetcher map job is already finished and the associated temporary mapred directory is deleted. The error message may get mixed with the progress output of the next fetch cycle causing additional confusion.\n\n\ndocuments/URLs causing the hung thread are never reported nor stored. That is, it's hard to track them down, and they will cause a hung thread again and again.\n\nThe problem is reproducible when fetching bigger documents and setting mapred.task.timeout to a low value (this will definitely cause hung threads).",
        "Issue Links": [
            "/jira/browse/NUTCH-569"
        ]
    },
    "NUTCH-1183": {
        "Key": "NUTCH-1183",
        "Summary": "Summary task for adding command line usage instructions to webgraph classes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.7",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Oct/11 21:15",
        "Updated": "22/May/13 03:54",
        "Resolved": "08/Dec/12 21:57",
        "Description": "The following files should provide output when called innacurately from the command line. Something similar to \n\nUsage: class -arg1, -arg2, etc etc\n\n\n\nwebgraph\nlinkrank\nscoreupdater\nnodedumper\nnodereader\n\nIf anyone would like to see further classes included in this task please add to the above list.",
        "Issue Links": []
    },
    "NUTCH-1184": {
        "Key": "NUTCH-1184",
        "Summary": "Fetcher to parse and follow Nth degree outlinks",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Oct/11 15:40",
        "Updated": "22/May/13 03:53",
        "Resolved": "20/Dec/11 10:11",
        "Description": "Fetcher improvements to parse and follow outlinks up to a specified depth. The number of outlinks to follow can be decreased by depth using a divisor. This patch introduces three new configuration directives:\n\n<property>\n  <name>fetcher.follow.outlinks.depth</name>\n  <value>-1</value>\n  <description>(EXPERT)When fetcher.parse is true and this value is greater than 0 the fetcher will extract outlinks\n  and follow until the desired depth is reached. A value of 1 means all generated pages are fetched and their first degree\n  outlinks are fetched and parsed too. Be careful, this feature is in itself agnostic of the state of the CrawlDB and does not\n  know about already fetched pages. A setting larger than 2 will most likely fetch home pages twice in the same fetch cycle.\n  It is highly recommended to set db.ignore.external.links to true to restrict the outlink follower to URL's within the same\n  domain. When disabled (false) the feature is likely to follow duplicates even when depth=1.\n  A value of -1 of 0 disables this feature.\n  </description>\n</property>\n\n<property>\n  <name>fetcher.follow.outlinks.num.links</name>\n  <value>4</value>\n  <description>(EXPERT)The number of outlinks to follow when fetcher.follow.outlinks.depth is enabled. Be careful, this can multiply\n  the total number of pages to fetch. This works with fetcher.follow.outlinks.depth.divisor, by default settings the followed outlinks\n  at depth 1 is 8, not 4.\n  </description>\n</property>\n\n<property>\n  <name>fetcher.follow.outlinks.depth.divisor</name>\n  <value>2</value>\n  <description>(EXPERT)The divisor of fetcher.follow.outlinks.num.links per fetcher.follow.outlinks.depth. This decreases the number\n  of outlinks to follow by increasing depth. The formula used is: outlinks = floor(divisor / depth * num.links). This prevents\n  exponential growth of the fetch list.\n  </description>\n</property>\n\n\nPlease, do not use this unless you know what you're doing. This feature does not consider the state of the CrawlDB nor does it consider generator settings such as limiting the number of pages per (domain|host|ip) queue. It is not polite to use this feature with high settings as it can fetch many pages from the same domain including duplicates.\nAlso, this feature will not work if fetcher.parse is disabled. With parsing enabled you might want to consider not to store downloaded content.",
        "Issue Links": [
            "/jira/browse/NUTCH-1174",
            "/jira/browse/NUTCH-1346",
            "/jira/browse/NUTCH-1212",
            "/jira/browse/NUTCH-1150"
        ]
    },
    "NUTCH-1185": {
        "Key": "NUTCH-1185",
        "Summary": "Decrease solr.commit.size",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.5",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/Oct/11 09:00",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Nov/11 12:00",
        "Description": "Default document batch size should be decreased to prevent OOMEs.",
        "Issue Links": []
    },
    "NUTCH-1186": {
        "Key": "NUTCH-1186",
        "Summary": "FreeGenerator always normalizes",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.20",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/Oct/11 13:19",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The FreeGenerator does not honor the -normalize option, it always normalizes all URL's in the input directory. The -filter option is respected.",
        "Issue Links": []
    },
    "NUTCH-1187": {
        "Key": "NUTCH-1104 Port issues from trunk NutchGora branch",
        "Summary": "Port NUTCH-1028 to nutchgora - log parser keys",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "01/Nov/11 12:20",
        "Updated": "02/Nov/11 11:19",
        "Resolved": "02/Nov/11 09:23",
        "Description": "This task is to port NUTCH-1028 to nutchgora - log parser keys. Very trivial, will attach patch and commit right away.",
        "Issue Links": []
    },
    "NUTCH-1188": {
        "Key": "NUTCH-1188",
        "Summary": "ERROR util.LogUtil - Cannot log with method [null]",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Zhang JinYan",
        "Created": "01/Nov/11 15:02",
        "Updated": "10/Nov/11 09:24",
        "Resolved": "10/Nov/11 09:24",
        "Description": "LogUtil has static fields,which is initialized like this:\n    FATAL = Logger.class.getMethod(\"error\", new Class[] \n{ Object.class }\n);\nbut the Logger has no such method\uff0cthe correct method is\uff1a\n    void org.slf4j.Logger.error(String msg)\nSo\uff0cLogUtil's static fields are not initialized correctly(they are null)\n-------------------------------\nRun crawl\uff0cyou will find msg in hadoop.log\uff1a\n    2011-11-01 22:38:14,442 ERROR util.LogUtil - Cannot log with method [null]\n    java.lang.NullPointerException\n\tat org.apache.nutch.util.LogUtil$1.flush(LogUtil.java:103)\n\tat java.io.PrintStream.write(PrintStream.java:432)\n\tat sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:202)\n\tat sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:272)\n\tat sun.nio.cs.StreamEncoder.flushBuffer(StreamEncoder.java:85)\n\tat java.io.OutputStreamWriter.flushBuffer(OutputStreamWriter.java:168)\n\tat java.io.PrintStream.newLine(PrintStream.java:496)\n\tat java.io.PrintStream.println(PrintStream.java:757)\n\tat java.lang.Throwable.printStackTraceAsCause(Throwable.java:492)\n\tat java.lang.Throwable.printStackTrace(Throwable.java:468)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:197)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:665)\n----------------------------\nPatch:\n    FATAL = Logger.class.getMethod(\"error\", new Class[] \n{ String.class }\n);",
        "Issue Links": []
    },
    "NUTCH-1189": {
        "Key": "NUTCH-902 Add all necessary files and configuration so that nutch can be used with different backends out-of-the-box",
        "Summary": "add commented out default settings to gora.properties files",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "01/Nov/11 15:28",
        "Updated": "27/Apr/12 05:37",
        "Resolved": "11/Jan/12 20:31",
        "Description": "This issues should have been dealt with as part of its parent issue, however I think as it is a fairly lareg task in itself, it needs to be done independently. The gora.properties file should, amongst other settings, and beside the extreme basic defaults for sqlstore, include defaults for opening HBase, Cassandra, etc servers on their default ports etc. Leaving this down to individual interpretation puts a huge owness of the user, hence constructing a barrier to entry for getting the configuration settings up and running.",
        "Issue Links": []
    },
    "NUTCH-1190": {
        "Key": "NUTCH-1190",
        "Summary": "MoreIndexingFilter refactor: move data formats used to parse \"lastModified\" to a config file.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.18",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Zhang JinYan",
        "Created": "01/Nov/11 15:52",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "16/Aug/20 19:07",
        "Description": "There many issues about missing date format:\nNUTCH-871\nNUTCH-912\nNUTCH-1015\nThe data formats can be diverse, so why not move those data formats to a extra config file?\nI move all the data formats from \"MoreIndexingFilter.java\" to a file named \"date-styles.txt\"(place in \"conf\"), which will be load on startup.\n\n  public void setConf(Configuration conf) {\n    this.conf = conf;\n    MIME = new MimeUtil(conf);\n    \n    URL res = conf.getResource(\"date-styles.txt\");\n    if(res==null){\n      LOG.error(\"Can't find resource: date-styles.txt\");\n    }else{\n      try {\n        List lines = FileUtils.readLines(new File(res.getFile()));\n        for (int i = 0; i < lines.size(); i++) {\n          String dateStyle = (String) lines.get(i);\n          if(StringUtils.isBlank(dateStyle)){\n            lines.remove(i);\n            i--;\n            continue;\n          }\n          dateStyle=StringUtils.trim(dateStyle);\n          if(dateStyle.startsWith(\"#\")){\n            lines.remove(i);\n            i--;\n            continue;\n          }\n          lines.set(i, dateStyle);\n        }\n        dateStyles = new String[lines.size()];\n        lines.toArray(dateStyles);\n      } catch (IOException e) {\n        LOG.error(\"Failed to load resource: date-styles.txt\");\n      }\n    }\n  }\n\n\nThen parse \"lastModified\" like this(sample):\n\n  private long getTime(String date, String url) {\n    ......\n    Date parsedDate = DateUtils.parseDate(date, dateStyles);\n    time = parsedDate.getTime();\n    ......\n    return time;\n  }\n\n\nThis path also contains the \"path\" of NUTCH-1140.\nFind more details in the patch file.",
        "Issue Links": [
            "/jira/browse/NUTCH-1015",
            "https://github.com/apache/nutch/pull/545"
        ]
    },
    "NUTCH-1191": {
        "Key": "NUTCH-1104 Port issues from trunk NutchGora branch",
        "Summary": "Port NUTCH-1102 to nutchgora - consistent use of fetcher.parse",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "01/Nov/11 16:16",
        "Updated": "03/Nov/11 04:15",
        "Resolved": "02/Nov/11 09:27",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1192": {
        "Key": "NUTCH-1192",
        "Summary": "Add '/runtime' to svn ignore",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4,                                            nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "02/Nov/11 13:24",
        "Updated": "03/Nov/11 05:03",
        "Resolved": "02/Nov/11 13:32",
        "Description": "Add '/runtime' to svn ignore. The .gitignore file already has the fix.",
        "Issue Links": []
    },
    "NUTCH-1193": {
        "Key": "NUTCH-1193",
        "Summary": "Incorrect url transform to lowercase: parameter solr",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.3",
        "Fix Version/s": "1.5",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "x",
        "Created": "02/Nov/11 14:10",
        "Updated": "18/Feb/12 04:37",
        "Resolved": "17/Feb/12 20:49",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1194": {
        "Key": "NUTCH-1194",
        "Summary": "Generator: CrawlDB lock should be released earlier",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.17",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Nov/11 17:42",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "05/May/20 12:10",
        "Description": "Lock on the CrawlDB is released when everything is finished. But when generating many segments, the lock remains in place while it's not neccessary anymore. If GENERATE_UPDATE_DB is false we can release the lock immediately after the selector has finished.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/514"
        ]
    },
    "NUTCH-1195": {
        "Key": "NUTCH-1195",
        "Summary": "Add Solr 4x (trunk) example schema",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.4",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "02/Nov/11 23:23",
        "Updated": "20/Dec/11 11:30",
        "Resolved": "03/Nov/11 21:45",
        "Description": "The conf/schema.xml that we ship works ok for Solr 3.x, but in Solr trunk some of the class names have been changed, and some field types have been redefined, so if you simply drop this schema into Solr it will cause severe errors and indexing won't work.\nI propose to add a version of the schema.xml file that is tailored to Solr 4.x so that users can deploy this schema when indexing to Solr trunk.",
        "Issue Links": []
    },
    "NUTCH-1196": {
        "Key": "NUTCH-1196",
        "Summary": "Update job should impose an upper limit on the number of inlinks (nutchgora)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "03/Nov/11 17:32",
        "Updated": "17/Nov/11 05:47",
        "Resolved": "16/Nov/11 15:17",
        "Description": "Currently the nutchgora branch does not limit the number of inlinks in the update job. This will result in some nasty out-of-memory exceptions and timeouts when the crawl is getting big. Nutch trunk already has a default limit of 10,000 inlinks. I will implement this in nutchgora too. Nutch trunk uses a sorting mechanism in the reducer itself, but I will implement it using standard Hadoop components instead (should be a bit faster). This means:\nThe keys of the reducer will be a \n{url,score} tuple.\n\nPartitioning will be done by {url}.\nSorting will be done by {url,score}\n.\nFinally grouping will be done by \n{url}\n again.\nThis ensures all indentical urls will be put in the same reducer, but in order of scoring.\nPatch should be ready by tomorrow. Please let me know when you have any comments or suggestions.",
        "Issue Links": []
    },
    "NUTCH-1197": {
        "Key": "NUTCH-1197",
        "Summary": "Add statically configured field values to solrindex-mapping.xml",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Andrzej Bialecki",
        "Created": "03/Nov/11 23:14",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "In some cases it's useful to be able to add to every document sent to Solr a set of predefined fields with static values. This could be implemented on the Solr side (with a custom UpdateRequestProcessor), but it may be less cumbersome to add them on the Nutch side.\nExample: let's say I have several Nutch configurations all indexing to the same Solr instance, and I want each of them to add its identifier as a field in all documents, e.g. \"origin\"=\"web_crawl_1\", \"origin\"=\"file_crawl\", \"origin\"=\"unlimited_crawl\", etc...",
        "Issue Links": [
            "/jira/browse/NUTCH-2052"
        ]
    },
    "NUTCH-1198": {
        "Key": "NUTCH-1198",
        "Summary": "Less verbose logging when unmapped mimetypes are trying to be parsed.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "04/Nov/11 15:56",
        "Updated": "18/Nov/11 08:30",
        "Resolved": "17/Nov/11 15:41",
        "Description": "In some scenarios one does not want to parse all mimetypes. Despite the fact that it is possible to configure what resources are being fetched (using URLFilters for instance), it cannot be avoided that resources with unmapped mimetypes are trying to be parsed. Currently this leads to log warnings with stacktraces. This possible masks more important parsing errors, such as errors in parsing itself.\nThis patch removes the stacktrace for \"no suitable parser found\" errors. Also it display the error only once. (As opposed to what is currently the case).",
        "Issue Links": []
    },
    "NUTCH-1199": {
        "Key": "NUTCH-1199",
        "Summary": "unfetched URLs problem",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            generator",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "08/Nov/11 06:29",
        "Updated": "08/Nov/11 09:52",
        "Resolved": "08/Nov/11 09:52",
        "Description": "we write a script to fetch unfetched urls:\n#first dump from readdb to a text file, and extract unfetched urls to a text file:\n        bin/nutch readdb $crawldb -dump $SITE_DIR/tmp/dump_urls.txt -format csv\n        cat $SITE_DIR/tmp/dump_urls.txt/part-00000 | grep db_unfetched > $SITE_DIR/tmp/dump_unf\n        unfetched_urls_file=\"$SITE_DIR/tmp/unfetched_urls/unfetched_urls.txt\"\n        cat $SITE_DIR/tmp/dump_unf | awk -F '\"' '\n{print $2}\n' >  $unfetched_urls_file\n        unfetched_count=`cat $unfetched_urls_file|wc -l`\n#next, we have a list of unfetched urls in unfetched_urls.txt , then, we use command freegen to create segments for #these urls, we can not use command generate because these url's were generated previously\n       if [[ $unfetched_count -lt $it_size ]]\n       then\n                        echo \"UNFETCHED $J , $it_size URLs from $unfetched_count generated\"\n                        ((J++))\n                        bin/nutch freegen $SITE_DIR/tmp/unfetched_urls/unfetched_urls.txt $crawlseg\n                        s2=`ls -d $crawlseg/2* | tail -1`\n                        bin/nutch fetch $s2\n                        bin/nutch parse $s2\n                        bin/nutch updatedb $crawldb $s2\n                        echo \"bin/nutch updatedb $crawldb $s2\" >> $SITE_DIR/updatedblog.txt\n                        get_new_links\n                        exit\n       fi\n\nif number of urls are greater than it_size, then package them\n        ij=1\n        while read line\n        do\n                let \"ind = $ij / $it_size\"\n                mkdir $SITE_DIR/tmp/unfetched_urls/unfetched_urls$ind/\n                echo $line >> $SITE_DIR/tmp/unfetched_urls/unfetched_urls$ind/unfetched_urls$ind.txt\n                echo $ind\n                ((ij++))\n                let \"completed=$ij % $it_size\"\n               if [[ $completed -eq 0 ]]\n\n               then\n                                                                  echo \"UNFETCHED $J , $it_size URLs from $unfetched_count generated\"\n                        ((J++))\n                        bin/nutch freegen $SITE_DIR/tmp/unfetched_urls/unfetched_urls$ind/unfetched_urls$ind.txt $crawlseg\n#finally fetch,parse and update new segment\n                        s2=`ls -d $crawlseg/2* | tail -1`\n                        bin/nutch fetch $s2\n                        bin/nutch parse $s2\n                        rm $crawldb/.locked\n                        bin/nutch updatedb $crawldb $s2\n                        echo \"bin/nutch updatedb $crawldb $s2\" >> $SITE_DIR/updatedblog.txt\n               fi\n        done <$unfetched_urls_file",
        "Issue Links": []
    },
    "NUTCH-1200": {
        "Key": "NUTCH-1200",
        "Summary": "Resolving Ivy dependencies in several plugins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Nov/11 01:06",
        "Updated": "01/Dec/11 12:08",
        "Resolved": "27/Nov/11 16:32",
        "Description": "When configuring Nutch 1.5-SNAPSHOT in Eclipse, I noticed that any plugins requiring additional libraries OVER AND ABOVE the ones specified in NUTCH_HOME/ivy/ivy.xml cannot resolve the dependencies. In specific the classes are \n\n- FeedParser <dependency org=\"net.java.dev.rome\" name=\"rome\" rev=\"1.0.0\" conf=\"*->master\"/>\n- URLAutomationFilter - <dependency org=\"dk.brics\" name=\"automaton\" rev=\"???\"/>\n- SWFParser <dependency org=\"com.google.gwt\" name=\"gwt-incubator\" rev=\"2.0.1\"/>\n- HTMLParser   <dependency org=\"net.sourceforge.nekohtml\" name=\"nekohtml\" rev=\"1.9.15\"/> \n\n\nFurther to this, I cannot locate the dk.brics dependency!\nFinally, the plugin/ivy.xml files for the above plugins cannot be parsed corectly due to the ${nutch.root} vairable.",
        "Issue Links": []
    },
    "NUTCH-1201": {
        "Key": "NUTCH-1201",
        "Summary": "Allow for different FetcherThread impls",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Nov/11 13:02",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "For certain cases we need to modify parts in FetcherThread and make it pluggable. This introduces a new config directive fetcher.impl that takes a FQCN and uses that setting Fetcher.fetch to load a class to use for job.setMapRunnerClass(). This new class has to extend Fetcher and and inner class FetcherThread. This allows for overriding methods in FetcherThread but also methods in Fetcher itself if required.\nA follow up on this issue would be to refactor parts of FetcherThread to make it easier to override small sections instead of copying the entire method body for a small change, which is now the case.",
        "Issue Links": []
    },
    "NUTCH-1202": {
        "Key": "NUTCH-1202",
        "Summary": "Fetcher timebomb kills long waiting fetch jobs",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "11/Nov/11 13:21",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The timebomb feature kills of mappers of jobs that have been waiting too long in the job queue. The timebomb feature should start at mapper initialization instead, not in job init.\nThoughts?",
        "Issue Links": []
    },
    "NUTCH-1203": {
        "Key": "NUTCH-1203",
        "Summary": "ParseSegment to list ms per record",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Nov/11 15:13",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Nov/11 15:16",
        "Description": "Small change to show number of milliseconds per parsed record.",
        "Issue Links": []
    },
    "NUTCH-1204": {
        "Key": "NUTCH-1204",
        "Summary": "not all of pages parsed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Invalid",
        "Affects Version/s": "1.3",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "12/Nov/11 06:39",
        "Updated": "22/May/13 03:53",
        "Resolved": "14/Nov/11 15:40",
        "Description": "when we fetch a site in multiple segments, and dump crawldb with readdb, the system says that some of pages are unfetched, and when we checked, we find that these pages were fetched and stored but was not parsed\nwe try to crawl a site with only html pages and edit suffix-urlfilter.txt and parser.timeout property and test it and find that only some of html pages are parsed\nthis is a critical situation for performance because fetching of sites is well but parsing of them in iterations cause refetching these sites",
        "Issue Links": []
    },
    "NUTCH-1205": {
        "Key": "NUTCH-1205",
        "Summary": "Upgrade gora modules to 0.2 in ivy/ivy.xml",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Nov/11 12:04",
        "Updated": "04/May/12 04:19",
        "Resolved": "03/May/12 12:58",
        "Description": "Although gora trunk is unstable, work is ongoing to get this fixed. For the time being, I think Nutchgora should use gora trunk as this will identify more vulnerabilities. I'll get the trivial patch submitted shortly.",
        "Issue Links": [
            "/jira/browse/GORA-84",
            "/jira/browse/NUTCH-896",
            "/jira/browse/NUTCH-902"
        ]
    },
    "NUTCH-1206": {
        "Key": "NUTCH-1206",
        "Summary": "tika parser of nutch 1.3 is failing to prcess pdfs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.3",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "dibyendu ghosh",
        "Created": "21/Nov/11 07:01",
        "Updated": "22/May/13 03:53",
        "Resolved": "02/Dec/11 17:05",
        "Description": "Please refer to this message: http://www.mail-archive.com/user%40nutch.apache.org/msg04315.html. Old parse-pdf parser seems to be able to parse old pdfs (checked with nutch 1.2) though it is not able to parse acrobat 9.0 version of pdfs. nutch 1.3 does not have parse-pdf plugin and it is not able to parse even older pdfs.\nmy code (TestParse.java):\n----------------------------\nbash-2.00$ cat TestParse.java\nimport java.io.File;\nimport java.io.FileInputStream;\nimport java.io.FileOutputStream;\nimport java.io.PrintStream;\nimport java.util.Iterator;\nimport java.util.Map;\nimport java.util.Map.Entry;\nimport org.apache.hadoop.conf.Configuration;\nimport org.apache.hadoop.io.Text;\nimport org.apache.nutch.metadata.Metadata;\nimport org.apache.nutch.parse.ParseResult;\nimport org.apache.nutch.parse.Parse;\nimport org.apache.nutch.parse.ParseStatus;\nimport org.apache.nutch.parse.ParseUtil;\nimport org.apache.nutch.parse.ParseData;\nimport org.apache.nutch.protocol.Content;\nimport org.apache.nutch.util.NutchConfiguration;\npublic class TestParse {\n    private static Configuration conf = NutchConfiguration.create();\n    public TestParse() {\n    }\n    public static void main(String[] args) \n{\n        String filename = args[0];\n        convert(filename);\n    }\n\n    public static String convert(String fileName) {\n        String newName = \"abc.html\";\n        try \n{\n            System.out.println(\"Converting \" + fileName + \" to html.\");\n            if (convertToHtml(fileName, newName))\n                return newName;\n        }\n catch (Exception e) \n{\n            (new File(newName)).delete();\n            System.out.println(\"General exception \" + e.getMessage());\n        }\n        return null;\n    }\n    private static boolean convertToHtml(String fileName, String newName)\n        throws Exception {\n        // Read the file\n        FileInputStream in = new FileInputStream(fileName);\n        byte[] buf = new byte[in.available()];\n        in.read(buf);\n        in.close();\n        // Parse the file\n        Content content = new Content(\"file:\" + fileName, \"file:\" +\nfileName,\n                                      buf, \"\", new Metadata(), conf);\n        ParseResult parseResult = new ParseUtil(conf).parse(content);\n        parseResult.filter();\n        if (parseResult.isEmpty()) \n{\n            System.out.println(\"All parsing attempts failed\");\n            return false;\n        }\n        Iterator<Map.Entry<Text,Parse>> iterator =\nparseResult.iterator();\n        if (iterator == null) \n{\n            System.out.println(\"Cannot iterate over successful parse\nresults\");\n            return false;\n        }\n        Parse parse = null;\n        ParseData parseData = null;\n        while (iterator.hasNext()) {\n            parse = parseResult.get((Text)iterator.next().getKey());\n            parseData = parse.getData();\n            ParseStatus status = parseData.getStatus();\n            // If Parse failed then bail\n            if (!ParseStatus.STATUS_SUCCESS.equals(status)) \n{\n                System.out.println(\"Could not parse \" + fileName + \". \" +\n                            status.getMessage());\n                return false;\n            }\n        }\n        // Start writing to newName\n        FileOutputStream fout = new FileOutputStream(newName);\n        PrintStream out = new PrintStream(fout, true, \"UTF-8\");\n        // Start Document\n        out.println(\"<html>\");\n        // Start Header\n        out.println(\"<head>\");\n        // Write Title\n        String title = parseData.getTitle();\n        if (title != null && title.trim().length() > 0) \n{\n            out.println(\"<title>\" + parseData.getTitle() + \"</title>\");\n        }\n\n        // Write out Meta tags\n        Metadata metaData = parseData.getContentMeta();\n        String[] names = metaData.names();\n        for (String name : names) {\n            String[] subvalues = metaData.getValues(name);\n            String values = null;\n            for (String subvalue : subvalues) \n{\n                values += subvalue;\n            }\n            if (values.length() > 0)\n                out.printf(\"<meta name=\\\"%s\\\" content=\\\"%s\\\"/>\\n\",\n                           name, values);\n        }\n        out.println(\"<meta http-equiv=\\\"Content-Type\\\"\ncontent=\\\"text/html;charset=UTF-8\\\"/>\");\n        // End Meta tags\n        out.println(\"</head>\"); // End Header\n        // Start Body\n        out.println(\"<body>\");\n        out.print(parse.getText());\n        out.println(\"</body>\"); // End Body\n        out.println(\"</html>\"); // End Document\n        out.close(); // Close the file\n        return true;\n    }\n}\n----------------------------\ncommand:\n======\nbash-2.00$ java -classpath\nconf:runtime/local/lib/nutch-1.3.jar:runtime/local/lib/hadoop-core-0.20.2.jar:runtime/local/lib/commons-logging-api-1.0.4.jar:runtime/local/lib/tika-core-0.9.jar:runtime/local/lib/log4j-1.2.15.jar:runtime/local/lib/oro-2.0.8.jar:.\nTestParse direct.pdf\n======\noutput:\n_____\nConverting direct.pdf to html.\nOct 19, 2011 5:05:19 PM org.apache.hadoop.conf.Configuration\ngetConfResourceAsInputStream\nINFO: found resource tika-mimetypes.xml at\nfile:/path/to/nutch/1.3/conf/tika-mimetypes.xml\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginManifestParser\nparsePluginFolder\nINFO: Plugins: looking in: /path/to/nutch/1.3/runtime/local/plugins\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO: Plugin Auto-activation mode: [true]\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO: Registered Plugins:\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   the nutch core extension points (nutch-extensionpoints)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   Tika Parser Plug-in (parse-tika)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO: Registered Extension-Points:\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   Nutch URL Normalizer\n(org.apache.nutch.net.URLNormalizer)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   Nutch Protocol (org.apache.nutch.protocol.Protocol)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   Nutch Segment Merge Filter\n(org.apache.nutch.segment.SegmentMergeFilter)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   Nutch URL Filter (org.apache.nutch.net.URLFilter)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   Nutch Indexing Filter\n(org.apache.nutch.indexer.IndexingFilter)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   HTML Parse Filter\n(org.apache.nutch.parse.HtmlParseFilter)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   Nutch Content Parser (org.apache.nutch.parse.Parser)\nOct 19, 2011 5:05:20 PM org.apache.nutch.plugin.PluginRepository\ndisplayStatusINFO:   Nutch Scoring (org.apache.nutch.scoring.ScoringFilter)\nOct 19, 2011 5:05:20 PM org.apache.hadoop.conf.Configuration\ngetConfResourceAsInputStream\nINFO: found resource parse-plugins.xml at\nfile:/path/to/nutch/1.3/conf/parse-plugins.xml\nOct 19, 2011 5:05:20 PM org.apache.nutch.parse.ParserFactory matchExtensions\nINFO: The parsing plugins: [org.apache.nutch.parse.tika.TikaParser] are\nenabled via the plugin.includes system property, and all claim to support\nthe content type application/pdf, but they are not mapped to it  in the\nparse-plugins.xml file\nOct 19, 2011 5:05:21 PM org.apache.nutch.parse.ParseUtil parse\nWARNING: Unable to successfully parse content file:direct.pdf of type\napplication/pdf\nOct 19, 2011 5:05:21 PM org.apache.nutch.parse.ParseResult filter\nWARNING: file:direct.pdf is not parsed successfully, filtering\nAll parsing attempts failed\n_____\nmy customized nutch-site.xml:\n~~~~~~~~~~~~~~~~~~~~\nbash-2.00$ cat conf/nutch-site.xml\n<?xml version=\"1.0\"?>\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n<configuration>\n  <property>\n    <name>plugin.folders</name>\n    <value>runtime/local/plugins</value>\n    <description>Directories where nutch plugins are located.  Each\n    element may be a relative or absolute path.  If absolute, it is used\n    as is.  If relative, it is searched for on the classpath.</description>\n  </property>\n  <property>\n    <name>plugin.includes</name>\n    <value>parse-tika</value>\n    <description>Regular expression naming plugin directory names to\n    include. Any plugin not matching this expression is excluded.\n    </description>\n  </property>\n</configuration>\n~~~~~~~~~~~~~~~~~~~~",
        "Issue Links": []
    },
    "NUTCH-1207": {
        "Key": "NUTCH-1207",
        "Summary": "ParserChecker to output signature",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Nov/11 13:37",
        "Updated": "22/May/13 03:54",
        "Resolved": "21/Nov/11 13:42",
        "Description": "ParserChecker should calculate and display the signature. Makes debugging a bit easier.",
        "Issue Links": []
    },
    "NUTCH-1208": {
        "Key": "NUTCH-1208",
        "Summary": "Don't include KEYS file in bin distribution",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "build",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Julien Nioche",
        "Created": "22/Nov/11 15:05",
        "Updated": "22/May/13 03:54",
        "Resolved": "03/Apr/12 12:40",
        "Description": "We should get rid of the KEYS file in the bin packaging (zip/tar) in 1.5.",
        "Issue Links": []
    },
    "NUTCH-1209": {
        "Key": "NUTCH-1209",
        "Summary": "Output from ParserChecker Url missing a newline",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "24/Nov/11 05:49",
        "Updated": "22/May/13 03:54",
        "Resolved": "24/Nov/11 05:52",
        "Description": "While working on:\nhttp://www.mail-archive.com/user@nutch.apache.org/msg04688.html\nI found out that the ParserChecker is missing a newline in its report.\nE.g., note:\n\n./bin/nutch org.apache.nutch.parse.ParserChecker http://vault.fbi.gov/watergate/watergate-summary-part-01-of-02/view\n\n\nproduces:\n\nfetching: http://vault.fbi.gov/watergate/watergate-summary-part-01-of-02/view\nparsing: http://vault.fbi.gov/watergate/watergate-summary-part-01-of-02/view\ncontentType: application/xhtml+xml\n---------\nUrl\n---------------\nhttp://vault.fbi.gov/watergate/watergate-summary-part-01-of-02/view---------\nParseData\n---------\nVersion: 5\n...snip\n\n\nNote that there is no space between view and -----.",
        "Issue Links": []
    },
    "NUTCH-1210": {
        "Key": "NUTCH-1210",
        "Summary": "DomainBlacklistFilter",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Nov/11 16:27",
        "Updated": "22/May/13 03:53",
        "Resolved": "23/Feb/12 12:33",
        "Description": "The current DomainFilter acts as a white list. We also need a filter that acts as a black list so we can allow tld's and/or domains with DomainFilter but blacklist specific subdomains. If we would patch the current DomainFilter for this behaviour it would break current semantics such as it's precedence. Therefore i would propose a new filter instead.",
        "Issue Links": [
            "/jira/browse/NUTCH-1240"
        ]
    },
    "NUTCH-1211": {
        "Key": "NUTCH-1211",
        "Summary": "URLFilterChecker command line help doesn't inform user of STDIN requirements",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "linkdb",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "25/Nov/11 01:54",
        "Updated": "22/May/13 03:54",
        "Resolved": "25/Nov/11 02:02",
        "Description": "In URLFilterChecker, the cmd line tool requires URLs to be fed into it on STDIN, but \nthat isn't documented anywhere, even in the tool help printed to STDOUT.",
        "Issue Links": []
    },
    "NUTCH-1212": {
        "Key": "NUTCH-1212",
        "Summary": "ParseOutputFormat has redundant code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "25/Nov/11 04:39",
        "Updated": "22/May/13 03:54",
        "Resolved": "02/Jan/12 11:57",
        "Description": "In ParseOutputFormat, I see a code block:\n\n         // collect outlinks for subsequent db update\n         Outlink[] links = parseData.getOutlinks();\n         int outlinksToStore = Math.min(maxOutlinks, links.length);\n         if (ignoreExternalLinks) {\n           try {\n             fromHost = new URL(fromUrl).getHost().toLowerCase();\n           } catch (MalformedURLException e) {\n             fromHost = null;\n           }\n         } else {\n           fromHost = null;\n         }\n\n\nThe if(ignoreExternalLinks) part then gets subsequently set and \nreset in the ensuing for loop:\n\n         int validCount = 0;\n         CrawlDatum adjust = null;\n         List<Entry<Text, CrawlDatum>> targets = new ArrayList<Entry<Text, CrawlDatum>>(outlinksToStore);\n         List<Outlink> outlinkList = new ArrayList<Outlink>(outlinksToStore);\n         for (int i = 0; i < links.length && validCount < outlinksToStore; i++) {\n           String toUrl = links[i].getToUrl();\n           // ignore links to self (or anchors within the page)\n           if (fromUrl.equals(toUrl)) {\n             continue;\n           }\n           if (ignoreExternalLinks) {\n             try {\n               toHost = new URL(toUrl).getHost().toLowerCase();\n             } catch (MalformedURLException e) {\n               toHost = null;\n             }\n             if (toHost == null || !toHost.equals(fromHost)) { // external links\n               continue; // skip it\n             }\n           }\n\n\nIsn't that redundant? I don't think the first if block is needed.",
        "Issue Links": [
            "/jira/browse/NUTCH-1184"
        ]
    },
    "NUTCH-1213": {
        "Key": "NUTCH-1213",
        "Summary": "Pass additional SolrParams when indexing to Solr",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Andrzej Bialecki",
        "Reporter": "Andrzej Bialecki",
        "Created": "25/Nov/11 10:14",
        "Updated": "22/May/13 03:53",
        "Resolved": "28/Nov/11 14:27",
        "Description": "This is a simple improvement of the SolrIndexer. It adds the ability to pass additional Solr parameters that are applied to each UpdateRequest. This is useful when you have to pass parameters specific to a particular indexing run, which are not in Solr invariants for the update handler, and modifying the Solr configuration for each different indexing run is inconvenient.",
        "Issue Links": []
    },
    "NUTCH-1214": {
        "Key": "NUTCH-1214",
        "Summary": "DomainStats tool should be named for what it's doing",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "29/Nov/11 16:54",
        "Updated": "22/May/13 03:54",
        "Resolved": "29/Nov/11 16:57",
        "Description": "DomainStats tool can calculate on host, domain and suffix. The job name should reflect these types.",
        "Issue Links": []
    },
    "NUTCH-1215": {
        "Key": "NUTCH-1215",
        "Summary": "UpdateDB should not require segment as input",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.6",
        "Component/s": "linkdb",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "29/Nov/11 17:18",
        "Updated": "22/May/13 03:53",
        "Resolved": "23/Oct/12 09:46",
        "Description": "UpdateDB requires an input segment. This causes the metrics for the records of the segment to change, e.g. from fetched to not_modified and changes an adaptive fetch schedule accordingly. This should not happen when one needs to update for filtering of normalizing or other maintenance.",
        "Issue Links": []
    },
    "NUTCH-1216": {
        "Key": "NUTCH-1216",
        "Summary": "Add trivial comment to lib/native/README.txt",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "06/Dec/11 19:45",
        "Updated": "11/Dec/11 04:26",
        "Resolved": "09/Dec/11 14:55",
        "Description": "This trivial issue simply adds missing comments to the above file. The WARN logging which is churned out has caused a small degree of confusion in the past, therefore this sorts that out :0)",
        "Issue Links": []
    },
    "NUTCH-1217": {
        "Key": "NUTCH-1217",
        "Summary": "Update NOTICE.txt to drop some copyrights",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Dec/11 12:13",
        "Updated": "27/Dec/11 05:12",
        "Resolved": "26/Dec/11 16:18",
        "Description": "We have many references to software copyrights which should be dropped. Most of these relate to the Lucene legacy days.\n-Carrot2\n-saxpath\n-jaxen\n-jdom\n-snowball\n-violinstrings\n-Jena\n-bouncycastle\n-fontbox\n-jempbox\n-pdfbox\n-rome\nAlso some need to be added\n-slf4j\n-activation\n-mortbay (jetty)\n-jline\n-junit\n-stax\n-wstx\nAs I am unfamiliar with most of these, and that is important to inlcude all references to software outside of the ASF, I would appreciate if this list could act as a beginning for completing this issue.",
        "Issue Links": []
    },
    "NUTCH-1218": {
        "Key": "NUTCH-881 Good quality documentation for Nutch",
        "Summary": "Improve trunk API documentation",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.10",
        "Component/s": "documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Dec/11 18:21",
        "Updated": "09/Jan/15 08:14",
        "Resolved": "09/Jan/15 06:05",
        "Description": "The trunk API Java documentation could do with some improving. This issue should track that. It should however not seek to change any functionality within the codebase, only to substantiate and improve the existing documentation.",
        "Issue Links": []
    },
    "NUTCH-1219": {
        "Key": "NUTCH-1219",
        "Summary": "Upgrade all jobs to new MapReduce API",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "13/Dec/11 16:46",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Apr/18 10:59",
        "Description": "We should upgrade to the new Hadoop API for Nutch trunk as already has been done for the Nutchgora branch. If i'm not mistaken we can already upgrade to the latest 0.20.5 version that still carries the legacy API so we can, without immediately upgrading to 0.21 or higher, port the jobs to the new API without having the need for a separate branch to work on.\nTo the committers who created/ported jobs in NutchGora, please write down your advice and experience.\nhttp://www.slideshare.net/sh1mmer/upgrading-to-the-new-map-reduce-api",
        "Issue Links": [
            "/jira/browse/NUTCH-2375"
        ]
    },
    "NUTCH-1220": {
        "Key": "NUTCH-1220",
        "Summary": "Upgrade Solr deps",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "14/Dec/11 10:42",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Oct/19 12:45",
        "Description": "SlfJ4 needs to be part of upgrade to Solr 3.5 but that breaks something else. Likely Hadoop has a different Slf4J version?\n\nException in thread \"main\" java.lang.NoSuchMethodError: org.slf4j.spi.LocationAwareLogger.log(Lorg/slf4j/Marker;Ljava/lang/String;ILjava/lang/String;[Ljava/lang/Object;Ljava/lang/Throwable;)V\n        at org.apache.commons.logging.impl.SLF4JLocationAwareLog.debug(SLF4JLocationAwareLog.java:133)\n        at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:136)\n        at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:180)\n        at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:159)\n        at org.apache.hadoop.security.UserGroupInformation.isSecurityEnabled(UserGroupInformation.java:216)\n        at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:409)\n        at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:395)\n        at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:1418)\n        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1319)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:226)\n        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:109)\n        at org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:544)\n        at org.apache.hadoop.mapred.FileInputFormat.addInputPath(FileInputFormat.java:339)\n        at org.apache.nutch.util.domain.DomainStatistics.run(DomainStatistics.java:108)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.util.domain.DomainStatistics.main(DomainStatistics.java:215)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
        "Issue Links": [
            "/jira/browse/NUTCH-1633",
            "/jira/browse/NUTCH-2600"
        ]
    },
    "NUTCH-1221": {
        "Key": "NUTCH-1219 Upgrade all jobs to new MapReduce API",
        "Summary": "Migrate DomainStatistics to MapReduce API",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "14/Dec/11 11:50",
        "Updated": "22/May/13 03:54",
        "Resolved": "16/Dec/11 11:17",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1222": {
        "Key": "NUTCH-1222",
        "Summary": "Upgrade to new Hadoop 0.22.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "14/Dec/11 13:39",
        "Updated": "22/May/13 03:53",
        "Resolved": "03/Apr/12 11:26",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1228"
        ]
    },
    "NUTCH-1223": {
        "Key": "NUTCH-1219 Upgrade all jobs to new MapReduce API",
        "Summary": "Migrate WebGraph to MapReduce API",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "lufeng",
        "Reporter": "Markus Jelsma",
        "Created": "14/Dec/11 13:40",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Apr/18 10:58",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-2375"
        ]
    },
    "NUTCH-1224": {
        "Key": "NUTCH-1219 Upgrade all jobs to new MapReduce API",
        "Summary": "Migrate FreeGenerator to MapReduce API",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "14/Dec/11 14:34",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Apr/18 10:58",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-2375"
        ]
    },
    "NUTCH-1225": {
        "Key": "NUTCH-1219 Upgrade all jobs to new MapReduce API",
        "Summary": "Migrate CrawlDBScanner to MapReduce API",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "14/Dec/11 14:34",
        "Updated": "22/May/13 03:53",
        "Resolved": "03/Apr/12 11:25",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1226": {
        "Key": "NUTCH-1219 Upgrade all jobs to new MapReduce API",
        "Summary": "Migrate CrawlDbReader to MapReduce API",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "15/Dec/11 13:47",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Apr/18 10:55",
        "Description": "Hadoop 0.21 only!",
        "Issue Links": [
            "/jira/browse/NUTCH-2375"
        ]
    },
    "NUTCH-1227": {
        "Key": "NUTCH-1227",
        "Summary": "Set mapreduce.map.speculative for Hadoop 0.21 or higher",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "18/Dec/11 16:45",
        "Updated": "22/May/13 03:54",
        "Resolved": "19/Dec/11 14:13",
        "Description": "Configuration option has been renamed. Fetcher still uses old config to disable speculative exection.",
        "Issue Links": []
    },
    "NUTCH-1228": {
        "Key": "NUTCH-1228",
        "Summary": "Change mapred.task.timeout to mapreduce.task.timeout in fetcher",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.14",
        "Fix Version/s": "2.4,                                            1.15",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Dec/11 14:15",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "26/Apr/18 10:33",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1222",
            "https://github.com/apache/nutch/pull/319"
        ]
    },
    "NUTCH-1229": {
        "Key": "NUTCH-1229",
        "Summary": "Add freegenerator, domainstat and crawldbscanner to log4j",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Dec/11 10:21",
        "Updated": "20/Dec/11 11:26",
        "Resolved": "20/Dec/11 10:22",
        "Description": "These are missing in our log4j.properties:\n\nIndex: conf/log4j.properties\n===================================================================\n--- conf/log4j.properties       (revision 1221184)\n+++ conf/log4j.properties       (working copy)\n@@ -32,6 +32,9 @@\n log4j.logger.org.apache.nutch.scoring.webgraph.ScoreUpdater=INFO,cmdstdout\n log4j.logger.org.apache.nutch.parse.ParserChecker=INFO,cmdstdout\n log4j.logger.org.apache.nutch.indexer.IndexingFiltersChecker=INFO,cmdstdout\n+log4j.logger.org.apache.nutch.tools.FreeGenerator=INFO,cmdstdout\n+log4j.logger.org.apache.nutch.util.domain.DomainStatistics=INFO,cmdstdout\n+log4j.logger.org.apache.nutch.tools.CrawlDBScanner=INFO,cmdstdout\n \n log4j.logger.org.apache.nutch=INFO\n log4j.logger.org.apache.hadoop=WARN",
        "Issue Links": []
    },
    "NUTCH-1230": {
        "Key": "NUTCH-1230",
        "Summary": "MimeType API deprecated and breaks with Tika 1.0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/11 11:25",
        "Updated": "22/May/13 03:53",
        "Resolved": "27/Dec/11 14:36",
        "Description": "We used Tika 1.0-SNAPSHOT in production and just switched to 1.1-SNAPSHOT. The new version triggers the following error:\n\n2011-12-21 12:29:56,665 ERROR http.Http - java.lang.IllegalAccessError: tried to access method org.apache.tika.mime.MimeTypes.getMimeType([B)Lorg/apache/tika/mime/MimeType; from class org.apache.nutch.util.MimeUtil\n2011-12-21 12:29:56,665 ERROR http.Http - at org.apache.nutch.util.MimeUtil.autoResolveContentType(MimeUtil.java:169)\n2011-12-21 12:29:56,665 ERROR http.Http - at org.apache.nutch.protocol.Content.getContentType(Content.java:292)\n2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.nutch.protocol.Content.<init>(Content.java:88)\n2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:142)\n2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.nutch.parse.ParserChecker.run(ParserChecker.java:82)\n2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)\n2011-12-21 12:29:56,666 ERROR http.Http - at org.apache.nutch.parse.ParserChecker.main(ParserChecker.java:138)",
        "Issue Links": [
            "/jira/browse/NUTCH-1064",
            "/jira/browse/NUTCH-1017",
            "/jira/browse/NUTCH-1041",
            "/jira/browse/NUTCH-1549",
            "/jira/browse/NUTCH-1231"
        ]
    },
    "NUTCH-1231": {
        "Key": "NUTCH-1231",
        "Summary": "Upgrade to Tika 1.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/11 11:30",
        "Updated": "22/May/13 03:53",
        "Resolved": "27/Dec/11 14:36",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1234",
            "/jira/browse/NUTCH-1230"
        ]
    },
    "NUTCH-1232": {
        "Key": "NUTCH-1232",
        "Summary": "Remove host  field from index-basic",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/11 15:20",
        "Updated": "22/May/13 03:54",
        "Resolved": "02/Jan/12 13:17",
        "Description": "Either fields needs to be removed, it makes no sense to have two identical values for separate fields. I propose to get rid of the site field and leave the host field. This may be a breaking change for some installations however.",
        "Issue Links": []
    },
    "NUTCH-1233": {
        "Key": "NUTCH-1233",
        "Summary": "Rely on Tika for outlink extraction",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/11 15:59",
        "Updated": "16/Feb/16 14:09",
        "Resolved": "16/Feb/16 13:39",
        "Description": "Tika provides outlink extraction features that are not used in Nutch. To be able to use it in Nutch we need Tika to return the rel attr value of each link, which it currently doesn't. There's a patch for Tika 1.1. If that patch is included in Tika and we upgraded to that new version this issue can be worked on. Here's preliminary code that does both Tika and current outlink extraction. This also includes parts of the Boilerpipe code.",
        "Issue Links": [
            "/jira/browse/TIKA-1835",
            "/jira/browse/TIKA-824",
            "/jira/browse/NUTCH-1234",
            "/jira/browse/NUTCH-2210",
            "/jira/browse/NUTCH-961",
            "/jira/browse/TIKA-975",
            "/jira/browse/TIKA-1835"
        ]
    },
    "NUTCH-1234": {
        "Key": "NUTCH-1234",
        "Summary": "Upgrade to Tika 1.1",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/11 16:00",
        "Updated": "22/May/13 03:53",
        "Resolved": "02/Apr/12 11:50",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1233",
            "/jira/browse/NUTCH-1231"
        ]
    },
    "NUTCH-1235": {
        "Key": "NUTCH-1235",
        "Summary": "Upgrade to new Hadoop 0.20.205.0",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Dec/11 14:41",
        "Updated": "22/May/13 03:54",
        "Resolved": "27/Dec/11 13:29",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1236": {
        "Key": "NUTCH-1236",
        "Summary": "Add link to site documentation to download older versions of Nutch.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Dec/11 14:06",
        "Updated": "05/Jan/12 15:07",
        "Resolved": "05/Jan/12 15:07",
        "Description": "As we are moving towards 2012 I thought it best to clear out my mailbox. I found an older email which requested the link to download older versions of Nutch. This was discussed and I think it would be best to get the link added to the site documentation.",
        "Issue Links": []
    },
    "NUTCH-1237": {
        "Key": "NUTCH-1237",
        "Summary": "Improve javac arguements for more verbose output",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Dec/11 16:23",
        "Updated": "06/Jan/12 04:24",
        "Resolved": "05/Jan/12 15:04",
        "Description": "When trying to fix another problem I stumbled across this one. I think it is important to ensure that the javac outputs info regarding deprecation and unchecked operations.",
        "Issue Links": []
    },
    "NUTCH-1238": {
        "Key": "NUTCH-1238",
        "Summary": "Fetcher throughput threshold must start before feeder finished",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Dec/11 20:25",
        "Updated": "22/May/13 03:54",
        "Resolved": "29/Dec/11 14:32",
        "Description": "Right now the fetcher's minimum throughput threshold is activated only when the feeder has finished. However, for various reasons a running fetch can be slow. This issue must change the feature to start checking earlier, but not right after initialization.",
        "Issue Links": []
    },
    "NUTCH-1239": {
        "Key": "NUTCH-1239",
        "Summary": "Webgraph should remove deleted pages from segment input",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "29/Dec/11 08:47",
        "Updated": "22/May/13 03:53",
        "Resolved": "02/Jan/12 13:12",
        "Description": "Webgraph's outlink job is currently unable to remove links. It should expand it's segment input and be able to remove nodes for pages that no longer exist.",
        "Issue Links": []
    },
    "NUTCH-1240": {
        "Key": "NUTCH-1240",
        "Summary": "Domain blacklist URL filter",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "29/Dec/11 17:17",
        "Updated": "22/May/13 03:54",
        "Resolved": "02/Jan/12 11:54",
        "Description": "We need a filter that behaves opposite of the current domain url filter for tld, domain and (sub)host black listing.",
        "Issue Links": [
            "/jira/browse/NUTCH-1210"
        ]
    },
    "NUTCH-1241": {
        "Key": "NUTCH-1241",
        "Summary": "CrawlDBScanner should also be able to find records",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/Jan/12 07:58",
        "Updated": "22/May/13 03:54",
        "Resolved": "04/Jan/12 09:10",
        "Description": "The CrawlDBScanner cannot find partial matches because it uses String.match(); Instead, it should be able to use the Matcher.find() to find partial matches. Right now regex \"http\" will never match any records. It can then also reuse a compiled pattern.",
        "Issue Links": [
            "/jira/browse/NUTCH-806"
        ]
    },
    "NUTCH-1242": {
        "Key": "NUTCH-1242",
        "Summary": "Allow disabling of URL Filters in ParseSegment",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Edward Drapkin",
        "Created": "04/Jan/12 22:31",
        "Updated": "22/May/13 03:53",
        "Resolved": "31/Jan/12 15:25",
        "Description": "Right now, the ParseSegment job does not allow you to disable URL filtration.  For reasons that aren't worth explaining, I need to do this, so I enabled this behavior through the use of a boolean configuration value \"parse.filter.urls\" which defaults to true.\nI've attached a simple, preliminary patch that enables this behavior with that configuration option.  I'm not sure if it should be made a command line option or not.",
        "Issue Links": []
    },
    "NUTCH-1243": {
        "Key": "NUTCH-1243",
        "Summary": "Junit jar removed from lib",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "05/Jan/12 11:29",
        "Updated": "24/May/13 04:20",
        "Resolved": "13/May/13 15:12",
        "Description": "When calling 'ant test' the junit jar is added to the lib dir by Ivy but gets removed before the test classes are compiled.\nThis seems to happen with Ivy 2.1 but not with Ivy 2.2.\nWe do have 2.2 in the /ivy directory but the ant script uses whatever is found in ~/.ant/lib - ideally we would like to be able to force the location of the jar file.\nActually the issue also happens with Ivy 2.2. I will commit a quick fix consisting of adding junit in the default ivy configuration, however it will be good to get to the bottom of this.",
        "Issue Links": []
    },
    "NUTCH-1244": {
        "Key": "NUTCH-1244",
        "Summary": "CrawlDBDumper to filter by regex",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Jan/12 14:10",
        "Updated": "29/Jul/13 13:40",
        "Resolved": "09/Jan/12 16:01",
        "Description": "The CrawlDBDumper tool should be able to filter records by an option regular expression.",
        "Issue Links": [
            "/jira/browse/NUTCH-806"
        ]
    },
    "NUTCH-1245": {
        "Key": "NUTCH-1245",
        "Summary": "URL gone with 404 after db.fetch.interval.max stays db_unfetched in CrawlDb and is generated over and over again",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            1.5",
        "Fix Version/s": "1.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "05/Jan/12 16:22",
        "Updated": "03/Nov/13 08:02",
        "Resolved": "20/Jun/13 20:56",
        "Description": "A document gone with 404 after db.fetch.interval.max (90 days) has passed\nis fetched over and over again but although fetch status is fetch_gone\nits status in CrawlDb keeps db_unfetched. Consequently, this document will\nbe generated and fetched from now on in every cycle.\nTo reproduce:\n\ncreate a CrawlDatum in CrawlDb which retry interval hits db.fetch.interval.max (I manipulated the shouldFetch() in AbstractFetchSchedule to achieve this)\nnow this URL is fetched again\nbut when updating CrawlDb with the fetch_gone the CrawlDatum is reset to db_unfetched, the retry interval is fixed to 0.9 * db.fetch.interval.max (81 days)\nthis does not change with every generate-fetch-update cycle, here for two segments:\n\n/tmp/testcrawl/segments/20120105161430\nSegmentReader: get 'http://localhost/page_gone'\nCrawl Generate::\nStatus: 1 (db_unfetched)\nFetch time: Thu Jan 05 16:14:21 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nMetadata: _ngt_: 1325776461784_pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\nCrawl Fetch::\nStatus: 37 (fetch_gone)\nFetch time: Thu Jan 05 16:14:48 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nMetadata: _ngt_: 1325776461784_pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\n\n/tmp/testcrawl/segments/20120105161631\nSegmentReader: get 'http://localhost/page_gone'\nCrawl Generate::\nStatus: 1 (db_unfetched)\nFetch time: Thu Jan 05 16:16:23 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nMetadata: _ngt_: 1325776583451_pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\nCrawl Fetch::\nStatus: 37 (fetch_gone)\nFetch time: Thu Jan 05 16:20:05 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nMetadata: _ngt_: 1325776583451_pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\n\n\nAs far as I can see it's caused by setPageGoneSchedule() in AbstractFetchSchedule. Some pseudo-code:\n\nsetPageGoneSchedule (called from update / CrawlDbReducer.reduce):\n    datum.fetchInterval = 1.5 * datum.fetchInterval // now 1.5 * 0.9 * maxInterval\n    datum.fetchTime = fetchTime + datum.fetchInterval // see NUTCH-516\n    if (maxInterval < datum.fetchInterval) // necessarily true\n       forceRefetch()\n\nforceRefetch:\n    if (datum.fetchInterval > maxInterval) // true because it's 1.35 * maxInterval\n       datum.fetchInterval = 0.9 * maxInterval\n    datum.status = db_unfetched // \n\n\nshouldFetch (called from generate / Generator.map):\n    if ((datum.fetchTime - curTime) > maxInterval)\n       // always true if the crawler is launched in short intervals\n       // (lower than 0.35 * maxInterval)\n       datum.fetchTime = curTime // forces a refetch\n\n\nAfter setPageGoneSchedule is called via update the state is db_unfetched and the retry interval 0.9 * db.fetch.interval.max (81 days). \nAlthough the fetch time in the CrawlDb is far in the future\n\n% nutch readdb testcrawl/crawldb -url http://localhost/page_gone\nURL: http://localhost/page_gone\nVersion: 7\nStatus: 1 (db_unfetched)\nFetch time: Sun May 06 05:20:05 CEST 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nScore: 1.0\nSignature: null\nMetadata: _pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\n\nthe URL is generated again because (fetch time - current time) is larger than db.fetch.interval.max.\nThe retry interval (datum.fetchInterval) oscillates between 0.9 and 1.35, and the fetch time is always close to current time + 1.35 * db.fetch.interval.max.\nIt's possibly a side effect of NUTCH-516, and may be related to NUTCH-578",
        "Issue Links": [
            "/jira/browse/NUTCH-578",
            "/jira/browse/NUTCH-1247",
            "/jira/browse/NUTCH-1588"
        ]
    },
    "NUTCH-1246": {
        "Key": "NUTCH-1246",
        "Summary": "Upgrade to Hadoop 1.0.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "12/Jan/12 15:40",
        "Updated": "18/Apr/12 09:37",
        "Resolved": "18/Apr/12 09:37",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1247": {
        "Key": "NUTCH-1247",
        "Summary": "CrawlDatum.retries should be int",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "12/Jan/12 18:41",
        "Updated": "29/May/15 20:12",
        "Resolved": "29/May/15 20:12",
        "Description": "CrawlDatum.retries is a byte and goes bad with larger values.\n12/01/12 18:35:22 INFO crawl.CrawlDbReader: retry -127: 1\n12/01/12 18:35:22 INFO crawl.CrawlDbReader: retry -128: 1",
        "Issue Links": [
            "/jira/browse/NUTCH-578",
            "/jira/browse/NUTCH-1245"
        ]
    },
    "NUTCH-1248": {
        "Key": "NUTCH-1248",
        "Summary": "Generator to select on status",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "13/Jan/12 11:11",
        "Updated": "22/May/13 03:54",
        "Resolved": "13/Jan/12 16:43",
        "Description": "Generator should be able to select entries on status.",
        "Issue Links": [
            "/jira/browse/NUTCH-1177"
        ]
    },
    "NUTCH-1249": {
        "Key": "NUTCH-1249",
        "Summary": "Resolve all issues flagged up by adding javac -Xlint arguement",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build",
        "Assignee": "Tejas Patil",
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Jan/12 16:00",
        "Updated": "20/Jun/13 09:44",
        "Resolved": "22/May/13 09:10",
        "Description": "There are a heap of issues flagged up by NUTCH-1237, I think over time it would be great to get these addressed and resolved.\nWhat is interesting is that adding the same arguements to /src/plugin/plugin-build.xml actually breaks my build as tests begin to fail.\nSome of this stuff is documented in the link below\nhttp://docs.oracle.com/javase/1.5.0/docs/tooldocs/windows/javac.html#options",
        "Issue Links": []
    },
    "NUTCH-1250": {
        "Key": "NUTCH-1250",
        "Summary": "parse-html does not parse links with empty anchor",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Andreas Janning",
        "Created": "17/Jan/12 16:27",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The parse-html plugin does not generate an outlink if the link has no anchor\nFor example the following HTML-Code does not create an Outlink:\n\n \n  <a href=\"example.com\"></a>\n\n\nThe JUnit-Test TestDOMContentUtils tries to test this but fails since there is a comment inside the <a>-Tag.\nTestDOMContentUtils.java\nnew String(\"<html><head><title> title </title>\"\n        + \"</head><body>\"\n        + \"<a href=\\\"g\\\"><!--no anchor--></a>\"\n        + \"<a href=\\\"g1\\\"> <!--whitespace-->  </a>\"\n        + \"<a href=\\\"g2\\\">  <img src=test.gif alt='bla bla'> </a>\"\n        + \"</body></html>\"), \n\n\nWhen you remove the comment the test fails.\nTestDOMContentUtils.java Test fails\nnew String(\"<html><head><title> title </title>\"\n        + \"</head><body>\"\n        + \"<a href=\\\"g\\\"></a>\" // no anchor\n        + \"<a href=\\\"g1\\\"> <!--whitespace-->  </a>\"\n        + \"<a href=\\\"g2\\\">  <img src=test.gif alt='bla bla'> </a>\"\n        + \"</body></html>\"),",
        "Issue Links": []
    },
    "NUTCH-1251": {
        "Key": "NUTCH-1251",
        "Summary": "SolrDedup to use proper Lucene catch-all query",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.6",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Arkadi Kosmynin",
        "Created": "17/Jan/12 22:40",
        "Updated": "22/May/13 03:53",
        "Resolved": "26/Jun/12 08:22",
        "Description": "Deletion of duplicates fails. This happens because the \"get all\" query used to get Solr index size is \"id:[* TO *]\", which is a range query. Lucene is trying to expand it to a Boolean query and gets as many clauses as there are ids in the index. This is too many in a real situation and it throws an exception. \nTo correct this problem, change the \"get all\" query (SOLR_GET_ALL_QUERY) to \"*:*\", which is the standard Solr \"get all\" query.\nIndexing log extract:\njava.io.IOException: org.apache.solr.client.solrj.SolrServerException: Error executing query\n\tat org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat.getRecordReader(SolrDeleteDuplicates.java:236)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:338)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)\nCaused by: org.apache.solr.client.solrj.SolrServerException: Error executing query\n\tat org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:95)\n\tat org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:118)\n\tat org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat.getRecordReader(SolrDeleteDuplicates.java:234)\n\t... 3 more\nCaused by: org.apache.solr.common.SolrException: Internal Server Error\nInternal Server Error\nrequest: http://localhost:8081/arch/select?q=id:[* TO *]&fl=id,boost,tstamp,digest&start=0&rows=82938&wt=javabin&version=2\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n\tat org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:89)\n\t... 5 more",
        "Issue Links": []
    },
    "NUTCH-1252": {
        "Key": "NUTCH-1252",
        "Summary": "SegmentReader -get shows wrong data",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            1.5",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "18/Jan/12 08:58",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Oct/12 20:22",
        "Description": "The command/option -get of the SegmentReader may show wrong data associated with the given URL. \nTo reproduce:\n\n% mkdir -p test_readseg/urls\n% echo -e \"http://nutch.apache.org/\\ttest=ApacheNutch\\nhttp://abc.test/\\ttest=AbcTest\\tnutch.score=10.0\" > test_readseg/urls/seeds\n\n% nutch inject test_readseg/crawldb test_readseg/urls\nInjector: starting at 2012-01-18 09:32:25\nInjector: crawlDb: test_readseg/crawldb\nInjector: urlDir: test_readseg/urls\nInjector: Converting injected urls to crawl db entries.\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2012-01-18 09:32:28, elapsed: 00:00:03\n\n% nutch generate test_readseg/crawldb test_readseg/segments/\nGenerator: starting at 2012-01-18 09:32:30\nGenerator: Selecting best-scoring urls due for fetch.\nGenerator: filtering: true\nGenerator: normalizing: true\nGenerator: jobtracker is 'local', generating exactly one partition.\nGenerator: Partitioning selected urls for politeness.\nGenerator: segment: test_readseg/segments/20120118093232\nGenerator: finished at 2012-01-18 09:32:34, elapsed: 00:00:03\n\n% nutch readseg -get test_readseg/segments/* 'http://nutch.apache.org/' -nocontent -noparse -nofetch -noparsedata -noparsetext\nSegmentReader: get 'http://nutch.apache.org/'\nCrawl Generate::\nVersion: 7\nStatus: 1 (db_unfetched)\nFetch time: Wed Jan 18 09:32:26 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 2592000 seconds (30 days)\nScore: 10.0\nSignature: null\nMetadata: _ngt_: 1326875550401test: AbcTest\n\n\nThe metadata and the score indicate that the CrawlDatum shown is the wrong one (that associated to http://abc.test/ but not to http://nutch.apache.org/).",
        "Issue Links": []
    },
    "NUTCH-1253": {
        "Key": "NUTCH-1253",
        "Summary": "Incompatible neko and xerces versions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Dennis Spathis",
        "Created": "18/Jan/12 14:26",
        "Updated": "01/May/14 06:23",
        "Resolved": "29/Jan/14 14:16",
        "Description": "The Nutch 1.4 distribution includes\n\nnekohtml-0.9.5.jar (under .../runtime/local/plugins/lib-\nnekohtml)\nxercesImpl-2.9.1.jar (under .../runtime/local/lib)\n\nThese two JARs appear to be incompatible versions. When the HtmlParser (configured to use neko) is invoked during a local-mode crawl, the parse fails due to an AbstractMethodError. (Note: To see the AbstractMethodError, rebuild the HtmlParser plugin and add a\ncatch(Throwable) clause in the getParse method to log the stacktrace.)\nI found that substituting a later, compatible version of nekohtml (1.9.11)\nfixes the problem.\nCuriously, and in support of the above, the nekohtml plugin.xml file in\nNutch 1.4 contains the following:\n<plugin\n   id=\"lib-nekohtml\"\n   name=\"CyberNeko HTML Parser\"\n   version=\"1.9.11\"\n   provider-name=\"org.cyberneko\">\n   <runtime>\n       <library name=\"nekohtml-0.9.5.jar\">\n           <export name=\"*\"/>\n       </library>\n   </runtime>\n</plugin>\nNote the conflicting version numbers (version tag is \"1.9.11\" but the\nspecified library is \"nekohtml-0.9.5.jar\").\nWas the 0.9.5 version included by mistake? Was the intention rather to\ninclude 1.9.11?",
        "Issue Links": []
    },
    "NUTCH-1254": {
        "Key": "NUTCH-1254",
        "Summary": "NTLMv2 is not supported and HttpClient returns error code 500",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Later",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Remi Tassing",
        "Created": "18/Jan/12 14:45",
        "Updated": "18/Jan/12 15:34",
        "Resolved": "18/Jan/12 15:34",
        "Description": "When trying to access some SharePoint(IIS) website using NTLMv2 authentication, Nutch fails and gets an error code 500. HttpClient only supports an early version of NTLM but not NTLMv2. HttpUrlConnection can be used instead.\n[1]http://oaklandsoftware.com/papers/ntlm.html\n[2]http://developer-resource.blogspot.com/2008/06/ntlm-authentication-from-java.html",
        "Issue Links": []
    },
    "NUTCH-1255": {
        "Key": "NUTCH-1255",
        "Summary": "Change ivy.xml of all plugins to remove \"nutch.root\" property",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "20/Jan/12 14:44",
        "Updated": "24/Jan/12 04:20",
        "Resolved": "23/Jan/12 14:02",
        "Description": "Change ivy.xml of all plugins to replace \"nutch.root\" by \"../../../\". This will allow zero-configuration adding of IvyDE libraries in Eclipse. Note that parse-html already has the latter.",
        "Issue Links": []
    },
    "NUTCH-1256": {
        "Key": "NUTCH-1256",
        "Summary": "WebGraph to dump host + score",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Jan/12 17:25",
        "Updated": "22/May/13 03:53",
        "Resolved": "31/Jan/12 14:19",
        "Description": "WebGraph's NodeDumper tool can dump url,score information but a host|domain,score output can also be put to good use. This is likely to require a new MapReduce job as the NodeDumper's atonomy is not suited to return max or or summed scores. Code could also be merged with the tool.",
        "Issue Links": [
            "/jira/browse/NUTCH-1145"
        ]
    },
    "NUTCH-1257": {
        "Key": "NUTCH-1257",
        "Summary": "Support for the x-robots-tag HTTP Header",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Mike Lissner",
        "Created": "25/Jan/12 07:58",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Google and Bing both currently support the x-robots-tag HTTP header. This is important, because they have a policy of not crawling links that are in a robots.txt file, and not indexing links that are set to noindex. In the case that a page is indexed but not crawled, Google and Bing will show the page in their results, but it will lack a snippet (since they didn't crawl it, they can't generate one). \nAs a result, the only way to block Google and Bing from having a page in their index is to use the robots meta tag in HTML pages and the x-robots-tag in other mimetypes.\nAs a site owner that needs to keep specific pages private, I cannot trust robots.txt to keep my pages out of Google and Bing, and I have to use the two robots standards. Since Nutch doesn't support the HTTP header, I have to block it from crawling ALL non-HTML pages on my site.\nThis is not an ideal state of affairs, and it would be great if Nutch supported the x-robots-tag HTTP header.\nI've done more research on this topic on my blog:\n\nhttp://michaeljaylissner.com/blog/support-for-x-robots-tag-http-header-and-robots-HTML-meta-tag\nhttp://michaeljaylissner.com/blog/respecting-privacy-while-providing-hundreds-of-thousands-of-public-documents",
        "Issue Links": []
    },
    "NUTCH-1258": {
        "Key": "NUTCH-1258",
        "Summary": "MoreIndexingFilter should be able to read Content-Type from both parse metadata and content metadata",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "25/Jan/12 11:26",
        "Updated": "22/May/13 03:54",
        "Resolved": "01/Mar/12 15:38",
        "Description": "The MoreIndexingFilter reads the Content-Type from parse metadata. However, this usually contains a lot of crap because web developers can set it to anything they like. The filter must be able to read the Content-Type field from content metadata as well because that contains the type detected by Tika's Detector.",
        "Issue Links": [
            "/jira/browse/NUTCH-1259"
        ]
    },
    "NUTCH-1259": {
        "Key": "NUTCH-1259",
        "Summary": "Store detected content type in crawldatum metadata",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Markus Jelsma",
        "Created": "25/Jan/12 13:35",
        "Updated": "22/May/13 03:53",
        "Resolved": "15/Feb/12 13:07",
        "Description": "The MIME-type detected by Tika's Detect() API is never added to a Parse's ContentMetaData or ParseMetaData. Because of this bad Content-Types will end up in the documents.",
        "Issue Links": [
            "/jira/browse/NUTCH-1258",
            "/jira/browse/NUTCH-1293"
        ]
    },
    "NUTCH-1260": {
        "Key": "NUTCH-1260",
        "Summary": "Fetcher should log fetching of redirects",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Jan/12 12:17",
        "Updated": "22/May/13 03:54",
        "Resolved": "27/Jan/12 13:14",
        "Description": "The fetching of fetcher redirects (see property http.redirect.max) is not logged.\nUsing the default log level (INFO) fetcher reports all URLs to be fetched except for redirects.\nOne log message for every fetched URL is useful, esp. when searching for errors.",
        "Issue Links": []
    },
    "NUTCH-1261": {
        "Key": "NUTCH-1261",
        "Summary": "Make numReducers configurable for indexer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "30/Jan/12 10:31",
        "Updated": "22/May/13 03:54",
        "Resolved": "30/Jan/12 11:27",
        "Description": "The indexer should be able be configured to use only a specified number of reducers.",
        "Issue Links": []
    },
    "NUTCH-1262": {
        "Key": "NUTCH-1262",
        "Summary": "Map `duplicating` content-types to a single type",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/Jan/12 09:15",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Jun/12 10:28",
        "Description": "Similar or duplicating content-types can end-up differently in an index. With, for example, both application/xhtml+xml and text/html it is impossible to use a single filter to select `web pages`.\nSee also: http://lucene.472066.n3.nabble.com/application-xhtml-xml-gt-text-html-td3699942.html\nContent-Type mapping is disabled by default and is enabled via moreIndexingFilter.mapMimeTypes. Example mapping file is provided in conf/.\n\n# target MIME-type <TAB> type1 [<TAB> type2 ...]\n\n# Map XHTML to HTML\ntext/html       application/xhtml+xml\n\n# Map XHTML and HTML to something else\nWeb page        text/html       application/xhtml+xml\n\n# Map some office documents to each other\nOffice document application/vnd.oasis.opendocument.text application/x-tika-msoffice",
        "Issue Links": []
    },
    "NUTCH-1263": {
        "Key": "NUTCH-1263",
        "Summary": "FetcherJob must put 'fetchTime' on input",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "31/Jan/12 12:59",
        "Updated": "03/Mar/12 04:18",
        "Resolved": "02/Mar/12 14:58",
        "Description": "The reducer of the fetcher reads the field fetchTime, but does not include in on the input. Trivial patch fixes this.",
        "Issue Links": []
    },
    "NUTCH-1264": {
        "Key": "NUTCH-1264",
        "Summary": "Configurable indexing plugin (index-metadata)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "01/Feb/12 12:17",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Feb/12 16:56",
        "Description": "We currently have several plugins already distributed or proposed which do very comparable things : \n\nparse-meta NUTCH-809 to generate metadata fields in parse-metadata and index them\nheadings NUTCH-1005 to generate headings fields in parse-metadata and index them\nindex-extra NUTCH-422 to index configurable fields\nurlmeta NUTCH-855 to propagate metadata from the seeds to the outlinks and index them\nindex-static NUTCH-940 to generate configurable static fields\n\nAll these plugins have in common that they allow to extract information from various sources and generate fields from them and are largely redundant. Instead this issue proposes to have a single plugin allowing to generate configurable fields from : \n\nstatic values\nparse metadata\ncontent metadata\ncrawldb metadata\n\nand let the other plugins focus on the parsing and extraction of the values to index. This will make the addition of new fields simpler by relying on a stable common plugin instead of multiplying the code in various plugins.\nThis plugin will replace index-extra NUTCH-422 and will serve as a basis for further improvements.",
        "Issue Links": []
    },
    "NUTCH-1265": {
        "Key": "NUTCH-1265",
        "Summary": "[nutchgora] - update to work with gora-0.2-incubating",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "Sujit Pal",
        "Created": "02/Feb/12 00:49",
        "Updated": "02/Feb/12 16:15",
        "Resolved": "02/Feb/12 16:15",
        "Description": "Currently nutchgora is set to work against gora-0.1.1. I wanted to work with a newer version of Cassandra, so used the gora trunk version. There is a slight API change in DataStoreFactory.createDataStore() - it now wants the Configuration as a parameter. Since I made this change locally anyway, thought it would be good to share to help you guys out when you decide to release a new version of nutchgora.\nPatch is attached. Thanks to whoever came up with the idea of backing nutch with a database .",
        "Issue Links": []
    },
    "NUTCH-1266": {
        "Key": "NUTCH-1266",
        "Summary": "Subcollection to optionally write to configured fields",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.5",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Feb/12 13:14",
        "Updated": "22/May/13 03:53",
        "Resolved": "09/Feb/12 09:55",
        "Description": "The subcollection plugin writes the contents of the name element of a given subcollection to the subcollection field. There are cases in which writing to fields other than subcollection is useful.",
        "Issue Links": [
            "/jira/browse/NUTCH-1381"
        ]
    },
    "NUTCH-1267": {
        "Key": "NUTCH-1264 Configurable indexing plugin (index-metadata)",
        "Summary": "urlmeta to delegate indexing to index-metadata",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "06/Feb/12 16:55",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Ideally we should get rid of urlmeta altogether and add the transmission of the meta to the outlinks in the core classes - not as a plugin. URLMeta is also a terrible name",
        "Issue Links": []
    },
    "NUTCH-1268": {
        "Key": "NUTCH-1264 Configurable indexing plugin (index-metadata)",
        "Summary": "parse-meta to delegate indexing to index-metadata",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "06/Feb/12 16:56",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1269": {
        "Key": "NUTCH-1269",
        "Summary": "Improve distribution of URLS with multi-segment generation",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "08/Feb/12 10:23",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "there are some problems with current Generate method, with maxNumSegments and maxHostCount options:\n1. first, size of generated segments are different\n2. with maxHostCount option, it is unclear that it was applied or not\n3. urls from one host are distributed non-uniform between segments\nwe change Generator.java as described below:\nin Selector class:\n    private int maxNumSegments;\n    private int segmentSize;\n    private int maxHostCount;\npublic void config\n...\n      maxNumSegments = job.getInt(GENERATOR_MAX_NUM_SEGMENTS, 1);\n      segmentSize=(int)job.getInt(GENERATOR_TOP_N, 10000000)/maxNumSegments;\n      maxHostCount=job.getInt(\"GENERATE_MAX_PER_HOST\", 100);  \n...\npublic void reduce(FloatWritable key, Iterator<SelectorEntry> values,\n        OutputCollector<FloatWritable,SelectorEntry> output, Reporter reporter)\n        throws IOException {\n\tint limit2=(int)((limit*3)/2);\n      while (values.hasNext()) {\n\tif(count == limit)\n                break;\n        if (count % segmentSize == 0 ) {\n          if (currentsegmentnum < maxNumSegments-1)\n{\n            currentsegmentnum++;\n          }\n          else\n                currentsegmentnum=0;\n        }\n        boolean full=true;\n        for(int jk=0;jk<maxNumSegments;jk++){\n        \tif (segCounts[jk]<segmentSize)\n{\n        \t\tfull=false;\n        \t}\n        }\n        if(full)\n{\n        \tbreak;\n        }\n        SelectorEntry entry = values.next();\n        Text url = entry.url;\n                //logWrite(\"Generated3:\"limit\"\"count\"\"+url.toString());\n        String urlString = url.toString();\n        URL u = null;\n        String hostordomain = null;\n        try {\n          if (normalise && normalizers != null) \n{\n            urlString = normalizers.normalize(urlString,\n                URLNormalizers.SCOPE_GENERATE_HOST_COUNT);\n          }\n\n          u = new URL(urlString);\n          if (byDomain) \n{\n            hostordomain = URLUtil.getDomainName(u);\n          }\n else \n{\n            hostordomain = new URL(urlString).getHost();\n          }\n\n\thostordomain = hostordomain.toLowerCase();\n        boolean countLimit=true;\n        // only filter if we are counting hosts or domains\n             int[] hostCount = hostCounts.get(hostordomain);\n             //host count: \n{a,b,c,d}\n means that from this host there are a urls in segment 0 and b urls in seg 1 and ...\n             if (hostCount == null) \n{\n                 hostCount = new int[maxNumSegments];\n                 for(int kl=0;kl<hostCount.length;kl++)\n                         hostCount[kl]=0;\n                 hostCounts.put(hostordomain, hostCount);\n             }\n \n                 int selectedSeg=currentsegmentnum;\n                 int minCount=hostCount[selectedSeg];\n                 for(int jk=0;jk<maxNumSegments;jk++){\n                         if(hostCount[jk]<minCount)\n{\n                                 minCount=hostCount[jk];\n                                 selectedSeg=jk;\n                         }\n                }\n                if(hostCount[selectedSeg]<=maxHostCount)\n{\n                        count++;\n                        entry.segnum = new IntWritable(selectedSeg);\n                        hostCount[selectedSeg]++;\n                        output.collect(key, entry);\n                }\n\n        } catch (Exception e) \n{\n          LOG.warn(\"Malformed URL: '\" + urlString + \"', skipping (\"\n                logWrite(\"Generate-malform:\"+hostordomain+\"-\"+url.toString());\n              + StringUtils.stringifyException(e) + \")\");\n          //continue;\n        }\n      }\n    }",
        "Issue Links": []
    },
    "NUTCH-1270": {
        "Key": "NUTCH-1270",
        "Summary": "some of Deflate encoded pages not fetched",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "08/Feb/12 10:37",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "11/Jun/14 15:43",
        "Description": "it is a problem with some of web pages that fetched but their content can not retrived\nafter this change, this error fixed\nwe change lib-http/src/java/org/apache/nutch/protocol/http/api/HttpBase.java\n  public byte[] processDeflateEncoded(byte[] compressed, URL url) throws IOException {\n    if (LOGGER.isTraceEnabled()) \n{ LOGGER.trace(\"inflating....\"); }\n\n    byte[] content = DeflateUtils.inflateBestEffort(compressed, getMaxContent());\n+    if(content==null)\n+    \tcontent = DeflateUtils.inflateBestEffort(compressed, 200000);\n    if (content == null)\n      throw new IOException(\"inflateBestEffort returned null\");\n    if (LOGGER.isTraceEnabled()) \n{\n      LOGGER.trace(\"fetched \" + compressed.length\n                 + \" bytes of compressed content (expanded to \"\n                 + content.length + \" bytes) from \" + url);\n    }\n    return content;\n  }",
        "Issue Links": [
            "/jira/browse/NUTCH-1736"
        ]
    },
    "NUTCH-1271": {
        "Key": "NUTCH-1271",
        "Summary": "Fix errors @ compile time",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "08/Feb/12 10:46",
        "Updated": "19/Feb/12 18:31",
        "Resolved": "19/Feb/12 18:30",
        "Description": "After adding the -Xlint commands to build.xml, we see many errors when compiling. These should be fixed.",
        "Issue Links": []
    },
    "NUTCH-1272": {
        "Key": "NUTCH-1272",
        "Summary": "Wrong property name in nutch-default.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Daniel Baur",
        "Created": "14/Feb/12 09:38",
        "Updated": "22/May/13 03:54",
        "Resolved": "14/Feb/12 12:06",
        "Description": "The property name of the index-static plugin is spelled wrong within the nutch-default.xml. The nutch-default.xml states that the name of the property is \"index-static\". However the correct property name should be \"index.static\" (when taking a look the source code of the plugin).\nWhen using the entry in nutch-default.xml as a template for your own configuration, the plugin will not work.",
        "Issue Links": []
    },
    "NUTCH-1273": {
        "Key": "NUTCH-1249 Resolve all issues flagged up by adding javac -Xlint arguement",
        "Summary": "Fix [deprecation] javac warnings",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Feb/12 14:28",
        "Updated": "22/May/13 03:54",
        "Resolved": "30/Apr/13 19:37",
        "Description": "As part of this task, these warnings should be resolved, however this particular strand of warnings can either be resolved by adding\n\n@SuppressWarnings(\"deprecation\")\n\n\nor by actually upgrading our class usage to rely upon non-deprecated classes. Which option is more appropriate for the project?",
        "Issue Links": [
            "/jira/browse/NUTCH-1549"
        ]
    },
    "NUTCH-1274": {
        "Key": "NUTCH-1249 Resolve all issues flagged up by adding javac -Xlint arguement",
        "Summary": "Fix [cast] javac warnings",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build",
        "Assignee": "Tejas Patil",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Feb/12 14:34",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 16:37",
        "Description": "A typical example of this is\n\ntrunk/src/java/org/apache/nutch/crawl/CrawlDatum.java:460: warning: [cast] redundant cast to int\n    [javac]         res ^= (int)(signature[i] << 24 + signature[i+1] << 16 + \n\n\nthese should all be fixed by replacing with the correct implementations.",
        "Issue Links": []
    },
    "NUTCH-1275": {
        "Key": "NUTCH-1249 Resolve all issues flagged up by adding javac -Xlint arguement",
        "Summary": "Fix [unchecked] javac warnings",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build",
        "Assignee": "Tejas Patil",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Feb/12 14:40",
        "Updated": "20/Jun/13 09:44",
        "Resolved": "22/May/13 09:11",
        "Description": "We can simply suppress these warnings using  \n\nSuppressWarnings [unchecked]\n\n\nHowever if there is a another method for resolving these warnings then they should be implemented if deemed beneficial to code quality.\nSome resources http://java.sun.com/docs/books/jls/third_edition/html/conversions.html#190772",
        "Issue Links": []
    },
    "NUTCH-1276": {
        "Key": "NUTCH-1249 Resolve all issues flagged up by adding javac -Xlint arguement",
        "Summary": "Fix [dep-ann]",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Feb/12 14:42",
        "Updated": "19/Feb/12 19:40",
        "Resolved": "19/Feb/12 18:25",
        "Description": "Generally speaking these are more straightforward than others as it should be a case of either annotating using\n\n@Deprecated\n\n\nor of course replacing the deprecated class method with another non-deprecated implementation. Hopefully most of these occurrences will be resolved within NUTCH-1273",
        "Issue Links": []
    },
    "NUTCH-1277": {
        "Key": "NUTCH-1249 Resolve all issues flagged up by adding javac -Xlint arguement",
        "Summary": "Fix [fallthrough] javac warnings",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build",
        "Assignee": "Tejas Patil",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Feb/12 14:49",
        "Updated": "22/May/13 03:53",
        "Resolved": "07/May/13 06:43",
        "Description": "This usually occurs when we have an instance where a switch statement(s) fall through (that is, one or more break statements are missing).\nWe need to determine where a simple\n\n@SuppressWarnings(\"fallthrough\")\n\n\nis required or whether we need to include the break statements in switch blocks",
        "Issue Links": []
    },
    "NUTCH-1278": {
        "Key": "NUTCH-1278",
        "Summary": "Fetch Improvement in threads per host",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "15/Feb/12 06:07",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "05/Apr/14 20:19",
        "Description": "the value of maxThreads is equal to fetcher.threads.per.host and is constant for every host\nthere is a possibility with using of dynamic values for every host that influeced with number of blocked requests.\nthis means that if number of blocked requests for one host increased, then we most decrease this value and increase http.timeout",
        "Issue Links": []
    },
    "NUTCH-1279": {
        "Key": "NUTCH-1279",
        "Summary": "Check if limit has been reached in GeneraterReducer must be the first check performance-wise.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "15/Feb/12 09:35",
        "Updated": "15/Feb/12 10:44",
        "Resolved": "15/Feb/12 09:39",
        "Description": "The (count >= limit) should be put up front in the reduce method of the generator, because that way when the limit is reached the reduce method will return faster.",
        "Issue Links": []
    },
    "NUTCH-1280": {
        "Key": "NUTCH-1280",
        "Summary": "language-identifier should have option to use detected value by Tika even when uncertain",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "16/Feb/12 10:41",
        "Updated": "22/May/13 03:54",
        "Resolved": "20/Feb/12 09:40",
        "Description": "Nutchtrunk has an option \"lang.identification.only.certain\", this should be the case for Nutchgora too. Note that it is set default to false. So this changes the default behaviour somewhat.\nPatch will be right up.",
        "Issue Links": []
    },
    "NUTCH-1281": {
        "Key": "NUTCH-1281",
        "Summary": "tika parser not work properly with unwanted file types that passed from filters in nutch",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "19/Feb/12 05:42",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "when in parse-plugins.xml, set this property:\n<mimeType name=\"*\">\n        <plugin id=\"parse-tika\" />\n</mimeType>\nall unwanted files that pass from all filters, refered to tika\nbut for some file types like .flv, tika parser has problem and hunged and cause to fail in parse Job.\nif this file types passed from regex-urlfilter and other filters, parse job failed.\nfor this problem I suggest that add some properties for valid file types, and use this code in TikaParser.java, like this:\npublic ParseResult getParse(Content content) {\n\t\tString mimeType = content.getContentType();\n+\t\tString[]validTypes=new String[]\n{\"application/pdf\",\"application/x-tika-msoffice\",\"application/x-tika- ooxml\",\"application/vnd.oasis.opendocument.text\",\"text/plain\",\"application/rtf\",\"application/rss+xml\",\"application/x-bzip2\",\"application/x-gzip\",\"application/x-javascript\",\"application/javascript\",\"text/javascript\",\"application/x-shockwave-flash\",\"application/zip\",\"text/xml\",\"application/xml\"}\n;\n+\t\tboolean valid=false;\n+\t\tfor(int k=0;k<validTypes.length;k++)\n{\n+\t\t\tif(validTypes[k].compareTo(mimeType.toLowerCase())==0)\n+\t\t\t\tvalid=true;\n+\t\t}\n+\t\tif(!valid)\n+\t                return new ParseStatus(ParseStatus.NOTPARSED, \"Can't parse for unwanted filetype \"+ mimeType).getEmptyParseResult(content.getUrl(), getConf());\n\t\tURL base;",
        "Issue Links": []
    },
    "NUTCH-1282": {
        "Key": "NUTCH-1282",
        "Summary": "linkdb scalability",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "linkdb",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "19/Feb/12 10:15",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "as described in NUTCH-1054, the linkdb is optional in solrindex and it's usage is only for anchor and not impact on scoring. \nas seemed, size of linkdb in incremental crawl grow very fast and make it unscalable for huge size of web sites.\nso, here is two choises, one, ignore invertlinks and linkdb from crawl, and second, make it scalable\nin invertlinks, there is 2 jobs, first for construct new linkdb from new parsed segments, and second for merge new linkdb with old linkdb. the second job is unscalable and we can ignore it with this changes in solrIndex:\nin the class IndexerMapReduce, reduce method, if fetchDatum == null or dbDatum == null or parseText == null or parseData == null, then add anchor to doc and update solr (no insert)\nhere also some changes required to NutchDocument.",
        "Issue Links": [
            "/jira/browse/NUTCH-1181"
        ]
    },
    "NUTCH-1283": {
        "Key": "NUTCH-1283",
        "Summary": "Radically update all Solr configuration in Nutchgora",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Feb/12 12:50",
        "Updated": "22/May/13 03:53",
        "Resolved": "18/Sep/12 19:59",
        "Description": "We're currently running with a Schema which states it's 1.4 :0| There should be better support for newer stuff going on over the Solrland. Thsi issue should track those improvements entirely.",
        "Issue Links": []
    },
    "NUTCH-1284": {
        "Key": "NUTCH-1284",
        "Summary": "Add site fetcher.max.crawl.delay as log output by default.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "fetcher",
        "Assignee": "Tejas Patil",
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Feb/12 18:58",
        "Updated": "22/May/13 03:54",
        "Resolved": "28/Jan/13 08:04",
        "Description": "Currently, when manually scanning our log output we cannot infer which pages are governed by a crawl delay between successive fetch attempts of any given page within the site. The value should be made available as something like:\n\n2012-02-19 12:33:33,031 INFO  fetcher.Fetcher - fetching http://nutch.apache.org/ (crawl.delay=XXXms)\n\n\nThis way we can easily and quickly determine whether the fetcher is having to use this functionality or not.",
        "Issue Links": [
            "/jira/browse/NUTCH-1042"
        ]
    },
    "NUTCH-1285": {
        "Key": "NUTCH-1285",
        "Summary": "Debian Packaging for Nutch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Feb/12 11:50",
        "Updated": "29/Jun/14 09:10",
        "Resolved": "17/Jun/14 11:21",
        "Description": "This is a utopian type issue which will not be addressed for some time due to many factors, outwith our control which exist within the Debian policy ecosystem. \nI've been in touch with Ioan over @ Apache James and they have recently (after a number of years) made some real progress with this. Some links are below\n[0] http://svn.apache.org/repos/asf/james/app\n[1] http://svn.apache.org/viewvc/james/app/trunk/pom.xml?view=markup\n[2] https://issues.apache.org/jira/browse/JAMES-1343\n[3] http://www.mail-archive.com/server-dev@james.apache.org/\n[4] http://www.debian.org/doc/debian-policy/\n[5] http://www.debian.org/doc/manuals/maint-guide/index.en.html",
        "Issue Links": []
    },
    "NUTCH-1286": {
        "Key": "NUTCH-1286",
        "Summary": "Refactoring/reimplementing crawling API (NutchApp)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.1",
        "Component/s": "administration gui,                                            REST_api,                                            web gui",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "20/Feb/12 14:55",
        "Updated": "20/Sep/15 12:52",
        "Resolved": "20/Sep/15 12:52",
        "Description": "This issue is to track changes we (Mathijs and I) have planned for the API and webapp in Nutchgora. We have a pretty good idea of how we want to be using the crawl API. It may involve some major refactoring or perhaps a side implementation next the current NutchApp functionality. It depends on how much we can reuse the existing components. The bottom line is that there will be a strictly defined Java API that provide everyting related from crawling/indexing to job control. (Listing jobs, tracking progress and aborting jobs being part of it). There will be no server or service for tracking crawling states, all will be persisted one way or the other and queryable from the API. The REST server shall be a very thin layer on top of the Java implementation. A rich web interface will be very easy layer too, once we have a cleanly (but extensive) defined API. But we will start to make to API usable from a simple command-line interface.\nMore details will be provided later on.. feel free to comment if you have suggestions/questions.",
        "Issue Links": [
            "/jira/browse/NUTCH-841"
        ]
    },
    "NUTCH-1287": {
        "Key": "NUTCH-1287",
        "Summary": "Upgrade to hsqldb 2.2.8",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "20/Feb/12 15:55",
        "Updated": "21/Feb/12 04:50",
        "Resolved": "20/Feb/12 16:09",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1288": {
        "Key": "NUTCH-1288",
        "Summary": "Generator should not generate filter and not found and denied and gone and permanently moved pages",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            generator",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "21/Feb/12 07:58",
        "Updated": "22/May/13 03:53",
        "Resolved": "21/Feb/12 10:13",
        "Description": "Generator should not generate filter and not found and denied and gone and permanently moved pages.\nin the shouldFetch method in AbstractFetchSchedule, CrawlDatum must checked against special states of fetch like not found, and not generate them again.\nso we can add a status in CrawlDatum that indicates invalid urls, and set this status in fetch.",
        "Issue Links": []
    },
    "NUTCH-1289": {
        "Key": "NUTCH-1289",
        "Summary": "In distributed mode URL's are not partitioned",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dan Rosher",
        "Created": "27/Feb/12 11:06",
        "Updated": "06/Mar/12 10:43",
        "Resolved": "06/Mar/12 10:43",
        "Description": "In distributed mode URL's are not partitioned to a specific machine which means the politeness policy is voided",
        "Issue Links": []
    },
    "NUTCH-1290": {
        "Key": "NUTCH-1290",
        "Summary": "crawlId not supported by all Tools",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Mathijs Homminga",
        "Created": "28/Feb/12 10:39",
        "Updated": "26/Apr/12 11:34",
        "Resolved": "26/Apr/12 11:34",
        "Description": "See also: https://issues.apache.org/jira/browse/NUTCH-907\nThe StorageUtils class exposes a createDataStore method which uses the default schema for a persistent class specified in the Gora configuration. \nThis method ignores Nutch' storage.schema property and the notion of a crawlId.\nTwo tools use this method instead of the createWebStore method (which does support the storage.schema property and a crawlId):\no.a.n.indexer.IndexerReducer (IndexerJob)\no.a.n.util.domain.DomainStatistics\nI propose that these two start using the createWebStore method and that we make remove the createDataStore method from the StorageUtils.\nAlso, these two tools should support the crawlId command line parameter.",
        "Issue Links": [
            "/jira/browse/NUTCH-882"
        ]
    },
    "NUTCH-1291": {
        "Key": "NUTCH-1291",
        "Summary": "Fetcher to stringify exception on // unexpected exception",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "29/Feb/12 13:52",
        "Updated": "22/May/13 03:54",
        "Resolved": "29/Feb/12 14:13",
        "Description": "During development we sometimes saw a less than helpful exception e.g. fetch of http://www.openindex.io/en/home.html failed with: java.lang.NullPointerException. This error must be a bit more descriptive.",
        "Issue Links": []
    },
    "NUTCH-1292": {
        "Key": "NUTCH-1292",
        "Summary": "Better exception logging and debugging during fetch.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "29/Feb/12 15:41",
        "Updated": "03/Mar/12 04:18",
        "Resolved": "02/Mar/12 15:09",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1293": {
        "Key": "NUTCH-1293",
        "Summary": "IndexingFiltersChecker to store detected content type in crawldatum metadata",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Mar/12 15:07",
        "Updated": "22/May/13 03:54",
        "Resolved": "03/May/12 09:17",
        "Description": "NUTCH-1259 is not implemented in the checker.",
        "Issue Links": [
            "/jira/browse/NUTCH-1259"
        ]
    },
    "NUTCH-1294": {
        "Key": "NUTCH-1294",
        "Summary": "IndexClean job with solr implementation.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Dan Rosher",
        "Created": "01/Mar/12 15:46",
        "Updated": "01/May/14 06:23",
        "Resolved": "13/Aug/13 15:28",
        "Description": "I started by copying/altering the trunk version of SolrClean, though is was inadequate for our needs. We needed to mark particular pages as gone even though they still might be visible on the web, this implementation abstracts the index cleaning process, has a Solr implementation, and adds a clean index plugin extension that allows others to tailor how pages might be removed from their store.",
        "Issue Links": []
    },
    "NUTCH-1295": {
        "Key": "NUTCH-1295",
        "Summary": "nutchgora restlet dependencies failing when remote repos is down",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "02/Mar/12 09:53",
        "Updated": "03/Mar/12 04:18",
        "Resolved": "02/Mar/12 10:26",
        "Description": "Currently the head of nutchgora cannot be build when running \"ant clean runtime\". This is because the restlet dependencies cannot be found. This is even though there are local restlet copies in the ivy2 cache dir. Did we not have this problem before?\nAnyway I found a solution. Basically I renamed the resolver name from the chain name. This way the restlet dependencies are read from the local cache when the remote one is not available. See patch for details.",
        "Issue Links": []
    },
    "NUTCH-1296": {
        "Key": "NUTCH-1296",
        "Summary": "nutchgora fetcher does not show correct 'threads' and 'resuming' properties",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "02/Mar/12 13:46",
        "Updated": "03/Mar/12 04:18",
        "Resolved": "02/Mar/12 13:49",
        "Description": "The nutchgora FetcherJob logs the 'threads' and 'resuming' properties just before fetching, but they are read from the config. (Ignoring the fact that they are specified as parameters too. These paramaters are later set on the config).\nTrivial fix will be right away.",
        "Issue Links": []
    },
    "NUTCH-1297": {
        "Key": "NUTCH-1297",
        "Summary": "it is better for fetchItemQueues to select items from greater queues first",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "04/Mar/12 06:32",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "05/Apr/14 20:14",
        "Description": "there is a situation that if we have multiple hosts in fetch, and size of hosts were different, large hosts have a long delay until the getFetchItem() in FetchItemQueues class select a url from them, so we can give them more priority.\nfor example if we have 10 url from host1 and 1000 url from host2, and have 5 threads, if all threads first selected from host1, we had more delay on fetch rather than a situation that threads first selected from host2, and when host 2 was busy, then selected from host1.",
        "Issue Links": [
            "/jira/browse/NUTCH-1687"
        ]
    },
    "NUTCH-1298": {
        "Key": "NUTCH-1298",
        "Summary": "Pass numTasks to FetcherJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dan Rosher",
        "Created": "05/Mar/12 17:42",
        "Updated": "07/Mar/12 04:13",
        "Resolved": "06/Mar/12 10:46",
        "Description": "Pass -numTasks N from crawler to FetcherJob",
        "Issue Links": []
    },
    "NUTCH-1299": {
        "Key": "NUTCH-1299",
        "Summary": "LinkRank inverter to ignore records without Node",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Mar/12 00:35",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Mar/12 17:32",
        "Description": "No Node object is passed from the inverter's mapper to the reducer, which expects one, causing the following exception:\n\njava.lang.NullPointerException\n        at org.apache.nutch.scoring.webgraph.LinkRank$Inverter.reduce(LinkRank.java:409)\n        at org.apache.nutch.scoring.webgraph.LinkRank$Inverter.reduce(LinkRank.java:356)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:519)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:396)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)\n        at org.apache.hadoop.mapred.Child.main(Child.java:249)\n\n\nThis never happens unless you have a funky web graph. Our web graph changes frequently, adding and deleting records. It's likely a large number of records deleted from the outlink database is responsible for this. This error, however, only showed up now, a great deal of time after we began deleting records.",
        "Issue Links": []
    },
    "NUTCH-1300": {
        "Key": "NUTCH-1300",
        "Summary": "Indexer to filter and normalize URL's",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Mar/12 00:42",
        "Updated": "17/Jul/13 18:38",
        "Resolved": "17/Jul/13 18:38",
        "Description": "Indexers should be able to normalize URL's. This is useful when a new normalizer is applied to the entire CrawlDB. Without it, some or all records in a segment cannot be indexed at all.",
        "Issue Links": [
            "/jira/browse/NUTCH-1323",
            "/jira/browse/NUTCH-1614"
        ]
    },
    "NUTCH-1301": {
        "Key": "NUTCH-1301",
        "Summary": "Index job resume switch to resume a failed job",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Dan Rosher",
        "Created": "06/Mar/12 11:39",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "This is also useful in nutchgora to allow for continuous indexing with -all -resume, as it is for fetching, cron scripts can then be independent without having to know the batchid.",
        "Issue Links": []
    },
    "NUTCH-1302": {
        "Key": "NUTCH-1302",
        "Summary": "nutchgora job failures should be noticed by submitter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Ferdy",
        "Reporter": "Ferdy",
        "Created": "06/Mar/12 12:32",
        "Updated": "07/Mar/12 04:13",
        "Resolved": "06/Mar/12 13:18",
        "Description": "I stumbled upon an issue where crawling seem to go right, only to notice much later on that jobs actually failed as a whole.\nThis is caused because for most jobs that are submitted, Nutchgora does not check the 'succeeded' boolean that is returned. This should be done and acted upon appropriately. (Either throwing an exception or returning non-zero exit codes).",
        "Issue Links": []
    },
    "NUTCH-1303": {
        "Key": "NUTCH-1303",
        "Summary": "Fetcher to skip queues for URLS getting repeated exceptions, based on percentage",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "07/Mar/12 06:45",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "as described in https://issues.apache.org/jira/browse/NUTCH-769, it is a good solution to skip queues with high exception value, but it is not easy to set value of fetcher.max.exceptions.per.queue when size of queues are different.\ni suggest that define a ratio instead of value, so if the ratio of exceptions per requests exceeds, then queue cleared.\nalso, it is not sufficient to keep fetcher from high exceptions, value of fetcher.throughput.threshold.pages ensures that a valueable throughput of fetch can gained against slow hosts, but it clean all queues not slow queue. i suggest for this one that this factor like fetcher.max.exceptions.per.queue enforce to each queue not all of them.",
        "Issue Links": []
    },
    "NUTCH-1304": {
        "Key": "NUTCH-1304",
        "Summary": "GeneratorMapper.java dosen't return when skipping and already generated mark",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Dan Rosher",
        "Created": "08/Mar/12 11:37",
        "Updated": "09/Mar/12 16:08",
        "Resolved": "09/Mar/12 16:08",
        "Description": "GeneratorMapper.java dosen't return when skipping and already generated mark",
        "Issue Links": []
    },
    "NUTCH-1305": {
        "Key": "NUTCH-1305",
        "Summary": "Domain(blacklist)URLFilter to trim entries",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "08/Mar/12 13:34",
        "Updated": "22/May/13 03:54",
        "Resolved": "08/Mar/12 13:53",
        "Description": "Both filters should handle entries with trailing whitespace.",
        "Issue Links": []
    },
    "NUTCH-1306": {
        "Key": "NUTCH-1306",
        "Summary": "Add option to not commit and clarify existing solr.commit.size",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Dan Rosher",
        "Created": "08/Mar/12 14:20",
        "Updated": "22/May/13 03:54",
        "Resolved": "09/Jul/12 11:39",
        "Description": "Commit after finished writing to solr index - otherwise a bit confusing not seeing the number of docs we expect in solr",
        "Issue Links": [
            "/jira/browse/NUTCH-1025"
        ]
    },
    "NUTCH-1307": {
        "Key": "NUTCH-1307",
        "Summary": "Improve formatting of ant targets for clearer project help",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "nutchgora,                                            1.5",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "08/Mar/12 15:30",
        "Updated": "09/Mar/12 10:41",
        "Resolved": "08/Mar/12 15:50",
        "Description": "This is a trivial formatting issue I will submit a patch shortly and fix it.",
        "Issue Links": []
    },
    "NUTCH-1308": {
        "Key": "NUTCH-1308",
        "Summary": "Add main() to ZipParser",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "1.13",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Lewis John McGibbney",
        "Created": "09/Mar/12 16:38",
        "Updated": "02/Jul/16 11:43",
        "Resolved": "02/Jul/16 10:46",
        "Description": "Two issues here...\n1) Recently ferdy committed NUTCH-965 which skips parsing of truncated documents. Parse zip has it's own implementation for the same when it should really draw on the aforementioned implementation.\n2) If (in the offending piece of code mentioned above) truncation occurs, we get an incorrect log message the \"Parser can't handle incomplete pdf files\"!!! This is incorrect, shouldn't be there, and should be removed.\n\n\n72      if (contentLen != null && contentInBytes.length != len) {\n73 \treturn new ParseStatus(ParseStatus.FAILED,\n74 \tParseStatus.FAILED_TRUNCATED, \"Content truncated at \"\n75 \t+ contentInBytes.length\n76 \t+ \" bytes. Parser can't handle incomplete pdf file.\")\n77 \t.getEmptyParseResult(content.getUrl(), getConf());\n78 \t}\n\n\nFor clarity, the issue is present in both Nutchgora branch[1] and Nutch trunk[2]\n[1] https://svn.apache.org/viewvc/nutch/branches/nutchgora/src/plugin/parse-zip/src/java/org/apache/nutch/parse/zip/ZipParser.java?diff_format=h&view=markup\n[2] https://svn.apache.org/viewvc/nutch/trunk/src/plugin/parse-zip/src/java/org/apache/nutch/parse/zip/ZipParser.java?diff_format=h&view=markup\n[2]",
        "Issue Links": [
            "/jira/browse/NUTCH-1603"
        ]
    },
    "NUTCH-1309": {
        "Key": "NUTCH-1309",
        "Summary": "fetch queue management",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "12/Mar/12 08:44",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "22/Nov/13 11:39",
        "Description": "when run fetch in hadoop with multiple concurrent mapper, there are multiple independent fetchQueues that make hard to manage them. i suggest that construct fetchQueues before begin of run with this line:\n    feeder = new QueueFeeder(input, fetchQueues, threadCount * 50);",
        "Issue Links": []
    },
    "NUTCH-1310": {
        "Key": "NUTCH-1310",
        "Summary": "Nutch to send HTTP-accept header",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.5",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "14/Mar/12 15:14",
        "Updated": "22/May/13 03:53",
        "Resolved": "16/Mar/12 13:07",
        "Description": "Nutch does not send a HTTP-accept header with its requests. This is usually not a problem but some firewall do not like it and will reject the request.",
        "Issue Links": []
    },
    "NUTCH-1311": {
        "Key": "NUTCH-1311",
        "Summary": "Add  response headers to datastore for the protocol-httpclient plugin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Dan Rosher",
        "Created": "16/Mar/12 11:01",
        "Updated": "18/Mar/12 16:35",
        "Resolved": "16/Mar/12 15:11",
        "Description": "Response Headers need to be added to the page to add to the datastore for this plugin",
        "Issue Links": []
    },
    "NUTCH-1312": {
        "Key": "NUTCH-1312",
        "Summary": "Nutchgora to send HTTP-accept header",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Ferdy",
        "Reporter": "Ferdy",
        "Created": "16/Mar/12 14:49",
        "Updated": "18/Mar/12 16:35",
        "Resolved": "16/Mar/12 14:57",
        "Description": "This is a port of NUTCH-1310 to Nutchgora. To keep it in line with NUTCH-1310, this patch only fixes it for protocol-http, so protocol-httpclient is still todo.",
        "Issue Links": []
    },
    "NUTCH-1313": {
        "Key": "NUTCH-1313",
        "Summary": "Nutch trunk add response headers to datastore for the protocol-httpclient plugin",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "16/Mar/12 15:10",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "For tracking progress the port of NUTCH-1311 to Nutch trunk.",
        "Issue Links": []
    },
    "NUTCH-1314": {
        "Key": "NUTCH-1314",
        "Summary": "Impose a limit on the length of outlink target urls",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Ferdy",
        "Created": "16/Mar/12 16:41",
        "Updated": "17/Jul/18 11:36",
        "Resolved": "17/Jul/18 11:36",
        "Description": "In the past we have encountered situations where crawling specific broken sites resulted in ridiciously long urls that caused the stalling of tasks. The regex plugins (normalizing/filtering) processed single urls for hours, if not indefinitely hanging.\nMy suggestion is to limit the outlink url target length as soon possible. It is a configurable limit, the default is 3000. This should be reasonably long enough for most uses. But sufficienly strict enough to make sure regex plugins do not choke on urls that are too long. Please see attached patch for the Nutchgora implementation.\nI'd like to hear what you think about this.",
        "Issue Links": [
            "/jira/browse/NUTCH-1106",
            "/jira/browse/NUTCH-1531"
        ]
    },
    "NUTCH-1315": {
        "Key": "NUTCH-1315",
        "Summary": "reduce speculation on but ParseOutputFormat doesn't name output files correctly?",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Rafael",
        "Created": "19/Mar/12 18:20",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "From time to time the Reducer log contains the following and one tasktracker gets blacklisted.\norg.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /user/test/crawl/segments/20120316065507/parse_text/part-00001/data for DFSClient_attempt_201203151054_0028_r_000001_1 on client xx.x.xx.xx.10, because this file is already being created by DFSClient_attempt_201203151054_0028_r_000001_0 on xx.xx.xx.9\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:1404)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1244)\n\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1186)\n\tat org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:628)\n\tat sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:563)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1388)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1384)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1093)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1382)\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1066)\n\tat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:225)\n\tat $Proxy2.create(Unknown Source)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)\n\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)\n\tat $Proxy2.create(Unknown Source)\n\tat org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.<init>(DFSClient.java:3245)\n\tat org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:713)\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:182)\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:555)\n\tat org.apache.hadoop.io.SequenceFile$RecordCompressWriter.<init>(SequenceFile.java:1132)\n\tat org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:397)\n\tat org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:354)\n\tat org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:476)\n\tat org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:157)\n\tat org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:134)\n\tat org.apache.hadoop.io.MapFile$Writer.<init>(MapFile.java:92)\n\tat org.apache.nutch.parse.ParseOutputFormat.getRecordWriter(ParseOutputFormat.java:110)\n\tat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.<init>(ReduceTask.java:448)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:490)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1093)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nI asked the hdfs-user mailing list and i got the following answer:\n\"Looks like you have reduce speculation turned on, but the\nParseOutputFormat you're using doesn't properly name its output files\ndistinctly based on the task attempt ID. As a workaround you can\nprobably turn off speculative execution for reduces, but you should\nalso probably file a Nutch bug.\"",
        "Issue Links": []
    },
    "NUTCH-1316": {
        "Key": "NUTCH-1316",
        "Summary": "create EmbeddedNutchInstance testing utility class",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Mar/12 21:52",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Jan/13 19:58",
        "Description": "I propose to create a new testing utility class called EmbeddedNutchInstance which provides two main methods; setup and teardown. This will take the pain out of firing up Nutch test instances in distributed environments and will enable us to test Nutch over the BigTop environment.",
        "Issue Links": [
            "/jira/browse/BIGTOP-284"
        ]
    },
    "NUTCH-1317": {
        "Key": "NUTCH-1317",
        "Summary": "Max content length by MIME-type",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Mar/12 19:18",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "The good old http.content.length directive is not sufficient in large internet crawls. For example, a 5MB PDF file may be parsed without issues but a 5MB HTML file may time out.",
        "Issue Links": []
    },
    "NUTCH-1318": {
        "Key": "NUTCH-1318",
        "Summary": "Parse time outs crash parsing fetcher",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Mar/12 19:24",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jun/12 11:30",
        "Description": "Some fetch lists can never be fetched and parsed successfully because a single timing out record can cause most and eventually all subsequeny records to time out as well. Finally the mapper will hang completely and so killing the entire fetch job, loosing 99% of the records that were processed.\nI'm not sure what's going on, something may be leaking somewhere.",
        "Issue Links": [
            "/jira/browse/NUTCH-1387"
        ]
    },
    "NUTCH-1319": {
        "Key": "NUTCH-1319",
        "Summary": "HostNormalizer",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Mar/12 21:04",
        "Updated": "07/Jul/15 11:59",
        "Resolved": "07/Jul/15 11:59",
        "Description": "Nutch would benefit from having a host normalizer. A host normalizer maps a given host to the desired host. A basic example is to map www.apache.org to apache.org. The Apache website is one of many on the internet that has a duplicate website on the same domain just because it allows both www and non-www to return HTTP 200 and proper content.\nIt is also able to handle wildcards such as *.example.org to example.org if there are multiple sub domains that actually point to the same website.\nLarge internet crawls tend to get polluted very quickly due to these problems. It also leads to skewed scores in the webgraph as different websites link to different versions of the same duplicate website. An example:\n\n# Force all sub domains to non-www.\n*.example.com example.com\n\n# Force www sub domain to non-www.\nwww.example.net example.net\n\n# Force non-www. sub domain to www\nexample.org www.example.org",
        "Issue Links": [
            "/jira/browse/NUTCH-737",
            "/jira/browse/NUTCH-1326",
            "/jira/browse/NUTCH-1702"
        ]
    },
    "NUTCH-1320": {
        "Key": "NUTCH-1320",
        "Summary": "IndexChecker and ParseChecker choke on IDN's",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Mar/12 12:57",
        "Updated": "22/May/13 03:53",
        "Resolved": "07/Jun/12 18:49",
        "Description": "These handy debug tools do not handle IDN's and throw an NPE\nbin/nutch parsechecker http://\u4f8b\u5b50.\u6e2c\u8a66/%E9%A6%96%E9%A0%81\n\nException in thread \"main\" java.lang.NullPointerException\n        at org.apache.nutch.indexer.IndexingFiltersChecker.run(IndexingFiltersChecker.java:71)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.IndexingFiltersChecker.main(IndexingFiltersChecker.java:116)",
        "Issue Links": [
            "/jira/browse/NUTCH-1321"
        ]
    },
    "NUTCH-1321": {
        "Key": "NUTCH-1321",
        "Summary": "IDNNormalizer",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "29/Mar/12 14:43",
        "Updated": "09/Dec/22 14:47",
        "Resolved": "09/Dec/22 14:43",
        "Description": "Right now, IDN's are indexed as ASCII. An IDNNormalizer is to be used with an indexer so it will encode ASCII URL's to their proper unicode equivalant.",
        "Issue Links": [
            "/jira/browse/NUTCH-1681",
            "/jira/browse/NUTCH-2746",
            "/jira/browse/NUTCH-1320"
        ]
    },
    "NUTCH-1322": {
        "Key": "NUTCH-1322",
        "Summary": "Indexer not to reindex unmodified docs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "30/Mar/12 20:05",
        "Updated": "23/Apr/12 08:53",
        "Resolved": "23/Apr/12 08:53",
        "Description": "IndexerMapReduce already attempts not to index unmodified pages if their fetch status is set to unmodified. This, however, doesn't always work. Some documents do not have that fetch status but are actually not modified at all.\nThe indexer should optionally be able not to reindex these pages.",
        "Issue Links": []
    },
    "NUTCH-1323": {
        "Key": "NUTCH-1323",
        "Summary": "AjaxNormalizer",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Apr/12 20:03",
        "Updated": "12/Feb/15 09:26",
        "Resolved": "12/Feb/15 08:30",
        "Description": "A two-way normalizer for Nutch able to deal with AJAX URL's, converting them to escaped_fragment URL's and back to an AJAX URL.\nhttps://developers.google.com/webmasters/ajax-crawling/",
        "Issue Links": [
            "/jira/browse/NUTCH-1300"
        ]
    },
    "NUTCH-1324": {
        "Key": "NUTCH-1324",
        "Summary": "DupeDB for Nutch",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Apr/12 20:06",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "A DupeDB for Nutch and associated tools to create and read a database containing information on duplicates.",
        "Issue Links": [
            "/jira/browse/NUTCH-1326",
            "/jira/browse/NUTCH-656"
        ]
    },
    "NUTCH-1325": {
        "Key": "NUTCH-1325",
        "Summary": "HostDB for Nutch",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12",
        "Component/s": "hostdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Apr/12 20:06",
        "Updated": "21/Jan/16 15:11",
        "Resolved": "21/Jan/16 13:59",
        "Description": "HostDB for Apache Nutch 1.x\n\nautomatically generates a HostDB based on CrawlDB information\nperiodically performs DNS lookup for all hosts and keeps track of DNS failures\ndiscovers homepage if www.example.org/ is a redirect\nkeeps track of host statistics such as number of URL's, 404's, not modifieds and redirects\naggregates CrawlDB metadata fields into totals, sums, min, max, average and configurable percentiles\ncan output lists of discovered homepage URL's for seed lists and static fetch interval\n*can output blacklists for hosts that have too many DNS failures to filter from the CrawlDB using domainblacklist-urlfilter\njust like CrawlDB support for JEXL expressions\n\nExamples\nGenerate for the first time, or update and existing HostDB:\n\nbin/nutch updatehostdb -hostdb crawl/hostdb -crawldb crawl/crawldb\n\n\nOptional filtering or normalizing:\n\nbin/nutch updatehostdb -hostdb crawl/hostdb -crawldb crawl/crawldb -filter -normalize\n\n\nDumping as CSV file:\n\nbin/nutch readhostdb crawl/hostdb output_directory\n\n\nGet only hostnames with have average response time above 50ms:\n\nbin/nutch readhostdb crawl/hostdb output_directory -dumpHostnames -expr \"(avg._rs_ > 50)\"\n\n\nGet only hosts that have over 50% 404's:\n\nbin/nutch readhostdb crawl/hostdb output_directory -dumpHostnames -expr \"(gone / numRecords > 0.5)\"\n\n\nFor JEXL expressions, all host metadata fields are available. All other fields are also available as:\nunfetched \u2013 number of unfetched records\nfetched \u2013 number of fetched records\ngone \u2013 number of  404's\nredirTemp \u2013 number if temporary redirects\nredirPerm \u2013 number if permanent redirects\nredirs \u2013 total number of redirects (redirTemp + redirPerm)\nnotModified \u2013 number of not modified records\nok \u2013 number of usable pages (fetched + notModified)\nnumRecords \u2013 total number of records\ndnsFailures \u2013 number of DNS failures\nAlso, see nutch-default for hostdb.* properties.",
        "Issue Links": [
            "/jira/browse/NUTCH-1149"
        ]
    },
    "NUTCH-1326": {
        "Key": "NUTCH-1326",
        "Summary": "HostDeduplicator for Nutch",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Apr/12 20:09",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "A host deduplicator able to emit rules for the HostNormalizer.",
        "Issue Links": [
            "/jira/browse/NUTCH-1324",
            "/jira/browse/NUTCH-1319"
        ]
    },
    "NUTCH-1327": {
        "Key": "NUTCH-1327",
        "Summary": "QueryStringNormalizer",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Apr/12 20:56",
        "Updated": "02/Jul/13 09:50",
        "Resolved": "02/Jul/13 08:37",
        "Description": "A normalizer for dealing with query strings. Sorting query strings is helpful in preventing duplicates for some (bad) websites.",
        "Issue Links": []
    },
    "NUTCH-1328": {
        "Key": "NUTCH-1328",
        "Summary": "a problem with regex-normalize.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "03/Apr/12 05:12",
        "Updated": "10/Jul/12 21:20",
        "Resolved": "10/Jul/12 21:20",
        "Description": "there is a regex-pattern in regex-normalize.xml:\n<pattern>([;_]?((?i)l|j|bv_)?((?i)sid|phpsessid|sessionid)=.*?)(?|&|#|$)</pattern>\nthat remove session ids from urls, but there is some sites, like:\nhttp://www.mehrnews.com/fa\nthat have urls, like:\nhttp://www.mehrnews.com/fa/newsdetail.aspx?NewsID=1567539\nand with this pattern, this url converted to an invalid url:\nhttp://www.mehrnews.com/fa/newsdetail.aspx?New",
        "Issue Links": [
            "/jira/browse/NUTCH-706"
        ]
    },
    "NUTCH-1329": {
        "Key": "NUTCH-1329",
        "Summary": "parser not extract outlinks to external web sites",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.4",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "04/Apr/12 12:20",
        "Updated": "30/Apr/13 20:56",
        "Resolved": "30/Apr/13 20:56",
        "Description": "found a bug in /src/plugin/parse-html/src/java/org/apache/nutch/parse/html/DOMContentUtils.java, that outlinks like www.example2.com from www.example1.com are inserted as www.example1.com/www.example2.com\ni correct this bug by testing that if outlink (www.example2.com) is a valid url, else inserted with it's base url\nso i replace these lines:\n                URL url = URLUtil.resolveURL(base, target);\n                outlinks.add(new Outlink(url.toString(),\n                                         linkText.toString().trim()));\nwith:\n                String host_temp=null;\n                try\n{\n                        host_temp=URLUtil.getDomainName(new URL(target));\n                }\n                catch(Exception eiuy)\n{\n                        host_temp=null;\n                }\n                URL url=null;\n                if(host_temp==null)// it is an internal outlink\n                    url = URLUtil.resolveURL(base, target);\n                else //it is an external link\n                        url=new URL(target);\n                outlinks.add(new Outlink(url.toString(),\n                                         linkText.toString().trim()));",
        "Issue Links": []
    },
    "NUTCH-1330": {
        "Key": "NUTCH-1330",
        "Summary": "OutlinkDB to preserve back up",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Apr/12 13:08",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jun/12 10:42",
        "Description": "The webgraph's outlinkDB is the single source for all scoring jobs and GB's that eventually come out. In case of disaster, that didn't happen yet, it should be able to preserve back up just like other DB's. This means users with an existing outlinkdb must move it from a crawl/webgraphdb/outlinks/ to crawl/webgraphdb/outlinks/current/.",
        "Issue Links": []
    },
    "NUTCH-1331": {
        "Key": "NUTCH-1331",
        "Summary": "limit crawler to defined depth",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.7",
        "Component/s": "generator,                                            parser,                                            storage",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "11/Apr/12 07:37",
        "Updated": "22/May/13 03:53",
        "Resolved": "21/Dec/12 11:35",
        "Description": "there is a need to limit crawler to some defined depth, and importance of this option is to avoid crawling of infinite loops, with dynamic generated urls, that occur in some sites, and to optimize crawler to select important urls.\nan option is define a iteration limit on generate,fetch,parse,updatedb cycle, but it works only if in each cycle, all of unfetched urls become fetched, (without recrawling them and with some other considerations)\nwe can define a new parameter in CrawlDatum, named depth, and like score-opic algorithm, compute depth of a link after parse, and in generate, only select urls with valid depth.",
        "Issue Links": [
            "/jira/browse/NUTCH-1508"
        ]
    },
    "NUTCH-1332": {
        "Key": "NUTCH-1332",
        "Summary": "db.max.outlinks.per.page not honored",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "13/Apr/12 10:47",
        "Updated": "01/May/12 12:42",
        "Resolved": "01/May/12 12:42",
        "Description": "In the WebGraph i spotted records with > db.max.outlinks.per.page outlinks. Somewhere down the line this setting is not honored.",
        "Issue Links": []
    },
    "NUTCH-1333": {
        "Key": "NUTCH-1333",
        "Summary": "Introduce AvroStore, DataFileAvroStore and Accumulo Datastore implementations",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Apr/12 18:18",
        "Updated": "15/Apr/12 19:01",
        "Resolved": "15/Apr/12 19:00",
        "Description": "This is to accomodate recent developments over @ Gora.",
        "Issue Links": []
    },
    "NUTCH-1334": {
        "Key": "NUTCH-1334",
        "Summary": "NPE in FetcherOutputFormat",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "16/Apr/12 12:51",
        "Updated": "22/May/13 03:53",
        "Resolved": "30/Apr/13 21:46",
        "Description": "If fetcher.parse or fetcher.store.content are set to false AND the write method receives an instance of Parse or Content, a NPE will be thrown.\nThis usually does not happen as the Fetcher does not output a Parse or Content based on the configuration, however this class is also used by the ArcSegmentCreator which is unaware of these parameters and will output a Parse or Content instance regardless of the configuration. One option would be to make the ArcSegmentCreator aware of the fetcher.* parameters and output things accordingly but it also makes sense to modify the FetcherOutputFormat so that it checks whether a subWriter has been created before trying to use it.",
        "Issue Links": []
    },
    "NUTCH-1335": {
        "Key": "NUTCH-1335",
        "Summary": "OutlinkDB to collect unique URL's only",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Apr/12 19:27",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The aggregating code in the Outlink reducer does not take care of incoming duplicates. When the input segments contain duplicates of a single URL they are collected.",
        "Issue Links": []
    },
    "NUTCH-1336": {
        "Key": "NUTCH-1336",
        "Summary": "Optionally not index db_notmodified pages",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Apr/12 10:06",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/Jun/12 07:37",
        "Description": "IndexerMapReduce already skips pages with fetch_notmodified as status. However, despite the fetch status, we may still consider a page not modified if status is db_notmodified.",
        "Issue Links": []
    },
    "NUTCH-1337": {
        "Key": "NUTCH-1337",
        "Summary": "WebGraph to follow redirects",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "scoring,                                            webgraph",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Apr/12 12:50",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "With the current WebGraph URL shortening services `steal` inlinks from the actual target pages. The WebGraph OutlinkDB Mapper should use the target URL instead if there is any.",
        "Issue Links": []
    },
    "NUTCH-1338": {
        "Key": "NUTCH-1338",
        "Summary": "Determine/remove activation WARN's from project builds.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.5",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Apr/12 18:26",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This doesn't seem to be doing any harm, but as discussed here[0] it should be investigated and addressed.\n[0] http://lucene.472066.n3.nabble.com/Failing-to-copy-activation-jar-to-build-lib-td3912612.html",
        "Issue Links": []
    },
    "NUTCH-1339": {
        "Key": "NUTCH-1339",
        "Summary": "Default URL normalization rules to remove page anchors completely",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "nutchgora,                                            1.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Apr/12 20:52",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The default rules of URLNormalizerRegex remove the anchor up to the first\noccurrence of ? or &. The remaining part of the anchor is kept\nwhich may cause a large, possibly infinite number of outlinks when the same document\nfetched again and again with different URLs,\nsee http://www.mail-archive.com/user%40nutch.apache.org/msg05940.html\nParameters in inner-page anchors are a common practice in AJAX web sites.\nCurrently, crawling AJAX content is not supported (NUTCH-1323).",
        "Issue Links": []
    },
    "NUTCH-1340": {
        "Key": "NUTCH-1340",
        "Summary": "Increase scalability by only removing markers when they actually exist for DbUpdaterReducer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "18/Apr/12 14:37",
        "Updated": "02/May/13 02:29",
        "Resolved": "26/Apr/12 09:02",
        "Description": "After applying GORA-120 (this already is a huge performance boost by itself) one of the major bottlenecks of the DbUpdaterReducer is the deletion of the markers. The update reducer simply sets every row to delete its markers. A lot of rows do not actually have the markers but the deletes are fired away in any case. Because the markers are already always on the input, a simple check to see if they exist greaty improves performance.\nIn particular it is very expensive in HBase, because every single Delete inmediately triggers a connection to the regionservers. (They ignore the \"autoflush=false\" directive). Although deletes can be done in batch, this is currently not supported by Gora. For one it is very difficult to implement in the current HBaseStore with regard to multithreading, and secondly I noticed performance did not increase significantly.\nBy performance debugging on a real life cluster this currently seems to be the biggest bottleneck of the DbUpdaterReducer. (Remember only after applying GORA-120)",
        "Issue Links": [
            "/jira/browse/GORA-120"
        ]
    },
    "NUTCH-1341": {
        "Key": "NUTCH-1341",
        "Summary": "NotModified time set to now but page not modified",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Apr/12 14:50",
        "Updated": "22/May/13 03:53",
        "Resolved": "23/Oct/12 13:29",
        "Description": "Servers tend to respond with incorrect or no value for LastModified. By comparing signatures or when (fetch.getStatus() == CrawlDatum.STATUS_FETCH_NOTMODIFIED) the reducer correctly sets the db_notmodified status for the CrawlDatum. The modifiedTime value, however, is not set accordingly.",
        "Issue Links": []
    },
    "NUTCH-1342": {
        "Key": "NUTCH-1342",
        "Summary": "Read time out protocol-http",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.4,                                            1.5",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Apr/12 09:39",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Oct/19 13:29",
        "Description": "For some reason some URL's always time out with protocol-http but not protocol-httpclient. The stack trace is always the same:\n\n2012-04-20 11:25:44,275 ERROR http.Http - Failed to get protocol output\njava.net.SocketTimeoutException: Read timed out\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.read(SocketInputStream.java:129)\n        at java.io.BufferedInputStream.read1(BufferedInputStream.java:256)\n        at java.io.BufferedInputStream.read(BufferedInputStream.java:317)\n        at java.io.FilterInputStream.read(FilterInputStream.java:116)\n        at java.io.PushbackInputStream.read(PushbackInputStream.java:169)\n        at java.io.FilterInputStream.read(FilterInputStream.java:90)\n        at org.apache.nutch.protocol.http.HttpResponse.readPlainContent(HttpResponse.java:228)\n        at org.apache.nutch.protocol.http.HttpResponse.<init>(HttpResponse.java:157)\n        at org.apache.nutch.protocol.http.Http.getResponse(Http.java:64)\n        at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:138)\n\n\nSome example URL's:\n\n404 http://www.fcgroningen.nl/tribunenamen/stemmen/\n301 http://shop.fcgroningen.nl/aanbieding",
        "Issue Links": [
            "/jira/browse/NUTCH-1825"
        ]
    },
    "NUTCH-1343": {
        "Key": "NUTCH-1343",
        "Summary": "Crawl sites with hashtags in url",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Invalid",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Roberto Gardenier",
        "Created": "20/Apr/12 11:17",
        "Updated": "01/May/12 12:24",
        "Resolved": "01/May/12 11:39",
        "Description": "Hello,\nIm currently trying to crawl a site which uses hashtags in the urls. I dont seem to get any results and Im hoping im just overlooking something.\nSite structure is as follows:\nhttp://domain.com (landingpage)\nhttp://domain.com/#/page1\nhttp://domain.com/#/page1/subpage1\nhttp://domain.com/#/page2\nhttp://domain.com/#/page2/subpage1\nand so on.\nI've pointed nutch to http://domain.com as start url and in my filter i've placed all kind of rules.\nFirst i thought this would be sufficient:\n+http\\://domain\\.com\\/#\nBut then i realised that # is used for comments so i escaped it:\n+http\\://domain\\.com\\/#\nStill no results. So i thought i could use the asterix for it:\n+http\\://domain\\.com\\/*\nStill no luck.. So i started using various regex stuff but without success.\nI noticed the following messages in hadoop.log:\nINFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\nIve researched on this setting but i dont know for sure if this affects my problem in a way. This property is set to false in my configs.\nI dont know if this is even related to the situation above but maybe it helps.\nAny help is very much appreciated! I've tried googling the problem but i couldnt find documentation or anyone else with this problem.\nMany thanks in advance. \nWith kind regard,\nRoberto Gardenier",
        "Issue Links": []
    },
    "NUTCH-1344": {
        "Key": "NUTCH-1344",
        "Summary": "BasicURLNormalizer to normalize https same as http",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.6",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "21/Apr/12 10:07",
        "Updated": "22/May/13 03:53",
        "Resolved": "10/Oct/12 21:19",
        "Description": "Most of the normalization done by BasicURLNormalizer (lowercasing host, removing default port, removal of page anchors, cleaning . and . in the path) is not done for URLs with protocol https.",
        "Issue Links": []
    },
    "NUTCH-1345": {
        "Key": "NUTCH-1345",
        "Summary": "JAVA_HOME should not be required",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ben McCann",
        "Created": "21/Apr/12 23:04",
        "Updated": "12/Jan/13 22:23",
        "Resolved": null,
        "Description": "Trying to run Nutch spits out the message \"Error: JAVA_HOME is not set.\"  I already have java on my path, so I really wish I didn't need to set JAVA_HOME.  It's an extra step to get up and running and is not updated by Ubuntu's update-alternatives, so it makes it a lot harder to switch between versions of Java.",
        "Issue Links": []
    },
    "NUTCH-1346": {
        "Key": "NUTCH-1346",
        "Summary": "Follow outlinks to ignore external",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Apr/12 14:03",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/Jun/12 07:03",
        "Description": "The follow outlinks feature already respects the db.ignore.external.links setting. However, this means that outlinks of fetched pages that are external are not saved in parse data. There should be a new setting to prevent the outlink follower from going external but still storing external outlinks.",
        "Issue Links": [
            "/jira/browse/NUTCH-1184"
        ]
    },
    "NUTCH-1347": {
        "Key": "NUTCH-1347",
        "Summary": "fetcher politeness related to map-reduce",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "01/May/12 07:29",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/Dec/12 13:56",
        "Description": "when Nutch is running on Hadoop , based on map-reduce concept, each map task do some thing on it's owned data, so, each fetcher map-task work with it's Queues and do not know any thing about other Queus. so, enforce delay between successive requests and maximum concurrent requests policies on it's Queues. but with a simple test we found that it's not good piliteness mechanism when we have multiple map tasks.",
        "Issue Links": []
    },
    "NUTCH-1348": {
        "Key": "NUTCH-1348",
        "Summary": "Solrindexer fails with a java.io.IOException error.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.5",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Christian Johnsson",
        "Created": "01/May/12 19:40",
        "Updated": "03/May/12 11:45",
        "Resolved": "03/May/12 11:45",
        "Description": "I'm unable to reproduce this error but it happens from time to time when i run solrindexer.\nI use the same commands as i did with 1.4 and about the same configuration and i haven't changed any solr settings. \nHave the same plugins active just to be able to compare.\nFrom time to time the solrindexer throws an error. It happends like 1-2 times out of 5 and there is no information in the solr log about it.\nNot sure if it's a bug but i though i might as well report it since i've been running 1.4 since it was released and never came across this error in that version.\n2012-05-01 20:44:14,861 INFO  httpclient.HttpMethodDirector - I/O exception (java.net.SocketException) caught when processing request: Connection reset\n2012-05-01 20:44:14,861 INFO  httpclient.HttpMethodDirector - Retrying request\n2012-05-01 20:44:15,808 INFO  solr.SolrWriter - Indexing 250 documents\n2012-05-01 20:44:36,153 WARN  mapred.LocalJobRunner - job_local_0001\njava.io.IOException\n\tat org.apache.nutch.indexer.solr.SolrWriter.makeIOException(SolrWriter.java:152)\n\tat org.apache.nutch.indexer.solr.SolrWriter.write(SolrWriter.java:126)\n\tat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:55)\n\tat org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:44)\n\tat org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:440)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:195)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:51)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:463)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:411)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216)\nCaused by: org.apache.solr.client.solrj.SolrServerException: org.apache.commons.httpclient.ProtocolException: Unbuffered entity enclosing request can not be repeated.\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:475)\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n\tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n\tat org.apache.nutch.indexer.solr.SolrWriter.write(SolrWriter.java:124)\n\t... 8 more\nCaused by: org.apache.commons.httpclient.ProtocolException: Unbuffered entity enclosing request can not be repeated.\n\tat org.apache.commons.httpclient.methods.EntityEnclosingMethod.writeRequestBody(EntityEnclosingMethod.java:487)\n\tat org.apache.commons.httpclient.HttpMethodBase.writeRequest(HttpMethodBase.java:2114)\n\tat org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1096)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:422)\n\t... 11 more\n2012-05-01 20:44:37,074 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\nIt's running on a single machine and no hadoop.\nIt's indexing around 50-80 000 smaller documents. Worked flawless in 1.4\nThats about it",
        "Issue Links": []
    },
    "NUTCH-1349": {
        "Key": "NUTCH-1349",
        "Summary": "Make batchId explcit within debug logging and improve CLI",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "03/May/12 19:48",
        "Updated": "09/May/12 04:16",
        "Resolved": "08/May/12 11:48",
        "Description": "I find this a pain when trying to locate the batchId of some urls which are skipped when going to the Solr index. My DEBUG log output gives me\n\n2012-05-03 20:44:55,268 DEBUG indexer.IndexerJob (IndexerJob.java:map(83)) - Skipping http://www.glasgowwheelers.com/; different batch id\n2012-05-03 20:44:55,259 DEBUG indexer.IndexerJob (IndexerJob.java:map(83)) - Skipping http://www.heraldscotland.com/; different batch id\n\n\nwhen I would actually like\n\n2012-05-03 20:44:55,268 DEBUG indexer.IndexerJob (IndexerJob.java:map(83)) - Skipping http://www.glasgowwheelers.com/; different batch id (ACTUAL BATCH ID)\n2012-05-03 20:44:55,259 DEBUG indexer.IndexerJob (IndexerJob.java:map(83)) - Skipping http://www.heraldscotland.com/; different batch id (ACTUAL BATCH ID)\n\n\npatch coming up soon",
        "Issue Links": []
    },
    "NUTCH-1350": {
        "Key": "NUTCH-1350",
        "Summary": "remove unused dependancy because of access restriction",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "04/May/12 08:03",
        "Updated": "05/May/12 04:20",
        "Resolved": "04/May/12 08:04",
        "Description": "CrawlTestUtil has an unused dependancy com.sun.net.httpserver.HttpContext that sometimes causes an \"access restriction\" error when used with certain jdks. I figured since it isn't used anyway I can just remove it.",
        "Issue Links": []
    },
    "NUTCH-1351": {
        "Key": "NUTCH-1351",
        "Summary": "DomainStatistics to aggregate by TLD",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/May/12 11:56",
        "Updated": "22/May/13 03:53",
        "Resolved": "07/Jun/12 18:21",
        "Description": "The DomainStatistics tool aggregates counts by host, domain or suffix but tld is missing.",
        "Issue Links": []
    },
    "NUTCH-1352": {
        "Key": "NUTCH-1352",
        "Summary": "Improve regex urlfilters/normalizers synchronization",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora,                                            1.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "07/May/12 08:55",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jun/12 10:15",
        "Description": "I noticed that during fetching a lot of the time the fetcherthreads are blocking on a monitor because of outlink normalizing/filtering. The cause of this: Some of the regex plugins use single lock synchronization.\nThis patch improves throughput by removing synchronization locks and replace them with threadlocals were needed.\nIt has been extensively tested in production. I will commit this later today when no objection.",
        "Issue Links": []
    },
    "NUTCH-1353": {
        "Key": "NUTCH-1353",
        "Summary": "nutchgora DomainStatistics support crawlId, counter bug and reformatting",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "07/May/12 09:18",
        "Updated": "09/May/12 04:16",
        "Resolved": "07/May/12 09:20",
        "Description": "This patch fixes three issues about nutchgora DomainStatistics:\n-crawlId support (note I closed NUTCH-1290 because I thought DomainStatistics was already fixed. This was not the case.)\n-A counter bug (NOT_FETCHED should be increased instead of FETCHED)\n-reformatting (convert tabs to spaces and clear unused imports)",
        "Issue Links": []
    },
    "NUTCH-1354": {
        "Key": "NUTCH-1354",
        "Summary": "nutchgora support fetcher.queue.depth.multiplier property",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "07/May/12 09:47",
        "Updated": "09/May/12 04:16",
        "Resolved": "07/May/12 09:49",
        "Description": "Like trunk, nutchgora should support fetcher.queue.depth.multiplier property too.",
        "Issue Links": []
    },
    "NUTCH-1355": {
        "Key": "NUTCH-1355",
        "Summary": "nutchgora Configure minimum throughput for fetcher",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "07/May/12 11:45",
        "Updated": "09/May/12 04:16",
        "Resolved": "07/May/12 15:29",
        "Description": "Like trunk, nutchgora should also have a feature to configure the fetcher with a minimum throughput. (See NUTCH-1067 for the work done by Markus).\nIt's implemented in almost the same way, except that the number of times throughput falls below threshold is measured sequentially. (The counter is reset when throughput is healthy again; this should work even better against temporary dips).\nDefaults to disabled. Will commit later today if there is no objection.",
        "Issue Links": []
    },
    "NUTCH-1356": {
        "Key": "NUTCH-1356",
        "Summary": "ParseUtil use ExecutorService instead of manually thread handling.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora,                                            1.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "07/May/12 12:10",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jun/12 10:18",
        "Description": "Because ParseUtil manages it's own parser threads by creating a thread for every parse it sometimes happens that specific parsers are very expensive. For example, parsers that have threadlocal fields will initialize them for every item to be parsed.\nBy simply introducing a caching ExecutorService the ParseUtil will be able to cache threads therefore parsing more efficient. See attached patch.",
        "Issue Links": []
    },
    "NUTCH-1357": {
        "Key": "NUTCH-1357",
        "Summary": "All gora mapreduce functionality should go through StorageUtils",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "09/May/12 13:08",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "I am trying to make the concept of crawlId work for ALL nutch jobs: it seems the biggest problem with it not working as expected is because of the various ways gora mapreduce is used in nutch.\nSome jobs use StorageUtils, some use GoraMapper/GoraReduce, some even use directly GoraInputFormat/GoraOutputFormat. But the only place the translation is made from crawlId into a schema name is in StorageUtils! Currently I am converting all calls to Gora* mapreduce initializing code to StorageUtils calls.",
        "Issue Links": []
    },
    "NUTCH-1358": {
        "Key": "NUTCH-1358",
        "Summary": "Do not accept bogus arguments",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "09/May/12 13:39",
        "Updated": "11/May/12 04:16",
        "Resolved": "09/May/12 13:41",
        "Description": "Some of the tools do not explicitely check every passed argument for validity. This can mask very frustrating issues because one passes wrong arguments and the tool does not fail fast.",
        "Issue Links": []
    },
    "NUTCH-1359": {
        "Key": "NUTCH-1359",
        "Summary": "Add raw_headers support",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "09/May/12 14:00",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This should enable us to capture raw headers, however as it may not be required within every type of job, or by every type of user, it should be made configurable.",
        "Issue Links": []
    },
    "NUTCH-1360": {
        "Key": "NUTCH-1360",
        "Summary": "Suport the storing of IP address connected to when web crawling",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "09/May/12 14:02",
        "Updated": "01/May/14 06:23",
        "Resolved": "02/Jan/14 11:56",
        "Description": "Simple issue enabling us to capture the specific IP address of the host which we connect to to fetch a page.",
        "Issue Links": [
            "/jira/browse/NUTCH-1660",
            "/jira/browse/NUTCH-1713"
        ]
    },
    "NUTCH-1361": {
        "Key": "NUTCH-1361",
        "Summary": "Fix mishandling of malformed urls in generator job",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "09/May/12 14:05",
        "Updated": "08/Jun/12 14:06",
        "Resolved": "08/Jun/12 14:06",
        "Description": "This relates to the handling of malformed urls within the Generator Mapper and Reducer. Currently we do not handle such cases. Is there scope here to extend this issue to 1.X trunk?",
        "Issue Links": []
    },
    "NUTCH-1362": {
        "Key": "NUTCH-1362",
        "Summary": "Fix error handling of urls with empty fields",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "09/May/12 14:09",
        "Updated": "12/May/12 04:17",
        "Resolved": "11/May/12 09:49",
        "Description": "Within o.a.n.util.TableUtil.reverseAppendSplits() a simple if (split.length > 0) block enables us to address this issue.",
        "Issue Links": []
    },
    "NUTCH-1363": {
        "Key": "NUTCH-1363",
        "Summary": "Make parsing in FetcherJob actually work.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "09/May/12 14:11",
        "Updated": "10/May/12 20:37",
        "Resolved": "10/May/12 20:06",
        "Description": "We know that parsing during fetching is not recommended, however for those that wish to dive into the abyss the functionality should be available. This issue will address this.",
        "Issue Links": []
    },
    "NUTCH-1364": {
        "Key": "NUTCH-1364",
        "Summary": "Add a counter in Generator for malformed urls",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "nutchgora,                                            1.6",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "09/May/12 14:13",
        "Updated": "28/Apr/14 00:45",
        "Resolved": "12/Jun/12 00:13",
        "Description": "This is a simple mechanism for counting the number of malformed urls we encounter within the Generator.",
        "Issue Links": []
    },
    "NUTCH-1365": {
        "Key": "NUTCH-1365",
        "Summary": "Fix crawlId functionalilty by making using of new gora configuration",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "10/May/12 07:53",
        "Updated": "14/Aug/12 07:31",
        "Resolved": "14/Aug/12 07:31",
        "Description": "With GORA-126 it is finally possible to make correctly use of crawlId throughout nutch. This patch changes StorageUtils so that the preferred schema name (crawlId + \"_\" + schema) is correctly set on gora.",
        "Issue Links": [
            "/jira/browse/NUTCH-1450",
            "/jira/browse/GORA-150"
        ]
    },
    "NUTCH-1366": {
        "Key": "NUTCH-1366",
        "Summary": "speed up indexing by eliminating the indexreducer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "11/May/12 12:38",
        "Updated": "15/May/12 05:42",
        "Resolved": "14/May/12 14:22",
        "Description": "Currently the indexer in Nutchgora consists of both mappers and reduces. But the reduce code does not actually iterate over any (grouped/sorted) values. It simply indexes individual key/value (String/Webpage) pairs. Therefore by moving this indexing code to the mapper we can eliminate the reduce step therefore making the indexing job much faster. (No more unnecessary spilling to disk/network and no cpu wasted to sorting).\nNote this is not (directly) applicable to trunk because trunk uses a quite different approach. Different types of input are combined to a single value in the reducer. Although I think it is possible to implement a similar optimization I am not sure how to do this. So if anyone wants this for trunk too feel free to implement a similar patch.",
        "Issue Links": []
    },
    "NUTCH-1367": {
        "Key": "NUTCH-1367",
        "Summary": "Port ParserChecker to Nutchgora",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "12/May/12 00:04",
        "Updated": "16/May/12 10:32",
        "Resolved": "16/May/12 10:32",
        "Description": "This is such a great tool. It has come in handy so many times I would go blue in the face if I had to try and count. e.g. for (int i = 0; i < infinity; i++)\nI think you get the idea.",
        "Issue Links": []
    },
    "NUTCH-1368": {
        "Key": "NUTCH-1368",
        "Summary": "SolrDeleteDuplicates.java:270",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "fw",
        "Created": "15/May/12 08:25",
        "Updated": "15/May/12 08:30",
        "Resolved": "15/May/12 08:30",
        "Description": "java.lang.NullPointerException\n\tat org.apache.hadoop.io.Text.encode(Text.java:388)\n\tat org.apache.hadoop.io.Text.set(Text.java:178)\n\tat org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:271)\n\tat org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:1)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:192)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:176)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)",
        "Issue Links": [
            "/jira/browse/NUTCH-1100"
        ]
    },
    "NUTCH-1369": {
        "Key": "NUTCH-1369",
        "Summary": "Improve ParserChecker in Nutchgora",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.2",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/May/12 22:48",
        "Updated": "22/May/13 03:53",
        "Resolved": "25/Apr/13 00:49",
        "Description": "This issue should bring the ParserChecker implementation in Nutchgora into line with trunk. WIP patch coming up.",
        "Issue Links": []
    },
    "NUTCH-1370": {
        "Key": "NUTCH-1370",
        "Summary": "Expose exact number of urls injected @runtime",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "injector",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/May/12 09:23",
        "Updated": "22/May/13 03:53",
        "Resolved": "22/Nov/12 14:57",
        "Description": "Example: When using trunk, currently we see \n\n2012-05-22 09:04:00,239 INFO  crawl.Injector - Injector: starting at 2012-05-22 09:04:00\n2012-05-22 09:04:00,239 INFO  crawl.Injector - Injector: crawlDb: crawl/crawldb\n2012-05-22 09:04:00,239 INFO  crawl.Injector - Injector: urlDir: urls\n2012-05-22 09:04:00,253 INFO  crawl.Injector - Injector: Converting injected urls to crawl db entries.\n2012-05-22 09:04:00,955 INFO  plugin.PluginRepository - Plugins: looking in:\n\n\nI would like to see\n\n2012-05-22 09:04:00,239 INFO  crawl.Injector - Injector: starting at 2012-05-22 09:04:00\n2012-05-22 09:04:00,239 INFO  crawl.Injector - Injector: crawlDb: crawl/crawldb\n2012-05-22 09:04:00,239 INFO  crawl.Injector - Injector: urlDir: urls\n2012-05-22 09:04:00,253 INFO  crawl.Injector - Injector: Injected N urls to crawl/crawldb\n2012-05-22 09:04:00,253 INFO  crawl.Injector - Injector: Converting injected urls to crawl db entries.\n2012-05-22 09:04:00,955 INFO  plugin.PluginRepository - Plugins: looking in:\n\n\nThis would make debugging easier and would help those who end up getting \n\n2012-05-22 09:04:04,850 WARN  crawl.Generator - Generator: 0 records selected for fetching, exiting ...",
        "Issue Links": [
            "/jira/browse/NUTCH-1471"
        ]
    },
    "NUTCH-1371": {
        "Key": "NUTCH-1371",
        "Summary": "Replace Ivy with Maven Ant tasks",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "22/May/12 10:11",
        "Updated": "07/Apr/17 14:25",
        "Resolved": "07/Apr/17 14:25",
        "Description": "We might move to Maven altogether but a good intermediate step could be to rely on the maven ant tasks for managing the dependencies. Ivy does a good job but we need to have a pom file anyway for publishing the artefacts which means keeping the pom.xml and ivy.xml contents in sync. Most devs are also more familiar with Maven, and it is well integrated in IDEs. Going the ANT+MVN way also means that we don't have to rewrite the whole building process and can rely on our existing script",
        "Issue Links": [
            "/jira/browse/NUTCH-2292"
        ]
    },
    "NUTCH-1372": {
        "Key": "NUTCH-1372",
        "Summary": "Improve execution of normalisers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "22/May/12 10:35",
        "Updated": "22/May/12 10:41",
        "Resolved": "22/May/12 10:41",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1373": {
        "Key": "NUTCH-1373",
        "Summary": "Implement consistent execution of normalising and filtering in Generator",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "22/May/12 10:40",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "As per discussion here [0] this issue should address the inconsistencies we see in the scheduled execution of normalising and filtering between Nutchgora Generator Mapper and trunk Generator mapper/reducer.\nHopefully we can come to some consensus as to the best approach acorss both dists. \n[0] http://www.mail-archive.com/user%40nutch.apache.org/msg06360.html",
        "Issue Links": []
    },
    "NUTCH-1374": {
        "Key": "NUTCH-1374",
        "Summary": "Workaround for license headers",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.4,                                            nutchgora",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "22/May/12 11:58",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Currently in both versions of Nutch we have two types of files which DO NOT contain license headers; namely all package.html files and the test files within the language detection plugin. On my initial tests, adding license headers to the language test files breaks the tests so we need to find a workaround (or the correct synatx) to add commented out license headers to these files.",
        "Issue Links": []
    },
    "NUTCH-1375": {
        "Key": "NUTCH-1375",
        "Summary": "extract main content of a html file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.8",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "22/May/12 12:19",
        "Updated": "25/Aug/13 15:52",
        "Resolved": "25/Aug/13 15:52",
        "Description": "i write a code, that can extract main content of a html (usally weblogs).\nthis content usally apperas in <body><p> tag but there is no insurance. also might be multiple tags with form of <body><p> but only one of them is main content. this code first find body node, and then compute weight of childs nodes that compute based on text volume and height. so the code find lowest node that have maximum text volume.\ni hope that improvement of this code cause to solutions to find fake or duplicated pages.",
        "Issue Links": [
            "/jira/browse/NUTCH-961"
        ]
    },
    "NUTCH-1376": {
        "Key": "NUTCH-1376",
        "Summary": "Add description parameter to every ant task",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.6,                                            2.1",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/May/12 21:33",
        "Updated": "30/Jul/12 04:22",
        "Resolved": "29/Jul/12 13:04",
        "Description": "This is really really easy to implement and makes the task of identifying ant target's a piece of cake",
        "Issue Links": []
    },
    "NUTCH-1377": {
        "Key": "NUTCH-1377",
        "Summary": "Add option to index via CloudSolrServer instead",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/May/12 13:48",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "06/Jan/19 20:39",
        "Description": "Nutch indexes to a specific Solr server. With SolrCloud on its way we can still use the current indexer and point to any server. However, the SolrCloudServer can connect to ZooKeeper instead and automatically find the correct server to index to.",
        "Issue Links": [
            "/jira/browse/NUTCH-1486",
            "/jira/browse/NUTCH-1480",
            "/jira/browse/NUTCH-2197",
            "/jira/browse/NUTCH-1662"
        ]
    },
    "NUTCH-1378": {
        "Key": "NUTCH-1378",
        "Summary": "HostDb NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "23/May/12 14:45",
        "Updated": "24/May/12 05:26",
        "Resolved": "23/May/12 14:48",
        "Description": "This is a no-brainer to fix a NPE when using the HostDb functionality. Will attach patch and commit right away.",
        "Issue Links": []
    },
    "NUTCH-1379": {
        "Key": "NUTCH-1379",
        "Summary": "NPE when reprUrl is null in ParseUtil",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "30/May/12 09:24",
        "Updated": "30/May/12 09:28",
        "Resolved": "30/May/12 09:28",
        "Description": "Sometimes reprUrl is null in ParseUtil. Exact cause is still fuzzy but this is a nice workaround for now.",
        "Issue Links": []
    },
    "NUTCH-1380": {
        "Key": "NUTCH-1380",
        "Summary": "Fetcher reducer not to configure filter/normalizers",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/Jun/12 13:08",
        "Updated": "19/Aug/22 09:03",
        "Resolved": "19/Aug/22 09:03",
        "Description": "The fetcher has an identity reducer but uses the configure method from the mapper, this means large filters are loaded in the reducer as well eating up lots of unnecessary heap space.",
        "Issue Links": [
            "/jira/browse/NUTCH-2375"
        ]
    },
    "NUTCH-1381": {
        "Key": "NUTCH-1381",
        "Summary": "Allow to override default subcollection field name",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/Jun/12 13:09",
        "Updated": "22/May/13 03:53",
        "Resolved": "07/Jun/12 18:18",
        "Description": "The subcollection filter by default uses the subcollection field name but since NUTCH-1266 allows to override it per subcollection. This issue should introduce a configuration directive to override the default field name globally.",
        "Issue Links": [
            "/jira/browse/NUTCH-1266"
        ]
    },
    "NUTCH-1382": {
        "Key": "NUTCH-1382",
        "Summary": "Adding support for EmbeddedSolrServer to SolrIndexer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Emre \u00c7elikten",
        "Created": "08/Jun/12 15:37",
        "Updated": "15/Nov/13 18:02",
        "Resolved": "15/Nov/13 18:02",
        "Description": "Here is a hack to allow somebody to plug their own SolrServer into SolrIndexer. It allows people to use EmbeddedSolrServer in Nutch.\nIt works by:\nadding a constructor in SolrIndexer with parameter SolrServer, \nadding an ugly method of getSolrServer into SolrUtils which returns SolrServer if there is one provided by the programmer or returns default getCommonsHttpSolrServer(...)\nreplacing every occurrence of getCommonsHttpSolrServer by getSolrServer.\nHope this helps. This is my first patch ever to FOSS community so I hope I am doing it correctly.",
        "Issue Links": []
    },
    "NUTCH-1383": {
        "Key": "NUTCH-1383",
        "Summary": "IndexingFiltersChecker to show error message instead of null pointer exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5,                                            1.6",
        "Fix Version/s": "1.6",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "09/Jun/12 21:43",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Oct/12 21:05",
        "Description": "IndexingFiltersChecker may throw null pointer exceptions if\n\ncontent returned by protocol implementation is null (artifact of NUTCH-1293)\nif one of the indexing filters sets doc to null (the interface IndexingFilter allows to exclude documents by returning null, cf. the IndexingFilter of NUTCH-966)",
        "Issue Links": []
    },
    "NUTCH-1384": {
        "Key": "NUTCH-1384",
        "Summary": "Typo in ParseSegment's run-method",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Matthias Agethle",
        "Created": "11/Jun/12 06:25",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Jun/12 09:30",
        "Description": "In the class org.apache.nutch.parse.ParseSegments there's a typo in the run-method: instead of checking wheter \"-noFilter\" was specified on the command-line, the code looks for \"-noilter\" (missing f, line 234).",
        "Issue Links": []
    },
    "NUTCH-1385": {
        "Key": "NUTCH-1385",
        "Summary": "More robust plug-in order properties in \"nutch-site.xml\"",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "indexer,                                            parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Andy Xue",
        "Created": "11/Jun/12 07:17",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Jun/12 09:28",
        "Description": "When listing multiple scoring filters in certain properties (listed below) in \"nutch-site.xml\", it is vital that no spaces/newlines/tabs are placed in front of the value content.\nE.g.:\nThis is fine:\n<value>org.apache.nutch.scoring.opic.OPICScoringFilter myFilter</value>\nEither of these will generate an exception:\n<value> org.apache.nutch.scoring.opic.OPICScoringFilter myFilter</value>\n<value>\norg.apache.nutch.scoring.opic.OPICScoringFilter\nmyFilter\n</value>\nAffects these properties in \"nutch-site.xml\":\n\nindexingfilter.order\nurlnormalizer.order\nurlfilter.order\nhtmlparsefilter.order\nscoring.filter.order\n\nSolution: replaced \n{order.split(\"\\\\s+\")}\n to \n{order.trim().split(\"\\\\s+\")}\n. Patch provided.",
        "Issue Links": []
    },
    "NUTCH-1386": {
        "Key": "NUTCH-1386",
        "Summary": "Headings filter not to add empty values",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Jun/12 10:20",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jun/12 10:22",
        "Description": "Headings filter can add empty values and doesn't trim the headings.",
        "Issue Links": []
    },
    "NUTCH-1387": {
        "Key": "NUTCH-1387",
        "Summary": "All parsers should respond to cancellation / interrupts.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "12/Jun/12 11:12",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "During parsing a TimeoutException can occur. This is caused whenever the FutureTask.get() cannot be completed within the specified timeout. The tricky part is that single urls might be perfectly able to complete within the timeout, but when there is a heavy concurrent load (a lot of semi-expensive parses) the parser load might stack up and cause many parses to timeout. This can be the case with parsing during fetch. But when using a separate parserjob this can also happen because Parser implementation do not necessarily have to respond to a thread interrupt. (Which is fired away with the task.cancel(true) call). If a parser does not check the Thread.interrupted state at regular intervals, it will just continue to run and eat up resources. I find it very helpful to debug stalling fetchers/parsers with the lazy men's profiler: kill -QUIT <process_id>. This will dump stacktraces, sometimes exposing the fact that hundreds of parser threads are still active in the background. (Of course many of them already timed out a long time ago).\nTo fix this, every parser should check it's interrupted state at regular intervals. (For example an html parse might be stuck walking the DOM tree, so checking after every Nth element would be an appropiate moment.)\nThis issue is for reference first. Fixing it all at once would be a huge task.",
        "Issue Links": [
            "/jira/browse/NUTCH-1318"
        ]
    },
    "NUTCH-1388": {
        "Key": "NUTCH-1388",
        "Summary": "Optionally maintain custom fetch interval despite AdaptiveFetchSchedule",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6",
        "Component/s": "injector",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Jun/12 13:05",
        "Updated": "11/Dec/13 03:10",
        "Resolved": "20/Jul/12 14:24",
        "Description": "During injection a custom fetch interval can be configured but it is not maintained with an AdaptiveFetchSchedule enabled.",
        "Issue Links": [
            "/jira/browse/NUTCH-1682",
            "/jira/browse/NUTCH-1683"
        ]
    },
    "NUTCH-1389": {
        "Key": "NUTCH-1389",
        "Summary": "parsechecker and indexchecker to report truncated content",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "indexer,                                            parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "12/Jun/12 20:46",
        "Updated": "22/May/13 03:53",
        "Resolved": "27/Mar/13 21:35",
        "Description": "ParserChecker and IndexingFiltersChecker should report when a document is truncated due to \n{http,file,ftp}\n.content.limit.\nTruncated content may cause text and metadata extraction to fail for PDF and other binary document formats.\nA hint that truncation (and not a broken plugin) is the possible reason would be useful.\nSee NUTCH-965 and ParseSegment.isTruncated(content).",
        "Issue Links": []
    },
    "NUTCH-1390": {
        "Key": "NUTCH-1390",
        "Summary": "readdb -url $url throws NPE with gora-cassandra",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.2",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Jun/12 23:24",
        "Updated": "22/May/13 03:53",
        "Resolved": "11/Feb/13 01:00",
        "Description": "After successfully injecting, generating, fetching (without parsing enabled), parsing, updatingdb, then executinga readdb passing a particular -url argument I get a lovely NPE\n\nlewis@lewis:~/ASF/nutchgora/runtime/local$ ./bin/nutch readdb -url http://www.trancearoundtheworld.com\nWebTableReader: java.lang.NullPointerException\n\tat org.apache.gora.cassandra.store.CassandraClient.getFamilyMap(CassandraClient.java:220)\n\tat org.apache.gora.cassandra.store.CassandraStore.execute(CassandraStore.java:108)\n\tat org.apache.nutch.crawl.WebTableReader.read(WebTableReader.java:234)\n\tat org.apache.nutch.crawl.WebTableReader.run(WebTableReader.java:476)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.WebTableReader.main(WebTableReader.java:412)",
        "Issue Links": []
    },
    "NUTCH-1391": {
        "Key": "NUTCH-1391",
        "Summary": "readdb -stats fires java.io.EOFException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Jun/12 23:31",
        "Updated": "22/May/13 03:53",
        "Resolved": "20/Jun/12 10:34",
        "Description": "This is confirmed to be the case with both HBase and Cassandra v1.1.1\nlewis@lewis:~/ASF/nutchgora/runtime/local$ ./bin/nutch readdb -stats\nWebTable statistics start\nWebTableReader: java.io.EOFException\n\tat java.io.DataInputStream.readFully(DataInputStream.java:180)\n\tat java.io.DataInputStream.readFully(DataInputStream.java:152)\n\tat org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1508)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1486)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1475)\n\tat org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1470)\n\tat org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders(SequenceFileOutputFormat.java:89)\n\tat org.apache.nutch.crawl.WebTableReader.run(WebTableReader.java:537)\n\tat org.apache.nutch.crawl.WebTableReader.processStatJob(WebTableReader.java:218)\n\tat org.apache.nutch.crawl.WebTableReader.run(WebTableReader.java:479)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.WebTableReader.main(WebTableReader.java:412)",
        "Issue Links": []
    },
    "NUTCH-1392": {
        "Key": "NUTCH-1392",
        "Summary": "-force and -resume arguments being ignored in ParserJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Jun/12 23:34",
        "Updated": "22/May/13 03:53",
        "Resolved": "14/Jun/12 12:34",
        "Description": "From the log below there is obviously something not right here as both -resume and -force are passed to the CLI but blatantly ignored within the log output.\nlewis@lewis:~/ASF/nutchgora/runtime/local$ ./bin/nutch parse\nUsage: ParserJob (<batchId> | -all) [-crawlId <id>] [-resume] [-force]\n    <batchId>     - symbolic batch ID created by Generator\n    -crawlId <id> - the id to prefix the schemas to operate on, \n \t \t    (default: storage.crawl.id)\n    -all          - consider pages from all crawl jobs\n    -resume       - resume a previous incomplete job\n    -force        - force re-parsing even if a page is already parsed\nlewis@lewis:~/ASF/nutchgora/runtime/local$ ./bin/nutch parse -all -resume -force\nParserJob: starting\nParserJob: resuming:\tfalse\nParserJob: forced reparse:\tfalse\nParserJob: parsing all\nParsing http://www.trancearoundtheworld.com/\nParserJob: success",
        "Issue Links": []
    },
    "NUTCH-1393": {
        "Key": "NUTCH-1393",
        "Summary": "Display consistent usage of GeneratorJob with 1.X",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.2",
        "Component/s": "administration gui,                                            generator",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Jun/12 23:38",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/Mar/13 19:38",
        "Description": "If we pass the generate argument to the nutch script, the Generator auto-spings into action and begins generating fetchlists. This should not be the case, instead it should print traditional usage to stdout. An example is below\n\nlewis@lewis:~/ASF/nutchgora/runtime/local$ ./bin/nutch generate\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: true\nGeneratorJob: done\nGeneratorJob: generated batch id: 1339628223-1694200031\n\n\nAll I wanted to do was get the usage params printed to stdout but instead it generated my batch willy nilly.",
        "Issue Links": []
    },
    "NUTCH-1394": {
        "Key": "NUTCH-1394",
        "Summary": "backport NUTCH-1232 Remove site field from index-basic",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.2",
        "Component/s": "indexer,                                            storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Jun/12 23:42",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/Dec/12 20:27",
        "Description": "This is a simple backport. The 2.0 Solr schema and mappings still contain the field \"site\" which has been removed in 1.x (NUTCH-1232). Should be done also in 2.0: it's easier to maintain only one Solr installation for all Nutch versions.",
        "Issue Links": []
    },
    "NUTCH-1395": {
        "Key": "NUTCH-1395",
        "Summary": "Show batchId when skipping within ParserJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "crawldb,                                            parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Jun/12 12:28",
        "Updated": "31/Aug/12 04:15",
        "Resolved": "30/Aug/12 20:52",
        "Description": "Although the ParserJob CLI has been smartened up, logging still lets us down where we are only teased with the 'different batch id' for an url which is skipped.\n\nParsing http://www.trancearoundtheworld.com/tatw/399\nParsing http://www.trancearoundtheworld.com/index.php\nSkipping http://www.aboveandbeyond.nu/music; different batch id\nParsing http://www.trancearoundtheworld.com/tatw/425\nParsing http://www.trancearoundtheworld.com/tatw/398\nParsing https://twitter.com/tatw\nParsing http://www.trancearoundtheworld.com/tatw/401\n\n\nI would like to see\n\nSkipping http://www.aboveandbeyond.nu/music; different batch id ($batchId)",
        "Issue Links": []
    },
    "NUTCH-1396": {
        "Key": "NUTCH-1396",
        "Summary": "Upgrade to Tika 1.1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "15/Jun/12 09:49",
        "Updated": "15/Jun/12 16:59",
        "Resolved": "15/Jun/12 11:32",
        "Description": "Copied code from trunk for MimeUtil and upgraded dependency to Tika 1.1",
        "Issue Links": []
    },
    "NUTCH-1397": {
        "Key": "NUTCH-1397",
        "Summary": "language-identifier incorrectly handles double-barreled language properties",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Jun/12 13:58",
        "Updated": "18/Oct/21 15:17",
        "Resolved": null,
        "Description": "Currently when language-identifier is activated is parses and identifies langauge-type=en, however does not identify en-GB or en-US. This issues should correct that.",
        "Issue Links": [
            "/jira/browse/NUTCH-2278",
            "/jira/browse/TIKA-1723"
        ]
    },
    "NUTCH-1398": {
        "Key": "NUTCH-1398",
        "Summary": "Upgrade to Hadoop 1.0.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.5.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "15/Jun/12 14:05",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/Jun/12 13:11",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1401"
        ]
    },
    "NUTCH-1399": {
        "Key": "NUTCH-1399",
        "Summary": "TestProtocolHttpClient fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "17/Jun/12 17:04",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/Jun/12 14:48",
        "Description": "the test fails because the http servers are not closed between tests",
        "Issue Links": []
    },
    "NUTCH-1400": {
        "Key": "NUTCH-1400",
        "Summary": "Remove developer -core option for bin/nutch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "nutchgora,                                            1.5.1",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Jun/12 08:50",
        "Updated": "22/May/13 03:54",
        "Resolved": "20/Jun/12 09:30",
        "Description": "As highlighted by Sebastian, we now have no relevant use for the -core option in the bin/nutch script. This should be removed to clean up the script.",
        "Issue Links": []
    },
    "NUTCH-1401": {
        "Key": "NUTCH-1401",
        "Summary": "Upgrade to Hadoop 1.0.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "nutchgora",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "19/Jun/12 12:46",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/Jun/12 13:14",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1398"
        ]
    },
    "NUTCH-1402": {
        "Key": "NUTCH-1402",
        "Summary": "Create AbstractScoringFilter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "19/Jun/12 12:49",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "11/Nov/13 15:38",
        "Description": "Most scoring filters don't need to implement all the methods defined by the interface. Having an AbstractScoringFilter would make it easier to implement a new scoring filter or understand existing ones.",
        "Issue Links": [
            "/jira/browse/NUTCH-1653"
        ]
    },
    "NUTCH-1403": {
        "Key": "NUTCH-1403",
        "Summary": "Add default ScoringFilter for manipulating metadata",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "19/Jun/12 12:56",
        "Updated": "01/Feb/21 21:29",
        "Resolved": "01/Feb/21 20:25",
        "Description": "This is currently done by the urlmeta plugin, which has too vague a name and a redundant indexing filter now that we have the index-metadata plugin. This scoring filter would help defining which metadata to pass from : \n\nthe crawl metadata to the content metadata\nthe content metadata to the parse metadata\nthe parse metadata to the crawldatum for the outlinks\nI'd make this scoring filter available by default i.e. not in a separate plugin as its functionalities are commonly used.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/458"
        ]
    },
    "NUTCH-1404": {
        "Key": "NUTCH-1404",
        "Summary": "Nutch script fails to find job file in deploy mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "nutchgora,                                            1.5.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "19/Jun/12 13:07",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/Jun/12 13:23",
        "Description": "See http://lucene.472066.n3.nabble.com/Nutch-1-5-Deploy-Mode-Doesn-t-Work-like-Nutch-1-4-Deploy-Mode-tp3990169.html",
        "Issue Links": []
    },
    "NUTCH-1405": {
        "Key": "NUTCH-1405",
        "Summary": "Allow to overwrite CrawlDatum's with injected entries",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5,                                            1.6",
        "Fix Version/s": "1.6",
        "Component/s": "injector",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Jun/12 09:15",
        "Updated": "22/May/13 03:53",
        "Resolved": "05/Jul/12 16:58",
        "Description": "Injector's reducer does not permit overwriting existing CrawlDatum entries. It is, however, useful to optionally overwrite so users can reset metadata manually.",
        "Issue Links": []
    },
    "NUTCH-1406": {
        "Key": "NUTCH-1406",
        "Summary": "index-metadata plugin: conversion to Solr date format",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Kristof",
        "Created": "20/Jun/12 21:34",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This improvement to the index-mdata plugin allows for conversion of selected fields to the Solr date format. The main benefit of this conversion is the possibility to create range facets.\nIn order to convert the values of selected metatags to Solr date format, you must specify in nutch-site.xml. This can be for example used with Dublin Core elements. A subdomain which would have pages with the meta tag dcterms.modified would be cic.gc.ca. dcterms.modified must also be defined in the metatags.names and index.parse.md properties.\n\n<property>\n\t<name>index.dateconvert.md</name>\n\t<value>metatag.dcterms.modified</value>\n\t<description>For plugin index-metadata: Indicate here the name of the html meta tag that should be converted to Solr date format.\n\t</description>\n</property>",
        "Issue Links": [
            "/jira/browse/NUTCH-809"
        ]
    },
    "NUTCH-1407": {
        "Key": "NUTCH-1407",
        "Summary": "BasicIndexingFilter to optionally add domain field",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Jun/12 12:59",
        "Updated": "22/May/13 03:53",
        "Resolved": "25/Jun/12 14:49",
        "Description": "The basic indexing filter already adds the host field to a NutchDocument but no domain field. In Solr you can copyField a host field and obtain a domain field but this is a bit cumbersome and not very user friendly.",
        "Issue Links": []
    },
    "NUTCH-1408": {
        "Key": "NUTCH-1408",
        "Summary": "RobotRulesParser main doesn't take URL's",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "22/Jun/12 11:40",
        "Updated": "22/May/13 03:53",
        "Resolved": "25/Jun/12 14:42",
        "Description": "lib-http's org.apache.nutch.protocol.http.api.RobotRulesParser main() takes a robot file and an URL file according to its usage output. It, however, expects URI paths not URL's and will therefore never work if an input contains URL's.",
        "Issue Links": []
    },
    "NUTCH-1409": {
        "Key": "NUTCH-1409",
        "Summary": "Remove deprecated properties db.{default,max}.fetch.interval, generate.max.per.host.by.ip",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Matthias Agethle",
        "Created": "22/Jun/12 14:07",
        "Updated": "22/Aug/14 23:21",
        "Resolved": "22/Aug/14 21:24",
        "Description": "1) Remove deprecated properties from nutch-default.xml (generate.max.per.host and db.default.fetch.interval).\n2) The already removed properties generate.max.per.host.by.ip and db.max.fetch.interval are still used in source code.",
        "Issue Links": [
            "/jira/browse/NUTCH-1514"
        ]
    },
    "NUTCH-1410": {
        "Key": "NUTCH-1410",
        "Summary": "impact of a map-reduce problem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "fetcher,                                            generator",
        "Assignee": null,
        "Reporter": "behnam nikbakht",
        "Created": "27/Jun/12 08:16",
        "Updated": "01/May/14 06:02",
        "Resolved": "18/Apr/14 09:07",
        "Description": "with a simple test , found that each mapper or reducer have a local view of variables. in Nutch, there are multiple places that share a variable between mappers or reducers , for example in generate there is a shared variable : hostCounts . or in fetcher , the last request time for each mapper (fetcherThread) is different from another.\nthis problem cause critical problems like send multiple requests to same host that cause to block.",
        "Issue Links": []
    },
    "NUTCH-1411": {
        "Key": "NUTCH-1411",
        "Summary": "nutchgora fetcher.store.content does not work",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "27/Jun/12 08:18",
        "Updated": "04/Sep/12 19:15",
        "Resolved": "09/Jul/12 15:22",
        "Description": "http://lucene.472066.n3.nabble.com/parse-and-solrindex-in-nutch-2-0-td3991247.html\nThe property fetcher.store.content doesn't do anything. Content is always stored. Fix or remove property, what do you think?",
        "Issue Links": []
    },
    "NUTCH-1412": {
        "Key": "NUTCH-1412",
        "Summary": "Upgrade commons lang",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6",
        "Component/s": "build",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "28/Jun/12 12:28",
        "Updated": "22/May/13 03:53",
        "Resolved": "28/Jun/12 15:40",
        "Description": "Upgrade commons-lang from 2.4 to 2.6.",
        "Issue Links": []
    },
    "NUTCH-1413": {
        "Key": "NUTCH-1413",
        "Summary": "Record response time",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "28/Jun/12 17:30",
        "Updated": "01/May/14 06:23",
        "Resolved": "22/Jan/14 21:24",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1630"
        ]
    },
    "NUTCH-1414": {
        "Key": "NUTCH-1414",
        "Summary": "Date extraction parse filter",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "28/Jun/12 17:31",
        "Updated": "19/Jul/16 00:03",
        "Resolved": null,
        "Description": "Date extraction parse filter for Nutch to provide means to extract an arbitrary page date (article date) from the parse text.",
        "Issue Links": []
    },
    "NUTCH-1415": {
        "Key": "NUTCH-1415",
        "Summary": "release packages to contain top level folder apache-nutch-x.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.6,                                            1.5.1",
        "Fix Version/s": "1.6,                                            2.1",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "30/Jun/12 13:35",
        "Updated": "22/May/13 03:54",
        "Resolved": "18/Sep/12 20:59",
        "Description": "The release packages should contain a top level folder named apache-nutch-x.x (x replaced by major and minor version) as in previous releases. Unpacking the packages from the command line via tar xvfz package.tar.gz or unzip package.zip should place all files in that folder. Cf. discussions on mailing lists:\n\nhttp://mail-archives.apache.org/mod_mbox/nutch-dev/201205.mbox/%3C4FBD613F.1020100@googlemail.com%3E\nhttp://mail-archives.apache.org/mod_mbox/nutch-user/201206.mbox/%3Czarafa.4fe9e41c.2e51.6a20afee54fe4ae7@mail.openindex.io%3E",
        "Issue Links": [
            "/jira/browse/NUTCH-1436"
        ]
    },
    "NUTCH-1416": {
        "Key": "NUTCH-1416",
        "Summary": "IndexerMapReduce can index older version of a document instead of latest one",
        "Type": "Bug",
        "Status": "Reopened",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Jianyun He",
        "Created": "01/Jul/12 08:10",
        "Updated": "25/Jun/15 15:13",
        "Resolved": null,
        "Description": "When we update the index,can not guarantee that the contents which be indexed is the latest.In the class IndexerMapReduce and method reduce(), it has the following code:\npublic void reduce(Text key, Iterator<NutchWritable> values,\n                     OutputCollector<Text, NutchDocument> output, Reporter reporter) throws IOException \n{\n   \u2026\u2026\n   }\n else if (value instanceof ParseData) \n{  \n      parseData = (ParseData)value;\n   }\n else if (value instanceof ParseText) \n{ \n      parseText = (ParseText)value;\n   }\n   \u2026\u2026\n}\nFor example,30 days ago,I fetched the web page A,now I fetch it again. Then the key A will correspond to two ParseData objects(located in different segments).But in this code,it does not compare the fetch time and simply overwrites the previous value.So the final value maybe the old one.",
        "Issue Links": [
            "/jira/browse/NUTCH-1617",
            "/jira/browse/NUTCH-1625"
        ]
    },
    "NUTCH-1417": {
        "Key": "NUTCH-1417",
        "Summary": "Remove o.a.n.metadata.Office",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5",
        "Fix Version/s": "1.6,                                            2.1",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "02/Jul/12 12:36",
        "Updated": "29/Jul/12 13:15",
        "Resolved": "29/Jul/12 13:15",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1418": {
        "Key": "NUTCH-1418",
        "Summary": "error parsing robots rules- can't decode path: /wiki/Wikipedia%3Mediation_Committee/",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Arijit Mukherjee",
        "Created": "02/Jul/12 18:05",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/May/13 11:50",
        "Description": "Since learning that nutch will be unable to crawl the javascript function calls in href, I started looking for other alternatives. I decided to crawl http://en.wikipedia.org/wiki/Districts_of_India.\n    I first tried injecting this URL and follow the step-by-step approach till fetcher - when I realized, nutch did not fetch anything from this website. I tried looking into logs/hadoop.log and found the following 3 lines - which I believe could be saying that nutch is unable to parse the robots.txt in the website and ttherefore, fetcher stopped?\n    2012-07-02 16:41:07,452 WARN  api.RobotRulesParser - error parsing robots rules- can't decode path: /wiki/Wikipedia%3Mediation_Committee/\n    2012-07-02 16:41:07,452 WARN  api.RobotRulesParser - error parsing robots rules- can't decode path: /wiki/Wikipedia_talk%3Mediation_Committee/\n    2012-07-02 16:41:07,452 WARN  api.RobotRulesParser - error parsing robots rules- can't decode path: /wiki/Wikipedia%3Mediation_Cabal/Cases/\n    I tried checking the URL using parsechecker and no issues there! I think it means that the robots.txt is malformed for this website, which is preventing fetcher from fetching anything. Is there a way to get around this problem, as parsechecker seems to go on its merry way parsing.",
        "Issue Links": []
    },
    "NUTCH-1419": {
        "Key": "NUTCH-1419",
        "Summary": "parsechecker and indexchecker to report protocol status",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.6",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "03/Jul/12 12:46",
        "Updated": "22/May/13 03:54",
        "Resolved": "26/Mar/13 19:43",
        "Description": "Parsechecker and indexchecker should report the protocol status when the fetch was not successful (status other than 200/ok).\nIn case of a redirect, the protocol status contains the URL a redirect points to. Usually, this URL should be checked instead of the original one which is not indexed. The content of a redirect response is less useful (and often empty):\n\n% nutch indexchecker http://lucene.apache.org/nutch/\nfetching: http://lucene.apache.org/nutch/\nparsing: http://lucene.apache.org/nutch/\ncontentType: text/html\ncontent :       301 Moved Permanently Moved Permanently The document has moved here . Apache/2.4.1 (Unix) OpenSSL/1.\ntitle : 301 Moved Permanently\nhost :  lucene.apache.org\ntstamp :        Tue Jul 03 13:27:32 CEST 2012\nurl :   http://lucene.apache.org/nutch/",
        "Issue Links": []
    },
    "NUTCH-1420": {
        "Key": "NUTCH-1420",
        "Summary": "Get rid of the dreaded \ufffd",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7,                                            2.2.1",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/Jul/12 12:57",
        "Updated": "27/Jun/13 17:15",
        "Resolved": "27/Jun/13 17:15",
        "Description": "Some pages, especially PDF's, produce sequences with the dreaded \ufffd character. This patch removes them from the title and content field.",
        "Issue Links": [
            "/jira/browse/NUTCH-1582"
        ]
    },
    "NUTCH-1421": {
        "Key": "NUTCH-1421",
        "Summary": "RegexURLNormalizer to only skip rules with invalid patterns",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.6",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "05/Jul/12 09:47",
        "Updated": "22/May/13 03:54",
        "Resolved": "23/Oct/12 20:54",
        "Description": "If a regex-normalize.xml file contains one rule with a syntactically invalid regular expression patterns, all rules are discarded and no normalization is done. \nIn combination with a detailed error message, RegexURLNormalizer should only skip the invalid rule but use all other (valid) rules.",
        "Issue Links": []
    },
    "NUTCH-1422": {
        "Key": "NUTCH-1422",
        "Summary": "bypass signature comparison when a document is redirected",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.4",
        "Fix Version/s": "1.9",
        "Component/s": "crawldb,                                            fetcher",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "06/Jul/12 14:04",
        "Updated": "15/Jul/14 09:53",
        "Resolved": "15/Jul/14 09:35",
        "Description": "In a long running continuous crawl with Nutch 1.4 URLs with a HTTP redirect (http.redirect.max = 0) are kept as not-modified in the CrawlDb. Short protocol (cf. attached dumped segment / CrawlDb data):\n 2012-02-23 :  injected\n 2012-02-24 :  fetched\n 2012-03-30 :  re-fetched, signature changed\n 2012-04-20 :  re-fetched, redirected\n 2012-04-24 :  in CrawlDb as db_notmodified, still indexed with old content!\nThe signature of a previously fetched document is not reset when the URL/doc is changed to a redirect at a later time. CrawlDbReducer.reduce then sets the status to db_notmodified because the new signature in with fetch status is identical to the old one.\nPossible fixes (??):\n\nreset the signature in Fetcher\nhandle this case in CrawlDbReducer.reduce",
        "Issue Links": [
            "/jira/browse/NUTCH-1502"
        ]
    },
    "NUTCH-1423": {
        "Key": "NUTCH-1423",
        "Summary": "Remove unused fields in LanguageIndexingFilter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "09/Jul/12 11:41",
        "Updated": "09/Jul/12 11:43",
        "Resolved": "09/Jul/12 11:43",
        "Description": "The LanguageIndexingFilter declares fields on the input that are not used. These fields must be removed.",
        "Issue Links": []
    },
    "NUTCH-1424": {
        "Key": "NUTCH-1424",
        "Summary": "fix fetcher timelimit logging",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "09/Jul/12 11:47",
        "Updated": "09/Jul/12 11:49",
        "Resolved": "09/Jul/12 11:49",
        "Description": "When fetching with timelimit, the log does not correctly reflect this. (Always shows -1).",
        "Issue Links": []
    },
    "NUTCH-1425": {
        "Key": "NUTCH-1425",
        "Summary": "DbUpdaterJob declares PREV_SIGNATURE on input twice",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "09/Jul/12 11:52",
        "Updated": "09/Jul/12 11:54",
        "Resolved": "09/Jul/12 11:54",
        "Description": "Although harmless, DbUpdaterJob should not declare input fields twice.",
        "Issue Links": []
    },
    "NUTCH-1426": {
        "Key": "NUTCH-1426",
        "Summary": "HostDb close() should close store instead of flush",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "09/Jul/12 12:15",
        "Updated": "09/Jul/12 12:17",
        "Resolved": "09/Jul/12 12:17",
        "Description": "The DataStore of HostDb should be closed instead of flushed when closing.",
        "Issue Links": []
    },
    "NUTCH-1427": {
        "Key": "NUTCH-1427",
        "Summary": "Reuse SelectorEntry in Generator.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "10/Jul/12 14:22",
        "Updated": "10/Jul/12 14:27",
        "Resolved": "10/Jul/12 14:27",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1428": {
        "Key": "NUTCH-1428",
        "Summary": "GeneratorMapper should not initialize filters/normalizers when they are disabled",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "10/Jul/12 15:00",
        "Updated": "10/Jul/12 15:02",
        "Resolved": "10/Jul/12 15:02",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1429": {
        "Key": "NUTCH-1429",
        "Summary": "CrawlDBReader to dump on exception and HTTP code",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "10/Jul/12 21:23",
        "Updated": "08/Jan/23 19:55",
        "Resolved": "08/Jan/23 19:54",
        "Description": "The CrawlDBReader tool can dump based on status and URL regex but not on status db_gone combined with an HTTP exception and HTTP response code.",
        "Issue Links": [
            "/jira/browse/NUTCH-1980"
        ]
    },
    "NUTCH-1430": {
        "Key": "NUTCH-1430",
        "Summary": "Freegenerator records overwrite CrawlDB records with AdaptiveFetchSchedule",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5",
        "Fix Version/s": "1.8",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Jul/12 12:02",
        "Updated": "13/Jun/13 13:11",
        "Resolved": "13/Jun/13 12:10",
        "Description": "Steps to reproduce:\nWithout AdaptiveFetchSchedule:\n\n$ bin/nutch readdb crawl/crawldb/ -url http://www.openindex.io/en/home.html\nURL: http://www.openindex.io/en/home.html\nVersion: 7\nStatus: 2 (db_fetched)\nFetch time: Thu Aug 16 13:58:23 CEST 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 2592000 seconds (30 days)\nScore: 0.0\nSignature: c2601ca503f2fc5edcb286501d7fb271\nMetadata: Content-Type: text/html_pst_: success(1), lastModified=0\n\n\nWith AdaptiveFetchSchedule:\n\n$ bin/nutch readdb crawl/crawldb/ -url http://www.openindex.io/en/home.html\nURL: http://www.openindex.io/en/home.html\nVersion: 7\nStatus: 2 (db_fetched)\nFetch time: Tue Jul 17 13:56:33 CEST 2012\nModified time: Tue Jul 17 13:55:33 CEST 2012\nRetries since fetch: 0\nRetry interval: 60 seconds (0 days)\nScore: 0.0\nSignature: 23567bb52ee8b905b8649c4305ed82ee\nMetadata: Content-Type: text/html_pst_: success(1), lastModified=0",
        "Issue Links": []
    },
    "NUTCH-1431": {
        "Key": "NUTCH-1431",
        "Summary": "Introduce link 'distance' and add configurable max distance in the generator",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "18/Jul/12 10:18",
        "Updated": "01/Sep/12 04:16",
        "Resolved": "31/Aug/12 15:57",
        "Description": "Introducing a new feature that enables to crawl URLs within a specific distance (shortest path) from the injected source urls. This is where the db-updater of Nutchgora really shines. Because every url in the reducer has all of its inlinks present, it is really easy to determine what the shortest path is to that url. (I would not know how to cleanly implement this feature for trunk).\nInjected urls have distance 0. Outlink urls on those pages have distance 1. Outlinks on those pages have distance 2, etc. Outlinks that already had a smaller distance will keep that distance. Of all inlinks to a page, it will always select the smallest distance in order to maintain the shortest path garantuee.\nGenerator now has a property 'generate.max.distance' (default set to -1) that specifies the maximum allowed distance of urls to select for fetch.\nNote that this is fundamentally different from the concept crawl 'depth'. Depth is used for crawl cycles. Distance allows to crawl for unlimited number of cycles AND always stay within a certain number of 'hops' from injected urls.\nI will attach a patch. Will commit in a few days. (It does not change crawl behaviour unless otherwise configured). Let me know if you have comments.",
        "Issue Links": []
    },
    "NUTCH-1432": {
        "Key": "NUTCH-1432",
        "Summary": "property storage.schema does not work anymore, should be storage.schema.webpage and storage.schema.host",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "19/Jul/12 07:35",
        "Updated": "22/May/13 03:53",
        "Resolved": "18/Sep/12 20:31",
        "Description": "Since the addition of the host table, the property storage.schema in nutch-default.xml does not work anymore. It should be storage.schema.webpage and storage.schema.host. Thanks Tianwei Sheng for reporting.",
        "Issue Links": []
    },
    "NUTCH-1433": {
        "Key": "NUTCH-1433",
        "Summary": "Upgrade to Tika 1.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "19/Jul/12 12:43",
        "Updated": "22/May/13 03:53",
        "Resolved": "20/Oct/12 09:16",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1434": {
        "Key": "NUTCH-1434",
        "Summary": "Indexer to delete robots noIndex",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.6",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Jul/12 11:35",
        "Updated": "22/May/13 03:53",
        "Resolved": "23/Aug/12 07:43",
        "Description": "Nutch does not treat pages with meta robots=\"noindex\" properly. All it does is remove the title and content fields from the parsed data. It does not stop those pages from being indexed, nor can it delete existing pages from the index if they change.",
        "Issue Links": [
            "/jira/browse/NUTCH-1553"
        ]
    },
    "NUTCH-1435": {
        "Key": "NUTCH-1435",
        "Summary": "Host jobs throw NullPointerException with MySQL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.1",
        "Component/s": "injector",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Jul/12 12:44",
        "Updated": "22/May/13 03:53",
        "Resolved": "23/Jul/12 11:12",
        "Description": "As described by Joan Espasa Arxer on user@ [0] the following NPE is thrown when we attempt to use the hostinject, updatehostdb and readhostdb jobs.\n\nHostInjectorJob: org.apache.gora.util.GoraException:\njava.lang.NullPointerException\nat\norg.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n at\norg.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\nat\norg.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:69)\n at org.apache.nutch.host.HostInjectorJob.inject(HostInjectorJob.java:146)\nat org.apache.nutch.host.HostInjectorJob.run(HostInjectorJob.java:160)\n at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\nat org.apache.nutch.host.HostInjectorJob.main(HostInjectorJob.java:174)\nCaused by: java.lang.NullPointerException\nat org.apache.gora.sql.store.SqlStore.addColumn(SqlStore.java:790)\nat org.apache.gora.sql.store.SqlStore.createSqlTable(SqlStore.java:802)\n at org.apache.gora.sql.store.SqlStore.initialize(SqlStore.java:165)\nat\norg.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n at\norg.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n... 6 more\n\n\nFerdy proposed a fix for this which I will patch against 2.X branch and duly attach. \n[0] http://www.mail-archive.com/user%40nutch.apache.org/msg06998.html",
        "Issue Links": [
            "/jira/browse/GORA-158"
        ]
    },
    "NUTCH-1436": {
        "Key": "NUTCH-1436",
        "Summary": "bin/nutch absent in zip package",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "23/Jul/12 19:58",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Jan/13 17:48",
        "Description": "The script bin/nutch is absent in the package apache-nutch-1.5.1-bin.zip,\nthe tar-bin package is not affected.",
        "Issue Links": [
            "/jira/browse/NUTCH-1415"
        ]
    },
    "NUTCH-1437": {
        "Key": "NUTCH-1437",
        "Summary": "HostInjectorJob to accept lines with or without protocol",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "25/Jul/12 12:50",
        "Updated": "25/Jul/12 12:52",
        "Resolved": "25/Jul/12 12:52",
        "Description": "The javadoc states the line MUST contain the protocol but in reality it just messes up the hosttable by putting it in the rowkey name. Simple patch fixes this to accept either with or without protocol and always store it properly.",
        "Issue Links": []
    },
    "NUTCH-1438": {
        "Key": "NUTCH-1438",
        "Summary": "ParserJob support for option -reparse",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "26/Jul/12 12:03",
        "Updated": "26/Jul/12 12:08",
        "Resolved": "26/Jul/12 12:08",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1439": {
        "Key": "NUTCH-1439",
        "Summary": "Define boost field as type float in schema-solr4.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.6",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Jul/12 19:18",
        "Updated": "22/May/13 03:53",
        "Resolved": "26/Jul/12 19:21",
        "Description": "As highlighted by shekhar sharma on user@ [0] there is an incorrectly defined field in the above file resulting in a ClassCastException when undertaking Solr tasks. The current and proposed solution is provided below\nCurrent:\n\n<field name=\"boost\" type=\"string\" stored=\"true\" indexed=\"false\"/>\n\n\nProposed:\n\n<field name=\"boost\" type=\"float\" stored=\"true\" indexed=\"false\"/>\n\n\n[0] http://www.mail-archive.com/user%40nutch.apache.org/msg07036.html",
        "Issue Links": []
    },
    "NUTCH-1440": {
        "Key": "NUTCH-1440",
        "Summary": "reconfigure non-existent stopwords_en.txt in schema-solr4.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.6,                                            2.1",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Jul/12 11:36",
        "Updated": "28/Jul/12 04:21",
        "Resolved": "27/Jul/12 11:56",
        "Description": "Also highlighted by shekhar sharma on user@ is the presence of the non-existent soptwords_en.txt. All occurrences should simply be changed to stopwords.txt to accommodate default settings.",
        "Issue Links": []
    },
    "NUTCH-1441": {
        "Key": "NUTCH-1441",
        "Summary": "AnchorIndexingFilter should use plain HashSet",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.6,                                            2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "30/Jul/12 12:19",
        "Updated": "22/May/13 03:54",
        "Resolved": "18/Sep/12 20:21",
        "Description": "AnchorIndexingFilter should use a plain HashSet, instead of WeakHashMap. WeakHashMap is unnecessary and can perhaps even cause bugs. (A WeakHashMap get its entries removed when the gc notices the keys are not elsewhere in use.)\nThis patch also makes the filter a bit faster by lazy instantiating the set. (No need to create one everytime when deduplication is off).",
        "Issue Links": []
    },
    "NUTCH-1442": {
        "Key": "NUTCH-1442",
        "Summary": "indexingfilter.order is property is misread in code",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Ferdy",
        "Created": "30/Jul/12 12:40",
        "Updated": "14/Aug/12 09:28",
        "Resolved": "13/Aug/12 20:40",
        "Description": "The property for indexfilters order is defined as indexingfilter.order in nutch-default, however the code in IndexingFilters tries to read it as indexingfilterhbase.order. Clearly this is an artifact from the past and should be fixed asap.",
        "Issue Links": [
            "/jira/browse/NUTCH-1453"
        ]
    },
    "NUTCH-1443": {
        "Key": "NUTCH-1443",
        "Summary": "Solr schema version is invalid",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.6,                                            2.1",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/Jul/12 21:19",
        "Updated": "22/May/13 03:53",
        "Resolved": "31/Jul/12 21:26",
        "Description": "Solr's schema.xml shipped with Nutch 1.5.x and in trunk have an invalid Solr schema version. The schema version is dictated by Solr, not by Nutch' version. The current Solr schema version is 1.5.",
        "Issue Links": []
    },
    "NUTCH-1444": {
        "Key": "NUTCH-1444",
        "Summary": "Indexing should not create temporary files (do not extend from FileOutputFormat)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "01/Aug/12 13:48",
        "Updated": "07/Aug/12 07:09",
        "Resolved": "01/Aug/12 14:21",
        "Description": "The creation of the tmp files is a thing from the past, where it was needed to create Lucene indices. For the the SolrIndexer this is not needed anymore. I have changed the indexer to not extend from FileOutputFormat. This greatly simplifies the code. (And makes room for ElasticIndexerJob which I am about to add to the codebase)",
        "Issue Links": []
    },
    "NUTCH-1445": {
        "Key": "NUTCH-1445",
        "Summary": "Add ElasticIndexerJob that indexes to elasticsearch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "01/Aug/12 14:12",
        "Updated": "31/Aug/12 12:41",
        "Resolved": "03/Aug/12 15:08",
        "Description": "We have created a new indexer job ElasticIndexerJob that indexes to elasticsearch. It is orginally based upon https://github.com/ctjmorgan/nutch-elasticsearch-indexer (Apache2 license), but we have modified it greatly to make it integrate as good as possible into Nutch. The greatest modification is that documents are asynchronously flushed in bulk to elasticsearch.\nElasticsearch rocks. Both performance and ease of confiugration is awesome. You simply deploy a server by unpacking the tar, configure the clustername, start the server and fire away indexing requests. Indices are automatically created. Fields are automapped. (Of course it is recommended to create your own optimized mapping, but that is beyond scope of this issue). Multiple servers connect without extra configuration, simply by using the same clustername. (By means of multicast). There a tons of advanced options, such as sharding, replication, disk striping etc.\nTo give an example of the performance: With 20+ nodes we are able to index over 1M docs (average sized webdocuments) per minute. The best part is that the added documents are almost instantly searchable, so there no hidden commit costs that Solr has. This is with out-of-the-box configuration.\n(I will attach patch and commit for Nutch2. Feel free to adapt for trunk.)",
        "Issue Links": [
            "/jira/browse/NUTCH-1462"
        ]
    },
    "NUTCH-1446": {
        "Key": "NUTCH-1446",
        "Summary": "Port NUTCH-1444 to trunk (Indexing should not create temporary files)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "06/Aug/12 14:48",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1047"
        ]
    },
    "NUTCH-1447": {
        "Key": "NUTCH-1447",
        "Summary": "Nutch 2.x with Cloudera CDH 4 get Error: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tr\u1ea7n Anh Tu\u1ea5n",
        "Created": "08/Aug/12 13:00",
        "Updated": "22/May/13 03:53",
        "Resolved": "25/Apr/13 00:46",
        "Description": "I'm trying to crawl using Nutch 2. \nI check out source from \nhttp://svn.apache.org/repos/asf/nutch/branches/2.x/ and config with \nmysql. \nI get error but when run nutch 1.5 everything okie  \nmkdir urls \necho nutch.apache.org > urls/seed.txt \nruntime/deploy/bin/nutch inject urls \n12/08/07 11:25:38 INFO crawl.InjectorJob: InjectorJob: starting \n12/08/07 11:25:38 INFO crawl.InjectorJob: InjectorJob: urlDir: urls \n12/08/07 11:25:41 WARN mapred.JobClient: Use GenericOptionsParser for \nparsing the arguments. Applications should implement Tool for the \nsame. \n12/08/07 11:25:44 INFO input.FileInputFormat: Total input paths to process : 1 \n12/08/07 11:25:45 INFO util.NativeCodeLoader: Loaded the native-hadoop library \n12/08/07 11:25:45 WARN snappy.LoadSnappy: Snappy native library is available \n12/08/07 11:25:45 INFO snappy.LoadSnappy: Snappy native \n12/08/07 11:25:47 INFO mapred.JobClient:  map 0% reduce 0% \n12/08/07 11:26:01 INFO mapred.JobClient: Task Id : \nattempt_201208071123_0001_m_000000_0, Status : FAILED \nError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, \nbut class was expected \nattempt_201208071123_0001_m_000000_0: SLF4J: Class path contains \nmultiple SLF4J bindings. \nattempt_201208071123_0001_m_000000_0: SLF4J: Found binding in \n[jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] \nattempt_201208071123_0001_m_000000_0: SLF4J: Found binding in \n[jar:file:/var/lib/hadoop-hdfs/cache/mapred/mapred/local/taskTracker/root/jobcache/job_201208071123_0001/jars/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] \nattempt_201208071123_0001_m_000000_0: SLF4J: See \nhttp://www.slf4j.org/codes.html#multiple_bindings for an explanation. \n12/08/07 11:26:05 INFO mapred.JobClient: Task Id : \nattempt_201208071123_0001_m_000000_1, Status : FAILED \nError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, \nbut class was expected \nattempt_201208071123_0001_m_000000_1: SLF4J: Class path contains \nmultiple SLF4J bindings. \nattempt_201208071123_0001_m_000000_1: SLF4J: Found binding in \n[jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] \nattempt_201208071123_0001_m_000000_1: SLF4J: Found binding in \n[jar:file:/var/lib/hadoop-hdfs/cache/mapred/mapred/local/taskTracker/root/jobcache/job_201208071123_0001/jars/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] \nattempt_201208071123_0001_m_000000_1: SLF4J: See \nhttp://www.slf4j.org/codes.html#multiple_bindings for an explanation. \n12/08/07 11:26:10 INFO mapred.JobClient: Task Id : \nattempt_201208071123_0001_m_000000_2, Status : FAILED \nError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, \nbut class was expected \nattempt_201208071123_0001_m_000000_2: SLF4J: Class path contains \nmultiple SLF4J bindings. \nattempt_201208071123_0001_m_000000_2: SLF4J: Found binding in \n[jar:file:/usr/lib/zookeeper/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] \nattempt_201208071123_0001_m_000000_2: SLF4J: Found binding in \n[jar:file:/var/lib/hadoop-hdfs/cache/mapred/mapred/local/taskTracker/root/jobcache/job_201208071123_0001/jars/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class] \nattempt_201208071123_0001_m_000000_2: SLF4J: See \nhttp://www.slf4j.org/codes.html#multiple_bindings for an explanation. \n12/08/07 11:26:19 INFO mapred.JobClient: Job complete: job_201208071123_0001 \n12/08/07 11:26:19 INFO mapred.JobClient: Counters: 7 \n12/08/07 11:26:19 INFO mapred.JobClient:   Job Counters \n12/08/07 11:26:19 INFO mapred.JobClient:     Failed map tasks=1 \n12/08/07 11:26:19 INFO mapred.JobClient:     Launched map tasks=4 \n12/08/07 11:26:19 INFO mapred.JobClient:     Data-local map tasks=4 \n12/08/07 11:26:19 INFO mapred.JobClient:     Total time spent by all \nmaps in occupied slots (ms)=18003 \n12/08/07 11:26:19 INFO mapred.JobClient:     Total time spent by all \nreduces in occupied slots (ms)=0 \n12/08/07 11:26:19 INFO mapred.JobClient:     Total time spent by all \nmaps waiting after reserving slots (ms)=0 \n12/08/07 11:26:19 INFO mapred.JobClient:     Total time spent by all \nreduces waiting after reserving slots (ms)=0 \n12/08/07 11:26:19 ERROR crawl.InjectorJob: InjectorJob: \njava.lang.RuntimeException: job failed: name=inject-p1 urls, \njobid=job_201208071123_0001 \n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:47) \n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:248) \n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:268) \n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:288) \n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70) \n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:298) \n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) \n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) \n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) \n        at java.lang.reflect.Method.invoke(Method.java:597) \n        at org.apache.hadoop.util.RunJar.main(RunJar.java:208) \nAnd log\nException in thread \"main\" java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.CounterGroup, but class was expected\n\tat org.apache.nutch.util.ToolUtil.recordJobStatus(ToolUtil.java:59)\n\tat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:249)\n\tat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:268)\n\tat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:288)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:298)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:208)\nThanks",
        "Issue Links": []
    },
    "NUTCH-1448": {
        "Key": "NUTCH-1448",
        "Summary": "Redirected urls should be handled more cleanly (more like an outlink url)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "13/Aug/12 08:46",
        "Updated": "01/Sep/12 04:16",
        "Resolved": "31/Aug/12 13:03",
        "Description": "This is specifically for Nutch2.x. Handling a redirects url like an outlink is much more cleaner because this makes it more simple to trace how new urls are added to the webpage database. Instant fetching of redirects won't work, but this is a small price to pay. (Note that this currently does not work at all, because the http.max.redirect property has no effect). Will be attaching a patch in the upcoming days.",
        "Issue Links": [
            "/jira/browse/NUTCH-1461"
        ]
    },
    "NUTCH-1449": {
        "Key": "NUTCH-1449",
        "Summary": "Optionally delete documents skipped by IndexingFilters",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "13/Aug/12 14:48",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "08/Jan/16 11:10",
        "Description": "Add configuration option to delete documents instead of skipping them if the indexing filters return null. This is useful to delete documents with new business logic in the indexing filter chain.",
        "Issue Links": []
    },
    "NUTCH-1450": {
        "Key": "NUTCH-1450",
        "Summary": "Upgrade to gora deps to 0.2.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.1",
        "Component/s": "build,                                            storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Aug/12 17:18",
        "Updated": "22/May/13 03:53",
        "Resolved": "13/Aug/12 17:32",
        "Description": "Since we released Gora 0.2.1 we should upgrade over here.",
        "Issue Links": [
            "/jira/browse/NUTCH-1365"
        ]
    },
    "NUTCH-1451": {
        "Key": "NUTCH-1451",
        "Summary": "Upgrade automaton jar to 1.11-8",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Aug/12 17:26",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Nov/12 13:58",
        "Description": "The latest version 1.11-8 was released September 7, 2011.\nThis library is significantly faster than the default regex parsing. I haven't got a clue what version we currently use but the license states 2005 so I'm guessing its been a long time since it was upgraded.\nI'll get a patch together and for completeness run independent test to compare results pre and post upgrade. It would be nice to see > marginal improvements :0)",
        "Issue Links": []
    },
    "NUTCH-1452": {
        "Key": "NUTCH-1452",
        "Summary": "hadoop.job.history.user.location in nutch-default making job history useless",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "14/Aug/12 08:30",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "There is still a property in nutch-default 'hadoop.job.history.user.location' that redirects the creation of history files from job output locations to a custom location. I noticed that the current value does not work well with cloudera (I have tested cdh3u4), because ${hadoop.log.dir} is not defined. This actually causes the job in the jobtracker to show empty info. (With 'incomplete' job status). This is only when the job moves to retired. When it is still in 'completed', all is looking well.\nThis property can be set to 'none', because the job history is ALSO stored in the central jobtracker location anyway. The 'hadoop.job.history.user.location' property specifies an extra location. But if it is set to an invalid value, it causes the central history location to NOT store it, so it seems. Please see for more details:\nhttp://hadoop.apache.org/common/docs/r1.0.3/cluster_setup.html\nBesides setting it to 'none', another option is to set it to 'history' which does work with cdh. (This writes all logs to 'history' in the user directory in the configured filesystem, usually dfs). The final option is to simply remove this value and not meddle with hadoop properties at all. But that actually requires all jobs to correctly ignore these files. I am not up to date how well this currently works with Nutch jobs. This question is most relevant for trunk, since trunk heavily relies on the filesystem for jobs.\nWhat do you think?\nA) Set property to 'none'\nB) Set property to 'history'\nC) Remove property, see what happens, possibly fix jobs\nD) ?\nFor now, I opt for A. But I think we need some more input with other distributions (for example official Hadoop 1.x) and also Nutch trunk.",
        "Issue Links": []
    },
    "NUTCH-1453": {
        "Key": "NUTCH-1453",
        "Summary": "Substantiate tests for IndexingFilters",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "indexer",
        "Assignee": "lufeng",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Aug/12 09:27",
        "Updated": "22/May/13 03:53",
        "Resolved": "18/Jan/13 21:08",
        "Description": "This issue is a follow up from the issues discussed in NUTCH-1442 where it was agreed that the current test is o.a.n.indexer.TestIndexingFilters is simply not doing us an justice.\nThere are some slight differences between trunk and 2.x but they both share the common problem that there needs to be more thorough testing undertaken.",
        "Issue Links": [
            "/jira/browse/NUTCH-1442"
        ]
    },
    "NUTCH-1454": {
        "Key": "NUTCH-1454",
        "Summary": "parsing chm failed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.9",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "14/Aug/12 20:11",
        "Updated": "13/Apr/14 16:26",
        "Resolved": "13/Apr/14 16:26",
        "Description": "(reported by Jan Riewe, see http://lucene.472066.n3.nabble.com/CHM-Files-and-Tika-td3999735.html)\nNutch fails to parse chm files with\n\n ERROR tika.TikaParser - Can't retrieve Tika parser for mime-type application/vnd.ms-htmlhelp\nTested with chm test files from Tika:\n\n % bin/nutch parsechecker file:/.../tika/trunk/tika-parsers/src/test/resources/test-documents/testChm.chm\n\n\nTika parses this document (but does not extract any content).",
        "Issue Links": [
            "/jira/browse/TIKA-1122"
        ]
    },
    "NUTCH-1455": {
        "Key": "NUTCH-1455",
        "Summary": "RobotRulesParser to match multi-word user-agent names",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "protocol",
        "Assignee": "Tejas Patil",
        "Reporter": "Sebastian Nagel",
        "Created": "14/Aug/12 22:02",
        "Updated": "29/Apr/13 20:32",
        "Resolved": "29/Apr/13 20:32",
        "Description": "If the user-agent name(s) configured in http.robots.agents contains spaces it is not matched even if is exactly contained in the robots.txt\nhttp.robots.agents = \"Download Ninja,*\"\nIf the robots.txt (http://en.wikipedia.org/robots.txt) contains\n\nUser-agent: Download Ninja\nDisallow: /\n\n\nall content should be forbidden. But it isn't:\n\n% curl 'http://en.wikipedia.org/robots.txt' > robots.txt\n% grep -A1 -i ninja robots.txt \nUser-agent: Download Ninja\nDisallow: /\n% cat test.urls\nhttp://en.wikipedia.org/\n% bin/nutch plugin lib-http org.apache.nutch.protocol.http.api.RobotRulesParser robots.txt test.urls 'Download Ninja'\n...\nallowed:        http://en.wikipedia.org/\n\n\nThe rfc (http://www.robotstxt.org/norobots-rfc.txt) states that\nThe robot must obey the first record in /robots.txt that contains a User-Agent line whose value contains the name token of the robot as a    substring.\nAssumed that \"Downlaod Ninja\" is a substring of itself it should match and http://en.wikipedia.org/ should be forbidden.\nThe point is that the agent name from the User-Agent line is split at spaces while the names from the http.robots.agents property are not (they are only split at \",\").",
        "Issue Links": [
            "/jira/browse/NUTCH-1031"
        ]
    },
    "NUTCH-1456": {
        "Key": "NUTCH-1456",
        "Summary": "Updater not setting batchId in markers correctly.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "15/Aug/12 12:10",
        "Updated": "07/Sep/12 14:21",
        "Resolved": "07/Sep/12 14:21",
        "Description": "The db updater job is not setting batchId in markers correctly. (Noticed thanks to various reporters on mailing list.)",
        "Issue Links": []
    },
    "NUTCH-1457": {
        "Key": "NUTCH-1457",
        "Summary": "Nutch2 Refactor the update process so that fetched items are only processed once",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "15/Aug/12 12:18",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1475"
        ]
    },
    "NUTCH-1458": {
        "Key": "NUTCH-1458",
        "Summary": "Support for raw HTML field added to Solr",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "None",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Max Dzyuba",
        "Created": "16/Aug/12 08:44",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "11/Jun/14 15:24",
        "Description": "At the moment, the \u201ccontent\u201d field holds only the parsed text from the page. It would be nice to have a separate field in Solr document that would hold raw HTML from the crawled page.",
        "Issue Links": [
            "/jira/browse/NUTCH-1785"
        ]
    },
    "NUTCH-1459": {
        "Key": "NUTCH-1459",
        "Summary": "Remove dead code (phase2) from InjectorJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "17/Aug/12 09:14",
        "Updated": "07/Sep/12 10:41",
        "Resolved": "07/Sep/12 08:18",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1460": {
        "Key": "NUTCH-1460",
        "Summary": "407 Proxy Failure",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Jay",
        "Created": "21/Aug/12 15:10",
        "Updated": "21/Aug/12 19:56",
        "Resolved": "21/Aug/12 19:56",
        "Description": "i am getting error while crawling an internet site with Nutch 1.5.1. i have specified all proxy details in nutch-site.xml file. when i run nutch crawler, i am seeing the following error message in logs.\n\"fetch of http://www.google.com/ failed with: Http code=407, url=http://www.google.com/\"",
        "Issue Links": []
    },
    "NUTCH-1461": {
        "Key": "NUTCH-1461",
        "Summary": "Problem with TableUtil",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Christian Johnsson",
        "Created": "28/Aug/12 00:53",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Affects parse and updatedb and parse.\nThink i got some missformated urls into hbase but i can't fin them.\nIt generates this error though. If i empty hbase and restart it goes for a couple of million pages indexed then it comes up again. Any tips on how to locate what row in the table that genereates this error?\n2012-08-28 01:48:10,871 WARN org.apache.hadoop.mapred.Child: Error running child\njava.lang.ArrayIndexOutOfBoundsException: 1\n\tat org.apache.nutch.util.TableUtil.unreverseUrl(TableUtil.java:98)\n\tat org.apache.nutch.parse.ParserJob$ParserMapper.map(ParserJob.java:102)\n\tat org.apache.nutch.parse.ParserJob$ParserMapper.map(ParserJob.java:76)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:647)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:323)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:266)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:260)\n2012-08-28 01:48:10,875 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task",
        "Issue Links": [
            "/jira/browse/NUTCH-1448"
        ]
    },
    "NUTCH-1462": {
        "Key": "NUTCH-1462",
        "Summary": "Elasticsearch not indexing when type==null in NutchDocument metadata",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "31/Aug/12 12:35",
        "Updated": "01/Sep/12 04:16",
        "Resolved": "31/Aug/12 12:49",
        "Description": "When the NutchDocument does not contain a defined type, the Elasticsearch indexer fails with \"type is missing\" messages. Thanks Matt MacDonald for reporting this.\nPatch will be right up.",
        "Issue Links": [
            "/jira/browse/NUTCH-1445"
        ]
    },
    "NUTCH-1463": {
        "Key": "NUTCH-1463",
        "Summary": "Elasticsearch indexer should wait and check response for last flush",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ferdy",
        "Created": "31/Aug/12 12:54",
        "Updated": "01/Sep/12 04:16",
        "Resolved": "31/Aug/12 12:57",
        "Description": "I noticed that the closing of the Elasticsearch does not correctly wait for the last flush to finish. Although this probably has no effect in general, for conciseness it should await and check the response for the last flush too.",
        "Issue Links": []
    },
    "NUTCH-1464": {
        "Key": "NUTCH-1464",
        "Summary": "index-static plugin doesn't allow the colon within the field value",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.5",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Luca Cavanna",
        "Created": "31/Aug/12 13:49",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "If I want to configure a static field with a value containing a colon, the index-static plugin does nothing. There's a string split based on the colon character and if the result is an array of length 2 everything is fine, otherwise nothing happens, the static field is not set.",
        "Issue Links": [
            "/jira/browse/NUTCH-1580",
            "/jira/browse/NUTCH-2052"
        ]
    },
    "NUTCH-1465": {
        "Key": "NUTCH-1465",
        "Summary": "Support sitemaps in Nutch",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Lewis John McGibbney",
        "Created": "04/Sep/12 13:57",
        "Updated": "12/Mar/18 08:29",
        "Resolved": "19/Jul/17 11:25",
        "Description": "I recently came across this rather stagnant codebase[0] which is ASL v2.0 licensed and appears to have been used successfully to parse sitemaps as per the discussion here[1].\n[0] http://sourceforge.net/projects/sitemap-parser/\n[1] http://lucene.472066.n3.nabble.com/Support-for-Sitemap-Protocol-and-Canonical-URLs-td630060.html",
        "Issue Links": [
            "/jira/browse/NUTCH-1622",
            "/jira/browse/NUTCH-1741",
            "https://github.com/apache/nutch/pull/189"
        ]
    },
    "NUTCH-1466": {
        "Key": "NUTCH-1466",
        "Summary": "SolrDeleteDuplicates: java.io.IOException: Job failed!",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Philippe",
        "Created": "06/Sep/12 09:17",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Sep/12 11:05",
        "Description": "Hi there,\ni'm facing a problem witch  just comes up by a specific site. When i try to crawl \"http://www.stimme.de/sport/\", the crawl process works fine. When it comes to the SolrDeleteDuplicates part it throws the following error.\nERROR:\nSolrIndexer: starting at 2012-09-06 09:40:37\nSolrIndexer: deleting gone documents: false\nSolrIndexer: URL filtering: false\nSolrIndexer: URL normalizing: false\njava.io.IOException: Job failed!\nSolrDeleteDuplicates: starting at 2012-09-06 09:41:20\nSolrDeleteDuplicates: Solr url: http://192.168.1.144:8983/solr/\nException in thread \"main\" java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1265)\n\tat org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:373)\n\tat org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:353)\n\tat org.apache.nutch.crawl.Crawl.run(Crawl.java:193)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.Crawl.main(Crawl.java:63)\nSince i read this thread about the similar problem http://lucene.472066.n3.nabble.com/SolrIndex-java-io-IOException-Job-failed-td3585509.html i rebuild with solr-solj-3.5 but still the same error. \nAny ideas how to fix that?\nI'm using solr-3.4 as Solr-Server.",
        "Issue Links": []
    },
    "NUTCH-1467": {
        "Key": "NUTCH-1467",
        "Summary": "nutch 1.5.1 not able to parse mutliValued metatags",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.9",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Kiran",
        "Created": "06/Sep/12 11:41",
        "Updated": "18/Jun/13 13:25",
        "Resolved": "13/Jun/13 20:51",
        "Description": "Hi,\nI have been able to parse metatags in an html page using http://wiki.apache.org/nutch/IndexMetatags. It does not work quite well when there are two metatags with same name but two different contents. \nDoes anyone encounter this kind of issue ?  \nAre there any changes that need to be made to the config files to make it work ?\nWhen there are two tags with same name and different content, it takes the value of the later tag and saves it rather than creating a multiValue field.\nEdit: I have attached the patch for the file and it is provided by DLA (Digital Library and Archives) http://scholar.lib.vt.edu/ of Virginia Tech. \nMany Thanks,",
        "Issue Links": [
            "/jira/browse/NUTCH-1583"
        ]
    },
    "NUTCH-1468": {
        "Key": "NUTCH-1468",
        "Summary": "Redirects that are external links not adhering to db.ignore.external.links",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.1",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Matt MacDonald",
        "Created": "09/Sep/12 11:35",
        "Updated": "05/Sep/14 05:58",
        "Resolved": "17/Sep/12 09:27",
        "Description": "Patch attached for this.\nHi,\nLikely this is a question for Ferdy but if anyone else has input\nthat'd be great. When running a crawl that I would expect to be\ncontained to a single domain I'm seeing the crawler jump out to other\ndomains. I'm using the trunk of Nutch 2.x which includes the following\ncommit: https://github.com/apache/nutch/commit/c5e2236f36a881ee7fec97aff3baf9bb32b40200\nThe goal is to perform a focused crawl against a single domain and\nrestrict the crawler from expanding beyond that domain. I've set the\ndb.ignore.external.links property to true. I do not want to add a\nregex to regex-urlfilter.txt as I will be adding several thousand\nurls. The domain that I am crawling has documents with outlinks that\nare still within the domain but then redirect to external domains.\ncat urls/seed.txt\nhttp://www.ci.watertown.ma.us/\ncat conf/nutch-site.xml\n...\n  <property>\n    <name>db.ignore.external.links</name>\n    <value>true</value>\n    <description>If true, outlinks leading from a page to external hosts\n    will be ignored. This is an effective way to limit the crawl to include\n    only initially injected hosts, without creating complex URLFilters.\n    </description>\n  </property>\n  <property>\n    <name>plugin.includes</name>\n   <value>protocol-http|urlfilter-regex|parse-(html|tika)|index-(basic|anchor)|urlnormalizer-(pass|regex|basic)|scoring-opic</value>\n   <description>Regular expression naming plugin directory names to\n    include.  Any plugin not matching this expression is excluded.\n    In any case you need at least include the nutch-extensionpoints plugin. By\n    default Nutch includes crawling just HTML and plain text via HTTP,\n    and basic indexing and search plugins. In order to use HTTPS please enable\n    protocol-httpclient, but be aware of possible intermittent\nproblems with the\n    underlying commons-httpclient library.\n    </description>\n  </property>\n...\nRunning\nbin/nutch crawl urls -depth 8 -topN 100000\nresults in the the crawl eventually fetching and parsing documents on\ndomains external to the only link in the seed.txt file.\nI would not expect to see urls like the following in my logs and in\nthe HBase webpage table:\nfetching http://www.masshome.com/tourism.html\nParsing http://www.disabilityinfo.org/\nI'm reviewing the code changes but am still getting up to speed on the\ncode base. Any ideas while I continue to dig around? Configuration\nissue or code?\nThanks,\nMatt",
        "Issue Links": []
    },
    "NUTCH-1469": {
        "Key": "NUTCH-1469",
        "Summary": "Upgrade commons-net dependency",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora,                                            1.5.1",
        "Fix Version/s": "2.5",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Sep/12 16:31",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Currently we are using the commons-net-1.2.0-dev artefact.\nThe most recent version on maven central is 3.1 [0]\n[0] http://search.maven.org/#artifactdetails|commons-net|commons-net|3.1|jar",
        "Issue Links": []
    },
    "NUTCH-1470": {
        "Key": "NUTCH-1470",
        "Summary": "Ensure test files are included for runtime testing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "nutchgora,                                            1.5.1",
        "Fix Version/s": "1.6,                                            2.1",
        "Component/s": "test",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Sep/12 22:46",
        "Updated": "22/May/13 03:54",
        "Resolved": "15/Sep/12 23:07",
        "Description": "As discussed h6ee [0], Vijith V highlighted the bug in both trunk and 2.X regarding test deps not being available to use @runtime.\nThis trivial issue fixes that.  \n[0] http://www.mail-archive.com/user%40nutch.apache.org/msg07486.html",
        "Issue Links": []
    },
    "NUTCH-1471": {
        "Key": "NUTCH-1471",
        "Summary": "make explicit which datastore urls are injected to",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "2.2",
        "Component/s": "injector",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Sep/12 20:40",
        "Updated": "22/May/13 03:53",
        "Resolved": "22/Nov/12 14:53",
        "Description": "This is simply from a logging perspective and helps when monitoring logs over a period of time and when using multiple Nutch bots. A simple info message stating which store urls are being injected to will suffice here.",
        "Issue Links": [
            "/jira/browse/NUTCH-1370"
        ]
    },
    "NUTCH-1472": {
        "Key": "NUTCH-1472",
        "Summary": "InvalidRequestException(why:(String didn't validate.) [webpage][f][ts] failed validation)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "ZhaiXuepan",
        "Created": "25/Sep/12 13:33",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "me.prettyprint.hector.api.exceptions.HInvalidRequestException: InvalidRequestException(why:(String didn't validate.) [webpage][f][ts] failed validation)\n\tat me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:45)\n\tat me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:264)\n\tat me.prettyprint.cassandra.model.ExecutingKeyspace.doExecuteOperation(ExecutingKeyspace.java:97)\n\tat me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:243)\n\tat me.prettyprint.cassandra.model.MutatorImpl.insert(MutatorImpl.java:69)\n\tat org.apache.gora.cassandra.store.HectorUtils.insertColumn(HectorUtils.java:47)\n\tat org.apache.gora.cassandra.store.CassandraClient.addColumn(CassandraClient.java:169)\n\tat org.apache.gora.cassandra.store.CassandraStore.addOrUpdateField(CassandraStore.java:341)\n\tat org.apache.gora.cassandra.store.CassandraStore.flush(CassandraStore.java:228)\n\tat org.apache.gora.cassandra.store.CassandraStore.close(CassandraStore.java:95)\n\tat org.apache.gora.mapreduce.GoraRecordWriter.close(GoraRecordWriter.java:55)\n\tat org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.close(MapTask.java:651)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)\nCaused by: InvalidRequestException(why:(String didn't validate.) [webpage][f][ts] failed validation)\n\tat org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:20253)\n\tat org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)\n\tat org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:922)\n\tat org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:908)\n\tat me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:246)\n\tat me.prettyprint.cassandra.model.MutatorImpl$3.execute(MutatorImpl.java:243)\n\tat me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:103)\n\tat me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:258)\n\t... 13 more",
        "Issue Links": []
    },
    "NUTCH-1473": {
        "Key": "NUTCH-1473",
        "Summary": "Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "ZhaiXuepan",
        "Created": "25/Sep/12 13:40",
        "Updated": "16/Apr/14 22:29",
        "Resolved": "16/Apr/14 22:29",
        "Description": "Exception in thread \"main\" org.apache.gora.util.GoraException: java.io.IOException: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n\tat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:167)\n\tat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:135)\n\tat org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:75)\n\tat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:214)\n\tat org.apache.nutch.crawl.Crawler.runTool(Crawler.java:62)\n\tat org.apache.nutch.crawl.Crawler.run(Crawler.java:133)\n\tat org.apache.nutch.crawl.Crawler.run(Crawler.java:246)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.Crawler.main(Crawler.java:253)\nCaused by: java.io.IOException: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n\tat org.apache.gora.sql.store.SqlStore.createSchema(SqlStore.java:226)\n\tat org.apache.gora.sql.store.SqlStore.initialize(SqlStore.java:172)\n\tat org.apache.gora.store.DataStoreFactory.initializeDataStore(DataStoreFactory.java:102)\n\tat org.apache.gora.store.DataStoreFactory.createDataStore(DataStoreFactory.java:161)\n\t... 8 more\nCaused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Column length too big for column 'text' (max = 21845); use BLOB or TEXT instead\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat com.mysql.jdbc.Util.handleNewInstance(Util.java:411)\n\tat com.mysql.jdbc.Util.getInstance(Util.java:386)\n\tat com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)\n\tat com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)\n\tat com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)\n\tat com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)\n\tat com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2625)\n\tat com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2119)\n\tat com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2415)\n\tat com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2333)\n\tat com.mysql.jdbc.PreparedStatement.executeUpdate(PreparedStatement.java:2318)\n\tat org.apache.gora.sql.store.SqlStore.createSchema(SqlStore.java:224)\n\t... 11 more",
        "Issue Links": [
            "/jira/browse/NUTCH-970"
        ]
    },
    "NUTCH-1474": {
        "Key": "NUTCH-1474",
        "Summary": "protocol-sftp needs configuration improvements",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "nutchgora",
        "Fix Version/s": "2.5",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Sep/12 16:40",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "This relates to a recent discussion on user@ [0] which brought about obvious improvements required in the plugin. Namely the Ivy configuration needs to fetch the JSch library and make it available to the CP.\nPatches welcome \n[0] http://www.mail-archive.com/user%40nutch.apache.org/msg07661.html",
        "Issue Links": []
    },
    "NUTCH-1475": {
        "Key": "NUTCH-1475",
        "Summary": "Index-More Plugin -- A better fall back value for date field",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1,                                            1.5.1",
        "Fix Version/s": "1.7,                                            2.2.1",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "James Sullivan",
        "Created": "07/Oct/12 01:49",
        "Updated": "11/Oct/19 15:36",
        "Resolved": "20/Jun/13 21:00",
        "Description": "Among other fields, the more plugin for Nutch 2.x provides a \"last modified\" and \"date\" field for the Solr index. The \"last modified\" field is the last modified date from the http headers if available, if not available it is left empty. Currently, the \"date\" field is the same as the \"last modified\" field unless that field is empty in which case getFetchTime is used as a fall back. I think getFetchTime is not a good fall back as it is the next fetch time and often a month or more in the future which doesn't make sense for the date field. Users do not expect webpages/documents with future dates. A more sensible fallback would be current date at the time it is indexed. \nThis is possible by simply changing line 97 of https://svn.apache.org/repos/asf/nutch/branches/2.x/src/plugin/index-more/src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java from\ntime = page.getFetchTime(); // use fetch time\nto\ntime = new Date().getTime();\nUsers interested in the getFetchTime value can still get it from the \"tstamp\" field.",
        "Issue Links": [
            "/jira/browse/NUTCH-1589",
            "/jira/browse/NUTCH-1457"
        ]
    },
    "NUTCH-1476": {
        "Key": "NUTCH-1476",
        "Summary": "SegmentReader getStats should set parsed = -1 if no parsing took place",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "08/Oct/12 22:06",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Oct/12 20:44",
        "Description": "The method getStats in SegmentReader sets the number of parsed documents (and also the number of parseErrors) to 0 if no parsing took place for a segment. The values should be set to -1 analogous to the number of fetched docs and fetchErrors.",
        "Issue Links": []
    },
    "NUTCH-1477": {
        "Key": "NUTCH-1477",
        "Summary": "NPE when injecting with DataFileAvroStore",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "Mike Baranczak",
        "Created": "11/Oct/12 01:11",
        "Updated": "03/Mar/22 14:27",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Fresh installation of Nutch 2.1, configured to use DataFileAvroStore. Injection job throws NullPointerException, see below. No error when I switch to MemStore.\njava.lang.NullPointerException\n\tat org.apache.avro.io.BinaryEncoder.writeString(BinaryEncoder.java:133)\n\tat org.apache.avro.generic.GenericDatumWriter.writeString(GenericDatumWriter.java:176)\n\tat org.apache.avro.generic.GenericDatumWriter.writeString(GenericDatumWriter.java:171)\n\tat org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:72)\n\tat org.apache.avro.generic.GenericDatumWriter.writeRecord(GenericDatumWriter.java:89)\n\tat org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:62)\n\tat org.apache.avro.generic.GenericDatumWriter.write(GenericDatumWriter.java:55)\n\tat org.apache.avro.file.DataFileWriter.append(DataFileWriter.java:245)\n\tat org.apache.gora.avro.store.DataFileAvroStore.put(DataFileAvroStore.java:54)\n\tat org.apache.gora.mapreduce.GoraRecordWriter.write(GoraRecordWriter.java:60)\n\tat org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:639)\n\tat org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n\tat org.apache.nutch.crawl.InjectorJob$UrlMapper.map(InjectorJob.java:185)\n\tat org.apache.nutch.crawl.InjectorJob$UrlMapper.map(InjectorJob.java:85)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)",
        "Issue Links": [
            "/jira/browse/GORA-174",
            "/jira/browse/GORA-183",
            "/jira/browse/NUTCH-842",
            "/jira/browse/GORA-174"
        ]
    },
    "NUTCH-1478": {
        "Key": "NUTCH-1478",
        "Summary": "Parse-metatags and index-metadata plugin for Nutch 2.x series",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.3",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Kiran",
        "Created": "18/Oct/12 21:09",
        "Updated": "20/Mar/14 01:19",
        "Resolved": "13/Mar/14 12:51",
        "Description": "I have ported parse-metatags and index-metadata plugin to Nutch 2.x series.  This will take multiple values of same tag and index in Solr as i patched before (https://issues.apache.org/jira/browse/NUTCH-1467).\nThe usage is same as described here (http://wiki.apache.org/nutch/IndexMetatags) but one change is that there is no need to give 'metatag' keyword before metatag names. For example my configuration looks like this (https://github.com/salvager/NutchDev/blob/master/runtime/local/conf/nutch-site.xml) \nThis is only the first version and does not include the junit test. I will update the new version soon.\nThis will parse the tags and index the tags in Solr. Make sure you create the fields in 'index.parse.md' in nutch-site.xml in schema.xml in Solr.\nPlease let me know if you have any suggestions\nThis is supported by DLA (Digital Library and Archives) of Virginia Tech.",
        "Issue Links": []
    },
    "NUTCH-1479": {
        "Key": "NUTCH-1479",
        "Summary": "nutch readhostdb and updatehostdb do not work with MySQL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.2",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "James Sullivan",
        "Created": "19/Oct/12 22:17",
        "Updated": "22/Oct/12 00:44",
        "Resolved": "22/Oct/12 00:44",
        "Description": "bin/nutch readhostdb and bin/nutch updatehostdb commands do not work with MySQL backend. For example readhostdb throw the following exception \u2013 HostDBReader: org.apache.gora.util.GoraException: java.io.IOException: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Specified key was too long; max key length is 512 bytes",
        "Issue Links": []
    },
    "NUTCH-1480": {
        "Key": "NUTCH-1480",
        "Summary": "SolrIndexer to write to multiple servers.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "22/Oct/12 14:50",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "01/Jun/18 17:49",
        "Description": "SolrUtils should return an array of SolrServers and read the SolrUrl as a comma delimited list of URL's using Configuration.getString(). SolrWriter should be able to handle this list of SolrServers.\nThis is useful if you want to send documents to multiple servers if no replication is available or if you want to send documents to multiple NOCs.\nedit:\nThis does not replace NUTCH-1377 but complements it. With NUTCH-1377 this issue allows you to index to multiple SolrCloud clusters at the same time.",
        "Issue Links": [
            "/jira/browse/NUTCH-945",
            "/jira/browse/NUTCH-945",
            "/jira/browse/NUTCH-1377",
            "https://github.com/apache/nutch/pull/218"
        ]
    },
    "NUTCH-1481": {
        "Key": "NUTCH-1481",
        "Summary": "When using MySQL as storage unicode characters within URLS cause nutch to fail",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.3",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Arni Sumarlidason",
        "Created": "26/Oct/12 01:29",
        "Updated": "27/Sep/14 22:01",
        "Resolved": "27/Sep/14 22:01",
        "Description": "MySQL's (innodb) primary key / unique key is restricted to 767 bytes.. currently the url of a web page is used as a primary key in nutch storage.\nwhen using latin1 character set on the 'id' column @ length 767 bytes/characters; unicode characters in urls cause jdbc to throw an exception,\njava.io.IOException: java.sql.BatchUpdateException: Incorrect string value: '\\xE2\\x80\\x8' for column 'id' at row 1\nwhen using utf8mb4 character set on the 'id' column @ length 190 characters / 760 bytes to fully support unicode characters; the field length becomes insufficient\nIt may be better to use a hash of the url as the primary key instead of the url itself. This would allow urls of any length and full utf8 support.",
        "Issue Links": []
    },
    "NUTCH-1482": {
        "Key": "NUTCH-1482",
        "Summary": "Rename HTMLParseFilter",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "29/Oct/12 15:59",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "See NUTCH-861 for a background discussion. We have changed the name in 2.x to better reflect what it does and I think we should do the same for 1.x.\nany objections?",
        "Issue Links": []
    },
    "NUTCH-1483": {
        "Key": "NUTCH-1483",
        "Summary": "Can't crawl filesystem with protocol-file plugin",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Rog\u00e9rio Pereira Ara\u00fajo",
        "Created": "31/Oct/12 16:14",
        "Updated": "01/Oct/19 13:00",
        "Resolved": "04/Nov/14 21:12",
        "Description": "I tried to follow the same steps described in this wiki page:\nhttp://wiki.apache.org/nutch/IntranetDocumentSearch\nI made all required changes on regex-urlfilter.txt and added the following entry in my seed file:\nfile:///home/rogerio/Documents/\nThe permissions are ok, I'm running nutch with the same user as folder owner, so nutch has all the required permissions, unfortunately I'm getting the following error:\norg.apache.nutch.protocol.file.FileError: File Error: 404\n        at org.apache.nutch.protocol.file.File.getProtocolOutput(File.java:105)\n        at org.apache.nutch.fetcher.FetcherReducer$FetcherThread.run(FetcherReducer.java:514)\nfetch of file://home/rogerio/Documents/ failed with: org.apache.nutch.protocol.file.FileError: File Error: 404\nWhy the logs are showing file://home/rogerio/Documents/ instead of file:///home/rogerio/Documents/ ???\nNote: The regex-urlfilter entry only works as expected if I add the entry \n+file://home/rogerio/Documents/ instead of +file:///home/rogerio/Documents/ as wiki says.",
        "Issue Links": [
            "/jira/browse/NUTCH-1076",
            "/jira/browse/NUTCH-1076"
        ]
    },
    "NUTCH-1484": {
        "Key": "NUTCH-1484",
        "Summary": "TableUtil unreverseURL fails on file:// URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "01/Nov/12 08:36",
        "Updated": "22/May/13 03:53",
        "Resolved": "12/Nov/12 21:22",
        "Description": "(reported by Rog\u00e9rio Pereira Ara\u00fajo, see NUTCH-1483)\nWhen crawling the local filesystem TableUtil.unreverseURL fails for URLs with empty host part (file:///Documents/). StringUtils.split(String, char) does not preserve empty parts which causes:\n\njava.lang.ArrayIndexOutOfBoundsException: 1\nat org.apache.nutch.util.TableUtil.unreverseUrl(TableUtil.java:98)",
        "Issue Links": []
    },
    "NUTCH-1485": {
        "Key": "NUTCH-1485",
        "Summary": "TableUtil reverseURL to keep userinfo part",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "01/Nov/12 09:53",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The reversed URL key does not contain the userinfo part of an URL (user name and password: ftp://user:password@ftp.xyz/file.txt, cf. RFC 3986 and http://en.wikipedia.org/wiki/URI_scheme. Keeping the userinfo would make it easy to crawl a fixed list of protected content. However, URLs with userinfo can be tricky, eg http://cnn.com&story=breaking_news@199.239.136.200/mostpopular, so it's ok when the default is to remove the userinfo. But this should be done in default URL normalizers.",
        "Issue Links": []
    },
    "NUTCH-1486": {
        "Key": "NUTCH-1486",
        "Summary": "Upgrade to Solr 4.10.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "01/Nov/12 15:28",
        "Updated": "19/Aug/15 16:31",
        "Resolved": "18/Aug/15 21:23",
        "Description": "When attempting to configure a 4 multicore 4.0 instance with Nutch schema-solr4.xml file, I get the following exceptions.\nThis has been discussed previously. As I see it we have two options\n1. Keep maintaining both schema options\n2. Ditch the more complex schema-solr4.xml in favour of vanilla schema.xml\nThoughts?\n\nSEVERE: Unable to create core: collection4\norg.apache.solr.common.SolrException: Unable to use updateLog: _version_field must exist in schema, using indexed=\"true\" stored=\"true\" and multiValued=\"false\" (_version_ does not exist)\n\tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:721)\n\tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:566)\n\tat org.apache.solr.core.CoreContainer.create(CoreContainer.java:850)\n\tat org.apache.solr.core.CoreContainer.load(CoreContainer.java:534)\n\tat org.apache.solr.core.CoreContainer.load(CoreContainer.java:356)\n\tat org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:308)\n\tat org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:107)\n\tat org.eclipse.jetty.servlet.FilterHolder.doStart(FilterHolder.java:114)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:754)\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:258)\n\tat org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1221)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:699)\n\tat org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:454)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.deploy.bindings.StandardStarter.processBinding(StandardStarter.java:36)\n\tat org.eclipse.jetty.deploy.AppLifeCycle.runBindings(AppLifeCycle.java:183)\n\tat org.eclipse.jetty.deploy.DeploymentManager.requestAppGoal(DeploymentManager.java:491)\n\tat org.eclipse.jetty.deploy.DeploymentManager.addApp(DeploymentManager.java:138)\n\tat org.eclipse.jetty.deploy.providers.ScanningAppProvider.fileAdded(ScanningAppProvider.java:142)\n\tat org.eclipse.jetty.deploy.providers.ScanningAppProvider$1.fileAdded(ScanningAppProvider.java:53)\n\tat org.eclipse.jetty.util.Scanner.reportAddition(Scanner.java:604)\n\tat org.eclipse.jetty.util.Scanner.reportDifferences(Scanner.java:535)\n\tat org.eclipse.jetty.util.Scanner.scan(Scanner.java:398)\n\tat org.eclipse.jetty.util.Scanner.doStart(Scanner.java:332)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.deploy.providers.ScanningAppProvider.doStart(ScanningAppProvider.java:118)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.deploy.DeploymentManager.startAppProvider(DeploymentManager.java:552)\n\tat org.eclipse.jetty.deploy.DeploymentManager.doStart(DeploymentManager.java:227)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.util.component.AggregateLifeCycle.doStart(AggregateLifeCycle.java:63)\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:53)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:91)\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:263)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.xml.XmlConfiguration$1.run(XmlConfiguration.java:1215)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.eclipse.jetty.xml.XmlConfiguration.main(XmlConfiguration.java:1138)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.eclipse.jetty.start.Main.invokeMain(Main.java:457)\n\tat org.eclipse.jetty.start.Main.start(Main.java:602)\n\tat org.eclipse.jetty.start.Main.main(Main.java:82)\nCaused by: org.apache.solr.common.SolrException: Unable to use updateLog: _version_field must exist in schema, using indexed=\"true\" stored=\"true\" and multiValued=\"false\" (_version_ does not exist)\n\tat org.apache.solr.update.UpdateLog.init(UpdateLog.java:236)\n\tat org.apache.solr.update.UpdateHandler.initLog(UpdateHandler.java:94)\n\tat org.apache.solr.update.UpdateHandler.<init>(UpdateHandler.java:123)\n\tat org.apache.solr.update.DirectUpdateHandler2.<init>(DirectUpdateHandler2.java:97)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat org.apache.solr.core.SolrCore.createInstance(SolrCore.java:476)\n\tat org.apache.solr.core.SolrCore.createUpdateHandler(SolrCore.java:544)\n\tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:705)\n\t... 45 more\nCaused by: org.apache.solr.common.SolrException: _version_field must exist in schema, using indexed=\"true\" stored=\"true\" and multiValued=\"false\" (_version_ does not exist)\n\tat org.apache.solr.update.VersionInfo.getAndCheckVersionField(VersionInfo.java:57)\n\tat org.apache.solr.update.VersionInfo.<init>(VersionInfo.java:83)\n\tat org.apache.solr.update.UpdateLog.init(UpdateLog.java:233)\n\t... 55 more\n01-Nov-2012 16:26:15 org.apache.solr.common.SolrException log\nSEVERE: null:org.apache.solr.common.SolrException: Unable to use updateLog: _version_field must exist in schema, using indexed=\"true\" stored=\"true\" and multiValued=\"false\" (_version_ does not exist)\n\tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:721)\n\tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:566)\n\tat org.apache.solr.core.CoreContainer.create(CoreContainer.java:850)\n\tat org.apache.solr.core.CoreContainer.load(CoreContainer.java:534)\n\tat org.apache.solr.core.CoreContainer.load(CoreContainer.java:356)\n\tat org.apache.solr.core.CoreContainer$Initializer.initialize(CoreContainer.java:308)\n\tat org.apache.solr.servlet.SolrDispatchFilter.init(SolrDispatchFilter.java:107)\n\tat org.eclipse.jetty.servlet.FilterHolder.doStart(FilterHolder.java:114)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.servlet.ServletHandler.initialize(ServletHandler.java:754)\n\tat org.eclipse.jetty.servlet.ServletContextHandler.startContext(ServletContextHandler.java:258)\n\tat org.eclipse.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1221)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:699)\n\tat org.eclipse.jetty.webapp.WebAppContext.doStart(WebAppContext.java:454)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.deploy.bindings.StandardStarter.processBinding(StandardStarter.java:36)\n\tat org.eclipse.jetty.deploy.AppLifeCycle.runBindings(AppLifeCycle.java:183)\n\tat org.eclipse.jetty.deploy.DeploymentManager.requestAppGoal(DeploymentManager.java:491)\n\tat org.eclipse.jetty.deploy.DeploymentManager.addApp(DeploymentManager.java:138)\n\tat org.eclipse.jetty.deploy.providers.ScanningAppProvider.fileAdded(ScanningAppProvider.java:142)\n\tat org.eclipse.jetty.deploy.providers.ScanningAppProvider$1.fileAdded(ScanningAppProvider.java:53)\n\tat org.eclipse.jetty.util.Scanner.reportAddition(Scanner.java:604)\n\tat org.eclipse.jetty.util.Scanner.reportDifferences(Scanner.java:535)\n\tat org.eclipse.jetty.util.Scanner.scan(Scanner.java:398)\n\tat org.eclipse.jetty.util.Scanner.doStart(Scanner.java:332)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.deploy.providers.ScanningAppProvider.doStart(ScanningAppProvider.java:118)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.deploy.DeploymentManager.startAppProvider(DeploymentManager.java:552)\n\tat org.eclipse.jetty.deploy.DeploymentManager.doStart(DeploymentManager.java:227)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.util.component.AggregateLifeCycle.doStart(AggregateLifeCycle.java:63)\n\tat org.eclipse.jetty.server.handler.AbstractHandler.doStart(AbstractHandler.java:53)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.doStart(HandlerWrapper.java:91)\n\tat org.eclipse.jetty.server.Server.doStart(Server.java:263)\n\tat org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:59)\n\tat org.eclipse.jetty.xml.XmlConfiguration$1.run(XmlConfiguration.java:1215)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.eclipse.jetty.xml.XmlConfiguration.main(XmlConfiguration.java:1138)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.eclipse.jetty.start.Main.invokeMain(Main.java:457)\n\tat org.eclipse.jetty.start.Main.start(Main.java:602)\n\tat org.eclipse.jetty.start.Main.main(Main.java:82)\nCaused by: org.apache.solr.common.SolrException: Unable to use updateLog: _version_field must exist in schema, using indexed=\"true\" stored=\"true\" and multiValued=\"false\" (_version_ does not exist)\n\tat org.apache.solr.update.UpdateLog.init(UpdateLog.java:236)\n\tat org.apache.solr.update.UpdateHandler.initLog(UpdateHandler.java:94)\n\tat org.apache.solr.update.UpdateHandler.<init>(UpdateHandler.java:123)\n\tat org.apache.solr.update.DirectUpdateHandler2.<init>(DirectUpdateHandler2.java:97)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:513)\n\tat org.apache.solr.core.SolrCore.createInstance(SolrCore.java:476)\n\tat org.apache.solr.core.SolrCore.createUpdateHandler(SolrCore.java:544)\n\tat org.apache.solr.core.SolrCore.<init>(SolrCore.java:705)\n\t... 45 more\nCaused by: org.apache.solr.common.SolrException: _version_field must exist in schema, using indexed=\"true\" stored=\"true\" and multiValued=\"false\" (_version_ does not exist)\n\tat org.apache.solr.update.VersionInfo.getAndCheckVersionField(VersionInfo.java:57)\n\tat org.apache.solr.update.VersionInfo.<init>(VersionInfo.java:83)\n\tat org.apache.solr.update.UpdateLog.init(UpdateLog.java:233)\n\t... 55 more\n\n01-Nov-2012 16:26:15 org.apache.solr.servlet.SolrDispatchFilter init\nINFO: user.dir=/home/lewis/ASF/solr/example\n01-Nov-2012 16:26:15 org.apache.solr.servlet.SolrDispatchFilter init\nINFO: SolrDispatchFilter.init() done\n2012-11-01 16:26:15.228:INFO:oejs.AbstractConnector:Started SocketConnector@0.0.0.0:8983",
        "Issue Links": [
            "/jira/browse/NUTCH-1568",
            "/jira/browse/NUTCH-1377",
            "/jira/browse/NUTCH-1523",
            "/jira/browse/NUTCH-2056",
            "/jira/browse/NUTCH-2061"
        ]
    },
    "NUTCH-1487": {
        "Key": "NUTCH-1487",
        "Summary": "Nutch parse fails first time for PDF files and works on reparse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "parser,                                            storage",
        "Assignee": null,
        "Reporter": "Kiran",
        "Created": "01/Nov/12 16:26",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The parser is failing to parse pdf files at one go and working on re-parsing command the number of times the total number of PDF files as discussed in the mailing list here (http://www.mail-archive.com/user%40nutch.apache.org/msg07952.html)",
        "Issue Links": []
    },
    "NUTCH-1488": {
        "Key": "NUTCH-1488",
        "Summary": "bin/nutch to run junit from any directory",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1,                                            1.5.1",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "01/Nov/12 20:01",
        "Updated": "22/May/13 03:54",
        "Resolved": "09/Nov/12 16:36",
        "Description": "It should be possible to run a JUnit test via bin/nutch junit (see http://wiki.apache.org/nutch/bin/nutch%20junit and NUTCH-672) from elsewhere not only from runtime/local/. All parts of the class path are absolute but test/classes/ is relative. Is there any reason for this?",
        "Issue Links": []
    },
    "NUTCH-1489": {
        "Key": "NUTCH-1489",
        "Summary": "elasticindex should report the indexed documents like solrindex does",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.1",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Rog\u00e9rio Pereira Ara\u00fajo",
        "Created": "02/Nov/12 18:34",
        "Updated": "12/Jan/13 20:30",
        "Resolved": "12/Jan/13 20:30",
        "Description": "When I run:\nnutch elasticindex elasticsearch\nTo index crawled documents in a standard elasticsearch setup, the process takes some time, finishes, but doesn't report how many documents was indexed, it would be nice to have the same feedback as solrindex.",
        "Issue Links": []
    },
    "NUTCH-1490": {
        "Key": "NUTCH-1490",
        "Summary": "Data Truncation exceptions when using mysql",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nathan Gass",
        "Created": "05/Nov/12 13:11",
        "Updated": "01/May/14 06:15",
        "Resolved": "01/May/14 06:15",
        "Description": "Nutch does not ensure the set (or implicit) maximal length for the following columns:\ntitle\nurls (id, baseUrl, reprUrl,\ntyp (contentType)\ninlinks\noutlinks\nTrying to store too much data in one of this columns results in an exception similar to this (copied from GORA-24, I will be able to add an newer stack trace later today):\njava.io.IOException: java.sql.BatchUpdateException: Data truncation: Data too long for column 'inlinks' at row 1 \nat org.apache.gora.sql.store.SqlStore.flush(SqlStore.java:340) \nat org.apache.gora.sql.store.SqlStore.close(SqlStore.java:185) \nat org.apache.gora.mapreduce.GoraRecordWriter.close(GoraRecordWriter.java:55) \nat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:567) \nat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:408) \nat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:216) \nCaused by: java.sql.BatchUpdateException: Data truncation: Data too long for column 'inlinks' at row 1 \nat com.mysql.jdbc.PreparedStatement.executeBatchSerially(PreparedStatement.java:2018) \nat com.mysql.jdbc.PreparedStatement.executeBatch(PreparedStatement.java:1449) \nat org.apache.gora.sql.store.SqlStore.flush(SqlStore.java:328) \n... 5 more\nI'll add my current fixes in later comments.",
        "Issue Links": []
    },
    "NUTCH-1491": {
        "Key": "NUTCH-1491",
        "Summary": "UTF-8 non-character codepoints in title",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Nathan Gass",
        "Created": "06/Nov/12 07:26",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Nov/12 09:17",
        "Description": "This is the same problem as NUTCH-1026 but for the title field. The attached patch just uses the same solution for this field too.",
        "Issue Links": []
    },
    "NUTCH-1492": {
        "Key": "NUTCH-1492",
        "Summary": "Support gora-dynamodb in Nutch 2.x",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.5",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "06/Nov/12 09:56",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "We recently committed GORA-103. With the introduction of this module, it is essential that it is thoroughly tested over at Nutch HQ. The primary purpose of this issue is to provide all GORA configuration and ivy/ivy.xml dependencies, however it should also act as a parent issue for any immediate problem encountered in making GORA-103 functionality available through Nutch 2.x",
        "Issue Links": [
            "/jira/browse/GORA-338",
            "/jira/browse/GORA-103"
        ]
    },
    "NUTCH-1493": {
        "Key": "NUTCH-1493",
        "Summary": "Error adding field 'contentLength'='' during solrindex using index-more",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1,                                            2.2",
        "Fix Version/s": "1.6,                                            2.2",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Nathan Gass",
        "Created": "07/Nov/12 17:51",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/Nov/12 14:13",
        "Description": "The contentLength can be an empty string (I assume this is possible because of NUTCH-1096), but solr does not accept this. The attached patch just checks for empty string contentLength and does not try to index it.",
        "Issue Links": []
    },
    "NUTCH-1494": {
        "Key": "NUTCH-1494",
        "Summary": "RSS feed plugin seems broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.7",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sourajit Basak",
        "Created": "09/Nov/12 11:34",
        "Updated": "27/Jan/15 21:07",
        "Resolved": "09/Jan/13 23:30",
        "Description": "The RSS \"feed\" plugin is broken.\nI had to change the plugin dependencies to point to the correct rome library version.\n<!-- changed to the version thats bundled with v1.5.1, previously 0.9 -->\n <library name=\"rome-1.0.0.jar\" />\n <!-- added this due to a CNFE from rome -->\n <library name=\"jdom-1.0.jar\" />\nStill it fails due to some (known) problem in rome.\nCaused by: java.lang.NullPointerException\n    at java.util.Properties$LineReader.readLine(Properties.java:434)\n    at java.util.Properties.load0(Properties.java:353)\n    at java.util.Properties.load(Properties.java:341)\n    at com.sun.syndication.io.impl.PropertiesLoader.<init>(PropertiesLoader.java:74)\n    at com.sun.syndication.io.impl.PropertiesLoader.getPropertiesLoader(PropertiesLoader.java:46)\n    at com.sun.syndication.io.impl.PluginManager.<init>(PluginManager.java:54)\n    at com.sun.syndication.io.impl.PluginManager.<init>(PluginManager.java:46)\n    at com.sun.syndication.feed.synd.impl.Converters.<init>(Converters.java:40)\n    at com.sun.syndication.feed.synd.SyndFeedImpl.<clinit>(SyndFeedImpl.java:59)",
        "Issue Links": [
            "/jira/browse/NUTCH-1053",
            "/jira/browse/NUTCH-1893"
        ]
    },
    "NUTCH-1495": {
        "Key": "NUTCH-1495",
        "Summary": "-normalize and -filter for updatedb command in nutch 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nathan Gass",
        "Created": "09/Nov/12 16:43",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "AFAIS in nutch 1.x you could change your url filters and normalizers during the crawl, and update the db using crawldb -normalize -filter. There does not seem to be a away to achieve the same in nutch 2.x?\nAnyway, I went ahead and tried to implement -normalize and -filter for the nutch 2.x updatedb command. I have no experience with any of the used technologies including java, so please check the attached code carefully before using it. I'm very interested to hear if this is the right approach or any other comments.",
        "Issue Links": []
    },
    "NUTCH-1496": {
        "Key": "NUTCH-1496",
        "Summary": "ParserJob logs skipped urls with level info",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nathan Gass",
        "Created": "11/Nov/12 14:43",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Nov/12 12:54",
        "Description": "ParserJob is the only one which logs all skipped urls with level info. Attached patch changes this to level debug, the same level already used by FetcherJob, IndexerJob, and GeneratorJob.",
        "Issue Links": []
    },
    "NUTCH-1497": {
        "Key": "NUTCH-1497",
        "Summary": "Better default gora-sql-mapping.xml with larger field sizes for MySQL",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.3",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "James Sullivan",
        "Created": "12/Nov/12 05:10",
        "Updated": "01/May/14 06:15",
        "Resolved": "01/May/14 06:15",
        "Description": "The current generic default gora-sql-mapping.xml has field sizes that are too small in almost all situations when used with MySQL. I have included a mapping which will work better for MySQL (takes slightly more space but will be able to handle larger fields necessary for real world use). Includes patch from Nutch-1490 and resolves the non-Unicode part of Nutch-1473. I believe it is not possible to use the same gora-sql-mapping for both hsqldb and MySQL without a significantly degraded lowest common denominator resulting. Should the user manually rename the attached file to gora-sql-mapping.xml or is there a way to have Nutch automatically use it when MySQL is selected in other configurations (Ivy.xml or gora.properties)?",
        "Issue Links": []
    },
    "NUTCH-1498": {
        "Key": "NUTCH-1498",
        "Summary": "Make index-basic consistent in trunk and 2.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Nov/12 19:39",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Currently the index-basic plugin supports more functionality in trunk than it does in 2.x. I see no reason why functionality shouldn't be made consistent.\nFor example \n\n2.x duplicates field values for host and site...\ntrunk supports configuration options for \"indexer.add.domain and indexer.max.content.length whereas 2.x does not.",
        "Issue Links": []
    },
    "NUTCH-1499": {
        "Key": "NUTCH-1499",
        "Summary": "Usage of multiple ipv4 addresses and network cards on fetcher machines",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.5.1",
        "Fix Version/s": "1.7",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Walter Tietze",
        "Created": "23/Nov/12 15:18",
        "Updated": "22/May/13 03:54",
        "Resolved": "08/May/13 22:11",
        "Description": "Adds for the fetcher threads the ability to use multiple configured ipv4 addresses.\nOn some cluster machines there are several ipv4 addresses configured where each ip address is associated with its own network interface.\nThis patch enables to configure the protocol-http and the protocol-httpclient  to use these network interfaces in a round robin style.\nIf the feature is enabled, a helper class reads at startup the network configuration. In each http network connection the next ip address is taken. This method is synchronized, but this should be no bottleneck for the overall performance of the fetcher threads.\nThis feature is tested on our cluster for the protocol-http and the protocol-httpclient protocol.",
        "Issue Links": []
    },
    "NUTCH-1500": {
        "Key": "NUTCH-1500",
        "Summary": "bin/crawl fails on step solrindex with wrong path to segment",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "28/Nov/12 22:16",
        "Updated": "22/May/13 03:54",
        "Resolved": "15/Jan/13 21:24",
        "Description": "The bin/crawl script calls the command (bin/nutch) solrindex with the wrong path to the segment which causes solrindex to fail.",
        "Issue Links": []
    },
    "NUTCH-1501": {
        "Key": "NUTCH-1501",
        "Summary": "Harmonize behavior of parsechecker and indexchecker",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.2",
        "Component/s": "indexer,                                            parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "05/Dec/12 22:09",
        "Updated": "22/May/13 03:54",
        "Resolved": "17/Apr/13 23:21",
        "Description": "Behaviour of ParserChecker and IndexingFiltersChecker has diverged between trunk and 2.x\n\nmissing in 2.x: NUTCH-1320, NUTCH-1207\nopen issue to be also applied to 2.x: NUTCH-1419, NUTCH-1389",
        "Issue Links": [
            "/jira/browse/NUTCH-1038"
        ]
    },
    "NUTCH-1502": {
        "Key": "NUTCH-1502",
        "Summary": "Test for CrawlDatum state transitions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2",
        "Fix Version/s": "1.9",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "06/Dec/12 21:40",
        "Updated": "15/Jul/14 09:53",
        "Resolved": "15/Jul/14 09:18",
        "Description": "An exhaustive test to check the matrix of CrawlDatum state transitions (CrawlStatus in 2.x) would be useful to detect errors esp. for continuous crawls where the number of possible transitions is quite large. Additional factors with impact on state transitions (retry counters, static and dynamic intervals) are also tested.\nThe tests will help to address the NUTCH-578 and NUTCH-1245. See the latter for a first sketchy patch.",
        "Issue Links": [
            "/jira/browse/NUTCH-1564",
            "/jira/browse/NUTCH-1422"
        ]
    },
    "NUTCH-1503": {
        "Key": "NUTCH-1503",
        "Summary": "Configuration properties not in sync between FetcherReducer and nutch-default.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "fetcher",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Dec/12 20:13",
        "Updated": "22/May/13 03:54",
        "Resolved": "12/Dec/12 19:53",
        "Description": "FetcherReducer.java\nBug: Following properties appear in FetcherReducer but not in nutch-default.xml\n\n290       useHostSettings = conf.getBoolean(\"fetcher.queue.use.host.settings\", false);\n300       this.timelimit = conf.getLong(\"fetcher.timelimit\", -1);\n450       this.byIP = conf.getBoolean(\"fetcher.threads.per.host.by.ip\", true);\n698       timelimit = context.getConfiguration().getLong(\"fetcher.timelimit\", -1); \n\n\nTherefore they cannot be used properly in code execution and must be updated, removed and/or added to nutch-default.xml.\nPatch coming up just now.",
        "Issue Links": []
    },
    "NUTCH-1504": {
        "Key": "NUTCH-1504",
        "Summary": "Pluggable url partitioner",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sourajit Basak",
        "Created": "17/Dec/12 17:38",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "At present, the url partition logic is hard wired inside nutch core. It should be pluggable like FetchSchedule customized via nutch-site.xml.\nThere might be use cases where a single domain needs to be partioned on some custom logic. The existing UrlPartitioner cannot handle such cases. \nHence the requirement.",
        "Issue Links": []
    },
    "NUTCH-1505": {
        "Key": "NUTCH-1505",
        "Summary": "java.lang.IllegalArgumentException during updatedb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Stanley Orlenko",
        "Created": "21/Dec/12 09:30",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "the command \nbin/nutch updatedb\nraises the exception. Here is a part of hadoop.log:\n2012-12-21 11:27:58,557 WARN  mapred.LocalJobRunner - job_local_0001\njava.lang.IllegalArgumentException: offset (0) + length (4) exceed the capacity of the array: 2\n        at org.apache.nutch.util.Bytes.explainWrongLengthOrOffset(Bytes.java:559)\n        at org.apache.nutch.util.Bytes.toInt(Bytes.java:740)\n        at org.apache.nutch.util.Bytes.toFloat(Bytes.java:611)\n        at org.apache.nutch.util.Bytes.toFloat(Bytes.java:598)\n        at org.apache.nutch.scoring.opic.OPICScoringFilter.distributeScoreToOutlinks(OPICScoringFilter.java:128)\n        at org.apache.nutch.scoring.ScoringFilters.distributeScoreToOutlinks(ScoringFilters.java:117)\n        at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:70)\n        at org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:37)\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)",
        "Issue Links": []
    },
    "NUTCH-1506": {
        "Key": "NUTCH-1506",
        "Summary": "Add UPDATE action to NutchIndexAction",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.7",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/12 10:45",
        "Updated": "22/May/13 03:53",
        "Resolved": "16/Jan/13 11:06",
        "Description": "NutchIndexAction should have an UPDATE option next to ADD and DELETE. It must also be allowed to be wrapped in NutchWritable for it to be used in future OutputFormats such as the Fetcher.",
        "Issue Links": []
    },
    "NUTCH-1507": {
        "Key": "NUTCH-1507",
        "Summary": "Remove FetcherOutput",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.7",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/12 10:51",
        "Updated": "22/May/13 03:54",
        "Resolved": "16/Jan/13 11:07",
        "Description": "The FetcherOutput class is not used anywhere and it and its references should be removed.",
        "Issue Links": []
    },
    "NUTCH-1508": {
        "Key": "NUTCH-1508",
        "Summary": "Port limit crawler to defined depth to 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "21/Dec/12 11:27",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1331"
        ]
    },
    "NUTCH-1509": {
        "Key": "NUTCH-1509",
        "Summary": "Implement read/write in NutchField",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.7",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/12 12:06",
        "Updated": "22/May/13 03:53",
        "Resolved": "16/Jan/13 11:10",
        "Description": "For some reason write() and readFields() in NutchField have not been implemented. Never assumed this could be true i wasted 30 minutes in finding out",
        "Issue Links": []
    },
    "NUTCH-1510": {
        "Key": "NUTCH-1510",
        "Summary": "Upgrade to Hadoop 1.1.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/12 13:30",
        "Updated": "22/May/13 03:53",
        "Resolved": "27/Dec/12 12:37",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1511": {
        "Key": "NUTCH-1511",
        "Summary": "Metadata in MYSQL updated with 'garbage'",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "J. Gobel",
        "Created": "01/Jan/13 13:56",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "After applying patch for Metadata parser (NUTCH-1478) I notice that the metadata field just before the crawl ends is populated with the correct information. However when the crawl is completely finished the metadata field is populated with 'garbage' csh\b\ufffd\ufffd\ufffd\ufffd\ufffd \nI notice in my SQL log file that the scoring plugin is overwriting the metadata field in a final data insertion with 'csh \\0\\0\\0\\0\\'. When I remove 'scoring-opic' out of 'plugin.includes' property in the nutch-site.xml , the metadata-field is crisp and clear.\nMYSQL LOG FILE: (I did a crawl on http://nutch.apache.org. Below you will see a fragments of my MYSQL log file, only the moments when data is written to the METADATA field in the MYSQL table.\nFirst Insertion .. here I suppose scoring-opic writes its information, csh ?\u20ac\\0\\0\\0 \n58 Query    INSERT INTO webpage (fetchInterval,fetchTime,id,markers,metadata,score )VALUES (2592000,1357122976493,'org.apache.nutch:http/',' dist 0 injmrk y\\0','\ncsh ?\u20ac\\0\\0\\0',1.0) ON DUPLICATE KEY UPDATE fetchInterval=2592000,fetchTime=1357122976493,markers=' dist 0 injmrk y\\0',metadata='\ncsh ?\u20ac\\0\\0\\0',score=1.0\nSecond Insertion - inhere scraped metada is inserted into metadata. \n 81 Query    INSERT INTO webpage (id,markers,metadata,outlinks,parseStatus,signature,text,title )VALUES ('org.apache.nutch:http/',\nThe final insertion -  please note that here the metadata field is overwritten with CSH\\0\\0\\0\\0\n90 Query    INSERT INTO webpage (fetchTime,id,inlinks,markers,metadata )VALUES (1359714995075,'org.apache.nutch:http/',' 0http://nutch.apache.org/\nNutch\\0','\f dist 0 injmrk y updmrk*1357122982-1745626508 _prsmrk*1357122982-1745626508 _gnmrk*1357122982-1745626508 ftcmrk*1357122982-1745626508\\0','\ncsh \\0\\0\\0\\0\\0') ON DUPLICATE KEY UPDATE fetchTime=1359714995075,inlinks=' 0http://nutch.apache.org/",
        "Issue Links": []
    },
    "NUTCH-1512": {
        "Key": "NUTCH-1512",
        "Summary": "SegmentMerger to normalize",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "03/Jan/13 14:42",
        "Updated": "22/May/13 03:53",
        "Resolved": "03/Jan/13 14:44",
        "Description": "SegmentMerger can not normalize, only filter. Issue to add a -normalize option.",
        "Issue Links": []
    },
    "NUTCH-1513": {
        "Key": "NUTCH-1513",
        "Summary": "Support Robots.txt for Ftp urls",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "None",
        "Assignee": "Tejas Patil",
        "Reporter": "Tejas Patil",
        "Created": "04/Jan/13 08:56",
        "Updated": "22/May/13 03:54",
        "Resolved": "21/May/13 01:32",
        "Description": "As per [0], a FTP website can have robots.txt like [1]. In the nutch code, Ftp plugin is not parsing the robots file and accepting all urls.\nIn \"src/plugin/protocol-ftp/src/java/org/apache/nutch/protocol/ftp/Ftp.java\"\n\n   public RobotRules getRobotRules(Text url, CrawlDatum datum) {\n    return EmptyRobotRules.RULES;\n  }\n\nIts not clear of this was part of design or if its a bug. \n[0] : https://developers.google.com/webmasters/control-crawl-index/docs/robots_txt\n[1] : ftp://example.com/robots.txt",
        "Issue Links": []
    },
    "NUTCH-1514": {
        "Key": "NUTCH-1514",
        "Summary": "Phase out the deprecated configuration properties (if possible)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "fetcher,                                            generator",
        "Assignee": "Tejas Patil",
        "Reporter": "Tejas Patil",
        "Created": "06/Jan/13 15:06",
        "Updated": "10/Jul/18 14:11",
        "Resolved": "03/May/13 19:29",
        "Description": "In reference to [0], the deprecated configuration properties can be removed (only if possible without affecting the functionality).\n[0] : http://mail-archives.apache.org/mod_mbox/nutch-user/201301.mbox/%3CCAFKhtFwvM7w-cVusGZWKeGdcWrVShPtBdfTdcn1NNpM1Z2-ovA@mail.gmail.com%3E",
        "Issue Links": [
            "/jira/browse/NUTCH-1409"
        ]
    },
    "NUTCH-1515": {
        "Key": "NUTCH-1515",
        "Summary": "RSS plugin broken and won't compile",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "09/Jan/13 23:32",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The feed plugin is completely broken in 2.x. It doesn't compile and need to some attention. There is absolutely no point in shipping the code if it is useless to users.",
        "Issue Links": [
            "/jira/browse/NUTCH-874"
        ]
    },
    "NUTCH-1516": {
        "Key": "NUTCH-1516",
        "Summary": "Nutch 2.x pom.xml out of sync with ivy.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.2",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Jan/13 21:01",
        "Updated": "22/May/13 03:53",
        "Resolved": "10/Jan/13 21:10",
        "Description": "The way that releases are done, the pom.xml in the 2.x is NOT updated, however the release branch and the corresponding tag ARE. This now means that the pom.xml in the 2.x branch is out of sync with the tags which are released from it.\nSome tweaking to the pom.xml and the release process should resolve this issue.",
        "Issue Links": []
    },
    "NUTCH-1517": {
        "Key": "NUTCH-1517",
        "Summary": "CloudSearch indexer",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "indexer",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "11/Jan/13 17:40",
        "Updated": "26/Aug/15 16:42",
        "Resolved": "26/Aug/15 12:42",
        "Description": "Once we have made the indexers pluggable, we should add a plugin for Amazon CloudSearch. See http://aws.amazon.com/cloudsearch/. Apparently it uses a JSON based representation Search Data Format (SDF), which we could reuse for a file based indexer.",
        "Issue Links": [
            "/jira/browse/NUTCH-1047"
        ]
    },
    "NUTCH-1518": {
        "Key": "NUTCH-1518",
        "Summary": "session cookies support",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "David Michael Gang",
        "Created": "15/Jan/13 07:58",
        "Updated": "15/May/14 09:44",
        "Resolved": "15/Jan/13 20:20",
        "Description": "There are internet sites, which in order to get crawled have to store cookies.\nFor example for a fake internet site\nwww.blala.com in order to fetch we have to send first\nwww.blala.com?username=x&password=y\nand then the browser stores a sesssion cookie.\nAfterwards i can fetch all pages of the domain www.blala.com/a/b/c.html\nI want a feature where we define in a file domains of urls and the url how to make logins.\nWhen nutch will see such a site and fetch the page it will login before.\nThe jira https://issues.apache.org/jira/browse/NUTCH-827\nis very similar to what i need.",
        "Issue Links": [
            "/jira/browse/NUTCH-827"
        ]
    },
    "NUTCH-1519": {
        "Key": "NUTCH-1519",
        "Summary": "Configuration Overrides not in sync between WebTableReader and nutch-default.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "crawldb,                                            storage",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jan/13 20:47",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "In 2.x HEAD the WebTableReader class [0] provides Overrides for properties such as \n\ncurrentJob.getConfiguration().setBoolean(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", false);\ncurrentJob.getConfiguration().setBoolean(\"db.reader.stats.sort\", sort);\n\n\nas well as\n\nConfiguration cfg = job.getConfiguration();\n    cfg.set(WebTableRegexMapper.regexParamName, regex);\n    cfg.setBoolean(WebTableRegexMapper.contentParamName, content);\n    cfg.setBoolean(WebTableRegexMapper.headersParamName, headers);\n    cfg.setBoolean(WebTableRegexMapper.linksParamName, links);\n    cfg.setBoolean(WebTableRegexMapper.textParamName, text);\n\n\nNone of these are actually present and therefore configurable an able to be Overridden.\nThis should be sorted out.\n[0] http://svn.apache.org/repos/asf/nutch/branches/2.x/src/java/org/apache/nutch/crawl/WebTableReader.java",
        "Issue Links": []
    },
    "NUTCH-1520": {
        "Key": "NUTCH-1520",
        "Summary": "SegmentMerger looses records",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Jan/13 08:59",
        "Updated": "19/Jul/13 14:24",
        "Resolved": "05/Jul/13 08:53",
        "Description": "It seems the SegmentMerger tool looses documents. You're likely to see less documents in an index if you index one or more already merged segments than if you index all unmerged segments.\nThis is really nasty!",
        "Issue Links": [
            "/jira/browse/NUTCH-1113",
            "/jira/browse/NUTCH-1616"
        ]
    },
    "NUTCH-1521": {
        "Key": "NUTCH-1521",
        "Summary": "CrawlDbFilter pass null url to urlNormailzers",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.9",
        "Component/s": "None",
        "Assignee": "lufeng",
        "Reporter": "lufeng",
        "Created": "22/Jan/13 07:29",
        "Updated": "17/Apr/14 01:59",
        "Resolved": "17/Apr/14 01:59",
        "Description": "urlNormalizers will get null url if we set CRAWLDB_PURGE_404, and it will throw NullPointerException. and the WARN Log will output something like this \"Skipping null NullPointerException\".",
        "Issue Links": []
    },
    "NUTCH-1522": {
        "Key": "NUTCH-1522",
        "Summary": "Upgrade to Tika 1.3",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7,                                            2.2.1",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "23/Jan/13 09:49",
        "Updated": "11/Oct/19 15:36",
        "Resolved": "27/Jun/13 17:10",
        "Description": "http://www.apache.org/dist/tika/CHANGES-1.3.txt",
        "Issue Links": []
    },
    "NUTCH-1523": {
        "Key": "NUTCH-1523",
        "Summary": "Upgrade solr-solr4j dependency to 4.1.0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Jan/13 03:45",
        "Updated": "22/May/13 03:54",
        "Resolved": "25/Apr/13 01:05",
        "Description": "Part of this includes upgrading use of the now deprecated CommonsHttpSolrServer used in o.a.n.i.solr.SolrUtils and used in o.a.n.i.solr.SolrIndexer.\nI don't yet know how this ties in with Julien proposed restructuring of the pluggable indexers but the issue is better logged and recorded than left to evaporate.",
        "Issue Links": [
            "/jira/browse/NUTCH-1486"
        ]
    },
    "NUTCH-1524": {
        "Key": "NUTCH-1524",
        "Summary": "Internal links are not being saved even with change in parameter (db.ignore.internal.links)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "linkdb",
        "Assignee": null,
        "Reporter": "Kiran",
        "Created": "24/Jan/13 21:20",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The internal links are not being saved. I have tried changing the parameter (db.ignore.internal.links) to false but still the internal links are not saved.",
        "Issue Links": []
    },
    "NUTCH-1525": {
        "Key": "NUTCH-1525",
        "Summary": "Generator to record external links even when  db.ignore.external.links set to true",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Jan/13 18:29",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "When fetching pages from specific domains we have various options e.g. use urlfilters, set the above property to true before injecting urls into the webdb etc. However with the former, it is recognised that complex regex can slow down processing and with the latter it means we disregard a number of urls which could potentially become useful in the future.\nUnfortunately there is no way to record external links encountered for future processing, although the wiki suggests that a very small patch to the generator code can allow you to log these links to hadoop.log. although this is better, a more robusts storage mechanism would be preferred. This may tie in with custom counters we've already specified or may require new counters to be implemented.",
        "Issue Links": []
    },
    "NUTCH-1526": {
        "Key": "NUTCH-1526",
        "Summary": "Create SegmentContentDumperTool for easily extracting out file contents from SegmentDirs",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "storage",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "28/Jan/13 02:32",
        "Updated": "06/Apr/15 16:58",
        "Resolved": "21/Sep/14 05:49",
        "Description": "It only took me 1.2 years, but I finally got around to it. This patch will deliver a SegmentContentDumper tool per the description here:\nhttp://s.apache.org/kv\nAnd per the interface here:\n\n./bin/nutch org.apache.nutch.tools.SegmentContentDumper [options]\n   -segmentRootDir full file path to the root segment directory, e.g., crawl/segments\n   -regexUrlPattern a regex URL pattern to select URL keys to dump from the content DB in each segment\n   -outputDir The output directory to write file names to.\n   -metadata --key=value where key is a Content Metadata key and value is a value to check.\n\n\nIf the URL and its content metadata have a matching key,value pair, dump it. Allow for regex matching on the value.",
        "Issue Links": []
    },
    "NUTCH-1527": {
        "Key": "NUTCH-1527",
        "Summary": "Port nutch-elasticsearch-indexer to Nutch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Lewis John McGibbney",
        "Created": "09/Feb/13 18:43",
        "Updated": "20/Jun/13 09:44",
        "Resolved": "19/Jun/13 09:16",
        "Description": "The source repos for this can be found here [0].\nThis issue should be inline with the work already done by Julien and others over at NUTCH-1047.\n[0] https://github.com/ctjmorgan/nutch-elasticsearch-indexer",
        "Issue Links": [
            "/jira/browse/NUTCH-1047"
        ]
    },
    "NUTCH-1528": {
        "Key": "NUTCH-1528",
        "Summary": "Port nutch-mongodb-indexer to Nutch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "09/Feb/13 18:45",
        "Updated": "21/Jun/13 18:39",
        "Resolved": "21/Jun/13 18:39",
        "Description": "The source repos can be found here [0].\nThis issue should be in line with the work done by Julien and others over at NUTCH-1047 \n[0] https://github.com/ctjmorgan/nutch-mongodb-indexer",
        "Issue Links": [
            "/jira/browse/NUTCH-1047"
        ]
    },
    "NUTCH-1529": {
        "Key": "NUTCH-1529",
        "Summary": "Port nutch-mongdb-parser to trunk",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "None",
        "Component/s": "injector",
        "Assignee": "lufeng",
        "Reporter": "Lewis John McGibbney",
        "Created": "09/Feb/13 18:52",
        "Updated": "21/Jun/13 18:39",
        "Resolved": "21/Jun/13 18:39",
        "Description": "The initial repos is here [0]\n[0] https://github.com/ctjmorgan/nutch-mongdb-parser",
        "Issue Links": []
    },
    "NUTCH-1530": {
        "Key": "NUTCH-1530",
        "Summary": "Umlauts (\u00fc\u00e4\u00f6) garbled when fetch and parse in separate calls (OK when fetcher.parse is true)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Edward Ackroyd",
        "Created": "12/Feb/13 18:37",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "When crawling http://www.spiegel.de (popular German news site) in separate fetch and parse calls (nutch fetch, then nutch parse, fetcher.parse=false) this lands in Cassandra (umlauts all garbled, for example '\u00ef\u00bf\u00bd' instead of '\u00f6'):\n[default@webpage] list p;\nRowKey: de.spiegel.www:http/\n=> (column=c, value=SPIEGEL ONLINE - Nachrichten Schlagzeilen Hilfe RSS Newsletter Mobil Wetter TV-Programm Dienstag, 12. Februar 2013 SPIEGEL ONLINE NACHRICHTEN Home Politik Deutschland Ausland \u00a0 Wirtschaft B\u00ef\u00bf\u00bdrse Verbraucher & Service Unternehmen & M\u00ef\u00bf\u00bdrkte Staat & Soziales Jobsuche Immowelt \u00a0 Panorama Justiz Leute Gesellschaft Partnersuche Eurojackpot Tarifvergleiche \u00a0 Sport Wintersport Fu\u00ef\u00bf\u00bdball Bundesliga...\nHowever, when fetcher.parse=true and the fetch call does the parsing, the correct umlauts land in Cassandra:\n[default@webpage] list p;\nRowKey: de.spiegel.www:http/\n=> (column=c, value=SPIEGEL ONLINE - Nachrichten Schlagzeilen Hilfe RSS Newsletter Mobil Wetter TV-Programm Dienstag, 12. Februar 2013 SPIEGEL ONLINE NACHRICHTEN Home Politik Deutschland Ausland \u00a0 Wirtschaft B\u00f6rse Verbraucher & Service Unternehmen & M\u00e4rkte Staat & Soziales Jobsuche Immowelt \u00a0 Panorama Justiz Leute Gesellschaft Partnersuche Eurojackpot Tarifvergleiche \u00a0 Sport Wintersport Fu\u00dfball Bundesliga...\nSeems the content is over-encoded when fetching/parsing in separate calls.",
        "Issue Links": []
    },
    "NUTCH-1531": {
        "Key": "NUTCH-1531",
        "Summary": "URL filtering takes long time for very long URLs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.6,                                            2.1,                                            1.7,                                            2.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "F\u0131rat K\u00dc\u00c7\u00dcK",
        "Created": "13/Feb/13 08:19",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "24/Oct/16 10:32",
        "Description": "Filtering very long urls (such as base64 image generators) take long time (hours). On reducing phase it locks down all the system for hours. Therefore some URL length limitation needed. We attached a little patch for this improvement.",
        "Issue Links": [
            "/jira/browse/NUTCH-1314"
        ]
    },
    "NUTCH-1532": {
        "Key": "NUTCH-1532",
        "Summary": "Replace 'segment' mapping field with batchId",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Feb/13 01:57",
        "Updated": "22/May/13 03:54",
        "Resolved": "08/Apr/13 00:38",
        "Description": "As described here [0], the segment field in solr-mapping.xml should be replaced with the batchId. This reflects the different architecture in 2.x.\n[0] http://www.mail-archive.com/user%40nutch.apache.org/msg08793.html",
        "Issue Links": [
            "/jira/browse/NUTCH-1533"
        ]
    },
    "NUTCH-1533": {
        "Key": "NUTCH-1533",
        "Summary": "Implement getPrevModifiedTime(), setPrevModifiedTime(), getBatchId() and setBatchId() accessors in o.a.n.storage.WebPage",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "storage",
        "Assignee": "lufeng",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Feb/13 05:05",
        "Updated": "22/May/13 03:54",
        "Resolved": "25/Mar/13 12:33",
        "Description": "NUTCH-1532 needs to obtain a batchId to add to NutchDocument prior to indexing. This is currently not available as we do not store the information in the WebPage. Additionally, we do not store the other ModifiedTime's but incorrectly set them in o.a.n.crawl.FetchSchedule#setFetchSchedule.\nAll the above accessors should be implemented.",
        "Issue Links": [
            "/jira/browse/NUTCH-1532"
        ]
    },
    "NUTCH-1534": {
        "Key": "NUTCH-1534",
        "Summary": "cassandra/hector exception: InvalidRequestException(why:column name must not be empty)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher,                                            parser",
        "Assignee": null,
        "Reporter": "Roland von Herget",
        "Created": "19/Feb/13 10:14",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "during bigger fetches (100k+ URLs), sometimes these errors occure:\n\n2013-02-19 09:32:09,639 WARN  fetcher.FetcherJob - Attempting to finish item from unknown queue: FetchItem [queueID=http://www.wer-kennt-wen.de, url=http\n://www.wer-kennt-wen.de/gallery/imageshow/mmfqq4y02q09, u=http://www.wer-kennt-wen.de/gallery/imageshow/mmfqq4y02q09, page=org.apache.nutch.storage.WebPa\nge@7b1ab444 {\n  \"baseUrl\":\"null\"\n  \"status\":\"34\"\n  \"fetchTime\":\"1361262537305\"\n  \"prevFetchTime\":\"1361257503835\"\n  \"fetchInterval\":\"0\"\n  \"retriesSinceFetch\":\"0\"\n  \"modifiedTime\":\"0\"\n  \"protocolStatus\":\"org.apache.nutch.storage.ProtocolStatus@40b98 {\n  \"code\":\"16\"\n  \"args\":\"[Http code=403, url=http://www.wer-kennt-wen.de/gallery/imageshow/mmfqq4y02q09]\"\n  \"lastModified\":\"0\"\n}\"\n  \"content\":\"null\"\n  \"contentType\":\"null\"\n  \"prevSignature\":\"null\"\n  \"signature\":\"null\"\n  \"title\":\"null\"\n  \"text\":\"null\"\n  \"parseStatus\":\"null\"\n  \"score\":\"0.0\"\n  \"reprUrl\":\"null\"\n  \"headers\":\"{Set-Cookie=WKWSESSID=9d968aeef3a709bc4bba9bb955b93e1e; path=/; domain=.wer-kennt-wen.de, Connection=close, Content-Type=text/html, Cache-Co\nntrol=no-store, no-cache, must-revalidate, post-check=0, pre-check=0, Date=Tue, 19 Feb 2013 08:28:57 GMT, P3P=CP=\"CAO OUR\", Expires=Thu, 19 Nov 1981 08:5\n2:00 GMT, Server=Apache, Pragma=no-cache}\"\n  \"outlinks\":\"{}\"\n  \"inlinks\":\"{}\"\n  \"markers\":\"{dist=0, _injmrk_=y, _ftcmrk_=1361257998-2045033576, _gnmrk_=1361257998-2045033576}\"\n  \"metadata\":\"{}\"\n}]\n2013-02-19 09:32:09,640 ERROR fetcher.FetcherJob - Unexpected error for http://www.wer-kennt-wen.de/gallery/imageshow/mmfqq4y02q09\nme.prettyprint.hector.api.exceptions.HInvalidRequestException: InvalidRequestException(why:column name must not be empty)\n        at me.prettyprint.cassandra.service.ExceptionsTranslatorImpl.translate(ExceptionsTranslatorImpl.java:52)\n        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:97)\n        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:90)\n        at me.prettyprint.cassandra.service.Operation.executeAndSetResult(Operation.java:101)\n        at me.prettyprint.cassandra.connection.HConnectionManager.operateWithFailover(HConnectionManager.java:233)\n        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.operateWithFailover(KeyspaceServiceImpl.java:131)\n        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.batchMutate(KeyspaceServiceImpl.java:102)\n        at me.prettyprint.cassandra.service.KeyspaceServiceImpl.batchMutate(KeyspaceServiceImpl.java:108)\n        at me.prettyprint.cassandra.model.MutatorImpl$3.doInKeyspace(MutatorImpl.java:248)\n        at me.prettyprint.cassandra.model.MutatorImpl$3.doInKeyspace(MutatorImpl.java:245)\n        at me.prettyprint.cassandra.model.KeyspaceOperationCallback.doInKeyspaceAndMeasure(KeyspaceOperationCallback.java:20)\n        at me.prettyprint.cassandra.model.ExecutingKeyspace.doExecute(ExecutingKeyspace.java:85)\n        at me.prettyprint.cassandra.model.MutatorImpl.execute(MutatorImpl.java:245)\n        at me.prettyprint.cassandra.model.MutatorImpl.insert(MutatorImpl.java:79)\n        at org.apache.gora.cassandra.store.CassandraClient.addSubColumn(CassandraClient.java:172)\n        at org.apache.gora.cassandra.store.CassandraStore.addOrUpdateField(CassandraStore.java:360)\n        at org.apache.gora.cassandra.store.CassandraStore.flush(CassandraStore.java:212)\n        at org.apache.gora.mapreduce.GoraRecordWriter.write(GoraRecordWriter.java:65)\n        at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:587)\n        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)\n        at org.apache.nutch.fetcher.FetcherReducer$FetcherThread.output(FetcherReducer.java:663)\n        at org.apache.nutch.fetcher.FetcherReducer$FetcherThread.run(FetcherReducer.java:557)\nCaused by: InvalidRequestException(why:column name must not be empty)\n        at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:19479)\n        at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:1035)\n        at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:1009)\n        at me.prettyprint.cassandra.service.KeyspaceServiceImpl$1.execute(KeyspaceServiceImpl.java:95)\n        ... 20 more",
        "Issue Links": [
            "/jira/browse/GORA-210",
            "/jira/browse/GORA-211"
        ]
    },
    "NUTCH-1535": {
        "Key": "NUTCH-1535",
        "Summary": "Crawl crashes with java.io.exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Adam89",
        "Created": "22/Feb/13 12:05",
        "Updated": "12/Mar/13 12:54",
        "Resolved": "12/Mar/13 12:54",
        "Description": "I started a crawl website using the command line with nutch 1.6 and it crashed after starting indexing. This is the error I get:\n2013-02-20 19:34:05,335 INFO  solr.SolrWriter - Indexing 5 documents\n2013-02-20 19:34:05,685 WARN  mapred.LocalJobRunner - job_local_0019\norg.apache.solr.common.SolrException: Bad Request\nBad Request\nrequest: http://localhost:8983/solr/update?wt=javabin&version=2\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n\tat org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n\tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n\tat org.apache.nutch.indexer.solr.SolrWriter.close(SolrWriter.java:142)\n\tat org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:48)\n\tat org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:466)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:530)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\n2013-02-20 19:34:05,956 ERROR solr.SolrIndexer - java.io.IOException: Job failed!\n2013-02-20 19:34:05,959 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: starting at 2013-02-20 19:34:05\n2013-02-20 19:34:05,960 INFO  solr.SolrDeleteDuplicates - SolrDeleteDuplicates: Solr url: http://localhost:8983/solr/\n2013-02-20 19:34:06,180 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2013-02-20 19:34:06,180 WARN  mapred.LocalJobRunner - job_local_0020\njava.lang.NullPointerException\n\tat org.apache.hadoop.io.Text.encode(Text.java:388)\n\tat org.apache.hadoop.io.Text.set(Text.java:178)\n\tat org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:270)\n\tat org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat$1.next(SolrDeleteDuplicates.java:241)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:236)\n\tat org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:216)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:48)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)\nI'm new in nutch and tried to crawl some webpages and index it to solr. I'm asking for explanation that is easy to understand. I'll be thankfull for any help.",
        "Issue Links": []
    },
    "NUTCH-1536": {
        "Key": "NUTCH-1536",
        "Summary": "Ant build file has hardcoded conf dir location",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "zm",
        "Created": "26/Feb/13 09:16",
        "Updated": "22/May/13 03:54",
        "Resolved": "26/Feb/13 19:54",
        "Description": "build.xml loads property files to change default locations of various dirs, including \"conf\". There is one spot in build.xml which has conf hardcoded instead of reading it off conf.dir property.",
        "Issue Links": []
    },
    "NUTCH-1537": {
        "Key": "NUTCH-1537",
        "Summary": "Legacy metadata package needs to take advantage of Apache Tika metadata package more.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Feb/13 23:11",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "In Nutch, classes from the metadata package are being used in quite a number of places. It is not currently being used to reflect the work going on in Apache Tika and we need to better leverage the vocabularies available to us from the dependency on Apache Tika.\nThe introduction of TikaCoreProperties in Tika 1.2 is not currently leveraged in Nutch. This is just one example of an improved way for us to add metadata to Nutch documents.",
        "Issue Links": []
    },
    "NUTCH-1538": {
        "Key": "NUTCH-1538",
        "Summary": "tuning of loaded fields during fetcherJob start-up",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.1",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Roland von Herget",
        "Created": "04/Mar/13 12:41",
        "Updated": "08/Jan/23 19:32",
        "Resolved": "08/Jan/23 19:32",
        "Description": "Main problem is, nutch is loading nearly every row & column from DB during startup of a fetcherJob when fetcher.parse=true.\nA parserJob needs e.g. the CONTENT field from db, to parse.\nThe fetcherJob adds all fields of the parserJob to it's needed fields, if running with fetcher.parse=true. [FetcherJob.getFields()]\nIf the nutch configuration saves all fetched data to DB (fetcher.store.content=true) you'll end up loading GBs of unused content during fetcherJob start-up.",
        "Issue Links": []
    },
    "NUTCH-1539": {
        "Key": "NUTCH-1539",
        "Summary": "Implement the Hypertext Induced Topic Search (HITS) algorithm in Nutch",
        "Type": "Bug",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "linkdb",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "05/Mar/13 03:27",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "In my Summer 2010 CSCI 572: Search Engines and Information Retrieval class, my student Yongqiang Li and I implemented the HITS algorithm in Nutch based on Jon Kleinberg's paper:\nAuthoritative Sources in a Hyperlinked Environment\nhttp://dl.acm.org/citation.cfm?id=324140\nI'll put up the code we had shortly.",
        "Issue Links": []
    },
    "NUTCH-1540": {
        "Key": "NUTCH-1540",
        "Summary": "Add Gora buffered read and write maximum limits to nutch-default.xml configuration.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "06/Mar/13 22:37",
        "Updated": "22/May/13 03:53",
        "Resolved": "06/Mar/13 22:40",
        "Description": "I've been experimenting by using this via the command line for some time. It is starting to annoy me, so I wanted to make this more accessible to us all.\nYou can now easily set this in nutch-site.xml\nPatch coming up.",
        "Issue Links": []
    },
    "NUTCH-1541": {
        "Key": "NUTCH-1541",
        "Summary": "Indexer plugin to write CSV",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.15",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "06/Mar/13 22:54",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Jul/18 09:34",
        "Description": "With the new pluggable indexer a simple plugin would be handy to write configurable fields into a CSV file - for further analysis or just for export.",
        "Issue Links": [
            "/jira/browse/NUTCH-1707",
            "https://github.com/apache/nutch/pull/294"
        ]
    },
    "NUTCH-1542": {
        "Key": "NUTCH-1542",
        "Summary": "adddays param for generator not present in 2.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "generator",
        "Assignee": "Tejas Patil",
        "Reporter": "Tejas Patil",
        "Created": "07/Mar/13 04:37",
        "Updated": "22/May/13 03:54",
        "Resolved": "11/Mar/13 02:15",
        "Description": "In 1.x, Generator had this param which could be used as a hack to crawl urls which were due to fetch in future. In 2.x, this param is not present. Its not clear why this was not ported from 1.x to 2.x. Unless it was left out for a strong reason, we should have it in 2.x as well.",
        "Issue Links": []
    },
    "NUTCH-1543": {
        "Key": "NUTCH-1543",
        "Summary": "Display consistent usage of DBUpdaterJob with 1.X",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": "lufeng",
        "Reporter": "lufeng",
        "Created": "12/Mar/13 07:58",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "As Lewis suggested in NUTCH-1393, i will add some help information in DbUpdaterJob.",
        "Issue Links": []
    },
    "NUTCH-1544": {
        "Key": "NUTCH-1544",
        "Summary": "Nutch crawls only first site from seed list",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.6",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Adam89",
        "Created": "12/Mar/13 13:00",
        "Updated": "22/May/13 03:53",
        "Resolved": "13/Mar/13 16:14",
        "Description": "Nutch crawls only first site from seed list and then finish. It doesn't give any error or something else. I'm leaving my log and regex urlfilter.\nRegards",
        "Issue Links": []
    },
    "NUTCH-1545": {
        "Key": "NUTCH-1545",
        "Summary": "capture batchId and remove references to segments in 2.x crawl script.",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "None",
        "Assignee": "lufeng",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Mar/13 20:05",
        "Updated": "30/May/13 15:13",
        "Resolved": "30/May/13 14:32",
        "Description": "The concept of segment is replaced by batchId in 2.x\nI'm currently getting rid of segments references in 2.x\nThis issue was flagged up and separate from NUTCH-1532 which I am working on.",
        "Issue Links": []
    },
    "NUTCH-1546": {
        "Key": "NUTCH-1546",
        "Summary": "Parsechecker and redirection",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "1.6,                                            2.1,                                            2.2",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Canan Girgin",
        "Created": "26/Mar/13 08:00",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "When \"bin/nutch parsechecker\" command(Nutch 2.1) used , it works fine.But if there is a redirected url, parseFilters turns wrong results. Because parse text contains redirect descriptions.\nIn case of a redirect, parsechecker should follow redirects.",
        "Issue Links": []
    },
    "NUTCH-1547": {
        "Key": "NUTCH-1547",
        "Summary": "BasicIndexingFilter - Problem to index full title",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "indexer",
        "Assignee": "lufeng",
        "Reporter": "Gustavo Rauber",
        "Created": "26/Mar/13 13:12",
        "Updated": "22/May/13 03:54",
        "Resolved": "28/Mar/13 13:10",
        "Description": "I have faced this issue when trying to index the entire title, just like the content, configuring its value on nutch-default.xml to -1 (indexer.max.title.length). I think the behavior should be the same as the content.\nIf you would like to fix it, just replace the line number 90:\nif (title.length() > MAX_TITLE_LENGTH) {      // truncate title if needed\nby this one:\nif (MAX_TITLE_LENGTH > -1 && title.length() > MAX_TITLE_LENGTH) {      // truncate title if needed\nStack Trace:\njava.lang.StringIndexOutOfBoundsException: String index out of range: -1\n\tat java.lang.String.substring(String.java:1937)\n\tat org.apache.nutch.indexer.basic.BasicIndexingFilter.filter(BasicIndexingFilter.java:91)\n\tat org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:109)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:272)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:53)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:519)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:420)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:260)\nCheers.",
        "Issue Links": []
    },
    "NUTCH-1548": {
        "Key": "NUTCH-1548",
        "Summary": "Move all Utils classes into Utils packages & dedup Utils generally",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "2.5",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Mar/13 19:37",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Currently we have IndexUtils, ProtocolStatusUtils and others hanging around within (IMHO) the wrong classes.\nWe should move these to the utils packages.\nWe also seem to be maintaining classes such as timing utils, etc.\nSurely to god, in this day and age, one or more of the dependencies we already pull containing something similar which we can use. We should delegate this to our dependency implementations, and contribute it to commons projects if it does not already exist.",
        "Issue Links": []
    },
    "NUTCH-1549": {
        "Key": "NUTCH-1549",
        "Summary": "Fix deprecated use of Tika MimeType API in o.a.n.util.MimeUtil",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "build",
        "Assignee": "Tejas Patil",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Mar/13 20:00",
        "Updated": "22/May/13 03:53",
        "Resolved": "30/Apr/13 20:53",
        "Description": "There are still problems with this issue (which actually builds on from the work undertaken by markus17 on NUTCH-1230). I meant to mention and address them ages ago and they recently resurfaced Whilst tackling NUTCH-1273. The following code is deprecated \n\n170 \t// If no mime-type header, or cannot find a corresponding registered\n171 \t// mime-type, then guess a mime-type from the url pattern\n172 \ttype = this.mimeTypes.getMimeType(url) != null ? this.mimeTypes\n173 \t.getMimeType(url) : type;\n174 \t}\n175 \t\n\n\nThanks to Nick Burch over on Tika, I attempted to upgrade it to the following\n\nString mt = getMimeType(url);\ntype = mt != null ? mt : type;\n\n\nWhich will of course not compile as the javac rightly flags incompatible types as the error.\nThis is present in both trunk and 2.x and we should address it once and for all.",
        "Issue Links": [
            "/jira/browse/NUTCH-1273",
            "/jira/browse/NUTCH-1230"
        ]
    },
    "NUTCH-1550": {
        "Key": "NUTCH-1550",
        "Summary": "xercesImpl and xmlParserAPIs (org.apache.xml) packages and classes only used in three Nutch classes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Invalid",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build,                                            parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Mar/13 23:50",
        "Updated": "22/May/13 03:54",
        "Resolved": "29/Mar/13 00:27",
        "Description": "DOMSerializerImpl from xerces is deprecated in our current artifact. It is replaced by the (still ancient but slightly newer org.apache.xml.serializer.dom3.LSSerializerImpl in [0]). \nUpon closer inspection it seems that find . | xargs grep \"org.apache.xml\" * only pulled up DOMBuilder, XMLChatacterRecognizer and DOMContentUtilsTest as the places where such classes are used.\nI am confused as to why they are included as primary dependencies within Nutch. Either these XML specific dependencies should be restricted dependencies to parse-html or else they should be removed and replaced by the new artifact [0].  \n[0] http://search.maven.org/#artifactdetails|xalan|serializer|2.7.1|jar",
        "Issue Links": []
    },
    "NUTCH-1551": {
        "Key": "NUTCH-1551",
        "Summary": "Improve WebTableReader field order and display batchId",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "crawldb",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "30/Mar/13 22:46",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/Apr/13 00:33",
        "Description": "I've made slight modifications to WebTableReader to dump a more appropriately structured fields when dumping the webdb. The structure now more closely reflects the set out of the webpage.avsc file.\nAdditionally, I've added the batchId however for backwards compatability with existing webdb's this is only appended to the string buffer if it is not null value.",
        "Issue Links": []
    },
    "NUTCH-1552": {
        "Key": "NUTCH-1552",
        "Summary": "possibility of a NPE in index-more plugin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "kaveh minooie",
        "Created": "02/Apr/13 22:00",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/May/13 22:07",
        "Description": "in line 203 of src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java the code attempt to read from variable contentType even thou it is possible for it to be null. for me, it happened when I tried to index  http://www.pscars.com/",
        "Issue Links": [
            "/jira/browse/NUTCH-956"
        ]
    },
    "NUTCH-1553": {
        "Key": "NUTCH-1553",
        "Summary": "Property 'indexer.delete.robots.noindex' not working when using parser-html.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.13",
        "Component/s": "indexer,                                            parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Alfonso Presa",
        "Created": "04/Apr/13 14:09",
        "Updated": "01/Jul/16 14:00",
        "Resolved": "01/Jul/16 13:17",
        "Description": "May be I'm doing something wrong, but it seems to me that NUTCH-1434 patch only works when using tika's parser. When using parser-html, \"robots\" metatag is only populated if parse-metatags plugin is enabled and it's done with the prefix \"metatag.\". So parseData.getMeta(\"robots\") returns nothing if not using tika.\nI guess the simplest solution would be to provide a fallback in case parseData.getMeta(\"robots\") is null and then get parseData.getMeta(\"metatag.robots\") in that case.\nAlso dependency of this property with parse-metadata plugin when using parse-html would be something interesting to document somewhere... (nutch-default.xml?)\nThanks!",
        "Issue Links": [
            "/jira/browse/NUTCH-1434"
        ]
    },
    "NUTCH-1554": {
        "Key": "NUTCH-1554",
        "Summary": "org.apache.nutch.net.protocols.HttpDateFormat should NOT be Locale.US aware",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Apr/13 21:26",
        "Updated": "22/May/13 03:53",
        "Resolved": "08/Apr/13 23:58",
        "Description": "I assume this is legacy code.\nCurrently the above class is Locale specific and really should not be.",
        "Issue Links": []
    },
    "NUTCH-1555": {
        "Key": "NUTCH-1555",
        "Summary": "Move to commons-cli for command line parsing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": "lufeng",
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Apr/13 23:38",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "I just accidentally passed in the following argument to parser job\n\nlaw@CEE279Law3-Linux:~/Downloads/asf/2.x/runtime/local$ ./bin/nutch parse updatedb\nParserJob: starting\nParserJob: resuming:\tfalse\nParserJob: forced reparse:\tfalse\nParserJob: batchId:\tupdatedb\nParserJob: success\n\n\nThis is a bug for sure",
        "Issue Links": []
    },
    "NUTCH-1556": {
        "Key": "NUTCH-1556",
        "Summary": "enabling updatedb to accept batchId",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "kaveh minooie",
        "Created": "10/Apr/13 08:12",
        "Updated": "01/May/14 16:01",
        "Resolved": "04/Dec/13 19:24",
        "Description": "So the idea here is to be able to run updatedb and fetch for different batchId simultaneously. I put together a patch. it seems to be working ( it does skip the rows that do not match the batchId), but I am worried if and how it might affect the sorting in the reduce part. anyway check it out. \nit also change the command line usage to this:\nUsage: DbUpdaterJob (<batchId> | -all) [-crawlId <id>]",
        "Issue Links": [
            "/jira/browse/NUTCH-1740",
            "/jira/browse/NUTCH-1632",
            "/jira/browse/NUTCH-1679",
            "/jira/browse/NUTCH-1667"
        ]
    },
    "NUTCH-1557": {
        "Key": "NUTCH-1557",
        "Summary": "File extraction and classification for any MIME types from segments",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Chao Yan",
        "Created": "12/Apr/13 06:52",
        "Updated": "13/Apr/13 01:22",
        "Resolved": null,
        "Description": "Basic idea is to implement a file dumper as a plugin to extra files from Nutch SequenceFiles. The file dumper should detect the content type and dump them into different directories based on content type. The extracted file will be renamed based on information from URL, metadata, and even content. File name should be globally unique with the correct file extension. The file dumper should also allow user to specify the formats of the files they want, and can be extended to specify any criteria on the extracted files. A more advanced goal is to implement it with MapReduce.",
        "Issue Links": []
    },
    "NUTCH-1558": {
        "Key": "NUTCH-1558",
        "Summary": "CharEncodingForConversion in ParseData's ParseMeta, not in ParseData's ContentMeta",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "17/Apr/13 14:16",
        "Updated": "15/Nov/13 17:56",
        "Resolved": "15/Nov/13 17:56",
        "Description": "This patch from GitHub user ysc fixes two bugs related to character encoding:\n\nCharEncodingForConversion in ParseData's ParseMeta, not in ParseData's ContentMeta\nif http response Header Content-Type return wrong coding\uff0cthen get coding from the original content of the page\n\nInformation about this pull request is here: http://s.apache.org/VOP",
        "Issue Links": []
    },
    "NUTCH-1559": {
        "Key": "NUTCH-1559",
        "Summary": "parse-metatags duplicates extracted metatags",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.17",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Apr/13 21:12",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "07/Nov/19 08:26",
        "Description": "If the plugin parse-metatags is used in combination with parse-tika, the extracted metatags (the pairs <name, value>) are duplicated.\nThe metatags are found twice in parse.getData().getParseMeta() and in metaTags.getGeneralTags(). Is this necessary? Maybe we should fix parse-tika in this point?",
        "Issue Links": [
            "/jira/browse/NUTCH-2567",
            "/jira/browse/NUTCH-2567",
            "https://github.com/apache/nutch/pull/481"
        ]
    },
    "NUTCH-1560": {
        "Key": "NUTCH-1560",
        "Summary": "index-metadata to add all values of multivalued metadata",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.7",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Apr/13 21:37",
        "Updated": "08/Jul/13 11:34",
        "Resolved": "13/Jun/13 20:50",
        "Description": "MetadataIndexer does not add all values of multivalued meta tags. This causes the fix for NUTCH-1467 to be almost useless.",
        "Issue Links": [
            "/jira/browse/NUTCH-1607",
            "/jira/browse/NUTCH-1583"
        ]
    },
    "NUTCH-1561": {
        "Key": "NUTCH-1561",
        "Summary": "improve usability of parse-metatags and index-metadata",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.9",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Apr/13 21:54",
        "Updated": "30/Jul/14 10:10",
        "Resolved": "30/Jul/14 08:55",
        "Description": "Usually, the plugins parse-metatags and index-metadata are used in combination: the former \"extracts\" meta tags, the latter adds the extracted tags as fields to the index. \nConfiguration of the two plugins differs which causes pitfalls and reduces the usability (see example config):\n\nthe property \"metatags.names\" of parse-metatags uses ';' as separator instead of ',' used by index-metadata\nmeta tags have to be lowercased in index-metadata\n\n\n<property>\n  <name>metatags.names</name>\n  <value>DC.creator;DCTERMS.bibliographicCitation</value>\n</property>\n\n<property>\n  <name>index.parse.md</name>\n  <value>metatag.dc.creator,metatag.dcterms.bibliographiccitation</value>\n</property>",
        "Issue Links": []
    },
    "NUTCH-1562": {
        "Key": "NUTCH-1562",
        "Summary": "Order of execution for scoring filters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "18/Apr/13 10:01",
        "Updated": "01/May/14 06:23",
        "Resolved": "07/Oct/13 10:08",
        "Description": "The documentation in nutch-default.xml states that :\n\n<property>\n  <name>scoring.filter.order</name>\n  <value></value>\n  <description>The order in which scoring filters are applied.\n  This may be left empty (in which case all available scoring\n  filters will be applied in the order defined in plugin-includes\n  and plugin-excludes), or a space separated list of implementation\n  classes.\n  </description>\n</property>\nhowever if no order is specified the filters are ordered randomly and not in the order defined in plugin-includes.\nThe other *order parameters (e.g. urlfilter.order) have a different documentation and \"are loaded and applied in system defined order\" which corresponds to what the code does.\nThe patch attached is for 1.x and puts the code in accordance with the documentation by ordering the filters according to the order of the plugins, which gives users more control without having to specify the classes explicitly in scoring.filter.order.\nWe could extend the same idea to the other *order params.",
        "Issue Links": []
    },
    "NUTCH-1563": {
        "Key": "NUTCH-1563",
        "Summary": "FetchSchedule#getFields is never used by GeneraterJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "generator",
        "Assignee": "lufeng",
        "Reporter": "lufeng",
        "Created": "18/Apr/13 15:23",
        "Updated": "29/May/13 15:41",
        "Resolved": "29/May/13 15:40",
        "Description": "The method of getFields in FetchSchedule if never used, so if user extends the FetchSchedule and want to get some fields of WebPage, it always return null.",
        "Issue Links": []
    },
    "NUTCH-1564": {
        "Key": "NUTCH-1564",
        "Summary": "AdaptiveFetchSchedule: sync_delta forces immediate refetch for documents not modified",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "None",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "19/Apr/13 12:11",
        "Updated": "15/Jul/14 09:25",
        "Resolved": null,
        "Description": "In a continuous crawl with adaptive fetch scheduling documents not modified for a longer time may be fetched in every cycle.\nA continous crawl is run daily with a 3 cycles and the following scheduling intervals (freshness matters):\n\ndb.fetch.schedule.class = org.apache.nutch.crawl.AdaptiveFetchSchedule\ndb.fetch.schedule.adaptive.sync_delta   = true (default)\ndb.fetch.schedule.adaptive.sync_delta_rate = 0.3 (default)\ndb.fetch.interval.default               = 172800 (2 days)\ndb.fetch.schedule.adaptive.min_interval =  86400 (1 day)\ndb.fetch.schedule.adaptive.max_interval = 604800 (7 days)\ndb.fetch.interval.max                   = 604800 (7 days)\n\n\nAt Apr 18 a URL is generated and fetched (from segment dump):\n\nCrawl Generate::\nStatus: 2 (db_fetched)\nFetch time: Mon Apr 15 19:43:22 CEST 2013\nModified time: Tue Mar 19 01:07:42 CET 2013\nRetries since fetch: 0\nRetry interval: 604800 seconds (7 days)\n\nCrawl Fetch::\nStatus: 33 (fetch_success)\nFetch time: Thu Apr 18 01:23:51 CEST 2013\nModified time: Tue Mar 19 01:07:42 CET 2013\nRetries since fetch: 0\nRetry interval: 604800 seconds (7 days)\n\n\nRunning CrawlDb update results in a next fetch time in the past (which forces an immediate refetch in the next cycle):\n\nStatus: 6 (db_notmodified)\nFetch time: Tue Apr 16 01:37:00 CEST 2013\nModified time: Tue Mar 19 01:07:42 CET 2013\nRetries since fetch: 0\nRetry interval: 604800 seconds (7 days)\n\n\nThis behavior is caused by the sync_delta calculation in AdaptiveFetchSchedule:\n\n  if (SYNC_DELTA) {\n    // try to synchronize with the time of change\n    long delta = (fetchTime - modifiedTime) / 1000L;\n    if (delta > interval) interval = delta;\n    refTime = fetchTime - Math.round(delta * SYNC_DELTA_RATE * 1000);\n  }\n  if (interval < MIN_INTERVAL) {\n    interval = MIN_INTERVAL;\n  } else if (interval > MAX_INTERVAL) {\n    interval = MAX_INTERVAL;\n  }\n...\ndatum.setFetchTime(refTime + Math.round(interval * 1000.0));\n\n\ndelta is 30 days (Apr 18 - Mar 19). refTime is then 9 days in the past (delta * 0.3). After adding interval (adjusted to MAX_INTERVAL = 7 days) to refTime the next fetch \"should\" take place 2 days in the past (Apr 16).\nAccording to the javadoc (if understood right), there are two aims of the sync_delta if we know that a document hasn't been modified for long:\n\nincrease the fetch interval immediately (not step by step)\nbecause we expect the document to be changed within the adaptive interval (but it hasn't), we shift the \"reference time\", i.e. we expect a change soon.\n\nThese two aims are somehow in contradiction. In any case, the next fetch time should be always within the range of (currentFetchTime + MIN_INTERVAL) and (currentFetchTime + MAX_INTERVAL) and never in the past.\nThis problem has been noted by pascaldimassimo in 1 and 2.",
        "Issue Links": [
            "/jira/browse/NUTCH-1502"
        ]
    },
    "NUTCH-1565": {
        "Key": "NUTCH-1565",
        "Summary": "Proper downloads page for Nutch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "25/Apr/13 02:28",
        "Updated": "22/May/13 03:53",
        "Resolved": "25/Apr/13 05:09",
        "Description": "As per recent discussion at INFRA and consequently on our dev@ list, I've produced a downloads page for us based on the one we have over at Gora.\nMy syntax is not correct in line with Forrests DTD and I can;t for the life of me find out what the hell is wrong. If someone could have a look, resolve and publish it would be greatly appreciated.\nLewis",
        "Issue Links": []
    },
    "NUTCH-1566": {
        "Key": "NUTCH-1566",
        "Summary": "bin/nutch to allow whitespace in paths",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "28/Apr/13 21:19",
        "Updated": "05/Jul/14 21:52",
        "Resolved": "05/Jul/14 21:15",
        "Description": "bin/nutch and bin/crawl choke if a path contains white space, eg, if JAVA_HOME is \"C:\\Program Files\\jdk\". If you don't have the permission to change the path it is impossible to run Nutch. This has been reported frequently (1, 2, and 3), see also NUTCH-19.",
        "Issue Links": []
    },
    "NUTCH-1567": {
        "Key": "NUTCH-1567",
        "Summary": "More useful logging for batch id (null) scenario",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher,                                            indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "30/Apr/13 16:45",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Recently a good bit of conversation surrounded this topic[0].\nThe logging IMHO is incorrect. It always shows null (representing a Mark which is not there) where I think it should represent the batchId identifier for the given page.\nThis is currently the case within FetcherJob, ParserJob and IndexerJob.\n[0] http://www.mail-archive.com/user%40nutch.apache.org/msg09422.html",
        "Issue Links": []
    },
    "NUTCH-1568": {
        "Key": "NUTCH-1568",
        "Summary": "port pluggable indexing architecture to 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.3",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "03/May/13 16:37",
        "Updated": "01/May/14 06:23",
        "Resolved": "15/Jan/14 12:03",
        "Description": "I would like to port the work done by Julien on NUTCH-1047 to 2.x. This issue should track that. It would be nice to do the upgrade in NUTCH-1486 before we do the upgrade so that people can get using with solr 4.x ASAP.",
        "Issue Links": [
            "/jira/browse/NUTCH-1486",
            "/jira/browse/NUTCH-1047"
        ]
    },
    "NUTCH-1569": {
        "Key": "NUTCH-1569",
        "Summary": "Upgrade 2.x to Gora 0.3",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.2",
        "Component/s": "build,                                            storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/May/13 18:49",
        "Updated": "24/May/13 04:20",
        "Resolved": "23/May/13 20:43",
        "Description": "We just released the Maven artifacts and I would like to upgrade before we push the RC for 2.2 \nPatch coming up",
        "Issue Links": [
            "/jira/browse/NUTCH-1572"
        ]
    },
    "NUTCH-1570": {
        "Key": "NUTCH-1570",
        "Summary": "Add filtering capability to Datastore Queries",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.5",
        "Component/s": "REST_api,                                            storage",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "10/May/13 19:07",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "For some time this issue has been discussed on various lists.\nWhen doing the upgrade of the Gora dependencies in NUTCH-1569, I  stumbled across a comment within o.a.n.api.DbReader#Iterator\n\n  public Iterator<Map<String,Object>> iterator(String[] fields, String startKey, String endKey,\n      String batchId) throws Exception {\n    Query<String,WebPage> q = store.newQuery();\n    String[] qFields = fields;\n    if (fields != null) {\n      HashSet<String> flds = new HashSet<String>(Arrays.asList(fields));\n      // remove \"url\"\n      flds.remove(\"url\");\n      if (flds.size() > 0) {\n        qFields = flds.toArray(new String[flds.size()]);\n      } else {\n        qFields = null;\n      }\n    }\n    q.setFields(qFields);\n    if (startKey != null) {\n      q.setStartKey(startKey);\n      if (endKey != null) {\n        q.setEndKey(endKey);\n      }\n    }\n    Result<String,WebPage> res = store.execute(q);\n    *XXX we should add the filtering capability to Query*\n    return new DbIterator(res, fields, batchId);\n  }\n\n\nI will link this issue to something over on Gora once we get around to the implementation.",
        "Issue Links": []
    },
    "NUTCH-1571": {
        "Key": "NUTCH-1571",
        "Summary": "SolrInputSplit doesn't implement Writable and crawl script doesn't pass crawlId to generate and updatedb tasks",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2.1",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "jefferyyuan",
        "Created": "16/May/13 03:22",
        "Updated": "27/Jun/13 17:13",
        "Resolved": "25/Jun/13 20:20",
        "Description": "I met tow issues when I run crawl script from 2.x brunch.\n1. It throws exception when run solrdedup task:\nException in thread \"main\" java.lang.NullPointerException\n        at org.apache.hadoop.io.serializer.SerializationFactory.getSerializer(SerializationFactory.java:73)\n        at org.apache.hadoop.mapreduce.split.JobSplitWriter.writeNewSplits(JobSplitWriter.java:123)\n        at org.apache.hadoop.mapreduce.split.JobSplitWriter.createSplitFiles(JobSplitWriter.java:74)\n        at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:968)\n        at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:979)\n        at org.apache.hadoop.mapred.JobClient.access$600(JobClient.java:174)\n        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:897)\n        at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:850)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)\n        at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:850)\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:500)\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:530)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.dedup(SolrDeleteDuplicates.java:371)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.run(SolrDeleteDuplicates.java:381)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates.main(SolrDeleteDuplicates.java:391)\nDebugged the code, and found this is because SolrInputSplit class doesn't implement Writable interface, so I made the change to implement the interface and the two methods: readFields and write.\n2. Nothing is really pushed to remote solr server. Looked at the code, and found out this is because tasks: generate and updatedb doesn't use crawlId parameter: added \"-crawlId $CRAWL_ID\" to them and crawl script works well now.\nAlso seems generate task doesn't use paramters: $CRAWL_ID/crawldb $CRAWL_ID/segments.",
        "Issue Links": [
            "/jira/browse/NUTCH-992"
        ]
    },
    "NUTCH-1572": {
        "Key": "NUTCH-1572",
        "Summary": "Nutch 2.x should use o.a.g.mem.store.MemStore for testing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.3.1",
        "Component/s": "test",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/May/13 20:34",
        "Updated": "20/Sep/15 12:53",
        "Resolved": "20/Sep/15 12:53",
        "Description": "As far as I am aware, there is really no need to be setting up and using hsqldb resources for testing Nutch + Gora functionality fro our tests when there is a MemStore available in Gora.\nIn particular TestInjector, TestGenerator, TestFetcher and TestGoraStorage all use gora-sql-incubating-0.1.1 and subsequently a HSQLDB server for tests... this is pretty unnecessary.\nIt also happens to be the fact that as of Gora 0.3, the above gora-sql artifact is now deprecated indefinitely.",
        "Issue Links": [
            "/jira/browse/GORA-225",
            "/jira/browse/NUTCH-1946",
            "/jira/browse/NUTCH-1569"
        ]
    },
    "NUTCH-1573": {
        "Key": "NUTCH-1573",
        "Summary": "Upgrade to most recent JUnit 4.x to improve test flexibility",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "build,                                            test",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/May/13 21:29",
        "Updated": "22/May/13 03:53",
        "Resolved": "19/May/13 21:36",
        "Description": "I wanted to try using the @Ignore functionality within JUnit, however I don't think it is available in the current JUnit version we use in Nutch. We should upgrade.",
        "Issue Links": []
    },
    "NUTCH-1574": {
        "Key": "NUTCH-1574",
        "Summary": "Crawling parent directories for http(s) protocol",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Bug",
        "Affects Version/s": "1.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Antoinette",
        "Created": "20/May/13 16:01",
        "Updated": "08/Jan/23 19:30",
        "Resolved": "08/Jan/23 19:30",
        "Description": "I am looking for a fix to prevent indexing the list of files crawled via http(s) protocol. For example: I have 10 files in a directory. Nutch finds and Solr indexes 11, the first being a list of the other 10 files.",
        "Issue Links": []
    },
    "NUTCH-1575": {
        "Key": "NUTCH-1575",
        "Summary": "support solr authentication in nutch 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.2",
        "Component/s": "indexer",
        "Assignee": "lufeng",
        "Reporter": "lufeng",
        "Created": "22/May/13 15:09",
        "Updated": "03/Jun/13 13:59",
        "Resolved": "29/May/13 15:47",
        "Description": "can solr authentication in nutch 2.x like 1.x",
        "Issue Links": []
    },
    "NUTCH-1576": {
        "Key": "NUTCH-1576",
        "Summary": "Need to keep hotStore.flush() exception catching",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.2",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "James Sullivan",
        "Created": "31/May/13 01:31",
        "Updated": "01/Jun/13 03:21",
        "Resolved": "31/May/13 19:55",
        "Description": "Still need exception checking for hoststorelflush() for those who have to use gora-core 0.2.1 otherwise Nutch 2.x will not compile.\n<!-- Uncomment this to use SQL as Gora backend. It should be noted that the \n    gora-sql 0.1.1-incubating artifact is NOT compatable with gora-core 0.3. Users should \n    downgrade to gora-core 0.2.1 in order to use SQL as a backend. -->\nIndex: src/java/org/apache/nutch/host/HostDb.java\n===================================================================\n\u2014 java/workspace/2.x/src/java/org/apache/nutch/host/HostDb.java\t(revision 1487824)\n+++ java/workspace/2.x/src/java/org/apache/nutch/host/HostDb.java\t(working copy)\n@@ -87,7 +87,11 @@\n             CacheHost removeFromCacheHost = notification.getValue();\n             if (removeFromCacheHost != NULL_HOST) {\n               if (removeFromCacheHost.timestamp < lastFlush.get()) {\n\nhostStore.flush();\n+                try \n{\n+                  hostStore.flush();\n+                }\n catch (IOException e) \n{\n+                  throw new RuntimeException(e);\n+                }\n                 lastFlush.set(System.currentTimeMillis());\n               }\n             }",
        "Issue Links": []
    },
    "NUTCH-1577": {
        "Key": "NUTCH-1577",
        "Summary": "Add target for creating eclipse project",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.1",
        "Fix Version/s": "1.7,                                            2.2",
        "Component/s": "None",
        "Assignee": "Tejas Patil",
        "Reporter": "Tejas Patil",
        "Created": "31/May/13 10:09",
        "Updated": "15/Dec/13 01:19",
        "Resolved": "31/May/13 22:36",
        "Description": "Currently, loading Nutch source code in Eclipse as a project is cumbersome and involves lot of manual steps as given over wiki. It would be great to automate this. Adding a ant target to do that would remove burden off from developers.",
        "Issue Links": []
    },
    "NUTCH-1578": {
        "Key": "NUTCH-1578",
        "Summary": "Upgrade to Hadoop 1.2.0",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7,                                            2.2.1",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/May/13 13:18",
        "Updated": "11/Oct/19 15:36",
        "Resolved": "27/Jun/13 17:10",
        "Description": "Hadoop 1.2.0 finally has the ability to run mappers in parallel when running in local mode. In trunk at least the generator seems to run slightly faster.",
        "Issue Links": []
    },
    "NUTCH-1579": {
        "Key": "NUTCH-1579",
        "Summary": "NPE when using solr indexing",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.3",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "shantharam anandkumar",
        "Created": "31/May/13 15:56",
        "Updated": "31/May/13 22:14",
        "Resolved": "31/May/13 22:14",
        "Description": "NPE while running crawl with solar\nhere is command \nbin/nutch crawl urls/ -solr http://localhost:8983/solr  -depth 3 -topN 20\nException in thread \"main\" java.lang.NullPointerException\n\tat java.util.Hashtable.put(Hashtable.java:394)\n\tat java.util.Properties.setProperty(Properties.java:143)\n\tat org.apache.hadoop.conf.Configuration.set(Configuration.java:438)\n\tat org.apache.nutch.indexer.IndexerJob.createIndexJob(IndexerJob.java:129)\n\tat org.apache.nutch.indexer.solr.SolrIndexerJob.run(SolrIndexerJob.java:44)\n\tat org.apache.nutch.crawl.Crawler.runTool(Crawler.java:69)\n\tat org.apache.nutch.crawl.Crawler.run(Crawler.java:194)\n\tat org.apache.nutch.crawl.Crawler.run(Crawler.java:252)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.Crawler.main(Crawler.java:261)",
        "Issue Links": []
    },
    "NUTCH-1580": {
        "Key": "NUTCH-1580",
        "Summary": "index-static returns object instead of value for index.static",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Antoinette",
        "Created": "06/Jun/13 11:10",
        "Updated": "27/Jun/13 21:14",
        "Resolved": "27/Jun/13 20:20",
        "Description": "index.static is trying to print out the string array and it's printing the string array object, not the contents of the array",
        "Issue Links": [
            "/jira/browse/NUTCH-1464"
        ]
    },
    "NUTCH-1581": {
        "Key": "NUTCH-1581",
        "Summary": "CrawlDB csv output to include metadata",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "13/Jun/13 11:39",
        "Updated": "02/Jul/13 09:50",
        "Resolved": "02/Jul/13 08:36",
        "Description": "Dumping the CrawlDB to CSV should include the CrawlDatum's metadata.",
        "Issue Links": []
    },
    "NUTCH-1582": {
        "Key": "NUTCH-1582",
        "Summary": "Garbage when microformats-reltag invoked in 2.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Jun/13 18:24",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "When I do a crawl of these pages with microformats-reltag activated, I get loads of garbage included within my dump of the webdb.\nhttp://www.amazon.com/Degree-Antiperspirant-Deodorant-Extreme-Blast/dp/B001ET769Y\nhttp://www.amazon.com/Cisco-WAP4410N-Wireless-N-Access-Point/dp/B001IYCMNA\n\nmetadata Rel-Tag :      \ufffd^A^@^B^@^@^@^Pget_range_slices^@^@^@^B^O^@^@^L^@^@^A^W^K^@^A^@^@^@(com.amazon.www:http/review/RZJZBDJMTYN4Y^O^@^B^L^@^@^@^B^L^@^B^K^@^A^@^@^@^Bil^O^@^B^L^@^@^@^B^K^@^A^@^@^@Jhttp://www.amazon.com/Cisco-WAP4410N-Wireless-N-Access-Point/dp/B001IYCMNA^K^@^B^@^@^@(Horrible Device, Two Years of Experience\n^@^C^@^D\ufffd]\ufffd\ufffd\ufffd\ufffd^@^K^@^A^@^@^@Qhttp://www.amazon.com/Degree-Antiperspirant-Deodorant-Extreme-Blast/dp/B001ET769Y^K^@^B^@^@^@(Horrible Device, Two Years of Experience\n^@^C^@^D\ufffd]\ufffd\ufffd\ufffd\ufffd^@^@^@^L^@^B^K^@^A^@^@^@^Bmk^O^@^B^L^@^@^@^A^K^@^A^@^@^@^Ddist^K^@^B^@^@^@^A1",
        "Issue Links": [
            "/jira/browse/NUTCH-1420"
        ]
    },
    "NUTCH-1583": {
        "Key": "NUTCH-1583",
        "Summary": "Headings does not support multiValued headings",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.7",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "18/Jun/13 13:21",
        "Updated": "20/Jun/13 09:44",
        "Resolved": "20/Jun/13 09:38",
        "Description": "Headings can now support multiple values since NUTCH-1560 and NUTCH-1467.",
        "Issue Links": [
            "/jira/browse/NUTCH-1467",
            "/jira/browse/NUTCH-1560"
        ]
    },
    "NUTCH-1584": {
        "Key": "NUTCH-1584",
        "Summary": "Port NUTCH-1405 Allow to overwrite CrawlDatum's with injected entries to 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "crawldb,                                            injector",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Jun/13 02:51",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "I was recently curious about what happens in 2.x when we inject similar but not identical seed lists in order to bootstrap a system.\nI started looking about and found NUTCH-1405.\nI think it would be great to port this concept to 2.x.\nThis issue should do exactly that.",
        "Issue Links": []
    },
    "NUTCH-1585": {
        "Key": "NUTCH-1585",
        "Summary": "Ensure duplicate tags do not exist in microformat-reltag tag set.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.2",
        "Fix Version/s": "1.7,                                            2.2.1",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Jun/13 03:25",
        "Updated": "27/Jun/13 17:14",
        "Resolved": "27/Jun/13 17:14",
        "Description": "A WebPage can have many many embedded tags and other such markup.\nCreating huge tag lists containing many many duplicates is counter productive to the process of parsing and extracting out such structure.\nWe should add a mechanism to only include single tag occurrences for the microformats-reltag parser.",
        "Issue Links": []
    },
    "NUTCH-1586": {
        "Key": "NUTCH-1586",
        "Summary": "Non-db_success records should have interval.max",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Jun/13 14:07",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "13/May/14 09:52",
        "Description": "When your default interval is low (e.g. when you start low using adaptive scheduling), records with redirect or gone status keep the default interval. There should be a switch to force 404's and redirects to use the max interval instead because these are usually the least interesting records for recrawling.",
        "Issue Links": []
    },
    "NUTCH-1587": {
        "Key": "NUTCH-1587",
        "Summary": "misspelled property \"threshold\" in conf/log4j.properties",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            2.2",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "20/Jun/13 08:03",
        "Updated": "01/May/14 06:22",
        "Resolved": "21/Nov/13 22:05",
        "Description": "Property \"threshold\" in conf/log4j.properties is misspelled (log4j.threshhold=ALL). Not critical, see HADOOP-7052 which also includes a working patch.",
        "Issue Links": []
    },
    "NUTCH-1588": {
        "Key": "NUTCH-1588",
        "Summary": "Port NUTCH-1245 URL gone with 404 after db.fetch.interval.max stays db_unfetched in CrawlDb and is generated over and over again to 2.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Jun/13 20:55",
        "Updated": "01/May/14 06:22",
        "Resolved": "02/Nov/13 14:17",
        "Description": "A document gone with 404 after db.fetch.interval.max (90 days) has passed\nis fetched over and over again but although fetch status is fetch_gone\nits status in CrawlDb keeps db_unfetched. Consequently, this document will\nbe generated and fetched from now on in every cycle.\nTo reproduce:\n\ncreate a CrawlDatum in CrawlDb which retry interval hits db.fetch.interval.max (I manipulated the shouldFetch() in AbstractFetchSchedule to achieve this)\nnow this URL is fetched again\nbut when updating CrawlDb with the fetch_gone the CrawlDatum is reset to db_unfetched, the retry interval is fixed to 0.9 * db.fetch.interval.max (81 days)\nthis does not change with every generate-fetch-update cycle, here for two segments:\n\n/tmp/testcrawl/segments/20120105161430\nSegmentReader: get 'http://localhost/page_gone'\nCrawl Generate::\nStatus: 1 (db_unfetched)\nFetch time: Thu Jan 05 16:14:21 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nMetadata: _ngt_: 1325776461784_pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\nCrawl Fetch::\nStatus: 37 (fetch_gone)\nFetch time: Thu Jan 05 16:14:48 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nMetadata: _ngt_: 1325776461784_pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\n\n/tmp/testcrawl/segments/20120105161631\nSegmentReader: get 'http://localhost/page_gone'\nCrawl Generate::\nStatus: 1 (db_unfetched)\nFetch time: Thu Jan 05 16:16:23 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nMetadata: _ngt_: 1325776583451_pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\nCrawl Fetch::\nStatus: 37 (fetch_gone)\nFetch time: Thu Jan 05 16:20:05 CET 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nMetadata: _ngt_: 1325776583451_pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\n\n\nAs far as I can see it's caused by setPageGoneSchedule() in AbstractFetchSchedule. Some pseudo-code:\n\nsetPageGoneSchedule (called from update / CrawlDbReducer.reduce):\n    datum.fetchInterval = 1.5 * datum.fetchInterval // now 1.5 * 0.9 * maxInterval\n    datum.fetchTime = fetchTime + datum.fetchInterval // see NUTCH-516\n    if (maxInterval < datum.fetchInterval) // necessarily true\n       forceRefetch()\n\nforceRefetch:\n    if (datum.fetchInterval > maxInterval) // true because it's 1.35 * maxInterval\n       datum.fetchInterval = 0.9 * maxInterval\n    datum.status = db_unfetched // \n\n\nshouldFetch (called from generate / Generator.map):\n    if ((datum.fetchTime - curTime) > maxInterval)\n       // always true if the crawler is launched in short intervals\n       // (lower than 0.35 * maxInterval)\n       datum.fetchTime = curTime // forces a refetch\n\n\nAfter setPageGoneSchedule is called via update the state is db_unfetched and the retry interval 0.9 * db.fetch.interval.max (81 days). \nAlthough the fetch time in the CrawlDb is far in the future\n\n% nutch readdb testcrawl/crawldb -url http://localhost/page_gone\nURL: http://localhost/page_gone\nVersion: 7\nStatus: 1 (db_unfetched)\nFetch time: Sun May 06 05:20:05 CEST 2012\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 6998400 seconds (81 days)\nScore: 1.0\nSignature: null\nMetadata: _pst_: notfound(14), lastModified=0: http://localhost/page_gone\n\n\nthe URL is generated again because (fetch time - current time) is larger than db.fetch.interval.max.\nThe retry interval (datum.fetchInterval) oscillates between 0.9 and 1.35, and the fetch time is always close to current time + 1.35 * db.fetch.interval.max.\nIt's possibly a side effect of NUTCH-516, and may be related to NUTCH-578",
        "Issue Links": [
            "/jira/browse/NUTCH-1245"
        ]
    },
    "NUTCH-1589": {
        "Key": "NUTCH-1589",
        "Summary": "Port NUTCH-1475 Index-More Plugin -- A better fall back value for date field to 2.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Jun/13 20:59",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Continuing directly from NUTCH-1475\nthe fetch datum (and not the current CrawlDatum from CrawlDb) is passed to IndexingFilter plugins, cf. conversation @user [1] (thanks, liaoks!).\nSince fetch datum contains the time the fetching has taken place, we should take this as last fallback value (and not the current time). To use the lastModified time from CrawlDatum (if set) is not wrong and is closer to 2.x\nWe should address this final part in this issue.",
        "Issue Links": [
            "/jira/browse/NUTCH-1475"
        ]
    },
    "NUTCH-1590": {
        "Key": "NUTCH-1590",
        "Summary": "[SECURITY] Frame injection vulnerability in published Javadoc",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "21/Jun/13 02:30",
        "Updated": "19/Jun/14 00:07",
        "Resolved": "17/Jun/14 14:17",
        "Description": "Hi All,\nOracle has announced [1], [2] a frame injection vulnerability in Javadoc\ngenerated by Java 5, Java 6 and Java 7 before update 22.\nThe infrastructure team has completed a scan of our current project\nwebsites and identified over 6000 instances of vulnerable Javadoc\ndistributed across most TLPs. The chances are the project(s) you\ncontribute to is(are) affected. A list of projects and the number of\naffected Javadoc instances per project is provided at the end of this\ne-mail.\nPlease take the necessary steps to fix any currently published Javadoc\nand to ensure that any future Javadoc published by your project does not\ncontain the vulnerability. The announcement by Oracle includes a link to\na tool that can be used to fix Javadoc without regeneration.\nThe infrastructure team is investigating options for preventing the\npublication of vulnerable Javadoc.\nThe issue is public and may be discussed freely on your project's dev list.\n[1]\nhttp://www.oracle.com/technetwork/topics/security/javacpujun2013-1899847.html\n[2] http://www.kb.cert.org/vuls/id/225657\nnutch.apache.org        8",
        "Issue Links": []
    },
    "NUTCH-1591": {
        "Key": "NUTCH-1591",
        "Summary": "Incorrect conversion of ByteBuffer to String",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.2.1",
        "Component/s": "crawldb,                                            indexer,                                            parser,                                            storage",
        "Assignee": null,
        "Reporter": "Jason Howes",
        "Created": "21/Jun/13 18:20",
        "Updated": "11/Oct/19 15:36",
        "Resolved": "27/Jun/13 17:07",
        "Description": "There are many occurrences of the following ByteBuffer-to-String conversion throughout the Nutch codebase:\n\nByteBuffer buf = ...;\nreturn new String(buf.array);\n\n\nThis approach assume that the ByteBuffer and its underlying array are aligned (i.e. ByteBuffer.arrayOffset() is equal to 0 and the length of the underlying array is the same as ByteBuffer.remaining()). In many cases this is not the case. The correct way to convert a ByteBuffer to a String (or stream thereof) is the following:\n\nByteBuffer buf = ...;\nreturn new String(buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());\n\n\nI noticed this bug when using Nutch with Cassandra. In most cases, the parsed content contains data from other columns (as well as garbage content) since the Cassandra client library returns ByteBuffers that are views on top of a larger byte[]. It also seems that others have hit this as well:\nhttp://grokbase.com/p/nutch/user/132jnq8s4r/slow-parse-on-hadoop\nI've attached a patch based on the release-2.2 tag of the 2.x branch on GitHub:\nhttps://github.com/apache/nutch/tree/release-2.2",
        "Issue Links": []
    },
    "NUTCH-1592": {
        "Key": "NUTCH-1592",
        "Summary": "TikaParser can uppercase the element names while generating the DOM",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6",
        "Fix Version/s": "1.10",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "26/Jun/13 20:29",
        "Updated": "11/Dec/14 11:50",
        "Resolved": "11/Dec/14 11:40",
        "Description": "The title says it all. The behaviour should be the same regardless of which parser is used",
        "Issue Links": []
    },
    "NUTCH-1593": {
        "Key": "NUTCH-1593",
        "Summary": "normalize option missing in SegmentMerger's usage",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "29/Jun/13 12:56",
        "Updated": "01/Jul/13 11:33",
        "Resolved": "01/Jul/13 10:03",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1594": {
        "Key": "NUTCH-1594",
        "Summary": "count variable is never changed in ParseUtil class",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": "lufeng",
        "Reporter": "lufeng",
        "Created": "29/Jun/13 15:11",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "in ParseUtil class the count variable is never change. the code is like this \nfor (int i = 0; count < maxOutlinks && i < outlinks.length; i++) \nso even if you define the \"db.max.outlinks.per.page\" parameter, it will not take effect.",
        "Issue Links": []
    },
    "NUTCH-1595": {
        "Key": "NUTCH-1595",
        "Summary": "Upgrade to Tika 1.4",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Jul/13 08:42",
        "Updated": "01/May/14 06:22",
        "Resolved": "05/Jul/13 10:29",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1596": {
        "Key": "NUTCH-1596",
        "Summary": "HeadingsParseFilter not thread safe",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Jul/13 09:08",
        "Updated": "04/Jul/13 12:09",
        "Resolved": "04/Jul/13 11:14",
        "Description": "The NodeWalker used by the HeadingsParseFilter sometimes reports a NullPointerException.\n\n2013-07-02 11:02:09,428 WARN  parse.ParseUtil - Error parsing .... with org.apache.nutch.parse.tika.TikaParser@2c8b586a\njava.util.concurrent.ExecutionException: java.lang.NullPointerException\n        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:262)\n        at java.util.concurrent.FutureTask.get(FutureTask.java:119)\n        at org.apache.nutch.parse.ParseUtil.runParser(ParseUtil.java:162)\n        at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:93)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.output(Fetcher.java:963)\n        at org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:722)\nCaused by: java.lang.NullPointerException\n        at org.apache.xerces.dom.ParentNode.nodeListItem(Unknown Source)\n        at org.apache.xerces.dom.ParentNode.item(Unknown Source)\n        at org.apache.nutch.util.NodeWalker.nextNode(NodeWalker.java:75)\n        at org.apache.nutch.parse.headings.HeadingsParseFilter.getElement(HeadingsParseFilter.java:84)\n        at org.apache.nutch.parse.headings.HeadingsParseFilter.filter(HeadingsParseFilter.java:47)\n        at org.apache.nutch.parse.HtmlParseFilters.filter(HtmlParseFilters.java:98)\n        at org.apache.nutch.parse.tika.TikaParser.getParse(TikaParser.java:210)\n        at org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:35)\n        at org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:24)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:166)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n        at java.lang.Thread.run(Thread.java:722)\n\n\nThis is strange because it only rarely fails and the nextNode() method checks hasNext() and there is no concurrent access if i'm correct.",
        "Issue Links": []
    },
    "NUTCH-1597": {
        "Key": "NUTCH-1597",
        "Summary": "HeadingsParseFilter to trim and remove exess whitespace",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Jul/13 09:11",
        "Updated": "04/Jul/13 10:10",
        "Resolved": "04/Jul/13 09:07",
        "Description": "Some elements have significant whitespace which should be removed.",
        "Issue Links": []
    },
    "NUTCH-1598": {
        "Key": "NUTCH-1598",
        "Summary": "ElasticSearchIndexer to read ImmutableSettings from config",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Jul/13 12:52",
        "Updated": "15/Aug/13 16:33",
        "Resolved": "05/Jul/13 09:04",
        "Description": "In some cases one must configure settings prior to indexing such as discovery.zen.ping.multicast.group or discovery.zen.ping.multicast.port if the node needs to find the cluster somewhere else. This patch allows for a key=value file in Nutch' config that is loaded in ImmutableSettings.",
        "Issue Links": []
    },
    "NUTCH-1599": {
        "Key": "NUTCH-1599",
        "Summary": "Obtain consensus on new description of Nutch",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "02/Jul/13 16:49",
        "Updated": "01/May/14 06:22",
        "Resolved": "03/Jul/13 19:31",
        "Description": "As we seem to be sustaining pushes and maintenance (touch wood) of two branches, I think it is about time we agreed on a more accurate description of what Nutch actually is.\nWe currently have (taken directly from our site)\n\nApache Nutch is an open source web-search software project. Stemming from Apache Lucene, it now builds on Apache Solr adding web-specifics, such as a crawler, a link-graph database and parsing support handled by Apache Tika for HTML and and array other document formats.\n\nNutch can run on a single machine, but gains a lot of its strength from running in a Hadoop cluster\n\nThe system can be enhanced (eg other document formats can be parsed) using a highly flexible, easily extensible and thoroughly maintained plugin infrastructure.\n\n\nI suggest/propose something along the lines of\n\nApache Nutch is an open source web-search software project. Stemming from Apache Lucene, the community now develops and maintains two branches:\n\n* 1.x; description of 1.x here\n\n* 2.x; description of 2.x here\n\nBoth branches add web-specifics, such as a crawler, a link-graph database and parsing support handled by Apache Tika for HTML and anarray other document formats.\n\nNutch can run on a single machine, but gains a lot of its strength from running in a Hadoop cluster\n\nThe system can be enhanced (eg other document formats can be parsed) using a highly flexible, easily extensible and thoroughly maintained plugin infrastructure.\n\n\nAny thoughts?",
        "Issue Links": []
    },
    "NUTCH-1600": {
        "Key": "NUTCH-1600",
        "Summary": "Injector overwrite does not always work properly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "injector",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "03/Jul/13 08:38",
        "Updated": "16/Oct/22 20:15",
        "Resolved": "04/Jul/13 08:50",
        "Description": "db.injector.update works as it should but db.injector.overwrite doesn't always seem to properly overwrite the record. This issue exists for some time and we've already fixed it in our dist of Nutch.\nThis record just has been updated (interval).\n\nInjector: starting at 2013-07-03 10:34:15\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: seeds\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 0\nInjector: total number of urls injected after normalization and filtering: 9\nInjector: Merging injected urls into crawl db.\nInjector: finished at 2013-07-03 10:34:21, elapsed: 00:00:05\nURL: url\nVersion: 7\nStatus: 2 (db_fetched)\nFetch time: Fri Jul 05 12:11:44 CEST 2013\nModified time: Fri Jun 28 12:11:44 CEST 2013\nRetries since fetch: 0\nRetry interval: 604800 seconds (7 days)\nScore: 0.0\nSignature: ba29ef3e680323a6d0da74c156800e03\nMetadata: Content-Type: text/html_pst_: success(1), lastModified=0\n\n\n\nIf we now overwrite the record, nothing happens. With this patch installed it overwrites the record as it should and also logs update & overwrite switches to console:\n\nInjector: starting at 2013-07-03 10:36:30\nInjector: crawlDb: crawl/crawldb\nInjector: urlDir: seeds\nInjector: Converting injected urls to crawl db entries.\nInjector: total number of urls rejected by filters: 0\nInjector: total number of urls injected after normalization and filtering: 9\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: true\nInjector: update: false\nInjector: finished at 2013-07-03 10:36:36, elapsed: 00:00:05\nURL: url\nVersion: 7\nStatus: 1 (db_unfetched)\nFetch time: Wed Jul 03 10:36:30 CEST 2013\nModified time: Thu Jan 01 01:00:00 CET 1970\nRetries since fetch: 0\nRetry interval: 14000 seconds (0 days)\nScore: 1.0\nSignature: null\nMetadata: fixedInterval: 14000.0",
        "Issue Links": []
    },
    "NUTCH-1601": {
        "Key": "NUTCH-1601",
        "Summary": "ElasticSearchIndexer fails to properly delete documents",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "03/Jul/13 10:48",
        "Updated": "04/Jul/13 09:08",
        "Resolved": "04/Jul/13 08:59",
        "Description": "Exception is thrown because the indexer does not properly set the type and index for delete commands. This comes from the original source so 2x may be affected as well.\n\nava.io.IOException\n        at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.makeIOException(ElasticIndexWriter.java:173)\n        at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.delete(ElasticIndexWriter.java:168)\n        at org.apache.nutch.indexer.IndexWriters.delete(IndexWriters.java:108)\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:52)\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:41)\n        at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.write(ReduceTask.java:458)\n        at org.apache.hadoop.mapred.ReduceTask$3.collect(ReduceTask.java:500)\n        at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:203)\n        at org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:53)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:522)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\nCaused by: org.elasticsearch.action.ActionRequestValidationException: Validation Failed: 1: index is missing;2: type is missing;\n        at org.elasticsearch.action.ValidateActions.addValidationError(ValidateActions.java:29)\n        at org.elasticsearch.action.support.replication.ShardReplicationOperationRequest.validate(ShardReplicationOperationRequest.java:126)\n        at org.elasticsearch.action.delete.DeleteRequest.validate(DeleteRequest.java:84)\n        at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:55)\n        at org.elasticsearch.client.node.NodeClient.execute(NodeClient.java:83)\n        at org.elasticsearch.client.support.AbstractClient.delete(AbstractClient.java:121)\n        at org.elasticsearch.action.delete.DeleteRequestBuilder.doExecute(DeleteRequestBuilder.java:147)\n        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:53)\n        at org.elasticsearch.action.support.BaseRequestBuilder.execute(BaseRequestBuilder.java:47)\n        at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.delete(ElasticIndexWriter.java:165)\n        ... 10 more\n2013-07-03 11:43:39,957 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n        at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\n        at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:185)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:195)",
        "Issue Links": []
    },
    "NUTCH-1602": {
        "Key": "NUTCH-1602",
        "Summary": "improve the readability of metadata in readdb dump normal",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "crawldb",
        "Assignee": "lufeng",
        "Reporter": "lufeng",
        "Created": "03/Jul/13 14:52",
        "Updated": "04/Jul/13 16:19",
        "Resolved": "04/Jul/13 15:08",
        "Description": "the dumped metadata format is not readable.\n\n$bin/nutch readdb crawldb/ -dump dir\nhttp://www.baidu.com/\tVersion: 7\nStatus: 3 (db_gone)\nFetch time: Sat Aug 17 22:35:37 CST 2013\nModified time: Thu Jan 01 08:00:00 CST 1970\nRetries since fetch: 0\nRetry interval: 3888000 seconds (45 days)\nScore: 1.0\nSignature: null\nMetadata: m1: v22m3: v3m2: v2m5: v5m4: m4_pst_: robots_denied(18), lastModified=0m6: v6\n\n\nso I improve the Metadata format to this\n\nMetadata: m1=v22;m3=v3;m2=v2;m5=v5;m4=m4;_pst_=robots_denied(18), lastModified=0;m6=v6;",
        "Issue Links": []
    },
    "NUTCH-1603": {
        "Key": "NUTCH-1603",
        "Summary": "ZIP parser complains about truncated PDF file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "05/Jul/13 13:02",
        "Updated": "01/May/14 06:22",
        "Resolved": "16/Apr/14 14:38",
        "Description": "ZipParser complains:\n\ncontentType: application/zip\n...\nStatus: failed(2,202): Content truncated at 63394 bytes. Parser can't handle incomplete pdf file.\n\n\nIt's a zip file, of course.",
        "Issue Links": [
            "/jira/browse/NUTCH-1308"
        ]
    },
    "NUTCH-1604": {
        "Key": "NUTCH-1604",
        "Summary": "ProtocolFactory not thread-safe",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "05/Jul/13 14:25",
        "Updated": "07/Oct/16 13:41",
        "Resolved": "08/Jul/13 08:50",
        "Description": "The method getProtocol() should be synchronized otherwise the Fetcher threads can access it around the same time and query the cache before it's had a chance of being populated properly. This would happen for a handful of calls until the subsequent ones get the cache but this should be fixed nonetheless e.g. when we want a guarantee that the same Protocol instance will be called for the same fetching session.\nThe other Factor classes which use the same cache mechanism would suffer from the same problem.",
        "Issue Links": []
    },
    "NUTCH-1605": {
        "Key": "NUTCH-1605",
        "Summary": "mime type detector recognizes xlsx as zip file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "07/Jul/13 20:30",
        "Updated": "05/Jul/14 21:15",
        "Resolved": "05/Jul/14 20:44",
        "Description": "With mime.type.magic as true (the default) Office Open XML spreadsheets (*.xlsx) are treated as zip files and not parsed correctly:\n\n% bin/nutch parsechecker http://localhost/test.xlsx\nfetching: http://localhost/test.xlsx\nparsing: http://localhost/test.xlsx\ncontentType: application/zip\n...\n\n\nXlsx files are formally zip files. Nevertheless, both HTTP header and file name are clear:\n\n% wget -d http://localhost/test.xlsx\n...\nHTTP/1.1 200 OK\n...\nContent-Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n...\n\n\nTika 1.4 detects the type correctly:\n\n% java -jar tika-app-1.4.jar -d http://localhost/test/test.xlsx\napplication/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        "Issue Links": []
    },
    "NUTCH-1606": {
        "Key": "NUTCH-1606",
        "Summary": "Check that Factory classes use the cache in a thread safe way",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "08/Jul/13 08:59",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "14/Oct/13 09:46",
        "Description": "I found in NUTCH-1604 that the ProtocolFactory class was not handling access to the cache properly. The same mechanism is used in other Factory classes so we should make sure that they are properly synchronized + make ObjectCache thread safe as well",
        "Issue Links": []
    },
    "NUTCH-1607": {
        "Key": "NUTCH-1607",
        "Summary": "Make inproper multiValued field configurable",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "-christian",
        "Created": "08/Jul/13 11:32",
        "Updated": "15/Nov/13 17:51",
        "Resolved": "15/Nov/13 17:51",
        "Description": "After using 1.7 some errors on commiting to Solr came up:\nNutch is indexing a specific URL where he is parsing a field, that - according to schema.xml for solr - is a single value field, e.g. description\nFor that particular site the description is declared twice - which is beyond our control, leading to an error when commiting those documents to Solr. Is there any workaround for that, e.g. telling nutch to take only the first occurance and ignore the rest? \nMaybe this is an regression from https://issues.apache.org/jira/browse/NUTCH-1560",
        "Issue Links": [
            "/jira/browse/NUTCH-1560"
        ]
    },
    "NUTCH-1608": {
        "Key": "NUTCH-1608",
        "Summary": "SolrDeleteDuplicates bug: choosing preferred page when duplicates does not work",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1,                                            2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Brian",
        "Created": "09/Jul/13 15:15",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "There is a bug in the code for deciding which version of a page to keep when there are duplicates.  This is a bug in the reduce function and is a common pitfall when using hadoop/mapreduce, as explained here:\nhttp://cornercases.wordpress.com/2011/08/18/hadoop-object-reuse-pitfall-all-my-reducer-values-are-the-same/\nThe issue is that in the reduce function getting the next iterator does not change the location of the reference returned, but only updates the content at the same location (and returns that same location - i.e., reference), so it is not correct to compare with a previously stored reference as they point to the same location and thus will be the same. Instead it is necessary to make a copy of the object to preserve it for later comparison. \nThe patch added also encodes additional preferences between URLs: after comparing the boost values it then compares the extension - preferring either no extension or a .htm or .html extension, then length - preferring shorter URLs, then timestamp.  This can be modified as desired by changing the contents of the \"isPreferredOver\" method.",
        "Issue Links": []
    },
    "NUTCH-1609": {
        "Key": "NUTCH-1609",
        "Summary": "java.net.MalformedURLException when running nutch crawl with apache-nutch-2.1.jar with hadoop",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "vishal toshniwal",
        "Created": "11/Jul/13 13:34",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "12/Jul/13 18:29",
        "Description": "I am getting   java.net.MalformedURLException  when running \"crawl\" for nutch 2.1 with hadoop. But it is working fine with the local mode\nFollowing is the exception\nbin/hadoop jar apache-nutch-2.1.job org.apache.nutch.crawl.Crawler urls2 -dir crawled -depth 3 -topN 5\nWarning: $HADOOP_HOME is deprecated.\n****hdfs://localhost:9000/user/impadmin/crawled\njava.lang.RuntimeException: java.io.IOException: java.io.IOException: java.net.MalformedURLException\n\tat org.apache.gora.mapreduce.GoraInputFormat.setConf(GoraInputFormat.java:115)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:723)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)\n\tat org.apache.hadoop.mapred.Child$4.run(Child.java:255)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1093)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:249)\nCaused by: java.io.IOException: java.io.IOException: java.net.MalformedURLException\n\tat org.apache.gora.util.IOUtils.loadFromConf(IOUtils.java:483)\n\tat org.apache.gora.mapreduce.GoraInputFormat.getQuery(GoraInputFormat.java:125)\n\tat org.apache.gora.mapreduce.GoraInputFormat.setConf(GoraInputFormat.java:112)\n\t... 9 more\nCaused by: java.io.IOException: java.net.MalformedURLException\n\tat org.apache.gora.sql.store.SqlStore.readMapping(SqlStore.java:878)\n\tat org.apache.gora.sql.store.SqlStore.initialize(SqlStore.java:163)\n\tat org.apache.gora.store.impl.DataStoreBase.readFields(DataStoreBase.java:181)\n\tat org.apache.gora.query.impl.QueryBase.readFields(QueryBase.java:222)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)\n\tat org.apache.hadoop.io.DefaultStringifier.fromString(DefaultStringifier.java:75)\n\tat org.apache.hadoop.io.DefaultStringifier.load(DefaultStringifier.java:133)\n\tat org.apache.gora.util.IOUtils.loadFromConf(IOUtils.java:480)\n\t... 11 more\nCaused by: java.net.MalformedURLException\n\tat java.net.URL.<init>(URL.java:601)\n\tat java.net.URL.<init>(URL.java:464)\n\tat java.net.URL.<init>(URL.java:413)\n\tat org.apache.xerces.impl.XMLEntityManager.setupCurrentEntity(Unknown Source)\n\tat org.apache.xerces.impl.XMLVersionDetector.determineDocVersion(Unknown Source)\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n\tat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\n\tat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\n\tat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\n\tat org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)\n\tat org.jdom.input.SAXBuilder.build(SAXBuilder.java:489)\n\tat org.jdom.input.SAXBuilder.build(SAXBuilder.java:807)\n\tat org.apache.gora.sql.store.SqlStore.readMapping(SqlStore.java:847)\n\t... 19 more\nException in thread \"main\" java.lang.RuntimeException: job failed: name=generate: 1373549310-1607767962, jobid=job_201307111857_0002\n\tat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:54)\n\tat org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:191)\n\tat org.apache.nutch.crawl.Crawler.runTool(Crawler.java:68)\n\tat org.apache.nutch.crawl.Crawler.run(Crawler.java:152)\n\tat org.apache.nutch.crawl.Crawler.run(Crawler.java:250)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.Crawler.main(Crawler.java:257)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:156)\nbin/nutch crawl urls -depth 3 -topN 5",
        "Issue Links": []
    },
    "NUTCH-1610": {
        "Key": "NUTCH-1610",
        "Summary": "Can't run individual unit tests for plugins in nutch 2.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "test",
        "Assignee": null,
        "Reporter": "Brian",
        "Created": "12/Jul/13 21:18",
        "Updated": "01/May/14 06:22",
        "Resolved": "01/Sep/13 21:06",
        "Description": "I can run \"ant test\" in the apache-nutch-2.2.1 directory but it runs all unit tests, and I just want to test a single plugin.\nPrevious ways posted online for running a single unit test don't work including:\n1) \"ant test\" in the plugin directory\nI get:\n\"BUILD FAILED ... Problem: failed to create task or type antlib:org.apache.ivy.ant:settings\"\n2) using bin/nutch with the junit argument, e.g.:\nbin/nutch junit org.apache.nutch.indexer.basic.TestBasicIndexingFilter\nIn runtime/local it says:\nClass not found \"org.apache.nutch.indexer.basic.TestBasicIndexingFilter\"\nIn src it says:\nError: Could not find or load main class junit.textui.TestRunner",
        "Issue Links": []
    },
    "NUTCH-1611": {
        "Key": "NUTCH-1611",
        "Summary": "Elastic Search Indexer Creates field in elastic search \"boost\" as a string value, so cannot be used in custom boost queries",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.2.1,                                            1.16",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Nicholas Waltham",
        "Created": "15/Jul/13 08:13",
        "Updated": "21/Jun/22 11:05",
        "Resolved": null,
        "Description": "Ordinarily, one can use a boost field in a custom_score query in elastic search to affect the ranking, nutch create such a field. However it is store in elastic search as a string, so cannot be used. Attempt to use the boost field in a query therefore creates the following error:\n PropertyAccessException[[Error: could not access: floatValue; in class: org.elasticsearch.index.field.data.strings.StringDocFieldData]\\n[Near : \n{... _score + (1 * doc.boost.floatValue / 100) ....}\n]       \nexample test query:\n{\n    \"query\" : {        \n      \"custom_score\" : {  \n        \"query\" : {\n      \"query_string\" : {\n         \"query\" : \"something\"\n      }},\n      \"script\" : \"_score + (1 * doc.boost.doubleValue / 100)\"\n      }\n   }\n}",
        "Issue Links": []
    },
    "NUTCH-1612": {
        "Key": "NUTCH-1612",
        "Summary": "Getting URl Malformed exception with Nutch 2.2 and Hadoop 1.0.3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Amit Yadav",
        "Created": "16/Jul/13 05:49",
        "Updated": "17/Jul/13 20:58",
        "Resolved": "17/Jul/13 19:14",
        "Description": "When I start crawling using bin/crawl  I am getting \"URLMalfomed Exception\". I am using Hbase as data store. I can see that the WebTable is created in the Hbase.\nI am able to run the same in local mode.\nAny help on this would be appreciable.",
        "Issue Links": []
    },
    "NUTCH-1613": {
        "Key": "NUTCH-1613",
        "Summary": "Timeouts in protocol-httpclient when crawling same host with >2 threads and added cookie strings for both http protocols",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Brian",
        "Created": "16/Jul/13 21:45",
        "Updated": "18/May/14 03:44",
        "Resolved": "12/May/14 12:59",
        "Description": "1.)  When using protocol-httpclient to crawl a single website (the same host) I would always get a bunch of timeout errors during fetching and the pages with errors would not be fetched. E.g.:\n2013-07-09 17:57:13,717 WARN  fetcher.FetcherJob - fetch of http://www.... failed with: org.apache.commons.httpclient.ConnectionPoolTimeoutException: Timeout waiting for connection\n2013-07-09 17:57:13,718 INFO  fetcher.FetcherJob - fetching http://www.... (queue crawl delay=0ms)\n2013-07-09 17:57:13,715 ERROR httpclient.Http - Failed with the following error: \norg.apache.commons.httpclient.ConnectionPoolTimeoutException: Timeout waiting for connection\n\tat org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.doGetConnection(MultiThreadedHttpConnectionManager.java:497)\n\tat org.apache.commons.httpclient.MultiThreadedHttpConnectionManager.getConnectionWithTimeout(MultiThreadedHttpConnectionManager.java:416)\n\tat org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:153)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)\n\tat org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)\n\tat org.apache.nutch.protocol.httpclient.HttpResponse.<init>(HttpResponse.java:95)\n\tat org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:174)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:133)\n\tat org.apache.nutch.fetcher.FetcherReducer$FetcherThread.run(FetcherReducer.java:518)\nThis is because by default the connection pool manager only allows 2 connections per host so if more than 2 threads are used the others will tend to time out waiting to get a connection.   The code previously set max connections correctly but not connection per host.\n2.) I also added at the same time simple modifications to both protocol-http and protocol-httpclient to allow specifying a cookie string in the conf file to include in request headers.  \nI use this to crawl site content requiring authentication - it is better for me to specify the cookie string for the authentication than go through the whole authentication process and specifying login info.\nThe nutch-site.xml property is the following:\n<property>\n        <name>http.cookie_string</name>\n        <value>XX_AL=authorization_value_goes_here</value>\n\t\t<description>String to use as the cookie value for HTTP requests</description>\n</property>\nAlthough I use it for authentication it can be used to specify any single cookie string for the crawl (httpclient does support different cookies for different hosts but I did not get into that).",
        "Issue Links": [
            "/jira/browse/NUTCH-827"
        ]
    },
    "NUTCH-1614": {
        "Key": "NUTCH-1614",
        "Summary": "Plugin to exclude URLs matching regex list from indexing - to enable crawl but do not index",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Brian",
        "Created": "17/Jul/13 17:11",
        "Updated": "12/Jun/14 09:13",
        "Resolved": null,
        "Description": "Some pages we need to crawl (such as some main pages and different views of a main page) to get all the other pages, but we don't want to index those pages themselves.  Therefore we cannot use the url filter approach.\nThis plugin uses a file containing regex strings (see included sample file).  If one of the regex strings matches with an entire URL, that URL will be excluded form indexing.\nThe file to use is specified by the following property in nutch-site.xml:\n<property>\n        <name>indexer.url.filter.exclude.regex.file</name>\n        <value>regex-indexer-exclude-urls.txt</value>\n        <description>\n            Holds the file name containing the regex strings.  Any URL matching one of these strings will be excluded from indexing. \n            \"#\" indicates a comment line and will be ignored.\n        </description>\n</property>",
        "Issue Links": [
            "/jira/browse/NUTCH-1300"
        ]
    },
    "NUTCH-1615": {
        "Key": "NUTCH-1615",
        "Summary": "Implementing A Feature for Fetching From Websites Dump",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.1",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Cihad Guzel",
        "Created": "19/Jul/13 12:58",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "18/Apr/14 08:25",
        "Description": "Some web sites provide dump (as like http://dumps.wikimedia.org/enwiki/ for wikipedia.org). We should fetch from dumps for such kind of web sites. Thus fetching  will be quicker.",
        "Issue Links": []
    },
    "NUTCH-1616": {
        "Key": "NUTCH-1616",
        "Summary": "SegmentMerger missing proper crawl_fetch datum",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Jul/13 14:08",
        "Updated": "08/Jan/14 13:12",
        "Resolved": "08/Jan/14 13:12",
        "Description": "Merged 26036 vs. unmerged 26038 indexed documents! There are two records on the merged segment that no longer have a crawl_fetch CrawlDatum with a fetch_success status. Instead, the only crawl_fetch CrawlDatum has status linked!\nThe original segment two crawl_fetch CrawlDatums with linked and the fetch_success status.\nWithout the fetch_success of not_modified status it is not going to be indexed.",
        "Issue Links": [
            "/jira/browse/NUTCH-1113",
            "/jira/browse/NUTCH-1617",
            "/jira/browse/NUTCH-1113",
            "/jira/browse/NUTCH-1520"
        ]
    },
    "NUTCH-1617": {
        "Key": "NUTCH-1617",
        "Summary": "IndexerMapReduce to consider latest fetchDatum",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Jul/13 14:16",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "IndexerMapReduce can skip not_modified or delete redirects and gone records but it only considers the first incoming fetchDatum. Instead, it should consider the last fetchDatum only based on CrawlDatum.fetchTime.\nThis affect indexing of multiple segments only.",
        "Issue Links": [
            "/jira/browse/NUTCH-1416",
            "/jira/browse/NUTCH-1616"
        ]
    },
    "NUTCH-1618": {
        "Key": "NUTCH-1618",
        "Summary": "Turn speculative execution off for Fetching",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1,                                            2.2,                                            2.3,                                            2.4",
        "Fix Version/s": "2.3",
        "Component/s": "fetcher",
        "Assignee": "Talat Uyarer",
        "Reporter": "Talat Uyarer",
        "Created": "25/Jul/13 08:59",
        "Updated": "04/May/14 22:10",
        "Resolved": "03/May/14 15:27",
        "Description": "We are using nutch for high volume crawls. We noticed that FetcherJob ReduceTask fetches some websites multiple times for long lasting queues. I have discovered the reason of this is mapred.reduce.tasks.speculative.execution settings in hadoop. 1.x has speculative execution turned off. I create a patch for 2.x",
        "Issue Links": []
    },
    "NUTCH-1619": {
        "Key": "NUTCH-1619",
        "Summary": "Writes Dmoz Description and Title information to db with snippet argument",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yasin K\u0131l\u0131n\u00e7",
        "Created": "02/Aug/13 12:51",
        "Updated": "01/May/14 06:22",
        "Resolved": "25/Aug/13 15:19",
        "Description": "We need Dmoz information of fetched URLs can be written to database. So these information can be used like snipppet by indexer of the search engine we are working on.",
        "Issue Links": []
    },
    "NUTCH-1620": {
        "Key": "NUTCH-1620",
        "Summary": "log how many URLs are generated and contained within a particular batchId",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "06/Aug/13 22:25",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Right now logging is as follows\nGeneratorJob: generated batch id: 1375827785-774274680\nIt would be nice if it was like\nGeneratorJob: generated batch id: 1375827785-774274680 (containing N urls)",
        "Issue Links": []
    },
    "NUTCH-1621": {
        "Key": "NUTCH-1621",
        "Summary": "Deprecated class o.a.n.crawl.Crawler is still in code base",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rui Gao",
        "Created": "08/Aug/13 13:24",
        "Updated": "01/May/14 06:22",
        "Resolved": "14/Nov/13 12:13",
        "Description": "depth (in deprecated class o.a.n.crawl.Crawler) and\nnumberOfRounds (in bin/crawl)\nmean the same.\n\nIt's better to remove the whole class Crawler to make things clear.",
        "Issue Links": [
            "/jira/browse/NUTCH-1087"
        ]
    },
    "NUTCH-1622": {
        "Key": "NUTCH-1622",
        "Summary": "Create Outlinks with metadata",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "1.8",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "08/Aug/13 15:04",
        "Updated": "05/Jul/15 01:05",
        "Resolved": "10/Jul/14 20:34",
        "Description": "Having the possibility to specify metadata when creating an outlink is extremely useful as it allows to pass information from a source page to the pages it links to. We use that routinely within our custom parsers in combination with the url-meta plugin.",
        "Issue Links": [
            "/jira/browse/NUTCH-1816",
            "/jira/browse/NUTCH-1465"
        ]
    },
    "NUTCH-1623": {
        "Key": "NUTCH-1623",
        "Summary": "Implement file.content.ignored function",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2,                                            2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "crawldb,                                            fetcher",
        "Assignee": null,
        "Reporter": "Osy",
        "Created": "12/Aug/13 22:22",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "For Nutch 2.2.1 in nutch-default.xml there is a description for this functionality (!! NO IMPLEMENTED YET !!):\nIf true, no file content will be saved during fetch.\nAnd it is probably what we want to set most of time, since\u00a0file://\u00a0URLs\nare meant to be local and we can always use them directly at parsing\nand indexing stages. Otherwise file contents will be saved.\nExactly what I need.\nThanks",
        "Issue Links": []
    },
    "NUTCH-1624": {
        "Key": "NUTCH-1624",
        "Summary": "Typo in WebTableReader  line 486",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "kaveh minooie",
        "Created": "13/Aug/13 22:39",
        "Updated": "01/May/14 06:22",
        "Resolved": "18/Aug/13 23:03",
        "Description": "the error message suggests to the user to use, among other things, '-stat', it should be '-stats'",
        "Issue Links": []
    },
    "NUTCH-1625": {
        "Key": "NUTCH-1625",
        "Summary": "IndexerMapReduce skips FETCH_NOTMODIFIED",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Aug/13 13:59",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/May/19 12:12",
        "Description": "IndexerMapReduce has the option to skip DB_NOTMODIFIED but legacy code also skips FETCH_NOTMODIFIED and the latter is not optional. We can keep the check but that should also include FETCH_NOTMODIFIED. Relying on FETCH_NOTMODIFIED isn't very useful anyway because since 1.5 orso we can safely rely on DB_NOTMODIFIED as it is properly set in the CrawlDBReducer.",
        "Issue Links": [
            "/jira/browse/NUTCH-1416"
        ]
    },
    "NUTCH-1626": {
        "Key": "NUTCH-1626",
        "Summary": "Homebrew formula for installing Nutch in Mac OS X",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Pennebaker",
        "Created": "19/Aug/13 20:21",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "20/Apr/14 16:44",
        "Description": "Manually installing nutch takes time and effort out of a developer's day. It would be a great convenience to have an install formula for Homebrew for Mac users!\nI have begun working on such a formula:\nhttps://github.com/mxcl/homebrew/pull/22004\nAfter `brew install nutch`, you can run `nutch`, but the associated tools like `nutch junit` aren't working for some reason.",
        "Issue Links": []
    },
    "NUTCH-1627": {
        "Key": "NUTCH-1627",
        "Summary": "Debian package for installing nutch",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Pennebaker",
        "Created": "19/Aug/13 20:23",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "20/Apr/14 16:43",
        "Description": "The simpler it is to install nutch, the easier it is to start using it. Could we please create a build task for generating a .deb installer for Debian/Ubuntu?\nEventually, it would be great to have a PPA, and then an official package in the Ubuntu apt repo.",
        "Issue Links": []
    },
    "NUTCH-1628": {
        "Key": "NUTCH-1628",
        "Summary": "Chocolatey package for Windows users",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Andrew Pennebaker",
        "Created": "19/Aug/13 20:25",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "20/Apr/14 16:43",
        "Description": "Setting up developer tools in Windows can be a trial. If we provided a Chocolatey package for nutch, it could bring more Windows users into the fold, encouraging them to use nutch as a dependency in larger software systems.",
        "Issue Links": []
    },
    "NUTCH-1629": {
        "Key": "NUTCH-1629",
        "Summary": "there is no need to fail on empty lines in seed file when injecting.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "kaveh minooie",
        "Created": "20/Aug/13 23:12",
        "Updated": "01/May/14 06:22",
        "Resolved": "23/Aug/13 08:53",
        "Description": "right now, if there is an empty line in a seed file, TableUtil.reversUrl would throw an exception that would kill the inject job.",
        "Issue Links": []
    },
    "NUTCH-1630": {
        "Key": "NUTCH-1630",
        "Summary": "How to achieve finishing fetch approximately at the same time for each queue (a.k.a adaptive queue size)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1,                                            2.2,                                            2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "21/Aug/13 10:43",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Problem Definition:\nWhen crawling, due to unproportional size of queues; fetching needs to wait for a long time for long lasting queues when shorter ones are finished. That means you may have to wait for a couple of days for some of queues.\nNormally we define max queue size with generate.max.count but that's a static value. However number of URLs to be fetched increases with each depth. Defining same length for all queues does not mean all queues will finish around the same time. This problem has been addressed by some other users before [1]. So we came up with a different approach to this issue.\nSolution:\nNutch has three mods for creating fetch queues (byHost, byDomain, ByIp). Our solution can be applicable to all three mods.\n1-Define a \"fetch workload of current queue\" (FW) value for each queue based on the previous fetches of that queue.\nWe calculate this by:\n    FW=average response time of previous depth * number of urls in current queue\n2- Calculate the harmonic mean [2] of all FW's to get the average workload of current depth (AW)\n3- Get the length for a queue by dividing AW by previously known average response time of that queue:\n    Queue Length=AW / average response time\nUsing this algoritm leads to a fetch phase where all queues finish up around the same time.\nAs soon as posible i will send my patch. Do you have any comments ? \n[1] http://osdir.com/ml/dev.nutch.apache.org/2011-11/msg00101.html\n[2] In our opinion; harmonic mean is best in our case because our data has a few points that are much higher than the rest.",
        "Issue Links": [
            "/jira/browse/NUTCH-1413",
            "/jira/browse/NUTCH-1659"
        ]
    },
    "NUTCH-1631": {
        "Key": "NUTCH-1631",
        "Summary": "Display Document Count Added To Solr Server",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.1,                                            2.2,                                            2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Furkan Kamaci",
        "Created": "21/Aug/13 16:52",
        "Updated": "01/May/14 06:22",
        "Resolved": "23/Aug/13 19:49",
        "Description": "Currently you can not see how many documents are added to Solr Server from Nutch. One should be able to see how many documents are added to Solr Server simultaneously (as a hadoop counter) and also total document count should be logged too after all documents are added to Solr Server.",
        "Issue Links": []
    },
    "NUTCH-1632": {
        "Key": "NUTCH-1632",
        "Summary": "add batchId argument for DbUpdaterJob",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "crawldb",
        "Assignee": "lufeng",
        "Reporter": "lufeng",
        "Created": "26/Aug/13 14:59",
        "Updated": "27/Aug/13 00:26",
        "Resolved": "27/Aug/13 00:23",
        "Description": "add batchId argument for DbUpdaterJob, you can put the batchId to DbUpdaterJob.",
        "Issue Links": [
            "/jira/browse/NUTCH-1556"
        ]
    },
    "NUTCH-1633": {
        "Key": "NUTCH-1633",
        "Summary": "slf4j is provided by hadoop and should not be included in the job file.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "kaveh minooie",
        "Created": "26/Aug/13 18:42",
        "Updated": "09/Jan/15 06:59",
        "Resolved": "25/Jun/14 11:06",
        "Description": "there are two issues with including slf4j in the job file. the minor of the two is that slf4j starts issuing warnings when it finds more than on instances in the classpath( GORA-272 ). the bigger issue happens when the versions of the slf4j in hadoop and nutch are not compatible (ex. hadoop 1.1.1 & nutch 2.1) which results in all nutch jobs to crash.",
        "Issue Links": [
            "/jira/browse/NUTCH-1220"
        ]
    },
    "NUTCH-1634": {
        "Key": "NUTCH-1634",
        "Summary": "readdb -stats show the result twice",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "kaveh minooie",
        "Created": "28/Aug/13 18:39",
        "Updated": "30/May/14 15:41",
        "Resolved": "30/May/14 14:51",
        "Description": "right now this is the ouput:\nWebTable statistics start\nStatistics for WebTable: \nstatus 2 (status_fetched):\t1115\nmin score:\t0.0\nretry 2:\t2\nretry 0:\t11369\njobs:\t{db_stats-job_local1037462743_0001={jobID=job_local1037462743_0001, jobName=db_stats, counters={File Input Format Counters =\n{BYTES_READ=0}, Map-Reduce Framework={MAP_OUTPUT_MATERIALIZED_BYTES=209, MAP_INPUT_RECORDS=11376, REDUCE_SHUFFLE_BYTES=0, SPILLED_RECORDS=24, MAP_OUTPUT_BYTES=602928, COMMITTED_HEAP_BYTES=1181220864, CPU_MILLISECONDS=0, SPLIT_RAW_BYTES=757, COMBINE_INPUT_RECORDS=45504, REDUCE_INPUT_RECORDS=12, REDUCE_INPUT_GROUPS=12, COMBINE_OUTPUT_RECORDS=12, PHYSICAL_MEMORY_BYTES=0, REDUCE_OUTPUT_RECORDS=12, VIRTUAL_MEMORY_BYTES=0, MAP_OUTPUT_RECORDS=45504}, FileSystemCounters={FILE_BYTES_READ=1819, FILE_BYTES_WRITTEN=166199}, File Output Format Counters ={BYTES_WRITTEN=373}}}}\nretry 1:\t5\nstatus 5 (status_redir_perm):\t69\nmax score:\t1.0\nTOTAL urls:\t11376\nstatus 3 (status_gone):\t3\nstatus 4 (status_redir_temp):\t3\nstatus 1 (status_unfetched):\t10186\navg score:\t0.00342827\nWebTable statistics: done\nstatus 2 (status_fetched):\t1115\nmin score:\t0.0\nretry 2:\t2\nretry 0:\t11369\njobs:\t{db_stats-job_local1037462743_0001={jobID=job_local1037462743_0001, jobName=db_stats, counters={File Input Format Counters ={BYTES_READ=0}\n, Map-Reduce Framework=\n{MAP_OUTPUT_MATERIALIZED_BYTES=209, MAP_INPUT_RECORDS=11376, REDUCE_SHUFFLE_BYTES=0, SPILLED_RECORDS=24, MAP_OUTPUT_BYTES=602928, COMMITTED_HEAP_BYTES=1181220864, CPU_MILLISECONDS=0, SPLIT_RAW_BYTES=757, COMBINE_INPUT_RECORDS=45504, REDUCE_INPUT_RECORDS=12, REDUCE_INPUT_GROUPS=12, COMBINE_OUTPUT_RECORDS=12, PHYSICAL_MEMORY_BYTES=0, REDUCE_OUTPUT_RECORDS=12, VIRTUAL_MEMORY_BYTES=0, MAP_OUTPUT_RECORDS=45504}\n, FileSystemCounters=\n{FILE_BYTES_READ=1819, FILE_BYTES_WRITTEN=166199}\n, File Output Format Counters ={BYTES_WRITTEN=373}}}}\nretry 1:\t5\nstatus 5 (status_redir_perm):\t69\nmax score:\t1.0\nTOTAL urls:\t11376\nstatus 3 (status_gone):\t3\nstatus 4 (status_redir_temp):\t3\nstatus 1 (status_unfetched):\t10186\navg score:\t0.00342827\nimho, it should be this:\nWebTable statistics start\nStatistics for WebTable: \nstatus 2 (status_fetched):\t1115\nmin score:\t0.0\nretry 2:\t2\nretry 0:\t11369\njobs:\t{db_stats-job_local801282144_0001={jobID=job_local801282144_0001, jobName=db_stats, counters={File Input Format Counters =\n{BYTES_READ=0}\n, Map-Reduce Framework=\n{MAP_OUTPUT_MATERIALIZED_BYTES=209, MAP_INPUT_RECORDS=11376, REDUCE_SHUFFLE_BYTES=0, SPILLED_RECORDS=24, MAP_OUTPUT_BYTES=602928, COMMITTED_HEAP_BYTES=1122631680, CPU_MILLISECONDS=0, SPLIT_RAW_BYTES=757, COMBINE_INPUT_RECORDS=45504, REDUCE_INPUT_RECORDS=12, REDUCE_INPUT_GROUPS=12, COMBINE_OUTPUT_RECORDS=12, PHYSICAL_MEMORY_BYTES=0, REDUCE_OUTPUT_RECORDS=12, VIRTUAL_MEMORY_BYTES=0, MAP_OUTPUT_RECORDS=45504}\n, FileSystemCounters=\n{FILE_BYTES_READ=1819, FILE_BYTES_WRITTEN=166191}\n, File Output Format Counters ={BYTES_WRITTEN=373}}}}\nretry 1:\t5\nstatus 5 (status_redir_perm):\t69\nmax score:\t1.0\nTOTAL urls:\t11376\nstatus 3 (status_gone):\t3\nstatus 4 (status_redir_temp):\t3\nstatus 1 (status_unfetched):\t10186\navg score:\t0.00342827\nWebTable statistics: done",
        "Issue Links": []
    },
    "NUTCH-1635": {
        "Key": "NUTCH-1635",
        "Summary": "New crawldb sometimes ends up in current",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "06/Sep/13 16:22",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "In some weird cases the newly created crawldb by updatedb ends up in crawl/crawldb/current/<NUMBER>/. So instead of replacing current/, it ends up inside current/! This causes the generator to fail.\nIt's impossible to reliably reproduce the problem. It only happened a couple of times in the last few years.",
        "Issue Links": []
    },
    "NUTCH-1636": {
        "Key": "NUTCH-1636",
        "Summary": "Indexer to normalize and filter repr URL",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.6,                                            1.7",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "07/Sep/13 21:21",
        "Updated": "20/Sep/13 22:32",
        "Resolved": "20/Sep/13 21:15",
        "Description": "Indexer if used with option -normalize and/or -filter (cf. NUTCH-1300) should also normalize and filter representation URLs. Otherwise URLs which are target of a redirect, and have repr URL set (see URLUtil.chooseRepr) may show up in index with an undesirable URL.",
        "Issue Links": []
    },
    "NUTCH-1637": {
        "Key": "NUTCH-1637",
        "Summary": "URLUtil is missing getProtocol",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "09/Sep/13 11:03",
        "Updated": "20/Sep/13 21:04",
        "Resolved": "12/Sep/13 11:06",
        "Description": "URLUtil should have getProtocol method as well.",
        "Issue Links": []
    },
    "NUTCH-1638": {
        "Key": "NUTCH-1638",
        "Summary": "SolrWriter Bad String comparision",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Aaron Scheffel",
        "Created": "09/Sep/13 13:52",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "19/Dec/14 23:03",
        "Description": "In version 2.2.1 and 1.7\nClass org.apache.nutch.indexer.solr.SolrWriter\nLine 66\ninputDoc.addField(solrMapping.mapKey(e.getKey()), val2);\nString sCopy = solrMapping.mapCopyKey(e.getKey());\nif (sCopy != e.getKey()) \n{\n\ninputDoc.addField(sCopy, val2);\n\n}\n\n\nThere is a string comparison \"sCopy != e.getKey()\" with a != and not an\nequals. It basically never works right. Not sure if someone was trying to\nbe clever but even if those strings are the same they happen to be\ndifferent objects. Which I'm assuming is due to how the strings are\nserialized in from the config files.\nSuggest its updated to\nString sCopy = solrMapping.mapCopyKey(e.getKey());\nif (!sCopy.equals(solrMapping.mapKey(e.getKey()))) \n{\n\n       inputDoc.addField(sCopy, val2);\n\n}\n\n\n-Aaron",
        "Issue Links": []
    },
    "NUTCH-1639": {
        "Key": "NUTCH-1639",
        "Summary": "bin/crawl fails on mac os",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "09/Sep/13 21:45",
        "Updated": "03/Oct/13 11:50",
        "Resolved": "03/Oct/13 11:11",
        "Description": "bin/crawl fails on Max OS in local mode when selecting last created segment: ls -l is not available.\nReported by tbuckner in NUTCH-1087, Maohua Liu/kiran [1, ascheffe [2.",
        "Issue Links": []
    },
    "NUTCH-1640": {
        "Key": "NUTCH-1640",
        "Summary": "OOM in ParseSegment Phase",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Mitesh Singh Jat",
        "Created": "12/Sep/13 06:33",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "07/Oct/13 09:24",
        "Description": "The nutch ParseSegment phase fails after 2 runs on same TaskTracker, with the following Exception:\n\nException in thread \"main\" org.apache.hadoop.ipc.RemoteException: java.io.IOException: java.lang.OutOfMemoryError: unable to create new native thread\n\tat java.lang.Thread.start0(Native Method)\n\tat java.lang.Thread.start(Thread.java:640)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType$JvmRunner.kill(JvmManager.java:553)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.killJvmRunner(JvmManager.java:317)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.killJvm(JvmManager.java:297)\n\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.taskKilled(JvmManager.java:289)\n\tat org.apache.hadoop.mapred.JvmManager.taskKilled(JvmManager.java:158)\n\tat org.apache.hadoop.mapred.TaskRunner.kill(TaskRunner.java:802)\n\tat org.apache.hadoop.mapred.TaskTracker$TaskInProgress.kill(TaskTracker.java:3315)\n\tat org.apache.hadoop.mapred.TaskTracker$TaskInProgress.jobHasFinished(TaskTracker.java:3287)\n\tat org.apache.hadoop.mapred.TaskTracker.purgeTask(TaskTracker.java:2316)\n\tat org.apache.hadoop.mapred.TaskTracker.fatalError(TaskTracker.java:3710)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:587)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1444)\n\tat org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1440)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:396)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1232)\n\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:1438)\n\n\tat org.apache.hadoop.ipc.Client.call(Client.java:1118)\n\tat org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)\n\tat $Proxy1.fatalError(Unknown Source)\n\tat org.apache.hadoop.mapred.Child.main(Child.java:310)\n\n\n\nWhereas similar parsing when done in Nutch Fetcher Phase (fetcher.parse=true, fetcher.store.content=false) does not give such issue.\nHence, on analysing the code of Fetcher and ParseSegment, it seems the issue\nshould be related to creation parseResult foreach url in ParseSegment.java.\n\n 95     ParseResult parseResult = null;\n 96     try {\n 97       parseResult = new ParseUtil(getConf()).parse(content); // <*****\n 98     } catch (Exception e) {\n 99       LOG.warn(\"Error parsing: \" + key + \": \" + StringUtils.stringifyException(e));\n100       return;\n101     }",
        "Issue Links": []
    },
    "NUTCH-1641": {
        "Key": "NUTCH-1641",
        "Summary": "Log timings for main jobs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "12/Sep/13 09:29",
        "Updated": "01/May/14 06:22",
        "Resolved": "20/Sep/13 08:03",
        "Description": "Patch attached logs out the timings for the FetcherJob, ParserJob and DbUpdaterJob in the same way as already done in the GeneratorJob.",
        "Issue Links": []
    },
    "NUTCH-1642": {
        "Key": "NUTCH-1642",
        "Summary": "mvn compile fails on Centos6.3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Xibao.Lv",
        "Created": "16/Sep/13 17:01",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "04/Oct/13 09:59",
        "Description": "Hi,all\nI am new. when i run 'mvn compile', it return some errors.Like Follows.\n1.[ERROR] Failed to execute goal on project nutch: Could not resolve dependencies for project org.apache.nutch:nutch:jar:2.2: The following artifacts could not be resolved: javax.jms:jms:jar:1.1, com.sun.jdmk:jmxtools:jar:1.2.1, com.sun.jmx:jmxri:jar:1.2.1, org.restlet.jse:org.restlet:jar:2.0.5, org.restlet.jse:org.restlet.ext.jackson:jar:2.0.5: Could not transfer artifact javax.jms:jms:jar:1.1 from/to java.net (https://maven-repository.dev.java.net/nonav/repository): No connector available to access repository java.net (https://maven-repository.dev.java.net/nonav/repository) of type legacy using the available factories WagonRepositoryConnectorFactory\nIt means that the org.restlet.jse can not find in main repository. Then I add repository address to pom.xml\n2.[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.jdom:jdom:jar -> duplicate declaration of version 1.1 @ line 268, column 29\n3.[ERROR] Failed to execute goal on project nutch: Could not resolve dependencies for project org.apache.nutch:nutch:jar:2.2: The following artifacts could not be resolved: javax.jms:jms:jar:1.1, com.sun.jdmk:jmxtools:jar:1.2.1, com.sun.jmx:jmxri:jar:1.2.1: Could not transfer artifact javax.jms:jms:jar:1.1 from/to java.net (https://maven-repository.dev.java.net/nonav/repository): No connector available to access repository java.net (https://maven-repository.dev.java.net/nonav/repository) of type legacy using the available factories WagonRepositoryConnectorFactory\nIt means that we can not find javax.jms in there(https://maven-repository.dev.java.net/nonav/repository). Google said log4j-1.2.15 dependency javax.jms, so we can use higher log4j, such as 1.2.16+.",
        "Issue Links": []
    },
    "NUTCH-1643": {
        "Key": "NUTCH-1643",
        "Summary": "Unnecessary fetching with http.content.limit when using protocol-http",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.1,                                            2.2,                                            2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "18/Sep/13 07:33",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "In protocol-http, Even If I have http.content.limit value set, protocol-http fetches files of all sizes (larger files are fetched until limit allows). \nBut when Parsing, parser skips incomplete files (if parser.skip.truncated configuration is true). It seems like an unnecessary effort to partially fetch contents larger than limit if they are not gonna be parsed.",
        "Issue Links": []
    },
    "NUTCH-1644": {
        "Key": "NUTCH-1644",
        "Summary": "Should have a parser that uses xpath",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Cihad Guzel",
        "Created": "22/Sep/13 20:32",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "May want to parse some url via xpath. May be blog or news web sites. Should be a plugin using xpath parse.",
        "Issue Links": [
            "/jira/browse/NUTCH-1870"
        ]
    },
    "NUTCH-1645": {
        "Key": "NUTCH-1645",
        "Summary": "Junit Test Case for Adaptive Fetch Schedule class",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "None",
        "Assignee": "Sertac TURKEL",
        "Reporter": "Talat Uyarer",
        "Created": "30/Sep/13 06:33",
        "Updated": "01/May/14 06:22",
        "Resolved": "30/Mar/14 20:00",
        "Description": "Currently there is not Test Case for Adaptive Fetch Schedule. Junit test Writes for its.",
        "Issue Links": []
    },
    "NUTCH-1646": {
        "Key": "NUTCH-1646",
        "Summary": "IndexerMapReduce to consider DB status",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Oct/13 10:44",
        "Updated": "07/Mar/14 18:18",
        "Resolved": "07/Mar/14 18:18",
        "Description": "IndexerMapReduce does not remove gone and redirects via DB status, only fetch status. This means segments merged before we fixed SegmentMerger may contain records that do not have a correct status. For example, some pages are gone on the web, gone in the CrawlDB, gone in the segments. But merging those old segments could cause a older status to prevail, causing it to be indexed although the CrawlDB says it's gone.",
        "Issue Links": [
            "/jira/browse/NUTCH-1707"
        ]
    },
    "NUTCH-1647": {
        "Key": "NUTCH-1647",
        "Summary": "protocol-http throws 'unzipBestEffort returned null' for redirected pages",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.9",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "01/Oct/13 12:03",
        "Updated": "13/Jun/14 11:53",
        "Resolved": "13/Jun/14 11:17",
        "Description": "bin/nutch indexchecker http://www.provinciegroningen.nl/actueel/dossiers/rwe-centrale \nFetch failed with protocol status: exception(16), lastModified=0: java.io.IOException: unzipBestEffort returned null\n\n2013-10-01 13:44:55,612 INFO  http.Http - http.proxy.host = null\n2013-10-01 13:44:55,612 INFO  http.Http - http.proxy.port = 8080\n2013-10-01 13:44:55,612 INFO  http.Http - http.timeout = 12000\n2013-10-01 13:44:55,612 INFO  http.Http - http.content.limit = 5242880\n2013-10-01 13:44:55,612 INFO  http.Http - http.agent = Mozilla/5.0 (compatible; OpenindexSpider; +http://www.openindex.io/en/webmasters/spider.html)\n2013-10-01 13:44:55,612 INFO  http.Http - http.accept.language = en-us,en-gb,en;q=0.7,*;q=0.3\n2013-10-01 13:44:55,613 INFO  http.Http - http.accept = text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\n2013-10-01 13:44:55,925 ERROR http.Http - Failed to get protocol output\njava.io.IOException: unzipBestEffort returned null\n        at org.apache.nutch.protocol.http.api.HttpBase.processGzipEncoded(HttpBase.java:317)\n        at org.apache.nutch.protocol.http.HttpResponse.<init>(HttpResponse.java:164)\n        at org.apache.nutch.protocol.http.Http.getResponse(Http.java:64)\n        at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:140)\n        at org.apache.nutch.indexer.IndexingFiltersChecker.run(IndexingFiltersChecker.java:86)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n        at org.apache.nutch.indexer.IndexingFiltersChecker.main(IndexingFiltersChecker.java:150)\n\n\nHaven't got a clue yet as to what the exact issue is.",
        "Issue Links": [
            "/jira/browse/NUTCH-1736",
            "/jira/browse/NUTCH-1089"
        ]
    },
    "NUTCH-1648": {
        "Key": "NUTCH-1648",
        "Summary": "Sentence Detection plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "\u0130lhami KALKAN",
        "Created": "02/Oct/13 10:43",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "23/Jun/14 10:16",
        "Description": "In parse progress, we need a plugin which detects sentence boundary of page content.",
        "Issue Links": [
            "/jira/browse/NUTCH-1649"
        ]
    },
    "NUTCH-1649": {
        "Key": "NUTCH-1649",
        "Summary": "Sentence Detection plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.2.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "\u0130lhami KALKAN",
        "Created": "02/Oct/13 10:55",
        "Updated": "02/Oct/13 11:02",
        "Resolved": "02/Oct/13 11:02",
        "Description": "In parse progress, we need a plugin which detects sentence boundary of page content.",
        "Issue Links": [
            "/jira/browse/NUTCH-1648"
        ]
    },
    "NUTCH-1650": {
        "Key": "NUTCH-1650",
        "Summary": "Adaptive Fetch Scheduler interval Wrong Set",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "04/Oct/13 07:28",
        "Updated": "01/May/14 06:22",
        "Resolved": "02/Nov/13 14:11",
        "Description": "After calculation interval time when setting it didn't check between max and min values.  Moreover if sync_delta is true. Interval set before changes. This patch fix this.",
        "Issue Links": []
    },
    "NUTCH-1651": {
        "Key": "NUTCH-1651",
        "Summary": "modifiedTime and prevmodifiedTime never set",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "04/Oct/13 07:58",
        "Updated": "17/May/14 17:12",
        "Resolved": "04/Nov/13 19:12",
        "Description": "modifiedTime is never set. If you use DefaultFetchScheduler, modifiedTime is always zero as default. But if you use AdaptiveFetchScheduler, modifiedTime is set only once in the beginning by zero-control of AdaptiveFetchScheduler.\nBut this is not sufficient since modifiedTime needs to be updated whenever last modified time is available. We corrected this with a patch.\nAlso we noticed that prevModifiedTime is not written to database and we corrected that too.\nWith this patch, whenever lastModifiedTime is available, we do two things. First we set modifiedTime in the Page object to prevModifiedTime. After that we set lastModifiedTime to modifiedTime.",
        "Issue Links": [
            "/jira/browse/NUTCH-1784"
        ]
    },
    "NUTCH-1652": {
        "Key": "NUTCH-1652",
        "Summary": "Avoid instanciation of MimeUtil for each Content object created",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "07/Oct/13 14:55",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "05/May/20 09:44",
        "Description": "Content objects instantiate and hold a MimeUtil in the constructor used by the HttpBase class. This is wasteful and unnecessarily slows down the creation of Content object as the MimeUtil creates a new Tika instance, reads from the configuration etc...\nInstead we could create a single instance of the MimeUtil class and pass it to the a new Content constructor   \n\npublic Content(String url, String base, byte[] content, String contentType,\n      Metadata metadata, MimeUtil mime)\n\n\nand create a single instance of MimeUtil in HttpBase. We would also need to make sure that the synchronisation is handled properly in MimeUtil (especially for the calls to Tika) as the creation of the Content is done in a multithreaded environment.",
        "Issue Links": []
    },
    "NUTCH-1653": {
        "Key": "NUTCH-1653",
        "Summary": "AbstractScoringFilter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "10/Oct/13 08:23",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "10/Oct/13 13:50",
        "Description": "Writing a new scoring filter can be a bit tedious because of the number of methods to write even if they are not used by the filter. This patch provides an AbstractScoringFilter which ScoringFilter implementations can extend for convenience",
        "Issue Links": [
            "/jira/browse/NUTCH-1402"
        ]
    },
    "NUTCH-1654": {
        "Key": "NUTCH-1654",
        "Summary": "FetchSchedule.setFetchSchedule called twice",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.8",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Oct/13 14:04",
        "Updated": "11/Oct/13 14:05",
        "Resolved": "11/Oct/13 14:05",
        "Description": "Caused by NUTCH-1586, schedule is called twice for non redirects.",
        "Issue Links": []
    },
    "NUTCH-1655": {
        "Key": "NUTCH-1568 port pluggable indexing architecture to 2.x",
        "Summary": "Indexer Plugin for Elastic Search",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "12/Oct/13 06:30",
        "Updated": "01/May/14 06:22",
        "Resolved": "16/Jan/14 22:05",
        "Description": "We should rewrite ElasticSearch indexer compatible with new Indexing Plugin Architect.",
        "Issue Links": []
    },
    "NUTCH-1656": {
        "Key": "NUTCH-1656",
        "Summary": "ParseMeta not passed to CrawlDatum for not_modified",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Oct/13 09:15",
        "Updated": "16/Oct/13 15:49",
        "Resolved": "16/Oct/13 14:57",
        "Description": "The ParseMeta not passed to CrawlDatum for not_modified. So if you add new plugins but pages stay not modified, you won't see your metadata in the CrawlDB.",
        "Issue Links": []
    },
    "NUTCH-1657": {
        "Key": "NUTCH-1657",
        "Summary": "ORIGINAL_CHAR_ENCODING and CHAR_ENCODING_FOR_CONVERSION never set in HTMLParser",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": "Talat Uyarer",
        "Reporter": "Talat Uyarer",
        "Created": "28/Oct/13 15:59",
        "Updated": "04/May/14 22:10",
        "Resolved": "03/May/14 13:48",
        "Description": "ORIGINAL_CHAR_ENCODING and CHAR_ENCODING_FOR_CONVERSION are never set in HTMLParser.java.\nIn 2.x, we didn't set this value any field. Actually we never use this value in 2.x I thought delete them. But Feng Lu guided me and I will set metadata field.",
        "Issue Links": [
            "/jira/browse/NUTCH-1677"
        ]
    },
    "NUTCH-1658": {
        "Key": "NUTCH-1658",
        "Summary": "Nutch mangles seed URLs and then reports on the mangled ones",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "1.7",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Steve Newcomb",
        "Created": "30/Oct/13 16:57",
        "Updated": "31/Oct/13 12:54",
        "Resolved": "31/Oct/13 12:48",
        "Description": "Note: I'm using Nutch to verify that each of a long list of URIs is good, so I use them all as seeds in a single-iteration crawls.\nSome seed URIs are mangled by Nutch, and Nutch then reports on the mangled versions (which are no good) instead of the original ones (which are good).  Two patterns have emerged from my tests:\n(1) If the query portion of the URI contains '//', it becomes '/', rendering the resource unfetchable.  Example:\nhttps://www.pay.gov/paygov/forms/formInstance.html?nc=1356014395287&agencyFormId=44568890&userFormSearch=https%3A//www.pay.gov/paygov/keywordSearchForms.html%3FshowingDetails=true&showingAll=false&sortProperty=agencyFormName&totalResults=1&keyword=apma&ascending=true&pageOffset=0\n(2) If the URI has a trailing '.', it disappears, apparently rendering the resource unfetchable.  Example:\nhttp://www.irs.gov/Individuals/ITIN-Policy-Change-Summary-for-2013.\nBoth of the above are known good URIs.  When they are used as seeds, Nutch 1.7 doesn't report about them, but instead reports about URIs that have been mangled as described above.  In the '//' -> '/' case, Nutch reports that robot access is denied, which is probably true.  In the trailing '.' case, Nutch says there's no such resource, which is true, but it's not the question I was trying to get Nutch to answer.)",
        "Issue Links": []
    },
    "NUTCH-1659": {
        "Key": "NUTCH-1630 How to achieve finishing fetch approximately at the same time for each queue (a.k.a adaptive queue size)",
        "Summary": "Custom partitioner for Adaptive Queue Size",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "\u0130lhami KALKAN",
        "Created": "31/Oct/13 13:31",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "According to workload, groups of queue are distributed to reducers.",
        "Issue Links": [
            "/jira/browse/NUTCH-1630"
        ]
    },
    "NUTCH-1660": {
        "Key": "NUTCH-1660",
        "Summary": "Index filter for Page's latitude and longitude",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Talat Uyarer",
        "Created": "02/Nov/13 15:19",
        "Updated": "06/Apr/15 16:57",
        "Resolved": "21/Jan/15 20:58",
        "Description": "I see some discuss about page's ip storing. I think If we have page's ip, we can index page's geo position as latitude and longitude. That use for location based searches. \nicebergx5 I know you have a patch about this in your secret patches   Can you share us ?",
        "Issue Links": [
            "/jira/browse/NUTCH-1360"
        ]
    },
    "NUTCH-1661": {
        "Key": "NUTCH-1661",
        "Summary": "Language based crawling",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "02/Nov/13 16:15",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "If you want to crawling based on specific language, Nutch has not any solution. I created a custom Fetch Scheduler. It controls page's language",
        "Issue Links": [
            "/jira/browse/NUTCH-1663"
        ]
    },
    "NUTCH-1662": {
        "Key": "NUTCH-1568 port pluggable indexing architecture to 2.x",
        "Summary": "Indexer Plugin for Solr Cloud",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "03/Nov/13 10:30",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "In main issue's patch use Solr Http connection. It doesnt support Solr Could. This plugin support Solr Cloud.",
        "Issue Links": [
            "/jira/browse/NUTCH-2197",
            "/jira/browse/NUTCH-1377"
        ]
    },
    "NUTCH-1663": {
        "Key": "NUTCH-1663",
        "Summary": "Crawl page with specified language",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "\u0130lhami KALKAN",
        "Created": "04/Nov/13 17:30",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "User can crawl pages with specified language. For example, we want to crawl pages which language is Turkish.",
        "Issue Links": [
            "/jira/browse/NUTCH-2217",
            "/jira/browse/NUTCH-1661"
        ]
    },
    "NUTCH-1664": {
        "Key": "NUTCH-1664",
        "Summary": "Support for Hadoop 2.x",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Paul Inventado",
        "Created": "06/Nov/13 16:35",
        "Updated": "06/Nov/13 22:00",
        "Resolved": "06/Nov/13 22:00",
        "Description": "Hadoop 2.x offers a lot of improvements, notably in my case, the high availability feature.. However, it seems that Nutch doesn't work on Hadoop 2.x or at least when I tried running it. In case it already does, it would be great if someone could point me to the right direction.",
        "Issue Links": []
    },
    "NUTCH-1665": {
        "Key": "NUTCH-1665",
        "Summary": "Generator to implement Tool",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "08/Nov/13 19:22",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "20/Apr/14 16:37",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1666": {
        "Key": "NUTCH-1666",
        "Summary": "Optimisation for BasicURLNormalizer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "11/Nov/13 09:54",
        "Updated": "12/Nov/13 00:31",
        "Resolved": "11/Nov/13 10:15",
        "Description": "The regular expressions in the BasicURLNormalizer are quite costly, the patch attached allows to skip the processing if a URL does not contain a sequence of interest (two slashes with zero, one or two dots in between).\nThis reduces the time spent in post processing the parsing quite a bit.",
        "Issue Links": []
    },
    "NUTCH-1667": {
        "Key": "NUTCH-1667",
        "Summary": "Updatedb always ignore batchId",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "14/Nov/13 08:42",
        "Updated": "01/May/14 06:22",
        "Resolved": "13/Jan/14 13:17",
        "Description": "batchId is not set in currentJob because we set batchId after creating currentJob, so in UpdateDbMapper batchId is null and will be assign to -all.\nI change to set batchId befor creating currentJob",
        "Issue Links": [
            "/jira/browse/NUTCH-1556"
        ]
    },
    "NUTCH-1668": {
        "Key": "NUTCH-1668",
        "Summary": "Remove package org.apache.nutch.indexer.solr",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "14/Nov/13 11:59",
        "Updated": "18/Nov/13 18:39",
        "Resolved": "18/Nov/13 12:08",
        "Description": "Now that NUTCH-656 has been committed, we will be able to use the generic deduplication and remove the SOLR specific one. All the SOLR related code can now live in the indexer-solr plugin and we can remove the package org.apache.nutch.indexer.solr as well as the SOLR dependency in the main Ivy file",
        "Issue Links": []
    },
    "NUTCH-1669": {
        "Key": "NUTCH-1669",
        "Summary": "FTP crawl does not use FTP's server root folder",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Rafael Thomas Goz Coutinho",
        "Created": "19/Nov/13 12:33",
        "Updated": "12/May/14 13:10",
        "Resolved": null,
        "Description": "Setup an FTP with root folder setup for a user (let's say test) pointing to /home/test/ftphome/\nAnd create a folder under it called target with a test.txt file:\n/home/test/ftphome/target/test.txt\nConfigure a URL to crawl as with depth of 1:\nftp://FTP_SERVER/target/\nIt will fail to crawl because the FTP plugin protocol assumes the path is always absolute. It will look into /target/ and not /home/test/ftphome/target/",
        "Issue Links": []
    },
    "NUTCH-1670": {
        "Key": "NUTCH-1670",
        "Summary": "set same crawldb directory in mergedb parameter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "crawldb",
        "Assignee": "lufeng",
        "Reporter": "lufeng",
        "Created": "20/Nov/13 14:48",
        "Updated": "02/Jan/14 20:42",
        "Resolved": "02/Jan/14 19:43",
        "Description": "when merge two crawldb using the same crawldb directory in bin/nutch merge paramater, it will throw data not found exception. \nbin/nutch mergedb crawldb_t1 crawldb_t1 crawldb_2\nbin/nutch generate crawldb_t1 segment",
        "Issue Links": []
    },
    "NUTCH-1671": {
        "Key": "NUTCH-1671",
        "Summary": "indexchecker to add digest field",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "21/Nov/13 22:40",
        "Updated": "01/May/14 06:22",
        "Resolved": "17/Mar/14 21:57",
        "Description": "IndexingFiltersChecker does not add field \"digest\" as done by IndexerMapReduce. Digest/signature could be also used by indexing filters which then may fail.",
        "Issue Links": []
    },
    "NUTCH-1672": {
        "Key": "NUTCH-1672",
        "Summary": "Inlinks are added twice in DbUpdateReducer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "24/Nov/13 08:34",
        "Updated": "01/May/14 06:22",
        "Resolved": "13/Jan/14 13:22",
        "Description": "The first for loop is redundant \nfor (ScoreDatum inlink : inlinkedScoreData) {\n      page.putToInlinks(new Utf8(inlink.getUrl()), new Utf8(inlink.getAnchor()));\n}\n...\nfor (ScoreDatum inlink : inlinkedScoreData) {\n      int inlinkDist = inlink.getDistance();\n      if (inlinkDist < smallestDist) \n{\n        smallestDist=inlinkDist;\n      }\n      page.putToInlinks(new Utf8(inlink.getUrl()), new Utf8(inlink.getAnchor()));\n}",
        "Issue Links": []
    },
    "NUTCH-1673": {
        "Key": "NUTCH-1673",
        "Summary": "Title isn't reset in MoreIndexingFilter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "24/Nov/13 08:42",
        "Updated": "01/May/14 06:22",
        "Resolved": "27/Nov/13 10:16",
        "Description": "In resetTitle function, title is added to doc. We need remove old title before add. Currently it will resulted in error when indexing to solr when title field is not multivalue field.\nprivate NutchDocument resetTitle(NutchDocument doc, WebPage page, String url) {\n...\n    for (int i = 0; i < patterns.length; i++) {\n      if (matcher.contains(contentDisposition.toString(), patterns[i])) \n{\n...\n        doc.add(\"title\", result.group(1));\n        break;\n      }\n    }\n    return doc;\n  }",
        "Issue Links": []
    },
    "NUTCH-1674": {
        "Key": "NUTCH-1674",
        "Summary": "Use batchId filter to enable scan (GORA-119) for Fetch,Parse,Update,Index",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "25/Nov/13 02:43",
        "Updated": "18/May/14 03:37",
        "Resolved": "15/May/14 08:14",
        "Description": "Nutch always scan the whole crawldb in each phrase (generate, fetch, parse, update, index). When crawldb is big, the time to scan is bigger than the actual processing time.\nWe really need to skip records while scanning using GORA-119 for example we can only get records belong to a specified batchId.\nIn my crawl the filter reduce the time to scan from 90 min to 30 min.",
        "Issue Links": [
            "/jira/browse/NUTCH-1714",
            "/jira/browse/GORA-119",
            "/jira/browse/NUTCH-1777"
        ]
    },
    "NUTCH-1675": {
        "Key": "NUTCH-1675",
        "Summary": "NutchField to support long",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "26/Nov/13 12:09",
        "Updated": "07/Jan/14 12:50",
        "Resolved": "07/Jan/14 12:21",
        "Description": "NutchField has no support for Long in readfields. Usually this is not a problem because in reducers it is only written to the output. But when using NutchField in mappers, then a reducer cannot read a Long.\n\njava.lang.RuntimeException: problem advancing post rec#0\n        at org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:1217)\n        at org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.moveToNext(ReduceTask.java:250)\n        at org.apache.hadoop.mapred.ReduceTask$ReduceValuesIterator.next(ReduceTask.java:246)\n        at org.apache.nutch.fetcher.Fetcher$FetcherReducer.reduce(Fetcher.java:1440)\n        at org.apache.nutch.fetcher.Fetcher$FetcherReducer.reduce(Fetcher.java:1401)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:522)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:421)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\nCaused by: java.io.EOFException\n        at java.io.DataInputStream.readFully(DataInputStream.java:197)\n        at org.apache.hadoop.io.Text.readString(Text.java:402)\n        at org.apache.nutch.indexer.NutchField.readFields(NutchField.java:89)\n        at org.apache.nutch.indexer.NutchDocument.readFields(NutchDocument.java:112)\n        at org.apache.nutch.indexer.NutchIndexAction.readFields(NutchIndexAction.java:81)\n        at org.apache.nutch.util.GenericWritableConfigurable.readFields(GenericWritableConfigurable.java:54)\n        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)\n        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)\n        at org.apache.hadoop.mapred.Task$ValuesIterator.readNextValue(Task.java:1276)\n        at org.apache.hadoop.mapred.Task$ValuesIterator.next(Task.java:1214)\n        ... 7 more",
        "Issue Links": []
    },
    "NUTCH-1676": {
        "Key": "NUTCH-1676",
        "Summary": "Add rudimentary SSL support to protocol-http",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.9",
        "Component/s": "protocol",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "28/Nov/13 09:12",
        "Updated": "18/May/14 03:44",
        "Resolved": "16/May/14 13:40",
        "Description": "Adding https support to our http protocol would be a good thing even if it does not handle the security. This would save us from having to use the http-client plugin which is buggy in its current form. \nPatch generated from https://github.com/Aloisius/nutch/commit/d3e15a1db0eb323ccdcf5ad69a3d3a01ec65762c#commitcomment-4720772\nNeeds testing...",
        "Issue Links": []
    },
    "NUTCH-1677": {
        "Key": "NUTCH-1677",
        "Summary": "ORIGINAL_CHAR_ENCODING and CHAR_ENCODING_FOR_CONVERSION are not set in Parse HTML",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "29/Nov/13 12:35",
        "Updated": "09/Jan/15 07:02",
        "Resolved": "09/Jan/15 07:02",
        "Description": "They are not set Webpage Metadata. But they are set metadata object.\nWe will need those values in any plugin. They should set in Webpage Metadata coluom family.",
        "Issue Links": [
            "/jira/browse/NUTCH-1657"
        ]
    },
    "NUTCH-1678": {
        "Key": "NUTCH-1678",
        "Summary": "Remove dependency on org.apache.oro",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.4",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "James Sullivan",
        "Created": "03/Dec/13 07:11",
        "Updated": "24/Oct/18 09:10",
        "Resolved": "13/Oct/18 10:19",
        "Description": "org.apache.oro has been archived for three years and it may be good to remove the dependency as Java has had built in regexes for quite some time now. There don't seem to have been any specific Perl5 functionality needed in the regexes so unless there are specific threading or performance reasons for continuing to use oro it may be time to lose the dependency. Attached patch needs to be checked thoroughly as I am rusty with Java and the unit tests are sparse.",
        "Issue Links": [
            "/jira/browse/NUTCH-2192",
            "https://github.com/apache/nutch/pull/390"
        ]
    },
    "NUTCH-1679": {
        "Key": "NUTCH-1679",
        "Summary": "UpdateDb using batchId, link may override crawled page.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Tien Nguyen Manh",
        "Created": "04/Dec/13 16:27",
        "Updated": "03/Mar/22 14:27",
        "Resolved": "16/Sep/15 04:23",
        "Description": "The problem is in Hbase store, not sure about other store.\nSuppose at first crawl cycle we crawl link A, then get an outlink B.\nIn second cycle we crawl link B which also has a link point to A\nIn second updatedb we load only page B from store, and will add A as new link because it doesn't know A already exist in store and will override A.\nUpdateDb must be run without batchId or we must set additionsAllowed=false\nHere are code for new page\n      page = new WebPage();\n      schedule.initializeSchedule(url, page);\n      page.setStatus(CrawlStatus.STATUS_UNFETCHED);\n      try \n{\n        scoringFilters.initialScore(url, page);\n      }\n catch (ScoringFilterException e) \n{\n        page.setScore(0.0f);\n      }\nnew page will override old page status, score, fetchTime, fetchInterval, retries, metadata[CASH_KEY]\n\ni think we can change something here so that new page will only update one column for example 'link' and if it is really a new page, we can initialize all above fields in generator\nor we add operator checkAndPut to store so when add new page we will check if already exist first",
        "Issue Links": [
            "/jira/browse/NUTCH-1922",
            "/jira/browse/GORA-411",
            "/jira/browse/NUTCH-1556"
        ]
    },
    "NUTCH-1680": {
        "Key": "NUTCH-1680",
        "Summary": "CrawldbReader to dump minRetry value",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "10/Dec/13 12:53",
        "Updated": "20/Jan/14 09:51",
        "Resolved": "20/Jan/14 09:30",
        "Description": "CrawlDBReader should be able to dump records based on minimum retry value.",
        "Issue Links": []
    },
    "NUTCH-1681": {
        "Key": "NUTCH-1681",
        "Summary": "In URLUtil.java, toUNICODE method does not work correctly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.2,                                            1.9",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "\u0130lhami KALKAN",
        "Created": "10/Dec/13 16:00",
        "Updated": "23/Dec/13 15:43",
        "Resolved": "23/Dec/13 15:10",
        "Description": "This method returns java.net.URISyntaxException when non-ascii character does in parameter like http://www.\u00e7evir.com.",
        "Issue Links": [
            "/jira/browse/NUTCH-1321",
            "/jira/browse/NUTCH-1685"
        ]
    },
    "NUTCH-1682": {
        "Key": "NUTCH-1682",
        "Summary": "Port optionally maintain custom fetch interval despite AdaptiveFetchSchedule to 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "11/Dec/13 02:44",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Port NUTCH-1388  to 2.x",
        "Issue Links": [
            "/jira/browse/NUTCH-1683",
            "/jira/browse/NUTCH-1388"
        ]
    },
    "NUTCH-1683": {
        "Key": "NUTCH-1683",
        "Summary": "Optionally maintain custom fetch interval despite AbstractFetchSchedule",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "11/Dec/13 03:08",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "DefaultFetchSchedul also change fetch interval so we should also maintain fixed fetch interval there.",
        "Issue Links": [
            "/jira/browse/NUTCH-1682",
            "/jira/browse/NUTCH-1388"
        ]
    },
    "NUTCH-1684": {
        "Key": "NUTCH-1684",
        "Summary": "ParseMeta to be added before fetch schedulers are run",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Dec/13 14:55",
        "Updated": "01/Jul/15 07:52",
        "Resolved": "01/Jul/15 07:01",
        "Description": "FetchSchedulers cannot operate on parseMeta in the CrawlDatum because it is added after the schedulers have run.",
        "Issue Links": []
    },
    "NUTCH-1685": {
        "Key": "NUTCH-1685",
        "Summary": "URLUtil.toUNICODE fails on IDNs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "20/Dec/13 22:43",
        "Updated": "23/Dec/13 10:42",
        "Resolved": "23/Dec/13 10:42",
        "Description": "URLUtil.toUNICODE() fails on IDNs and returns null instead of the Unicode URL. The constructor of URI obviously does not accept IDN host names. For http://www.xn--evir-zoa.com/ the constructor IDN() throws the exception:\n\njava.net.URISyntaxException: Illegal character in hostname at index 11: http://www.\u00e7evir.com/\n\n\nPrincipally, IDN.toUnicode() can convert URLs (not only domain or host names). However, it does not convert URLs with host part consisting of only two parts: http://xn--uni-tbingen-xhb.de/. Is that the reason why we need URLUtil.toUNICODE() ?",
        "Issue Links": [
            "/jira/browse/NUTCH-1681"
        ]
    },
    "NUTCH-1686": {
        "Key": "NUTCH-1686",
        "Summary": "Optimize UpdateDb to load less field from Store",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "23/Dec/13 03:40",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "While running large crawl i found that updatedb run very slow, especially the Map task which loading data from store.\nWe can't use filter by batchId to load less url due to bug in NUTCH-1679 so we must always update the whole table.\nAfter checking the field loaded in UpdateDbJob i found that it load many fields from store (at least 15/25 field) which make updatedb slow\nI think that UpdateDbJob only need to load few field: SCORE, OUTLINKS, METADATA which is used to compute link score, distance that i think the main purpose of this job.\nThe other fields is used to compute url schedule to parser and fetcher, we can move code to Parser or Fetcher whithout loading much new field because many field are generated from parser. WE can also use gora filter for Fetcher or Parser so load new field is not a problem.\nI also add new field SCOREMETA to WebPage to store CASH, and DISTANCE. It is currently store in METADATA. field CASH is used in OPICScoring which is used only in UpdateDB and distance is used only in Generator and Updater so move both field two new Metadata field can prevent reading METADATA at Generator and Updater, METADATA contains many data that is used only at Parser and Indexer\nSo with new change\nUpdateDb only load SCORE, SCOREMATA (CASH, DISTANCE), OUTLINKS, MAKERS: we don't need to load big family Fetch and INLINKS.\nGenerator only load SCOREMETA (which is smaller than current METADATA)",
        "Issue Links": [
            "/jira/browse/NUTCH-1693"
        ]
    },
    "NUTCH-1687": {
        "Key": "NUTCH-1687",
        "Summary": "Pick queue in Round Robin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "23/Dec/13 03:58",
        "Updated": "08/Jan/23 19:42",
        "Resolved": "08/Jan/23 19:42",
        "Description": "Currently we chose queue to pick url from start of queues list, so queue at the start of list have more change to be pick first, that can cause problem of long tail queue, which only few queue available at the end which have many urls.\npublic synchronized FetchItem getFetchItem() {\n      final Iterator<Map.Entry<String, FetchItemQueue>> it =\n        queues.entrySet().iterator(); ==> always reset to find queue from start\n      while (it.hasNext()) {\n....\nI think it is better to pick queue in round robin, that can make reduce time to find the available queue and make all queue was picked in round robin and if we use TopN during generator there are no long tail queue at the end.",
        "Issue Links": [
            "/jira/browse/NUTCH-2767",
            "/jira/browse/NUTCH-1297"
        ]
    },
    "NUTCH-1688": {
        "Key": "NUTCH-1688",
        "Summary": "Port DeleteDuplicate based on crawlDB only to 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "23/Dec/13 04:16",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1690",
            "/jira/browse/NUTCH-656"
        ]
    },
    "NUTCH-1689": {
        "Key": "NUTCH-1689",
        "Summary": "Improve CrawlDb stats",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "23/Dec/13 04:30",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Crawldb stats now is slow due to it load all fields from store, I change to load only necessary fields.",
        "Issue Links": []
    },
    "NUTCH-1690": {
        "Key": "NUTCH-1690",
        "Summary": "IndexClean: mark url as unindexed after clean to not delete again",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "23/Dec/13 04:52",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "We should marked a deleted page to not delete it again and again. That can simply done by remove Index marker when we delete.\nI also change to delete duplicated url in solrclean.",
        "Issue Links": [
            "/jira/browse/NUTCH-1688"
        ]
    },
    "NUTCH-1691": {
        "Key": "NUTCH-1691",
        "Summary": "DomainBlacklist url filter does not allow -D filter file override",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "02/Jan/14 15:48",
        "Updated": "09/Dec/22 14:49",
        "Resolved": "09/Dec/22 14:49",
        "Description": "This filter does not accept -Durlfilter.domainblacklist.file= overrides. The plugin's file attribute is always used.",
        "Issue Links": [
            "/jira/browse/NUTCH-1694",
            "/jira/browse/NUTCH-2419"
        ]
    },
    "NUTCH-1692": {
        "Key": "NUTCH-1692",
        "Summary": "SegmentReader broken in distributed mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Jan/14 16:45",
        "Updated": "01/Jul/15 07:52",
        "Resolved": "01/Jul/15 07:01",
        "Description": "SegmentReader -list option ignores the -no* options, causing the following exception in distributed mode:\n\nException in thread \"main\" java.lang.NullPointerException\n        at java.util.ComparableTimSort.sort(ComparableTimSort.java:146)\n        at java.util.Arrays.sort(Arrays.java:472)\n        at org.apache.hadoop.mapred.SequenceFileOutputFormat.getReaders(SequenceFileOutputFormat.java:85)\n        at org.apache.nutch.segment.SegmentReader.getStats(SegmentReader.java:463)\n        at org.apache.nutch.segment.SegmentReader.list(SegmentReader.java:441)\n        at org.apache.nutch.segment.SegmentReader.main(SegmentReader.java:587)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:606)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:160)",
        "Issue Links": []
    },
    "NUTCH-1693": {
        "Key": "NUTCH-1693",
        "Summary": "TextMD5Signature computed on textual content",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Tien Nguyen Manh",
        "Created": "03/Jan/14 04:24",
        "Updated": "22/Aug/14 23:21",
        "Resolved": "22/Aug/14 22:24",
        "Description": "I create a new MD5Signature that based on textual content. In our case we use boilerpipe to extract main text from content so this signature is more effective to deduplicate.",
        "Issue Links": [
            "/jira/browse/NUTCH-1686"
        ]
    },
    "NUTCH-1694": {
        "Key": "NUTCH-1694",
        "Summary": "Consider removing URL filter attribute warnings.",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "03/Jan/14 13:39",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "URL filters such as domain, domainblacklist and prefix and likely others emit warnings if the configuration file-attribute in plugin.xml is empty or missing. Instead, we should only emit a warning if the plugin ends up with zero rules.\nSee also NUTCH-1691",
        "Issue Links": [
            "/jira/browse/NUTCH-1691"
        ]
    },
    "NUTCH-1695": {
        "Key": "NUTCH-1695",
        "Summary": "NutchDocument.toString()",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Jan/14 13:17",
        "Updated": "08/Jan/14 12:02",
        "Resolved": "08/Jan/14 09:41",
        "Description": "We need a NutchDocument.toString() for easier debugging.",
        "Issue Links": []
    },
    "NUTCH-1696": {
        "Key": "NUTCH-1696",
        "Summary": "Enable use of (Gora) SNAPSHOT dependencies",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Jan/14 18:14",
        "Updated": "01/May/14 06:22",
        "Resolved": "08/Jan/14 11:24",
        "Description": "For some time it has been on my radar to enable use of SNAPSHOT dependencies for use within Nutch. Specifically, this relates to gora-* SNAPSHOT's available here [0].\nI am working on a patch which updates ivy.xml and ivysettings.xml t enable this, however it seems almost like black magic right now.\nI'll upload the patch once I get my build working. \n[0] https://repository.apache.org/content/repositories/snapshots/org/apache/gora/",
        "Issue Links": []
    },
    "NUTCH-1697": {
        "Key": "NUTCH-1697",
        "Summary": "SegmentMerger to implement Tool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "09/Jan/14 10:43",
        "Updated": "21/Apr/15 08:57",
        "Resolved": "21/Apr/15 07:43",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1698": {
        "Key": "NUTCH-1698",
        "Summary": "crawl script should not specify solrUrl to accommodate pluggable indexing architecture",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Jan/14 14:39",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "19/Jun/14 15:15",
        "Description": "Currently the crawl script is invoke as follows\ncrawl <seedDir> <crawlDir> <solrURL> <numberOfRounds>\nthe solrURL parameter is incorrect, this should be changed to <indexServer> or something similar.\nAny preferences?",
        "Issue Links": []
    },
    "NUTCH-1699": {
        "Key": "NUTCH-1699",
        "Summary": "Tika Parser - Image Parse Bug",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.3",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Mehmet Zahid Yuzuguldu",
        "Created": "14/Jan/14 07:29",
        "Updated": "01/May/14 06:22",
        "Resolved": "15/Jan/14 15:21",
        "Description": "TikaParser is not extract metadatas from mime type of png, gif and bmp images.",
        "Issue Links": []
    },
    "NUTCH-1700": {
        "Key": "NUTCH-1700",
        "Summary": "Remove deprecated code in src/plugin/creativecommons/build.xml",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Jan/14 08:57",
        "Updated": "23/Apr/14 16:04",
        "Resolved": "23/Apr/14 15:26",
        "Description": "There is lots of xml relating to bygone days of the Nutch webgui.\nWe can simply remove a lot of this code.\nhttps://svn.apache.org/repos/asf/nutch/trunk/src/plugin/creativecommons/build.xml",
        "Issue Links": []
    },
    "NUTCH-1701": {
        "Key": "NUTCH-1701",
        "Summary": "Make Solr Document Boost as an option",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "15/Jan/14 07:42",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Nutch SolrIndexer use Nutch score as document boost by default. We should make it as an option because we can use nutch score to boost in different way such as boost at query time via function query",
        "Issue Links": []
    },
    "NUTCH-1702": {
        "Key": "NUTCH-1702",
        "Summary": "Port HostNormalizer to 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "15/Jan/14 08:00",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Port NUTCH-1319 to 2.x",
        "Issue Links": [
            "/jira/browse/NUTCH-1319"
        ]
    },
    "NUTCH-1703": {
        "Key": "NUTCH-1703",
        "Summary": "Nutch ignores alt text of images",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Canan Girgin",
        "Created": "15/Jan/14 09:19",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "If you put image as link alt text of that image is equivalent to the anchor text of text link. During content parse nutch does not give image alt text and  anchor text for that link is empty.",
        "Issue Links": []
    },
    "NUTCH-1704": {
        "Key": "NUTCH-1704",
        "Summary": "Port DomainBlacklist urlfilter to 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "15/Jan/14 15:04",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Port NUTCH-1210 to 2.x",
        "Issue Links": []
    },
    "NUTCH-1705": {
        "Key": "NUTCH-1705",
        "Summary": "Make configuration option for HtmlParser & TikaParser to extract text or title for noIndex page",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tien Nguyen Manh",
        "Created": "15/Jan/14 16:11",
        "Updated": "15/Jan/14 16:11",
        "Resolved": null,
        "Description": "Currently HtmlParser and TikaParser always skip extracting text and title for noIndex page - page which have noIndex robots metatags.\nBut some parse-filter may still interested in text and title such as NUTCH-1661, where we may decide wether to follow a page by it's language.",
        "Issue Links": []
    },
    "NUTCH-1706": {
        "Key": "NUTCH-1706",
        "Summary": "IndexerMapReduce does not remove db_redir_temp etc",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Jan/14 15:42",
        "Updated": "07/Mar/14 18:51",
        "Resolved": "07/Mar/14 18:17",
        "Description": "Code path is wrong in IndexerMapReduce, the delete code should be located after all reducer values have been gathered.",
        "Issue Links": [
            "/jira/browse/NUTCH-1707"
        ]
    },
    "NUTCH-1707": {
        "Key": "NUTCH-1707",
        "Summary": "DummyIndexingWriter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.8",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Jan/14 10:53",
        "Updated": "10/Feb/14 12:52",
        "Resolved": "10/Feb/14 11:46",
        "Description": "DummyIndexingWriter that is supposed to write COMMAND\\tID to a plain text file. This plain text file can then be parsed by a unit test to check whether IndexerMapReduce actually behaves as it should.",
        "Issue Links": [
            "/jira/browse/NUTCH-1541",
            "/jira/browse/NUTCH-1706",
            "/jira/browse/NUTCH-1646"
        ]
    },
    "NUTCH-1708": {
        "Key": "NUTCH-1708",
        "Summary": "use same id when indexing and deleting redirects",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "20/Jan/14 00:12",
        "Updated": "29/Jul/14 17:34",
        "Resolved": "29/Jul/14 15:13",
        "Description": "Redirect targets are indexed using \"representative URL\"\n\nin Fetcher repr URL is determined by URLUtil.chooseRepr() and stored in CrawlDatum (CrawlDb). Repr URL is either source or target URL of the redirect pair.\nNutchField \"url\" is filled by basic indexing filter with repr URL\nid field used as unique key is filled from url per solrindex-mapping.xml\n\nDeletion of redirects is done in IndexerMapReduce.reduce() by key which is the URL of the redirect source. If the source URL is chosen as repr URL a redirect target may get erroneously deleted.\nTest crawl with seed http://wiki.apache.org/nutch which redirects to http://wiki.apache.org/nutch/. DummyIndexWriter (NUTCH-1707) indicates that same URL is deleted and added:\n\ndelete  http://wiki.apache.org/nutch\nadd     http://wiki.apache.org/nutch",
        "Issue Links": [
            "/jira/browse/NUTCH-1820"
        ]
    },
    "NUTCH-1709": {
        "Key": "NUTCH-1709",
        "Summary": "Generated classes o.a.n.storage.Host and o.a.n.storage.ProtocolStatus contain methods not defined in source .avsc",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Jan/14 20:09",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "When using the GoraCompiler currently packaged with gora-core-0.4-SNAPSHOT, the following methods are removed from o.a.n.storage.Host or o.a.n.storage.ProtocolStatus\nHost.java\n  public boolean contains(String key) {\n    return metadata.containsKey(new Utf8(key));\n  }\n  \n  public String getValue(String key, String defaultValue) {\n    if (!contains(key)) return defaultValue;\n    return Bytes.toString(metadata.get(new Utf8(key)));\n  }\n  \n  public int getInt(String key, int defaultValue) {\n    if (!contains(key)) return defaultValue;\n    return Integer.parseInt(getValue(key,null));\n  }\n  public long getLong(String key, long defaultValue) {\n    if (!contains(key)) return defaultValue;\n    return Long.parseLong(getValue(key,null));\n  }\n\n\nProtocolStatus.java\n  /**\n   * A convenience method which returns a successful {@link ProtocolStatus}.\n   * @return the {@link ProtocolStatus} value for 200 (success).\n   */\n  public boolean isSuccess() {\n    return code == ProtocolStatusUtils.SUCCESS; \n  }\n\n\nThis results in compilation errors... I am not sure if it is good practice for non-default methods to be contained within generated Persistent classes. This is certainly the case with newer versions of Avro when using the Java API.\ncompile-core:\n[javac] Compiling 104 source files to /home/mary/Downloads/apache/2.x/build/classes\n[javac] warning: [options] bootstrap class path not set in conjunction with -source 1.6\n[javac] /home/mary/Downloads/apache/2.x/src/java/org/apache/nutch/fetcher/FetcherReducer.java:345: error: cannot find symbol\n[javac]                                        host.getInt(\"q_mt\", maxThreads),\n[javac]                                            ^\n[javac]   symbol:   method getInt(String,int)\n[javac]   location: variable host of type Host\n[javac] /home/mary/Downloads/apache/2.x/src/java/org/apache/nutch/fetcher/FetcherReducer.java:346: error: cannot find symbol\n[javac]                                        host.getLong(\"q_cd\", crawlDelay),\n[javac]                                            ^\n[javac]   symbol:   method getLong(String,long)\n[javac]   location: variable host of type Host\n[javac] /home/mary/Downloads/apache/2.x/src/java/org/apache/nutch/fetcher/FetcherReducer.java:347: error: cannot find symbol\n[javac]                                        host.getLong(\"q_mcd\", minCrawlDelay));\n[javac]                                            ^\n[javac]   symbol:   method getLong(String,long)\n[javac]   location: variable host of type Host\n[javac] /home/mary/Downloads/apache/2.x/src/java/org/apache/nutch/parse/ParserChecker.java:114: error: cannot find symbol\n[javac]     if(!protocolOutput.getStatus().isSuccess()) {\n[javac]                                   ^\n[javac]   symbol:   method isSuccess()\n[javac]   location: class ProtocolStatus\n[javac] Note: /home/mary/Downloads/apache/2.x/src/java/org/apache/nutch/storage/Host.java uses unchecked or unsafe operations.\n[javac] Note: Recompile with -Xlint:unchecked for details.\n[javac] 4 errors\n[javac] 1 warning\nI think it would be a good idea to find another home for such methods as it will undoubtedly avoid problems when we do Gora upgrades in the future.\nRight now I don't have a suggestion but will work on a solution non-the-less.",
        "Issue Links": []
    },
    "NUTCH-1710": {
        "Key": "NUTCH-1710",
        "Summary": "Add gora package logging to log4j.properties",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Jan/14 22:27",
        "Updated": "01/May/14 06:22",
        "Resolved": "22/Jan/14 22:31",
        "Description": "We can currently set package level logging for nutch, hadoop and zookeeper. This simple improvement will enable us to do the same for Gora (very good for DEBUG in Eclipse).",
        "Issue Links": []
    },
    "NUTCH-1711": {
        "Key": "NUTCH-1711",
        "Summary": "Normalizer does not encode exclamation mark",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Jan/14 13:05",
        "Updated": "23/Jun/15 13:06",
        "Resolved": "23/Jun/15 13:06",
        "Description": "$ bin/nutch org.apache.nutch.net.URLNormalizerChecker\nChecking combination of all URLNormalizers available\nhttp://nutch.apache.org/bla!\nhttp://nutch.apache.org/bla!\n\n\nI never noticed that many URL encoders do not encode the exclamation mark until just now. SolrCloud uses the character to delimit the composite ID in SolrCloud, if you end with the exclamation mark, you will get an error!\nAny thoughts on this?",
        "Issue Links": []
    },
    "NUTCH-1712": {
        "Key": "NUTCH-1712",
        "Summary": "Use MultipleInputs in Injector to make it a single mapreduce job",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.12",
        "Component/s": "injector",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Tejas Patil",
        "Created": "23/Jan/14 14:53",
        "Updated": "25/Feb/16 22:02",
        "Resolved": "25/Feb/16 22:00",
        "Description": "Currently Injector creates two mapreduce jobs:\n1. sort job: get the urls from seeds file, emit CrawlDatum objects.\n2. merge job: read CrawlDatum objects from both crawldb and output of sort job. Merge and emit final CrawlDatum objects.\nUsing MultipleInputs, we can read CrawlDatum objects from crawldb and urls from seeds file simultaneously and perform inject in a single map-reduce job.\nAlso, here are additional things covered with this jira:\n1. Pushed filtering and normalization above metadata extraction so that the unwanted records are ruled out quickly.\n2. Migrated to new mapreduce API\n3. Improved documentation \n4. New junits with better coverage\nRelevant discussion over nutch-dev can be found here:\nhttp://mail-archives.apache.org/mod_mbox/nutch-dev/201401.mbox/%3CCAFKhtFyXO6WL7gyUV+a5Y1pzNtdCoqPz4jz_up_bkp9cJe80kg@mail.gmail.com%3E",
        "Issue Links": [
            "/jira/browse/NUTCH-2049",
            "/jira/browse/NUTCH-1772"
        ]
    },
    "NUTCH-1713": {
        "Key": "NUTCH-1713",
        "Summary": "IpAddressResolver and DNSCache",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "23/Jan/14 15:13",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Hi Lewis,\naccording to the mail I sent to you, I provide my patch for storing ip addresses in apache-nutch-1.5.1 as attachment.\n( https://issues.apache.org/jira/browse/NUTCH-289 might also be appropriate!)\nIn our project MIA (http://mia-marktplatz.de/) we spider the german www. To stay polite we had to switch to a 'byIP' policy to guarantee request frequencies of at least one minute per server. Crawling 'byHost' was no option, because many sites use up to some thousand subdomains hosted at a single server with one ip address.\nIn proceeding with our crawl I realized that crawling by IP seemed to slow down, because in the process of generating the url lists nutch has to determine the ip address to build up the queues for urls according to their ip addresses.\nThis solution is a simple solution which writes the once determined ip address into the metadata field of the CrawlDatum object. When a crawl cycle has finished its fetch job an additional map-reduce job is started to determine the ip addresses of newly fetched and parsed urls. New urls are inserted into the crawldb with their ip addresses if an ip address could have been determined.\nIn this solution there exist also the two classes IpAddressResolver.java and DNSCache.java which cache already fetched ip addresses from the DNS and control the number of concurrent calls to the DNS from each map job. Since many urls with the same ip address should be generated into a queue I wanted to minimize the load which is taken to build up the queues. Caching ip addresses in-memory shouldn't be memory-consuming. To avoid to many concurrent requests to a DNS from the crawler, I added some code to restrict the number of parallel requests to the DNS.\nI use this piece of code in production since about three-quarters this year and it seems to work fine. The four configuration entries should be self-explaining.\nCheers, Walter",
        "Issue Links": [
            "/jira/browse/NUTCH-1360"
        ]
    },
    "NUTCH-1714": {
        "Key": "NUTCH-1714",
        "Summary": "Nutch 2.x upgrade to Gora 0.4",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": "Alparslan Avc\u0131",
        "Reporter": "Alparslan Avc\u0131",
        "Created": "23/Jan/14 16:00",
        "Updated": "04/Jun/14 11:19",
        "Resolved": "15/May/14 08:10",
        "Description": "Nutch upgrade for GORA_94 branch has to be implemented. We can discuss the details in this issue.",
        "Issue Links": [
            "/jira/browse/NUTCH-1674",
            "/jira/browse/GORA-326",
            "/jira/browse/NUTCH-1778",
            "/jira/browse/NUTCH-1777",
            "/jira/browse/NUTCH-1791"
        ]
    },
    "NUTCH-1715": {
        "Key": "NUTCH-1715",
        "Summary": "RobotRulesParser adds additional '*' to the robots name",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "fetcher",
        "Assignee": "Tejas Patil",
        "Reporter": "Tejas Patil",
        "Created": "24/Jan/14 17:28",
        "Updated": "01/May/14 06:22",
        "Resolved": "24/Jan/14 17:47",
        "Description": "In RobotRulesParser, when Nutch creates a agent string from multiple agents, it combines agents from both 'http.agent.name' and 'http.robots.agents'. Along with that it appends a wildcard (ie. *) to it in the end. This is sent to crawler commons while parsing the rules. The wildcard gets matched first in robots file with (User-agent: *) if that comes before any other matching rule thus resulting in a allowed url being robots denied. \nThis issue was reported by markus17. The discussion over nutch-user is here:\nhttp://mail-archives.apache.org/mod_mbox/nutch-user/201401.mbox/%3CCAFKhtFzBRpVv4MULSxw8RDRR_wbivOt%3DnhFX-w621BR8q%2BxVDQ%40mail.gmail.com%3E",
        "Issue Links": []
    },
    "NUTCH-1716": {
        "Key": "NUTCH-1716",
        "Summary": "RobotRulesParser adds extra '*' to the robots name",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "fetcher",
        "Assignee": "Tejas Patil",
        "Reporter": "Tejas Patil",
        "Created": "24/Jan/14 17:38",
        "Updated": "01/May/14 06:22",
        "Resolved": "24/Jan/14 17:41",
        "Description": "In RobotRulesParser, when Nutch creates a agent string from multiple agents, it combines agents from both 'http.agent.name' and 'http.robots.agents'. Along with that it appends a wildcard (ie. *) to it in the end. This is sent to crawler commons while parsing the rules. The wildcard gets matched first in robots file with (User-agent: *) if that comes before any other matching rule thus resulting in a allowed url being robots denied. \nThis bug was reported by @Markus Jelsma. The discussion over nutch-user can be found here:\nhttp://mail-archives.apache.org/mod_mbox/nutch-user/201401.mbox/%3CCAFKhtFzBRpVv4MULSxw8RDRR_wbivOt%3DnhFX-w621BR8q%2BxVDQ%40mail.gmail.com%3E",
        "Issue Links": []
    },
    "NUTCH-1717": {
        "Key": "NUTCH-1717",
        "Summary": "HostDB not to complain if filters/normalizers are disabled",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Jan/14 15:14",
        "Updated": "19/Nov/21 15:28",
        "Resolved": "19/Nov/21 15:28",
        "Description": "Some filters and normalizers must not be enabled using the HostDB. Nutch should not complain if filtering and/or normalizing is disabled.",
        "Issue Links": []
    },
    "NUTCH-1718": {
        "Key": "NUTCH-1718",
        "Summary": "redefine http.robots.agent as \"additional agent names\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "28/Jan/14 08:54",
        "Updated": "20/Jun/14 22:54",
        "Resolved": "20/Jun/14 22:17",
        "Description": "The description of property http.robots.agent in nutch-default.xml recommends to add a '*' to the list of agent names. This will cause the same problem as described in NUTCH-1715. The description should be updated. Also regarding \"order of precedence\" which is dictated since NUTCH-1031 only by ordering of user agents in robots.txt.\n\n<property>\n  <name>http.robots.agents</name>\n  <value>*</value>\n  <description>The agent strings we'll look for in robots.txt files,\n  comma-separated, in decreasing order of precedence. You should\n  put the value of http.agent.name as the first agent name, and keep the\n  default * at the end of the list. E.g.: BlurflDev,Blurfl,*\n  </description>\n</property>",
        "Issue Links": []
    },
    "NUTCH-1719": {
        "Key": "NUTCH-1719",
        "Summary": "DomainStatistics fails in 2.x because URL is not unreversed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerhard Gossen",
        "Created": "28/Jan/14 13:51",
        "Updated": "01/May/14 06:22",
        "Resolved": "30/Jan/14 10:58",
        "Description": "With Nutch 2.x, org.apache.nutch.util.domain.DomainStatistics always returns the counts only for FETCHED/NOT_FETCHED. The reason is that the mapper tries to create a java.net.URL directly from the row key without unreversing it first and silently ignores the thrown exception.\nThe attached patch calls TableUtil.unreverseUtil first. In my test (against current 2.x-trunk) it produces correct results.",
        "Issue Links": []
    },
    "NUTCH-1720": {
        "Key": "NUTCH-1720",
        "Summary": "Duplicate lines in HttpBase.java",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Walter Tietze",
        "Created": "29/Jan/14 14:35",
        "Updated": "01/May/14 06:22",
        "Resolved": "16/Apr/14 14:29",
        "Description": "In class HttpBase exist duplicate lines in check for http state '410'.\nMoreover in the additional log message the Text-Object is used instead of the URL object.\nSource File is ./src/plugin/lib-http/src/java/org/apache/nutch/protocol/http/api/HttpBase.java",
        "Issue Links": []
    },
    "NUTCH-1721": {
        "Key": "NUTCH-1721",
        "Summary": "Upgrade to Crawler commons 0.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7,                                            2.2,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.8",
        "Component/s": "None",
        "Assignee": "Tejas Patil",
        "Reporter": "Tejas Patil",
        "Created": "31/Jan/14 14:15",
        "Updated": "01/May/14 06:22",
        "Resolved": "09/Feb/14 09:11",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1722": {
        "Key": "NUTCH-1722",
        "Summary": "FetcherJob#fetch throws NullPointerException for null batchId",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Gerhard Gossen",
        "Created": "03/Feb/14 16:23",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The documentation of FetcherJob#fetch claims that a null batchId parameter is interpreted as \"fetch all generated fetchlists\". However, the implementation tries to do batchId.equals(Nutch.ALL_BATCH_ID_STR), which throws a NullPointerException when batchId is null.\nThe provided patch sets a null batchId to Nutch.ALL_BATCH_ID_STR, which should be the documented behavior.",
        "Issue Links": []
    },
    "NUTCH-1723": {
        "Key": "NUTCH-1723",
        "Summary": "nutch updatedb fails due to avro (de)serialization issues on images",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3,                                            2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "crawldb,                                            parser",
        "Assignee": null,
        "Reporter": "Koen Smets",
        "Created": "04/Feb/14 20:06",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Running `bin/crawl` for 2 iterations using either the nutch-2.2.1 release or  the latest 2.x checkout on a seed file containing for example http://www.mountsinai.on.ca and http://www.dhzb.de (or any other webpage with image files with no obvious file extensions) causes to throw either java.lang.IllegalArgument, IOException and/or OutOfBoundsExceptions in the the readFields function of WebPageWritable:\n  @Override\n  public void readFields(DataInput in) throws IOException \n{\n    webPage = IOUtils.deserialize(getConf(), in, webPage, WebPage.class);\n  }\n\n  @Override\n  public void write(DataOutput out) throws IOException \n{\n    IOUtils.serialize(getConf(), out, webPage, WebPage.class);\n  }\n\n2014-02-04 13:50:15,421 INFO  util.WebPageWritable - Try reading fields: ...\n2014-02-04 13:50:15,423 ERROR util.WebPageWritable - Error - Failed to read fields: http://www.mountsinai.on.ca/carousel/patient-care-banner/image\n2014-02-04 13:50:15,423 ERROR util.WebPageWritable - Error - Reading fields of the WebPage class failed - java.lang.IllegalArgumentException\n2014-02-04 13:50:15,425 ERROR util.WebPageWritable - Error - Printing stacktrace - java.lang.IllegalArgumentException\nOr, \njava.lang.IndexOutOfBoundsException\n        at java.nio.Buffer.checkBounds(Buffer.java:559)\n        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:143)\n        at org.apache.avro.ipc.ByteBufferInputStream.read(ByteBufferInputStream.java:52)\n        at org.apache.avro.io.DirectBinaryDecoder.doReadBytes(DirectBinaryDecoder.java:183)\n        at org.apache.avro.io.BinaryDecoder.readString(BinaryDecoder.java:265)\n        at org.apache.gora.mapreduce.FakeResolvingDecoder.readString(FakeResolvingDecoder.java:131)\n        at org.apache.avro.generic.GenericDatumReader.readString(GenericDatumReader.java:280)\n        at org.apache.avro.generic.GenericDatumReader.readMap(GenericDatumReader.java:191)\n        at org.apache.gora.avro.PersistentDatumReader.readMap(PersistentDatumReader.java:183)\n        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:83)\n        at org.apache.gora.avro.PersistentDatumReader.readRecord(PersistentDatumReader.java:139)\n        at org.apache.avro.generic.GenericDatumReader.read(GenericDatumReader.java:80)\n        at org.apache.gora.avro.PersistentDatumReader.read(PersistentDatumReader.java:103)\n        at org.apache.gora.avro.PersistentDatumReader.read(PersistentDatumReader.java:98)\n        at org.apache.gora.mapreduce.PersistentDeserializer.deserialize(PersistentDeserializer.java:73)\n        at org.apache.gora.mapreduce.PersistentDeserializer.deserialize(PersistentDeserializer.java:36)\n        at org.apache.gora.util.IOUtils.deserialize(IOUtils.java:205)\n        at org.apache.nutch.util.WebPageWritable.readFields(WebPageWritable.java:45)\n        at org.apache.nutch.util.GenericWritableConfigurable.readFields(GenericWritableConfigurable.java:54)\n        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)\n        at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)\n        at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:117)\n        at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:176)\n        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)\nThe exceptions are caused by image files that sneak through the urlfilter (no extension indicating an image file) and that get (properly?) parsed by tika library.\nNote that silently catching the thrown exceptions causes corruption of the Cassandra database, as the deserializer reads over multiple webpage entries in the DataInput. Resulting in a loss of several pages of other host present in the seed file.\nMoreover, if one makes sure that the image pages don't end up in the DataInput written by DBUpdateMapper, e.g. by configuring nutch-site.xml to disable the tika parser, the nutch dbupdate finishes properly.\n<property>\n  <name>plugin.excludes</name>\n  <value>parse-tika</value>\n</property>\nI highly suspect that the issues are due to gora's dependency on the outdated avro-1.3.3 library.",
        "Issue Links": []
    },
    "NUTCH-1724": {
        "Key": "NUTCH-1724",
        "Summary": "LinkDBReader to support regex output filtering",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "10/Feb/14 10:57",
        "Updated": "13/Feb/15 15:09",
        "Resolved": "13/Feb/15 12:28",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1725": {
        "Key": "NUTCH-1725",
        "Summary": "CleaningJob's reducer does not commit deleted docs.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "\u0130lhami KALKAN",
        "Created": "11/Feb/14 13:55",
        "Updated": "04/May/14 22:10",
        "Resolved": "03/May/14 13:22",
        "Description": "In cleanup(Context context) method, \"if condition\" has logical problem.",
        "Issue Links": []
    },
    "NUTCH-1726": {
        "Key": "NUTCH-1726",
        "Summary": "HeadingsFilter does not find nested nodes",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Feb/14 12:08",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Filter won't find:\n\n<h1><span>apache nutch</span></h1>\n\n\nThe getNodeValue() tries to read data from children but should traverse nodes instead.",
        "Issue Links": []
    },
    "NUTCH-1727": {
        "Key": "NUTCH-1727",
        "Summary": "Configurable length for Tlds",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sertac TURKEL",
        "Created": "12/Feb/14 16:39",
        "Updated": "01/May/14 06:22",
        "Resolved": "27/Mar/14 17:19",
        "Description": "Length of the tld  should be selectable, there is some available tld's like .travel and url-validator plugin filters this type of urls.",
        "Issue Links": []
    },
    "NUTCH-1728": {
        "Key": "NUTCH-1728",
        "Summary": "indexer-solr plugin is not delete docs from solr",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "\u0130lhami KALKAN",
        "Created": "13/Feb/14 13:29",
        "Updated": "04/May/14 22:10",
        "Resolved": "02/May/14 09:58",
        "Description": "Missing \"delete\" variable used in delete(String key) method setting.",
        "Issue Links": []
    },
    "NUTCH-1729": {
        "Key": "NUTCH-1729",
        "Summary": "Upgrade to Tika 1.5",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.8",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "20/Feb/14 09:09",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "21/Feb/14 12:07",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1730": {
        "Key": "NUTCH-1730",
        "Summary": "Scoring-depth optionally not to increment depth for external hosts",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Feb/14 16:56",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Currently, the plugin always increments depth, even when coming or going to external hosts.",
        "Issue Links": []
    },
    "NUTCH-1731": {
        "Key": "NUTCH-1731",
        "Summary": "Better cmd line parsing for NutchServer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "2.3",
        "Component/s": "REST_api",
        "Assignee": "Fjodor Vershinin",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Feb/14 17:57",
        "Updated": "01/May/14 06:22",
        "Resolved": "14/Apr/14 18:35",
        "Description": "We can't currently stop a running server without killing the job via pid or something similar.\nA simple switch should be added to permit this.\nAll is needs to do is call NutchServer#stop which will check to see if there are running tasks... if not then gracefully shut down the server instance.",
        "Issue Links": []
    },
    "NUTCH-1732": {
        "Key": "NUTCH-1732",
        "Summary": "IndexerMapReduce to delete explicitly not indexable documents",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "28/Feb/14 14:40",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "In a continuous crawl a previously successfully indexed document (identified by a URL) can become \"not indexable\" for a couple of reasons and must then explicitly deleted from the index. Some cases are handled in IndexerMapReduce (duplicates, gone documents or redirects, cf. NUTCH-1139) but others are not:\n\nfailed to parse (but previously successfully parsed): e.g., the document became larger and is now truncated\nrejected by indexing filter (but previously accepted)\n\nIn both cases (maybe there are more) the document should be explicitly deleted (if -deleteGone is set). Note that this cannot be done in CleaningJob because data from segments is required. \nWe should also update/add a description for -deleteGone: it does not only trigger deletion of gone documents but also of redirects and duplicates (and unparseable and skipped docs).",
        "Issue Links": []
    },
    "NUTCH-1733": {
        "Key": "NUTCH-1733",
        "Summary": "parse-html to support HTML5 charset definitions",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Mar/14 10:55",
        "Updated": "01/May/14 06:22",
        "Resolved": "09/Apr/14 22:08",
        "Description": "HTML 5 allows to specify the character encoding of a page per\n\n<meta charset=\"...\">\nUnicode Byte Order Mark (BOM)\n\nThese are allowed in addition to previous HTTP/http-equiv Content-Type, see [1.\nParse-html ignores both meta charset and BOM, falls back to the default encoding (cp1252). Parse-tika sets the encoding appropriately.",
        "Issue Links": []
    },
    "NUTCH-1734": {
        "Key": "NUTCH-1734",
        "Summary": "Make SolrIndexWriter more intelligent",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7,                                            2.2.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lajos Moczar",
        "Created": "13/Mar/14 12:40",
        "Updated": "16/Mar/14 13:18",
        "Resolved": null,
        "Description": "The current mapping of the NutchDocument to SolrDocument is based on the fields in the former which potentially can cause problems when you are using an existing Solr schema:\n1) the existing logic requires Solr to support all Nutch fields, which might not be the case (like segment).\n2) you can map a Nutch field to at most 2 Solr fields (i.e. one via a <field> and one via a <copy> tag because the source attribute is the Map key and therefore you can only have one.\nAdditionally, it would be nice to support some level of transformations, literals, etc, like used in Solr DIH.\nI propose to make the code more intelligent so that, while supporting the existing \"strict\" mapping that people are used to, allows more flexible and intelligent mapping. It will also include a transformation architecture that can be expanded over time.\nThe general approach is to reverse the building of the SolrDocument, and populate the doc based on the Solr destination fields as defined in solrindex-mapping.xml, i.e., it populates the doc based on what the target Solr wants to receive, not just what Nutch wants to send. The Map of fields in solrindex-mapping.xml will be keyed by dest, i.e. the Solr field name, not source. That way one can map a source to multiple destinations. A mapping type attribute (defaults to just a simple copy from Nutch to Solr) will support literals and transformations.\nNote that a default \"strict\" mapping (i.e. the Solr schema by default MUST support all NutchDocument fields) will be supported for backwards compatibility. I assume this will be what people want.\nI will submit patches in due course.",
        "Issue Links": []
    },
    "NUTCH-1735": {
        "Key": "NUTCH-1735",
        "Summary": "code dedup fetcher queue redirects",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.9",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Mar/14 00:37",
        "Updated": "10/Feb/15 22:18",
        "Resolved": "05/Apr/14 17:06",
        "Description": "20 lines of duplicated code in Fetcher when a new FetchItem is created for a redirect and queued.",
        "Issue Links": []
    },
    "NUTCH-1736": {
        "Key": "NUTCH-1736",
        "Summary": "Can't fetch page if http response header contains Transfer-Encoding\uff1achunked",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.8",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "ysc",
        "Created": "15/Mar/14 07:32",
        "Updated": "09/Jan/15 06:58",
        "Resolved": "11/Jun/14 15:57",
        "Description": "fetching: http://szs.mof.gov.cn/zhengwuxinxi/zhengcefabu/201402/t20140224_1046354.html\nFetch failed with protocol status: EXCEPTION: java.io.IOException: unzipBestEffort returned null",
        "Issue Links": [
            "/jira/browse/NUTCH-1647",
            "/jira/browse/NUTCH-1270"
        ]
    },
    "NUTCH-1737": {
        "Key": "NUTCH-1737",
        "Summary": "Upgrade to recent JUnit 4.x",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Mar/14 21:27",
        "Updated": "29/Mar/14 21:38",
        "Resolved": "29/Mar/14 00:58",
        "Description": "While trunk still remains on JUnit 3.8.1, 2.x uses JUnit 4.11 and has already upgraded tests (NUTCH-1573). This makes it difficult to port tests which use JUnit 4 features from 2.x to trunk. There are two solutions:\n\n(lightweight, trivial patch attached) upgrade only ivy dependency, upgrade tests later\nupgrade also all tests to use JUnit 4 annotations and setup(), cf. NUTCH-1573",
        "Issue Links": []
    },
    "NUTCH-1738": {
        "Key": "NUTCH-1738",
        "Summary": "Expose number of URLs generated per batch in GeneratorJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "generator",
        "Assignee": "Talat Uyarer",
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Mar/14 10:19",
        "Updated": "01/May/14 06:22",
        "Resolved": "18/Mar/14 22:07",
        "Description": "GeneratorJob contains one trivial line of logging\nGeneratorJob.java\nLOG.info(\"GeneratorJob: generated batch id: \" + batchId);\n\n\nI propose to improve this logging by exposing how many URL's are contained within the generated batch. Something like\nGeneratorJob.java\nLOG.info(\"GeneratorJob: generated batch id: \" + batchId + \" containing \" + $numOfURLs + \" URLs\");",
        "Issue Links": []
    },
    "NUTCH-1739": {
        "Key": "NUTCH-1739",
        "Summary": "ExecutorService field in ParseUtil.java not be right use and cause memory leak",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6,                                            1.7",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "ysc",
        "Created": "18/Mar/14 06:44",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "03/Apr/14 15:20",
        "Description": "########################Problem########################\njava.lang.Exception: java.lang.OutOfMemoryError: unable to create new native thread\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:354)\nCaused by: java.lang.OutOfMemoryError: unable to create new native thread\n        at java.lang.Thread.start0(Native Method)\n        at java.lang.Thread.start(Thread.java:640)\n        at java.util.concurrent.ThreadPoolExecutor.addThread(ThreadPoolExecutor.java:681)\n        at java.util.concurrent.ThreadPoolExecutor.addIfUnderMaximumPoolSize(ThreadPoolExecutor.java:727)\n        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:655)\n        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:92)\n        at org.apache.nutch.parse.ParseUtil.runParser(ParseUtil.java:159)\n        at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:93)\n        at org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:97)\n        at org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:44)\n        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:366)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)\n        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:138)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)\n        at java.lang.Thread.run(Thread.java:662)\n########################Analysis########################\nMy server use JDK32. I began thought it was not specify enough memory. I passed the test of java -Xmx2600m -version so I known my server can use the max memory is 2.6G. So, I add one line config  NUTCH_HEAPSIZE=2000 to the script of bin/nutch. But it's not solve the problem.\nThen, I check the source code to see where to produce so many threads. I find the code\n\n parseResult = new ParseUtil(getConf()).parse(content); \n\n\nwhich in line 97 of the java source file org.apache.nutch.parse.ParseSegment.java's map method.\nContinue, In the constructor of ParseUtil,  instantiate a CachedThreadPool object which no limit of the pool size , see the code:\n\nexecutorService = Executors.newCachedThreadPool(new ThreadFactoryBuilder()\n      .setNameFormat(\"parse-%d\").setDaemon(true).build());\n\n\nThrough the above analyse, I know each map method's output will  instantiate a CachedThreadPool and not to close it. So, ExecutorService field in ParseUtil.java not be right use and cause memory leak.\n########################Solution########################\nEach map method use a shared FixedThreadPool object which's size can be config in nutch-site.xml, more detail see the patch file.",
        "Issue Links": []
    },
    "NUTCH-1740": {
        "Key": "NUTCH-1740",
        "Summary": "BatchId parameter is not set in DbUpdaterJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Alparslan Avc\u0131",
        "Created": "18/Mar/14 14:38",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/May/14 16:01",
        "Description": "BatchId is not set in DbUpdaterJob since batchId is set to configuration after creating currentJob.",
        "Issue Links": [
            "/jira/browse/NUTCH-1556"
        ]
    },
    "NUTCH-1741": {
        "Key": "NUTCH-1741",
        "Summary": "Support of Sitemaps in Nutch 2.x",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "fetcher,                                            generator",
        "Assignee": "Cihad Guzel",
        "Reporter": "Alparslan Avc\u0131",
        "Created": "21/Mar/14 15:31",
        "Updated": "03/Mar/22 14:27",
        "Resolved": "26/Jan/16 19:24",
        "Description": "Sitemap support has to be implemented for 2.x branch. It is being discussed in NUTCH-1465 for trunk.",
        "Issue Links": [
            "/jira/browse/NUTCH-1816",
            "/jira/browse/NUTCH-1465",
            "https://github.com/apache/nutch/pull/304"
        ]
    },
    "NUTCH-1742": {
        "Key": "NUTCH-1742",
        "Summary": "Please delete old releases from mirroring system",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebb",
        "Created": "22/Mar/14 02:41",
        "Updated": "25/Mar/14 13:55",
        "Resolved": "22/Mar/14 18:19",
        "Description": "To reduce the load on the ASF mirrors, projects are required to delete old releases [1]\nPlease can you remove all non-current releases?\nThanks!\nAlso, please note that the download page refers to 1.7 in the first body paragraph. That should be 1.8\n[1] http://www.apache.org/dev/release.html#when-to-archive",
        "Issue Links": []
    },
    "NUTCH-1743": {
        "Key": "NUTCH-1743",
        "Summary": "parsechecker to show outlinks",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "26/Mar/14 23:02",
        "Updated": "01/May/14 06:22",
        "Resolved": "16/Apr/14 14:56",
        "Description": "ParserChecker in 2.x (unlike 1.x/trunk) does not show the extracted outlinks of a page.",
        "Issue Links": []
    },
    "NUTCH-1744": {
        "Key": "NUTCH-1744",
        "Summary": "FTP Issue when entering Passive mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.7",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Rafael Thomas Goz Coutinho",
        "Created": "01/Apr/14 16:17",
        "Updated": "01/Apr/14 18:08",
        "Resolved": "01/Apr/14 18:08",
        "Description": "I 'm crawling an FTP url and am getting this error:\n14/04/01 12:15:36 INFO ftp.Ftp: ftp> 227 Entering Passive Mode (10,201,140,86,145,106)\n14/04/01 12:15:36 INFO fetcher.Fetcher: fetch of ftp://192.168.1.202/ failed with: java.lang.NoSuchFieldError: socketFactory\n\tat org.apache.nutch.protocol.ftp.Client.__openPassiveDataConnection(Client.java:192)\n\tat org.apache.nutch.protocol.ftp.Client.retrieveList(Client.java:324)\n\tat org.apache.nutch.protocol.ftp.FtpResponse.getFileAsHttpResponse(FtpResponse.java:322)\n\tat org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:264)\n\tat org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:129)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:703)",
        "Issue Links": []
    },
    "NUTCH-1745": {
        "Key": "NUTCH-1745",
        "Summary": "Upgrade to ElasticSearch 1.1.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "indexer",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "02/Apr/14 15:14",
        "Updated": "07/Aug/14 10:01",
        "Resolved": "04/Apr/14 14:44",
        "Description": "ElasticSearch is currently at version 1.1.0. The patch attached upgrades the dependencies, fixes a couple of changes required by 1.1.0 and also : \n\nremoves the need for having ES in the main ivy dependency - it is now only required at the plugin level\nimproves the logic around using the cluster name or an explicit host:port to connect to ES : the clustername is not required nor set when using host:port\nuses a more sensible default value for the port",
        "Issue Links": []
    },
    "NUTCH-1746": {
        "Key": "NUTCH-1746",
        "Summary": "OutOfMemoryError in Mappers",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7",
        "Fix Version/s": "None",
        "Component/s": "generator,                                            injector",
        "Assignee": null,
        "Reporter": "Greg Padiasek",
        "Created": "03/Apr/14 13:56",
        "Updated": "19/Oct/17 21:21",
        "Resolved": null,
        "Description": "Initially I found that Generator was throwing OutOfMemoryError exception no matter how much RAM I allocated to JVM. I fixed the problem by moving URLFilters, URLNormalizers and ScoringFilters to top-level class as singletons and re-using them in all Generator mapper instances.\nThen I found the same problem in Injector and applied analogical fix.\nNow it seems that this issue may be common in all Nutch Mapper implementations.\nI was wondering if it would it be possible to integrate this kind of change\nin the upstream code base and potentially update all vulnerable Mapper classes.",
        "Issue Links": [
            "/jira/browse/NUTCH-2407"
        ]
    },
    "NUTCH-1747": {
        "Key": "NUTCH-1747",
        "Summary": "Use AtomicInteger as semaphore in Fetcher",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "fetcher",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "05/Apr/14 07:27",
        "Updated": "14/Apr/14 11:38",
        "Resolved": "05/Apr/14 19:55",
        "Description": "In Fetcher we currently use \nSet<FetchItem>  inProgress = Collections.synchronizedSet(new HashSet<FetchItem>());\nas semaphore within the FetchItemQueues to keep track of the URLs being fetched and prevent threads from pulling from them. It works fine but we could use AtomicIntegers instead as all we need is the counts, not the contents.\nThis change would have little impact on the performance but would make the code a bit cleaner.",
        "Issue Links": []
    },
    "NUTCH-1748": {
        "Key": "NUTCH-1748",
        "Summary": "urlfilter-validator to allow .. (two dots) inside file names (path elements)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sertac TURKEL",
        "Created": "05/Apr/14 20:35",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Unix systems accept files containing two dots \"abc..xyz.txt\". So\nurlfilter-validator should not  reject this kind of urls. Also paths containing \"/../\" or \"/..\" in final position should be still rejected.",
        "Issue Links": []
    },
    "NUTCH-1749": {
        "Key": "NUTCH-1749",
        "Summary": "Optionally exclude title from content field",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.7",
        "Fix Version/s": "1.20",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Greg Padiasek",
        "Created": "06/Apr/14 04:04",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The HTML parser plugin inserts document title into document content. Since the title alone can be retrieved via DOMContentUtils.getTitle() and content is retrieved via DOMContentUtils.getText(), there is no need to duplicate title in the content. When title is included in the content it becomes difficult/impossible to extract document body without title. A need to extract document body without title is visible when user wants to index or display body and title separately.\nAttached is a patch which prevents including title in document content in the HTML parser plugin.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/285"
        ]
    },
    "NUTCH-1750": {
        "Key": "NUTCH-1750",
        "Summary": "Improvement of Fetcher's reportStatus",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "06/Apr/14 13:05",
        "Updated": "09/Apr/14 10:14",
        "Resolved": "09/Apr/14 08:36",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1751": {
        "Key": "NUTCH-1751",
        "Summary": "Empty anchors should not index",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sertac TURKEL",
        "Created": "09/Apr/14 06:43",
        "Updated": "01/May/14 06:22",
        "Resolved": "09/Apr/14 23:40",
        "Description": "Empty anchors should not index.",
        "Issue Links": []
    },
    "NUTCH-1752": {
        "Key": "NUTCH-1752",
        "Summary": "cache robots.txt rules per protocol:host:port",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Apr/14 09:00",
        "Updated": "18/May/14 03:44",
        "Resolved": "12/May/14 19:40",
        "Description": "HttpRobotRulesParser caches rules from robots.txt per \"protocol:host\" (before NUTCH-1031 caching was per \"host\" only). The caching should be per \"protocol:host:port\". In doubt, a request to a different port may deliver a different robots.txt. \nApplying robots.txt rules to a combination of host, protocol, and port is common practice:\nNorobots RFC 1996 draft does not mention this explicitly (could be derived from examples) but others do:\n\nWikipedia: \"each protocol and port needs its own robots.txt file\"\nGoogle webmasters: \"The directives listed in the robots.txt file apply only to the host, protocol and port number where the file is hosted.\"",
        "Issue Links": []
    },
    "NUTCH-1753": {
        "Key": "NUTCH-1753",
        "Summary": "Eclipse dependecy problem for 2.x",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": "Talat Uyarer",
        "Reporter": "Talat Uyarer",
        "Created": "09/Apr/14 11:16",
        "Updated": "04/May/14 22:10",
        "Resolved": "02/May/14 08:10",
        "Description": "When running Nutch 2.x on eclipse if dependencies is not added in eclipse target of build.xml some plugins do not work correctly.",
        "Issue Links": [
            "/jira/browse/NUTCH-710"
        ]
    },
    "NUTCH-1754": {
        "Key": "NUTCH-1754",
        "Summary": "remove BOM from extracted plain text",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "09/Apr/14 22:05",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "(reported by jlafitte, see NUTCH-1733)\nPlain-text content extracted by parse-html should not contain a leading Unicode Byte Order Mark (BOM), ev. followed by white space characters.",
        "Issue Links": []
    },
    "NUTCH-1755": {
        "Key": "NUTCH-1755",
        "Summary": "Project name bug in build.xml",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "build,                                            nutchNewbie",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "09/Apr/14 23:11",
        "Updated": "07/Jan/15 18:09",
        "Resolved": "07/Jan/15 18:08",
        "Description": "When one enters an incorrect ant target as follows, you can see that project name is presented to us as \"${name}\". This should resolve to final.name value in default.properties http://svn.apache.org/repos/asf/nutch/branches/2.x/default.properties.\nI would imagine that this bug exists in both 1.X branch and 2.x.\nlewismc@vm-0:~/apache/2.x$ ant plugins-test                                                                                                                                                      \nBuildfile: /home/sarahp098/apache/2.x/build.xml\nTrying to override old definition of task javac\n[taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\nBUILD FAILED\nTarget \"plugins-test\" does not exist in the project \"${name}\".",
        "Issue Links": []
    },
    "NUTCH-1756": {
        "Key": "NUTCH-1756",
        "Summary": "Security layer for NutchServer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Apr/14 18:39",
        "Updated": "23/Aug/16 21:52",
        "Resolved": "23/Aug/16 21:52",
        "Description": "It will be beneficial to have a security layer for NutchServer once we make improvements upon it. I hope that GSoC goes ahead this year so we can tackle such issues.\nThis issue should implement a standard security layer for REST API calls. It should also add/expose this functionality through the WebApp.",
        "Issue Links": [
            "/jira/browse/NUTCH-2243",
            "/jira/browse/NUTCH-2022"
        ]
    },
    "NUTCH-1757": {
        "Key": "NUTCH-1757",
        "Summary": "ParserChecker to take custom metadata as input",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "nutchNewbie,                                            parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "16/Apr/14 15:14",
        "Updated": "23/May/14 19:54",
        "Resolved": "21/May/14 19:50",
        "Description": "The patch attached allows to pass custom metadata on the command line (-md key=value) to the ParserChecker. This allows to have a similar behaviour as injecting metadata via the seed files. Some custom parser implementations can rely on such metadata, which is why the ParserChecker must allow to pass them.",
        "Issue Links": []
    },
    "NUTCH-1758": {
        "Key": "NUTCH-1758",
        "Summary": "IndexChecker to send document to IndexWriters",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9",
        "Component/s": "indexer,                                            nutchNewbie",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "16/Apr/14 15:31",
        "Updated": "29/May/14 13:53",
        "Resolved": "29/May/14 10:21",
        "Description": "The patch attached allows to send the document which is being tested to the IndexWriters configured. This is a good way of testing that the latter are properly configured and that the indexing behaves as expected",
        "Issue Links": []
    },
    "NUTCH-1759": {
        "Key": "NUTCH-1759",
        "Summary": "Upgrade to Crawler Commons 0.4",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Apr/14 22:33",
        "Updated": "01/May/14 06:22",
        "Resolved": "28/Apr/14 11:03",
        "Description": "We recently releaed CC 0.4. We should upgrade.",
        "Issue Links": []
    },
    "NUTCH-1760": {
        "Key": "NUTCH-1760",
        "Summary": "Crawl script fails to find job file if called from outside bin dir",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Hosking",
        "Created": "17/Apr/14 07:58",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "17/Apr/14 09:26",
        "Description": "The crawl script that comes with all the version of Nutch I have checked set the local/distributed operating mode using a relative path (i.e. \"../nutch-.job\").\nBash seems to be taking this as relative to the location that the crawl script was called from, not the scripts actual location.\nThe result is that the script thinks it is in local mode because it cannot find the job file.  When trying to carry out a crawl jobs are submitted to Hadoop properly, but ifs that test for local (or not) mode fail and give strange results/result in crashes.\nUsing the first bash snippet from here I have modified the crawl script to look for a job file relative to the script location on disk.\nI have attached a patch with my modifications.",
        "Issue Links": [
            "/jira/browse/NUTCH-1761"
        ]
    },
    "NUTCH-1761": {
        "Key": "NUTCH-1761",
        "Summary": "Crawl script fails to find job file if not started from inside bin dir",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "David Hosking",
        "Created": "17/Apr/14 08:01",
        "Updated": "17/Apr/14 11:59",
        "Resolved": "17/Apr/14 10:03",
        "Description": "The crawl script that comes with all the version of Nutch I have checked set the local/distributed operating mode using a relative path (i.e. \"../*nutch-*.job\").\nBash seems to be taking this as relative to the location that the crawl script was called from, not the scripts actual location.\nThe result is that the script thinks it is in local mode because it cannot find the job file.  When trying to carry out a crawl jobs are submitted to Hadoop properly, but ifs that test for local (or not) mode fail and give strange results/result in crashes.\nUsing the first bash snippet from here I have modified the crawl script to look for a job file relative to the script location on disk.\nI have attached a patch with my modifications.",
        "Issue Links": [
            "/jira/browse/NUTCH-1760"
        ]
    },
    "NUTCH-1762": {
        "Key": "NUTCH-1762",
        "Summary": "project web site's search (provided by lucid) is broken",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Ahmet Arslan",
        "Created": "23/Apr/14 01:14",
        "Updated": "25/Apr/14 18:02",
        "Resolved": "24/Apr/14 18:11",
        "Description": "Lucid Imagination is now LucidWorks. Site search link needs to be updated.",
        "Issue Links": [
            "/jira/browse/CONNECTORS-521"
        ]
    },
    "NUTCH-1763": {
        "Key": "NUTCH-1763",
        "Summary": "Improving comments on the Injector Class",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.14",
        "Component/s": "injector",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Diaa",
        "Created": "24/Apr/14 12:33",
        "Updated": "19/Oct/17 22:09",
        "Resolved": "19/Oct/17 21:36",
        "Description": "I think the Injector class could use some improvements in the comments.\nI am attaching a few improvements to that and will keep adding as I understand it more.",
        "Issue Links": []
    },
    "NUTCH-1764": {
        "Key": "NUTCH-1764",
        "Summary": "readdb to show command-line help if no action (-stats, -dump, etc.) given",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            1.9",
        "Fix Version/s": "1.9",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Diaa",
        "Created": "25/Apr/14 11:43",
        "Updated": "11/May/14 15:36",
        "Resolved": "26/Apr/14 22:13",
        "Description": "If you run the command readdb with just one argument nothing happens and no usage warning is issued.\nExample: bin/nutch readdb crawldb\nActual Result: Nothing happens\nExpected Result: \"Usage CrawlDbReader ... \"\nThe issue is due to \"if (args.length < 1) \" which should be 2",
        "Issue Links": []
    },
    "NUTCH-1765": {
        "Key": "NUTCH-1765",
        "Summary": "SolrClean to remove redirected URLs from Solr",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Iain Lopata",
        "Created": "26/Apr/14 22:41",
        "Updated": "15/Jul/14 09:42",
        "Resolved": "15/Jul/14 09:42",
        "Description": "SolrClean currently only removes urls with a status of STATUS_DB_GONE from the Solr Index.  It should also remove urls with a status of  STATUS_DB_REDIR_TEMP and  STATUS_DB_REDIR_PERM.",
        "Issue Links": []
    },
    "NUTCH-1766": {
        "Key": "NUTCH-1766",
        "Summary": "Generator to unlock crawldb and remove tempdir if generate job fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.9",
        "Component/s": "generator",
        "Assignee": "Julien Nioche",
        "Reporter": "Diaa",
        "Created": "27/Apr/14 21:54",
        "Updated": "18/May/14 03:44",
        "Resolved": "12/May/14 08:00",
        "Description": "Currently if the generate job fails the following happens:\n1. generate temp directory remains\n2. crawldb remains in lock\nThe next time generate is called the following happens:\nIO exception due to crawldb being in lock mode\nAttached is a patch that cleans up after a job fails in generate by unlocking crawldb and removing the generate temp directory so that if a job fails the next time generate is called it wouldn't give an IO Exception.",
        "Issue Links": []
    },
    "NUTCH-1767": {
        "Key": "NUTCH-1767",
        "Summary": "remove special treatment of \"params\" in relative links",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "27/Apr/14 22:19",
        "Updated": "20/Jun/14 23:53",
        "Resolved": "20/Jun/14 22:56",
        "Description": "RFC 1808 specified that path elements of URLs may contains so-called params startet by \";\", e.g. \";type=a\". If the base URL contains a path param while the link target does not, params are transferred to the target:\n\nStep 5: \n a) if the embedded URL's <params> is non-empty, we skip to\n     step 7; otherwise, it inherits the <params> of the base URL (if any)\nThis behaviour has been implemented with NUTCH-436. Later (NUTCH-1115) it had been made optional and configurable by property parser.fix.embeddedparams. NUTCH-797 made the changes of both issues inactive for 1.x (not applied to 2.x) with reference to RFC 3986.\nRFC 3986 which obsoletes RFC 1808 does not mention params and examples given in sect. 5.4. \"Reference Resolution Examples\" contradict RFC 1808. Also Wikipedia states:\n\nHistorically, each segment was specified to contain parameters separated from it using a semicolon (\";\"), though this was rarely used in practice and current specifications allow but no longer specify such semantics.\nAccordingly, any special treatment of \"params\" in relative links should be removed from Nutch. At a first glance, this would include:\n\n2.x parse-html and parse-tika\n\t\nremove fixEmbeddedParams(...)\nchange unit tests to follow examples from RFC 3986\n\n\n1.x\n\t\nremove unused fixEmbeddedParams(...) from parse-html\nremove property parser.fix.embeddedparams from nutch-default.xml",
        "Issue Links": []
    },
    "NUTCH-1768": {
        "Key": "NUTCH-1768",
        "Summary": "Upgrade to ElasticSearch 1.1.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "30/Apr/14 09:11",
        "Updated": "30/May/14 15:41",
        "Resolved": "30/May/14 14:57",
        "Description": "See https://issues.apache.org/jira/browse/NUTCH-1745\nElasticSearch is currently at version 1.1.0. The patch attached upgrades the dependencies, fixes a couple of changes required by 1.1.0 and also :\nremoves the need for having ES in the main ivy dependency - it is now only required at the plugin level\nimproves the logic around using the cluster name or an explicit host:port to connect to ES : the clustername is not required nor set when using host:port\nuses a more sensible default value for the port",
        "Issue Links": []
    },
    "NUTCH-1769": {
        "Key": "NUTCH-1769",
        "Summary": "REST API refactoring",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "REST_api",
        "Assignee": "Fjodor Vershinin",
        "Reporter": "Ivan Vershinin",
        "Created": "02/May/14 08:03",
        "Updated": "26/Jun/14 01:54",
        "Resolved": "26/Jun/14 00:46",
        "Description": "I'd reviewed REST API code, and realized that it is old and clunky. I want make some refactoring of this part and propose these changes as patch.",
        "Issue Links": [
            "/jira/browse/NUTCH-841"
        ]
    },
    "NUTCH-1770": {
        "Key": "NUTCH-1770",
        "Summary": "Nutch is failing to parse all PDFs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Rog\u00e9rio Pereira Ara\u00fajo",
        "Created": "03/May/14 13:24",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "09/May/14 08:51",
        "Description": "I'm trying to craw a filesystem directory containing several PDFs, but when the parsing stage starts, I'm getting the error described on ticket PDFBOX-1122",
        "Issue Links": []
    },
    "NUTCH-1771": {
        "Key": "NUTCH-1771",
        "Summary": "Solrindex fails if a segment is corrupted or incomplete",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            1.10",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Diaa",
        "Created": "12/May/14 09:55",
        "Updated": "10/Apr/15 23:33",
        "Resolved": "09/Apr/15 22:22",
        "Description": "When using solrindex to index multiple segments via -dir segment,\nthe indexing fails if one or more segments are corrupted/incomplete (generated but not fetched for example)\nThe failure is simply java.io exception.\nDeleting the segment fixes the issue.\nThe expected behavior should be one of the following:\n\nskipping the segment and proceeding with others (while logging)\nstopping the indexing and logging the failed segment",
        "Issue Links": [
            "/jira/browse/NUTCH-1905",
            "/jira/browse/NUTCH-1978"
        ]
    },
    "NUTCH-1772": {
        "Key": "NUTCH-1772",
        "Summary": "Injector does not need merging if no pre-existing crawldb",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "12/May/14 15:30",
        "Updated": "18/May/14 03:44",
        "Resolved": "16/May/14 07:59",
        "Description": "The injector currently works as following : \n\nMapReduce job 1 - Mapper :  converts input lines into CrawlDatum objects with normalisation and filtering\nMapReduce job 1 - Reducer : identity reducers. Can still have duplicates at this stage\nMapReducer job 2 - Mapper : CrawlDbFilter on existing crawldb (if any) + output of previous job\nMapReducer job 2 - Reducer : deduplication\n\nIf there is no existing crawldb (which will often be the case at injection time) we don't really need to do the second mapreduce job and could simply take the output of the MR job #1 as CrawlDB provided that we do the deduplication as part of the reduce step.\nIf there is a crawldb then the reduce step of the MR job #1 is not really needed and we could have that step as map only.",
        "Issue Links": [
            "/jira/browse/NUTCH-1712"
        ]
    },
    "NUTCH-1773": {
        "Key": "NUTCH-1773",
        "Summary": "Solr Indexer fails",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Ralf",
        "Created": "13/May/14 01:02",
        "Updated": "19/May/14 14:07",
        "Resolved": "19/May/14 14:07",
        "Description": "When using crawl script or solrindexer by itself (/bin/nutch solrindex) in localmode it fails with:\nhduser@bl4ck1c3:~/nutch-2.3/runtime/local$ bin/nutch solrindex TestCrawl18 -reindex\nIndexingJob: starting\nActive IndexWriters :\nSOLRIndexWriter\n\tsolr.server.url : URL of the SOLR instance (mandatory)\n\tsolr.commit.size : buffer size when sending to SOLR (default 1000)\n\tsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n\tsolr.auth : use authentication (default false)\n\tsolr.auth.username : use authentication (default false)\n\tsolr.auth : username for authentication\n\tsolr.auth.password : password for authentication\nSolrIndexerJob: java.lang.IllegalStateException: Target host must not be null, or set in parameters.\n\tat org.apache.http.impl.client.DefaultRequestDirector.determineRoute(DefaultRequestDirector.java:787)\n\tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:414)\n\tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:906)\n\tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:805)\n\tat org.apache.http.impl.client.AbstractHttpClient.execute(AbstractHttpClient.java:784)\n\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:393)\n\tat org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:197)\n\tat org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:117)\n\tat org.apache.solr.client.solrj.SolrServer.commit(SolrServer.java:168)\n\tat org.apache.solr.client.solrj.SolrServer.commit(SolrServer.java:146)\n\tat org.apache.nutch.indexwriter.solr.SolrIndexWriter.commit(SolrIndexWriter.java:146)\n\tat org.apache.nutch.indexer.IndexWriters.commit(IndexWriters.java:127)\n\tat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:171)\n\tat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:187)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:196)\nwhen using the new INDEX command it finishes, but nothing is added to Solr:\nhduser@bl4ck1c3:~/nutch-2.3/runtime/local$ bin/nutch index TestCrawl18 -reindex\nIndexingJob: starting\nActive IndexWriters :\nSOLRIndexWriter\n\tsolr.server.url : URL of the SOLR instance (mandatory)\n\tsolr.commit.size : buffer size when sending to SOLR (default 1000)\n\tsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n\tsolr.auth : use authentication (default false)\n\tsolr.auth.username : use authentication (default false)\n\tsolr.auth : username for authentication\n\tsolr.auth.password : password for authentication\nLog shows:\n2014-05-13 03:01:13,781 INFO  indexer.IndexingJob - IndexingJob: starting\n2014-05-13 03:01:14,108 INFO  indexer.IndexingFilters - Adding org.apache.nutch.analysis.lang.LanguageIndexingFilter\n2014-05-13 03:01:14,109 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2014-05-13 03:01:14,109 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2014-05-13 03:01:14,335 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.more.MoreIndexingFilter\n2014-05-13 03:01:14,336 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2014-05-13 03:01:14,336 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2014-05-13 03:01:14,620 WARN  zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable\n2014-05-13 03:01:14,768 WARN  zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable\n2014-05-13 03:01:14,968 WARN  zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable\n2014-05-13 03:01:15,243 WARN  zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable\n2014-05-13 03:01:15,276 WARN  zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable\n2014-05-13 03:01:15,326 WARN  zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable\n2014-05-13 03:01:15,386 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2014-05-13 03:01:15,403 INFO  solr.SolrMappingReader - source: content dest: content\n2014-05-13 03:01:15,403 INFO  solr.SolrMappingReader - source: title dest: title\n2014-05-13 03:01:15,403 INFO  solr.SolrMappingReader - source: host dest: host\n2014-05-13 03:01:15,404 INFO  solr.SolrMappingReader - source: batchId dest: batchId\n2014-05-13 03:01:15,404 INFO  solr.SolrMappingReader - source: boost dest: boost\n2014-05-13 03:01:15,404 INFO  solr.SolrMappingReader - source: digest dest: digest\n2014-05-13 03:01:15,404 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2014-05-13 03:01:15,405 INFO  indexer.IndexingFilters - Adding org.apache.nutch.analysis.lang.LanguageIndexingFilter\n2014-05-13 03:01:15,405 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2014-05-13 03:01:15,405 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2014-05-13 03:01:15,405 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.more.MoreIndexingFilter\n2014-05-13 03:01:15,405 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2014-05-13 03:01:15,405 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2014-05-13 03:01:15,426 WARN  zookeeper.ClientCnxnSocket - Connected to an old server; r-o mode will be unavailable\n2014-05-13 03:01:15,442 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2014-05-13 03:01:16,144 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2014-05-13 03:01:16,144 INFO  indexer.IndexingJob - Active IndexWriters :\nSOLRIndexWriter\n\tsolr.server.url : URL of the SOLR instance (mandatory)\n\tsolr.commit.size : buffer size when sending to SOLR (default 1000)\n\tsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n\tsolr.auth : use authentication (default false)\n\tsolr.auth.username : use authentication (default false)\n\tsolr.auth : username for authentication\n\tsolr.auth.password : password for authentication\n2014-05-13 03:01:16,145 INFO  solr.SolrMappingReader - source: content dest: content\n2014-05-13 03:01:16,145 INFO  solr.SolrMappingReader - source: title dest: title\n2014-05-13 03:01:16,145 INFO  solr.SolrMappingReader - source: host dest: host\n2014-05-13 03:01:16,145 INFO  solr.SolrMappingReader - source: batchId dest: batchId\n2014-05-13 03:01:16,145 INFO  solr.SolrMappingReader - source: boost dest: boost\n2014-05-13 03:01:16,145 INFO  solr.SolrMappingReader - source: digest dest: digest\n2014-05-13 03:01:16,145 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2014-05-13 03:01:16,338 INFO  solr.SolrIndexWriter - Total 0 document is added.\n2014-05-13 03:01:16,338 INFO  indexer.IndexingJob - IndexingJob: done.",
        "Issue Links": []
    },
    "NUTCH-1774": {
        "Key": "NUTCH-1774",
        "Summary": "Crawling from REST API giving NullPointerException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "sreemanth pulagam",
        "Created": "13/May/14 06:20",
        "Updated": "17/May/14 18:35",
        "Resolved": "17/May/14 18:35",
        "Description": "Crawling is not working from REST API.\nSteps to reproduce.\n-----------------------\n1. Start the Nutch server (port 9000).\n2. Submit the PUT request , to create/initiate crawl job.\n   eg: \n           URL: http://localhost:9000/nutch/jobs \n           HTTP METHOD: PUT\n           Content: \n                {\n                   \"crawl\":\"123\",\n                   \"type\":\"crawl\",\n                   \"conf\":\"default\",\n                   \"args\":\n{\n                      \"class\":\"org.apache.nutch.crawl.Crawler\",\n                      \"seed\":\"http://www.somesite.com\",\n                      \"seedDir\":\"runtime/local/url/url.txt\",\n                      \"depth\":2\n                   }\n                }\n3. Getting the following exception in Generator phase. \n2014-05-13 11:37:57,863 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(435)) - job_local1326997137_0002\njava.lang.NullPointerException\n\tat org.apache.avro.util.Utf8.<init>(Utf8.java:37)\n\tat org.apache.nutch.crawl.GeneratorReducer.setup(GeneratorReducer.java:100)\n\tat org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:174)\n\tat org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:649)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:418)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:398)",
        "Issue Links": []
    },
    "NUTCH-1775": {
        "Key": "NUTCH-1775",
        "Summary": "IndexingFilter: document origin of passed CrawlDatum",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "13/May/14 21:41",
        "Updated": "22/Aug/14 23:21",
        "Resolved": "22/Aug/14 22:28",
        "Description": "Only the fetch datum from segment is passed to IndexingFilters, the datum from CrawlDb is not available to IndexingFilters. This fact should be documented because there may be subtle differences between fetch and db datum (e.g., fetch time).",
        "Issue Links": []
    },
    "NUTCH-1776": {
        "Key": "NUTCH-1776",
        "Summary": "Log incorrect plugin.folder file path",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Diaa",
        "Created": "14/May/14 23:34",
        "Updated": "05/Jul/14 22:41",
        "Resolved": "05/Jul/14 21:42",
        "Description": "Currently if plugin.folder is set incorrectly there is just an IOException with no details or way of knowing that this was caused by the plugin.folder property.\nThis is a minor patch to log as an error that the path in plugin.folder is incorrect.",
        "Issue Links": []
    },
    "NUTCH-1777": {
        "Key": "NUTCH-1777",
        "Summary": "Fetcher not getting all the entries in input",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "15/May/14 08:19",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "See comments in  NUTCH-1714 :\nThe Generator marks 50K entries with GENERATE_MARK but the Fetcher shows only 49,461 as Map Input Records (and the same number as Reduce input records) => looks like we are not getting all the records we should be getting. I dumped the content of the table pre-fetching and it contains the right number of entries i.e. 50K\nThis was noticed after applying NUTCH-1714 and NUTCH-1674 but could also have been the case before that.",
        "Issue Links": [
            "/jira/browse/NUTCH-1674",
            "/jira/browse/NUTCH-1714"
        ]
    },
    "NUTCH-1778": {
        "Key": "NUTCH-1778",
        "Summary": "Generator not logging number of URLs in batch correctly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Julien Nioche",
        "Created": "15/May/14 08:21",
        "Updated": "08/Dec/14 19:54",
        "Resolved": "08/Dec/14 19:48",
        "Description": "See discussion in NUTCH-1714: \nThe Generator displayed 'generated batch id: 1399626659-15643 containing 0 URLs' but as I just explained it marked 50K entries correctly",
        "Issue Links": [
            "/jira/browse/NUTCH-1714"
        ]
    },
    "NUTCH-1779": {
        "Key": "NUTCH-1779",
        "Summary": "Apply formatting to the code",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "15/May/14 08:36",
        "Updated": "09/Jan/15 06:41",
        "Resolved": "09/Jan/15 06:34",
        "Description": "The various patches that we committed recently have not followed the Nutch format. We should apply the formatting with Eclipse using the file eclipse-codeformat.xml prior to shipping Nutch 2.3",
        "Issue Links": []
    },
    "NUTCH-1780": {
        "Key": "NUTCH-1780",
        "Summary": "ttl and gc_grace_seconds attributes are missing from gora-cassandra-mapping.xml file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "kaveh minooie",
        "Created": "15/May/14 19:38",
        "Updated": "03/Jun/14 10:22",
        "Resolved": "17/May/14 00:39",
        "Description": "after upgrading to Gora 0.4 ( NUTCH-1714) we need extra properties in C* mapping file. I also added a few, IMHO, helpful hints.",
        "Issue Links": [
            "/jira/browse/NUTCH-1791"
        ]
    },
    "NUTCH-1781": {
        "Key": "NUTCH-1781",
        "Summary": "Update gora-*-mapping.xml and gora.proeprties to reflect Gora 0.4",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/May/14 00:12",
        "Updated": "05/Jun/14 16:05",
        "Resolved": "05/Jun/14 00:19",
        "Description": "There are a number of datastore specific improvements baked in to Gora 0.4 which we need to make explicit in the above files.",
        "Issue Links": []
    },
    "NUTCH-1782": {
        "Key": "NUTCH-1782",
        "Summary": "NodeWalker to return current node",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/May/14 15:22",
        "Updated": "06/Jun/14 10:44",
        "Resolved": "06/Jun/14 10:02",
        "Description": "See title; very useful if one returns a NodeWalker instance for further processing but still need to get the currentNode in calling method.",
        "Issue Links": []
    },
    "NUTCH-1783": {
        "Key": "NUTCH-1783",
        "Summary": "Cleanup temp folders in case of failures",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.8",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Diaa",
        "Created": "16/May/14 20:01",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "06/Jan/19 20:32",
        "Description": "In the Classes:\n1. Injector \n2. CrawlDBReader \n3. Deduplication\n4. SegmentReader \nWhen a crash occurs the temp folders aren't deleted.\nThis patch handles the issue",
        "Issue Links": [
            "/jira/browse/NUTCH-2518",
            "/jira/browse/NUTCH-2375"
        ]
    },
    "NUTCH-1784": {
        "Key": "NUTCH-1784",
        "Summary": "modifiedTime and prevmodifiedTime never set",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "hanchi",
        "Created": "17/May/14 11:52",
        "Updated": "18/May/14 05:58",
        "Resolved": "17/May/14 17:12",
        "Description": "modifiedTime is never set. If you use DefaultFetchScheduler, modifiedTime is always zero as default. But if you use AdaptiveFetchScheduler, modifiedTime is set only once in the beginning by zero-control of AdaptiveFetchScheduler.\nBut this is not sufficient since modifiedTime needs to be updated whenever last modified time is available. We corrected this with a patch.\nAlso we noticed that prevModifiedTime is not written to database and we corrected that too.\nWith this patch, whenever lastModifiedTime is available, we do two things. First we set modifiedTime in the Page object to prevModifiedTime. After that we set lastModifiedTime to modifiedTime.",
        "Issue Links": [
            "/jira/browse/NUTCH-1651"
        ]
    },
    "NUTCH-1785": {
        "Key": "NUTCH-1785",
        "Summary": "Ability to index raw content",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/May/14 14:53",
        "Updated": "22/Nov/19 15:42",
        "Resolved": "30/Jul/15 21:30",
        "Description": "Some use-cases require Nutch to actually write the raw content a configured indexing back-end. Since Content is never read, a plugin is out of the question and therefore we need to force IndexJob to process Content as well.",
        "Issue Links": [
            "/jira/browse/NUTCH-2032",
            "/jira/browse/NUTCH-2032",
            "/jira/browse/NUTCH-2254",
            "/jira/browse/NUTCH-1944",
            "/jira/browse/NUTCH-1458"
        ]
    },
    "NUTCH-1786": {
        "Key": "NUTCH-1786",
        "Summary": "CrawlDb should follow db.url.normalizers and db.url.filters",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Diaa",
        "Created": "21/May/14 21:37",
        "Updated": "26/May/14 20:02",
        "Resolved": "26/May/14 10:47",
        "Description": "Currently crawldb provides the ability to normalize and filter urls in the update phase but only through the command line by using -normalize and -filter. It does not use nutch-default.xml to set the defaults for these parameters.\nAttached is a patch that fixes this issue.",
        "Issue Links": []
    },
    "NUTCH-1787": {
        "Key": "NUTCH-1787",
        "Summary": "update and complete API doc overview page",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "25/May/14 15:38",
        "Updated": "25/Jun/14 00:12",
        "Resolved": "24/Jun/14 21:44",
        "Description": "The API overview page (e.g., 1.8) needs review and completion:\n\nparse filters shown instead of parsers (latter missing at all); same for URL normalizers and URL filters. As a consequency, parse and URL filter plugins are listed in section \"Core\".\nsome plugins not listed at all (eg scoring-depth), neither in overview nor in package list\nmissing package descriptions for many plugins",
        "Issue Links": []
    },
    "NUTCH-1788": {
        "Key": "NUTCH-1788",
        "Summary": "Tika may return multiple values for Title on PDF's",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Talat Uyarer",
        "Created": "26/May/14 04:06",
        "Updated": "05/Jun/14 03:42",
        "Resolved": "05/Jun/14 03:00",
        "Description": "When pdf is parsed by Tike. It generate more than 1 title. In sample Solr schema we should define as multivalued field.",
        "Issue Links": []
    },
    "NUTCH-1789": {
        "Key": "NUTCH-1789",
        "Summary": "Migrate Nutch site to Apache CMS",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "29/May/14 23:54",
        "Updated": "11/Jun/14 04:09",
        "Resolved": "11/Jun/14 04:08",
        "Description": "Having put some work in on it, I am ready to push our site documentation to the more manageable platform provided by the Apache CMS.\nI'm going to start the migrations, ensuring that all sdtructure and code is in place before opening a ticket with INFRA and linking it here.\nN.B. There will be several commits for this issue.... please don't be alarmed",
        "Issue Links": []
    },
    "NUTCH-1790": {
        "Key": "NUTCH-1790",
        "Summary": "solrdedup causes OutOfMemoryError in Solr",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.7,                                            2.2",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Greg Padiasek",
        "Created": "31/May/14 16:12",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "31/May/14 18:26",
        "Description": "Nutch 1.7 and 2.2.1 use Hadoop 1.2. In this version Hadoop overwrites \"mapred.map.tasks\" variable set in mapred-site.xml and in local mode always sets it to 1. As a result Nutch creates a query to read ALL Solr documents at once in one giant response. This in turn causes Solr to consume all RAM given number of documents is high. I found this issue with Solr running with 2M+ docs, 1GB JVM RAM, 20% of which is used under normal conditions. When running \"solrdedup\", memory usage exceeds available RAM, solr throws OutOfMemoryError and the dedup job fails.\nI think this could be solved in one of two ways: either by upgrading Nutch to a later version of Hadoop lib (which hopefully does not hard-code \"mapred.map.tasks\" value anymore), or by changing the SolrDeleteDuplicates class to \"stream\" documents in batches. The later would make Nutch less dependent on Hadoop version and this was my choice. Attached is a patch that implements batch reading in local mode with user defined batch size. The \"streaming\" is potentially also applicable in distributed mode.",
        "Issue Links": []
    },
    "NUTCH-1791": {
        "Key": "NUTCH-1791",
        "Summary": "Null pointer exceptions with gora-cassandra-0.4",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "generator,                                            storage",
        "Assignee": null,
        "Reporter": "Koen Smets",
        "Created": "03/Jun/14 10:20",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Latest nutch-2.x source checkout fails to run with Cassandra 2.0.2 (and also Cassandra 2.0.7) as storage backend both in normal Nutch operations (inject, generate, fetch) cycle as in the junit tests TestGoraStorage\n\n2014-06-03 11:24:23,495 INFO  connection.CassandraHostRetryService (CassandraHostRetryService.java:<init>(48)) - Downed Host Retry service started with queue size -1 and retry delay 10s\n2014-06-03 11:24:23,535 INFO  service.JmxMonitor (JmxMonitor.java:registerMonitor(52)) - Registering JMX me.prettyprint.cassandra.service_Test Cluster:ServiceType=hector,MonitorType=hector\nException in thread \"main\" java.lang.NullPointerException\n\tat org.apache.gora.cassandra.query.CassandraResult.updatePersistent(CassandraResult.java:121)\n\tat org.apache.gora.cassandra.query.CassandraResult.nextInner(CassandraResult.java:57)\n\tat org.apache.gora.query.impl.ResultBase.next(ResultBase.java:114)\n\tat org.apache.nutch.storage.TestGoraStorage.readWrite(TestGoraStorage.java:93)\n\tat org.apache.nutch.storage.TestGoraStorage.main(TestGoraStorage.java:230)\n\n\nAfter injecting:\n\nksmets@precise64 ~/l/a/r/local> ./bin/nutch inject urls\nInjectorJob: starting at 2014-06-03 11:55:11\nInjectorJob: Injecting urlDir: urls\nInjectorJob: Using class org.apache.gora.cassandra.store.CassandraStore as the Gora storage class.\nInjectorJob: total number of urls rejected by filters: 0\nInjectorJob: total number of urls injected after normalization and filtering: 1\nInjector: finished at 2014-06-03 11:55:13, elapsed: 00:00:02\n\nksmets@precise64 ~/l/a/r/local> ./bin/nutch readdb -stats\nWebTable statistics start\nStatistics for WebTable:\nmin score:\t1.0\nretry 0:\t1\njobs:\t{db_stats-job_local1403358409_0001={jobID=job_local1403358409_0001, jobName=db_stats, counters={File Input Format Counters ={BYTES_READ=0}, Map-Reduce Framework={MAP_OUTPUT_MATERIALIZED_BYTES=97, MAP_INPUT_RECORDS=1, REDUCE_SHUFFLE_BYTES=0, SPILLED_RECORDS=12, MAP_OUTPUT_BYTES=53, COMMITTED_HEAP_BYTES=358612992, CPU_MILLISECONDS=0, SPLIT_RAW_BYTES=769, COMBINE_INPUT_RECORDS=4, REDUCE_INPUT_RECORDS=6, REDUCE_INPUT_GROUPS=6, COMBINE_OUTPUT_RECORDS=6, PHYSICAL_MEMORY_BYTES=0, REDUCE_OUTPUT_RECORDS=6, VIRTUAL_MEMORY_BYTES=0, MAP_OUTPUT_RECORDS=4}, FileSystemCounters={FILE_BYTES_READ=974145, FILE_BYTES_WRITTEN=1144369}, File Output Format Counters ={BYTES_WRITTEN=225}}}}\nmax score:\t1.0\nTOTAL urls:\t1\nstatus 0 (null):\t1\navg score:\t1.0\nWebTable statistics: done\n\nksmets@precise64 ~/l/a/r/local> ./bin/nutch readdb -url http://example.com/\nkey:\thttp://example.com/\nbaseUrl:\tnull\nstatus:\t0 (null)\nfetchTime:\t1401789311270\nprevFetchTime:\t0\nfetchInterval:\t2592000\nretriesSinceFetch:\t0\nmodifiedTime:\t0\nprevModifiedTime:\t0\nprotocolStatus:\t(null)\nparseStatus:\t(null)\ntitle:\tnull\nscore:\t1.0\nmarkers:\torg.apache.gora.persistency.impl.DirtyMapWrapper@eb173c\nreprUrl:\tnull\nmetadata _csh_ : \t?\ufffd\n\n\nAfter generating,\n\nksmets@precise64 ~/l/a/r/local> ./bin/nutch generate -topN 1\nGeneratorJob: starting at 2014-06-03 11:55:38\nGeneratorJob: Selecting best-scoring urls due for fetch.\nGeneratorJob: starting\nGeneratorJob: filtering: true\nGeneratorJob: normalizing: true\nGeneratorJob: topN: 1\nGeneratorJob: finished at 2014-06-03 11:55:40, time elapsed: 00:00:02\nGeneratorJob: generated batch id: 1401789338-222512082 containing 1 URLs\n\nksmets@precise64 ~/l/a/r/local> ./bin/nutch readdb -stats\nWebTable statistics start\nStatistics for WebTable:\njobs:\t{db_stats-job_local73029265_0001={jobID=job_local73029265_0001, jobName=db_stats, counters={File Input Format Counters ={BYTES_READ=0}, Map-Reduce Framework={MAP_OUTPUT_MATERIALIZED_BYTES=6, MAP_INPUT_RECORDS=0, REDUCE_SHUFFLE_BYTES=0, SPILLED_RECORDS=0, MAP_OUTPUT_BYTES=0, COMMITTED_HEAP_BYTES=358612992, CPU_MILLISECONDS=0, SPLIT_RAW_BYTES=769, COMBINE_INPUT_RECORDS=0, REDUCE_INPUT_RECORDS=0, REDUCE_INPUT_GROUPS=0, COMBINE_OUTPUT_RECORDS=0, PHYSICAL_MEMORY_BYTES=0, REDUCE_OUTPUT_RECORDS=0, VIRTUAL_MEMORY_BYTES=0, MAP_OUTPUT_RECORDS=0}, FileSystemCounters={FILE_BYTES_READ=974054, FILE_BYTES_WRITTEN=1144028}, File Output Format Counters ={BYTES_WRITTEN=98}}}}\nTOTAL urls:\t0\nWebTable statistics: done\n\nksmets@precise64 ~/l/a/r/local> ./bin/nutch readdb -url http://example.com/\nWebTableReader: java.lang.NullPointerException\n\tat org.apache.gora.cassandra.query.CassandraResult.updatePersistent(CassandraResult.java:121)\n\tat org.apache.gora.cassandra.query.CassandraResult.nextInner(CassandraResult.java:57)\n\tat org.apache.gora.query.impl.ResultBase.next(ResultBase.java:114)\n\tat org.apache.nutch.crawl.WebTableReader.read(WebTableReader.java:238)\n\tat org.apache.nutch.crawl.WebTableReader.run(WebTableReader.java:494)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.WebTableReader.main(WebTableReader.java:430)",
        "Issue Links": [
            "/jira/browse/GORA-395",
            "/jira/browse/NUTCH-1780",
            "/jira/browse/NUTCH-1714"
        ]
    },
    "NUTCH-1792": {
        "Key": "NUTCH-1792",
        "Summary": "Refactor resource loading in plugin tests",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "None",
        "Component/s": "test",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Jun/14 14:00",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Right now we have a strange method for loading test resources e.g.\nurlString = \"file:\" + sampleDir + fileSeparator + sampleFiles[i];\nFile file = new File(sampleDir + fileSeparator + sampleFiles[i]);\nThis works fine from the command line but fails to locate and load the resource within Eclipse IDE... not ideal.\nI am investigating whether we can do getClass().getResource...",
        "Issue Links": []
    },
    "NUTCH-1793": {
        "Key": "NUTCH-1793",
        "Summary": "HttpRobotRulesParser not configured properly => \"http.robots.403.allow\" property is not read",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "16/Jun/14 15:28",
        "Updated": "17/Jun/14 09:21",
        "Resolved": "17/Jun/14 08:42",
        "Description": "HttpRobotRulesParser is instanciated by HttpBase with an empty constructor and the conf() method is called later but relies on the super implementation and as a result  the  \"http.robots.403.allow\" property is not read.\nAm OK to leave this property in HttpRobotRulesParser but it should override setConf so that the property is initialised properly.",
        "Issue Links": []
    },
    "NUTCH-1794": {
        "Key": "NUTCH-1794",
        "Summary": "IndexingFilterChecker to optionally dumpText",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Jun/14 12:05",
        "Updated": "17/Jun/14 15:05",
        "Resolved": "17/Jun/14 14:24",
        "Description": "This tool should, just as parsechecker, optionally dump the full text.",
        "Issue Links": []
    },
    "NUTCH-1795": {
        "Key": "NUTCH-1795",
        "Summary": "Please create a DOAP file for your TLP",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebb",
        "Created": "19/Jun/14 17:43",
        "Updated": "02/Aug/14 21:36",
        "Resolved": "02/Aug/14 21:36",
        "Description": "There used to be a DOAP file for NUTCH at http://nutch.apache.org/doap.rdf\nHowever this has gone.\nPlease create a new DOAP if necessary and get it added to files.xml as per\nhttp://projects.apache.org/create.html#submit\nRemember, if you ever move or rename the DOAP file in future, please\nensure that files.xml is updated to point to the new location.",
        "Issue Links": []
    },
    "NUTCH-1796": {
        "Key": "NUTCH-1796",
        "Summary": "Ensure Gora object builders are used as oppose to empty constructors.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "storage",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Jun/14 02:04",
        "Updated": "20/Jun/14 08:54",
        "Resolved": "20/Jun/14 02:13",
        "Description": "Since NUTCH-1714 was committed, we have had broken tests. This is due to Gora object constrctors being used as oppose to the Object .build()ers.\nThe constrctors are more efficient in memory, however they do not set default values for fields which means we can sometimes get NPE when we attempt to serde data.",
        "Issue Links": []
    },
    "NUTCH-1797": {
        "Key": "NUTCH-1797",
        "Summary": "remove unused package o.a.n.html",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "23/Jun/14 20:09",
        "Updated": "16/Dec/14 21:42",
        "Resolved": "16/Dec/14 20:59",
        "Description": "The package org.apache.nutch.html has been removed in trunk with NUTCH-837 but is still present in 2.x. It contains only one class/file (Entities.java) which is not used at all. It should be removed including Entities.java, or the class should be moved to package o.a.n.util.",
        "Issue Links": []
    },
    "NUTCH-1798": {
        "Key": "NUTCH-1798",
        "Summary": "Crawl script not calling index command correctly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Aaron Bedward",
        "Created": "24/Jun/14 10:48",
        "Updated": "27/Jun/14 07:43",
        "Resolved": "27/Jun/14 07:31",
        "Description": "Hopefully this is something i am doing wrong.  I have checked out 2.x as i would like to use the new metatag extraction features.  I have then run ant runtime to build,  I have updated the nutch-site.xml like so:\n<property>\n  <name>plugin.includes</name>\n <value>protocol-http|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|more|metatags)|indexer-elasticsearch|urlnormalizer-(pass|regex|basic)|scoring-opic</value>\n <description>Regular expression naming plugin directory names to\n  include.  Any plugin not matching this expression is excluded.\n  In any case you need at least include the nutch-extensionpoints plugin. By\n  default Nutch includes crawling just HTML and plain text via HTTP,\n  and basic indexing and search plugins. In order to use HTTPS please enable \n  protocol-httpclient, but be aware of possible intermittent problems with the \n  underlying commons-httpclient library.\n  </description>\n</property>\n  <property>\n      <name>elastic.cluster</name>\n      <value>elasticsearch</value>\n      <description>The cluster name to discover. Either host and potr must be defined\n        or cluster.</description>\n  </property>\nI have then created a folder called urls and added seed.txt.\ni ran the following commands \nbin/nutch inject urls\nbin/nutch generate -topN 1000  \nbin/nutch fetch -all\nbin/nutch parse -all\nbin/nutch updatedb\nbin/nutch index  -all \nit runs no errors however no documents have been index\ni also tried setting up the following with solr and no documents are indexed\nLog:\n2014-06-24 02:57:57,804 INFO  parse.ParserJob - ParserJob: success\n2014-06-24 02:57:57,805 INFO  parse.ParserJob - ParserJob: finished at 2014-06-24 02:57:57, time elapsed: 00:00:06\n2014-06-24 02:57:59,823 INFO  indexer.IndexingJob - IndexingJob: starting\n2014-06-24 02:58:00,815 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2014-06-24 02:58:00,815 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2014-06-24 02:58:01,774 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.more.MoreIndexingFilter\n2014-06-24 02:58:01,776 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2014-06-24 02:58:01,776 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2014-06-24 02:58:03,946 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2014-06-24 02:58:04,920 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.elastic.ElasticIndexWriter\n2014-06-24 02:58:05,261 INFO  elasticsearch.node - [Silver] version[1.1.0], pid[21885], build[2181e11/2014-03-25T15:59:51Z]\n2014-06-24 02:58:05,261 INFO  elasticsearch.node - [Silver] initializing ...\n2014-06-24 02:58:05,377 INFO  elasticsearch.plugins - [Silver] loaded [], sites []\n2014-06-24 02:58:08,339 INFO  elasticsearch.node - [Silver] initialized\n2014-06-24 02:58:08,339 INFO  elasticsearch.node - [Silver] starting ...\n2014-06-24 02:58:08,431 INFO  elasticsearch.transport - [Silver] bound_address \n{inet[/0:0:0:0:0:0:0:0:9301]}\n, publish_address \n{inet[/10.0.2.15:9301]}\n2014-06-24 02:58:11,540 INFO  cluster.service - [Silver] detected_master [Doughboy][U02ugUDtRZW4ttx6lbyMLg][dev-ElasticSearch][inet[/10.0.2.4:9300]], added \n{[Doughboy][U02ugUDtRZW4ttx6lbyMLg][dev-ElasticSearch][inet[/10.0.2.4:9300]],[Silver Squire][2NyU10FARvaL92rU5GqpcA][nutch][inet[/10.0.2.15:9300]],}\n, reason: zen-disco-receive(from master [[Doughboy][U02ugUDtRZW4ttx6lbyMLg][dev-ElasticSearch][inet[/10.0.2.4:9300]]])\n2014-06-24 02:58:11,553 INFO  elasticsearch.discovery - [Silver] elasticsearch/jXIC3VT6THukKDFB7GMw7Q\n2014-06-24 02:58:11,562 INFO  elasticsearch.http - [Silver] bound_address \n{inet[/0:0:0:0:0:0:0:0:9201]}\n, publish_address \n{inet[/10.0.2.15:9201]}\n2014-06-24 02:58:11,566 INFO  elasticsearch.node - [Silver] started\n2014-06-24 02:58:11,568 INFO  basic.BasicIndexingFilter - Maximum title length for indexing set to: 100\n2014-06-24 02:58:11,569 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.basic.BasicIndexingFilter\n2014-06-24 02:58:11,581 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.more.MoreIndexingFilter\n2014-06-24 02:58:11,581 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2014-06-24 02:58:11,581 INFO  indexer.IndexingFilters - Adding org.apache.nutch.indexer.anchor.AnchorIndexingFilter\n2014-06-24 02:58:11,716 INFO  elastic.ElasticIndexWriter - Processing remaining requests [docs = 0, length = 0, total docs = 0]\n2014-06-24 02:58:11,717 INFO  elastic.ElasticIndexWriter - Processing to finalize last execute\n2014-06-24 02:58:11,717 INFO  elasticsearch.node - [Silver] stopping ...\n2014-06-24 02:58:11,751 INFO  elasticsearch.node - [Silver] stopped\n2014-06-24 02:58:11,751 INFO  elasticsearch.node - [Silver] closing ...\n2014-06-24 02:58:11,756 INFO  elasticsearch.node - [Silver] closed\n2014-06-24 02:58:11,759 WARN  mapred.FileOutputCommitter - Output path is null in cleanup\n2014-06-24 02:58:12,511 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.elastic.ElasticIndexWriter\n2014-06-24 02:58:12,511 INFO  indexer.IndexingJob - Active IndexWriters :\nElasticIndexWriter\n\telastic.cluster : elastic prefix cluster\n\telastic.host : hostname\n\telastic.port : port  (default 9300)\n\telastic.index : elastic index command \n\telastic.max.bulk.docs : elastic bulk index doc counts. (default 250) \n\telastic.max.bulk.size : elastic bulk index length. (default 2500500 ~2.5MB)\n2014-06-24 02:58:12,525 INFO  elasticsearch.node - [Lifeguard] version[1.1.0], pid[21885], build[2181e11/2014-03-25T15:59:51Z]\n2014-06-24 02:58:12,525 INFO  elasticsearch.node - [Lifeguard] initializing ...\n2014-06-24 02:58:12,555 INFO  elasticsearch.plugins - [Lifeguard] loaded [], sites []\n2014-06-24 02:58:13,025 INFO  elasticsearch.node - [Lifeguard] initialized\n2014-06-24 02:58:13,025 INFO  elasticsearch.node - [Lifeguard] starting ...\n2014-06-24 02:58:13,032 INFO  elasticsearch.transport - [Lifeguard] bound_address \n{inet[/0:0:0:0:0:0:0:0:9301]}\n, publish_address \n{inet[/10.0.2.15:9301]}\n2014-06-24 02:58:16,063 INFO  cluster.service - [Lifeguard] detected_master [Doughboy][U02ugUDtRZW4ttx6lbyMLg][dev-ElasticSearch][inet[/10.0.2.4:9300]], added \n{[Doughboy][U02ugUDtRZW4ttx6lbyMLg][dev-ElasticSearch][inet[/10.0.2.4:9300]],[Silver Squire][2NyU10FARvaL92rU5GqpcA][nutch][inet[/10.0.2.15:9300]],}\n, reason: zen-disco-receive(from master [[Doughboy][U02ugUDtRZW4ttx6lbyMLg][dev-ElasticSearch][inet[/10.0.2.4:9300]]])\n2014-06-24 02:58:16,072 INFO  elasticsearch.discovery - [Lifeguard] elasticsearch/MWiqtTiqS5aC_M7QvGtfyg\n2014-06-24 02:58:16,074 INFO  elasticsearch.http - [Lifeguard] bound_address \n{inet[/0:0:0:0:0:0:0:0:9201]}\n, publish_address \n{inet[/10.0.2.15:9201]}\n2014-06-24 02:58:16,076 INFO  elasticsearch.node - [Lifeguard] started\n2014-06-24 02:58:16,076 INFO  indexer.IndexingJob - IndexingJob: done.",
        "Issue Links": []
    },
    "NUTCH-1799": {
        "Key": "NUTCH-1799",
        "Summary": "ANT Eclipse task discovers all plugin jars automatically",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "25/Jun/14 11:51",
        "Updated": "09/Jul/14 14:01",
        "Resolved": "09/Jul/14 14:01",
        "Description": "At the moment the jars for the various plugins are added one by one and the ant eclipse task fails to add the ones for indexer-elastic. Instead of having to maintain this manually the path attached adds automatically all the jar files found in the plugin directories.",
        "Issue Links": []
    },
    "NUTCH-1800": {
        "Key": "NUTCH-1800",
        "Summary": "Documentation for Nutch 1.X REST API",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "documentation,                                            REST_api",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Jun/14 00:32",
        "Updated": "21/Jun/16 12:03",
        "Resolved": "30/Oct/15 22:04",
        "Description": "This issue should build on NUTCH-1769 with full Java documentation for all classes in the following packages\norg.apache.nutch.api.*\nI am assigning this one to fjodor.vershinin as he is doing an excellent job on the REST API. His UML graphic in [0] and commantary shows that he has a good understanding of the REST API and its functionality.\nThank you fjodor.vershinin great work.\n[0] https://wiki.apache.org/nutch/NutchRESTAPI#UML_Graphic",
        "Issue Links": [
            "/jira/browse/NUTCH-2157"
        ]
    },
    "NUTCH-1801": {
        "Key": "NUTCH-1801",
        "Summary": "Improve handling of test dependencies in ANT+Ivy",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "26/Jun/14 08:27",
        "Updated": "15/Jul/14 08:40",
        "Resolved": "15/Jul/14 08:40",
        "Description": "The chain of dependencies between ANT tasks needs fixing. The main issue is that the dependencies with a 'test' scope in Ivy are not resolved properly or rather the resolution task works fine but is not called from the upper level 'test' tasks. This can easily be reproduced by marking the junit dependency in ivy.xml as conf=\"test->default\".\nIdeally we'd want to have a separate lib dir for the test dependencies so that they do not get copied into the job file where they are absolutely not needed.",
        "Issue Links": []
    },
    "NUTCH-1802": {
        "Key": "NUTCH-1801 Improve handling of test dependencies in ANT+Ivy",
        "Summary": "Move TestbedProxy to test environment",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "26/Jun/14 09:32",
        "Updated": "30/Jun/14 13:45",
        "Resolved": "30/Jun/14 13:40",
        "Description": "The proxy task relies on the test classpath but its code is in src/java/org/apache/nutch/tools/proxy. One of the benefits of moving it to tests is that its dependencies would not be shipped in the job file where they are not needed (e.g. servlet stuff). The Ant task would work as before.",
        "Issue Links": [
            "/jira/browse/NUTCH-1803"
        ]
    },
    "NUTCH-1803": {
        "Key": "NUTCH-1801 Improve handling of test dependencies in ANT+Ivy",
        "Summary": "Put test dependencies in a separate lib dir",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "26/Jun/14 09:38",
        "Updated": "15/Jul/14 09:53",
        "Resolved": "30/Jun/14 12:39",
        "Description": "See main issue NUTCH-1801. This would mean that these libs do not get included in the job file and provides a cleaner separation.",
        "Issue Links": [
            "/jira/browse/NUTCH-1804",
            "/jira/browse/NUTCH-1802"
        ]
    },
    "NUTCH-1804": {
        "Key": "NUTCH-1801 Improve handling of test dependencies in ANT+Ivy",
        "Summary": "Move JUnit dependency to test scope",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.9",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "26/Jun/14 09:48",
        "Updated": "15/Jul/14 08:51",
        "Resolved": "15/Jul/14 08:39",
        "Description": "Should work straight with core tests after applying NUTCH-1803 but requires fixing the build for the plugins by either add the main test dependencies to their classpath or force them to declare JUnit as a test dependency in their own ivy.xml. The latter is probably cleaner but we need to make sure that the test dependencies do not get added to the built version of the plugin.",
        "Issue Links": [
            "/jira/browse/NUTCH-1803"
        ]
    },
    "NUTCH-1805": {
        "Key": "NUTCH-1805",
        "Summary": "Remove unnecessary transitive dependencies from Hadoop core",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "26/Jun/14 10:34",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "01/Oct/19 12:50",
        "Description": "The Hadoop libs are not included in the job file as a Hadoop cluster must already be available in order to use it, however some of its transitive dependencies make it to the job file. We already prevent some but could extend that to :\n\t\t\t<exclude org=\"org.mortbay.jetty\"/>\n\t\t\t<exclude org=\"com.sun.jersey\"/>\n\t\t\t<exclude org=\"tomcat\"/>\nNote that we need some of the Hadoop classes and dependencies in order to run Nutch in local mode.\nAlternatively we could have a separate Ivy profile only for Hadoop and store the dependencies in a separate location so that they do not get copied to the job jar, however this is probably an overkill if the dependencies above are not needed when running in local mode.",
        "Issue Links": []
    },
    "NUTCH-1806": {
        "Key": "NUTCH-1806",
        "Summary": "Delegate processing of URL domains to crawler commons",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "26/Jun/14 12:45",
        "Updated": "08/Jan/23 19:48",
        "Resolved": null,
        "Description": "We have code in src/java/org/apache/nutch/util/domain and a resource file conf/domain-suffixes.xml to handle URL domains. This is used mostly from URLUtil.getDomainName.\nThe resource file is not necessarily up to date and since crawler commons has a similar functionality we should use it instead of having to maintain our own resources.",
        "Issue Links": [
            "/jira/browse/NUTCH-1942"
        ]
    },
    "NUTCH-1807": {
        "Key": "NUTCH-1807",
        "Summary": "avoid methods relying on system-specific default locale / charset",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "29/Jun/14 20:26",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Many methods in Java (and libraries) used to convert Strings, Numbers, Dates rely on the system-specific default locale / character set. This may cause strange behaviour and errors impossible to reproduce on other systems, see thetaphi's blog post, and discussions in NUTCH-1693 and NUTCH-1554.\nA search with the forbidden-apis client shows 120 calls of such methods in trunk (without test classes):\n\n# compile Nutch before check: all tested class files\n# are then located in build/ (including plugins)\n% CLASSPATH=`find build/ -name '*.jar' | tr '\\n' ':'`\n% java -jar forbiddenapis-1.5.1.jar -d build/ -c $CLASSPATH \\\n      -b jdk-unsafe-1.8 -b commons-io-unsafe-2.4\n\n\nIt is also possible to integrate the check into the ant build (to avoid that \"forbidden\" calls slip into the code again).",
        "Issue Links": [
            "/jira/browse/NUTCH-2254",
            "/jira/browse/NUTCH-2815",
            "/jira/browse/NUTCH-2264"
        ]
    },
    "NUTCH-1808": {
        "Key": "NUTCH-1808",
        "Summary": "JMX/JMS artifacts cannot be resolved from fresh repository",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Valerio Schiavoni",
        "Created": "30/Jun/14 21:16",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Jul/14 12:40",
        "Description": "How to reproduce:\ngit clone https://github.com/apache/nutch.git\ncd nutch\ngit checkout 2.x\nmvn clean install\n[INFO] Scanning for projects...\n[WARNING] \n[WARNING] Some problems were encountered while building the effective model for org.apache.nutch:nutch:jar:2.2\n[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.jdom:jdom:jar -> duplicate declaration of version 1.1 @ line 256, column 29\n[WARNING] \n[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.\n[WARNING] \n[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.\n[WARNING] \n[INFO] \n[INFO] ------------------------------------------------------------------------\n[INFO] Building Apache Nutch 2.2\n[INFO] ------------------------------------------------------------------------\n[WARNING] The POM for org.restlet.jse:org.restlet:jar:2.0.5 is missing, no dependency information available\n[WARNING] The POM for org.restlet.jse:org.restlet.ext.jackson:jar:2.0.5 is missing, no dependency information available\n[INFO] ------------------------------------------------------------------------\n[INFO] BUILD FAILURE\n[INFO] ------------------------------------------------------------------------\n[INFO] Total time: 0.978s\n[INFO] Finished at: Mon Jun 30 23:11:03 CEST 2014\n[INFO] Final Memory: 20M/982M\n[INFO] ------------------------------------------------------------------------\n[ERROR] Failed to execute goal on project nutch: Could not resolve dependencies for project org.apache.nutch:nutch:jar:2.2: The following artifacts could not be resolved: javax.jms:jms:jar:1.1, com.sun.jdmk:jmxtools:jar:1.2.1, com.sun.jmx:jmxri:jar:1.2.1, org.restlet.jse:org.restlet:jar:2.0.5, org.restlet.jse:org.restlet.ext.jackson:jar:2.0.5: Could not transfer artifact javax.jms:jms:jar:1.1 from/to java.net (https://maven-repository.dev.java.net/nonav/repository): No connector available to access repository java.net (https://maven-repository.dev.java.net/nonav/repository) of type legacy using the available factories WagonRepositoryConnectorFactory -> [Help 1]\n[ERROR] \n[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.\n[ERROR] Re-run Maven using the -X switch to enable full debug logging.\n[ERROR] \n[ERROR] For more information about the errors and possible solutions, please read the following articles:\n[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException",
        "Issue Links": []
    },
    "NUTCH-1809": {
        "Key": "NUTCH-1809",
        "Summary": "Duplicate jdom dependency",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Valerio Schiavoni",
        "Created": "30/Jun/14 21:19",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Jul/14 12:41",
        "Description": "The dependency to jdom is declared twice in the pom.xml, first time at lines 202-207, second time at lines 257-261.\nThe double declaration causes Maven to report the following warning:\n[WARNING] Some problems were encountered while building the effective model for org.apache.nutch:nutch:jar:2.2\n[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.jdom:jdom:jar -> duplicate declaration of version 1.1 @ line 256, column 29\n[WARNING] \n[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.\n[WARNING] \n[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.",
        "Issue Links": [
            "/jira/browse/NUTCH-1810"
        ]
    },
    "NUTCH-1810": {
        "Key": "NUTCH-1810",
        "Summary": "Duplicate jdom dependency",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Valerio Schiavoni",
        "Created": "30/Jun/14 21:20",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The dependency to jdom is declared twice in the pom.xml, first time at lines 202-207, second time at lines 257-261.\nThe double declaration causes Maven to report the following warning:\n[WARNING] Some problems were encountered while building the effective model for org.apache.nutch:nutch:jar:2.2\n[WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.jdom:jdom:jar -> duplicate declaration of version 1.1 @ line 256, column 29\n[WARNING] \n[WARNING] It is highly recommended to fix these problems because they threaten the stability of your build.\n[WARNING] \n[WARNING] For this reason, future Maven versions might no longer support building such malformed projects.",
        "Issue Links": [
            "/jira/browse/NUTCH-1809"
        ]
    },
    "NUTCH-1811": {
        "Key": "NUTCH-1811",
        "Summary": "bin/nutch junit to use junit 4 test runner",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "test",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "01/Jul/14 19:53",
        "Updated": "10/Jul/14 21:57",
        "Resolved": "10/Jul/14 20:51",
        "Description": "After upgrade to JUnit 4 (NUTCH-1573 and NUTCH-1737) the call bin/nutch junit fails with \"No tests found\". The JUnit 4 test runner org.junit.runner.JUnitCore should be used instead of junit.textui.TestRunner.",
        "Issue Links": []
    },
    "NUTCH-1812": {
        "Key": "NUTCH-1812",
        "Summary": "Create Vagrant artifacts for 2.X branch",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Jul/14 14:19",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Vagrant [0] is a very useful tool which I believe we can use to create VM containers to help aid the process of setting up and running a Nutch 2.X build. Right now, provisioning Nutch 2.X is a PITA for new users.\nWe've been working with Vagrant a lot. I feel that it would be really nice to just drop in an image into virtualbox (or something similar) and then just start.\n[0] http://www.vagrantup.com/",
        "Issue Links": []
    },
    "NUTCH-1813": {
        "Key": "NUTCH-1813",
        "Summary": "Use \\u.... escapes for non-ASCII chars in TestURLUtil",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.3",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Valerio Schiavoni",
        "Created": "09/Jul/14 13:41",
        "Updated": "08/Jan/23 19:22",
        "Resolved": "08/Jan/23 19:21",
        "Description": "To reproduce, git clone the latest 2.x branch and execute the TestURLUtil tests.\nThere are 4 test failures and 1 error.\nFailing tests:\ntestToUNICODE:org.junit.ComparisonFailure: expected:<http://www.[\ufffd\ufffd]evir.com> but was:<http://www.[\u00e7]evir.com>\n\tat org.junit.Assert.assertEquals(Assert.java:115)\n\tat org.junit.Assert.assertEquals(Assert.java:144)\n\tat org.apache.nutch.util.TestURLUtil.testToUNICODE(TestURLUtil.java:263)\ntestChooseRepr:org.junit.ComparisonFailure: expected:<http://www.[b].com> but was:<http://www.[a].com>\n\tat org.junit.Assert.assertEquals(Assert.java:115)\n\tat org.junit.Assert.assertEquals(Assert.java:144)\n\tat org.apache.nutch.util.TestURLUtil.testChooseRepr(TestURLUtil.java:179)\ntestGetDomainName:\norg.junit.ComparisonFailure: expected:<[apache.]org> but was:<[]org>\n\tat org.junit.Assert.assertEquals(Assert.java:115)\n\tat org.junit.Assert.assertEquals(Assert.java:144)\n\tat org.apache.nutch.util.TestURLUtil.testGetDomainName(TestURLUtil.java:35)\ntestToASCII:\njava.lang.AssertionError: expected:<http://www.xn--evir-zoa.com> but was:<null>\n\tat org.junit.Assert.fail(Assert.java:88)\n\tat org.junit.Assert.failNotEquals(Assert.java:743)\n\tat org.junit.Assert.assertEquals(Assert.java:118)\n\tat org.junit.Assert.assertEquals(Assert.java:144)\n\tat org.apache.nutch.util.TestURLUtil.testToASCII(TestURLUtil.java:273)",
        "Issue Links": []
    },
    "NUTCH-1814": {
        "Key": "NUTCH-1814",
        "Summary": "TestMimeUtils#testBinaryFiles fails under Eclipse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Valerio Schiavoni",
        "Created": "10/Jul/14 08:55",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "10/Jul/14 16:16",
        "Description": "Test TestMimeUtils#testBinaryFiles fails in finding the input test directory 'test-mime-util'. :\njava.io.FileNotFoundException: ./test-mime-util/test.xlsx (No such file or directory)\n\tat java.io.FileInputStream.open(Native Method)\n\tat java.io.FileInputStream.<init>(FileInputStream.java:146)\n\tat com.google.common.io.Files$1.getInput(Files.java:109)\n\tat com.google.common.io.Files$1.getInput(Files.java:106)\n\tat com.google.common.io.ByteStreams.toByteArray(ByteStreams.java:250)\n\tat com.google.common.io.Files.toByteArray(Files.java:204)\n\tat org.apache.nutch.util.TestMimeUtil.getMimeType(TestMimeUtil.java:82)\n\tat org.apache.nutch.util.TestMimeUtil.testBinaryFiles(TestMimeUtil.java:124)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat junit.framework.TestCase.runTest(TestCase.java:176)\n\tat junit.framework.TestCase.runBare(TestCase.java:141)\n\tat junit.framework.TestResult$1.protect(TestResult.java:122)\n\tat junit.framework.TestResult.runProtected(TestResult.java:142)\n\tat junit.framework.TestResult.run(TestResult.java:125)\n\tat junit.framework.TestCase.run(TestCase.java:129)\n\tat junit.framework.TestSuite.runTest(TestSuite.java:255)\n\tat junit.framework.TestSuite.run(TestSuite.java:250)\n\tat org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)\n\tat org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:50)\n\tat org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)\n\tat org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)\nIn my case, a simple fix is to change the initialisation of sampleDir from:\nprivate File sampleDir = new File(System.getProperty(\"test.build.data\", \".\"),\n      \"test-mime-util\");\nto\nprivate File sampleDir = new File(System.getProperty(\"test.build.data\", \".\"),\n      \"src/testresources/test-mime-util\");",
        "Issue Links": []
    },
    "NUTCH-1815": {
        "Key": "NUTCH-1815",
        "Summary": "Metadata Parsed with parse-tika is Duplicated",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.8",
        "Fix Version/s": "None",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Jonathan Cooper-Ellis",
        "Created": "10/Jul/14 17:40",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "When Nutch is configured to parse metatags and index metadata from HTML documents, disabling parse-html (and using parse-tika instead) causes each metadata field to be indexed twice with identical content.\nI only modified plugin.includes (description and keywords metatags are included in nutch-site.xml by default, so I did not modify those):\n<property>\n  <name>plugin.includes</name>\n  <value>protocol-http|urlfilter-regex|parse-(tika|metatags)|index-(basic|anchor|metadata)|indexer-solr|scoring-opic|urlnormalizer-(pass|regex|basic)</value>\n  <description>...</description>\n</property>\nSample output:\n$ bin/nutch indexchecker http://www.bizjournals.com/bizjournals/washingtonbureau/2014/07/yes-millions-of-uninsured-americans-did-get.html\nfetching: http://www.bizjournals.com/bizjournals/washingtonbureau/2014/07/yes-millions-of-uninsured-americans-did-get.html\nparsing: http://www.bizjournals.com/bizjournals/washingtonbureau/2014/07/yes-millions-of-uninsured-americans-did-get.html\ncontentType: text/html\ncontent :\tCommonwealth Fund survey: Obamacare helped 9.5 million Americans get health insurance, thanks to exc\ntitle :\tCommonwealth Fund survey: Obamacare helped 9.5 million Americans get health insurance, thanks to exc\nhost :\twww.bizjournals.com\ntstamp :\tThu Jul 10 17:34:56 UTC 2014\nmetatag.description :\tA new survey by the Commonwealth Fund found that 9.5 million previously uninsured Americans got cove\nmetatag.description :\tA new survey by the Commonwealth Fund found that 9.5 million previously uninsured Americans got cove\nurl :\thttp://www.bizjournals.com/bizjournals/washingtonbureau/2014/07/yes-millions-of-uninsured-americans-\nIn this case, metatag.description appears twice. If parse-html is added back to plugin.includes and the same command is run, metatag.description will only appear once.",
        "Issue Links": []
    },
    "NUTCH-1816": {
        "Key": "NUTCH-1816",
        "Summary": "Port 'Create Outlinks with Metadata' e.g. NUTCH-1622 to 2.X",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Jul/14 20:32",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Please see NUTCH-1622 for an item description and proposed solution for this issue.",
        "Issue Links": [
            "/jira/browse/NUTCH-1622",
            "/jira/browse/NUTCH-1741"
        ]
    },
    "NUTCH-1817": {
        "Key": "NUTCH-1817",
        "Summary": "Remove pom.xml from source",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.9",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "16/Jul/14 08:46",
        "Updated": "16/Jul/14 10:51",
        "Resolved": "16/Jul/14 10:12",
        "Description": "See discussion on http://mail-archives.apache.org/mod_mbox/nutch-user/201407.mbox/%3CCA+-fM0sQc_HHN77ARgypjr5N-A_=P_Lk4Tod-0SD0Dj8xk0z4A@mail.gmail.com%3E",
        "Issue Links": []
    },
    "NUTCH-1818": {
        "Key": "NUTCH-1818",
        "Summary": "Add deps-test-compile task for building plugins",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.9",
        "Component/s": "build",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "17/Jul/14 12:00",
        "Updated": "17/Jul/14 12:51",
        "Resolved": "17/Jul/14 12:42",
        "Description": "From email thread : \n[After ant clean] the target \"compile-test\" of lib-regex-filter which fails.\nShould it be really called for target \"runtime\"?\n  <target name=\"deps-jar\">\n    <ant target=\"jar\" inheritall=\"false\" dir=\"../lib-regex-filter\"/>\n    <ant target=\"compile-test\" inheritall=\"false\" dir=\"../lib-regex-filter\"/>\n  </target>\nThis is the source of the problem indeed, the second line should not be there : the test classes are not required at that stage. Both urlfilter-automaton and urlfilter-regex have the same problem, which was not apparent until I introduced a cleaner separation between the compilation and test deps.\nI've fixed that for trunk in revision 1611303.\n------------------------------------------------\nThis revealed another hidden issue which is that the test classes for the dependency plugins (e.g. lib-regex-filter) are not compiled anymore in the deps-jar step and we can't refer to them in the classpath.\nThere actually is a task deps-test which gets overriden by the plugins but this is not used by compile-test and despite its name has nothing to do with the tests but with the deployment of the dependency plugins. \nThe best approach is to add a new overridable tasks for the plugins 'deps-test-compile' and trigger the compilation of the test classes for the dep plugins from there.",
        "Issue Links": []
    },
    "NUTCH-1819": {
        "Key": "NUTCH-1819",
        "Summary": "Check for batchId input in GeneratorJob#run",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "generator",
        "Assignee": "Fjodor Vershinin",
        "Reporter": "Fjodor Vershinin",
        "Created": "20/Jul/14 21:23",
        "Updated": "31/Jul/14 05:17",
        "Resolved": "31/Jul/14 01:24",
        "Description": "When I run it GeneratorJob from command line, it parses batchId without any problem. However, if I run it using API (run method), then it gives exception. Problem is caused by fact, that batchId is parsed only in main method.\nI would propose simple patch to fix this problem.",
        "Issue Links": []
    },
    "NUTCH-1820": {
        "Key": "NUTCH-1820",
        "Summary": "remove field \"orig\" which duplicates \"id\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Jul/14 16:15",
        "Updated": "01/Nov/14 18:45",
        "Resolved": "01/Nov/14 18:45",
        "Description": "The indexing filter plugin index-basic (2.x only) adds a field \"orig\" which contains the \"real\" URL (not the reprUrl) and duplicates the field \"id\" (also regarding the field params: stored=\"true\" indexed=\"true\"). The field \"orig\" should be removed from index-basic and schema.xml.",
        "Issue Links": [
            "/jira/browse/NUTCH-1708"
        ]
    },
    "NUTCH-1821": {
        "Key": "NUTCH-1821",
        "Summary": "Nutch Crawl class for EMR",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.6",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Luis Lopez",
        "Created": "21/Jul/14 22:37",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "22/Jul/14 08:18",
        "Description": "Hi all,\nSome of us are using Amazon EMR to deploy/run Nutch and from what I've been reading in the users mailing list there are 2 common issues people run into... first, EMR supports up to Hadoop 1.0.3 (which is kind of old) and second, from Nutch 1.8+ the Crawl class has been deprecated. \nThe first issue poses a problem when we try to deploy recent Nutch versions. The most recent version that is supported by EMR is 1.6, the second issue is that EMR receives a jar and main class to do its job and from 1.8 the Crawl class has been removed.\nAfter some tests we completed a branch (from Nutch 1.6 + Hadoop 1.0.3) that improves the old Crawl class so it scales, since 1.6 is an old version I wonder how can we contribute back to those that need to use ElasticMapreduce.\nThe things we did are:\na) Add num fetchers as a parameter to the Crawl class.\n    For some reason the generator was always defaulting to one list( see: http://stackoverflow.com/questions/10264183/why-does-nutch-only-run-the-fetch-step-on-one-hadoop-node-when-the-cluster-has) creating just one fetch map task... with the new parameter we can adjust the map tasks to fit the cluster size.\nb) Index documents on each Crawl cycle and not at the end.\n     We had performance/memory issues when we tried to index all the documents when the whole crawl is done, we moved the index part into the main Crawl cycle.\nc) We added an option to delete segments after their content is indexed into Solr. It saves HDF space since the EC2 instances we use don't have a lot of space.\nSo far these fixes have allowed us to scale out Nutch and be efficient with Amazon EMR clusters. If you guys think that there is some value on these changes we can  submit a patch file.\nLuis.",
        "Issue Links": []
    },
    "NUTCH-1822": {
        "Key": "NUTCH-1822",
        "Summary": "Page outlinks  clearance is not appropriate",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.1",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Riyaz Shaik",
        "Created": "22/Jul/14 12:57",
        "Updated": "08/Jan/23 19:13",
        "Resolved": "08/Jan/23 19:13",
        "Description": "1. When a page is re-crawled and identified with new outlink urls along with the existing urls, old outlinks are getting removed and only new urls are updated to hbase. \nEx:\n Crawl cycle 1 for www.123.com, identified outlinks are \nol  --> abc.com \nol --> pqr.com \nCrawlcyle 2 of same www.123.com, the outlinks are\n(note that abc.com is removed and added with xyz.com) \nol --> pqr.com \nol --> xyz.com \nAt the end of crawlcycle 2, base has only xyz.com as outlink\nol -->xyz.com\nExpected:\nol --> pqr.com \nol --> xyz.com \n2. If some of the outlinks of the page got removed and no new outlinks are added to the page then page re-crawl is not clearing the obsolete/removed outlinks from hbase.\nEx: Cycle 1 crawled page : www.test.com, identified outlinks are\nol -->link1\nol-->link2\nol-->link3\nCycle 2 same page(www.text.com) re-crawled, identified outlinks are\n(Note: only removed the link2 no new links are added)\n ol-->link1\nol-->link3\n but the end of the cycle 2.,it has all the 3 outlinks in hbase\nin habse:\nol -->link1\nol-->link2\nol-->link3\nexpected:\n ol-->link1\nol-->link3\nAs per the code ParseUtil.java, it seems to be removing the old links and insets onlythe new links. \nif (page.getOutlinks() != null) \n{ page.getOutlinks().clear(); }\n\nhttp://lucene.472066.n3.nabble.com/Nutch-New-outlinks-removes-old-valid-outlinks-td4146676.html\nThanks\nRiyaz",
        "Issue Links": []
    },
    "NUTCH-1823": {
        "Key": "NUTCH-1823",
        "Summary": "Upgrade to elasticsearch 1.4.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Vineet Sharma",
        "Created": "07/Aug/14 14:31",
        "Updated": "06/Apr/15 16:57",
        "Resolved": "12/Dec/14 00:21",
        "Description": "Elasticsearch is currently at version 1.3.x. We should start working towards upgrading the plugin to 1.2 and 1.3.",
        "Issue Links": []
    },
    "NUTCH-1824": {
        "Key": "NUTCH-1824",
        "Summary": "protocol-http using proxy not working with https sites",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Phu Kieu",
        "Created": "15/Aug/14 17:14",
        "Updated": "09/May/16 17:58",
        "Resolved": null,
        "Description": "https sites do not work with protocol-http using a proxy.\nFurther inspection of the source shows that it is not issuing CONNECT requests when it encounters an https address.\n2014-08-15 09:27:20,295 INFO  api.HttpRobotRulesParser - Couldn't get robots.txt for https://*****: java.net.ConnectException: Connection refused\n2014-08-15 09:27:20,296 ERROR http.Http - Failed to get protocol output\njava.net.ConnectException: Connection refused\n\tat java.net.PlainSocketImpl.socketConnect(Native Method)\n\tat java.net.PlainSocketImpl.doConnect(PlainSocketImpl.java:382)\n\tat java.net.PlainSocketImpl.connectToAddress(PlainSocketImpl.java:241)\n\tat java.net.PlainSocketImpl.connect(PlainSocketImpl.java:228)\n\tat java.net.SocksSocketImpl.connect(SocksSocketImpl.java:431)\n\tat java.net.Socket.connect(Socket.java:527)\n\tat org.apache.nutch.protocol.http.HttpResponse.<init>(HttpResponse.java:126)\n\tat org.apache.nutch.protocol.http.Http.getResponse(Http.java:72)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:183)\n\tat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:715)",
        "Issue Links": []
    },
    "NUTCH-1825": {
        "Key": "NUTCH-1825",
        "Summary": "protocol-http may hang for certain web pages",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Phu Kieu",
        "Created": "15/Aug/14 22:26",
        "Updated": "06/Nov/14 22:52",
        "Resolved": "06/Nov/14 21:53",
        "Description": "There is a rare case where protocol-http will wait for data even when all the data has been sent.\nPatch is attached; please test and confirm.",
        "Issue Links": [
            "/jira/browse/NUTCH-1342"
        ]
    },
    "NUTCH-1826": {
        "Key": "NUTCH-1826",
        "Summary": "indexchecker fails if solr.server.url not configured",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "22/Aug/14 22:15",
        "Updated": "25/Oct/14 04:36",
        "Resolved": "02/Oct/14 21:37",
        "Description": "IndexingFiltersChecker fails unnecessarily with an exception if indexer-solr is among plugin.includes but solr.server.url is not configured. Even without -DdoIndex (cf. NUTCH-1758).\n\n% nutch indexchecker http://localhost/\nfetching: http://localhost/\nException in thread \"main\" java.lang.RuntimeException: Missing SOLR URL. Should be set via -D solr.server.url\n  ...\n  at o.a.n.indexer.IndexingFiltersChecker.run(IndexingFiltersChecker.java:98)",
        "Issue Links": [
            "/jira/browse/NUTCH-1864"
        ]
    },
    "NUTCH-1827": {
        "Key": "NUTCH-1827",
        "Summary": "Port NUTCH-1467 and NUTCH-1561 to 2.x",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "23/Aug/14 10:12",
        "Updated": "20/Oct/14 21:42",
        "Resolved": "20/Oct/14 20:45",
        "Description": "NUTCH-1467 and NUTCH-1561 which include improvements to plugins parse-metatags and index-metadata should be ported from 1.x to 2.x.",
        "Issue Links": []
    },
    "NUTCH-1828": {
        "Key": "NUTCH-1828",
        "Summary": "bin/crawl : incorrect handling of nutch errors",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "nutchNewbie",
        "Assignee": null,
        "Reporter": "Mathieu Bouchard",
        "Created": "25/Aug/14 13:21",
        "Updated": "29/Aug/14 11:53",
        "Resolved": "29/Aug/14 11:24",
        "Description": "We are using Solr with Nutch to provide a complete search engine for our website.\nI created a cron job that would use Nutch to crawl and update the Solr index each night. This cron job is trying to automatically correct some errors that could result in a corrupt crawldb. However, it seems that the bin/crawl command doesn't correctly propagate errors coming from bin/nutch.\nHere is an exemple from the bin/crawl script :\n    $bin/nutch inject $CRAWL_PATH/crawldb $SEEDDIR\n    if [ $? -ne 0 ]\n      then exit $?\n    fi\nEven if there is an error in the nutch inject command, the crawl script always returns 0. The way I understand it, the exit code returned is the result of the shell test and not the result of the nutch inject command.\nTo correct this, we would need to modify the script with something like :\n    $bin/nutch inject $CRAWL_PATH/crawldb $SEEDDIR\n    RETCODE=$?\n    if [ $RETCODE -ne 0 ]\n      then exit $RETCODE\n    fi",
        "Issue Links": [
            "/jira/browse/NUTCH-1829"
        ]
    },
    "NUTCH-1829": {
        "Key": "NUTCH-1829",
        "Summary": "Generator : unable to distinguish real errors",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "nutchNewbie",
        "Assignee": "Julien Nioche",
        "Reporter": "Mathieu Bouchard",
        "Created": "25/Aug/14 13:30",
        "Updated": "08/Dec/14 19:53",
        "Resolved": "08/Dec/14 19:50",
        "Description": "The bin/nutch generate command is returning the same error code (-1) if there is an error or no new segment to process, so there is no way to tell if the error is real or not from a shell script. This problem is related to NUTCH-1828.\nThe problem can be fixed by modifying the following Java source file:\nhttp://svn.apache.org/viewvc/nutch/trunk/src/java/org/apache/nutch/crawl/Generator.java?revision=1619934&view=markup\nAt line 711, if there are no new segment, the generator returns -1, which is the same return code returned at line 714 if there was an error.",
        "Issue Links": [
            "/jira/browse/NUTCH-1828"
        ]
    },
    "NUTCH-1830": {
        "Key": "NUTCH-1830",
        "Summary": "Solr Delete Duplicates: Adding option to exclude IDs matching specified patterns",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Brian",
        "Created": "27/Aug/14 19:14",
        "Updated": "03/Sep/14 19:27",
        "Resolved": null,
        "Description": "The SolrDeleteDuplicates class and associated function has been helpful for getting rid of duplicate pages from variations of URLs.  However, there are some cases where the pages are very similar in terms of textual content but still need to be kept as distinct searchable pages.\nSometimes the textual content of two documents is very close so they would be counted as duplicates by the duplicate detector, but we may want both of them to be searchable.  \nFor example for some products or resellers of our products, the webpage template is the same and only a small amount of text may differ between different products/resellers.  Therefore some are counted as duplicates, but we want all to be included and searchable on our site, so people can find things by name, even if it is not in a key field.\nWe can manually specify which group of URLs these pages correspond to (via some regexes) to prevent them from being potentially deleted as duplicates.\nAs a result this provides a mechanism for manually excluding documents via ID from deduplication.\nThis patch adds an option to the configuration of nutch-site.xml, allowing users to specify a file containing a list of regular expressions with a new property \"solr.exclude.from.dedup.regex.file\":\n\n<property>\n   <name>solr.exclude.from.dedup.regex.file</name>\n   <value>regex-exclude-urls-from-dedup.txt</value>\n   <description>\n      Holds the file name of the file containing any regular expressions specifying URLs (ids) to be excluded from the Solr Deduplication process.\n      I.e., any URL matching one of the regular expressions will not be subject to potential deduplication.\n      Each pattern string must start on its own line with a \"-\" character at the beginning - all other lines will be ignored.\n      Also, the URLs must match the entire pattern.\n   </description>\n</property>\n\n\nThe property specifies a file name containing a list of regular expressions, indicated by the line starting with \"-\"\n   -If any ID matches one of these expressions during the deduplication process, the document with that ID will be skipped\n        --I.e., it will not be subject to deduplication\nHere is an example file:\n\n#Allows specifying regular expressions for which any matching URLs\n#will not be subjected to potential deduplication\n#Requires regex strings to match full URL\n#Each regex string must start with \"-\" all other lines are ignored.\n\n#Excludeall reseller pages from deduplication:\n-.*/company/reseller/.*",
        "Issue Links": []
    },
    "NUTCH-1831": {
        "Key": "NUTCH-1831",
        "Summary": "compiling against gora-0.5 fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.3",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "kaveh minooie",
        "Created": "27/Aug/14 22:24",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "19/Dec/14 21:45",
        "Description": "currenty if you try to compile nutch against Gora 0.5 you will get following errors:\nclean-lib:\nresolve-default:\n[ivy:resolve] :: Apache Ivy 2.3.0-local-20140109133456 - 20140109133456 :: http://ant.apache.org/ivy/ ::\n[ivy:resolve] :: loading settings :: file = /sources/nutch/ivy/ivysettings.xml\n[taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\ncopy-libs:\n[copy] Copying 128 files to /sources/nutch/build/lib\ncompile-core:\n[javac] Compiling 200 source files to /sources/nutch/build/classes\n[javac] warning: [options] bootstrap class path not set in conjunction with -source 1.6\n[javac] /sources/nutch/src/java/org/apache/nutch/storage/WebPage.java:8: error: WebPage is not abstract and does not override abstract method getFieldsCount() in PersistentBase\n[javac] public class WebPage extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {\n[javac]        ^\n[javac] /sources/nutch/src/java/org/apache/nutch/storage/ProtocolStatus.java:11: error: ProtocolStatus is not abstract and does not override abstract method getFieldsCount() in PersistentBase\n[javac] public class ProtocolStatus extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {\n[javac]        ^\n[javac] /sources/nutch/src/java/org/apache/nutch/storage/ParseStatus.java:8: error: ParseStatus is not abstract and does not override abstract method getFieldsCount() in PersistentBase\n[javac] public class ParseStatus extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {\n[javac]        ^\n[javac] /sources/nutch/src/java/org/apache/nutch/storage/Host.java:12: error: Host is not abstract and does not override abstract method getFieldsCount() in PersistentBase\n[javac] public class Host extends org.apache.gora.persistency.impl.PersistentBase implements org.apache.avro.specific.SpecificRecord, org.apache.gora.persistency.Persistent {\n[javac]        ^\n[javac] Note: Some input files use unchecked or unsafe operations.\n[javac] Note: Recompile with -Xlint:unchecked for details.\n[javac] 4 errors\n[javac] 1 warning\nBUILD FAILED\n/sources/nutch/build.xml:101: Compile failed; see the compiler error output for details.",
        "Issue Links": []
    },
    "NUTCH-1832": {
        "Key": "NUTCH-1832",
        "Summary": "Make Nutch work without an indexer",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "03/Sep/14 15:50",
        "Updated": "17/Sep/14 18:43",
        "Resolved": "03/Sep/14 20:43",
        "Description": "Nutch used to work out of the box, without requiring an indexing backend. As of 1.9, that's not the case anymore (it's possible even before that). Thanks to markus17 for pointing out that this is due to the indexing-solr plugin being enabled by default. We should disable it by default, so that the regression is removed.",
        "Issue Links": []
    },
    "NUTCH-1833": {
        "Key": "NUTCH-1833",
        "Summary": "Include version number within nutch binary usage statement",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "documentation",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Rishi Verma",
        "Created": "03/Sep/14 22:44",
        "Updated": "04/Sep/14 00:51",
        "Resolved": "03/Sep/14 23:56",
        "Description": "The nutch binary [1] currently does not include version information. This is useful to know, especially to distinguish between 1.x and 2.x versions of Nutch deployments.\n\u2013\n[1] http://svn.apache.org/repos/asf/nutch/trunk/src/bin/nutch",
        "Issue Links": []
    },
    "NUTCH-1834": {
        "Key": "NUTCH-1834",
        "Summary": "GeneratorMapper behavior depends on log level",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Gerhard Gossen",
        "Created": "04/Sep/14 13:13",
        "Updated": "23/Dec/14 16:41",
        "Resolved": "23/Dec/14 16:03",
        "Description": "In GeneratorMapper, two return statements are shifted by a line so that they end up in a if (LOG.isWarnEnabled()) block. Therefore a change of the log level will change the behavior of the generator, i.e. an URL where an IndexFilters throws an exception will still be generated.\nThe attached patch removes the if block completely by using SLF4J's placeholders. This avoids the unnecessary string concatenations that made the if necessary.",
        "Issue Links": []
    },
    "NUTCH-1835": {
        "Key": "NUTCH-1835",
        "Summary": "Nutch's Solr schema doesn't work with Solr 4.9 because of the RealTimeGet handler",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "04/Sep/14 17:00",
        "Updated": "05/Sep/14 05:58",
        "Resolved": "05/Sep/14 05:33",
        "Description": "Nutch's schema.xml file doesn't work with Solr 4.9 which has the RealTimeGetHandler implemented out of the box:\nhttp://stackoverflow.com/questions/19064361/solr-4-4-0-is-giving-error-code-500\nThe simple answer is to add a field to our schema.xml:\n\n<field name=\"_version_\" type=\"long\" indexed=\"true\" stored=\"true\"/>\n\n\nThis patch does that.",
        "Issue Links": []
    },
    "NUTCH-1836": {
        "Key": "NUTCH-1836",
        "Summary": "Timeouts in protocol-httpclient when crawling same host with >2 threads NUTCH-1613 is not a complete solution",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Adrian Newby",
        "Created": "07/Sep/14 18:08",
        "Updated": "05/Jul/15 02:47",
        "Resolved": null,
        "Description": "NUTCH-1613 provided a fix for the hardcoded limitation of 2 threads for protocol-httpclient.  However, just extending the hardwired 10 max threads and allocating them all to a single host only provides a partial solution.  It is still possible to exhaust the thread pool and observe timeouts depending on the settings of:\n\nfetcher.threads.per.host (nutch-site.xml)\nmapred.tasktracker.map.tasks.maximum (mapred-site.xml)\n\nIt would perhaps be more robust to set the httpclient thread pool as a derivative of these two configuration parameters as below:\n\n    params.setMaxTotalConnections(maxThreadsTotal);\n\n// Add the following lines ...\n\n\n\t// --------------------------------------------------------------------------------\n\t// Modification to increase the number of available connections for\n\t// multi-threaded crawls.\n\t// --------------------------------------------------------------------------------\n\tconnectionManager.setMaxConnectionsPerHost(conf.getInt(\"fetcher.threads.per.host\", 10));\n\tconnectionManager.setMaxTotalConnections(conf.getInt(\"mapred.tasktracker.map.tasks.maximum\", 5) * conf.getInt(\"fetcher.threads.per.host\", 10));\n\tLOG.debug(\"setMaxConnectionsPerHost: \" + connectionManager.getMaxConnectionsPerHost());\n\tLOG.debug(\"setMaxTotalConnections  : \" + connectionManager.getMaxTotalConnections());\n\t// --------------------------------------------------------------------------------",
        "Issue Links": []
    },
    "NUTCH-1837": {
        "Key": "NUTCH-1837",
        "Summary": "Upgrade to Tika 1.6",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "build,                                            parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "08/Sep/14 18:07",
        "Updated": "08/Sep/14 22:16",
        "Resolved": "08/Sep/14 20:59",
        "Description": "Tika 1.6 has just been released.\nLets upgrade",
        "Issue Links": []
    },
    "NUTCH-1838": {
        "Key": "NUTCH-1838",
        "Summary": "Host and domain based regex and automaton filtering",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "09/Sep/14 12:25",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "18/Jan/16 20:39",
        "Description": "Both regex and automaton filter pass all URL's through all rules although this makes little sense if you have a lot of generated rules for many different hosts or domains. This patch allows the users to configure specific rules for a specific host or domain only, making filtering much more efficient.\nEach rule has an optional hostOrDomain field, the filter is applied for rules that have no hostOrDomain and for URL's that match the rule's host name and domain name.\nThe following line enables hostOrDomain specific rules:\n\n> www.example.org\n\n\nThe following line disables/resets it again:\n\n<\n\n\nfull example:\n\n-some generic filter\n+another generic filter\n\n> www.example.org\n-rule only applied to URL's of www.example.org\n+another rule only applied to URL's of www.example.org\n\n> apache.org\n-rule only applied to URL's of apache.org\n+another rule only applied to URL's of apache.org\n\n<\n-more generic rules\n+and another one",
        "Issue Links": []
    },
    "NUTCH-1839": {
        "Key": "NUTCH-1839",
        "Summary": "Improve WebGraph CLI parsing",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "linkdb",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Sep/14 07:18",
        "Updated": "06/Apr/15 16:58",
        "Resolved": "23/Sep/14 13:46",
        "Description": "Right now the WebGraph parsing can be improved to at least print out the options as well as what the options represent.\nPatch coming up.",
        "Issue Links": []
    },
    "NUTCH-1840": {
        "Key": "NUTCH-1840",
        "Summary": "the describe function in SolrIndexWriter is not correct",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "kaveh minooie",
        "Created": "11/Sep/14 21:04",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "12/Sep/14 07:47",
        "Description": "the describe function in SolrIndexWriter is not correct",
        "Issue Links": []
    },
    "NUTCH-1841": {
        "Key": "NUTCH-1841",
        "Summary": "Two nits with developer wiki page",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Arthur Cinader",
        "Created": "16/Sep/14 05:07",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "17/Sep/14 16:24",
        "Description": "the text: \"as well as make sure that the and build and javadoc are successful with your new code\"\n\"and build\" should be \"ant build\"\nthe text: \"Together we can all life each other higher.\"\n\"all life each other\" should be \"all lift each other\"\nPS: I created a wiki account under ArthurCinader and if you give me edit rights, I'll fix myself.",
        "Issue Links": []
    },
    "NUTCH-1842": {
        "Key": "NUTCH-1842",
        "Summary": "crawl.gen.delay has a wrong default value in nutch-default.xml or is being parsed incorrectly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.16",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "kaveh minooie",
        "Created": "16/Sep/14 21:42",
        "Updated": "03/Mar/22 14:27",
        "Resolved": "15/Nov/18 10:35",
        "Description": "this is from nutch-default.xml:\n<property>\n  <name>crawl.gen.delay</name>\n  <value>604800000</value>\n  <description>\n   This value, expressed in milliseconds, defines how long we should keep the lock on records \n   in CrawlDb that were just selected for fetching. If these records are not updated \n   in the meantime, the lock is canceled, i.e. they become eligible for selecting. \n   Default value of this is 7 days (604800000 ms).\n  </description>\n</property>\nthis is the from o.a.n.crawl.Generator.configure(JobConf job)\ngenDelay = job.getLong(GENERATOR_DELAY, 7L) * 3600L * 24L * 1000L;\nthe value in config file is in milliseconds but the code expect it to be in days. I reported this couple of years ago on the mailing list as well. I didn't post a patch becaue I am not sure which one needs to be fixed. considering all the other values in config file are in milliseconds it can be argued to that consistency matters, but 'day' is a much more reasonable unit for this property.\nAlso this value is not being used in 2.x ?",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/393"
        ]
    },
    "NUTCH-1843": {
        "Key": "NUTCH-1843",
        "Summary": "Upgrade to Gora 0.5",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "build,                                            storage",
        "Assignee": "Talat Uyarer",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Sep/14 20:34",
        "Updated": "01/Nov/14 16:45",
        "Resolved": "01/Nov/14 16:45",
        "Description": "We just released Gora 0.5 \nhttp://www.mail-archive.com/dev%40gora.apache.org/msg05236.html\nWe should upgrade before releasing Nutch 2.3",
        "Issue Links": [
            "/jira/browse/GORA-388"
        ]
    },
    "NUTCH-1844": {
        "Key": "NUTCH-1844",
        "Summary": "testresources/testcrawl not referenced anywhere in code",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "test",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "21/Sep/14 05:53",
        "Updated": "25/Sep/14 06:26",
        "Resolved": "25/Sep/14 05:32",
        "Description": "While working on NUTCH-1526 in Review Board https://reviews.apache.org/r/9119/ lewismc tried to test out the ./bin/nutch dump tool on src/testresources/testcrawl and found that it failed due to an old o.a.h.io.UTF8 key type (instead of the o.a.h.io.Text) type. \nI looked into this - how were Nutch tests passing using this old code? I found that Andrzej a long time ago wrote a tool to update the index from the old UFT8 key format to Text - I also found that no where in the Nutch code is the testcrawl referenced.\nMy suggestion: \n\nwe remove the testcrawl (it's not used)\nif we don't remove it, we at least run Andrzej's tool on it and then upgrade it to use o.a.h.io.Text keys.\n\nI'll take care of this.",
        "Issue Links": []
    },
    "NUTCH-1845": {
        "Key": "NUTCH-1845",
        "Summary": "Nutch cannot save inlinks",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zhiwen Sun",
        "Created": "22/Sep/14 02:08",
        "Updated": "22/Sep/14 05:37",
        "Resolved": "22/Sep/14 05:37",
        "Description": "Nutch use inlinks to save the referers of a link, but inlinks are not found in storage.\nWhen I  use HBase as backend storage, I can't find any columns in il column family.",
        "Issue Links": []
    },
    "NUTCH-1846": {
        "Key": "NUTCH-1846",
        "Summary": "Implement Shiro WebApp Security",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Sep/14 19:42",
        "Updated": "04/Jan/22 17:04",
        "Resolved": "13/Oct/19 22:35",
        "Description": "We could improve the Nutch 2.X Web App by adding a security layer with Apache Shiro [0]. This would make remote servers much more secure. \nAn example tutorial is here - http://shiro.apache.org/webapp-tutorial.html\n[0] http://shiro.apache.org/",
        "Issue Links": [
            "/jira/browse/NUTCH-2925"
        ]
    },
    "NUTCH-1847": {
        "Key": "NUTCH-1847",
        "Summary": "Resolve error in log when we navigate to WebApp SettingsPage",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Sep/14 20:54",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Currently when I navigate to org.apache.nutch.webui.pages.settings.SettingsPage I get the following in my log tail\nhadoop.log\n2014-09-22 16:39:12,537 ERROR java.JavaSerializer - Error serializing object class org.apache.nutch.webui.pages.settings.SettingsPage [object=[Page class = org.apache.nutch.webui.pages.settings.SettingsPage, id = 21, render count = 1]]\norg.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream$ObjectCheckException: The object type is not Serializable!\nA problem occurred while checking object with type: org.apache.nutch.webui.model.NutchConfig\nField hierarchy is:\n  21 [class=org.apache.nutch.webui.pages.settings.SettingsPage, path=21]\n    private java.lang.Object org.apache.wicket.MarkupContainer.children [class=[Ljava.lang.Object;]\n      private org.apache.wicket.util.time.Duration de.agilecoders.wicket.core.markup.html.bootstrap.common.NotificationPanel.duration[2] [class=org.apache.wicket.markup.html.WebMarkupContainer, path=21:settingsTable]\n        private java.lang.Object org.apache.wicket.MarkupContainer.children [class=org.apache.wicket.markup.repeater.RefreshingView, path=21:settingsTable:settings]\n          private java.lang.Object org.apache.wicket.MarkupContainer.children [class=[Ljava.lang.Object;]\n            private java.lang.Object org.apache.wicket.MarkupContainer.children[0] [class=org.apache.wicket.markup.repeater.Item, path=21:settingsTable:settings:1]\n              java.lang.Object org.apache.wicket.Component.data [class=org.apache.wicket.model.CompoundPropertyModel]\n                private java.lang.Object org.apache.wicket.model.ChainingModel.target [class=org.apache.nutch.webui.model.NutchConfig] <----- field that is causing the problem\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck(CheckingObjectOutputStream.java:387)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.check(CheckingObjectOutputStream.java:361)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.checkFields(CheckingObjectOutputStream.java:645)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck(CheckingObjectOutputStream.java:569)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.check(CheckingObjectOutputStream.java:361)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.checkFields(CheckingObjectOutputStream.java:645)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck(CheckingObjectOutputStream.java:569)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.check(CheckingObjectOutputStream.java:361)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck(CheckingObjectOutputStream.java:432)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.check(CheckingObjectOutputStream.java:361)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.checkFields(CheckingObjectOutputStream.java:645)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck(CheckingObjectOutputStream.java:569)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.check(CheckingObjectOutputStream.java:361)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.checkFields(CheckingObjectOutputStream.java:645)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck(CheckingObjectOutputStream.java:569)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.check(CheckingObjectOutputStream.java:361)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck(CheckingObjectOutputStream.java:432)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.check(CheckingObjectOutputStream.java:361)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.checkFields(CheckingObjectOutputStream.java:645)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.internalCheck(CheckingObjectOutputStream.java:569)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.check(CheckingObjectOutputStream.java:361)\n\tat org.apache.wicket.core.util.objects.checker.CheckingObjectOutputStream.writeObjectOverride(CheckingObjectOutputStream.java:713)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:343)\n\tat org.apache.wicket.serialize.java.JavaSerializer$SerializationCheckerObjectOutputStream.writeObjectOverride(JavaSerializer.java:268)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:343)\n\tat org.apache.wicket.serialize.java.JavaSerializer.serialize(JavaSerializer.java:78)\n\tat org.apache.wicket.pageStore.DefaultPageStore.serializePage(DefaultPageStore.java:376)\n\tat org.apache.wicket.pageStore.DefaultPageStore.storePage(DefaultPageStore.java:150)\n\tat org.apache.wicket.page.PageStoreManager$PersistentRequestAdapter.storeTouchedPages(PageStoreManager.java:412)\n\tat org.apache.wicket.page.RequestAdapter.commitRequest(RequestAdapter.java:181)\n\tat org.apache.wicket.page.AbstractPageManager.commitRequest(AbstractPageManager.java:98)\n\tat org.apache.wicket.page.PageManagerDecorator.commitRequest(PageManagerDecorator.java:73)\n\tat org.apache.wicket.page.PageAccessSynchronizer$2.commitRequest(PageAccessSynchronizer.java:258)\n\tat org.apache.wicket.Application$2.onDetach(Application.java:1666)\n\tat org.apache.wicket.request.cycle.RequestCycleListenerCollection$3.notify(RequestCycleListenerCollection.java:105)\n\tat org.apache.wicket.request.cycle.RequestCycleListenerCollection$3.notify(RequestCycleListenerCollection.java:101)\n\tat org.apache.wicket.util.listener.ListenerCollection$1.notify(ListenerCollection.java:120)\n\tat org.apache.wicket.util.listener.ListenerCollection.reversedNotify(ListenerCollection.java:144)\n\tat org.apache.wicket.util.listener.ListenerCollection.reversedNotifyIgnoringExceptions(ListenerCollection.java:113)\n\tat org.apache.wicket.request.cycle.RequestCycleListenerCollection.onDetach(RequestCycleListenerCollection.java:100)\n\tat org.apache.wicket.request.cycle.RequestCycle.onDetach(RequestCycle.java:640)\n\tat org.apache.wicket.request.cycle.RequestCycle.detach(RequestCycle.java:589)\n\tat org.apache.wicket.request.cycle.RequestCycle.processRequestAndDetach(RequestCycle.java:293)\n\tat org.apache.wicket.protocol.http.WicketFilter.processRequestCycle(WicketFilter.java:259)\n\tat org.apache.wicket.protocol.http.WicketFilter.processRequest(WicketFilter.java:201)\n\tat org.apache.wicket.protocol.http.WicketFilter.doFilter(WicketFilter.java:282)\n\tat org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)\n\tat org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)\n\tat org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\n\tat org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\n\tat org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\n\tat org.mortbay.jetty.Server.handle(Server.java:326)\n\tat org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\n\tat org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\n\tat org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\n\tat org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\n\tat org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\n\tat org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)\n\tat org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\nCaused by: java.io.NotSerializableException: org.apache.nutch.webui.model.NutchConfig\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\n\tat java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)\n\tat java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)\n\tat java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)\n\tat java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)\n\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)\n\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)\n\tat org.apache.wicket.serialize.java.JavaSerializer$SerializationCheckerObjectOutputStream.writeObjectOverride(JavaSerializer.java:260)\n\t... 35 more\n2014-09-22 16:39:12,538 WARN  pageStore.DefaultPageStore - Page [Page class = org.apache.nutch.webui.pages.settings.SettingsPage, id = 21, render count = 1] cannot be serialized. See previous logs for possible reasons.",
        "Issue Links": []
    },
    "NUTCH-1848": {
        "Key": "NUTCH-1848",
        "Summary": "Bug in DashboardPage.html instances counter",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "web gui",
        "Assignee": "Nima Falaki",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Sep/14 21:00",
        "Updated": "29/Sep/14 04:48",
        "Resolved": "29/Sep/14 01:22",
        "Description": "When I run crawls from the WebApp, I am experiencing an irregularly increasing Instance counter. Please see attachment for an example. It should be noted that in this example I have no crawling instances running.",
        "Issue Links": []
    },
    "NUTCH-1849": {
        "Key": "NUTCH-1849",
        "Summary": "Evaluate Bokeh for stats visualization within StatisticsPage",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "23/Sep/14 00:17",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "We are interested in working with our partners at Continuum Analytics to implement the Statistics page (currently empty) using Bokeh [0]. AFAIK Bokeh has a REST API which we can interact with to dynamically visualize crawl data statistics in a Dashboard-style manner.\n[0] http://bokeh.pydata.org/",
        "Issue Links": []
    },
    "NUTCH-1850": {
        "Key": "NUTCH-1850",
        "Summary": "Add hover popup box for ConfigurationPage property descriptions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "23/Sep/14 00:20",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Right now the ConfigurationPage in the WebApp does not include any property descriptions meaning that users are left to infer whatever they want from properties.\nWe could improve this by implementing some hover-like functionality for dynamically displaying property descriptions.",
        "Issue Links": []
    },
    "NUTCH-1851": {
        "Key": "NUTCH-1851",
        "Summary": "Add/Update wiki pages for NutchServer and WebApp",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "23/Sep/14 01:27",
        "Updated": "27/Sep/14 20:05",
        "Resolved": "27/Sep/14 20:05",
        "Description": "Since we comitted the Web App we should have documentation for it.\nLinks should be added from the Nutch 2 tutorial as well.",
        "Issue Links": []
    },
    "NUTCH-1852": {
        "Key": "NUTCH-1852",
        "Summary": "Runtime error on Hadoop 2.4.0 caused by hadoop-core",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.3",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Viacheslav Dobromyslov",
        "Created": "23/Sep/14 09:52",
        "Updated": "25/Sep/14 06:45",
        "Resolved": "25/Sep/14 06:45",
        "Description": "Tested latest Nutch 2.3-SNAPSHOT with Amazon Hadoop 2.4.0 and got the following error:\n\n14/09/23 05:37:14 INFO mapreduce.Job: Task Id : attempt_1411446093851_0003_m_000000_0, Status : FAILED\nError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected\n\n\nIt's caused by hadoop-core dependency which states that TaskAttemptContext is class. But this class was transformed into an interface in new hadoop-common.",
        "Issue Links": []
    },
    "NUTCH-1853": {
        "Key": "NUTCH-1853",
        "Summary": "Add commented out WebGraph executions to ./bin/crawl",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "23/Sep/14 14:42",
        "Updated": "27/Sep/14 23:26",
        "Resolved": "27/Sep/14 21:57",
        "Description": "Right now it is a pain in the back side to have to continuously add the WebGraph-like commands to every new ./bin/crawl script. I propose we add commented out commands to the script we maintain at \nhttps://svn.apache.org/repos/asf/nutch/trunk/src/bin/crawl",
        "Issue Links": []
    },
    "NUTCH-1854": {
        "Key": "NUTCH-1854",
        "Summary": "./bin/crawl fails with a parsing fetcher",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Lewis John McGibbney",
        "Created": "25/Sep/14 02:18",
        "Updated": "18/May/15 00:50",
        "Resolved": "18/Apr/15 20:42",
        "Description": "If you run ./bin/crawl with a parsing fetcher e.g.\n<property>\n>   <name>fetcher.parse</name>\n>   <value>false</value>\n>   <description>If true, fetcher will parse content. Default is false,\n> which means\n>   that a separate parsing step is required after fetching is\n> finished.</description>\n> </property>\nwe get a horrible message as follows\nException in thread \"main\" java.io.IOException: Segment already parsed!\nWe could improve this by making logging more complete and by adding a trigger to the crawl script which would check for crawl_parse for a given segment and then skip parsing if this is present.",
        "Issue Links": []
    },
    "NUTCH-1855": {
        "Key": "NUTCH-1855",
        "Summary": "Upgrade Hadoop dependencies to Hadoop 2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "25/Sep/14 06:41",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "We should upgrade our Hadoop dependency 1 to 2",
        "Issue Links": []
    },
    "NUTCH-1856": {
        "Key": "NUTCH-1856",
        "Summary": "Document webpage.avsc and host.avsc",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "25/Sep/14 17:16",
        "Updated": "06/Apr/15 16:57",
        "Resolved": "09/Jan/15 03:54",
        "Description": "We can easily document Avro schema files as defined in the current Avro specification\nhttp://avro.apache.org/docs/current/spec.html#schema_complex\nWe should document both of the above files to provide more meaningful comments in generated source.",
        "Issue Links": []
    },
    "NUTCH-1857": {
        "Key": "NUTCH-1857",
        "Summary": "readb -dump -format csv should use comma",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "crawldb",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "25/Sep/14 20:00",
        "Updated": "01/Oct/14 21:50",
        "Resolved": "29/Sep/14 22:07",
        "Description": "The -dump -format csv option currently uses ASCII character ';' %3B which is not a comma but instead a semi-colon.\nThis is a pain in the back side as I always need to override this within the Solr update request.\nWe should change the behavhiour to default to the common comma... as indicated here\nhttp://www.ietf.org/rfc/rfc4180.txt",
        "Issue Links": []
    },
    "NUTCH-1858": {
        "Key": "NUTCH-1858",
        "Summary": "Migrate Nutch documentation from Moin Moin to Confluence",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Sep/14 19:16",
        "Updated": "08/Dec/20 12:53",
        "Resolved": "08/Dec/20 12:53",
        "Description": "We've had initial support for moving the documentation out of MoinMoin and on to Confluence.\nIf we manage confluence correctly then this 'could' pave a path for us having much more structured, meaningful and useful documentation over MoinMoin.",
        "Issue Links": []
    },
    "NUTCH-1859": {
        "Key": "NUTCH-1859",
        "Summary": "Make Nutch webapp port configurable",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "web gui",
        "Assignee": "Nima Falaki",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Sep/14 19:30",
        "Updated": "30/Sep/14 00:42",
        "Resolved": "30/Sep/14 00:37",
        "Description": "Right now the Nutch WebApp only runs on port 8080... which will undoubtedly have people running into conflicts all over the place.\nWe should do the same as with nutchserver and make this configurable.",
        "Issue Links": []
    },
    "NUTCH-1860": {
        "Key": "NUTCH-1860",
        "Summary": "Protocol IMAPS Support",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Sep/14 20:09",
        "Updated": "18/Feb/21 02:00",
        "Resolved": null,
        "Description": "Implementing the Internet Messaging Access Protocol within Nutch would open up a new use case which is crawling and indexing of  email servers via IMAP.",
        "Issue Links": [
            "/jira/browse/NUTCH-1861"
        ]
    },
    "NUTCH-1861": {
        "Key": "NUTCH-1861",
        "Summary": "Implement POP3 Protocol",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Do",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Sep/14 20:11",
        "Updated": "11/Oct/20 00:37",
        "Resolved": "11/Oct/20 00:37",
        "Description": "Implementing the Post Office Protocol within Nutch would open up a new use case which is crawling and indexing of some mail servers.\nThis is particularly useful for investigation purposes or for porting/mapping mail from one server to another. \nWe may be able to kil two bird with the one stone by implementing both IMAP and POP3 protocols under the one plugin.\nhttp://commons.apache.org/proper/commons-net/",
        "Issue Links": [
            "/jira/browse/NUTCH-1860"
        ]
    },
    "NUTCH-1862": {
        "Key": "NUTCH-1862",
        "Summary": "Port flexible readdb dump formatting options to 2.X",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Sep/14 04:02",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Right now in 1.X we can format the crawldb dump as follows\n\n-format csv - in Csv format\n-format normal - dump in standard format (default option), and\n-format crawldb - dump as CrawlDB\n\nWe should port this to 2.X as it is extremely helpful for cross language support to be able to read alternative data input formats e.g. CSV",
        "Issue Links": []
    },
    "NUTCH-1863": {
        "Key": "NUTCH-1863",
        "Summary": "Add JSON format dump output to readdb command",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "1.17",
        "Component/s": "crawldb",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Sep/14 04:04",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "27/Dec/19 16:44",
        "Description": "Opening up the ability for third parties to consume Nutch crawldb data as JSON would be a poisitive thing IMHO.\nThis issue should improve the readdb functionality of both 1.X to enable JSON dumps of crawldb data.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/490"
        ]
    },
    "NUTCH-1864": {
        "Key": "NUTCH-1864",
        "Summary": "Bug in indexchecker CLI parsing and invocation of index-solr plugin by default",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "30/Sep/14 00:01",
        "Updated": "02/Oct/14 21:54",
        "Resolved": "02/Oct/14 21:33",
        "Description": "I noticed ok that we have a bug in indexchecker tool where \n\nthe command line parsing is buggy, it expects the args.length -1 argument to be the URL IIRC.\nEven if indexer-solr is NOT activated, I get the following message\n\nlmcgibbn@LMC-032857 /usr/local/trunk/runtime/local $ ./bin/nutch indexchecker -dumpText http://nasa.gov\nfetching: http://nasa.gov\nException in thread \"main\" java.lang.RuntimeException: Missing SOLR URL. Should be set via -D solr.server.url\nSOLRIndexWriter\n\tsolr.server.url : URL of the SOLR instance (mandatory)\n\tsolr.commit.size : buffer size when sending to SOLR (default 1000)\n\tsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n\tsolr.auth : use authentication (default false)\n\tsolr.auth.username : username for authentication\n\tsolr.auth.password : password for authentication\n\tat org.apache.nutch.indexwriter.solr.SolrIndexWriter.setConf(SolrIndexWriter.java:192)\n\tat org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:159)\n\tat org.apache.nutch.indexer.IndexWriters.<init>(IndexWriters.java:57)\n\tat org.apache.nutch.indexer.IndexingFiltersChecker.run(IndexingFiltersChecker.java:98)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.indexer.IndexingFiltersChecker.main(IndexingFiltersChecker.java:178)\nThese issues should be rectified as this is an extremely useful tool which is broken right now.",
        "Issue Links": [
            "/jira/browse/NUTCH-1826"
        ]
    },
    "NUTCH-1865": {
        "Key": "NUTCH-1865",
        "Summary": "Enable use of SNAPSHOT's with Nutch Ivy dependency management",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "30/Sep/14 16:38",
        "Updated": "27/Oct/14 18:51",
        "Resolved": "27/Oct/14 18:10",
        "Description": "Right now in 2.X we are able to tuse SNAPSHOT dependencies from http://respository.apache.org.\nWe should port this to trunk as it is really helpful for example if you would like to use Tika SNAPSHOT which has loads of goodies.",
        "Issue Links": []
    },
    "NUTCH-1866": {
        "Key": "NUTCH-1866",
        "Summary": "ant eclipse target should not delete runtime",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "build",
        "Assignee": "Nima Falaki",
        "Reporter": "Lewis John McGibbney",
        "Created": "30/Sep/14 23:16",
        "Updated": "01/Oct/14 19:28",
        "Resolved": "01/Oct/14 17:57",
        "Description": "This is a pretty nasty bug which can really screw things up for you.\nThe eclipse target should not delete runtime whenever invoked... this is terrible.",
        "Issue Links": []
    },
    "NUTCH-1867": {
        "Key": "NUTCH-1867",
        "Summary": "CrawlDbReader: use setFloat to pass min score",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "01/Oct/14 21:45",
        "Updated": "05/Oct/14 20:50",
        "Resolved": "05/Oct/14 20:36",
        "Description": "The float value \"min\" score in the CrawlDbTopNMapper is passed via property \"db.reader.topn.min\" as a long (multiplied by 1Mio.). The comment \"no setFloat() in the API\" is no longer valid, the method exists in Configuration and should be used. Reported by lewismc, see NUTCH-1857.",
        "Issue Links": []
    },
    "NUTCH-1868": {
        "Key": "NUTCH-1868",
        "Summary": "Document and improve CLI for FileDumper tool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "02/Oct/14 22:44",
        "Updated": "07/Oct/14 18:50",
        "Resolved": "07/Oct/14 18:23",
        "Description": "I think we could do a better job at explaining what he FileDumper is.\nI've added this aswell as improved CLI parsing and help.\nA further improvement to this tool would be to add a flag for individual/list of mimetypes. This would clean up generated raw output from the tool.",
        "Issue Links": []
    },
    "NUTCH-1869": {
        "Key": "NUTCH-1869",
        "Summary": "Add a flag to -mimeType fiag to FileDumper",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "02/Oct/14 22:47",
        "Updated": "07/Oct/14 18:50",
        "Resolved": "07/Oct/14 18:22",
        "Description": "A -mimeType flag would provide more expressive and custom use of this tool.\nThe flag should support an array of strings as input for users who wish to export certain mimetypes only.",
        "Issue Links": []
    },
    "NUTCH-1870": {
        "Key": "NUTCH-1870",
        "Summary": "Generic xsl parser plugin",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Albinscode",
        "Created": "05/Oct/14 19:36",
        "Updated": "14/Jul/20 16:49",
        "Resolved": null,
        "Description": "The aim of this plugin is to use XSLT to extract metadata from HTML DOM structures.\n\n\n\n Your Data \n --> \n Parse-html plugin  or TIKA plugin \n --> \n DOM structure \n --> \nXSLT plugin \n\n\n\nThe main advantage is that:\n\nYou won't have to produce any java code, only XSLT and configuration\nIt can process DOM structure from DocumentFragment (@see NekoHtml and @see TagSoup)\nIt is HtmlParseFilter plugin compatible and can be plugged as any other plugin (parse-js, parse-swf, etc...)\n\nThis topic has been discussed on http://www.mail-archive.com/dev%40nutch.apache.org/msg15257.html",
        "Issue Links": [
            "/jira/browse/NUTCH-1871",
            "/jira/browse/NUTCH-1644",
            "https://github.com/apache/nutch/pull/439"
        ]
    },
    "NUTCH-1871": {
        "Key": "NUTCH-1871",
        "Summary": "Generic xsl parser plugin",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.9",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Albinscode",
        "Created": "05/Oct/14 19:38",
        "Updated": "05/Oct/14 20:09",
        "Resolved": "05/Oct/14 20:09",
        "Description": "The aim of this plugin is to use XSLT to extract metadata from HTML DOM structures.\n\n\n\n Your Data \n --> \n Parse-html plugin  or TIKA plugin \n --> \n DOM structure \n --> \nXSLT plugin \n\n\n\nThe main advantage is that:\n\nYou won't have to produce any java code, only XSLT and configuration\nIt can process DOM structure from DocumentFragment (@see NekoHtml and @see TagSoup)\nIt is HtmlParseFilter plugin compatible and can be plugged as any other plugin (parse-js, parse-swf, etc...)\n\nThis topic has been discussed on http://www.mail-archive.com/dev%40nutch.apache.org/msg15257.html",
        "Issue Links": [
            "/jira/browse/NUTCH-1870"
        ]
    },
    "NUTCH-1872": {
        "Key": "NUTCH-1872",
        "Summary": "enables control over how injected metadata is propagated",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jonathan Cooper-Ellis",
        "Created": "09/Oct/14 20:39",
        "Updated": "23/Oct/14 20:51",
        "Resolved": null,
        "Description": "This builds on NUTCH-655 and NUTCH-855, allowing users some control over which outlinks receive injected metadata. A new configuration property \"urlmeta.rule\" has been introduced, with a default value of \"all\".\nThe value \"all\" indicated that \"urlmeta.tags\" should be propagated to all outlinks. Other options include: \"host\" (propagated to outlinks with the same host as the url with which the metadata was injected), \"domain\" (same, except with the same domain), \"prefix\" (treats the injected url as a prefix, so metadata is only propagated to urls that extend the injected url).\nWould appreciate feedback on whether you think this is a useful feature, and if its implemented properly.",
        "Issue Links": []
    },
    "NUTCH-1873": {
        "Key": "NUTCH-1873",
        "Summary": "Solr IndexWriter/Job to report number of docs indexed.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "1.11",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Oct/14 23:23",
        "Updated": "08/May/15 23:50",
        "Resolved": "08/May/15 23:30",
        "Description": "It is annoying when reading logs to NOT know how many docs were indexed at a certain point in time. Of course i could go to the Solr server and see this... if I have access to the URL, but this is not always the case. I do however always have access to the logs.",
        "Issue Links": []
    },
    "NUTCH-1874": {
        "Key": "NUTCH-1874",
        "Summary": "FileDumper comment typos",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Arthur Cinader",
        "Reporter": "Arthur Cinader",
        "Created": "11/Oct/14 05:30",
        "Updated": "11/Oct/14 20:50",
        "Resolved": "11/Oct/14 20:10",
        "Description": "Patch to fix two trivial comment typos in FileDumper",
        "Issue Links": []
    },
    "NUTCH-1875": {
        "Key": "NUTCH-1875",
        "Summary": "Add 'version' field to Solr schema as required by new Solr servers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Oct/14 20:03",
        "Updated": "11/Oct/14 20:07",
        "Resolved": "11/Oct/14 20:07",
        "Description": "The f<ield name=\"version\" type=\"long\" indexed=\"true\" stored=\"true\"/> is absent from the current Solr schema in Nutch trunk. It is nothing other than a pain for people to manually add this to the schema if and when they wish to use Solr for indexing content.",
        "Issue Links": []
    },
    "NUTCH-1876": {
        "Key": "NUTCH-1876",
        "Summary": "Upgrade to Crawler Commons 0.5",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Oct/14 04:58",
        "Updated": "16/Oct/14 14:51",
        "Resolved": "16/Oct/14 14:02",
        "Description": "We just released crawler commons 0.5\nWe should upgrade here in Nutch.\nhttps://code.google.com/p/crawler-commons/",
        "Issue Links": []
    },
    "NUTCH-1877": {
        "Key": "NUTCH-1877",
        "Summary": "Suffix URL filter to ignore query string by default",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Oct/14 15:35",
        "Updated": "05/Dec/14 20:50",
        "Resolved": "05/Dec/14 19:55",
        "Description": "Suffix URL filter entry: .mp3\nDoes not filter out: http://www.example.org/file.mp3?a=b",
        "Issue Links": []
    },
    "NUTCH-1878": {
        "Key": "NUTCH-1483 Can't crawl filesystem with protocol-file plugin",
        "Summary": "urlnormalizer-regex to keep third slash in file:///path/index.html",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "18/Oct/14 21:32",
        "Updated": "09/Jan/15 07:01",
        "Resolved": "04/Nov/14 21:14",
        "Description": "The rule\n\n<!-- removes duplicate slashes -->\n<regex>\n  <pattern>(?&lt;!:)/{2,}</pattern>\n  <substitution>/</substitution>\n</regex>\n\n\nin regex-normalize.xml removes the third slash in file:///path/index.html. The resulting URL file://path/index.html fails to fetch because path is interpreted as host part of the URL as in file://localhost/path/index.html, cf. wikipedia, RFC 1738 (1994), and RFC 3986 (2005).\n(split as sub-task from NUTCH-1483)",
        "Issue Links": [
            "/jira/browse/NUTCH-1879"
        ]
    },
    "NUTCH-1879": {
        "Key": "NUTCH-1483 Can't crawl filesystem with protocol-file plugin",
        "Summary": "Regex URL normalizer should remove multiple slashes after file: protocol",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "19/Oct/14 20:41",
        "Updated": "04/Nov/14 21:58",
        "Resolved": "04/Nov/14 21:15",
        "Description": "urlnormalizer-regex should replace multiple slashes after file: protocol by a single slash (file:/// -> file:/):\n\nrequired by NUTCH-1483 to get a consistent canonical form for file URL because URL.toString() also emits the single-slash form\nwould obsolete NUTCH-1878",
        "Issue Links": [
            "/jira/browse/NUTCH-1878"
        ]
    },
    "NUTCH-1880": {
        "Key": "NUTCH-1483 Can't crawl filesystem with protocol-file plugin",
        "Summary": "URLUtil should not add additional slashes for file URLs",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "19/Oct/14 20:49",
        "Updated": "04/Nov/14 21:58",
        "Resolved": "04/Nov/14 21:15",
        "Description": "UrlUtil.toASCII(String url) and .toUNICODE(String url) add two slashes to file URLs if it contains a single slash: file:/path/index.html becomes file:///path/index.html. Both methods should keep the single slash to get a behavior consistent with URL.toString(). See NUTCH-1483 for details.",
        "Issue Links": []
    },
    "NUTCH-1881": {
        "Key": "NUTCH-1881",
        "Summary": "ant target resolve-default to keep test libs",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "19/Oct/14 21:25",
        "Updated": "12/Jan/15 22:25",
        "Resolved": "12/Jan/15 20:46",
        "Description": "The ant target resolve-default depends on clean-lib which causes all libs to be removed, including test libs which are then not resolved again. After\n\n% ant resolve-test resolve-default\n\n\nbuild/test/lib/ is missing. Also any targets which depend on resolve-default can cause the libs to disappear. There is no impact on the build as such but Eclipse does complain if libs are missing (eg. junit) and there is no fast way to let them re-appear (except running target \"test\" which takes long).",
        "Issue Links": []
    },
    "NUTCH-1882": {
        "Key": "NUTCH-1882",
        "Summary": "ant eclipse target to add output path to src/test",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "19/Oct/14 21:46",
        "Updated": "21/Oct/14 18:51",
        "Resolved": "21/Oct/14 17:53",
        "Description": "class files for test classes are placed by ant build in build/test/classes/.\nEclipse project configuration should follow this convention which would required to add the output path to the source folder:\n\n<classpathentry kind=\"src\" path=\"src/test/\" output=\"build/test/classes\" />",
        "Issue Links": []
    },
    "NUTCH-1883": {
        "Key": "NUTCH-1883",
        "Summary": "bin/crawl: use function to run bin/nutch and check exit value",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "25/Oct/14 11:12",
        "Updated": "11/Nov/14 16:50",
        "Resolved": "11/Nov/14 16:21",
        "Description": "In bin/crawl for every Nutch command the exit value is checked explicitly:\n\n\"$bin/nutch\" ...\nRETCODE=$?\nif [ $RETCODE -ne 0 ] \nthen exit $RETCODE \nfi\n\n\nThis could be simplified by calling bin/nutch from one function which does the check. The function could also echo the command, show an error message, etc. The main advantage is short and clear code. In case a special treatment of exit values is required (cf. NUTCH-1829) we still could call bin/nutch directly.",
        "Issue Links": []
    },
    "NUTCH-1884": {
        "Key": "NUTCH-1884",
        "Summary": "NullPointerException in parsechecker and indexchecker with symlinks in file URL",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Angela Wang",
        "Created": "31/Oct/14 04:57",
        "Updated": "06/Nov/14 22:52",
        "Resolved": "06/Nov/14 22:01",
        "Description": "I have downloaded the Nutch source code from github (https://github.com/apache/nutch), applied the patches (NUTCH-1879 and NUTCH-1880), and then reinstalled the Nutch.  Now the good news is that all urls contain only 1 slash. But unfortunately, the java.lang.NullPointerException warning/error still exists for both of the parsechecker and indexchecker commands.\nBelow is the running log:\n(1) $ ./nutch parsechecker \"file:/Users/AngelaWang/Documents/programs/oodt/cas-curator/staging/products/xml/\"\nfetching: file:/Users/AngelaWang/Documents/programs/oodt/cas-curator/staging/products/xml/\nparsing: file:/Users/AngelaWang/Documents/programs/oodt/cas-curator/staging/products/xml/\ncontentType: text/html\nsignature: 17bdb44990391c96bb8d48d1802ff11c\nCouldn't pass score, url file:/Users/AngelaWang/Documents/programs/oodt/cas-curator/staging/products/xml/ (java.lang.NullPointerException)\n---------\nUrl\n---------------\nfile:/Users/AngelaWang/Documents/programs/oodt/cas-curator-0.6/staging/products/xml/\n---------\nParseData\n---------\nVersion: 5\nStatus: success(1,0)\nTitle: Index of /Users/AngelaWang/Documents/programs/oodt/cas-curator-0.6/staging/products/xml\nOutlinks: 2\n  outlink: toUrl: file:/Users/AngelaWang/Documents/programs/oodt/cas-curator-0.6/staging/products/ anchor: ../\n  outlink: toUrl: file:/Users/AngelaWang/Documents/programs/oodt/cas-curator-0.6/staging/products/xml/monitor.xml anchor: monitor.xml\nContent Metadata: Content-Length=352 nutch.crawl.score=0.0 Last-Modified=Tue, 14 Oct 2014 20:05:50 GMT Content-Type=text/html \nParse Metadata: CharEncodingForConversion=windows-1252 OriginalCharEncoding=windows-1252 \n(2) $ ./nutch indexchecker \"file:/Users/AngelaWang/Documents/programs/oodt/cas-curator/staging/products/xml/\"\nfetching: file:/Users/AngelaWang/Documents/programs/oodt/cas-curator/staging/products/xml/\nparsing: file:/Users/AngelaWang/Documents/programs/oodt/cas-curator/staging/products/xml/\ncontentType: text/html\nException in thread \"main\" java.lang.NullPointerException\n\tat org.apache.nutch.indexer.IndexingFiltersChecker.run(IndexingFiltersChecker.java:139)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.indexer.IndexingFiltersChecker.main(IndexingFiltersChecker.java:177)",
        "Issue Links": []
    },
    "NUTCH-1885": {
        "Key": "NUTCH-1483 Can't crawl filesystem with protocol-file plugin",
        "Summary": "Protocol-file should treat symbolic links as redirects",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "2.3,                                            1.10",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "31/Oct/14 19:18",
        "Updated": "04/Nov/14 21:58",
        "Resolved": "04/Nov/14 21:16",
        "Description": "(reported by angela_wang, see NUTCH-1884, [1 and [2)\nIf a file is a symbolic link or contains a link on it's path:, protocol-file follows the link immediately and returns a Content object with the canonical path (all symbolic links resolved) in field \"Location\". This may cause\n\nthe Parse object not available under its expected URL (see NUTCH-1884)\ndubious CrawlDatums (status fetched!) in CrawlDb (first URL is a symbolic link to second item):\n\nfile:/var/www/redir_test.html   Version: 7\nStatus: 2 (db_fetched)\n...\nSignature: null\nMetadata: \n        Content-Type=text/html\n        _pst_=success(1), lastModified=0\n\nfile:/var/www/test.html Version: 7\nStatus: 2 (db_fetched)\n...\nSignature: 50fa8436398f0ecb6b15eaba0574ef23\nMetadata: \n        Content-Type=text/html\n        _pst_=success(1), lastModified=0\n\n\nBecause signature is null these will never result in duplicates in index.\n\nProtocol-file should instead explicitly redirect to the link target. This should be the default, optionally we could add a property to restore the old behavior.\nShould not be difficult to resolve: FileResponse already has status \"redirect\" for symlinks, but File.getProtocolOutput() then resolves the links internally. So we just need to return a redirect response before links are resolved/followed.",
        "Issue Links": []
    },
    "NUTCH-1886": {
        "Key": "NUTCH-1886",
        "Summary": "Review and update default.properties",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "01/Nov/14 16:36",
        "Updated": "17/Sep/15 07:03",
        "Resolved": "28/Jan/15 00:27",
        "Description": "Right now default.properties contains all sorts of outdated garbage. We need to review and update where necessary. I don't think that this is a major issue and I doubt the patches will be sigificant.",
        "Issue Links": []
    },
    "NUTCH-1887": {
        "Key": "NUTCH-1887",
        "Summary": "Specify HTMLMapper to use in TikaParser",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "05/Nov/14 15:45",
        "Updated": "07/Nov/14 10:50",
        "Resolved": "07/Nov/14 10:00",
        "Description": "The TikaParser currently relies on the default HTMLMapper used by Tika. The HTMLMapper is used in Tika to filter / normalise the HTML elements passed as SAX events. By default it uses a DefaultHtmlMapper which removes some of the input.\nThis patch allows to specify which HTMLMapper implementation to use (if any).",
        "Issue Links": []
    },
    "NUTCH-1888": {
        "Key": "NUTCH-1888",
        "Summary": "Specify HTMLMapper to use in TikaParser",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "07/Nov/14 09:57",
        "Updated": "09/Jan/15 06:58",
        "Resolved": "15/Dec/14 16:55",
        "Description": "Port of https://issues.apache.org/jira/browse/NUTCH-1887 for 2.x",
        "Issue Links": []
    },
    "NUTCH-1889": {
        "Key": "NUTCH-1889",
        "Summary": "Store all values from Tika metadata in Nutch metadata",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "07/Nov/14 10:25",
        "Updated": "30/Jan/15 08:56",
        "Resolved": "30/Jan/15 08:38",
        "Description": "Tika metadata can be multivalued but we currently keep only the first value in the TikaParser.",
        "Issue Links": []
    },
    "NUTCH-1890": {
        "Key": "NUTCH-1890",
        "Summary": "Major Typo in Documentation for Integrating Nutch and Solr",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "documentation",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Boadu Akoto Charles Jnr",
        "Created": "17/Nov/14 09:16",
        "Updated": "17/Nov/14 23:09",
        "Resolved": "17/Nov/14 21:53",
        "Description": "Problematic Page: https://wiki.apache.org/nutch/NutchTutorial\n1. Duplicated Text\nIn section \"6. Integrate Solr with Nutch\" the following line is asked to be commented from:\n<!--   <filter class=\"solr.\nEnglishPorterFilterFactory\" protected=\"protwords.txt\"/> -->\nto \n<!--   <filter class=\"solr.\nEnglishPorterFilterFactory\" protected=\"protwords.txt\"/> -->\nbut I think it should rather read from:\n<filter class=\"solr.\nEnglishPorterFilterFactory\" protected=\"protwords.txt\"/>\nto\n<!--   <filter class=\"solr.\nEnglishPorterFilterFactory\" protected=\"protwords.txt\"/> -->\n2. Addition of extra step\nAfter going through the recommended steps in Section 6  to integrate with solr, I got an error. The error read 'field text not defined'. This error is so because apparently in my solrconfig.xml, I had defined 'text' as my default field but it was not defined the schema.xml that I imported from the nutch conf folder.\nI propose that either the schema.xml in the nutch conf folder be shipped with the 'text' field already defined or an extra step be added to Section 6 that reads:\nAdd the following line under the definition of 'content' field:\n<field name=\"text\" type=\"text\" stored=\"true\" indexed=\"true\"/>\nor better till steps be added to allow the user to change the default field in solrconfig.xml from 'text' to 'content'  whichever solution seems the most appropriate.",
        "Issue Links": []
    },
    "NUTCH-1891": {
        "Key": "NUTCH-1891",
        "Summary": "Can't run nutch2.3-snapshot on hadoop2.4.0 using gora0.5 and mongodb as backend datastore",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "wilco sheh",
        "Created": "23/Nov/14 03:57",
        "Updated": "25/Nov/14 06:58",
        "Resolved": "25/Nov/14 06:58",
        "Description": "java.lang.Exception: java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected\n\tat org.apache.gora.mapreduce.GoraOutputFormat.getRecordWriter(GoraOutputFormat.java:83)\n\tat org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.<init>(MapTask.java:624)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:744)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:744)",
        "Issue Links": []
    },
    "NUTCH-1892": {
        "Key": "NUTCH-1892",
        "Summary": "Update the FileDumper tool to fetch only those URLs with status db_fetched in nutch",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "nutchNewbie",
        "Assignee": null,
        "Reporter": "Prasanth Iyer",
        "Created": "26/Nov/14 21:47",
        "Updated": "26/Nov/14 21:47",
        "Resolved": null,
        "Description": "The FileDumper tool is a tool that reads the crawled data from Nutch and dumps this data into its raw files. This tool currently dumps every single file irrespective of status, duplicates etc. This cause files that are fetched in error or files that have not been fetched because they were made unavailable by the server to also be dumped. \nThe fix should be to fetch only those files that were fetched with status db_fetched by Nutch.",
        "Issue Links": []
    },
    "NUTCH-1893": {
        "Key": "NUTCH-1893",
        "Summary": "Parse-tika fails to parse feed files",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Angela Wang",
        "Created": "06/Dec/14 07:31",
        "Updated": "17/Sep/15 07:02",
        "Resolved": "27/Jan/15 21:46",
        "Description": "In the Nutch parse step, I received the following error. It seems the parse-tika plugin has broken. \n$ /cygdrive/d/nutch_trunk/runtime/local/bin/nutch parse -D mapred.reduce.tasks=2 -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true -D mapred.skip.attempts.to.start.skipping=2 -D mapred.skip.map.max.skip.records=1 crawlId/segments/20141118235323\njava.lang.ExceptionInInitializerError\n\tat com.sun.syndication.io.SyndFeedInput.build(SyndFeedInput.java:136)\n\tat org.apache.tika.parser.feed.FeedParser.parse(FeedParser.java:70)\n\tat org.apache.nutch.parse.tika.TikaParser.getParse(TikaParser.java:103)\n\tat org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:95)\n\tat org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:101)\n\tat org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:44)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:358)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:307)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)\nCaused by: java.lang.NullPointerException\n\tat java.util.Properties$LineReader.readLine(Properties.java:434)\n\tat java.util.Properties.load0(Properties.java:353)\n\tat java.util.Properties.load(Properties.java:341)\n\tat com.sun.syndication.io.impl.PropertiesLoader.<init>(PropertiesLoader.java:74)\n\tat com.sun.syndication.io.impl.PropertiesLoader.getPropertiesLoader(PropertiesLoader.java:46)\n\tat com.sun.syndication.io.impl.PluginManager.<init>(PluginManager.java:54)\n\tat com.sun.syndication.io.impl.PluginManager.<init>(PluginManager.java:46)\n\tat com.sun.syndication.feed.synd.impl.Converters.<init>(Converters.java:40)\n\tat com.sun.syndication.feed.synd.SyndFeedImpl.<clinit>(SyndFeedImpl.java:59)\n\t... 10 more",
        "Issue Links": [
            "/jira/browse/TIKA-1516",
            "/jira/browse/NUTCH-1494",
            "https://github.com/rometools/rome/issues/130"
        ]
    },
    "NUTCH-1894": {
        "Key": "NUTCH-1894",
        "Summary": "Revert \"Normalize duplicate slashes in URL's\"",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jigal van Hemert",
        "Created": "08/Dec/14 10:30",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "02/Jan/15 14:57",
        "Description": "Duplicate slashes are allowed in URL's according to the RFC's and quite a few websites use them in a meaningful way. These websites have specific information at certain positions in the URL and an empty segment indicates that there is no data for that information.\nRemoving duplicate slashes makes the URL invalid in those cases and the target page can't be fetched for indexing.\nSee issue NUTCH-1011",
        "Issue Links": []
    },
    "NUTCH-1895": {
        "Key": "NUTCH-1895",
        "Summary": "run() method in Crawler.java doesnt put Nutch.ARG_BATCH in argMap",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "crawldb,                                            indexer",
        "Assignee": null,
        "Reporter": "FeiTian",
        "Created": "10/Dec/14 02:33",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "19/Dec/14 23:05",
        "Description": "I am using Nutch 2.2.1 and Solr 4.10.1.\nOS: Win7.\nEnv: MyEclipse 10.\nJAVA: jdk1.7.0_71\nI am using command:\n  urls -depth 3 -topN 10 -solr http://localhost:8080/solr/collection2\nto import data to Solr.\nand using:\n  gora.sqlstore.jdbc.driver=com.mysql.jdbc.Driver\n  gora.sqlstore.jdbc.url=jdbc:mysql://192.168.0.69:3306/nutch?createDatabaseIfNotExist=true&useUnicode=true&characterEncoding=utf8&autoReconnect=true&zeroDateTimeBehavior=convertToNull\n  gora.sqlstore.jdbc.user=root\n  gora.sqlstore.jdbc.password=123456\nto import data to mysql.\nBut I got null pointer exception on batchId, then I found:\nIn SolrIndexerJob.java, we need to get batchId from args:\n  @Override\n  public Map<String,Object> run(Map<String,Object> args) throws Exception \n{\n    String solrUrl = (String)args.get(Nutch.ARG_SOLR);\n    String batchId = (String)args.get(Nutch.ARG_BATCH);\n    NutchIndexWriterFactory.addClassToConf(getConf(), SolrWriter.class);\n    getConf().set(SolrConstants.SERVER_URL, solrUrl);\n\n    currentJob = createIndexJob(getConf(), \"solr-index\", batchId);\n\n    currentJob.waitForCompletion(true);\n    ToolUtil.recordJobStatus(null, currentJob, results);\n    return results;\n  }\n\nBut in Crawler.java, we did not put batchid in argMap:\n @Override\n  public int run(String[] args) throws Exception {\n    if (args.length == 0) \n{\n      System.out.println(\"Usage: Crawler (<seedDir> | -continue) [-solr <solrURL>] [-threads n] [-depth i] [-topN N] [-numTasks N]\");\n      return -1;\n    }\n\n...\n    Map<String,Object> argMap = ToolUtil.toArgMap(\n        Nutch.ARG_THREADS, threads,\n        Nutch.ARG_DEPTH, depth,\n        Nutch.ARG_TOPN, topN,\n        Nutch.ARG_SOLR, solrUrl,\n        Nutch.ARG_SEEDDIR, seedDir,\n        Nutch.ARG_NUMTASKS, numTasks);\n    run(argMap);\n    return 0;\n  }",
        "Issue Links": []
    },
    "NUTCH-1896": {
        "Key": "NUTCH-1896",
        "Summary": "SolrDeleteDuplicates does not use the mapped Solr field names from solrindex-mapping.xml",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Brian",
        "Created": "10/Dec/14 16:28",
        "Updated": "10/Dec/14 16:32",
        "Resolved": null,
        "Description": "SolrDeleteDuplicates uses the hard-coded field names specified in SolrConstants.java to get all the fields (id, content, etc.) from Solr when deleting duplicates.\nHowever this ignores the mappings specified in solrindex-mapping.xml - these fields may have been mapped to other fields at index time.\nE.g.:\nAt index time, \"id\" is mapped to \"asset_id\"\nAt dedup time - \"id\" is used to get the field from Solr - error - no such field exists in Solr.\nSolrDeleteDuplicates should use the same mappings defined for indexing, otherwise it can't be used for any setup renaming the internal nutch fields used in deduplication.  \nThe way I fixed it was to instantiate the SolrMappingReader during initialization and store the mapped field names in the hadoop configuration, e.g.:\n\n  public void setSolrFieldMappings() throws IOException{\n    SolrMappingReader solrMapping = SolrMappingReader.getInstance(getConf());\n\n\tgetConf().set(SolrConstants.ID_FIELD, \n\t\t          solrMapping.mapKey(SolrConstants.ID_FIELD_DEFAULT));\n\tgetConf().set(SolrConstants.BOOST_FIELD, \n\t\t          solrMapping.mapKey(SolrConstants.BOOST_FIELD_DEFAULT));\n\tgetConf().set(SolrConstants.TIMESTAMP_FIELD, \n\t\t          solrMapping.mapKey(SolrConstants.TIMESTAMP_FIELD_DEFAULT));\n\tgetConf().set(SolrConstants.TITLE_FIELD, \n\t\t          solrMapping.mapKey(SolrConstants.TITLE_FIELD_DEFAULT));\n\tgetConf().set(SolrConstants.CONTENT_FIELD, \n\t\t          solrMapping.mapKey(SolrConstants.CONTENT_FIELD_DEFAULT));\n\n  }\n\n\nCalled in dedup method:\n\n  public boolean dedup(String solrUrl)\n  throws IOException, InterruptedException, ClassNotFoundException {\n    LOG.info(\"SolrDeleteDuplicates: starting...\");\n    LOG.info(\"SolrDeleteDuplicates: Solr url: \" + solrUrl);\n    \n    getConf().set(SolrConstants.SERVER_URL, solrUrl);\n    \t\n\tsetSolrFieldMappings();\n    \n    Job job = new Job(getConf(), \"solrdedup\");\n\n    job.setInputFormatClass(SolrInputFormat.class);\n    job.setOutputFormatClass(NullOutputFormat.class);\n    job.setMapOutputKeyClass(Text.class);\n    job.setMapOutputValueClass(SolrRecord.class);\n    job.setMapperClass(Mapper.class);\n    job.setReducerClass(SolrDeleteDuplicates.class);\n\n    return job.waitForCompletion(true);    \n  }\n\n\nThen the fields are accessed elsewhere through the conf, e.g.:\n\n    public RecordReader<Text, SolrRecord> createRecordReader(InputSplit split,\n        TaskAttemptContext context) throws IOException, InterruptedException {\n      Configuration conf = context.getConfiguration();\n      SolrServer solr = SolrUtils.getCommonsHttpSolrServer(conf);\n      SolrInputSplit solrSplit = (SolrInputSplit) split;\n      final int numDocs = (int) solrSplit.getLength();\n      \n      SolrQuery solrQuery = new SolrQuery( SOLR_GET_ALL_QUERY );\n      solrQuery.setFields( conf.get(SolrConstants.ID_FIELD), \n                           conf.get(SolrConstants.BOOST_FIELD),\n                           conf.get(SolrConstants.TIMESTAMP_FIELD),\n                           conf.get(SolrConstants.TITLE_FIELD),\n                           conf.get(SolrConstants.CONTENT_FIELD));\n      solrQuery.setStart(solrSplit.getDocBegin());\n      solrQuery.setRows(numDocs);\n\n      QueryResponse response;\n      try {\n        response = solr.query(solrQuery);\n      } catch (final SolrServerException e) {\n        throw new IOException(e);\n      }\n\n      final SolrDocumentList solrDocs = response.getResults();\n      return new SolrRecordReader(solrDocs, numDocs);\n    }\n  }",
        "Issue Links": []
    },
    "NUTCH-1897": {
        "Key": "NUTCH-1897",
        "Summary": "Easier debugging of plugin XML errors",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Dec/14 14:31",
        "Updated": "12/Dec/14 10:50",
        "Resolved": "12/Dec/14 10:29",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1898": {
        "Key": "NUTCH-1898",
        "Summary": "Add -dumpRawHTML prameter to parsechecker tool",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Dec/14 18:39",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The ability to obtain raw HTML alongside all of the other parse data we get within existing parsechecker would compliment the tool.\nThis issue should merely append the raw HTML markup to the existing output. It should be an optional parameter, same as -dumpText",
        "Issue Links": []
    },
    "NUTCH-1899": {
        "Key": "NUTCH-1899",
        "Summary": "upgrade restlet lib to prevent build failure",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "build",
        "Assignee": "Talat Uyarer",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Dec/14 21:10",
        "Updated": "09/Jan/15 03:42",
        "Resolved": "09/Jan/15 03:42",
        "Description": "If org.restlet.jse libs are not contained in the local ivy cache, the build may fail in step ivy:resolve [1, [2.",
        "Issue Links": []
    },
    "NUTCH-1900": {
        "Key": "NUTCH-1900",
        "Summary": "DockerFile for Nutch 2.x",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.1",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Talat Uyarer",
        "Created": "16/Dec/14 08:29",
        "Updated": "21/May/15 17:34",
        "Resolved": "21/May/15 17:34",
        "Description": "We can create Dockerfile can be used to build a Docker image running the latest Nutch 2.x branch in local mode for easy starts newbies. Dockerfile can do this by setting up necessary dependencies and settings, checking out the 2.x branch of Nutch from GitHub etc, and then building Nutch. .",
        "Issue Links": []
    },
    "NUTCH-1901": {
        "Key": "NUTCH-1901",
        "Summary": "ability to tag Urls and index those tags using Solr",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Krishna Chaitanya",
        "Created": "16/Dec/14 12:23",
        "Updated": "16/Dec/14 13:10",
        "Resolved": "16/Dec/14 13:10",
        "Description": "Requirement is that we have few urls which are tagged. We need to crawl those urls and index them in solr along with those tags. I donot know whether this can be achieved with the current functionality but if we can achieve this with current functionality reply to this issue orelse it will be really helpful to give a patch or include this in future versions",
        "Issue Links": []
    },
    "NUTCH-1902": {
        "Key": "NUTCH-1902",
        "Summary": "Missing nekohtml.jar",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Cao Manh Dat",
        "Created": "18/Dec/14 10:05",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "07/Jan/15 21:44",
        "Description": "There are missing nekohtml in lib folder of nutch release. So Nutch will throw an exception when parse row.\nFix this issue by adding nekohtml.jar (can be download here at http://nekohtml.sourceforge.net/) and place in lib folder.",
        "Issue Links": []
    },
    "NUTCH-1903": {
        "Key": "NUTCH-1903",
        "Summary": "Resolve-default failed with branch 2.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Cao Manh Dat",
        "Created": "19/Dec/14 04:19",
        "Updated": "27/Dec/14 01:37",
        "Resolved": "27/Dec/14 01:37",
        "Description": "I meet this problem with lastest code at branch 2.x. \n/Volumes/Data/workspace/opensource/apache nutch/nutch-git/build.xml:468: impossible to ivy retrieve: java.lang.RuntimeException: problem during retrieve of org.apache.nutch#nutch: java.lang.RuntimeException: Multiple artifacts of the module org.apache.avro#avro-ipc;1.7.6 are retrieved to the same file! Update the retrieve pattern to fix this error.",
        "Issue Links": []
    },
    "NUTCH-1904": {
        "Key": "NUTCH-1904",
        "Summary": "Schema for Solr4 doesn't include _version_ field",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "04/Jan/15 20:35",
        "Updated": "04/Jan/15 20:50",
        "Resolved": "04/Jan/15 20:36",
        "Description": "The schema-solr4.xml doesn't include the version field. I fixed this for the schema.xml just forgot to add it for solr4.",
        "Issue Links": []
    },
    "NUTCH-1905": {
        "Key": "NUTCH-1905",
        "Summary": "Nutch index tool should be resilient to segments that don't have crawl_* data",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "04/Jan/15 20:46",
        "Updated": "10/Apr/15 23:33",
        "Resolved": "10/Apr/15 23:33",
        "Description": "When running the ./bin/nutch index command with the -dir <path/to/segment/dir> I noticed that if you have a segment directory that doesn't include crawl_* or parse_* data, that the indexer fails (correctly). However, the indexer should be more resilient in those cases - we can add a simple check to see if those dirs are present in the segment, and proceed if they are, otherwise, ignore that segment and print a message (and go to the other segments).",
        "Issue Links": [
            "/jira/browse/NUTCH-1771"
        ]
    },
    "NUTCH-1906": {
        "Key": "NUTCH-1906",
        "Summary": "Typo in CrawlDbReader command line help",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "crawldb",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Jan/15 15:38",
        "Updated": "17/Apr/15 18:52",
        "Resolved": "17/Apr/15 18:25",
        "Description": "Currently the CrawlDbReader tool, when invoked without any command line arguments helps us as follows\n\n[mdeploy@crawl local]$ ./bin/nutch readdb\nUsage: CrawlDbReader <crawldb> (-stats | -dump <out_dir> | -topN <nnnn> <out_dir> [<min>] | -url <url>)\n\t<crawldb>\tdirectory name where crawldb is located\n\t-stats [-sort] \tprint overall statistics to System.out\n\t\t[-sort]\tlist status sorted by host\n\t-dump <out_dir> [-format normal|csv|crawldb]\tdump the whole db to a text file in <out_dir>\n\t\t[-format csv]\tdump in Csv format\n\t\t[-format normal]\tdump in standard format (default option)\n\t\t[-format crawldb]\tdump as CrawlDB\n\t\t[-regex <expr>]\tfilter records with expression\n\t\t[-retry <num>]\tminimum retry count\n\t\t[-status <status>]\tfilter records by CrawlDatum status\n\t-url <url>\tprint information on <url> to System.out\n\t-topN <nnnn> <out_dir> [<min>]\tdump top <nnnn> urls sorted by score to <out_dir>\n\t\t[<min>]\tskip records with scores below this value.\n\t\t\tThis can significantly improve performance.\n\n\nThe code that bothers me is\n\n\t-stats [-sort] \tprint overall statistics to System.out\n\t\t[-sort]\tlist status sorted by host\n\n\nThe inclusion of the double -sort is not necessary or required.\nHaving looked through the code there is no other optional flag which we can substitute for the second one (which I thought may lead to this being a placeholder for something else) therefore we can just remove it.",
        "Issue Links": []
    },
    "NUTCH-1907": {
        "Key": "NUTCH-1907",
        "Summary": "Incorrect output of Outlinks to Hosts within HostDbUpdateReducer",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1",
        "Fix Version/s": "2.3",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Jan/15 16:44",
        "Updated": "09/Jan/15 06:41",
        "Resolved": "09/Jan/15 06:14",
        "Description": "I explained that I found a big in the 2.X HostDb.\nI was looking into the code within Nutch 2.X HostDbUpdateReducer and\n'think' I've discovered a bug in the way we output Host data.\nhttps://github.com/apache/nutch/blob/2.x/src/java/org/apache/nutch/host/HostDbUpdateReducer.java#L87\nI feel that the following code\n\nhost.getInlinks().put(new Utf8(outlink), new\nUtf8(Integer.toString(outlinkCount.getCount(outlink))));\n\n\nshould be changed to the following\n\nhost.getOutlinks().put(new Utf8(outlink), new\nUtf8(Integer.toString(outlinkCount.getCount(outlink))));\n\n\nNotice the difference in population of Outlinks to Host instead of repeated Inlinks.",
        "Issue Links": []
    },
    "NUTCH-1908": {
        "Key": "NUTCH-1908",
        "Summary": "HTMLMetaProcessor should be able to recognise and retrieve metatags from <body>",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Jan/15 06:47",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "I am regularly experiencing HTML authors who permit and publish <meta> tags within the <body> of an (X)HTML document.\nRight now, it would appear that the Nutch policy is to ignore such markup.\nEvidence of this exists within HTMLMetaProcessor (0) as follows\n\n private static final void getMetaTagsHelper(HTMLMetaTags metaTags, Node node, URL currURL) {\n    if (node.getNodeType() == Node.ELEMENT_NODE) {\n    if (\"body\".equalsIgnoreCase(node.getNodeName())) {\n      // META tags should not be under body\n      return;\n    }\n...\n\n\nIn a utopian WWW it would be OK to make the statement that 'META tags should not be under body', however I am afraid that this is not always the case. It is not a utopian WWW. An improvement in Nutch would therefore be for us to recognize that HTML authors, or machines, do put <meta> tags into the <body>.\nOver in Any23 and in crawler commons, we have taken the approach that, 'letting people off' with having sh*tty markup is OK. I think in this case, this also makes sense in Nutch.\nI will implement a patch which permits explicit extraction of <meta> tags from <body> as well as <head>\n(0) https://github.com/apache/nutch/blob/trunk/src/plugin/parse-tika/src/java/org/apache/nutch/parse/tika/HTMLMetaProcessor.java#L56",
        "Issue Links": []
    },
    "NUTCH-1909": {
        "Key": "NUTCH-1909",
        "Summary": "remove testresources/testcrawl from 2.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "test",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jan/15 17:24",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "With NUTCH-1844 src/testresources/testcrawl have been removed from Nutch 1.x. They are still included in 2.x although definitely useless: data structures (crawldb, linkdb, segments) cannot be read by 2.x.",
        "Issue Links": []
    },
    "NUTCH-1910": {
        "Key": "NUTCH-1910",
        "Summary": "Port NUTCH-1660 index-geoip to 2.X",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Jan/15 23:27",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "NUTCH-1660 implements a pretty comprehensive, if rather complex to configure, mechanism for reverse geocoding IP addresses associated with servers from which we obtain Web pages.\nIt would be nice to port this to 2.X",
        "Issue Links": []
    },
    "NUTCH-1911": {
        "Key": "NUTCH-1911",
        "Summary": "Improve DomainStatistics tool command line parsing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            2.2.1",
        "Fix Version/s": "1.10,                                            1.11",
        "Component/s": "util",
        "Assignee": "Michael Joyce",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Jan/15 16:47",
        "Updated": "11/Nov/15 17:55",
        "Resolved": "11/Nov/15 16:54",
        "Description": "The DomainStatistic's tool could be improved based on the comments addressed in this mai thread\nFor convenience, I've also pasted them below\n\nYou cannot just tell it where the crawldb is, you need to tell it where the directory is, so specifying current is ok, but not part-*\nPatch should be trivial work",
        "Issue Links": []
    },
    "NUTCH-1912": {
        "Key": "NUTCH-1912",
        "Summary": "Dump tool -mimetype parameter needs to be optional to prevent NPE",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "tool",
        "Assignee": "Tyler Bui-Palsulich",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Jan/15 16:52",
        "Updated": "14/Jan/15 00:07",
        "Resolved": "13/Jan/15 19:47",
        "Description": "There is a bug in NUTCH-1869 which, although the new feature works correctly, can result in a NPE if the new flag is not provided. This is hellish.\nWe can possibly add an 'all' option which would dump everything.\nThe purpose of NUTCH-1869 was so that you can have fine grained control over what content you wish to dump.",
        "Issue Links": [
            "/jira/browse/NUTCH-1914"
        ]
    },
    "NUTCH-1913": {
        "Key": "NUTCH-1913",
        "Summary": "LinkDB to implement db.ignore.external.links",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.11",
        "Component/s": "linkdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Jan/15 14:59",
        "Updated": "12/Feb/15 09:26",
        "Resolved": "12/Feb/15 08:47",
        "Description": "LinkDB needs an option to ignore external links.",
        "Issue Links": [
            "/jira/browse/NUTCH-1932"
        ]
    },
    "NUTCH-1914": {
        "Key": "NUTCH-1914",
        "Summary": "Improve error messages of segment dumper tool",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tyler Bui-Palsulich",
        "Created": "12/Jan/15 17:21",
        "Updated": "13/Jan/15 19:33",
        "Resolved": "13/Jan/15 19:32",
        "Description": "The segment dumper from NUTCH-1526 is useful. But, if you enter an invalid option (e.g. ./nutch dump -invalid), nothing is printed.",
        "Issue Links": [
            "/jira/browse/NUTCH-1912"
        ]
    },
    "NUTCH-1915": {
        "Key": "NUTCH-1915",
        "Summary": "Error in Nutch 2.X WebApp stalls progress bar",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Jan/15 20:08",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "When I define a crawl within the Nutch 2.X webapp on the above stack I sometimes get the following stack trace\n\n2015-01-12 14:48:25,943 INFO  fetcher.FetcherJob - fetching http://www.darpa.mil/Our_Work/I2O/Personnel/Mr__Steve_Jameson.aspx (queue crawl delay=5000ms)\n2015-01-12 14:48:26,563 ERROR impl.RemoteCommandExecutor - Remote command failed\njava.util.concurrent.TimeoutException\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:201)\n\tat org.apache.nutch.webui.client.impl.RemoteCommandExecutor.executeRemoteJob(RemoteCommandExecutor.java:61)\n\tat org.apache.nutch.webui.client.impl.CrawlingCycle.executeCrawlCycle(CrawlingCycle.java:58)\n\tat org.apache.nutch.webui.service.impl.CrawlServiceImpl.startCrawl(CrawlServiceImpl.java:69)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\n\tat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\n\tat org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:97)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n2015-01-12 14:48:26,563 INFO  impl.CrawlingCycle - Executed remote command data: FETCH status: FAILED\n2015-01-12 14:48:27,088 INFO  fetcher.FetcherJob - 10/10 spinwaiting/active, 71 pages, 1 errors, 1.4 1 pages/s, 275 146 kb/s, 193 URLs in 4 queues\n\n\nRight now I don't know what this relates to but I know that it stalls the task execution progress bar within the CrawlsPage",
        "Issue Links": []
    },
    "NUTCH-1916": {
        "Key": "NUTCH-1916",
        "Summary": "Apache Nutch CXF-based REST services",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "crawldb,                                            fetcher,                                            indexer,                                            linkdb,                                            metadata",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "14/Jan/15 14:20",
        "Updated": "06/Feb/15 18:40",
        "Resolved": "06/Feb/15 18:40",
        "Description": "Instead of focusing on porting directly the Nutch 2.x RESTlet Services, I am going to focus on creating an Apache CXF based set of Nutch services. I have written CXF before and am familiar with it and I think it represents a good opportunity to eat our own Apache dogfood.\nI'll begin by creating and spec'ing out the endPoints based on what we have in Nutch 2.x and then building on top of that to expose more data, information and control to the users. \nThis will be the master issue to track work here.",
        "Issue Links": [
            "/jira/browse/NUTCH-1931",
            "/jira/browse/NUTCH-1931"
        ]
    },
    "NUTCH-1917": {
        "Key": "NUTCH-1917",
        "Summary": "index.parse.md, index.content.md and index.db.md should support wildcard",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.20",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Jan/15 21:06",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Right now metatags.names supports the '*' character for a catch all.\nI believe that the above index properties should also support catch all as a mechanism for quickly building augmented data models from crawl data. Individual identification and manual inclusion of tags one by one is error prone and time consuming.",
        "Issue Links": []
    },
    "NUTCH-1918": {
        "Key": "NUTCH-1918",
        "Summary": "TikaParser specifies a default namespace when generating DOM",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "15/Jan/15 11:01",
        "Updated": "30/Jan/15 09:50",
        "Resolved": "30/Jan/15 09:06",
        "Description": "The DOM generated by parse-tika differs from the one done by parse-html. Ideally we should be able to use either parsers with the same XPath expressions.\nThis is related to NUTCH-1592, but this time instead of being a matter of uppercases, the problem comes from the namespace used. \nThis issue has been investigated and fixed in storm-crawler https://github.com/DigitalPebble/storm-crawler/pull/58.\nHere is what Guillaume explained there :\nWhen parsing the content, Tika creates a properly formatted XHTML document: all elements are created within the namespace XHTML.\nHowever in XPath 1.0, there's no concept of default namespace so XPath expressions such as //BODY doesn't match anything. To make this work we should use //ns1:BODY and define a NamespaceContext which associates ns1 with \"http://www.w3.org/1999/xhtml\"\nTo keep the XPathExpressions simpler, I modified the DOMBuilder which is our SaxHandler used to convert the SAX Events into a DOM tree to ignore a \"default name space\" and the ParserBolt initializes it with the XHTML namespace. This way //BODY matches.",
        "Issue Links": []
    },
    "NUTCH-1919": {
        "Key": "NUTCH-1919",
        "Summary": "Getting timeout when server returns Content-Length: 0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "15/Jan/15 11:24",
        "Updated": "16/Jan/15 11:52",
        "Resolved": "16/Jan/15 11:31",
        "Description": "This has been investigated in fixed in the Storm-Crawler https://github.com/DigitalPebble/storm-crawler/issues/48.\n\ncurl -I \"http://www.dailynewslosangeles.com/\"\nHTTP/1.1 301 Moved Permanently\nLocation: http://www.dailynews.com\nConnection: close\nContent-Length: 0\nContent-Type: text/html; charset=UTF-8\nwhen fetching with Nutch we are getting a timeout exception :\n\n./nutch parsechecker -D http.agent.name=\"PebbleCrawler\" \"http://www.dailynewslosangeles.com/\"\nfetching: http://www.dailynewslosangeles.com/\nFetch failed with protocol status: exception(16), lastModified=0: java.net.SocketTimeoutException: Read timed out\nThe reason for this is that we are trying to read from the stream even though we know that the content length is 0.\nThe patch attached fixes the issue.",
        "Issue Links": []
    },
    "NUTCH-1920": {
        "Key": "NUTCH-1920",
        "Summary": "Upgrade Nutch to use Java 1.7",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jan/15 19:10",
        "Updated": "17/Sep/15 07:02",
        "Resolved": "28/Jan/15 00:26",
        "Description": "In order to build the Nutch Javadoc securely, we rely upon no less than Java version 7u25 or greater. See NUTCH-1590.\nindexer-elastic also requires a JDK 1.7 in order compile.\nWe should make the upgrade and state support for Java 1.7 based on the following announcement from Oracle\n\nEnd of Public Updates for Oracle JDK 7\n\nThe April 2015 CPU release will be the last Oracle JDK 7 publicly available update. For more information, and details on how to receive longer term support for Oracle JDK 7, please see the Oracle Java SE Support Roadmap.",
        "Issue Links": []
    },
    "NUTCH-1921": {
        "Key": "NUTCH-1921",
        "Summary": "Optionally disable HTTP if-modified-since header",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Jan/15 12:21",
        "Updated": "03/Mar/15 13:17",
        "Resolved": "03/Mar/15 13:17",
        "Description": "Records with fetch_not_modified are not parsed and are not passed through parse filters, index filters and are not being indexed. This is a huge problem if you modified parser filter, indexing filter or whatever behaviour in the pipe line because changes never show up in the index.",
        "Issue Links": [
            "/jira/browse/NUTCH-1932"
        ]
    },
    "NUTCH-1922": {
        "Key": "NUTCH-1922",
        "Summary": "DbUpdater overwrites fetch status for URLs from previous batches, causes repeated re-fetches",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3.1",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerhard Gossen",
        "Created": "26/Jan/15 11:02",
        "Updated": "16/Sep/15 04:27",
        "Resolved": "16/Sep/15 04:27",
        "Description": "When Nutch 2 finds a link to a URL that was crawled in a previous batch, it resets the fetch status of that URL to unfetched. This makes this URL available for a re-fetch, even if its crawl interval is not yet over.\nTo reproduce, using version 2.3:\n\n# Nutch configuration\nant runtime\ncd runtime/local\nmkdir seeds\necho http://www.l3s.de/~gossen/nutch/a.html > seeds/1.txt\nbin/crawl seeds test 2\n\n\nThis uses two files a.html and b.html that link to each other.\nIn batch 1, Nutch downloads a.html and discovers the URL of b.html. In batch 2, Nutch downloads b.html and discovers the link to a.html. This should update the score and link fields of a.html, but not the fetch status. However, when I run bin/nutch readdb -crawlId test -url http://www.l3s.de/~gossen/nutch/a.html | grep -a status, it returns status: 1 (status_unfetched).\nExpected would be status: 2 (status_fetched).\nThe reason seems to be that DbUpdateReducer assumes that links to a URL not processed in the same batch always belong to new pages. Before NUTCH-1556, all pages in the crawl DB were processed by the DBUpdate job, but that change skipped all pages with a different batch ID, so I assume that this introduced this behavior.",
        "Issue Links": [
            "/jira/browse/NUTCH-1679"
        ]
    },
    "NUTCH-1923": {
        "Key": "NUTCH-1900 DockerFile for Nutch 2.x",
        "Summary": "Nutch + Cassandra Docker",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.1",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Jan/15 20:38",
        "Updated": "21/May/15 17:41",
        "Resolved": "21/May/15 17:33",
        "Description": "Apache Nutch With Cassandra With Elasticsearch and Hadoop on Docker\nhttps://github.com/Meabed/nutch-cassandra-docker",
        "Issue Links": []
    },
    "NUTCH-1924": {
        "Key": "NUTCH-1900 DockerFile for Nutch 2.x",
        "Summary": "Nutch + HBase Docker",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Pending Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.1",
        "Component/s": "build",
        "Assignee": "Rados\u0142aw Stankiewicz",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Jan/15 20:38",
        "Updated": "14/May/15 22:57",
        "Resolved": "14/May/15 22:57",
        "Description": "ZooKeeper 3.4.5 Hadoop 0.20.204 HBase 0.90.4 Nutch 2.2.1\nhttps://registry.hub.docker.com/u/stankiewicz/hbase_hadoop_nutch/dockerfile/",
        "Issue Links": []
    },
    "NUTCH-1925": {
        "Key": "NUTCH-1925",
        "Summary": "Upgrade Tika to version 1.7",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "build",
        "Assignee": "Markus Jelsma",
        "Reporter": "Tyler Bui-Palsulich",
        "Created": "28/Jan/15 16:14",
        "Updated": "26/Feb/15 03:49",
        "Resolved": "22/Feb/15 19:54",
        "Description": "Hi Folks. Nutch currently uses version 1.6 of Tika. There were no significant API changes between 1.6 and 1.7. So, this should be a one line update.",
        "Issue Links": []
    },
    "NUTCH-1926": {
        "Key": "NUTCH-1926",
        "Summary": "AdaptiveFetchScheduler reads nutch-default settings as float but it needs integer.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher",
        "Assignee": "Talat Uyarer",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Jan/15 16:36",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "This issue is document as follows\n\n> - AdaptiveFetchSchedular do not work.  In default settings float, it needs \n> integer.\nConfirmed, in nutch-default.xml these two properties are defined as floats\nbut read as integers. Configuration.getInt(name) then returns the default value.\n\n\n  <name>db.fetch.schedule.adaptive.min_interval</name>\n  <value>60.0</value>\n\n  <name>db.fetch.schedule.adaptive.max_interval</name>\n  <value>31536000.0</value>\n\nIs there already a Jira open? The problem applies to 1.x and 2.x.\nBut it's hardly a blocker: the default values are also 60 and 31536000.",
        "Issue Links": []
    },
    "NUTCH-1927": {
        "Key": "NUTCH-1927",
        "Summary": "Create a whitelist of IPs/hostnames to allow skipping of RobotRules parsing",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "29/Jan/15 20:55",
        "Updated": "11/May/15 18:43",
        "Resolved": "18/Apr/15 16:35",
        "Description": "Based on discussion on the dev list, to use Nutch for some security research valid use cases (DDoS; DNS and other testing), I am going to create a patch that allows a whitelist:\n\n<property>\n  <name>robot.rules.whitelist</name>\n  <value>132.54.99.22,hostname.apache.org,foo.jpl.nasa.gov</value>\n  <description>Comma separated list of hostnames or IP addresses to ignore robot rules parsing for.\n  </description>\n</property>",
        "Issue Links": [
            "/jira/browse/NUTCH-1992"
        ]
    },
    "NUTCH-1928": {
        "Key": "NUTCH-1928",
        "Summary": "Indexing filter of documents by the MIME type",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "30/Jan/15 22:14",
        "Updated": "23/Feb/15 13:56",
        "Resolved": "23/Feb/15 13:55",
        "Description": "This allows to filter the indexed documents by the MIME type property of the crawled content. Basically this will allow you to restrict the MIME type of the contents that will be stored in Solr/Elasticsearch index without the need to restrict the crawling/parsing process, so no need to use URLFilter plugin family. Also this address one particular corner case when certain URLs doesn't have any format to filter such as some RSS feeds (http://www.awesomesite.com/feed) and it will end in your index mixed with all your HTML content.\nA configuration can file specified on the mimetype.filter.file property in the nutch-site.xml. This file use the same format as the urlfilter-suffix plugin. If no mimetype.filter.file key is found an allow all policy is used instead, so all your crawled documents will be indexed.",
        "Issue Links": []
    },
    "NUTCH-1929": {
        "Key": "NUTCH-1929",
        "Summary": "Consider implementing dependency injection for crawl HTTPS sites that use self signed certificates",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "31/Jan/15 00:45",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "It was mentioned a while ago that \"to be able to crawl sites with a self signed certificate required a simple code modification the protocol-httpclient plugin.\"\n\nin org.apache.nutch.protocol.httpclient.Http\n\nReplace:\n\nProtocolSocketFactory factory = new SSLProtocolSocketFactory();\n\nWith:\n\nProtocolSocketFactory factory = new DummySSLProtocolSocketFactory();\n\n\nI can confirm that this patch actually fixes the issue, however the thread hangs on a question which was never answered.\n\"Is there dependency injection that can be used?\"\nThis issue needs to investigate the required logic which we can implement to make the decision at runtime.",
        "Issue Links": [
            "/jira/browse/NUTCH-827"
        ]
    },
    "NUTCH-1930": {
        "Key": "NUTCH-1930",
        "Summary": "Fetcher erases Markers for certain URLs / documents",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Michiel",
        "Created": "02/Feb/15 15:45",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "During an active crawling project, I noticed what appears to be a bug in the fetcher: the markers for certain pages (PDFs especially) are either not saved, or erased altogether. The pages are thus not parsed, nor updated in the DB. They keep appearing in the generate lists and fetch lists. Note that this is a separate issue from NUTCH-1922. That one involves correctly parsed pages. This bug prevents certain pages from getting correct markers set.\nAlthough I'm still new to Nutch and no java expert, I'm currently trying to debug this. Because it seems to be rather easy to replicate the error, so it seemed sensible to share my findings so far. If I find out more myself, I'll update this issue.\nFor this test, I injected two test URLs which never seemed to get parsed, even though they are valid documents which are not excluded by any filters. I use a http.content.limit of 64 MB, and tika is used for parsing documents. Note that these are just two examples, I can provide more if needed.\n\nhttp://www.aanvalopschooluitval.nl/userfiles/file/projectenbank/Flex%20Lectoraat.pdf\nhttp://www.prettywoman-utrecht.nl/wp-content/uploads/PrettyWoman-methodiek_web.pdf\n\nSteps:\n1) Whenever a batch gets generated, the GENERATE_MARK is set. So far so good.\n2) During fetch, map() inside FetcherJob checks if this GENERATE_MARK is set. If so, it continues. Still, so far so good.\n3) After fetch, output() inside FetcherReducer sets the FETCH_MARK. I've logged the marker, and it gets set with the correct batchId. It gets a value.\n4) However, when another nutch command is run, all the markers from these example URLs appear to have been erased. Not only is FETCH_MARK suddenly not set, GENERATE_MARK is also erased. Thus, the parser will think the URL hasn't been fetched yet. The fetchStatus, however, is nicely set to \"2 (status_fetched)\". It's just the markers that are not correctly set.\nMy first assumption was that FETCH_MARK was not saved. However, as noted in step 3), it gets the correct value. Also, GENERATE_MARK is erased after the process is complete, so something else goes wrong. Somewhere before the end of FetcherJob, the markers for certain pages are erased. Note that all other values, like content, baseUrl, fetchtimes and fetchStatus, are saved correctly for these URLs.\nFinally, for testing purposes, here is an example URL that DOES work: http://www.aanvalopschooluitval.nl/userfiles/file/2011/Plusvoorzieningenkrant.pdf",
        "Issue Links": []
    },
    "NUTCH-1931": {
        "Key": "NUTCH-1931",
        "Summary": "Apache Nutch 1.x REST service and crawler visualization",
        "Type": "Improvement",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "03/Feb/15 04:57",
        "Updated": "27/Oct/15 20:49",
        "Resolved": null,
        "Description": "Currently Nutch 1.x does not have a REST API like 2.x. The REST services would help provide more information about a crawl to the user. \nAlong with Dr. Chris Mattmann, I would be working on writing the CXF based services and using them to create D3 based visualizations. These visualizations will help give insights on what is happening during a Nutch crawl.",
        "Issue Links": [
            "/jira/browse/NUTCH-1916",
            "/jira/browse/NUTCH-1966",
            "/jira/browse/NUTCH-2151",
            "/jira/browse/NUTCH-1916"
        ]
    },
    "NUTCH-1932": {
        "Key": "NUTCH-1932",
        "Summary": "Automatically remove orphaned pages",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "04/Feb/15 16:34",
        "Updated": "25/Oct/17 20:32",
        "Resolved": "25/Oct/17 14:51",
        "Description": "Orphan scoring filter that determines whether a page has become orphaned, e.g. it has no more other pages linking to it. If a page hasn't been linked to after markGoneAfter seconds, the page is marked as gone and is then removed by an indexer.  If a page hasn't been linked to after markOrphanAfter seconds, the page is removed from the CrawlDB.\nNote: if you have this plugin enabled you MUST make sure you visit 'almost' every URL at least once within the refetch interval. If you don't, non-orphans may be marked as orphan and get deleted! You can use NUTCH-2368 to make sure this is the case.",
        "Issue Links": [
            "/jira/browse/NUTCH-1921",
            "/jira/browse/NUTCH-1913",
            "https://github.com/apache/nutch/pull/211"
        ]
    },
    "NUTCH-1933": {
        "Key": "NUTCH-1933",
        "Summary": "nutch-selenium plugin",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "protocol",
        "Assignee": "Mohammad Al-Mohsin",
        "Reporter": "Mo Omer",
        "Created": "04/Feb/15 18:37",
        "Updated": "06/Apr/15 16:56",
        "Resolved": "26/Feb/15 18:31",
        "Description": "I updated the plugin nutch-selenium plugin to run against trunk.\nI feel that there is a good bit of work to be done here however early testing on my system are that it works.",
        "Issue Links": [
            "/jira/browse/NUTCH-1948"
        ]
    },
    "NUTCH-1934": {
        "Key": "NUTCH-1934",
        "Summary": "Refactor Fetcher in trunk",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "04/Feb/15 20:34",
        "Updated": "08/May/15 04:36",
        "Resolved": "08/May/15 04:36",
        "Description": "Put simply Fetcher is too big.\nThis is kinda strange as the size of this file is unique (I think) from every other class within Nutch. The others are reasonably well modularized and split into constituent classes which make sense.",
        "Issue Links": []
    },
    "NUTCH-1935": {
        "Key": "NUTCH-1935",
        "Summary": "too many open files",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.2",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jefferyyuan",
        "Created": "04/Feb/15 21:56",
        "Updated": "19/Feb/15 19:34",
        "Resolved": "19/Feb/15 19:34",
        "Description": "This is to track the \" too many open files\" issue from http://lucene.472066.n3.nabble.com/Cannot-run-program-quot-chmod-quot-too-many-open-files-td4109753.html\nRecently I also hit this too many open files issues, after run our crawler application for about 2-5 months, it will fail \" too many open files\" errors.\nWe have to restart nutch server and hbase.\nNutch Server opens 4249 files\nlsof -p 17849 | wc -l\n4249\nThere are a lot of pipe, sock, and connections to hbase.\ncat sorted | grep pipe | wc -l\n1624\ncat sorted | grep 2181 | wc -l \n813\n907 java    17849 user 1465u  sock                0,7      0t0 65043894 can't identify protocol\n1843 java    17849 user 2744u  IPv6           76971706      0t0      TCP localhost:35073->localhost:2181 (ESTABLISHED) // connect to hbase\ncat sorted | grep anon_inode | wc -l\n812\nThanks...",
        "Issue Links": []
    },
    "NUTCH-1936": {
        "Key": "NUTCH-1936",
        "Summary": "GSoC 2015 - Move Nutch to Hadoop 2.X",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11,                                            2.3.1",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "05/Feb/15 17:53",
        "Updated": "17/Sep/15 07:05",
        "Resolved": "17/Sep/15 07:05",
        "Description": "The Nutch PMC discussed ideas for a good 2015 GSoC project. It appears that porting the (trunk) codebase to Hadoop 2.X seems to an attractive option and one which would present an excellent learning experience for a summer student.\nA more comprehensive description of this issue should be included within either a mentor-defined project description or a successful student application.",
        "Issue Links": [
            "/jira/browse/NUTCH-2049"
        ]
    },
    "NUTCH-1937": {
        "Key": "NUTCH-1937",
        "Summary": "Error: Could not find or load main class bin.crawl",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Nishant Jani",
        "Created": "07/Feb/15 20:24",
        "Updated": "07/Feb/15 21:42",
        "Resolved": "07/Feb/15 21:33",
        "Description": "I set up Nutch, I have set NUTCH_HOME, NUTCH_JAVA_HOME and JAVA_HOME.  Im working on Nutch 1.10-snapshot. When I run the following command bin/nutch bin/crawl urls/ I get Error: Could not find or load main class bin.crawl\nCould anyone let me know the fix for the same.",
        "Issue Links": []
    },
    "NUTCH-1938": {
        "Key": "NUTCH-1938",
        "Summary": "Unable to load realm info from SCDynamicStore",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "injector",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Pranshu Kumar",
        "Created": "08/Feb/15 07:25",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "08/Feb/15 19:05",
        "Description": "Pranshus-MacBook-Air:apache-nutch-1.9 pranshukumar$ bin/crawl urls -dir crawl -depth 3 -topN 5\nInjector: starting at 2015-02-07 23:23:16\nInjector: crawlDb: -dir/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\n2015-02-07 23:23:16.958 java[857:35681] Unable to load realm info from SCDynamicStore\nInjector: Total number of urls rejected by filters: 0\nInjector: Total number of urls after normalization: 2\nInjector: Merging injected urls into crawl db.\nInjector: overwrite: false\nInjector: update: false\nInjector: URLs merged: 2\nInjector: Total new urls injected: 0\nInjector: finished at 2015-02-07 23:23:19, elapsed: 00:00:02\n This is the error that i get -Unable to load realm info from SCDynamicStore.\ni also did set up the java environment \nPranshus-MacBook-Air:apache-nutch-1.9 pranshukumar$ export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home",
        "Issue Links": []
    },
    "NUTCH-1939": {
        "Key": "NUTCH-1939",
        "Summary": "Fetcher fails to follow redirects",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "10/Feb/15 22:18",
        "Updated": "13/Feb/15 00:07",
        "Resolved": "12/Feb/15 11:54",
        "Description": "As reported by leoyey in NUTCH-1735 which introduced the regression: with http.redirect.max > 0 Fetcher does not follow redirects.",
        "Issue Links": []
    },
    "NUTCH-1940": {
        "Key": "NUTCH-1940",
        "Summary": "Port HTTP POST Authentication to 2.X",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "plugin",
        "Assignee": "Talat Uyarer",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Feb/15 04:21",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "This issues relates to NUTCH-827",
        "Issue Links": [
            "/jira/browse/NUTCH-827"
        ]
    },
    "NUTCH-1941": {
        "Key": "NUTCH-1941",
        "Summary": "Optional rolling http.agent.name's",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "fetcher,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Feb/15 06:23",
        "Updated": "17/Sep/15 07:02",
        "Resolved": "27/Mar/15 21:46",
        "Description": "In some scenarios, even whilst adhering to fetcher.crawl.delay, web admins can block your fetcher based merely on your crawler name. \nI propose the ability to implement rolling http.agent.name's which could be substituted every 5 seconds for example. This would mean that successive requests to the same domain would be sent with different http.agent.name. \nThis behavior should be off by default.",
        "Issue Links": []
    },
    "NUTCH-1942": {
        "Key": "NUTCH-1942",
        "Summary": "Remove TopLevelDomain",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "12/Feb/15 14:48",
        "Updated": "08/Jan/23 19:48",
        "Resolved": null,
        "Description": "We should leverage the domain related utilities from crawler-commons instead of duplicating them in the `org.apache.nutch.util.domain` package. For instance we could deprecate TopLevelDomain and call the corresponding class in CC instead. The resources in CC are more up-to-date and it is less code to maintain.\nThis would be a good task for someone willing to get to know the Nutch codebase better and impress us all with the extent of his/her skills.",
        "Issue Links": [
            "/jira/browse/NUTCH-1806"
        ]
    },
    "NUTCH-1943": {
        "Key": "NUTCH-1943",
        "Summary": "Form authentication should not be global and ignore <authScope>",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Feb/15 22:08",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Taken from wastl-nagel's comments on NUTCH-827\nthe form authentication is global and ignores <authScope>. So you have to restrict your crawl to the form authentication pages only. Ideally, also form authentication should be bound to a scope (one host, one URL prefix, etc.) same as HTTP authentication.",
        "Issue Links": [
            "/jira/browse/NUTCH-827"
        ]
    },
    "NUTCH-1944": {
        "Key": "NUTCH-1944",
        "Summary": "Add raw content to indexes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.1",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Feb/15 04:34",
        "Updated": "02/Jun/15 19:06",
        "Resolved": "10/Apr/15 05:20",
        "Description": "The issues is described very well here\nhttps://github.com/Meabed/nutch2-index-html",
        "Issue Links": [
            "/jira/browse/NUTCH-1785"
        ]
    },
    "NUTCH-1945": {
        "Key": "NUTCH-1945",
        "Summary": "Test for XLSX parser",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.10,                                            2.3.1",
        "Fix Version/s": "1.17",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/Feb/15 23:08",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "12/May/20 13:35",
        "Description": "Add a test for Excel spreadsheets (xlsx) files: because the are formally also zip files (as well as other composite files) the MIME type detection is crucial also for parsing, cf. NUTCH-1605 and NUTCH-1925.",
        "Issue Links": []
    },
    "NUTCH-1946": {
        "Key": "NUTCH-1946",
        "Summary": "Upgrade to Gora 0.6.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Feb/15 20:22",
        "Updated": "12/Sep/16 10:04",
        "Resolved": "20/Sep/15 12:51",
        "Description": "Apache Gora 0.6.1 was released recently.\nWe should upgrade before pushing Nutch 2.3.1 as it will come in very handy for the new Docker containers.",
        "Issue Links": [
            "/jira/browse/NUTCH-2050",
            "/jira/browse/NUTCH-1572",
            "/jira/browse/NUTCH-2101",
            "https://github.com/apache/gora/pull/21"
        ]
    },
    "NUTCH-1947": {
        "Key": "NUTCH-1947",
        "Summary": "Overhaul o.a.n.parse.OutlinkExtractor.java",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Feb/15 22:02",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Right now in both trunk and 2.X, the OutlinkExtractor.java class need a bit of TLC. It is referencing JDK1.5 in a few places, there are misleading URL entries and it boasts some interesting @Deprecation methods which we could ideally remove.",
        "Issue Links": []
    },
    "NUTCH-1948": {
        "Key": "NUTCH-1948",
        "Summary": "Make the Selenium remote web driver specification, configuration and selection available via a Factory-type mechanism",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "23/Feb/15 18:28",
        "Updated": "24/Aug/15 22:44",
        "Resolved": "24/Aug/15 22:44",
        "Description": "Right now we only use the FirefoxDriver, however we could also use the following\n\nChromeDriver <https://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/chrome/ChromeDriver.html>,\nFirefoxDriver <https://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/firefox/FirefoxDriver.html>,\nInternetExplorerDriver <https://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/ie/InternetExplorerDriver.html>, and\nSafariDriver <https://selenium.googlecode.com/git/docs/api/java/org/openqa/selenium/safari/SafariDriver.html>\n\nThey could be available via a Factory-type mechanism which would allow us to define the driver within nutch-site.xml even.",
        "Issue Links": [
            "/jira/browse/NUTCH-1933"
        ]
    },
    "NUTCH-1949": {
        "Key": "NUTCH-1949",
        "Summary": "Dump out the Nutch data into the Common Crawl format",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "crawldb,                                            linkdb,                                            storage,                                            tool",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Giuseppe Totaro",
        "Created": "24/Feb/15 04:40",
        "Updated": "04/Mar/15 21:31",
        "Resolved": "04/Mar/15 18:49",
        "Description": "We are going to develop a CommonCrawlDataDumper.java class. The CommonCrawlDataDumper is a tool able to perfom the following steps:  \n\ndeserialize the crawled data from Nutch\nmap serialized data on the proper JSON structure\nserialize the data into CBOR format\noptionally, compress the serialized data using gzip\n\nThis tool has to be able to work with either single Nutch segments or directory including segments as input data.\nThanks lewismc and chrismattmann for your great suggestions, support and code.",
        "Issue Links": []
    },
    "NUTCH-1950": {
        "Key": "NUTCH-1950",
        "Summary": "File name too long when bin/nutch dump",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "segment",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chong Li",
        "Created": "24/Feb/15 19:06",
        "Updated": "04/Mar/15 03:25",
        "Resolved": "04/Mar/15 02:21",
        "Description": "When bin/dump in version 1.10-trunk, there will be an exception saying \"File name too long\". When crawling, the length of the url may be longer than 255 bytes and nutch save the file using the url as file name. It can be saved in segments but when dumping the files to local file system, the length of the filename can not be longer than 255 bytes. \nThe FileDumper.java need to be changed to handle such exception.",
        "Issue Links": []
    },
    "NUTCH-1951": {
        "Key": "NUTCH-1951",
        "Summary": "Parse tool to accept a parent segment directory as well as individual segment directory",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Feb/15 22:22",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Right now the parse tool can only accept an individual segment directory as an argument.\nThis issue proposes to augment the tool to accept a parent directory containing many segments directories and to recursively parse through them.",
        "Issue Links": []
    },
    "NUTCH-1952": {
        "Key": "NUTCH-1952",
        "Summary": "Add a timezone to the Nutch log4j.properties configuration",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Feb/15 22:30",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "It would be nice to have a timezone associated with the log entries so that when one logs in to a remote server it is obvious exactly when the logs were produced.\nSome previous hints on how this can be achieved can be found on threads below\nhttps://www.google.com/search?q=log4j+time+zone&ie=utf-8&oe=utf-8",
        "Issue Links": []
    },
    "NUTCH-1953": {
        "Key": "NUTCH-1953",
        "Summary": "Integrate \u00b5Block into Nutch to block Ads",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Trevor Lewis",
        "Created": "28/Feb/15 02:22",
        "Updated": "28/Feb/15 02:22",
        "Resolved": null,
        "Description": "I feel \u00b5Block (https://github.com/gorhill/uBlock) or any other related plugin which can block Ads can be integrated into Nutch that way we can skip the Ads in the Nutch crawls.",
        "Issue Links": []
    },
    "NUTCH-1954": {
        "Key": "NUTCH-1954",
        "Summary": "FilenameTooLong error appears in CommonCrawlDumper",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "commoncrawl",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "07/Mar/15 04:31",
        "Updated": "07/Mar/15 05:50",
        "Resolved": "07/Mar/15 04:55",
        "Description": "The issue from NUTCH-1950 is appearing in the CommonCrawlDumper tool as well (FilenameTooLong). I'm going to apply that fix here as well (based on MD5/message digest).",
        "Issue Links": []
    },
    "NUTCH-1955": {
        "Key": "NUTCH-1955",
        "Summary": "ByteWritable missing in NutchWritable",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "10/Mar/15 10:53",
        "Updated": "13/Mar/15 15:50",
        "Resolved": "13/Mar/15 14:58",
        "Description": "Obvious. Add ByteWritable.",
        "Issue Links": []
    },
    "NUTCH-1956": {
        "Key": "NUTCH-1956",
        "Summary": "Members to be public in URLCrawlDatum",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "10/Mar/15 14:45",
        "Updated": "13/Mar/15 15:50",
        "Resolved": "13/Mar/15 14:56",
        "Description": "URLCrawlDatum's datum member cannot be accessed from other unit tests.",
        "Issue Links": []
    },
    "NUTCH-1957": {
        "Key": "NUTCH-1957",
        "Summary": "FileDumper output file name collisions",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Renxia Wang",
        "Created": "11/Mar/15 07:04",
        "Updated": "15/Mar/15 04:51",
        "Resolved": "15/Mar/15 04:25",
        "Description": "The FileDumper extracts file base name and extension and use <basename>.<extension>(e.g. given the url https://www.aoncadis.org/contact/Yarrow%20Axford/project.html, the <basename>.<extension> will be project.html) as the file name to dump the file. \nCode from FileDumper.java: \nString url = key.toString();\nString baseName = FilenameUtils.getBaseName(url);\nString extension = FilenameUtils.getExtension(url);\n...\nString filename = baseName + \".\" + extension;\nThis introduce file name collision and leads to loss of data when using bin/nutch dump. \nSample logs:\n2015-03-10 23:38:01,192 INFO  tools.FileDumper - Dumping URL: http://beringsea.eol.ucar.edu/data/\n2015-03-10 23:38:01,193 INFO  tools.FileDumper - Skipping writing: [testFileName/.html]: file already exists\n2015-03-10 23:38:16,717 INFO  tools.FileDumper - Dumping URL: http://catalog.eol.ucar.edu/\n2015-03-10 23:38:16,719 INFO  tools.FileDumper - Skipping writing: [testFileName/.html]: file already exists\n2015-03-10 23:38:46,411 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Carin%20Ashjian/project.html\n2015-03-10 23:38:46,411 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,411 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Christopher%20Arp/project.html\n2015-03-10 23:38:46,411 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,411 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Dr.%20Knut%20Aagaard/project.html\n2015-03-10 23:38:46,412 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,412 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Eric%20C.%20Apel/project.html\n2015-03-10 23:38:46,412 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,412 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/John%20T.%20Andrews/project.html\n2015-03-10 23:38:46,412 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,412 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Juha%20Alatalo/project.html\n2015-03-10 23:38:46,413 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,413 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Kerim%20Aydin/project.html\n2015-03-10 23:38:46,413 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,413 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Knut%20Aagaard/project.html\n2015-03-10 23:38:46,413 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,413 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Mary%20Albert/project.html\n2015-03-10 23:38:46,414 INFO  tools.FileDumper - Skipping writing: [testFileName/project.html]: file already exists\n2015-03-10 23:38:46,414 INFO  tools.FileDumper - Dumping URL: https://www.aoncadis.org/contact/Yarrow%20Axford/project.html",
        "Issue Links": []
    },
    "NUTCH-1958": {
        "Key": "NUTCH-1958",
        "Summary": "Remove scoring-opic from nutch-default.xml",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Mar/15 09:58",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "I propose we remove scoring-opic from nutch-default. We all know it is flawed for any kind of incremental crawl, which most of us do. It is also useless if you want to perform a single crawl, if you must crawl all records of a domain, using OPIC for prioritizing URLS makes no sense. It also confuses users as we have seen in the past and recently [1].\nWhat do you think?\n[1]: http://lucene.472066.n3.nabble.com/Nutch-documents-have-huge-scores-in-Solr-td4192064.html",
        "Issue Links": []
    },
    "NUTCH-1959": {
        "Key": "NUTCH-1959",
        "Summary": "Improving CommonCrawlFormat implementations",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Giuseppe Totaro",
        "Created": "11/Mar/15 17:04",
        "Updated": "26/Mar/15 04:50",
        "Resolved": "26/Mar/15 04:36",
        "Description": "CommonCrawlFormat is an interface for Java classes that implement methods for writing data into Common Crawl format. AbstractCommonCrawlFormat is an abstract class that implements CommonCrawlFormat and provides abstract methods for \"CommonCrawl formatter\" classes.\nYou can find in attachment a PATCH that includes some improvements for CommonCrawlFormat-based classes;\n\nCommonCrawlFormat and AbstractCommonCrawlFormat now provide only the getJsonData() method, responsible for getting out JSON data.\nAbstractCommonCrawlFormat provides also the abstract methods that each subclass has to implement in order to handle JSON objects.\nCommonCrawlFormatSimple is a StringBuilder-based formatter that now provide also escaping of JSON string values.\n\nThis PATCH aims at providing a better interface for implementing/extending CommonCrawlFormat classes.\nI would really appreciate your feedback.\nThanks a lot,\nGiuseppe",
        "Issue Links": [
            "/jira/browse/NUTCH-1974"
        ]
    },
    "NUTCH-1960": {
        "Key": "NUTCH-1960",
        "Summary": "JUnit test for dump method of CommonCrawlDataDumper",
        "Type": "Test",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Giuseppe Totaro",
        "Created": "11/Mar/15 17:10",
        "Updated": "11/Apr/15 04:41",
        "Resolved": "11/Apr/15 04:41",
        "Description": "Hi all,\nyou can find in attachment the PATCH including an extremely simple JUnit test for dump method of CommonCrawlDataDumper class.\nEssentially, it checks if dump is able to create a given list of files from Butch segments (in testresources).\nThanks a lot,\nGiuseppe",
        "Issue Links": []
    },
    "NUTCH-1961": {
        "Key": "NUTCH-1961",
        "Summary": "Provide multipart compression of Common Crawl data",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Giuseppe Totaro",
        "Created": "11/Mar/15 17:20",
        "Updated": "11/Mar/15 17:20",
        "Resolved": null,
        "Description": "Using -gzip option in CommonCrawlDataDumper, users are able to compress data and create a TAR archive (using the Apache Commons Compress. \nWe could provide also the opportunity to make multipart compressed archive using a threshold. I did some tests using a CountingOutputStream \"in the middle\" in order to count bytes written, but it requires to flush the output streams at each iteration.\nFurthermore, gzip does not support multipart compression (we can split the archive in multiple .tar.gz files but they have to be unzipped individually), whereas zip does (even though this feature is not supported yet in Apache Commons Compress).\nI would really appreciate your feedback/ideas about this.\nThanks a lot,\nGiuseppe",
        "Issue Links": []
    },
    "NUTCH-1962": {
        "Key": "NUTCH-1962",
        "Summary": "Need to have mimetype-filter.txt file available by default",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Mar/15 22:47",
        "Updated": "21/Mar/15 06:57",
        "Resolved": "21/Mar/15 06:57",
        "Description": "By default the mimetype-filter.txt file quoted within nutch-default.xml is not available. We need to provide this as it is a PITA to constantly have to add it it new crawler configurations.\nhttps://github.com/apache/nutch/blob/trunk/conf/nutch-default.xml#L1616-L1625",
        "Issue Links": []
    },
    "NUTCH-1963": {
        "Key": "NUTCH-1963",
        "Summary": "CommonsCrawlDataDumper is too long ( > 100 bytes) when -gzip option invoked",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "commoncrawl",
        "Assignee": "Giuseppe Totaro",
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Mar/15 18:13",
        "Updated": "23/Apr/15 23:36",
        "Resolved": "23/Apr/15 23:36",
        "Description": "When invoking the commoncrawldump tool with the -gzip option and -mimtype application/pdf I get the following stack trace which results in a failure of the task\n\njava.lang.RuntimeException: file name 'Socio-Economic%20Impact%20of%20Ebola%20on%20Households%20in%20Liberia%20Nov%2019%20(final,%20revised).pdf' is too long ( > 100 bytes)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.handleLongName(TarArchiveOutputStream.java:674)\n\tat org.apache.commons.compress.archivers.tar.TarArchiveOutputStream.putArchiveEntry(TarArchiveOutputStream.java:275)\n\tat org.apache.nutch.tools.CommonCrawlDataDumper.dump(CommonCrawlDataDumper.java:400)\n\tat org.apache.nutch.tools.CommonCrawlDataDumper.main(CommonCrawlDataDumper.java:236)\n\n\nThe workaround consists of not using the -gzip option, instead delaying this until a later task, however this is a workaround and not a solution.\nWe need to fix this in order for the tool to work as designed and required.",
        "Issue Links": []
    },
    "NUTCH-1964": {
        "Key": "NUTCH-1964",
        "Summary": "tmp directory not cleaned up after using commoncrawldump tool",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "commoncrawl",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Mar/15 17:58",
        "Updated": "16/Apr/15 19:29",
        "Resolved": "16/Apr/15 19:29",
        "Description": "After using the commoncrawldump tool I am seeing a persistent tmp directory in the directory where I invoked the tool from e.g.\n\n[mdeploy@crawl local]$ ls\nbin  conf  lib  logs  plugins  test  tmp_1426114168524-231608436\n\n\nWe need to make sure that this is cleaned up after invoking the tool.",
        "Issue Links": []
    },
    "NUTCH-1965": {
        "Key": "NUTCH-1965",
        "Summary": "My",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "LEARNING TESTING",
        "Created": "16/Mar/15 10:44",
        "Updated": "16/Mar/15 10:58",
        "Resolved": "16/Mar/15 10:58",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-1966": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Configuration endpoint for 1x REST API",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "17/Mar/15 07:31",
        "Updated": "29/Sep/15 19:29",
        "Resolved": "19/Mar/15 03:51",
        "Description": "The current work going on towards developing a REST service for the Nutch 1x branch includes many endpoints. This issue deals with the Configuration endpoint of the REST service. \nThis is a sub issue for the major issue https://issues.apache.org/jira/browse/NUTCH-1931.",
        "Issue Links": [
            "/jira/browse/NUTCH-1931"
        ]
    },
    "NUTCH-1967": {
        "Key": "NUTCH-1967",
        "Summary": "Possible SIooBE in MimeAdaptiveFetchSchedule",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "17/Mar/15 15:36",
        "Updated": "18/Mar/15 08:51",
        "Resolved": "18/Mar/15 08:05",
        "Description": "It seems that sometimes, or since a change, charset is now available in the CrawlDatum's MIME type. This fetch schedule can trip over it.",
        "Issue Links": []
    },
    "NUTCH-1968": {
        "Key": "NUTCH-1968",
        "Summary": "File Name too long issue of DumpFileUtil.java file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Xin Zhang",
        "Created": "18/Mar/15 07:52",
        "Updated": "23/Mar/15 19:19",
        "Resolved": "19/Mar/15 04:23",
        "Description": "With the helpful patch that Renxia posts https://issues.apache.org/jira/browse/NUTCH-1957, I figure out that we need to solve the file name collision, otherwise we will lose data. However, when I use this patch to execute bin/nutch dump, I get file name too long error as follows:\nzhangxin0804@zhangxin0804-VirtualBox:~/Desktop/Nutch/nutch/runtime/local$ bin/nutch dump -outputDir outputDir -segment TestCrawl2/segments\njava.io.FileNotFoundException:/home/zhangxin0804/Desktop/Nutch/nutch/runtime/local/outputDir/86/fc/830433456bfbcff5f7b53661cc24d9d4_maps.php?submitted=true&year=2014&month=6&imgs%5b%5d=nationaltavgrank&imgs%5b%5d=nationaltmaxrank&imgs%5b%5d=nationaltminrank&imgs%5b%5d=nationalpcpnrank&imgs%5b%5d=regionaltavgrank&imgs%5b%5d=regionaltmaxrank&imgs%5b%5d=regionaltminrank&imgs%5b%5d=regionalpcpnrank&imgs%5b%5d=statewidetavgrank&imgs%5b%5d=statewidetmaxrank&imgs%5b%5d=statewidetminrank&imgs%5b%5d=statewidepcpnrank&imgs%5b%5d=divisionaltavgrank&imgs%5b%5d=divisionaltmaxrank&imgs%5b%5d=divisionaltminrank&imgs%5b%5d=divisionalpcpnrank&ts=3 (File name too long)\n\tat java.io.FileOutputStream.open(Native Method)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:221)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:171)\n\tat org.apache.nutch.tools.FileDumper.dump(FileDumper.java:221)\n\tat org.apache.nutch.tools.FileDumper.main(FileDumper.java:309)\nI dig into this patch and find it only checks the length of fileBaseName in /nutch/trunk/src/java/org/apache/nutch/util/DumpFileUtil.java. Therefore, if the <extension> is too long, the final outputFullPath is still too long which means it will throw exception in FileDumper.java Probably not everyone will meet this issue and it is maybe a minor bug, correct me if I am wrong. Meanwhile, is that OK to truncate fileExtension name as we did on fileBase name to solve this problem?",
        "Issue Links": []
    },
    "NUTCH-1969": {
        "Key": "NUTCH-1969",
        "Summary": "URL Normalizer properly handling slashes",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "plugin",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Markus Jelsma",
        "Created": "18/Mar/15 08:13",
        "Updated": "27/Apr/15 14:20",
        "Resolved": "27/Apr/15 01:41",
        "Description": "This is a URL normalizer we use that is simple to use and generate for  dealing with hosts that mix up slash suffixed URL's with non-slash suffixed URL's.\nIt is similar to the host nomalizer, reducing the number of duplicates while crawling. It takes the new line delimited rules, separated by either a tabulator or whitespace, followed by a + (PLUS) or - (MINUS) sign denoting whether or not a slash is to be added to the path.\nThe normalizer ignores pages that look like files with extensions, see tests.\nNote: the normalizer must be enhanced to not take hosts as first argument of a rule, but host/path prefixes because some hosts need different rules depending on the root path. For example,\n\nexample.org/cms/news/1/2/3/4 is a CMS that doesn't accept slashes, if they are suffixed, the user is redirected to a non-slash page;\nexample.org/files/a/b/ wants to do it just the other way around.",
        "Issue Links": []
    },
    "NUTCH-1970": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Pretty print JSON output in config resource",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "19/Mar/15 03:51",
        "Updated": "29/Mar/15 13:23",
        "Resolved": "29/Mar/15 05:29",
        "Description": "We can use Jackson to pretty print the JSON output from the Nutch server.",
        "Issue Links": []
    },
    "NUTCH-1971": {
        "Key": "NUTCH-1971",
        "Summary": "The crawldb.url.filters property is not present in any configuration file",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Luis Lopez",
        "Created": "19/Mar/15 22:51",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "15/May/20 17:30",
        "Description": "In CrawlDbFilter.java there is a line for getting a boolean that sets if the filters are going to be applied or not: \n  public static final String URL_FILTERING = \"crawldb.url.filters\";\nHowever in nutch-default.xml that property is not present. Currently the only way to set this value is using the -filter parameter from the command line. \nThe same applies to:  \npublic static final String URL_NORMALIZING = \"crawldb.url.normalizers\";\npublic static final String URL_NORMALIZING_SCOPE = \"crawldb.url.normalizers.scope\";",
        "Issue Links": []
    },
    "NUTCH-1972": {
        "Key": "NUTCH-1972",
        "Summary": "Dockerfile for Nutch 1.x",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "deployment",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Michael Joyce",
        "Created": "20/Mar/15 01:43",
        "Updated": "15/Apr/15 14:38",
        "Resolved": "10/Apr/15 04:58",
        "Description": "Hi folks,\nI noticed that there was a Docker file for Nutch 2.x but I didn't see anything for 1.x. I figured I would throw something up real quick. Note that this currently doesn't install Solr. I didn't need it at the time when I was making this, but I'll work on getting it added before too long.",
        "Issue Links": []
    },
    "NUTCH-1973": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Job Administration end point for the REST service",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "20/Mar/15 08:14",
        "Updated": "18/May/15 00:50",
        "Resolved": "22/Apr/15 01:47",
        "Description": "This sub task deals with implementing the functionality documented at https://wiki.apache.org/nutch/Nutch_1.X_RESTAPI",
        "Issue Links": []
    },
    "NUTCH-1974": {
        "Key": "NUTCH-1974",
        "Summary": "keyPrefix option for CommonCrawlDataDumper tool",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Giuseppe Totaro",
        "Created": "23/Mar/15 19:17",
        "Updated": "26/Mar/15 04:50",
        "Resolved": "26/Mar/15 03:01",
        "Description": "Hi all, you can find in attachment a new patch including the support for -keyPrefix command line option. This is an optional flag that enables to add a prefix to every key value in the output format.\nMoreover, in this patch, the CommonCrawlDataDumper tool uses the method in DumpFileUtil (NUTCH-1968) as suggested by chrismattmann.",
        "Issue Links": [
            "/jira/browse/NUTCH-1959"
        ]
    },
    "NUTCH-1975": {
        "Key": "NUTCH-1975",
        "Summary": "New configuration for CommonCrawlDataDumper tool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Giuseppe Totaro",
        "Created": "28/Mar/15 00:48",
        "Updated": "16/Apr/15 14:18",
        "Resolved": "03/Apr/15 14:36",
        "Description": "Hi all, you can find in attachment a new patch including support for new options for CommonCrawlDataDumper.\nIn particultar, new options are passed to CommonCrawlFormat object (which provides methods to create JSON output) using a configuration object (CommonCrawlConfig).\nIn particular, in this patch CommonCrawlDataDumper provides support for the following options:\n\n-SimpleDataFormat: enables timestamps in GMT epoche (milliseconds) format.\n-epochFilename: files extracted will be organized in a reversed-DNS tree based on the FQDN of the webpage, followed by a SHA1 hash of the complete URL. Scraped data will be stored in these directories as individual GMT-timestamped files using \"epoche time (in milliseconds)\" plus file extension.\n-jsonArray: organizes both request and response headers into a JSON array instead of using a JSON sub-object.\n*-reverseKey: enables to use the same layout as described for -epochFilename option, with underscore in place of directory separators.\n\nYou can use the options above in addition to the options already supported, as described in the Nutch wiki page.\nThis patch starts from NUTCH-1974.\nThanks chrismattmann and annieburgess for supporting me on this work.",
        "Issue Links": []
    },
    "NUTCH-1976": {
        "Key": "NUTCH-1976",
        "Summary": "Allow Users to Set Hostname for Server",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Tyler Bui-Palsulich",
        "Created": "29/Mar/15 02:18",
        "Updated": "29/Mar/15 05:50",
        "Resolved": "29/Mar/15 05:17",
        "Description": "Users should be able to provide a hostname when starting the Nutch Server, rather than be stuck with localhost. Patch incoming.",
        "Issue Links": []
    },
    "NUTCH-1977": {
        "Key": "NUTCH-1977",
        "Summary": "commoncrawldump java heap space",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "commoncrawl",
        "Assignee": null,
        "Reporter": "Jiaheng Zhang",
        "Created": "29/Mar/15 06:30",
        "Updated": "01/Apr/15 05:47",
        "Resolved": "01/Apr/15 05:47",
        "Description": "When using the commoncrawldump component, we get the error:\nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:3236)\n\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:113)\n\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:140)\n\tat com.fasterxml.jackson.dataformat.cbor.CBORGenerator._flushBuffer(CBORGenerator.java:1365)\n\tat com.fasterxml.jackson.dataformat.cbor.CBORGenerator.close(CBORGenerator.java:896)\n\tat org.apache.nutch.tools.CommonCrawlDataDumper.serializeCBORData(CommonCrawlDataDumper.java:461)\n\tat org.apache.nutch.tools.CommonCrawlDataDumper.dump(CommonCrawlDataDumper.java:375)\n\tat org.apache.nutch.tools.CommonCrawlDataDumper.main(CommonCrawlDataDumper.java:256)\nand \nException in thread \"main\" java.lang.OutOfMemoryError: Java heap space\n\tat java.util.Arrays.copyOf(Arrays.java:2367)\n\tat java.lang.StringCoding.safeTrim(StringCoding.java:89)\n\tat java.lang.StringCoding.access$100(StringCoding.java:50)\n\tat java.lang.StringCoding$StringDecoder.decode(StringCoding.java:154)\n\tat java.lang.StringCoding.decode(StringCoding.java:193)\n\tat java.lang.StringCoding.decode(StringCoding.java:254)\n\tat java.lang.String.<init>(String.java:536)\n\tat java.io.ByteArrayOutputStream.toString(ByteArrayOutputStream.java:208)\n\tat org.apache.nutch.tools.CommonCrawlFormatJackson.generateJson(CommonCrawlFormatJackson.java:80)\n\tat org.apache.nutch.tools.AbstractCommonCrawlFormat.getJsonData(AbstractCommonCrawlFormat.java:121)\n\tat org.apache.nutch.tools.CommonCrawlDataDumper.dump(CommonCrawlDataDumper.java:361)\n\tat org.apache.nutch.tools.CommonCrawlDataDumper.main(CommonCrawlDataDumper.java:256)\nThe segment files' size is 1.41GB. However we successfully dump the files with the segments' size of 100M.",
        "Issue Links": []
    },
    "NUTCH-1978": {
        "Key": "NUTCH-1978",
        "Summary": "solrindex will fail when indexing corrupted segments",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Chong Li",
        "Created": "31/Mar/15 06:11",
        "Updated": "31/Mar/15 20:36",
        "Resolved": "31/Mar/15 20:36",
        "Description": "The same issue from NUTCH-1771 but seems like this bug will appear in most of the versions since they all don't have the code to handle the corrupted segments.\nForm NUTCH-1771, people pointed out that it will be very hard to handle this in the hadoop layer, and the program should skip the corrupted segments instead of end the program. By corrupted segments I mean that the segment may be just generated and doesn't have the content.\nSo my initial idea is to check if the segment folder is valid before putting the segment into the hadoop job. If the segment is not valid, we can simply just skip that segment. We can check if the segment folder contains exactly 6 sub directories as there should be. The other  approach will be to check all the six sub directories and see if they are exactly the six dir that should appear.",
        "Issue Links": [
            "/jira/browse/NUTCH-1771"
        ]
    },
    "NUTCH-1979": {
        "Key": "NUTCH-1979",
        "Summary": "CrawlDbReader to implement Tool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/Mar/15 09:11",
        "Updated": "31/Mar/15 19:51",
        "Resolved": "31/Mar/15 17:35",
        "Description": "Evident, and a must-have when running on Hadoop 2.x with named queues.",
        "Issue Links": []
    },
    "NUTCH-1980": {
        "Key": "NUTCH-1980",
        "Summary": "Jexl expressions for CrawlDbReader",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Apr/15 21:54",
        "Updated": "08/Jan/23 19:54",
        "Resolved": "01/Jul/15 07:38",
        "Description": "Jexl expression support for the CrawlDbReader. This allows you to read items from the database based on their metadata with flexilibity and boolean logic. Some examples\n\n* Get all english pages\n-expr \"lang=en\"\n\n* Get all english pages that have a low response time\n-expr \"lang=en && _rs_ > 5000\"",
        "Issue Links": [
            "/jira/browse/NUTCH-1429"
        ]
    },
    "NUTCH-1981": {
        "Key": "NUTCH-1981",
        "Summary": "Upgrade icu4j",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.9",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Marko Asplund",
        "Created": "02/Apr/15 12:18",
        "Updated": "17/Sep/15 07:02",
        "Resolved": "11/Apr/15 22:13",
        "Description": "The icu4j version from 2009 is causing some compatibility issues with custom plugins we're developing. Please upgrade to a more recent version.\nI'm attaching a patch to this issue. Nutch builds and all tests pass without source code changes.",
        "Issue Links": []
    },
    "NUTCH-1982": {
        "Key": "NUTCH-1982",
        "Summary": "Make Git ignore IDE project files and add note about IDE setup",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.16",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Marko Asplund",
        "Created": "02/Apr/15 12:31",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "23/Sep/19 08:45",
        "Description": "Make Git ignore IDE project files and add note about IDE setup",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/471"
        ]
    },
    "NUTCH-1983": {
        "Key": "NUTCH-1983",
        "Summary": "CommonCrawlDumper and FileDumper don't dump correct JSON",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "dumpers,                                            tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Chris A. Mattmann",
        "Created": "10/Apr/15 04:42",
        "Updated": "10/Apr/15 23:50",
        "Resolved": "10/Apr/15 23:31",
        "Description": "The FileDumper and CommonCrawlDumper tools provide a JSON summarization of the dumped mimeType. The JSON summarization right now incorrectly uses an outer \"{\" instead of \"[\" (list) structure (incorrect JSON) and also it neglects to include a double quote prefix to the count values.",
        "Issue Links": []
    },
    "NUTCH-1984": {
        "Key": "NUTCH-1984",
        "Summary": "Eliminate unnecessary dependencies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Marko Asplund",
        "Created": "10/Apr/15 10:45",
        "Updated": "22/Nov/19 15:47",
        "Resolved": "22/Nov/19 15:47",
        "Description": "When I build (ant runtime) Nutch from 2.x branch some unnecessary dependencies seem to get included in lib (runtime/local/lib).\nthe scope of dependencies should be minimized and e.g. test-only dependencies should not be included in runtime lib. e.g. the following may not be actually required at runtime:\n\njunit-4.11.jar\n*jetty*\nhsqldb-2.2.8.jar\nh2-1.4.180.jar\n\nanother issue is that multiple versions get included in lib for some libraries. these cases should be reviewed and the redundant versions eliminated. in some cases including a single version may suffice. here's some examples where multiple versions get included:\n\nstax-api-1.0-2.jar, stax-api-1.0.1.jar\ncommons-httpclient-3.0.1.jar, httpclient-4.2.6.jar\ncommons-collections-3.2.1.jar, commons-collections4-4.0.jar\ncommons-lang-2.6.jar, commons-lang3-3.1.jar\nservlet-api-2.5-20081211.jar, servlet-api-2.5-6.1.14.jar\n\ndoes Nutch really require oro-2.0.8.jar? it seems to compile without it.",
        "Issue Links": []
    },
    "NUTCH-1985": {
        "Key": "NUTCH-1985",
        "Summary": "Adding a main() method to the MimeTypeIndexingFilter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.10",
        "Component/s": "indexer,                                            metadata,                                            plugin",
        "Assignee": null,
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "14/Apr/15 20:30",
        "Updated": "24/Apr/15 02:50",
        "Resolved": "24/Apr/15 02:30",
        "Description": "This make very easy the testing of different rules files to check the expressions used to filter the content based on the MIME type detected. Until now the only way to check this was to do test crawls and check the stored data in Solr/Elasticsearch. \nThis allows calling the file using the bin/nutch plugin command, something like:\nbin/nutch plugin mimetype-filter org.apache.nutch.indexer.filter.MimeTypeIndexingFilter -h\nTwo options are accepted, -h, --help for showing the help and -rules for specifying a rules file to be used, this makes easy to play with different rules file until you get the desired behavior. \nAfter invoking the class, a valid MIME type must be entered for each line, and the output will be the same MIME type with a + or - sign in the beginning, indicating if the given MIME type is allowed or denied respectively.",
        "Issue Links": []
    },
    "NUTCH-1986": {
        "Key": "NUTCH-1986",
        "Summary": "Clarify Elastic Search Indexer Plugin Settings",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "documentation,                                            indexer,                                            plugin",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Michael Joyce",
        "Created": "15/Apr/15 15:12",
        "Updated": "17/Apr/15 20:50",
        "Resolved": "17/Apr/15 20:37",
        "Description": "Was working on getting indexing into elastic search working and realized that the majority of my difficulties were simply me misunderstanding what the config needed. Patch incoming to hopefully clarify what is needed by default, what each option does, and add any helpful defaults.",
        "Issue Links": []
    },
    "NUTCH-1987": {
        "Key": "NUTCH-1987",
        "Summary": "Make bin/crawl indexer agnostic",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Michael Joyce",
        "Created": "15/Apr/15 15:45",
        "Updated": "21/Apr/15 04:08",
        "Resolved": "21/Apr/15 02:48",
        "Description": "The crawl script makes it a bit challenging to use an indexer that isn't Solr. For instance, when I want to use the indexer-elastic plugin I still need to call the crawler script with a fake Solr URL otherwise it will skip the indexing step all together.\n\nbin/crawl urls/ crawl/ \"http://fakeurl.com:9200\" 1\n\n\nIt would be nice to keep configuration for the Solr indexer in the conf files (to mirror the elastic search indexer conf and others) and to make the indexing parameter simply toggle whether indexing does or doesn't occur instead of also trying to configure the indexer at the same time.",
        "Issue Links": []
    },
    "NUTCH-1988": {
        "Key": "NUTCH-1988",
        "Summary": "Make nested output directory dump optional",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10,                                            1.11",
        "Component/s": "dumpers",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Michael Joyce",
        "Created": "15/Apr/15 19:17",
        "Updated": "29/Oct/15 22:37",
        "Resolved": "29/Oct/15 21:31",
        "Description": "NUTCH-1957 added nested directories to the bin/nutch dump output to help avoid naming conflicts in output files. It would be nice to be able to specify that you want the older flat directory output as an optional parameter.",
        "Issue Links": []
    },
    "NUTCH-1989": {
        "Key": "NUTCH-1989",
        "Summary": "Handling invalid URLs in CommonCrawlDataDumper",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Giuseppe Totaro",
        "Created": "16/Apr/15 15:50",
        "Updated": "20/Apr/15 19:51",
        "Resolved": "18/Apr/15 16:32",
        "Description": "Hi all,\nrunning the CommonCrawlDataDumper tool (bin/nutch commoncrawldump) with the new options (as described in NUTCH-1975) I noticed there are some problems if an invalid URL is detected.\nFor example, the following URLs (that I found in crawled data) break the naming schema provided by using -epochFilename command-line option:\n\nhttp://www/\nhttp:/\n\nMore in detail, using -epochFilename option, files extracted will be organized in a reversed-DNS tree based on the FQDN of the webpage, followed by a SHA1 hash of the complete URL. When the tool detect the URLs as above, it is not able to build the reversed-DNS tree.\nYou can find in attachment a simple patch for detecting invalid URLs. The patch uses the Apache Commons Validator APIs to detect invalid URLs:\n\nUrlValidator urlValidator = new UrlValidator();\nif (!urlValidator.isValid(url)) {\n  LOG.warn(\"Not valid URL detected: \" + url);\n}\n\n\nThe tool logs a warning message if an invalid URL is detected. I am just wondering if we can perform a specific action if invalid URLs occur. We could skip invalid URLs but I notice that also the following URLs are detected as invalid:\n\n2015-04-15 13:49:40,386 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://www.reddit.com/r/agora/comments/22ezoa/how_to_buy_drugs_on_agora_hur_man_k\u00f6per_droger_p\u00e5/\n2015-04-15 13:49:41,603 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://www/\n2015-04-15 13:49:41,632 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http:/\n2015-04-15 13:49:44,601 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://allthingsvice.com/2012/05/30/the-great-420-scam/\\/\\/allthingsvice.com\\/2012\\/05\\/30\\/the-great-420-scam\\/\n2015-04-15 13:50:34,821 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://www.reddit.com/r/agora/comments/22ezoa/how_to_buy_drugs_on_agora_hur_man_k\u00f6per_droger_p\u00e5/\n2015-04-15 13:50:35,847 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://www/\n2015-04-15 13:50:35,866 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http:/\n2015-04-15 13:50:38,605 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://allthingsvice.com/2012/05/30/the-great-420-scam/\\/\\/allthingsvice.com\\/2012\\/05\\/30\\/the-great-420-scam\\/\n2015-04-15 13:51:20,013 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://antilop.cc/sr/users/nomad bloodbath\n2015-04-15 13:51:20,499 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/gaming/2015/04/mortal-kombat-x-charges-players-for-easy-fatalities/\\/\\/ars.to\\/1aPaqvW\n2015-04-15 13:51:20,500 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/gaming/2015/04/mortal-kombat-x-charges-players-for-easy-fatalities/\\/\\/arstechnica.com\n2015-04-15 13:51:20,500 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/gaming/2015/04/mortal-kombat-x-charges-players-for-easy-fatalities/\\/\\/arstechnica.com\\/gaming\\/2015\\/04\\/mortal-kombat-x-charges-players-for-easy-fatalities\\/\n2015-04-15 13:51:20,500 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/gaming/2015/04/mortal-kombat-x-charges-players-for-easy-fatalities/\\/\\/cdn.arstechnica.net\\/wp-content\\/themes\\/arstechnica\\/assets\n2015-04-15 13:51:20,500 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/gaming/2015/04/mortal-kombat-x-charges-players-for-easy-fatalities/\\/civis\n2015-04-15 13:51:20,588 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/tech-policy/2014/11/prosecutor-silk-road-2-0-suspect-did-admit-to-everything/\\/\\/ars.to\\/1tECmHU\n2015-04-15 13:51:20,589 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/tech-policy/2014/11/prosecutor-silk-road-2-0-suspect-did-admit-to-everything/\\/\\/arstechnica.com\n2015-04-15 13:51:20,589 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/tech-policy/2014/11/prosecutor-silk-road-2-0-suspect-did-admit-to-everything/\\/\\/arstechnica.com\\/tech-policy\\/2014\\/11\\/prosecutor-silk-road-2-0-suspect-did-admit-to-everything\\/\n2015-04-15 13:51:20,590 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/tech-policy/2014/11/prosecutor-silk-road-2-0-suspect-did-admit-to-everything/\\/\\/cdn.arstechnica.net\\/wp-content\\/themes\\/arstechnica\\/assets\n2015-04-15 13:51:20,590 WARN  tools.CommonCrawlDataDumper - Not valid URL detected: http://arstechnica.com/tech-policy/2014/11/prosecutor-silk-road-2-0-suspect-did-admit-to-everything/\\/civis\n\n\nI would be very pleased to get your feedback on action to perform when invalid URLs are detected, avoiding to drop off data and break the naming schema if -epochFilename option is used.\nNow I am going to add a counter for invalid URLs. Thanks lewismc for supporting me on this work.",
        "Issue Links": []
    },
    "NUTCH-1990": {
        "Key": "NUTCH-1990",
        "Summary": "Use URI.normalise() in BasicURLNormalizer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "None",
        "Assignee": "Julien Nioche",
        "Reporter": "Julien Nioche",
        "Created": "16/Apr/15 16:02",
        "Updated": "17/Sep/15 07:04",
        "Resolved": "22/Apr/15 09:55",
        "Description": "One of the things that BasicURLNormalizer is to remove unnecessary dot segments in path.\nInstead of implementing the logic ourselves with some antiquated regex library, we should simply use http://docs.oracle.com/javase/7/docs/api/java/net/URI.html#normalize() which does the same and is probably more efficient.",
        "Issue Links": []
    },
    "NUTCH-1991": {
        "Key": "NUTCH-1991",
        "Summary": "Tika mime detection not using Nutch supplied tika-mimetypes.xml for content based detection",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2,                                            2.3,                                            1.8,                                            2.4,                                            1.9,                                            2.2.1,                                            1.10,                                            1.11,                                            2.3.1",
        "Fix Version/s": "1.10",
        "Component/s": "util",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Iain Lopata",
        "Created": "17/Apr/15 00:03",
        "Updated": "26/Apr/15 03:00",
        "Resolved": "25/Apr/15 15:48",
        "Description": "From Nutch Version 1.5 onwards the MimeUtil.java class that acts as a facade to Tika to perform mime type detection uses a process that attempts a match using the mimetype returned by the server, the filename and the content. NUTCH-1045 provided for the use of an external tika-mimetype.xml file which provides the configuration for this process.  However, the content based detection did not use this file, but instead reverted to using the configuration included in the tika library.  Consequently, any content based match rules added to the nutch version of the configuration file were not used.",
        "Issue Links": []
    },
    "NUTCH-1992": {
        "Key": "NUTCH-1992",
        "Summary": "Port whitelist from NUTCH-1927 to 2.x",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "fetcher,                                            protocol,                                            robots",
        "Assignee": null,
        "Reporter": "Chris A. Mattmann",
        "Created": "18/Apr/15 16:35",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Port the whitelist capability from NUTCH-1927 to 2.x",
        "Issue Links": [
            "/jira/browse/NUTCH-1927"
        ]
    },
    "NUTCH-1993": {
        "Key": "NUTCH-1993",
        "Summary": "Nutch does not use backup parsers",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Arkadi Kosmynin",
        "Created": "21/Apr/15 06:00",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "19/Jul/18 13:06",
        "Description": "From reading the code it is clear that it is designed to allow using several parsers to parse a document in a sequence, until it is successfully parsed. In practice, this does not work because these lines \nif (parseResult != null && !parseResult.isEmpty())\n        return parseResult;\nbreak the loop even if the parsing has failed because parseResult is not empty anyway, it contains a ParseData with ParseStatus.FAILED.\nA fix:\nif ( parseResult.isAnySuccess() ) \n        return parseResult;\nWhere parseResult.isAnySuccess() returns true if any of the parsing attempts were successful.\nThis fix is important because it allows use of backup parsers as originally designed and thus increase index completeness.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/364"
        ]
    },
    "NUTCH-1994": {
        "Key": "NUTCH-1994",
        "Summary": "Upgrade to Apache Tika 1.8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10,                                            2.3.1",
        "Fix Version/s": "1.10,                                            2.3.1",
        "Component/s": "build,                                            parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "21/Apr/15 15:59",
        "Updated": "29/Apr/15 19:52",
        "Resolved": "23/Apr/15 21:38",
        "Description": "Tika 1.8 was released this morning.\nLets upgrade then release Nutch trunk.",
        "Issue Links": []
    },
    "NUTCH-1995": {
        "Key": "NUTCH-1995",
        "Summary": "Add support for wildcard to http.robot.rules.whitelist",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "robots",
        "Assignee": "Giuseppe Totaro",
        "Reporter": "Giuseppe Totaro",
        "Created": "22/Apr/15 06:36",
        "Updated": "10/Dec/15 16:00",
        "Resolved": "10/Dec/15 16:00",
        "Description": "The http.robot.rules.whitelist (NUTCH-1927) configuration parameter allows to specify a comma separated list of hostnames or IP addresses to ignore robot rules parsing for.\nAdding support for wildcard in http.robot.rules.whitelist could be very useful and simplify the configuration, for example, if we need to give many hostnames/addresses. Here is an example:\n\n<name>http.robot.rules.whitelist</name>\n  <value>*.sample.com</value>\n  <description>Comma separated list of hostnames or IP addresses to ignore \n  robot rules parsing for. Use with care and only if you are explicitly\n  allowed by the site owner to ignore the site's robots.txt!\n  </description>\n</property>",
        "Issue Links": []
    },
    "NUTCH-1996": {
        "Key": "NUTCH-1996",
        "Summary": "Make protocol-selenium README part of plugin",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Apr/15 16:34",
        "Updated": "22/Apr/15 16:59",
        "Resolved": "22/Apr/15 16:35",
        "Description": "This is a simple issue which merely ports the documentation from the selenium plugin over to the source code\nhttps://github.com/momer/nutch-selenium/blob/master/README.md",
        "Issue Links": []
    },
    "NUTCH-1997": {
        "Key": "NUTCH-1997",
        "Summary": "Add CBOR \"magic header\" to CommonCrawlDataDumper output",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Giuseppe Totaro",
        "Created": "22/Apr/15 18:33",
        "Updated": "04/May/15 20:58",
        "Resolved": "25/Apr/15 15:57",
        "Description": "For each file extracted from Nutch crawled data, CommonCrawlDataDumper wraps a single string value, representing the JSON text, into CBOR. \nFor instance, using the Unix hexdump tool, we can see that, as expected, the first byte of all files is \"0x7F\" (the first three bits are \"011\", that is the major type for strings, and the following 5 bits are \"11010\", meaning a uint32_t encodes the length of following text), and the following 4 bytes (single-precision float) encodes the right length of file (as described in RFC7049). Therefore, a CBOR tag is currently included into the file (a list of cbor tags is available here).\nIn order to add support for CBOR detection using Apache Tika (as described in TIKA-1610), it would be great if CommonCrawlDataDumper tool is able to add the self-describing CBOR \"magic header\" (Tag 55799) to CBOR-encoded output files. \nThanks a lot Lukeliush for this great research. Thanks chrismattmann for supporting me on this work.",
        "Issue Links": [
            "/jira/browse/TIKA-1610"
        ]
    },
    "NUTCH-1998": {
        "Key": "NUTCH-1998",
        "Summary": "Add support for user-defined file extension to CommonCrawlDataDumper",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "tool",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Giuseppe Totaro",
        "Created": "22/Apr/15 18:40",
        "Updated": "11/May/15 21:56",
        "Resolved": "11/May/15 21:07",
        "Description": "CommonCrawlDataDumper tool is able to generate CBOR-encoded files, extracted from Nutch crawled data, using the Common Crawl format. By default, CommonCrawlDataDumper uses the original file extension.\nWe are going to add support for a command-line option (e.g., -extension) that allows the user to provide a file extension to use in place of the original one.",
        "Issue Links": [
            "/jira/browse/TIKA-1610"
        ]
    },
    "NUTCH-1999": {
        "Key": "NUTCH-1999",
        "Summary": "Add http://nutch.apache.org/robots.txt",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Julien Nioche",
        "Created": "23/Apr/15 10:53",
        "Updated": "08/Sep/22 13:30",
        "Resolved": "08/Sep/22 13:30",
        "Description": "http://nutch.apache.org/robots.txt => 404 not found\nAren't we funny! Go and tell webmasters to have a robots.txt after that!",
        "Issue Links": [
            "/jira/browse/NUTCH-2826",
            "https://github.com/apache/nutch-site/pull/1"
        ]
    },
    "NUTCH-2000": {
        "Key": "NUTCH-2000",
        "Summary": "Link inversion fails with .locked already exists.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Julien Nioche",
        "Created": "23/Apr/15 11:03",
        "Updated": "25/Jun/15 18:57",
        "Resolved": "25/Jun/15 18:42",
        "Description": "using standard crawl script with a brand new test dir in local mode I am getting \nLink inversion\n/data/BLABLABLA/runtime/local/bin/nutch invertlinks /data/BLABLABLA/testCrawl2//linkdb /data/BLABLABLA/testCrawl2//segments/20150423114335\nLinkDb: java.io.IOException: lock file /data/BLABLABLA/testCrawl2/linkdb/.locked already exists.\nPS: 2000!",
        "Issue Links": []
    },
    "NUTCH-2001": {
        "Key": "NUTCH-2001",
        "Summary": "SubCollection Field Name incorrect in nutch-default.xml",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.8,                                            1.9",
        "Fix Version/s": "1.10",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Jeff Cocking",
        "Created": "23/Apr/15 22:20",
        "Updated": "27/Apr/15 01:50",
        "Resolved": "27/Apr/15 01:37",
        "Description": "SubcollectionIndexingFilter.java is looking for the following variable in nutch-default.xml (at line 56).:\n fieldName = conf.get(\"subcollection.default.fieldname\", \"subcollection\");\nnutch-default.xml lists the following:\n<property>\n  <name>subcollection.default.field</name>\n  <value>subcollection</value>\n  <description>\n  The default field name for the subcollections.\n  </description>\n</property>\nThe field name for nutch-default.xml should be changed from subcollection.default.field to subcollection.default.fieldname.",
        "Issue Links": []
    },
    "NUTCH-2002": {
        "Key": "NUTCH-2002",
        "Summary": "ParserChecker and IndexingFiltersChecker to check robots.txt",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.17",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Julien Nioche",
        "Created": "27/Apr/15 14:47",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "05/May/20 13:59",
        "Description": "ParserChecker could check whether a given URL is allowed by the robots.txt directives.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/521"
        ]
    },
    "NUTCH-2003": {
        "Key": "NUTCH-2003",
        "Summary": "topN is not work correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "29/Apr/15 09:32",
        "Updated": "22/Nov/19 15:46",
        "Resolved": "22/Nov/19 15:45",
        "Description": "I want to crawl top 1000 urls which are ordered by scores from webpage table. It doesnt work correctly. \nWhen I use topN parameter,  it is divided by map task counts (topN/ maptaskcounts = maptasktopN) Every map tasks generate maptasktopN urls of map tasks. Assume as I have 25 map tasks and I set topN parameter as 1000 and maptasktopN is calculated as 40. As Result We dont have top 1000 highest scored urls, we have 1000 urls of generated 40 highest scored urls per 25 map tasks.",
        "Issue Links": []
    },
    "NUTCH-2004": {
        "Key": "NUTCH-2004",
        "Summary": "ParseChecker does not handle redirects",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "29/Apr/15 19:29",
        "Updated": "22/Jul/15 17:00",
        "Resolved": "06/May/15 23:33",
        "Description": "At the moment ParseChecker doesn't handle redirects. If it gets anything but a success status it errors out. It would be nice if it handled redirects a bit more gracefully based on the http.redirects config setting.",
        "Issue Links": []
    },
    "NUTCH-2005": {
        "Key": "NUTCH-2005",
        "Summary": "Implement HTrace'ing in Nutch",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "08/May/15 23:55",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Recent developments within the tracing community have brought projects like Apache HTrace (Incubating) into the Apache Incubator opening up the possibility of utilizing tracing logic to better understand distributed applications, systems and systems-of-systems. As many will know, tracing involves a specialized use of logging to record information about a program\u2019s execution. Although many use cases involve the use of tracing within distributed systems such as Hadoop and databases, few tracing experiments belong within the field of large scale, distributed Web search. \nThis issue will combine comprehensive tracing mechanisms in Apache HTrace (Incubating) with the scalable, flexible crawling architecture presented by Apache Nutch 2.X.\nAs essentially every job (Inject, Generate, Fetch Parse, UpdateDB, etc.) in Nutch 2.X interacts with a stack of complex underlying components (known as the search stack) comprehensive tracing would provide insight into system performance, latency, etc. \nEvery job (a class which extends NutchTool and implements Tool) within Nutch 2.X therefore needs to be analyzed for suitability and appropriateness for tracing. Once this is understood a ranked list of tools should be produced, the ranking will be based upon which tools are most suited to tracing... I would suggest that FetcherJob be the top as it enables us to trace not only the HTTPSocketConnections but also writing of data through Gora --> DataStore.",
        "Issue Links": []
    },
    "NUTCH-2006": {
        "Key": "NUTCH-2006",
        "Summary": "IndexingFiltersChecker  to take custom metadata as input",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "11/May/15 14:37",
        "Updated": "15/May/15 14:58",
        "Resolved": "15/May/15 14:04",
        "Description": "Similar to NUTCH-1757 but for IndexingFiltersChecker.",
        "Issue Links": []
    },
    "NUTCH-2007": {
        "Key": "NUTCH-2007",
        "Summary": "add test libs to classpath of bin/nutch junit",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "bin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "12/May/15 19:47",
        "Updated": "27/May/15 21:31",
        "Resolved": "27/May/15 19:34",
        "Description": "To run a unit test via bin/nutch junit libs/jars from test/lib (cf. NUTCH-1803) should be on the classpath.",
        "Issue Links": []
    },
    "NUTCH-2008": {
        "Key": "NUTCH-2008",
        "Summary": "IndexerMapReduce to use single instance of NutchIndexAction for deletions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "13/May/15 11:32",
        "Updated": "14/May/15 10:50",
        "Resolved": "14/May/15 10:09",
        "Description": "For every URL/document to be deleted a new instance of NutchIndexAction is created in IndexerMapReduce (in multiple positions):\n\nNutchIndexAction action = new NutchIndexAction(null,\n   NutchIndexAction.DELETE);\noutput.collect(key, action);\n\n\nSince the index action does not hold any data specific to any URL/document it would be more efficient to re-use a single instance.",
        "Issue Links": []
    },
    "NUTCH-2009": {
        "Key": "NUTCH-2009",
        "Summary": "Fetcher does not work with batchID",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3.1",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Saskia Vola",
        "Created": "13/May/15 17:40",
        "Updated": "16/Sep/15 04:33",
        "Resolved": "16/Sep/15 04:33",
        "Description": "The fetcher does only work with the option -all.\nIt does not work when providing a batch-ID. \n$home/apache-nutch-2.3/runtime/local$ bin/nutch fetch 1431538082-1014788459\nFetcherJob: starting at 2015-05-13 19:28:30\nFetcherJob: batchId: 1431538082-1014788459\nFetcherJob: threads: 10\nFetcherJob: parsing: false\nFetcherJob: resuming: false\nFetcherJob : timelimit set for : -1\njava.lang.IllegalArgumentException: can't serialize class org.apache.avro.util.Utf8\n\tat org.bson.BasicBSONEncoder._putObjectField(BasicBSONEncoder.java:284)\n\tat org.bson.BasicBSONEncoder.putObject(BasicBSONEncoder.java:185)\n\tat org.bson.BasicBSONEncoder.putObject(BasicBSONEncoder.java:131)\n\tat com.mongodb.DefaultDBEncoder.writeObject(DefaultDBEncoder.java:33)\n\tat com.mongodb.OutMessage.putObject(OutMessage.java:289)\n\tat com.mongodb.OutMessage.writeQuery(OutMessage.java:211)\n\tat com.mongodb.OutMessage.query(OutMessage.java:86)\n\tat com.mongodb.DBCollectionImpl.find(DBCollectionImpl.java:81)\n\tat com.mongodb.DBCollectionImpl.find(DBCollectionImpl.java:66)\n\tat com.mongodb.DBCursor._check(DBCursor.java:458)\n\tat com.mongodb.DBCursor._hasNext(DBCursor.java:546)\n\tat com.mongodb.DBCursor.hasNext(DBCursor.java:571)\n\tat org.apache.gora.mongodb.query.MongoDBResult.nextInner(MongoDBResult.java:69)\n\tat org.apache.gora.query.impl.ResultBase.next(ResultBase.java:114)\n\tat org.apache.gora.mapreduce.GoraRecordReader.nextKeyValue(GoraRecordReader.java:119)\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:531)\n\tat org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)",
        "Issue Links": [
            "/jira/browse/NUTCH-2029"
        ]
    },
    "NUTCH-2010": {
        "Key": "NUTCH-2010",
        "Summary": "Implement isFetchingInProgress Utility Function in Fetcher",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "14/May/15 20:56",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The aim here is to stop (without killing) a Nutch crawl if the data being fetched is not of value to the user. The user can infer this by implementing some visualization on top of the backported REST API for Nutch trunk (could probably also do with with 2.X REST API as well tbh).\nI suggest that we implement a convenience utility function in potentially Fetcher.java which would looking something like the following\n\npublic static boolean isFetchingInProgress() {\n  return fetchingInProgress;\n}\n\n\nThe fetchingInProgress should be set to tru whenever fetcher threads are working and should be set to false whenever all fetcher threads are unoccupied and back in the pool vacant.\nThis would be a powerful mechanism for determining if a crawl could be stopped without corrupting data as currently happens when a fetcher task is interrupted.",
        "Issue Links": []
    },
    "NUTCH-2011": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Endpoint to support realtime JSON output from the fetcher",
        "Type": "Sub-task",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "15/May/15 13:04",
        "Updated": "02/Oct/15 08:57",
        "Resolved": null,
        "Description": "This fix will create an endpoint to query the Nutch REST service and get a real-time JSON response of the current/past Fetched URLs. \nThis endpoint also includes pagination of the output to reduce data transfer bw in large crawls.",
        "Issue Links": []
    },
    "NUTCH-2012": {
        "Key": "NUTCH-2012",
        "Summary": "Merge parsechecker and indexchecker",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/May/15 13:50",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Apr/18 12:21",
        "Description": "ParserChecker and IndexingFiltersChecker have evolved from simple tools to check parsers and parsefilters resp. indexing filters to powerful tools which emulate the crawling of a single URL/document:\n\ncheck robots.txt (NUTCH-2002)\nfollow redirects (NUTCH-2004)\n\nKeeping both tools in sync takes extra work (cf. NUTCH-1757/NUTCH-2006, also NUTCH-2002, NUTCH-2004 are done only for parsechecker). It's time to merge them\n\neither into one general debugging tool, keeping parsechecker and indexchecker as aliases\ncentralize common code in one utility class",
        "Issue Links": [
            "/jira/browse/NUTCH-2145",
            "/jira/browse/NUTCH-2554",
            "https://github.com/apache/nutch/pull/310",
            "https://github.com/apache/nutch/pull/348"
        ]
    },
    "NUTCH-2013": {
        "Key": "NUTCH-2013",
        "Summary": "Fetcher: missing logs \"fetching ...\" on stdout",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/May/15 19:40",
        "Updated": "18/May/15 22:06",
        "Resolved": "18/May/15 21:51",
        "Description": "When running Fetcher no messages fetching ... do appear on stdout, there are only -activeThreads=10, ... messages. This is caused by the refactoring of Fetcher in NUTCH-1934:\n\nlogging class is now FetchThread but it is not configured to log to stdout in log4j.properties\nalternatively, FetcherTread's LOG could still be obtained from Fetcher.class\nother refactored classes could be affected as well (FetchItem, etc.)",
        "Issue Links": []
    },
    "NUTCH-2014": {
        "Key": "NUTCH-2014",
        "Summary": "Fetcher hang-up on completion",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/May/15 20:19",
        "Updated": "18/May/15 22:07",
        "Resolved": "18/May/15 21:38",
        "Description": "Although fetcher has done its work it does not shut down and exit but continues to log (and before reports its status to the task tracker):\n\n-activeThreads=11, spinWaiting=0, fetchQueues.totalSize=33, fetchQueues.getQueueCount=1\n-activeThreads=11, spinWaiting=10, fetchQueues.totalSize=26, fetchQueues.getQueueCount=1\n-activeThreads=11, spinWaiting=9, fetchQueues.totalSize=9, fetchQueues.getQueueCount=1\n-activeThreads=9, spinWaiting=7, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=0\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=0\n...\n(last message continues)\n\n\nA possible hint: activeThreads should never exceed 10 (configured per default). Looks like the corresponding variable is lost/mixed-up during fetcher refactorization (NUTCH-1934).",
        "Issue Links": []
    },
    "NUTCH-2015": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Make FetchNodeDb optional (off by default) if NutchServer is not used",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher,                                            REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "16/May/15 08:28",
        "Updated": "02/Jun/15 04:52",
        "Resolved": "02/Jun/15 04:29",
        "Description": "Currently, the FetchNodes are created even if the NutchServer is not used causing memory exceptions. This patch makes the fetcher report to the FetchNodeDb only if the crawl is invoked from the REST service (ie NutchServer)",
        "Issue Links": []
    },
    "NUTCH-2016": {
        "Key": "NUTCH-2016",
        "Summary": "Remove unused class OldFetcher",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "18/May/15 09:37",
        "Updated": "26/Jun/15 19:35",
        "Resolved": "25/Jun/15 18:50",
        "Description": "The class OldFetcher is not actively maintained and lacks all features added to the new threaded Fetcher (started in 2007, used as default fetcher since 2009). Time to remove it from the code base (trunk/1.x only)?",
        "Issue Links": []
    },
    "NUTCH-2017": {
        "Key": "NUTCH-2017",
        "Summary": "Remove debug log from MimeUtil",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "18/May/15 09:54",
        "Updated": "08/Jun/15 21:50",
        "Resolved": "08/Jun/15 20:53",
        "Description": "My patch for NUTCH-1991 contained debug logs on level WARN (sorry, should definitely check the patch files before uploading them ). Needless warnings are now shown:\n\n2015-05-13 16:37:58,396 WARN  util.MimeUtil - >>null",
        "Issue Links": []
    },
    "NUTCH-2018": {
        "Key": "NUTCH-2018",
        "Summary": "Ensure that the Docker containers for Nutch 2.X are part of the Release Management Documentation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "docker,                                            documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "21/May/15 17:46",
        "Updated": "23/Sep/15 01:18",
        "Resolved": "23/Sep/15 01:18",
        "Description": "We need to ensure that the new docker containers which live within [https://github.com/apache/nutch/tree/2.x/docker|the docker package] are functional and working when making releases. This means documenting how the code should be updated prior to a release. This work is essential to keep them working.",
        "Issue Links": []
    },
    "NUTCH-2019": {
        "Key": "NUTCH-2019",
        "Summary": "ClassPathException sending topN argument for /job/create using Nutch 2.x RESTApi",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3.1",
        "Component/s": "generator",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "21/May/15 18:24",
        "Updated": "26/May/15 16:41",
        "Resolved": "26/May/15 15:42",
        "Description": "As described [http://www.mail-archive.com/user%40nutch.apache.org/msg13549.html|here], the issues is as follows\nLooking at the source I think it was caused by the following line:\ntopN = (Long) args.get(Nutch.ARG_TOPN);\nI managed to get it temporarily \"fixed\" by replacing the above with the following:\ntry{\n  topN = (Long) args.get(Nutch.ARG_TOPN);\n} catch(Exception e) {\n  topN = Long.parseLong(args.get(Nutch.ARG_TOPN).toString());\n  LOG.warn(\"Error: \" + e + \"test: \" + args.get(Nutch.ARG_TOPN));\n}\nI guess it complains about ClassCastException because JSON stores the argument as int or string and not long.\nRegards,\nAlex",
        "Issue Links": []
    },
    "NUTCH-2020": {
        "Key": "NUTCH-2020",
        "Summary": "Establish Butch - the Continuous Benchmarking Evaluation for Nutch",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.4,                                            1.11",
        "Fix Version/s": "None",
        "Component/s": "deployment",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "21/May/15 21:14",
        "Updated": "20/Jun/16 02:12",
        "Resolved": null,
        "Description": "I would like to initiate something I've provisionally called BUTCH wit the aim of providing a continuous benchmarking evaluation for Nutch. \nI wrote a utility script called [nipt](https://github.com/lewismc/nipt/blob/master/bootstrap.sh) which essentially pulls the top 1M URL's from Alexa, does some simple reformatting using sed and provides us with a flat file containing the top 1M URLs.\nLoads of these are obviously porn (and god knows whatever else) related so I would not advise injecting this garbage into any crawldb that you own or administer.\nI want to augment the [Benchmark tool](https://github.com/apache/nutch/blob/trunk/src/java/org/apache/nutch/tools/Benchmark.java) to imitate injecting the script and fetching the URLs. Essentially this could run continuously with us sending results to the dev@ list or making them available via some GUI.\nThe first step is for me to code this up. The second stage is for me to get Apache Infra to provide us with some nice machines (courtesy of Rackspace) which can host this for us.",
        "Issue Links": []
    },
    "NUTCH-2021": {
        "Key": "NUTCH-2021",
        "Summary": "Use protocol-selenium to Capture Screenshots of the Page as it is Fetched",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/May/15 00:07",
        "Updated": "22/Jul/15 20:44",
        "Resolved": "22/Jul/15 04:10",
        "Description": "This should be a piece of cake. It can be done as follows\n\nWebDriver driver = new FirefoxDriver();\ndriver.get(\"http://www.google.com/\");\nFile scrFile = ((TakesScreenshot)driver).getScreenshotAs(OutputType.FILE);\n// Now you can do whatever you need to do with it, for example copy somewhere\nFileUtils.copyFile(scrFile, new File(\"/usr/local/pics/screenshot.png\"));",
        "Issue Links": []
    },
    "NUTCH-2022": {
        "Key": "NUTCH-2022",
        "Summary": "Investigate better documentation for the Nutch REST API's",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "2.4,                                            1.11",
        "Component/s": "REST_api",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/May/15 17:40",
        "Updated": "27/Jun/16 19:44",
        "Resolved": "27/Jun/16 19:44",
        "Description": "Over on Apache Tika we use Miredot for better representation of the Tika REST API.\nBased on recent development on both 1.X and 2.x REST API's, it would be nice to have a better interface for people to see.\nAn example of Miredot REST API docs can be seen on Tika REST API docs",
        "Issue Links": [
            "/jira/browse/NUTCH-1756"
        ]
    },
    "NUTCH-2023": {
        "Key": "NUTCH-2023",
        "Summary": "/admin does not exist on the Nutch 1.X REST API",
        "Type": "Wish",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "22/May/15 18:13",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "The /admin endpoint does not exist on the Nutch 1.X endpoint.\nWhen I ping it I get\n\n    Status Code: 404 Not Found\n    Content-Length: 0\n    Date: Fri, 22 May 2015 18:06:04 GMT\n    Server: Jetty(8.1.15.v20140411)\n\n\nWe should implement it.",
        "Issue Links": []
    },
    "NUTCH-2024": {
        "Key": "NUTCH-2024",
        "Summary": "httpcore classpath jar conflict when invoking protocol-selenium",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "27/May/15 23:21",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "22/Nov/19 15:45",
        "Description": "I fear that there is a classpath issue right now with httpcore when protocol-selenium is invoked and used.\nThe error I get is something similar to the following\n\nCaused by: java.lang.NoSuchFieldError: INSTANCE\n    at org.apache.http.impl.io.DefaultHttpRequestWriterFactory.<init>(DefaultHttpRequestWriterFactory.java:52)\n    at org.apache.http.impl.io.DefaultHttpRequestWriterFactory.<init>(DefaultHttpRequestWriterFactory.java:56)\n    at org.apache.http.impl.io.DefaultHttpRequestWriterFactory.<clinit>(DefaultHttpRequestWriterFactory.java:46)\n    at org.apache.http.impl.conn.ManagedHttpClientConnectionFactory.<init>(ManagedHttpClientConnectionFactory.java:72)\n    at org.apache.http.impl.conn.ManagedHttpClientConnectionFactory.<init>(ManagedHttpClientConnectionFactory.java:84)\n    at org.apache.http.impl.conn.ManagedHttpClientConnectionFactory.<clinit>(ManagedHttpClientConnectionFactory.java:59)\n    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager$InternalConnectionFactory.<init>(PoolingHttpClientConnectionManager.java:487)\n    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.<init>(PoolingHttpClientConnectionManager.java:147)\n    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.<init>(PoolingHttpClientConnectionManager.java:136)\n    at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.<init>(PoolingHttpClientConnectionManager.java:112)\n    at org.apache.http.impl.client.HttpClientBuilder.build(HttpClientBuilder.java:726)\n\n\nI managed to workaround this by removing all httpcore dependencies which are older than httpcore-4.3.2.jar which is required by protocol-selenium.\nI need to look into a more appropriate fix... possibly an upgrade of the httpcore libraries which are peppered around everywhere.\nWe will see.",
        "Issue Links": []
    },
    "NUTCH-2025": {
        "Key": "NUTCH-2025",
        "Summary": "Create Conda Package as part of Nutch Release Management Cycle",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "build,                                            documentation",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "28/May/15 00:33",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Anaconda is a widely used Python distribution from Continuum Analytics. Conda, the package manager, is an excellent way for us to get Nutch to a wider audience and a number of heros from Continuum have already been implementing conda packages for Nutch.\nMy goal for this issue is to make the generation and automated deployment of conda packages to Binstar part of the Nutch release management cycle.",
        "Issue Links": []
    },
    "NUTCH-2026": {
        "Key": "NUTCH-2026",
        "Summary": "Crawl endpoint for the REST api",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "31/May/15 05:59",
        "Updated": "31/May/15 05:59",
        "Resolved": null,
        "Description": "An endpoint for the REST api to run the Nutch crawl like the command line option  bin/crawl",
        "Issue Links": []
    },
    "NUTCH-2027": {
        "Key": "NUTCH-2027",
        "Summary": "seed list REST endpoint for Nutch 1.10",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Asitang Mishra",
        "Created": "31/May/15 06:02",
        "Updated": "07/Jun/15 16:31",
        "Resolved": "07/Jun/15 16:31",
        "Description": "The endpoint for Nutch 1.10 that enables the user to set the seedlist for the REST api with a REST call.",
        "Issue Links": []
    },
    "NUTCH-2028": {
        "Key": "NUTCH-2028",
        "Summary": "java.lang.IllegalArgumentException: can't serialize class org.apache.avro.util.Utf8",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3.1",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Roman P",
        "Created": "31/May/15 16:33",
        "Updated": "20/Sep/15 12:56",
        "Resolved": "20/Sep/15 12:56",
        "Description": "Compiled Nutch 2.3 with MongoDB as a persistence. Getting exception when fetching. Searched for similar errors online, noticed that this issue was addressed in gora 0.6. Tried recompiling with 0.6 but then getting different exception, seems that it's incompatible with hadoop 1.2.0. Tried different versions of hadoop with no luck.\nFetcherJob: starting at 2015-05-31 09:29:04\nFetcherJob: batchId: all\nFetcherJob: threads: 10\nFetcherJob: parsing: false\nFetcherJob: resuming: false\nFetcherJob : timelimit set for : -1\njava.lang.IllegalArgumentException: can't serialize class org.apache.avro.util.Utf8\n\tat org.bson.BasicBSONEncoder._putObjectField(BasicBSONEncoder.java:284)\n\tat org.bson.BasicBSONEncoder.putObject(BasicBSONEncoder.java:185)\n\tat org.bson.BasicBSONEncoder.putObject(BasicBSONEncoder.java:131)\n\tat com.mongodb.DefaultDBEncoder.writeObject(DefaultDBEncoder.java:33)\n\tat com.mongodb.OutMessage.putObject(OutMessage.java:289)\n\tat com.mongodb.OutMessage.writeQuery(OutMessage.java:211)\n\tat com.mongodb.OutMessage.query(OutMessage.java:86)\n\tat com.mongodb.DBCollectionImpl.find(DBCollectionImpl.java:81)\n\tat com.mongodb.DBCollectionImpl.find(DBCollectionImpl.java:66)\n\tat com.mongodb.DBCursor._check(DBCursor.java:458)\n\tat com.mongodb.DBCursor._hasNext(DBCursor.java:546)\n\tat com.mongodb.DBCursor.hasNext(DBCursor.java:571)\n\tat org.apache.gora.mongodb.query.MongoDBResult.nextInner(MongoDBResult.java:69)\n\tat org.apache.gora.query.impl.ResultBase.next(ResultBase.java:114)\n\tat org.apache.gora.mapreduce.GoraRecordReader.nextKeyValue(GoraRecordReader.java:119)\n\tat org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:531)\n\tat org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:364)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:223)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)",
        "Issue Links": []
    },
    "NUTCH-2029": {
        "Key": "NUTCH-2029",
        "Summary": "Mark.checkMark returns empty string when null is expected with mongodb storage",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3.1",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "Alexander Yastrebov",
        "Created": "01/Jun/15 15:45",
        "Updated": "16/Sep/15 04:33",
        "Resolved": "16/Sep/15 04:30",
        "Description": "Gora mongodb backend returns empty Utf8 for null field values.\nThis leads to Mark.checkMark never returns null, so multiple checks on null fail e.g. in DbUpdateMapper, GeneratorMapper etc.\nTemporal fix is to check whether value is null or empty\nSee patch\nhttps://github.com/newpointer/nutch/commit/4bdc0bab39ede7c01ee057c3429f5f0e90d5e48f\nand related gora issue\nhttps://issues.apache.org/jira/browse/GORA-423",
        "Issue Links": [
            "/jira/browse/NUTCH-2009",
            "/jira/browse/GORA-423"
        ]
    },
    "NUTCH-2030": {
        "Key": "NUTCH-2030",
        "Summary": "ParseZip plugin is not able to extract language from zip document,this could solve that problem.",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Eyeris Rodriguez Rueda",
        "Created": "02/Jun/15 13:19",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Actually parse-zip plugin don\u00b4t extract language from zip document, therefore lang field is empty in solr or elastic. If the package(.zip) contains a list of documents so the lang field could be multivalued to support that list of languages. A simple change to parse-zip pluging could fix this problem. I will use Language Identifier class from tika and analyze each document inside.",
        "Issue Links": []
    },
    "NUTCH-2031": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Create Admin End point for Nutch 1.x REST service",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "02/Jun/15 19:01",
        "Updated": "03/Jun/15 05:02",
        "Resolved": "03/Jun/15 04:06",
        "Description": "This addresses the server administration endpoint for the REST service.",
        "Issue Links": []
    },
    "NUTCH-2032": {
        "Key": "NUTCH-2032",
        "Summary": "Plugin to index the raw content of a readable document.",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "indexer,                                            parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Luis Lopez",
        "Created": "03/Jun/15 19:01",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "22/Nov/19 15:42",
        "Description": "This is related to https://issues.apache.org/jira/browse/NUTCH-1785 and \nhttps://issues.apache.org/jira/browse/NUTCH-1458\nWe created a couple plugins to index the raw content of readable documents. If we include these plugins in the plugin chain we'll index the raw content of a readable document, i.e. XML, HTML, CSV, TXT etc. The index-rawcontent plugin is not designed to index binary files, however having the full content of an HTML/XML or a CSV document is really critical for some of us.",
        "Issue Links": [
            "/jira/browse/NUTCH-1785",
            "/jira/browse/NUTCH-1785"
        ]
    },
    "NUTCH-2033": {
        "Key": "NUTCH-2033",
        "Summary": "parse-tika skips valid documents.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.20",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Luis Lopez",
        "Created": "03/Jun/15 19:14",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "If we run:\n\nbin/nutch parsechecker -dumpText http://ngdc.noaa.gov/geoportal/openSearchDescription\n\n\nwe\u2019ll get:\n\nStatus: failed(2,0): Can't retrieve Tika parser for mime-type application/opensearchdescription+xml\n\n\nthe same occurs  for:\n\nbin/nutch parsechecker -dumpText http://petstore.swagger.io/v2/swagger.json\n\n\nBoth perfectly valid documents if they were returned as \"application/xml\" and \"text/plain\" respectively. \nThis happens because parse-tika uses the mime type to retrieve a suitable parser, some composite mime types are not included in this list even though they are perfectly valid and parsable documents. This not taking into account that servers often return incorrect mime types for the documents requested.\nWe created a helper class as a workaround for this issue. The class uses regex expressions to define synonyms. In the first case any mime type that matches \"application/(.*)+xml\" will be replaced by \"application/xml\". This way parse-tika will parse the document just fine.",
        "Issue Links": []
    },
    "NUTCH-2034": {
        "Key": "NUTCH-2034",
        "Summary": "CrawlDB filtered documents counter.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.14",
        "Component/s": "crawldb",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Luis Lopez",
        "Created": "03/Jun/15 19:20",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "17/Dec/17 15:16",
        "Description": "When we are doing big crawls we would like to know how many of the URLs are being discarded by the regex filters, this is only presented in the Inject class:\nInjector: Total number of urls rejected by filters: 0\nIt will be nice to have a counter in the CrawlDB class so we know in every round how many were discarded by our filters:\nCrawlDb update: Total number of URLs filtered by regex filters: 31415",
        "Issue Links": []
    },
    "NUTCH-2035": {
        "Key": "NUTCH-2035",
        "Summary": "Regex filter using case sensitive rules.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Luis Lopez",
        "Created": "03/Jun/15 19:26",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "15/Dec/17 16:52",
        "Description": "Regex expressions are computationally expensive and having \u201cEXE|exe|JPG|jpg\u201d etc etc..... adds up if we use complex rules.\nRegex filter should use case insensitive rules to make the rules more readable and improve performance.",
        "Issue Links": []
    },
    "NUTCH-2036": {
        "Key": "NUTCH-2036",
        "Summary": "Adding some continuous crawl goodies to the crawl script",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "bin,                                            tool,                                            util",
        "Assignee": null,
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "04/Jun/15 12:50",
        "Updated": "26/Jun/15 19:36",
        "Resolved": "25/Jun/15 13:56",
        "Description": "Although Nutch does not support continuous crawling out of the box, and yes this is somehow doable using cron or even sometimes irrelevant due the size of the crawl its a nice feature to have. \nThis patch basically just adds a new parameter option to the bin/crawl script (w|-wait) which adds a time to wait if the generator returns 0 (when no URLs are scheduled for fetching). \nThis new parameter has the NUMBER[SUFFIX] format, if no suffix is provided the amount of time is assumed to be in seconds. Other valid suffixes are: \ns - second\nm - minutes\nh - hours\nd - days\nIf a -1 value is passed to the parameter or its not used at all the default behaviour of exciting the script is used.",
        "Issue Links": []
    },
    "NUTCH-2037": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Job endpoint to support Indexing from the REST API",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "08/Jun/15 00:02",
        "Updated": "09/Jun/15 06:51",
        "Resolved": "09/Jun/15 06:02",
        "Description": "The job administration end point will now support indexing through this patch. The documentation of how to run this is on the Nutch REST API wiki. User defined indexer can be configured through the Configuration endpoint.",
        "Issue Links": []
    },
    "NUTCH-2038": {
        "Key": "NUTCH-2038",
        "Summary": "Naive Bayes classifier based html Parse filter (for filtering outlinks)",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher,                                            injector,                                            parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Asitang Mishra",
        "Created": "10/Jun/15 17:37",
        "Updated": "01/Jul/15 15:43",
        "Resolved": "01/Jul/15 06:23",
        "Description": "A html parse filter that will filter out the outlinks in two stages. \nClassify the parse text and decide if the parent page is relevant. If relevant then don't filter the outlinks. If irrelevant then go thru each outlink and see if the url contains any of the important words from a list. If it does then let it pass.",
        "Issue Links": [
            "/jira/browse/NUTCH-2056",
            "/jira/browse/NUTCH-2057"
        ]
    },
    "NUTCH-2039": {
        "Key": "NUTCH-2039",
        "Summary": "Relevance based scoring filter",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "11/Jun/15 04:28",
        "Updated": "19/Jun/15 19:15",
        "Resolved": "19/Jun/15 03:20",
        "Description": "A ScoringFilter plugin that uses a similarity measure to calculate the similarity between a given page(gold standard) and the currently parsed page. The score obtained from this similarity is then distributed to its outlinks. This filter aims to focus the crawler to crawl/explore relevant pages.",
        "Issue Links": []
    },
    "NUTCH-2040": {
        "Key": "NUTCH-2040",
        "Summary": "Upgrade to recent version of Crawler-Commons",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Jun/15 19:41",
        "Updated": "12/Jun/18 16:43",
        "Resolved": "12/Jun/18 15:42",
        "Description": "Crawler Commons 0.6 was released. We should upgrade.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/346"
        ]
    },
    "NUTCH-2041": {
        "Key": "NUTCH-2041",
        "Summary": "indexer fails if linkdb is missing",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "indexer,                                            linkdb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Jun/15 14:33",
        "Updated": "25/Jun/15 20:16",
        "Resolved": "25/Jun/15 19:08",
        "Description": "If the linkdb is missing the indexer fails with\n\n2015-06-17 12:52:10,621 ERROR ...cause:org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: .../linkdb/current\n\n\nIf both db.ignore.internal.links and db.ignore.external.links there will be no LinkDb even if \"invertlinks\" is run (as consequence of NUTCH-1913). The script \"bin/crawl\" does not know about the values of these two properties and calls indexer with \"-linkdb .../linkdb\" which will then fail.\nSince \"bin/crawl\" is agnostic to properties defined in nutch-site.xml we solution similar to NUTCH-1854: make the tool/job more tolerant and log a warning instead of raising an error.",
        "Issue Links": []
    },
    "NUTCH-2042": {
        "Key": "NUTCH-2042",
        "Summary": "parse-html increase chunk size used to detect charset",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "2.3.1,                                            1.12",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "18/Jun/15 15:16",
        "Updated": "08/Dec/15 23:00",
        "Resolved": "08/Dec/15 21:46",
        "Description": "The chunk used to detect the encoding of a document is set to 2000 bytes. Although it is definitely best practice to \"define\" the character set on top, 2000 bytes are sometimes not enough: 20 longer <link> elements pointing to javascript and css libs may \"hide\" the <meta> element containing content type and encoding. Same problem has been observed in TIKA-357 and solved by increasing the buffer size to 8 kB.",
        "Issue Links": []
    },
    "NUTCH-2043": {
        "Key": "NUTCH-2043",
        "Summary": "Interface and high level design for classification using models",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            injector,                                            parser",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "18/Jun/15 18:02",
        "Updated": "18/Jun/15 18:08",
        "Resolved": null,
        "Description": "To discuss and come up with a high level design or an interface for classification based on a model in nutch. This should be as general as possible, so that we can decide on a design of the model, data transformation and feature extraction for different kinds of classifications and create a common development ground for all.",
        "Issue Links": []
    },
    "NUTCH-2044": {
        "Key": "NUTCH-2044",
        "Summary": "Support for an expanded HttpHeaders list",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "metadata",
        "Assignee": null,
        "Reporter": "Soren Scott",
        "Created": "18/Jun/15 21:50",
        "Updated": "20/Jul/15 16:30",
        "Resolved": "20/Jul/15 16:30",
        "Description": "Is there currently any consideration for either a) expanding the current HttpHeaders list from HttpHeaders.java to include at least the current permanent or provisional headers or b) revising that handler to iterate some unknown KVP for the headers? Either as a configurable widget or something along those lines?\nI am mostly interested in the Accept headers to help inform some additional actions on the fetched responses but even from an accurate assessment of the crawls, the full set of headers provided by a request is important. I know that we frown on non-standard keys but, again, imperfect world .",
        "Issue Links": []
    },
    "NUTCH-2045": {
        "Key": "NUTCH-2045",
        "Summary": "index-basic incorrect assignment of next fetch time (page.getFetchTime()) as page fetch time",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3.1",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Jun/15 18:31",
        "Updated": "22/Dec/16 06:28",
        "Resolved": "23/Jun/15 22:32",
        "Description": "The issue here as flagged up when using indexer-elastic plugin where the page fetch time is incorrectly assigned as the NEXT fetch time as oppose to the time at which the page was actually fetched (prevFetchTime).\nThe ML thread for this issue can be found below\nhttp://www.mail-archive.com/user%40nutch.apache.org/msg13661.html",
        "Issue Links": []
    },
    "NUTCH-2046": {
        "Key": "NUTCH-2046",
        "Summary": "The crawl script should be able to skip an initial injection.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.14",
        "Component/s": "crawldb,                                            injector",
        "Assignee": "Julien Nioche",
        "Reporter": "Luis Lopez",
        "Created": "24/Jun/15 16:13",
        "Updated": "18/Apr/17 18:58",
        "Resolved": "18/Apr/17 18:07",
        "Description": "When our crawl gets really big a new injection takes considerable time as it updates crawldb, the crawl script should be able to skip the injection and go directly to the generate call.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/161"
        ]
    },
    "NUTCH-2047": {
        "Key": "NUTCH-2047",
        "Summary": "Improvements to the relevance scoring plugin",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "scoring",
        "Assignee": null,
        "Reporter": "Sujen Shah",
        "Created": "24/Jun/15 22:58",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "To discuss the results and improvements on the scoring-similarity plugin using the cosine similarity model. \nCurrently, the outlinks are distributed the same score as the parent URL. Which means an irrelevant URL(with a relevant parent) would be fetched for one more round before it gets a lower score and filtered. So we would require one additional fetch/parse to score these irrelevant urls(from relevant parents) lower. \nAny suggestions on this are appreciated.",
        "Issue Links": []
    },
    "NUTCH-2048": {
        "Key": "NUTCH-2048",
        "Summary": "parse-tika: fix dependencies in plugin.xml",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "26/Jun/15 10:15",
        "Updated": "04/Aug/15 22:13",
        "Resolved": "04/Aug/15 21:27",
        "Description": "Duplicate library dependencies listed in parse-tika's plugin.xml should be cleaned up. There are a duplicates, only the version differs, e.g.:\n\ntika-parsers-1.7.jar\ntika-parsers-1.8.jar\n\n\nNot critical because libs which are not present should be just ignored.",
        "Issue Links": []
    },
    "NUTCH-2049": {
        "Key": "NUTCH-2049",
        "Summary": "Upgrade Trunk to Hadoop > 2.4 stable",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Jun/15 19:32",
        "Updated": "24/Aug/15 21:03",
        "Resolved": "24/Aug/15 17:58",
        "Description": "Convo here - http://www.mail-archive.com/dev%40nutch.apache.org/msg18225.html\nI am +1 for taking trunk (or a branch of trunk) to explicit dependency on > Hadoop 2.6.\nWe can run our tests, we can validate, we can fix.\nI will be doing validation on 2.X in paralegal as this is what I use on my own projects.",
        "Issue Links": [
            "/jira/browse/NUTCH-1712",
            "/jira/browse/NUTCH-1936"
        ]
    },
    "NUTCH-2050": {
        "Key": "NUTCH-2050",
        "Summary": "Upgrade HBase and Hadoop versioning on 2.X HBase Docker",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.1",
        "Component/s": "docker",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Jun/15 22:22",
        "Updated": "20/Sep/15 13:40",
        "Resolved": "20/Sep/15 12:55",
        "Description": "We are working on old versioning.\nLets sort this out.\n2.X works perfectly with Hadoop 2.X.",
        "Issue Links": [
            "/jira/browse/NUTCH-1946"
        ]
    },
    "NUTCH-2051": {
        "Key": "NUTCH-2051",
        "Summary": "Add 'publish-local-m2' ant target for creating local maven artifacts",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Matt DeBoer",
        "Created": "27/Jun/15 07:22",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "This patch adds the capability to publish the nutch jar to the user's local m2 repository; this is useful in case you want to build the latest SNAPSHOT locally, and use it as a maven dependency in another project.\nUse `ant publish-local-m2` to trigger it.",
        "Issue Links": []
    },
    "NUTCH-2052": {
        "Key": "NUTCH-2052",
        "Summary": "Enhance index-static to allow configurable delimiters",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Peter Ciuffetti",
        "Created": "29/Jun/15 17:16",
        "Updated": "05/Jul/15 03:08",
        "Resolved": "05/Jul/15 03:08",
        "Description": "The index-static plugin has a set of fixed-value delimiters that control the parsing of the property index.static.\ncomma is used to separate fields\ncolon is used to separate field name from field value\nspace is used to separate multiple values in the field\nThis set of choices makes it impossible to have a fixed field value containing a space, comma or colon.\nThe proposed enhancement is to allow configuration properties to override any of these defaults.\nindex.static.fieldsep (default comma)\nindex.static.keysep (default colon)\nindex.static.valuesep (default space)",
        "Issue Links": [
            "/jira/browse/NUTCH-1464",
            "/jira/browse/NUTCH-2059",
            "/jira/browse/NUTCH-940",
            "/jira/browse/NUTCH-1197"
        ]
    },
    "NUTCH-2053": {
        "Key": "NUTCH-2053",
        "Summary": "Uncessary dependencies included in ivy.xml (post NUTCH-2038)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "01/Jul/15 03:39",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Currently in trunk we have an unnecessary dependency included within ivy/ivy.xml\nhttps://github.com/apache/nutch/blob/trunk/ivy/ivy.xml#L99-L101\nThis needs to be removed.\nasitang can you please provide context as to why this is OK? I don't want to break your code so sorry for lack of understanding. Thanks",
        "Issue Links": []
    },
    "NUTCH-2054": {
        "Key": "NUTCH-2054",
        "Summary": "When Using Form Auth settings can not read response body",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Talat Uyarer",
        "Reporter": "Talat Uyarer",
        "Created": "01/Jul/15 12:11",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "If server uses compressed http connection. The original patch can not read content.",
        "Issue Links": []
    },
    "NUTCH-2055": {
        "Key": "NUTCH-2055",
        "Summary": "Random Crawl Delay",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Talat Uyarer",
        "Created": "01/Jul/15 13:53",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Some Firewalls can block that request with same delay time. I create a patch for random crawl delay between 0 and max Crawl Delay settings.",
        "Issue Links": []
    },
    "NUTCH-2056": {
        "Key": "NUTCH-2056",
        "Summary": "Move the Mahout and Lucene dependencies to the plugin from the main ivy.xml for the Naive Bayes Parse Filter (NUTCH-2038)",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Asitang Mishra",
        "Created": "01/Jul/15 15:29",
        "Updated": "03/Aug/15 21:54",
        "Resolved": null,
        "Description": "Move the Mahout and Lucene dependencies to the plugin from the main ivy.xml for the Naive Bayes Parse Filter (NUTCH-2038). We were facing classNotFoundException while doing it.",
        "Issue Links": [
            "/jira/browse/NUTCH-1486",
            "/jira/browse/NUTCH-2038"
        ]
    },
    "NUTCH-2057": {
        "Key": "NUTCH-2057",
        "Summary": "Put all the files produced during training of the model for Naive Bayes classifier, in the Naive Bayes Parse Filter (NUTCH-2038), in a single folder",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "01/Jul/15 15:32",
        "Updated": "01/Jul/15 15:43",
        "Resolved": null,
        "Description": "Put all the files produced during training of the model for Naive Bayes classifier, in the Naive Bayed Parse Filter (NUTCH-2038), in a single folder. The training produces many files in the local directory. Should be a single folder say \"model\" that contains all these files.",
        "Issue Links": [
            "/jira/browse/NUTCH-2038"
        ]
    },
    "NUTCH-2058": {
        "Key": "NUTCH-2058",
        "Summary": "Indexer plugin that allows RegEx replacements on the NutchDocument field values",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "indexer,                                            parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Peter Ciuffetti",
        "Created": "02/Jul/15 20:40",
        "Updated": "20/Nov/15 18:31",
        "Resolved": "20/Nov/15 18:31",
        "Description": "This is the description of a IndexingFilter plugin I'm developing that allows regex replacements on field values prior to indexing to your search engine.\nPlugin name: index-replace\nProperty name: index.replace.regexp\nUse case example:\nI'm indexing Nutch-created documents to a pre-existing SOLR core.  In this case I need to coerce the documents into the schema and field formats expected by the existing core.  The features of index-static and solrindex-mapping.xml get me most of the way.  Among other things, I need to generate identifiers from the web URLs.  So I need to do something like a regex replace on the id provided and then (with solrindex-mapping.xml) move this to the field name defined by the existing core.\nAnother use case might be to refactor all URLs stored in the document so they route through a redirector gateway.\nThe following is from the draft description in nutch-default.xml\nDescription:\nAllows indexing-time regexp replace manipulation of metadata fields. The format of the property is a list of regexp replacements, one line per field being modified.  To use this property, add index-replace to your list of activated plugins.\nExample:\n\n<property>\n  <name>index.replace.regexp</name>\n  <value>\n        fldname1=/regexp/replacement/flags\n        fldname2=/regexp/replacement/flags\n  </value>\n</property>\n\n\nField names would be one of those from https://wiki.apache.org/nutch/IndexStructure. The replacements will happen in the order listed. If a field needs multiple replacement operations they may be listed more than once.\nThe field name precedes the equal sign.  The first character after the equal sign signifies the delimiter for the regexp, the replacement value and the flags.\nThe regexp and the optional flags should correspond to Pattern.compile(String regexp, int flags) defined here: http://docs.oracle.com/javase/7/docs/api/java/util/regex/Pattern.html#compile%28java.lang.String,%20int%29\nThe flags is an integer sum of the flag values defined in http://docs.oracle.com/javase/7/docs/api/constant-values.html (Sec: java.util.regex.Pattern)\nPatterns are compiled when the plugin is initialized for efficiency.\nEscaping: since the regexp is being read from a config file, any escaped values must be double escaped.  Eg:  \n\n  id=/\\\\s+//\n\n\n will cause the escaped \\s+ match pattern to be used.\nThe replacement value should correspond to Java Matcher(CharSequence input).replaceAll(String replacement):  http://docs.oracle.com/javase/7/docs/api/java/util/regex/Matcher.html#replaceAll%28java.lang.String%29\nMulti-valued Fields\nIf a field has multiple values, the replacement will be applied to each value in turn.\nNon-string Datatypes\nReplacement is possible only on String field datatypes.  If the field you name in the property is not a String datatype, it will be silently ignored.\nHost and URL specific replacements\nIf the replacements should apply only to specifc pages, then add a sequence like\n\n    hostmatch=hostmatchpattern\n    fld1=/regexp/replace/flags\n    fld2=/regexp/replace/flags\n\n\n    or\n\n    urlmatch=urlmatchpattern\n    fld1=/regexp/replace/flags\n    fld2=/regexp/replace/flags\n\n\nWhen using Host and URL replacements, all replacements preceding the first hostmatch or urlmatch will apply to all Nutch documents.  Replacements following a hostmatch or urlmatch will be applied to Nutch documents that match the host or url field (up to the next hostmatch or urlmatch line).  hostmatch and urlmatch patterns must be unique in this property.",
        "Issue Links": []
    },
    "NUTCH-2059": {
        "Key": "NUTCH-2059",
        "Summary": "protocol-httpclient, protocol-http unit test errors on Jenkins",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Peter Ciuffetti",
        "Created": "04/Jul/15 20:27",
        "Updated": "08/Aug/15 20:54",
        "Resolved": "08/Aug/15 20:29",
        "Description": "This is an occasional error on the build of the Nutch trunk visible in Jenkins builds.  It happens on either protocol-http or protocol-httpclient, which can be running at the same time given the multi-threaded test setup.\n\n[junit] Running org.apache.nutch.protocol.httpclient.TestProtocolHttpClient\n[junit] Tests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 2.377 sec\n[junit] Test org.apache.nutch.protocol.http.TestProtocolHttp FAILED\n\n\nEvidence of failure on Jenkins go back to\nFailed > Console Output  #3154 \tJun 8, 2015 4:00:00 AM\nhttps://builds.apache.org/view/All/job/Nutch-trunk/3154/consoleFull\nAnd are repeated at...\nhttps://builds.apache.org/view/All/job/Nutch-trunk/3190/console\nhttps://builds.apache.org/view/All/job/Nutch-trunk/3189/console\nSome possibly related tickets\nNUTCH-1836 Timeouts in protocol-httpclient when crawling same host with >2 threads \nNUTCH-1086 Rewrite protocol-httpclient\nThe unit tests are not failing for me on my sandbox, but there are some exceptions being output to the log related to headers being sent on JSP pages after the response writer is invoked.\n\njava.lang.IllegalStateException: STREAM\n        at org.mortbay.jetty.Response.getWriter(Response.java:616)",
        "Issue Links": [
            "/jira/browse/NUTCH-1086",
            "/jira/browse/NUTCH-2052"
        ]
    },
    "NUTCH-2060": {
        "Key": "NUTCH-2060",
        "Summary": "dedup is removing entries with status db_gone",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Steven Hayles",
        "Created": "07/Jul/15 07:55",
        "Updated": "02/Mar/16 22:36",
        "Resolved": null,
        "Description": "Using the standard bin/crawl script, Solr is never informed when a previously indexed document has been deleted.\n\"bin/nutch update\" sets db_gone status in the crawl db for requests returning HTTP 404 status.\n\"bin/nutch dedup\" remove entries with status db_gone from the crawl db .\nAs a result \"bin/nutch clean\" never sees the db_gone status, so does not inform Solr.",
        "Issue Links": []
    },
    "NUTCH-2061": {
        "Key": "NUTCH-2061",
        "Summary": "Make core upgrades to all org.apache.httpcomponents dependencies",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jul/15 21:48",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "We are running into more and more issues with regards to our use of the older httpcomponents versions. We need to upgrade as this is becoming more and more of a problem with various plugins using the httpcore, httpclient, etc. libraries as well as our usage of these libraries within the core codebase.",
        "Issue Links": [
            "/jira/browse/NUTCH-1486"
        ]
    },
    "NUTCH-2062": {
        "Key": "NUTCH-2062",
        "Summary": "Add Plugin for interacting with Selenium WebDriver",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "plugin",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Michael Joyce",
        "Created": "20/Jul/15 15:57",
        "Updated": "02/Aug/15 23:54",
        "Resolved": "02/Aug/15 23:39",
        "Description": "The protocol-selenium plugin is great for pulling webpages that dynamically load content. However, I've run into use cases where I need to actively interact with a page in Selenium before it becomes useful. For instance, I may need to paginate through a table to get all results that I'm interested in. This plugin will handle that use case.",
        "Issue Links": []
    },
    "NUTCH-2063": {
        "Key": "NUTCH-2063",
        "Summary": "Add -mimeStats flag to FileDumper tool",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "dumpers",
        "Assignee": "Michael Joyce",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Jul/15 20:29",
        "Updated": "22/Jul/15 16:59",
        "Resolved": "22/Jul/15 12:51",
        "Description": "Right now in order to get a MimeType distribution for any given number of segments, one is required to dump some data. This is a waste if one just wishes to see the mime type distribution across a number of segments.\nAn improvement to the FileDumper tool would be the addition of a -mimeStats flag which would not attempt to dump any data but instead merely provide the total stats message providing insight into how the FileDumper should be best used.",
        "Issue Links": []
    },
    "NUTCH-2064": {
        "Key": "NUTCH-2064",
        "Summary": "URLNormalizer basic to encode reserved chars and decode non-reserved chars",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "21/Jul/15 14:11",
        "Updated": "10/Nov/15 12:09",
        "Resolved": "10/Nov/15 11:17",
        "Description": "NUTCH-1098 rewritten to work on trunk. Unit test is identical to 1098.",
        "Issue Links": []
    },
    "NUTCH-2065": {
        "Key": "NUTCH-2065",
        "Summary": "Domain URL filter to support protocols",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "21/Jul/15 15:08",
        "Updated": "23/Dec/15 09:22",
        "Resolved": "23/Dec/15 09:20",
        "Description": "The filter allows all protocols for all whitelisted domains, hosts or suffixes but it usually makes little sense to index both http and https URL's of the same domain. This is not unlike the host URL filter, which prevents indexing of duplicate hosts e.g. apache.org and www.apache.org.",
        "Issue Links": [
            "/jira/browse/NUTCH-2189",
            "/jira/browse/NUTCH-2190"
        ]
    },
    "NUTCH-2066": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Parameterize Generate REST endpoint",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "23/Jul/15 17:48",
        "Updated": "03/Aug/15 00:51",
        "Resolved": "03/Aug/15 00:02",
        "Description": "Allow user to specify crawldb and segment db in the Generate Job REST endpoint",
        "Issue Links": []
    },
    "NUTCH-2067": {
        "Key": "NUTCH-2067",
        "Summary": "HttpFormAuthentication unable to decode login page when server responds with GZIP encoding",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "patrick peck",
        "Created": "24/Jul/15 09:42",
        "Updated": "24/Jul/15 09:42",
        "Resolved": null,
        "Description": "The method org.apache.nutch.protocol.httpclient.HttpFormAuthentication#httpGetPageContent() which is used to download the login page when doing form authentication, fails to take into account that the response body may be gzip encoded which is possible given the fact that the Http.configureClient() method sets the Accept-Encoding header to \"x-gzip, gzip, deflate\".\nIt's also not possible to override the Accept-Encoding header, since it's overridden by the default (or, to be more exact: if you add an\n    <additionalPostHeaders>\n      <field name=\"Accept-Encoding\" value=\"identity\" />\n    </additionalPostHeaders>\nto the configuration, the http client sends out the Accept-Encoding header twice, first with the above configuration, second with the default configuration.)",
        "Issue Links": []
    },
    "NUTCH-2068": {
        "Key": "NUTCH-2068",
        "Summary": "Allow subcollection overrides via metadata",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Jul/15 09:47",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "16/Mar/17 10:52",
        "Description": "Similar to index-metdata but overrides subcollection. If both subcollection and index-metadata are active, you will get two values for the field possible causing multivalued field errors.",
        "Issue Links": []
    },
    "NUTCH-2069": {
        "Key": "NUTCH-2069",
        "Summary": "Ignore external links based on domain",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher,                                            parser",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "29/Jul/15 11:12",
        "Updated": "06/Apr/17 09:06",
        "Resolved": "20/Nov/15 16:23",
        "Description": "We currently have `db.ignore.external.links` which is a nice way of restricting the crawl based on the hostname. This adds a new parameter 'db.ignore.external.links.domain' to do the same based on the domain.",
        "Issue Links": [
            "/jira/browse/NUTCH-2365"
        ]
    },
    "NUTCH-2070": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Parameterize Fetch REST Endpoint",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "29/Jul/15 22:44",
        "Updated": "27/Oct/15 20:42",
        "Resolved": "27/Oct/15 20:42",
        "Description": "The user can manually send a segment to fetch by using the \"segment\" key in the args passed.",
        "Issue Links": []
    },
    "NUTCH-2071": {
        "Key": "NUTCH-2071",
        "Summary": "A parser failure on a single document may fail crawling job if parser.timeout=-1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.14,                                            1.15",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Arkadi Kosmynin",
        "Created": "30/Jul/15 06:11",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "17/Jul/18 11:30",
        "Description": "java.io.IOException: Job failed!\n        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1357)\n        at org.apache.nutch.parse.ParseSegment.parse(ParseSegment.java:213)\n        <...>\nCaused by: java.lang.IncompatibleClassChangeError: class org.apache.tika.parser.asm.XHTMLClassVisitor has interface org.objectweb.asm.ClassVisitor as super class\n                at java.lang.ClassLoader.defineClass1(Native Method)\n                at java.lang.ClassLoader.defineClass(ClassLoader.java:760)\n                at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n                at java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n                at java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n                at java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n                at java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n                at java.security.AccessController.doPrivileged(Native Method)\n                at java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n                at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n                at java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n                at org.apache.tika.parser.asm.ClassParser.parse(ClassParser.java:51)\n                at org.apache.nutch.parse.tika.TikaParser.getParse(TikaParser.java:98)\n                at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:103)\nSuggested fix in ParseUtil:\nReplace \n            if (maxParseTime!=-1)\n                       parseResult = runParser(parsers[i], content);\n            else \n                       parseResult = parsers[i].getParse(content);\nwith\n      try\n\n{\n            if (maxParseTime!=-1)\n                       parseResult = runParser(parsers[i], content);\n            else \n                       parseResult = parsers[i].getParse(content);\n      }\n catch( Throwable e )\n      {\n        LOG.warn( \"Parsing \" + content.getUrl() + \" with \" + parsers[i].getClass().getName() + \" failed: \" + e.getMessage() ) ;\n        parseResult = null ;\n      }",
        "Issue Links": [
            "/jira/browse/NUTCH-2378",
            "/jira/browse/NUTCH-2316",
            "https://github.com/apache/nutch/pull/358"
        ]
    },
    "NUTCH-2072": {
        "Key": "NUTCH-2072",
        "Summary": "Deflate encoding support is broken when http.content.limit is set to -1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Tanguy Moal",
        "Created": "30/Jul/15 09:22",
        "Updated": "02/Aug/15 23:54",
        "Resolved": "02/Aug/15 23:38",
        "Description": "The method DeflateUtils.inflateBestEffort(byte[] in, int sizeLimit) is not designed to have sizeLimit set to a negative value.\nThe fix can be simply to mimic what's done with gzip encoding : if getMaxContent() < 0 then use Integer.MAX_VALUE for the sizeLimit argument.",
        "Issue Links": []
    },
    "NUTCH-2073": {
        "Key": "NUTCH-2073",
        "Summary": "Unable to create index on elasticsearch through nutch",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "None",
        "Component/s": "nutchNewbie",
        "Assignee": null,
        "Reporter": "anurag kumar",
        "Created": "01/Aug/15 09:16",
        "Updated": "21/Jun/22 11:07",
        "Resolved": "21/Jun/22 11:07",
        "Description": "Nutch 2.3 + ElasticSearch 1.4 + HBase 0.94 Setup issue. \ni followed the https://gist.github.com/xrstf/b48a970098a8e76943b9 for set up nutch with elastic search, after integration data is store in hbase table 'webpage' but no index file is created on elasticsearch,  [docs = 0, length = 0, total docs = 0] .  i want to know how to display nutch crawl data in elasticsearch.",
        "Issue Links": []
    },
    "NUTCH-2074": {
        "Key": "NUTCH-2074",
        "Summary": "Javascript link not parsed by JSParseFilter",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Hadjiat Souad",
        "Created": "03/Aug/15 10:22",
        "Updated": "03/Aug/15 10:22",
        "Resolved": null,
        "Description": "JSParseFilter can't extract properly this link :\njavascript:tb_show('','http://dummy.url/3S/FRA/contenus/ext/endeca/html/dummy-page.html?TB_iframe=true&height=310&width=600','');\nI have run a junit test in debug mode and it seems that the regular expression JSParseFilter.STRING_PATTERN matches ',' only, and doesn't extract the url.\nAs I'm not the best in regular expressions, I can't propose a patch..\nThe complete html element is : \n<a class=\"last\" href=\"javascript:tb_show('','http://dummy.url/3S/FRA/contenus/ext/endeca/html/dummy-page.html?TB_iframe=true&height=310&width=600','');\">Dummy url</a>",
        "Issue Links": []
    },
    "NUTCH-2075": {
        "Key": "NUTCH-2075",
        "Summary": "Generate will not choose URL without distance marker",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Alexandre Demeyer",
        "Created": "07/Aug/15 15:03",
        "Updated": "22/Nov/19 15:27",
        "Resolved": "22/Nov/19 13:21",
        "Description": "It appears that there is a bug about certain links where nutch erases all markers and not only the inject, generate, fetch, parse, update markers but also the distance marker.\nThe problem is that Nutch Generator doesn't check the validity of the marker distance (check if it's null) and keep wrong links (without the distance marker) in the GeneratorMapper. When the distance filter is activated, GeneratorMapper choose also URL without markers and so it doesn't repect the limit.\nI think it's in relation with the problem mention here : NUTCH-1930.\nThis patch doesn't solved the problem which is all markers are erased (without any reasons apparently ..). But it can allow to stop the crawl...\nIn order to find a solution about stopping crawl with problematics URL, I proposed this solution which is simply to avoid the URL when the distance marker is NULL.\n(Sorry if i put the code here)\ncrawl/GeneratorMapper.java (initial code)\n// filter on distance\n    if (maxDistance > -1) {\n      CharSequence distanceUtf8 = page.getMarkers().get(DbUpdaterJob.DISTANCE);\n      if (distanceUtf8 != null) {\n        int distance = Integer.parseInt(distanceUtf8.toString());\n        if (distance > maxDistance) {\n          return;\n        }\n      }\n    }\n\n\n\ncrawl/GeneratorMapper.java (patch code)\n// filter on distance\n    if (maxDistance > -1) {\n      CharSequence distanceUtf8 = page.getMarkers().get(DbUpdaterJob.DISTANCE);\n      if (distanceUtf8 != null) {\n        int distance = Integer.parseInt(distanceUtf8.toString());\n        if (distance > maxDistance) {\n          return;\n        }\n      }\n      else\n      {\n        // No distance marker, URL problem\n        return;\n      }\n    }\n\n\n\nExample of link where the problem appears (put an http.content.limit highter than the content-length PDF) :\nhttp://www.annales.org/archives/x/marchal2.pdf\nHope it can help ...",
        "Issue Links": []
    },
    "NUTCH-2076": {
        "Key": "NUTCH-2076",
        "Summary": "exceptions are not handled when using method waitForCompletion in a try block",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2,                                            2.3",
        "Fix Version/s": "2.5",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "songwanging",
        "Created": "10/Aug/15 21:53",
        "Updated": "22/Nov/19 15:27",
        "Resolved": "22/Nov/19 13:21",
        "Description": "Locations: src\\java\\org\\apache\\nutch\\crawl\\WebTableReader.java\nwhen using function waitForCompletion in a try block, exceptions are not handled :\nwaitForCompletion might throw  : IOException, InterruptedException, ClassNotFoundException\nso when calling this function in a try block, we should use a catch block to handle potential Exceptions.\npublic Map<String, Object> run(Map<String, Object> args) throws Exception {\n...\ntry \n{\n      currentJob.waitForCompletion(true);\n    }\n finally {\n      ToolUtil.recordJobStatus(null, currentJob, results);\n      if (!currentJob.isSuccessful()) \n{\n        fileSystem.delete(tmpFolder, true);\n        return results;\n      }\n    }\n...\n}",
        "Issue Links": [
            "/jira/browse/NUTCH-2442"
        ]
    },
    "NUTCH-2077": {
        "Key": "NUTCH-2077",
        "Summary": "Upgrade to Tika 1.10",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11,                                            2.3.1",
        "Component/s": "None",
        "Assignee": "Michael Joyce",
        "Reporter": "Tyler Bui-Palsulich",
        "Created": "12/Aug/15 05:07",
        "Updated": "28/Aug/15 21:43",
        "Resolved": "28/Aug/15 20:53",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2078": {
        "Key": "NUTCH-2078",
        "Summary": "Nutch 2.3 issue while running eclipse",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Pradumna Panditrao",
        "Created": "12/Aug/15 11:06",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Hi,\nI am trying to run nutch 2.3 through eclipse, while running through eclipse it throws following error as:\n2015-08-12 16:01:44,319 INFO  crawl.InjectorJob (InjectorJob.java:inject(250)) - InjectorJob: starting at 2015-08-12 16:01:44\n2015-08-12 16:01:44,326 INFO  crawl.InjectorJob (InjectorJob.java:inject(251)) - InjectorJob: Injecting urlDir: /home/pradumna/Desktop/Mongo_n_ES/nutch2.3/runtime/local/urls\n2015-08-12 16:01:45,937 ERROR crawl.InjectorJob (InjectorJob.java:run(278)) - InjectorJob: java.lang.ClassNotFoundException: org.apache.gora.sql.store.SqlStore\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:366)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:355)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:354)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:425)\n\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:358)\n\tat java.lang.Class.forName0(Native Method)\n\tat java.lang.Class.forName(Class.java:191)\n\tat org.apache.nutch.storage.StorageUtils.getDataStoreClass(StorageUtils.java:93)\n\tat org.apache.nutch.storage.StorageUtils.createWebStore(StorageUtils.java:77)\n\tat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:218)\n\tat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\n\tat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)\n\tat org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:286)\nIn this error it mentioned related to sql db, However, I have already change the DB to mongo db in gora.properties file.\nSo kindly suggest the solution for the same.\nI have added the hadoop jar as per eclipse need & it has some issue with the same at file core-default.xml",
        "Issue Links": []
    },
    "NUTCH-2079": {
        "Key": "NUTCH-2079",
        "Summary": "Tika Parsing plugin issue",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "deployment",
        "Assignee": null,
        "Reporter": "Pradumna Panditrao",
        "Created": "12/Aug/15 11:21",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Hi,\nI am trying to parse particular data & post the same on the mongodb, however when I am trying to do some modifications into into parse tika plugin, it has too much inter connectivity with other classes & it misses the data. I want to pick up particular data from website using the same plugin & put into mongo db.\nPlease suggest for the same.",
        "Issue Links": []
    },
    "NUTCH-2080": {
        "Key": "NUTCH-2080",
        "Summary": "Eclipse compilation issue",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3.1",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Pradumna Panditrao",
        "Created": "13/Aug/15 08:49",
        "Updated": "16/Sep/15 04:31",
        "Resolved": "16/Sep/15 04:31",
        "Description": "Hi,\nI am trying to compile the nutch 2.3 on eclipse.  I have changed the gora properties with the mongodb. However while runtime, it throws error with  \"InjectorJob: java.lang.ClassNotFoundException:org.apache.gora.sql.store. SqlStore\". It indicates it is not redirecting to mongodb.\nPlease let me know the solution for the same.",
        "Issue Links": []
    },
    "NUTCH-2081": {
        "Key": "NUTCH-2081",
        "Summary": "outseq and vectors directories pollute $NUTCH_HOME",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Aug/15 00:23",
        "Updated": "28/Aug/15 20:34",
        "Resolved": null,
        "Description": "Right now, when one builds and tests Nutch you are left with generated 'outseq' and 'vectors' directories containing resources required for the parsefilter-naivebayes plugin.\nThis is messy and should be cleaned up.\nAn option would be to fire these into 'conf'.",
        "Issue Links": []
    },
    "NUTCH-2082": {
        "Key": "NUTCH-2082",
        "Summary": "Upgrade to Apache Tika 1.10",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "1.11,                                            2.3.1",
        "Component/s": "build,                                            plugin",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Aug/15 00:29",
        "Updated": "19/Aug/15 23:41",
        "Resolved": "19/Aug/15 23:41",
        "Description": "Tika 1.10 is hot\nhttp://search.maven.org/#artifactdetails|org.apache.tika|tika|1.10|pom\nLets upgrade",
        "Issue Links": []
    },
    "NUTCH-2083": {
        "Key": "NUTCH-2083",
        "Summary": "Implement functionality to shadow nutch-selenium-grid-plugin from Mo Omer",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Aug/15 22:47",
        "Updated": "26/Aug/15 02:54",
        "Resolved": "26/Aug/15 02:21",
        "Description": "This issue should augment the lib-selenium src/plugin/lib-selenium/src/java/org/apache/nutch/protocol/selenium/HttpWebClient.java and implement the same functionality as provided within momer's [https://github.com/momer/nutch-selenium-grid-plugin|nutch-selenium-grid-plugin].",
        "Issue Links": []
    },
    "NUTCH-2084": {
        "Key": "NUTCH-2084",
        "Summary": "Track changes in input dirs for SegmentMerger",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "25/Aug/15 07:27",
        "Updated": "26/Aug/15 09:54",
        "Resolved": "26/Aug/15 09:10",
        "Description": "When merging 1000's of segments, and one is corrupt, broken, whatever, the merge goes with the least number of shared input dirs, or nothing. This patch tracks when input dirs change due to a segment missing any directory.",
        "Issue Links": []
    },
    "NUTCH-2085": {
        "Key": "NUTCH-2085",
        "Summary": "Upgrade Guava",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "25/Aug/15 12:59",
        "Updated": "26/Aug/15 09:54",
        "Resolved": "26/Aug/15 09:11",
        "Description": "Upgrade Guava to 16.0.1. Higher will break tests.",
        "Issue Links": []
    },
    "NUTCH-2086": {
        "Key": "NUTCH-2086",
        "Summary": "Nutch 1.X Webui",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "25/Aug/15 21:46",
        "Updated": "04/Nov/15 06:29",
        "Resolved": "04/Nov/15 06:29",
        "Description": "To port the Apache Wicket based webui in Nutch 2.X to 1.X",
        "Issue Links": []
    },
    "NUTCH-2087": {
        "Key": "NUTCH-2087",
        "Summary": "\"No form exists: loginform\" while crawling a site (using Apache Nutch 1.10)",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ayub",
        "Created": "28/Aug/15 05:25",
        "Updated": "28/Aug/15 05:49",
        "Resolved": null,
        "Description": "Hi wastl-nagel,\nI am crawling a site where I need to login first, then crawl the page and index data back in Elastic search. \nSo, I have set up the authentication by using HttpFormAuthentication. Please find here template that I am using- https://issues.apache.org/jira/secure/attachment/12743064/NUTCH-1940.patch.\nBut, interestingly, first time I  do login and cookie is generated successfully. After that page is redirected to home page that I want to crawl. But, code is again trying to find login page here; since there is no login form (because we have already logged in ), following error is thrown:\nfailed with: java.lang.RuntimeException: java.lang.IllegalArgumentException: No form exists: loginform\nPlease help.\nThanks,\nAyub.",
        "Issue Links": []
    },
    "NUTCH-2088": {
        "Key": "NUTCH-2088",
        "Summary": "Add Optional Execution to Interactive Selenium Handlers",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "plugin",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Michael Joyce",
        "Created": "28/Aug/15 19:00",
        "Updated": "29/Aug/15 23:54",
        "Resolved": "29/Aug/15 23:42",
        "Description": "At the moment, all the Handlers run for every URL when using the interactive-selenium plugin. Often times when trying to do a deep crawl of a site you'll want to handle various subdomains and paths/files differently. You can effectively filter in the handlers at the moment, but only once you've loaded the WebDriver and incurred the associated overhead. It would be much nicer if the handler interface allowed for this check to occur prior to the request to retrieve page content.",
        "Issue Links": []
    },
    "NUTCH-2089": {
        "Key": "NUTCH-2089",
        "Summary": "Move Nutch 2.x to compile on JDK 8",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "build",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Lewis John McGibbney",
        "Created": "31/Aug/15 16:11",
        "Updated": "06/Sep/16 19:40",
        "Resolved": "03/Sep/16 04:56",
        "Description": "Public support updates for JDK 1.7 stopped in April of this year.\nhttps://www.java.com/en/download/faq/java_7.xml\nIn our next release we should shift support to JDK 1.8.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/116",
            "https://github.com/apache/nutch/pull/151"
        ]
    },
    "NUTCH-2090": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Refactor Seed Resource in REST API",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "04/Sep/15 00:58",
        "Updated": "12/Sep/15 17:19",
        "Resolved": "12/Sep/15 17:19",
        "Description": "Refactoring the SeedResource in Nutch Rest service for it to be the same as Nutch 2x and for easier porting of Nutch webui.",
        "Issue Links": []
    },
    "NUTCH-2091": {
        "Key": "NUTCH-2091",
        "Summary": "Increase robustness and crawling versatility of Nutch for the Deep Web",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "08/Sep/15 17:45",
        "Updated": "28/Sep/15 22:24",
        "Resolved": null,
        "Description": "Nutch fails to grab a page or crawl in a manner that is more productive in certain cases. This issue is to discuss those specific cases and try to generalize them into Nutch to make it even more robust and productive.\nI came across three websites and got many issues. I have toned down those issues into fine points.\n1. Some websites detect that the crawler is not a browser (marketwired) (cookie validations) and send it to the first page again and again.\n2. Some data behind a click (detect which clicks: javascript void) of 'a tag' that is not a link exactly (an improvement for the selenium plugin)\n3. When clicked something on a page and the page changed, how to get back the page before clicking further (can\u2019t obviously look for a back button or cross button. Can save the old state juxtapose with new info and only take the extra info)\n4. Differentiate between a navigation link and a common link in a forum page so that both links can be used differently to decide the progress of the crawler (nav links decide the rounds and other links we can go one round)\n5. Bring the capability of changing # to ? (pataxia.com). Right now url normalization completely removes the part after # thinking that it's a simple anchor tag.\n6. Easy route-decision in property file to decide how the fetcher will behave (instead of going all BFS or DFS, there should be a away to make it go DEPTH-LIMITED search. Esp good for forums and the likes of it. And users can give some known inputs like depth etc. to direct the crawler if they know something specific about the site)\n7. A forum can be roughly generalized into: a meta topic page (no nav links) -> post list (with nav links) -> post page (with nav links) : How to make nutch aware of this structure/heirachy. If manually give simple clues as well. Can be seen as an extension of the last point.\n8. Sometimes even nav links are not actual links but ajax requests.\nNOTE: Nav links (definition here): the structure on a web page (like a forum) which gives us an option to go to various pages by numbers or next, previous, first and or last pages.",
        "Issue Links": []
    },
    "NUTCH-2092": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Unit Test for NutchServer",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "09/Sep/15 03:46",
        "Updated": "12/Sep/15 17:54",
        "Resolved": "12/Sep/15 17:41",
        "Description": "A small test case to start the server and test the admin endpoint.",
        "Issue Links": []
    },
    "NUTCH-2093": {
        "Key": "NUTCH-2093",
        "Summary": "Indexing filters have no signature in CrawlDatum if crawled via FreeGenerator",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Sep/15 12:13",
        "Updated": "15/Sep/15 17:11",
        "Resolved": "15/Sep/15 06:53",
        "Description": "In IndexerMapReduce, a fetchDatum is passed to the indexing filters. However, when this fetchDatum was created via FreeGenerator, it has no signature attached, and indexing filters don't see it.\nThis patch copies the signature from the dbDatum just before passed to indexing filters.",
        "Issue Links": []
    },
    "NUTCH-2094": {
        "Key": "NUTCH-2094",
        "Summary": "Stopping and Restarting a crawl has issues in the Web UI",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.3.1",
        "Component/s": "web gui",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Prerna Satija",
        "Created": "11/Sep/15 18:01",
        "Updated": "09/Jan/16 02:06",
        "Resolved": "19/Sep/15 05:23",
        "Description": "I have created a stop button in Nutch webapp to stop a running crawl from the UI on click of a \"stop\" button. While testing, I found that I am able to stop a crawl successfully but when I restart a stopped crawl and try to stop it, it doesn't stop.",
        "Issue Links": []
    },
    "NUTCH-2095": {
        "Key": "NUTCH-2095",
        "Summary": "WARC exporter for the CommonCrawlDataDumper",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "commoncrawl,                                            tool",
        "Assignee": null,
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "11/Sep/15 19:17",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "23/Jul/18 11:50",
        "Description": "Adds the possibility of exporting the nutch segments to a WARC files.\nFrom the usage point of view a couple of new command line options are available:\n-warc: enables the functionality to export into WARC files, if not specified the default JACKSON formatter is used.\n-warcSize: enable the option to define a max file size for each WARC file, if not specified a default of 1GB per file is used as recommended by the WARC ISO standard.\nThe usual -gzip flag can be used to enable compression on the WARC files.\nSome changes to the default CommonCrawlDataDumper were done, essentially some changes to the Factory and to the Formats. This changes avoid creating a new instance of a CommmonCrawlFormat on each URL read from the segments.",
        "Issue Links": []
    },
    "NUTCH-2096": {
        "Key": "NUTCH-2096",
        "Summary": "Explicitly indicate broswer binary to use when selecting selenium remote option in config",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "plugin",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Kim Whitehall",
        "Created": "11/Sep/15 22:26",
        "Updated": "12/Sep/15 19:13",
        "Resolved": "12/Sep/15 17:18",
        "Description": "When using the selenium grid, not defining the binary version on nodes that have multiple versions of browsers can lead to errors. \nThe solution proposed is to extend the DesiredCapabilities capabilities provided in the \"remote\" case of $NUTCH_HOME/src/plugin/lib-selenium/src/java/org/apache/nutch/protocol/selenium/HttpWebClient.java provided in NUTCH-2083 to explicitly indicate the browser path.",
        "Issue Links": []
    },
    "NUTCH-2097": {
        "Key": "NUTCH-2097",
        "Summary": "Proposal for Nutch 3.x",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Nadeem Douba",
        "Created": "14/Sep/15 21:37",
        "Updated": "15/Sep/15 12:23",
        "Resolved": null,
        "Description": "This is a parent issue which contains a proposal for Nutch 3.x. It's based on my branch (mr2-mvn at https://github.com/allfro/nutch).",
        "Issue Links": []
    },
    "NUTCH-2098": {
        "Key": "NUTCH-2098",
        "Summary": "Add null SeedUrl constructor",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Aron Ahmadia",
        "Created": "15/Sep/15 14:17",
        "Updated": "18/Sep/15 06:07",
        "Resolved": "18/Sep/15 05:32",
        "Description": "The SeedUrl class currently doesn't provide a null constructor, and therefore can't correctly implement the Serializable interface to instantiate from JSON objects.\nThis patch adds a null constructor for the class.",
        "Issue Links": []
    },
    "NUTCH-2099": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Refactoring the REST endpoints for integration with webui",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "15/Sep/15 16:13",
        "Updated": "19/Sep/15 05:53",
        "Resolved": "19/Sep/15 05:14",
        "Description": "This PR changes the structure of the arguments in the REST endpoints. Earlier the args were accepted in a Map<String, String> form and now it is Map<String, Object>. This is to allow Wicket to create the proper requests objects and send it to NutchServer. \nWith the above, I have also added the metadata required for these services in Nutch metadata.",
        "Issue Links": []
    },
    "NUTCH-2100": {
        "Key": "NUTCH-2100",
        "Summary": "Nutch dump command doesnt dump anything",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Kim Whitehall",
        "Created": "15/Sep/15 20:12",
        "Updated": "16/Sep/15 00:10",
        "Resolved": "16/Sep/15 00:10",
        "Description": "When running the cmd \nnutch dump -segment segment -outputDir dumpFolder -mimeStats\nI receive the following \nDumper File Stats: \nTOTAL Stats:\n[\n]\nThe log indicates that segments are being skipped. \nNote, if I use nutch/readseg -dump  I can see there is content there. \nThe log is shown below:\n2015-09-15 20:10:56,142 INFO  tools.FileDumper - Accepting all mimetypes.\n2015-09-15 20:10:56,782 WARN  util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n2015-09-15 20:10:57,057 INFO  tools.FileDumper - Processing segment: [/.../segments/20150915195411/crawl_generate]\n2015-09-15 20:10:57,057 WARN  tools.FileDumper - Skipping segment: [/.../segments/20150915195411/crawl_generate/content/part-00000/data]: no data directory present\n2015-09-15 20:10:57,057 INFO  tools.FileDumper - Processing segment: [/.../segments/20150915195411/crawl_fetch]\n2015-09-15 20:10:57,057 WARN  tools.FileDumper - Skipping segment: [/.../segments/20150915195411/crawl_fetch/content/part-00000/data]: no data directory present\n2015-09-15 20:10:57,058 INFO  tools.FileDumper - Processing segment: [/.../segments/20150915195411/content]\n2015-09-15 20:10:57,058 WARN  tools.FileDumper - Skipping segment: [/.../segments/20150915195411/content/content/part-00000/data]: no data directory present\n2015-09-15 20:10:57,058 INFO  tools.FileDumper - Processing segment: [/.../segments/20150915195411/parse_text]\n2015-09-15 20:10:57,058 WARN  tools.FileDumper - Skipping segment: [/.../segments/20150915195411/parse_text/content/part-00000/data]: no data directory present\n2015-09-15 20:10:57,058 INFO  tools.FileDumper - Processing segment: [/.../segments/20150915195411/parse_data]\n2015-09-15 20:10:57,058 WARN  tools.FileDumper - Skipping segment: [/.../segments/20150915195411/parse_data/content/part-00000/data]: no data directory present\n2015-09-15 20:10:57,058 INFO  tools.FileDumper - Processing segment: [/.../segments/20150915195411/crawl_parse]\n2015-09-15 20:10:57,058 WARN  tools.FileDumper - Skipping segment: [/.../segments/20150915195411/crawl_parse/content/part-00000/data]: no data directory present\n2015-09-15 20:10:57,059 INFO  tools.FileDumper - Dumper File Stats: \nTOTAL Stats:\n[\n]",
        "Issue Links": []
    },
    "NUTCH-2101": {
        "Key": "NUTCH-2101",
        "Summary": "Upgrade Nutch 2.X to Hadoop 2.5.1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Sep/15 04:43",
        "Updated": "20/Sep/15 12:52",
        "Resolved": "20/Sep/15 12:52",
        "Description": "As we did over on NUTCH-2049, we should upgrade Nutch 2.3.1 to work with Hadoop 2.4.0. This is the natural move to fit in nicely with Gora 0.6.1.",
        "Issue Links": [
            "/jira/browse/NUTCH-1946"
        ]
    },
    "NUTCH-2102": {
        "Key": "NUTCH-2102",
        "Summary": "WARC Exporter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "commoncrawl,                                            dumpers",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "16/Sep/15 10:48",
        "Updated": "22/Sep/15 14:53",
        "Resolved": "22/Sep/15 14:05",
        "Description": "This patch adds a WARC exporter http://bibnum.bnf.fr/warc/WARC_ISO_28500_version1_latestdraft.pdf. Unlike the code submitted in https://github.com/apache/nutch/pull/55 which is based on the CommonCrawlDataDumper, this exporter is a MapReduce job and hence should be able to cope with large segments in a timely fashion and also is not limited to the local file system.\nLater on we could have a WARCImporter to generate segments from WARC files, which is outside the scope of the CCDD anyway. Also WARC is not specific to CommonCrawl, which is why the package name does not reflect it.\nI don't think it would be a problem to have both the modified CCDD and this class providing similar functionalities.\nThis class is called in the following way \n./nutch org.apache.nutch.tools.warc.WARCExporter /data/nutch-dipe/1kcrawl/warc -dir /data/nutch-dipe/1kcrawl/segments/",
        "Issue Links": []
    },
    "NUTCH-2103": {
        "Key": "NUTCH-2103",
        "Summary": "Nutch 2.3 has an old version of hbase jar in runtime/lib folder",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Mobin Ranjbar",
        "Created": "16/Sep/15 15:30",
        "Updated": "22/Nov/19 15:25",
        "Resolved": "22/Nov/19 15:25",
        "Description": "Hi there,\nNutch 2.3 has an old version of hbase jar file(hbase-0.94.14.jar) in runtime/lib folder. I have downloaded hbase 0.94.14 but it does not start because of \"Server IPC version 9 cannot communicate with client version 4\". I can run Hbase 1.1.2 but jar file does not included.\nHow can I solve this?\nThanks",
        "Issue Links": []
    },
    "NUTCH-2104": {
        "Key": "NUTCH-2104",
        "Summary": "Add documentation to the protocol-selenium plugin Readme file re: selenium grid implementation",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "documentation,                                            plugin,                                            protocol",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Kim Whitehall",
        "Created": "17/Sep/15 01:51",
        "Updated": "19/Sep/15 05:53",
        "Resolved": "19/Sep/15 05:27",
        "Description": "Adding some documentation to the protocol-selenium Readme file with regards to advice on using the selenium grid. Namely:\n(1) parameters to set for optimization of the grid \n(2) pitfalls to beware of when using the grid",
        "Issue Links": []
    },
    "NUTCH-2105": {
        "Key": "NUTCH-2105",
        "Summary": "Update Nutch Cassandra Dockerfile to work with Gora Nutch 2.3.1",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "docker",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Sep/15 06:55",
        "Updated": "23/Sep/15 01:45",
        "Resolved": "23/Sep/15 01:00",
        "Description": "Since we are updating NUTCH-2050 it would be excellent to have the Nutch + Hadoop + Gora + Cassandra stack up-to-date and ready to use as part of the 2.3.1 release. This issue should review the Dockerfile and update it where necessary.",
        "Issue Links": []
    },
    "NUTCH-2106": {
        "Key": "NUTCH-2106",
        "Summary": "Runtime to contain Selenium and dependencies only once",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Sep/15 11:59",
        "Updated": "21/Sep/15 22:24",
        "Resolved": "21/Sep/15 21:16",
        "Description": "All Selenium-based plugins contain the same dependendent jars which significantly affects the size of runtime and bin package:\n\n% du -hs runtime/local/plugins/*selenium/ runtime/deploy/*.job\n25M runtime/local/plugins/lib-selenium/\n25M runtime/local/plugins/protocol-interactiveselenium/\n25M runtime/local/plugins/protocol-selenium/\n182M runtime/deploy/apache-nutch-1.11-SNAPSHOT.job\n\n\nSince all plugins depend on the same Selenium version we could bundle the dependencies in lib-selenium and let the other plugins load it from there:\n\nlet lib-selenium export all dependent libs, e.g.:\nlib-selenium/plugin.xml\n<runtime>\n  ...\n  <library name=\"selenium-java-2.44.0.jar\">\n    <export name=\"*\"/>\n  </library>\n\n\nboth protocol plugins already import lib-selenium: the dependencies in ivy.xml can be removed\n\nAs expected, these changes make the runtime smaller:\n\n25M runtime/local/plugins/lib-selenium/\n20K runtime/local/plugins/protocol-interactiveselenium/\n16K runtime/local/plugins/protocol-selenium/\n138M runtime/deploy/apache-nutch-1.11-SNAPSHOT.job\n\n\nOpen points:\n\nI've tested only protocol-selenium using chromedriver. Should also test protocol-interactiveselenium?\nWhat about phantomjsdriver-1.2.1.jar? It was contained in lib-selenium and protocol-selenium but not protocol-interactiveselenium. Is there a reason for this?",
        "Issue Links": []
    },
    "NUTCH-2107": {
        "Key": "NUTCH-2107",
        "Summary": "plugin.xml to validate against plugin.dtd",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10,                                            1.11",
        "Fix Version/s": "1.11,                                            2.3.1",
        "Component/s": "plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Sep/15 12:16",
        "Updated": "01/Dec/15 22:04",
        "Resolved": "01/Dec/15 21:17",
        "Description": "Some of the plugin.xml do not validate against the plugin.dtd:\n\n% xmllint --noout --dtdvalid ./src/plugin/plugin.dtd src/plugin/urlnormalizer-regex/plugin.xml\nsrc/plugin/urlnormalizer-regex/plugin.xml:30: element requires: validity error : Element requires content does not follow the DTD, expecting (import)+, got (include )\nsrc/plugin/urlnormalizer-regex/plugin.xml:31: element include: validity error : No declaration for element include\nsrc/plugin/urlnormalizer-regex/plugin.xml:31: element include: validity error : No declaration for attribute file of element include\nDocument src/plugin/urlnormalizer-regex/plugin.xml does not validate against ./src/plugin/plugin.dtd\n\n% ...\nsrc/plugin/subcollection/plugin.xml:22: element plugin: validity error : Element plugin content does not follow the DTD, expecting (runtime? , requires? , extension-point* , extension*), got (requires runtime extension )\n\n% ...\nsrc/plugin/lib-selenium/plugin.xml:76: element requires: validity error : Element requires content does not follow the DTD, expecting (import)+, got (library library )\nsrc/plugin/lib-selenium/plugin.xml:80: element library: validity error : Element library content does not follow the DTD, expecting (export)*, got (export exclude )\nsrc/plugin/lib-selenium/plugin.xml:82: element exclude: validity error : No declaration for element exclude\nsrc/plugin/lib-selenium/plugin.xml:82: element exclude: validity error : No declaration for attribute name of element exclude",
        "Issue Links": []
    },
    "NUTCH-2108": {
        "Key": "NUTCH-2091 Increase robustness and crawling versatility of Nutch for the Deep Web",
        "Summary": "Add a function to the selenium interactive plugin interface to do multiple manipulation of driver and then return the data",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "17/Sep/15 21:19",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "08/Oct/15 18:58",
        "Description": "In the interactive selenium plugin we have to create handler classes for each manipulation of a page. Sometimes we need to manipulate a page in many ways and keep track of those manipulations. Like clicking on say each link in a table and then refreshing to get the original page back as even one click can make all other links go away. This can be done in a single loop. Which will be a little too much work and way complicated using multiple handlers. So, I am proposing a new function \"String multiProcessDriver(WebDriver driver)\"  that takes the driver and returns a concatenated String along with the already present \"void processDriver(WebDriver driver)\".",
        "Issue Links": []
    },
    "NUTCH-2109": {
        "Key": "NUTCH-2091 Increase robustness and crawling versatility of Nutch for the Deep Web",
        "Summary": "Create a  brute force click-all-ajax-links utility fucntion for selenium interactive plugin",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "17/Sep/15 21:21",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "08/Oct/15 18:58",
        "Description": "A function that clicks each ajax link on a page and then concatenates the changes to a single string and returns it.",
        "Issue Links": []
    },
    "NUTCH-2110": {
        "Key": "NUTCH-2091 Increase robustness and crawling versatility of Nutch for the Deep Web",
        "Summary": "Create the capability to provide seeds in the form of \"url+xpath(including option to enter seach terms).selenium\"",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "17/Sep/15 21:28",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "09/Oct/15 23:50",
        "Description": "Create the capability to provide seeds in the form of \"url+xpath(including option to enter seach terms).selenium\" to be used by selenium protocols/plugins as urls/flow to reach to a specific ajax based page or save the state of a selenium operation for the next fetching round.",
        "Issue Links": [
            "/jira/browse/NUTCH-2127",
            "/jira/browse/NUTCH-2126"
        ]
    },
    "NUTCH-2111": {
        "Key": "NUTCH-2111",
        "Summary": "Delete temporary files location for selenium tmp files after driver quits",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": "Kim Whitehall",
        "Reporter": "Kim Whitehall",
        "Created": "19/Sep/15 01:37",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "23/Sep/15 16:55",
        "Description": "When using the selenium plug in (local mode or selenium grid), a large # tmp files can be generated for each webdriver executed. The default location for selenium is the /tmp library. Thus very quickly (and inadvertently) the nutch-selenium interaction can lead to filesystem issues. \nI propose to include a config in nutch-default.xml that allows users to specify where they want the selenium tmp files to be written.",
        "Issue Links": []
    },
    "NUTCH-2112": {
        "Key": "NUTCH-2112",
        "Summary": "Missing org.restlet.jee when building with gora-solr",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "None",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Steven W",
        "Created": "19/Sep/15 22:06",
        "Updated": "17/May/16 21:46",
        "Resolved": "17/May/16 21:12",
        "Description": "When I build with the gora-solr backend, I get this missing dependency error: org.restlet.jee#org.restlet;2.2.1: not found\nRepro:\nClone 2.x branch\nUncomment `<dependency org=\"org.apache.gora\" name=\"gora-solr\" rev=\"0.5\" conf=\"*->default\" />` in ivy.xml\nrun ant runtime\nI'm not familiar with Ivy, but I tried adjusting the repos in ivysettings.xml, but that didn't fix it.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/113"
        ]
    },
    "NUTCH-2113": {
        "Key": "NUTCH-2113",
        "Summary": "Need documentation for using various Gora backends",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Steven W",
        "Created": "19/Sep/15 22:12",
        "Updated": "22/Nov/19 15:24",
        "Resolved": "22/Nov/19 15:24",
        "Description": "It would be very helpful to write documentation and tutorials for setting up Nutch and Gora withe backends other than HBase.\nAs an example, I am trying to setup Nutch -> Gora -> SOLR now, and I have found nothing online stating how this would work, or what the pros/cons are doing this vs. using HBase.\nI opened a question on StackOverflow here: http://stackoverflow.com/questions/32673602/ and I will link back to this ticket there as well.",
        "Issue Links": []
    },
    "NUTCH-2114": {
        "Key": "NUTCH-2114",
        "Summary": "kkk",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "administration gui,                                            commoncrawl,                                            injector",
        "Assignee": null,
        "Reporter": "Badreddine Ahmed",
        "Created": "19/Sep/15 22:45",
        "Updated": "20/Sep/15 10:39",
        "Resolved": "20/Sep/15 10:39",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2115": {
        "Key": "NUTCH-2115",
        "Summary": "Add total counts to dump stats",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "dumpers,                                            util",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "23/Sep/15 19:33",
        "Updated": "23/Sep/15 20:57",
        "Resolved": "23/Sep/15 20:00",
        "Description": "It would be nice if the \"dump\" tool included total counts for the mimetype stats that it gives. Something along the lines of the following would be great when you have to deal with some larger crawls and don't want to bother doing the math yourself.\n\nDumper File Stats: \nTOTAL Stats:\n[\n    {\"mimeType\":\"application/xhtml+xml\",\"count\":\"2\"}\n    {\"mimeType\":\"application/octet-stream\",\"count\":\"1\"}\n    {\"mimeType\":\"text/html\",\"count\":\"23\"}\n]\nTotal count: 26\n\nFILTERED Stats:\n[\n    {\"mimeType\":\"text/html\",\"count\":\"23\"}\n]\nTotal filtered count: 23",
        "Issue Links": []
    },
    "NUTCH-2116": {
        "Key": "NUTCH-2116",
        "Summary": "NutchServer and NutchApp should contain shutdown hooks",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "nutch server",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "23/Sep/15 19:56",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Right now when one tries to stop a running NutchServer and/or accompanying Nutch WebApp the process is not killed or stopped gracefully. \nWe should investigate and implement running both services with graceful shutdown hooks.",
        "Issue Links": []
    },
    "NUTCH-2117": {
        "Key": "NUTCH-2117",
        "Summary": "NutchServer CLI Option for CMD_PORT is incorrect and should be CMD_HOST",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "nutch server",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Sep/15 00:21",
        "Updated": "24/Sep/15 00:58",
        "Resolved": "24/Sep/15 00:23",
        "Description": "As shown in https://github.com/apache/nutch/pull/63, the issue here is that the NutchServer CLI option for port should be changed to host.",
        "Issue Links": []
    },
    "NUTCH-2118": {
        "Key": "NUTCH-2118",
        "Summary": "browser requests sometimes timeout when using the selenium grid because of port access issues",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Kim Whitehall",
        "Created": "24/Sep/15 16:10",
        "Updated": "22/Nov/19 15:24",
        "Resolved": null,
        "Description": "Using the selenium plugin with the selenium grid, it was observed that sometimes timeouts occur because the port timeouts. \nError log below:\nERROR selenium.Http - Failed to get protocol output\njava.lang.RuntimeException: org.openqa.selenium.WebDriverException: Unable to bind to locking port 7054 within 45000 ms",
        "Issue Links": []
    },
    "NUTCH-2119": {
        "Key": "NUTCH-2119",
        "Summary": "Eclipse shows build path errors on building Nutch",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "25/Sep/15 00:46",
        "Updated": "25/Sep/15 01:57",
        "Resolved": "25/Sep/15 01:09",
        "Description": "On running ant eclipse and importing the project in eclipse, Eclipse throws build path errors for missing test packages. (geoip/test, lib-selenium/test, protocol-selenium/test)",
        "Issue Links": []
    },
    "NUTCH-2120": {
        "Key": "NUTCH-2120",
        "Summary": "Remove MapWritable from trunk codebase",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "25/Sep/15 01:19",
        "Updated": "12/Nov/15 15:54",
        "Resolved": "12/Nov/15 15:19",
        "Description": "MapWritable has been deprecated for a good while.\nWe should remove it from the codebase and make sure we are not using it anywhere (I don't think we are).",
        "Issue Links": []
    },
    "NUTCH-2121": {
        "Key": "NUTCH-2121",
        "Summary": "Update javadoc link for Hadoop 2.4.0 in default.properties",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "25/Sep/15 01:23",
        "Updated": "25/Sep/15 01:57",
        "Resolved": "25/Sep/15 01:26",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2122": {
        "Key": "NUTCH-2122",
        "Summary": "Implement Javadoc package-info.java for webui packages",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "2.4",
        "Component/s": "nutch server",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Lewis John McGibbney",
        "Created": "25/Sep/15 01:59",
        "Updated": "30/Aug/16 22:47",
        "Resolved": "30/Aug/16 21:56",
        "Description": "sujenshah I noticed that the Javadoc does not contain package.html displaying package level introductory Javadoc as every other package does.\nhttp://nutch.apache.org/apidocs/apidocs-1.10/index.html",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/149"
        ]
    },
    "NUTCH-2123": {
        "Key": "NUTCH-2123",
        "Summary": "Seed List REST API returns Text but headers indicate/require JSON",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Aron Ahmadia",
        "Created": "25/Sep/15 05:58",
        "Updated": "06/Oct/15 01:00",
        "Resolved": "05/Oct/15 23:53",
        "Description": "nutch.py: POST Endpoint: /seed/create\nnutch.py: POST Request data: {'seedUrls': [\n{'id': 0, 'url': 'http://aron.ahmadia.net', 'seedList': None}\n], 'id': '12345', 'name': 'aron'}\nnutch.py: POST Request headers: \n{'Accept': 'application/json'}\nnutch.py: Response headers: \n{'content-type': 'application/json', 'server': 'Jetty(8.1.15.v20140411)', 'content-length': '64', 'date': 'Fri, 25 Sep 2015 05:49:09 GMT'}\nnutch.py: Response status: 200\nresp.headers\n{'content-type': 'application/json', 'server': 'Jetty(8.1.15.v20140411)', 'content-length': '64', 'date': 'Fri, 25 Sep 2015 05:49:09 GMT'}\n\nresp.text\n'/var/folders/3s/pw2prx7n7vd22qqrlssmtn900000gp/T/1443160149187-0'",
        "Issue Links": []
    },
    "NUTCH-2124": {
        "Key": "NUTCH-2124",
        "Summary": "redirect following same link again and again , max redirect exceed and went db_gone",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Yogendra Kumar Soni",
        "Created": "28/Sep/15 14:03",
        "Updated": "07/Oct/15 21:00",
        "Resolved": "07/Oct/15 19:03",
        "Description": "Hello, followredirect is not working in trunk. please see the below log.\nFetcher: throughput threshold retries: 5\nfetcher.maxNum.threads can't be < than 50 : using 50 instead\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=1\n\nfetching http://www.wikipedia.com/wiki/URL_redirection (queue crawl delay=5000ms)\nfetching http://www.wikipedia.com/wiki/URL_redirection (queue crawl delay=5000ms)\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=2\nfetching http://www.wikipedia.com/wiki/URL_redirection (queue crawl delay=5000ms)\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=2\nfetching http://www.wikipedia.com/wiki/URL_redirection (queue crawl delay=5000ms)\nfetching http://www.wikipedia.com/wiki/URL_redirection (queue crawl delay=5000ms)\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=2\n-activeThreads=1, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=2\n - redirect count exceeded http://www.wikipedia.com/wiki/URL_redirection\nThread FetcherThread has no more work available\n-finishing thread FetcherThread, activeThreads=0\n-activeThreads=0, spinWaiting=0, fetchQueues.totalSize=0, fetchQueues.getQueueCount=2\n-activeThreads=0\nFetcher: finished at 2015-09-28 19:32:05, elapsed: 00:00:09\nParsing : 20150928193153",
        "Issue Links": []
    },
    "NUTCH-2125": {
        "Key": "NUTCH-2125",
        "Summary": "Metrics tool for relevancy",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "tool",
        "Assignee": null,
        "Reporter": "Kim Whitehall",
        "Created": "28/Sep/15 15:05",
        "Updated": "28/Sep/15 15:11",
        "Resolved": null,
        "Description": "Purpose: a metric for determining if the \u201crelevancy\u201d of a crawl after each round and the \u201crelevancy\u201d of a page. NB: this is not a scoring plugin. By default, the first 25 terms will be stored. \n\nReturn the topN terms per a page\n\n\nReturn the topN terms per a segment  based on tf-idf\n\n\nLeverage Apache Lucene libs",
        "Issue Links": []
    },
    "NUTCH-2126": {
        "Key": "NUTCH-2091 Increase robustness and crawling versatility of Nutch for the Deep Web",
        "Summary": "Use selenium protocol for specific sites",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "28/Sep/15 18:14",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "22/Nov/19 15:23",
        "Description": "Right now if one uses selenium or seleniuminteractive plugins. The fetcher uses them for all the fetches. There will be situations where we don't want to go through the overhead of using selenium for all the seeds. \nCan provide some standardized key value pairs tell the protocol recognizer in nutch that certain seeds will be used with selenium plugin. Later on we can keep appending these key value pairs to the outlinks or only outlinks that are of the same domain.",
        "Issue Links": [
            "/jira/browse/NUTCH-2678",
            "/jira/browse/NUTCH-2110"
        ]
    },
    "NUTCH-2127": {
        "Key": "NUTCH-2091 Increase robustness and crawling versatility of Nutch for the Deep Web",
        "Summary": "Provide the selenium protocol with basic authentication capabilities.",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "28/Sep/15 18:20",
        "Updated": "09/Oct/15 23:50",
        "Resolved": null,
        "Description": "Provide the interactiveselenium with basic authentication capabilities.\nGive some standardized key value pairs as part of the seed url that tell Nutch about the authentication for a page. This can be further appended to outlinks that are of the same domain.",
        "Issue Links": [
            "/jira/browse/NUTCH-2110"
        ]
    },
    "NUTCH-2128": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Refactor configuration end point",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "29/Sep/15 19:37",
        "Updated": "04/Dec/15 07:11",
        "Resolved": "27/Oct/15 20:38",
        "Description": "To better define the endpoint to create a new configuration and add a new endpoint to update a particular property value of a configuration.",
        "Issue Links": []
    },
    "NUTCH-2129": {
        "Key": "NUTCH-2129",
        "Summary": "Track Protocol Status in Crawl Datum",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Michael Joyce",
        "Created": "01/Oct/15 00:48",
        "Updated": "05/Nov/15 06:51",
        "Resolved": "18/Oct/15 19:34",
        "Description": "It's become necessary on a few crawls that I run to get protocol status code stats. After speaking with lewismc it seemed that there might not be a super convenient way of doing this as is, but it would be great to be able to add the functionality necessary to pull this information out.",
        "Issue Links": []
    },
    "NUTCH-2130": {
        "Key": "NUTCH-2130",
        "Summary": "copyField rawcontent creates error within schema.xml",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "01/Oct/15 03:07",
        "Updated": "16/Nov/15 20:45",
        "Resolved": "16/Nov/15 20:37",
        "Description": "The presence of the rawcontent copyField within the Nutch Solr schema.xml is creating problems for users when attempting to index NutchDocuments into Solr.\nThe rawcontent field is produced by the index-html plugin however in committing this feature we have forgotten to add the field definition to schema.xml before applying the copyField instruction.\nThere are two ways to resolve this\n\nremove rawcontent from copyField, or\nadd rawcontent as a field prior to it's copyFields defintiion.\n\nI propose to do the latter and will submit a patch ASAP unless someone else is able to do so.\nThis was explained on this thread",
        "Issue Links": []
    },
    "NUTCH-2131": {
        "Key": "NUTCH-2131",
        "Summary": "Problem running nutch(crawl) with selenium",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Do",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "nutch server",
        "Assignee": null,
        "Reporter": "Ashwini",
        "Created": "01/Oct/15 10:31",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "22/Nov/19 15:20",
        "Description": "Hello,\nI had a few issues with running selenium on Ubuntu.\nI am trying to follow the tutorial that has a  description to install the nutch selenium plugin, https://github.com/apache/nutch/tree/trunk/src/plugin/protocol-selenium\nI was successfully able to include the plugin and build nutch again.\nBut during the crawling process,\nI get the error \"Unable to connect to host 127.0.0.1 on port 7055 after 45000 ms\" .\nI tried to do research on this and I think that the Firefox version I am using and Selenium jars are incompatible.(I'm not sure if this is the issue)\nSo I downgraded my Firefox to version(41 downgraded to 33), but I am still getting the same error.\nIs there a compatible version of firefox that I need to install or is there any other problem?\nI am using selenium that is integrated in nutch-1.10 and nutch version is 1.10.\nI have used 2.44.0 selenium standalone software with firefox version 33 and everything works fine. \nPlease help me with this.",
        "Issue Links": []
    },
    "NUTCH-2132": {
        "Key": "NUTCH-2132",
        "Summary": "Publisher/Subscriber model for Nutch to emit events",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13",
        "Component/s": "fetcher,                                            REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Sujen Shah",
        "Created": "02/Oct/15 08:52",
        "Updated": "15/Apr/17 11:31",
        "Resolved": "07/Sep/16 16:26",
        "Description": "It would be nice to have a Pub/Sub model in Nutch to emit certain events (ex- Fetcher events like fetch-start, fetch-end, a fetch report which may contain data like outlinks of the current fetched url, score, etc). \nA consumer of this functionality could use this data to generate real time visualization and generate statics of the crawl without having to wait for the fetch round to finish. \nThe REST API could contain an endpoint which would respond with a url to which a client could subscribe to get the fetcher events.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/138"
        ]
    },
    "NUTCH-2133": {
        "Key": "NUTCH-2133",
        "Summary": "Transfer Selenium Documentation to Wiki",
        "Type": "Improvement",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "1.20",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Michael Joyce",
        "Created": "06/Oct/15 15:01",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "There's a decent chunk of Selenium related documentation stuck in READMEs for various plugins. I would be nice to get this stuff pushed to the wiki.\nE.G.: https://github.com/apache/nutch/blob/trunk/src/plugin/protocol-selenium/README.md",
        "Issue Links": []
    },
    "NUTCH-2134": {
        "Key": "NUTCH-2134",
        "Summary": "Redirection and cookie handling using protocol  plugins",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.20",
        "Component/s": "fetcher,                                            plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Yogendra Kumar Soni",
        "Created": "08/Oct/15 07:00",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "fetcher thread is handling redirection logic and it is failing where redirection 301 takes to some other domain and get authentication cookies from there and again redirects to same url with cookies (even if domain is in urlfilter). HttpClient 4.5 uses automatic redirection handling with cookies, which is working well. In place of handling redirects in fetcherThread we can enable user to use HTTPclients 4.5 redirection handling. In other words  enable redirection handling logic selection using HttpBase or protocol plugin.",
        "Issue Links": []
    },
    "NUTCH-2135": {
        "Key": "NUTCH-2135",
        "Summary": "Ant Eclipse build does not include protocol-interactiveselenium",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12,                                            1.13",
        "Fix Version/s": "1.14",
        "Component/s": "protocol",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "10/Oct/15 02:05",
        "Updated": "26/Sep/17 08:16",
        "Resolved": "26/Sep/17 08:16",
        "Description": "target eclipse in the build.xml file does not include protocol-interactiveselenium so while importing the project into eclipse, it does not add that folder.  \nOn adding that to the build file, I found that eclipse throws errors as the package naming in classes belonging to the org.apache.nutch.protocol.interactiveselenium.handlers is incomplete. \nHave made both those changes in this PR.",
        "Issue Links": [
            "/jira/browse/NUTCH-2430"
        ]
    },
    "NUTCH-2136": {
        "Key": "NUTCH-2136",
        "Summary": "Implement a different version of Naive Bayes Parse Filter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.10",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "12/Oct/15 03:24",
        "Updated": "12/Oct/15 18:10",
        "Resolved": "12/Oct/15 18:04",
        "Description": "There has been many dependency issues with the first implementation of Naive Bayes Parse Filter. The major dependencies were Mahout and Lucene. There was also the issue where the training process failed in the distributed mode due to the fact that  a nested hadoop job was unable to run on the cluster.\nTo remove all these issues and make the filter be able to run in a distributed environment I am going to implement my own version of Naive Bayes without any dependency on any machine learning libraries.",
        "Issue Links": [
            "/jira/browse/NUTCH-2137"
        ]
    },
    "NUTCH-2137": {
        "Key": "NUTCH-2137",
        "Summary": "add changes.txt and ALV2 headers to the Naive Bayes Parse Filter",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Asitang Mishra",
        "Created": "12/Oct/15 18:05",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "12/Oct/15 18:18",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-2136"
        ]
    },
    "NUTCH-2138": {
        "Key": "NUTCH-2138",
        "Summary": "Tika cannot OCR embedded images from PDF",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "jean blue",
        "Created": "13/Oct/15 14:24",
        "Updated": "20/Mar/16 15:19",
        "Resolved": null,
        "Description": "Tika 1.10 is able to OCR embedded images if PDFParser.properties is modified accordingly in tika-app-1.10.jar but parse-tika doesn't if same modifications are made in runtime/local/plugins/parse-tika/tika-parsers-1.10.jar",
        "Issue Links": []
    },
    "NUTCH-2139": {
        "Key": "NUTCH-2139",
        "Summary": "Basic plugin to index inlinks and outlinks",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "13/Oct/15 23:10",
        "Updated": "22/Aug/16 20:38",
        "Resolved": "22/Aug/16 20:38",
        "Description": "Basic plugin that allows to index the inlinks and outlinks of the web pages, this could be very useful for analytic purposes, including neat visualizations using d3.js for instance.",
        "Issue Links": []
    },
    "NUTCH-2140": {
        "Key": "NUTCH-2140",
        "Summary": "Atomic update and optimistic concurrency update using Solr",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.9",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "14/Oct/15 13:06",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The SOLRIndexWriter plugin allows to index the documents into a Solr server. The plugin replaces the documents that already are indexed into Solr. Sometimes, replace only one field or add new fields and keep the others values of the documents indexed is useful.\nSolr supports two approaches for this task: Atomic update and optimistic concurrency update. However, the SOLRIndexWriter plugin doesn't support that approaches.",
        "Issue Links": []
    },
    "NUTCH-2141": {
        "Key": "NUTCH-2141",
        "Summary": "Change the InteractiveSelenium plugin handler Interface to return page content",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "plugin",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Balaji Gurumurthy",
        "Created": "15/Oct/15 00:55",
        "Updated": "18/Oct/15 19:55",
        "Resolved": "18/Oct/15 19:40",
        "Description": "The handler interface in the protocol-interactiveselenium plugin currently provide methods to manipulate the page content and the HTTPResponse class read's the page content from the driver. This limits the amount of HTML content that could be returned to nutch.\nThe processDriver method could return a String object instead. This is particularly helpful  in cases such as handling pagination when multiple pages' content can be appended and returned from the handler.",
        "Issue Links": []
    },
    "NUTCH-2142": {
        "Key": "NUTCH-2142",
        "Summary": "Nutch File Dump - FileNotFoundException (Invalid Argument) Error",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10,                                            1.11",
        "Fix Version/s": "1.11",
        "Component/s": "tool,                                            util",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Karanjeet Singh",
        "Created": "15/Oct/15 21:46",
        "Updated": "18/Oct/15 19:55",
        "Resolved": "18/Oct/15 19:30",
        "Description": "Got FileNotFoundException while running nutch dump.\nCause: Character '?' in file name/extension producing the below error.\nError Details\njava.io.FileNotFoundException: /media/PATRO/Karan/nutch_12Oct/other_gun_urls/img/99/fb/97d3980f9954b597f372d092b97eff22_27tlt_recon_1_black_g_10_handle_.jpeg? (Invalid argument)\nat java.io.FileOutputStream.open(Native Method)\nat java.io.FileOutputStream.(FileOutputStream.java:221)\nat java.io.FileOutputStream.(FileOutputStream.java:171)\nat org.apache.nutch.tools.FileDumper.dump(FileDumper.java:222)\nat org.apache.nutch.tools.FileDumper.main(FileDumper.java:325)",
        "Issue Links": []
    },
    "NUTCH-2143": {
        "Key": "NUTCH-2143",
        "Summary": "GeneratorJob ignores batch id passed as argument",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "generator",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/15 22:06",
        "Updated": "07/Jan/16 21:42",
        "Resolved": "07/Jan/16 20:57",
        "Description": "The batch id passed to GeneratorJob by option/argument -batchId <id> is ignored and a generated batch id is used to mark the current batch. Log snippets from a run of bin/crawl:\n\nbin/nutch generate ... -batchId 1444941073-14208\n...\nGeneratorJob: generated batch id: 1444941074-858443668 containing 1 URLs\n\nFetching : \nbin/nutch fetch ... 1444941073-14208 ...\n...\nQueueFeeder finished: total 0 records. Hit by time limit :0\n\n\nThe generated URLs are marked with the wrong batch id:\n\nhbase(main):010:0> scan 'test_webpage'\nROW                            COLUMN+CELL\n org.apache.nutch:http/        column=f:bid, timestamp=1444941077080, value=1444941074-858443668\n ...\n org.apache.nutch:http/        column=mk:_gnmrk_, timestamp=1444941077080, value=1444941074-858443668\n\n\nand fetcher will not fetch anything. This problem was reported by Sherban Drulea [1, [2.",
        "Issue Links": []
    },
    "NUTCH-2144": {
        "Key": "NUTCH-2144",
        "Summary": "Plugin to override db.ignore.external to exempt interesting external domain URLs",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12",
        "Component/s": "crawldb,                                            fetcher",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Thamme Gowda",
        "Created": "19/Oct/15 06:53",
        "Updated": "29/Feb/16 07:06",
        "Resolved": "29/Feb/16 07:06",
        "Description": "Create a rule based urlfilter plugin that allows focused crawler (db.ignore.external.links=true) to fetch static resources from external domains.\nThe generalized version of this: This plugin should permit interesting URLs from external domains (by overriding db.ignore.external). The interesting urls are decided from a combination of regex and mime-type rules.\nConcrete use case:\n  When using Nutch to crawl images from a set of domains, the crawler needs to fetch all images which may be linked from CDNs and other domains. In this scenario, allowing all external links and then writing hundreds of regular expressions is not feasible for large number of domains.",
        "Issue Links": [
            "/jira/browse/NUTCH-2221"
        ]
    },
    "NUTCH-2145": {
        "Key": "NUTCH-2145",
        "Summary": "parse/index checker fail to fetch valid percent-encoded URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.11",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "19/Oct/15 11:35",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Apr/18 12:21",
        "Description": "Parsechecker and indexchecker fail to fetch valid URLs containing percent-encoded characters. The percent-encoding is broken by escaping % again:\n\n% bin/nutch parsechecker 'https://de.wikipedia.org/wiki/%C3%84sop'\nfetching: https://de.wikipedia.org/wiki/%25C3%2584sop\nFetch failed with protocol status: gone(11), lastModified=0: https://de.wikipedia.org/wiki/%25C3%2584sop",
        "Issue Links": [
            "/jira/browse/NUTCH-2012"
        ]
    },
    "NUTCH-2146": {
        "Key": "NUTCH-2146",
        "Summary": "hashCode on the Outlink class",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10,                                            1.11",
        "Fix Version/s": "1.11",
        "Component/s": "parser",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "20/Oct/15 22:59",
        "Updated": "30/Oct/15 23:01",
        "Resolved": "30/Oct/15 21:56",
        "Description": "The Outlink class doesn't have a hashCode method. This doesn't cause any trouble with the already implemented plugins but if a developer tries to use a HashSet of outlinks in a custom plugin the Outlink instances with same data (toUrl, anchor) gets added several times. In contrast the Inlink class does have a hashCode method:\nhttps://github.com/apache/nutch/blob/trunk/src/java/org/apache/nutch/crawl/Inlink.java#L75-L77.",
        "Issue Links": []
    },
    "NUTCH-2147": {
        "Key": "NUTCH-2147",
        "Summary": "MetadataScoringFilter for Nutch",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "plugin,                                            scoring",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Oct/15 23:25",
        "Updated": "12/Jun/18 19:51",
        "Resolved": null,
        "Description": "This issue originally started by envisioning an implementation of a LanguagePreferenceScoringFilter so that Nutch could easily be made into a directed crawler based on crawl administrator ranking preferences of languages we wish to crawl. \nRight now this is not possible.\nWe already detect and index language within the language-identifier plugin as well as within parse-tika irrc, however currently the presence of a language does not effect scoring of pages.\nThe scope of this issue has changed to make it more generally applicable for a wider variety of use cases. This will therefore take advantage of NUTCH-1980 by pulling (amongst other things) Language entries from the CrawlDB Metadata.",
        "Issue Links": []
    },
    "NUTCH-2148": {
        "Key": "NUTCH-2148",
        "Summary": "Review and update mapred --> mapreduce config params in crawl script",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10,                                            2.3.1",
        "Fix Version/s": "1.11",
        "Component/s": "bin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "21/Oct/15 01:03",
        "Updated": "22/Oct/15 04:54",
        "Resolved": "22/Oct/15 03:47",
        "Description": "Configuration parameters inside of $NUTCH_HOME/src/bin/crawl currently include\n\ncommonOptions=\"-D mapred.reduce.tasks=$numTasks -D mapred.child.java.opts=-Xmx1000m -D mapred.reduce.tasks.speculative.execution=false -D mapred.map.tasks.speculative.execution=false -D mapred.compress.map.output=true\"\n\n\nas well as\n\n  skipRecordsOptions=\"-D mapred.skip.attempts.to.start.skipping=2 -D mapred.skip.map.max.skip.records=1\"\n  __bin_nutch parse $commonOptions $skipRecordsOptions \"$CRAWL_PATH\"/segments/$SEGMENT\n\n\nIn all honesty as part of the upgrade to Hadoop 2.4.0, this should have been addressed!!! woops.",
        "Issue Links": []
    },
    "NUTCH-2149": {
        "Key": "NUTCH-2149",
        "Summary": "REST endpoint to read Nutch sequence files",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "23/Oct/15 07:03",
        "Updated": "04/Dec/15 07:12",
        "Resolved": "25/Oct/15 18:21",
        "Description": "This endpoint enables reading of the webgraph data like nodes, links and any other sequence file in the Nutch ecosystem via a RESTful interface. \nThe current API documentation for this Reader endpoint is available at - http://docs.nutchpytonutchrestapi.apiary.io/\nThanks to https://github.com/ContinuumIO/nutchpy for the initial work.",
        "Issue Links": []
    },
    "NUTCH-2150": {
        "Key": "NUTCH-2150",
        "Summary": "Add ProtocolStatus Utility",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "util",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "27/Oct/15 19:35",
        "Updated": "11/Nov/15 17:55",
        "Resolved": "11/Nov/15 16:59",
        "Description": "It would be nice to have a utility for dumping protocol status code information for a crawl database. This will be a utility for getting a dump of the protocol status codes that builds off of NUTCH-2129",
        "Issue Links": []
    },
    "NUTCH-2151": {
        "Key": "NUTCH-2151",
        "Summary": "Service endpoint for REST API",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "27/Oct/15 20:45",
        "Updated": "26/Sep/19 16:15",
        "Resolved": null,
        "Description": "The service endpoint will enable users to call Nutch jobs like dump, commoncrawldump, readseg, etc via the REST api.",
        "Issue Links": [
            "/jira/browse/NUTCH-1931"
        ]
    },
    "NUTCH-2152": {
        "Key": "NUTCH-2151 Service endpoint for REST API",
        "Summary": "CommonCrawl dump via Service endpoint",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.15",
        "Component/s": "REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "27/Oct/15 20:50",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "19/Jul/18 12:54",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/355"
        ]
    },
    "NUTCH-2153": {
        "Key": "NUTCH-2153",
        "Summary": "Nutch REST API (DB) uses POST instead of GET to request",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Aron Ahmadia",
        "Created": "28/Oct/15 16:54",
        "Updated": "28/Oct/15 17:38",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2154": {
        "Key": "NUTCH-2154",
        "Summary": "Nutch REST API (DB) suffering NullPointerException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "REST_api",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Aron Ahmadia",
        "Created": "28/Oct/15 17:21",
        "Updated": "31/Oct/15 00:54",
        "Resolved": "30/Oct/15 23:55",
        "Description": "Not sure what's causing this.  I tried this request both before and after a crawl had completed.\nnutch.py: POST Endpoint: /db/crawldb\nnutch.py: POST Request data: \n{'type': 'stats', 'crawlId': 'crawl_aahmadia_2015-10-28T13_17_15.034351', 'confId': 'default'}\nnutch.py: POST Request headers: \n{'Accept': 'application/json'}\nnutch.py: Response headers: \n{'Date': 'Wed, 28 Oct 2015 17:18:54 GMT', 'Content-Length': '0', 'Server': 'Jetty(8.1.15.v20140411)'}\nnutch.py: Response status: 500\nnutch log:\njava.lang.NullPointerException\n\tat org.apache.nutch.crawl.CrawlDbReader.query(CrawlDbReader.java:747)\n\tat org.apache.nutch.service.resources.DbResource.crawlDbStats(DbResource.java:95)\n\tat org.apache.nutch.service.resources.DbResource.readdb(DbResource.java:52)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.cxf.service.invoker.AbstractInvoker.performInvocation(AbstractInvoker.java:181)\n\tat org.apache.cxf.service.invoker.AbstractInvoker.invoke(AbstractInvoker.java:97)\n\tat org.apache.cxf.jaxrs.JAXRSInvoker.invoke(JAXRSInvoker.java:200)\n\tat org.apache.cxf.jaxrs.JAXRSInvoker.invoke(JAXRSInvoker.java:99)\n\tat org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:59)\n\tat org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:96)\n\tat org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:307)\n\tat org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:121)\n\tat org.apache.cxf.transport.http.AbstractHTTPDestination.invoke(AbstractHTTPDestination.java:251)\n\tat org.apache.cxf.transport.http_jetty.JettyHTTPDestination.doService(JettyHTTPDestination.java:261)\n\tat org.apache.cxf.transport.http_jetty.JettyHTTPHandler.handle(JettyHTTPHandler.java:70)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1088)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1024)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:370)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)\n\tat org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:696)\n\tat org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:53)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)",
        "Issue Links": []
    },
    "NUTCH-2155": {
        "Key": "NUTCH-2155",
        "Summary": "Create a \"crawl completeness\" utility",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "util",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "28/Oct/15 20:43",
        "Updated": "11/Nov/15 16:54",
        "Resolved": "11/Nov/15 16:37",
        "Description": "I've found it useful to have a tool for dumping some \"completeness\" information from a crawl similar to how domainstats does but including fetched and unfetched counts per domain/host. This is especially nice when doing vertical crawls over a few domains or just to see how much of a host/domain you've covered with your crawl so far.",
        "Issue Links": []
    },
    "NUTCH-2156": {
        "Key": "NUTCH-2151 Service endpoint for REST API",
        "Summary": "Dump via Services end point",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "29/Oct/15 00:09",
        "Updated": "26/Sep/19 16:15",
        "Resolved": null,
        "Description": "Expose the ./bin/nutch dump command via the REST api. \nPlease review the documentation of the api design on http://docs.apachenutchrestapi.apiary.io/# and give your feedbacks. \nThank you all for your inputs",
        "Issue Links": []
    },
    "NUTCH-2157": {
        "Key": "NUTCH-2157",
        "Summary": "Parent Issue for Addressing Miredot REST API Warnings",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.20",
        "Component/s": "documentation,                                            REST_api",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "29/Oct/15 20:55",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "This is a parent issue for addressing the numerous warning as stated within the Miredot warnings. An example can be seen here \nhttp://people.apache.org/~lewismc/miredot/#warnings\nFor context on this issue please see NUTCH-1800\nIt is a large issue with a lot of work so I assume that we can hammer through it gradually as oppose to all at once.",
        "Issue Links": [
            "/jira/browse/NUTCH-1800"
        ]
    },
    "NUTCH-2158": {
        "Key": "NUTCH-2158",
        "Summary": "Upgrade to Tika 1.11",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "parser",
        "Assignee": "Julien Nioche",
        "Reporter": "Chris A. Mattmann",
        "Created": "03/Nov/15 22:06",
        "Updated": "26/Nov/15 08:12",
        "Resolved": "26/Nov/15 08:12",
        "Description": "Upgrade parse-tika to 1.11 release for Tika.",
        "Issue Links": []
    },
    "NUTCH-2159": {
        "Key": "NUTCH-2159",
        "Summary": "Ensure that all WebApp files are copied into generated artifacts for 1.X Webapp",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "04/Nov/15 06:32",
        "Updated": "05/Nov/15 04:21",
        "Resolved": "05/Nov/15 03:08",
        "Description": "Right now in trunk you get a hellish message when you initiate both webapp and startserver commands from within the bin/nutch script.\nThis issue addresses the bug which essentially after a ton of debugging relates to a failure for all Webapp files e.g. .css, .properties, .html, etc. being included within the generated artifact.\nPatch coming up ASAP.",
        "Issue Links": []
    },
    "NUTCH-2160": {
        "Key": "NUTCH-2160",
        "Summary": "Upgrade Selenium Java to 2.48.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "04/Nov/15 07:39",
        "Updated": "12/Nov/15 15:54",
        "Resolved": "12/Nov/15 15:21",
        "Description": "Current Selenium support is pegged at a very old version of Firefox. The attached patch, running with the most recent version of Selenium Java, works with Firefox 38.4.0 very well. The remainder of the lib-selenium dependencies have also been updated.\nThanks\nkwhitehall can you please scope if you get a wee minute?",
        "Issue Links": []
    },
    "NUTCH-2161": {
        "Key": "NUTCH-2161",
        "Summary": "Interrupted failed and/or killed tasks fail to clean up temp directories in HDFS",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "05/Nov/15 09:12",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "10/May/18 12:58",
        "Description": "If for example one kills an inject or generate job, Nutch does not clean up 'temporary' directories and I have witnessed them remain within HDFS. This is far from ideal if we have a large team of users all hammering away on Yarn and persisting data into HDFS.\nWe should investigate how to clean up these directories such that a cluster admin is not left with all of the dross at the end of the long day",
        "Issue Links": [
            "/jira/browse/NUTCH-2518"
        ]
    },
    "NUTCH-2162": {
        "Key": "NUTCH-2162",
        "Summary": "Nutch Webapp Crawl fails as it tries to index",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.20",
        "Component/s": "web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "05/Nov/15 23:20",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "Right now a crawl task fails on the trunk version of the WebApp due to it attempting to index. No indexer is defined by default so this is a major bug.",
        "Issue Links": []
    },
    "NUTCH-2163": {
        "Key": "NUTCH-2163",
        "Summary": "Utilize current JVM threads to augment URLClassLoader with newly discovered classes",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "util",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Nov/15 02:30",
        "Updated": "07/Nov/15 02:30",
        "Resolved": null,
        "Description": "I found this code a while back and have been thinking about OSGi again for Nutch. \nOur justification here is that we want to dynamically create InteractiveSeleniumHandler's and inject the code into the .job artifacts which can then be used in the next round of fetching. \nThe code looks like the following\n\n+    List<URL> nutchConfigurationClasspathURLs = new ArrayList<URL>();\n+\n+    // Collect classpath URLs from Hadoop's Configuration class CL\n+    URLClassLoader hadoopBundleConfigurationClassLoader = (URLClassLoader) conf.getClassLoader();\n+    for (URL hadoopBundleClasspathURL : hadoopBundleConfigurationClassLoader.getURLs()) {\n+      nutchConfigurationClasspathURLs.add(hadoopBundleClasspathURL);\n+    }\n+\n+    // Append classpath URLs from current thread, which ostensibly include a Nutch job file\n+    URLClassLoader tccl = (URLClassLoader) Thread.currentThread().getContextClassLoader();\n+    for (URL tcclClasspathURL : tccl.getURLs()) {\n+      nutchConfigurationClasspathURLs.add(tcclClasspathURL);\n+    }\n+\n+    URLClassLoader nutchConfigurationClassLoader = new URLClassLoader(nutchConfigurationClasspathURLs.toArray(new URL[0]));\n+    // Reset the Configuration object's CL to the new one\n+    conf.setClassLoader(nutchConfigurationClassLoader);\n\n\nThe Thread.currentThread().getContextClassLoader(); is the secret sauce... however I just wonder what thoughts are about this approach?\nWe have, from time to time over the years discussed Nutch and I spoke with bdelacretaz a good few years ago @ApacheCon but I don't have the time to implement total OSGi coverage of the Nutch codebase.",
        "Issue Links": []
    },
    "NUTCH-2164": {
        "Key": "NUTCH-2164",
        "Summary": "Inconsistent 'Modified Time' in crawl db",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.13",
        "Component/s": "crawldb,                                            fetcher",
        "Assignee": null,
        "Reporter": "Thamme Gowda",
        "Created": "09/Nov/15 08:56",
        "Updated": "23/Aug/16 09:18",
        "Resolved": "23/Aug/16 08:41",
        "Description": "The 'Modified time' in crawldb is invalid. It is set to (0-Timezone Difference)\nHow to verify/reproduce:\n  Run 'nutch readdb /path/to/crawldb -dump yy' and then inspect content of 'yy'\nThe following improvements can be done:\n1. Set modified time by DefaultFetchSchedule\n2. Set ProtocolStatus.lastModified if modified time is available in protocol response headers\nThis issue is also discussed in dev mailing lists: http://www.mail-archive.com/dev@nutch.apache.org/msg19803.html#",
        "Issue Links": [
            "/jira/browse/NUTCH-2242",
            "https://github.com/apache/nutch/pull/108"
        ]
    },
    "NUTCH-2165": {
        "Key": "NUTCH-2165",
        "Summary": "FileDumper Util hard codes part-# folder name",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "1.11",
        "Component/s": "tool",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "11/Nov/15 15:17",
        "Updated": "09/Jan/16 02:05",
        "Resolved": "12/Nov/15 18:53",
        "Description": "Hi folks, lewismc and I were just discussing this off list. It seems that the part-##### folders seem to be hard coded to part-00000 in the FileDumper utility which could prove problematic.",
        "Issue Links": []
    },
    "NUTCH-2166": {
        "Key": "NUTCH-2166",
        "Summary": "Add reverse URL format to dump tool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            1.10",
        "Fix Version/s": "1.11",
        "Component/s": "tool",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "11/Nov/15 16:28",
        "Updated": "09/Jan/16 02:05",
        "Resolved": "17/Nov/15 23:43",
        "Description": "Update the FileDumper tool with an option for dumping files to the output directory in reverse URL format.\nSo the file for \nhttp://bar.foo.com:8983/to/index.html?a=b\nWould dump to\n<output folder>/com/foo/bar/8983/http/to/index.html?a=b",
        "Issue Links": []
    },
    "NUTCH-2167": {
        "Key": "NUTCH-2166 Add reverse URL format to dump tool",
        "Summary": "Backport TableUtil from 2.x for URL reversing",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "tool",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "11/Nov/15 16:30",
        "Updated": "12/Nov/15 16:54",
        "Resolved": "12/Nov/15 16:10",
        "Description": "The TableUtil file provides a number of helpful utilities functions for URL reversing that would be useful to have in 1.x",
        "Issue Links": []
    },
    "NUTCH-2168": {
        "Key": "NUTCH-2168",
        "Summary": "Parse-tika fails to retrieve parser",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "12/Nov/15 22:05",
        "Updated": "09/Jan/16 13:42",
        "Resolved": "09/Jan/16 13:19",
        "Description": "The plugin parse-tika fails to parse most (all?) kinds of document types (PDF, xlsx, ...) when run via ParserChecker or ParserJob:\n\n2015-11-12 19:14:30,903 INFO  parse.ParserJob - Parsing http://localhost/pdftest.pdf\n2015-11-12 19:14:30,905 INFO  parse.ParserFactory - ...\n2015-11-12 19:14:30,907 ERROR tika.TikaParser - Can't retrieve Tika parser for mime-type application/pdf\n2015-11-12 19:14:30,913 WARN  parse.ParseUtil - Unable to successfully parse content http://localhost/pdftest.pdf of type application/pdf\n\n\nThe same document is successfully parsed by TestPdfParser.",
        "Issue Links": []
    },
    "NUTCH-2169": {
        "Key": "NUTCH-2169",
        "Summary": "Integrate index-html into Nutch build",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.3.1",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "12/Nov/15 22:59",
        "Updated": "08/Jan/16 21:47",
        "Resolved": "08/Jan/16 21:06",
        "Description": "The plugin index-html (added by NUTCH-1944) is loosely integrated:\n\ncode is in Nutch version control\nno build (compile, javadoc generation)\nsrc/plugin/index-html/src/java/org/apache/nutch/indexer/html/package.html contains a description how to do the integration\n\nWell, the plugin should be available just by adding it to plugin.includes without any extra efforts.",
        "Issue Links": []
    },
    "NUTCH-2170": {
        "Key": "NUTCH-2170",
        "Summary": "When i am crawling the URL  http://www.aossama.com/. it is crawling url like this com.aossama.www.http/",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2",
        "Fix Version/s": "None",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "prabhakar",
        "Created": "13/Nov/15 10:24",
        "Updated": "14/Nov/15 02:37",
        "Resolved": "14/Nov/15 02:37",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2171": {
        "Key": "NUTCH-2171",
        "Summary": "Upgrade Nutch Trunk to Java 1.8",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13",
        "Component/s": "None",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Nov/15 19:11",
        "Updated": "23/Feb/17 00:32",
        "Resolved": "23/Feb/17 00:32",
        "Description": "Lambda expressions are fantastic. I tried to undertake a small exercise which would indicate how many we could implement however this was a fruitless effort. A patch is going to be a better approach. This task involves upgrading various properties in default.properties as well as a systemic source code analysis with the aim of implementing Java 8 goodies throughout.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/174"
        ]
    },
    "NUTCH-2172": {
        "Key": "NUTCH-2172",
        "Summary": "index-more: document format of contenttype-mapping.txt",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.12",
        "Component/s": "indexer,                                            metadata",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Nicola Tonellotto",
        "Created": "18/Nov/15 15:19",
        "Updated": "06/Dec/15 21:54",
        "Resolved": "06/Dec/15 21:22",
        "Description": "The index-more plugin uses the conf/contenttype-mapping.txt file to build up the mimeMap hash table (in the readConfiguration() method).\nThe line splitting is performed around \"\\t\", so it silently skip lines separated by simple spaces or more than one tab (see line 325).\nChanging the single-char string \"\\t\" with the regex \"s+\" should do the magic.",
        "Issue Links": []
    },
    "NUTCH-2173": {
        "Key": "NUTCH-2173",
        "Summary": "String.join in FileDumper breaks the build",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "tool",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "18/Nov/15 17:05",
        "Updated": "18/Nov/15 20:58",
        "Resolved": "18/Nov/15 19:45",
        "Description": "The new FileDumper changes use a 1.8 String function that breaks the build on 1.7\n\nString.join\n\n\nThanks kwhitehall for finding this.",
        "Issue Links": []
    },
    "NUTCH-2174": {
        "Key": "NUTCH-2174",
        "Summary": "I setup Nutch in eclipse and run process inject(generate,fetech,parse) update But I get file and need to read it",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.10",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Osama",
        "Created": "19/Nov/15 10:54",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "19/Nov/15 21:29",
        "Description": "I setup Nutch in eclipse and run process inject(generate,fetech,parse) update But I get file and need to read it  you can see files in this link\nhttps://drive.google.com/folderview?id=0ByuPN2zZS0ZsQzgtTlpNMVcxcWM&usp=sharing",
        "Issue Links": []
    },
    "NUTCH-2175": {
        "Key": "NUTCH-2175",
        "Summary": "Typos in property descriptions in nutch-default.xml",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "24/Nov/15 13:34",
        "Updated": "24/Nov/15 19:18",
        "Resolved": "24/Nov/15 15:39",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2176": {
        "Key": "NUTCH-2176",
        "Summary": "Clean up of log4j.properties",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.11",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "26/Nov/15 08:03",
        "Updated": "02/Dec/15 12:54",
        "Resolved": "02/Dec/15 12:40",
        "Description": "Properties file:\n\nmissing DeduplicationJob\nstill has CrawldbScanner\nstill has reverted HostDB stuff\nis not sorted",
        "Issue Links": []
    },
    "NUTCH-2177": {
        "Key": "NUTCH-2177",
        "Summary": "Generator produces only one partition even in distributed mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.11",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "26/Nov/15 15:38",
        "Updated": "01/Dec/15 13:57",
        "Resolved": "01/Dec/15 12:49",
        "Description": "See https://github.com/apache/nutch/blob/trunk/src/java/org/apache/nutch/crawl/Generator.java#L542\n'mapred.job.tracker' is deprecated and has been replaced by 'mapreduce.jobtracker.address', however when running Nutch on EMR mapreduce.jobtracker.address has local as a value. As a result we generate a single partition i.e. have a single map fetching later on (which defeats the object of having a distributed crawler).\nWe should probably detect whether we are running on YARN instead, see http://stackoverflow.com/questions/29680155/why-there-is-a-mapreduce-jobtracker-address-configuration-on-yarn",
        "Issue Links": []
    },
    "NUTCH-2178": {
        "Key": "NUTCH-2178",
        "Summary": "DeduplicationJob to optionally group on host or domain",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Nov/15 15:14",
        "Updated": "08/Jan/16 11:54",
        "Resolved": "08/Jan/16 11:15",
        "Description": "Add optional grouping to DeduplicationJob.\nUsage: DeduplicationJob <crawldb> [-group <none|host|domain>]",
        "Issue Links": []
    },
    "NUTCH-2179": {
        "Key": "NUTCH-2179",
        "Summary": "Cleanup job for SOLR Performance Boost",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.9,                                            1.10,                                            1.11",
        "Fix Version/s": "1.14",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "David Johnson",
        "Created": "01/Dec/15 19:46",
        "Updated": "13/Feb/18 19:24",
        "Resolved": "13/Feb/18 19:24",
        "Description": "During a cleanup job, index deletes are scheduled one by one, which can make a large job take days",
        "Issue Links": [
            "/jira/browse/NUTCH-2197"
        ]
    },
    "NUTCH-2180": {
        "Key": "NUTCH-2180",
        "Summary": "FileDumper dumps data, but breaks midway on corrupt segments",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "bin,                                            dumpers",
        "Assignee": "Michael Joyce",
        "Reporter": "Harshavardhan Manjunatha",
        "Created": "03/Dec/15 16:10",
        "Updated": "10/Dec/15 05:16",
        "Resolved": "10/Dec/15 03:03",
        "Description": "FileDumper should ignore corrupt segments, and continue dumping data instead of throwing NullPointerException\n$ bin/nutch dump -segment ../../../segments/ -outputDir ./firstDump/ -flatdir\njava.lang.NullPointerException\n\tat org.apache.nutch.tools.FileDumper.dump(FileDumper.java:175)\n\tat org.apache.nutch.tools.FileDumper.main(FileDumper.java:417)\n$",
        "Issue Links": []
    },
    "NUTCH-2181": {
        "Key": "NUTCH-2181",
        "Summary": "Add Webpage for 3rd Party Connectors/Libraries to Apache Nutch",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "08/Dec/15 01:49",
        "Updated": "15/Dec/17 17:06",
        "Resolved": "15/Dec/17 17:06",
        "Description": "It would be nice to have a webpage/wiki page dedicated to 3rd party libraries which can be used with Nutch.\nhttp://github.com/chrismattmann/nutch-python.git is an example",
        "Issue Links": []
    },
    "NUTCH-2182": {
        "Key": "NUTCH-2182",
        "Summary": "Make reverseUrlDirs file dumper option hash the URL for consistency",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "tool",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "08/Dec/15 23:01",
        "Updated": "16/Dec/15 22:59",
        "Resolved": "16/Dec/15 22:10",
        "Description": "At the moment the \"reverseUrlDirs\" option for FileDumper is terribly brittle and fails on a fair number of edge cases. A more robust way to handle the reverse URL approach to dumping a file is to reverse the server part and hash the URL to use as the file name. This gives us a nice split of files while avoiding a number of likely classes that causes dumps to fail.",
        "Issue Links": [
            "/jira/browse/NUTCH-2187"
        ]
    },
    "NUTCH-2183": {
        "Key": "NUTCH-2183",
        "Summary": "Improvement to SegmentChecker for skipping non-segments present in segments directory",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "indexer,                                            segment",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "09/Dec/15 03:00",
        "Updated": "10/Dec/15 05:16",
        "Resolved": "10/Dec/15 03:05",
        "Description": "The scenario is that you have a bunch of Nutch data which has been gathered over some period of time. Some of the data structures are present, some are not. In segments directory for example there is .zip files (don't ask why) and in other directories there are .tar.gz files, etc.\nThis patch improves the SegmentChecker to skip directories or files present within the segments directory which are not 14 characters in length as ALL segments are. It also uses this check for individual segments if used by the IndexingJob. This means that we can prevent the Indexer blowing up if it is run on one segment (e.g. without -dir option) and detects some arbitrary directory present within segments/ which actually turns out not to be a segment afterall.",
        "Issue Links": []
    },
    "NUTCH-2184": {
        "Key": "NUTCH-2184",
        "Summary": "Enable IndexingJob to function with no crawldb",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.17",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Dec/15 02:36",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "09/Jan/20 10:38",
        "Description": "Sometimes when working with distributed team(s), we have found that we can 'loose' data structures which are currently considered as critical e.g. crawldb, linkdb and/or segments.\nIn my current scenario I have a requirement to index segment data with no accompanying crawldb or linkdb. \nAbsence of the latter is OK as linkdb is optional however currently in IndexerMapReduce crawldb is mandatory. \nThis ticket should enhance the IndexerMapReduce code to support the use case where you ONLY have segments and want to force an index for every record present.",
        "Issue Links": [
            "/jira/browse/NUTCH-2456",
            "https://github.com/apache/nutch/pull/95",
            "https://github.com/apache/nutch/pull/95",
            "https://github.com/apache/nutch/pull/486"
        ]
    },
    "NUTCH-2185": {
        "Key": "NUTCH-2185",
        "Summary": "protocol-soda-consumer plugin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Dec/15 00:01",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "15/Dec/17 17:05",
        "Description": "I'm finishing off a Nutch protocol implementation for interacting with the popular Socrata Open Data platform via their soda-java api. I feel that this would be useful for Government and other public sector organizations who make their data available through the Socrata platforms so it is my intention to propose it as a protocol-soda-consumer plugin for Nutch.",
        "Issue Links": []
    },
    "NUTCH-2186": {
        "Key": "NUTCH-2186",
        "Summary": "-addBinaryContent flag can cause \"String length must be a multiple of four\" error in IndexingJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Later",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Dec/15 22:19",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "28/Jun/18 09:46",
        "Description": "When using the following indexing command\n\n./runtime/local/bin/nutch index -crawldb /usr/local/trunk_new1/esdswg_crawl/crawldb/ -linkdb /usr/local/trunk_new1/esdswg_crawl/linkdb/ -segmentDir /usr/local/trunk_new1/esdswg_crawl/segments -addBinaryContent -deleteGone\n\n\nI am able to generate the following error in my Solr logs\n\nmsg=String length must be a multiple of four.\n\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:178)\n\tat org.apache.solr.update.AddUpdateCommand.getLuceneDocument(AddUpdateCommand.java:78)\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:238)\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:164)\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:69)\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:51)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd(DistributedUpdateProcessor.java:926)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1080)\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:692)\n\tat org.apache.solr.update.processor.LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:100)\n\tat org.apache.solr.handler.loader.XMLLoader.processUpdate(XMLLoader.java:247)\n\tat org.apache.solr.handler.loader.XMLLoader.load(XMLLoader.java:174)\n\tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:99)\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:74)\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:1967)\n\tat org.apache.solr.servlet.SolrDispatchFilter.execute(SolrDispatchFilter.java:777)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:418)\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:207)\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1419)\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:455)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:137)\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:557)\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:231)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1075)\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:384)\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:193)\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1009)\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:255)\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:154)\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)\n\tat org.eclipse.jetty.server.Server.handle(Server.java:368)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:489)\n\tat org.eclipse.jetty.server.BlockingHttpConnection.handleRequest(BlockingHttpConnection.java:53)\n\tat org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:953)\n\tat org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1014)\n\tat org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:953)\n\tat org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)\n\tat org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)\n\tat org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IllegalArgumentException: String length must be a multiple of four.\n\tat org.apache.solr.common.util.Base64.base64ToByteArray(Base64.java:98)\n\tat org.apache.solr.schema.BinaryField.createField(BinaryField.java:79)\n\tat org.apache.solr.schema.FieldType.createFields(FieldType.java:304)\n\tat org.apache.solr.update.DocumentBuilder.addField(DocumentBuilder.java:50)\n\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:125)\n\t... 43 more",
        "Issue Links": []
    },
    "NUTCH-2187": {
        "Key": "NUTCH-2187",
        "Summary": "Change FileDumper SHAs to all uppercase",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "tool",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "16/Dec/15 21:59",
        "Updated": "16/Dec/15 22:06",
        "Resolved": "16/Dec/15 22:06",
        "Description": "It would be nice to have the reverseUrlDirs options dump SHAs in all uppercase for consistency",
        "Issue Links": [
            "/jira/browse/NUTCH-2182"
        ]
    },
    "NUTCH-2188": {
        "Key": "NUTCH-2188",
        "Summary": "While crawling with solr url (kerberos enabled) Error: org.apache.solr.common.SolrException: Unauthorized",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.9",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Mohankumar K H",
        "Created": "17/Dec/15 06:11",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "28/Jun/18 09:40",
        "Description": "15/12/16 21:49:22 INFO mapreduce.Job: Task Id : attempt_1449548680888_0063_r_000002_0, Status : FAILED\nError: org.apache.solr.common.SolrException: Unauthorized\nUnauthorized\nrequest: https://hdrdn001c.cps.intel.com:8985/solr/nutch_std_config/update?wt=javabin&version=2\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:430)\n        at org.apache.solr.client.solrj.impl.CommonsHttpSolrServer.request(CommonsHttpSolrServer.java:244)\n        at org.apache.solr.client.solrj.request.AbstractUpdateRequest.process(AbstractUpdateRequest.java:105)\n        at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:155)\n        at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:118)\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:44)\n        at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.close(ReduceTask.java:502)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:456)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nContainer killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143",
        "Issue Links": []
    },
    "NUTCH-2189": {
        "Key": "NUTCH-2189",
        "Summary": "Domain filter must deactivate if no rules are present",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "21/Dec/15 12:34",
        "Updated": "15/Feb/16 13:13",
        "Resolved": "15/Feb/16 13:13",
        "Description": "We just erased an entire CrawlDB by accident due to a misconfiguration and the nice fact that the domain filter deletes everything if it has no rules. This issue will deactivate the filter if no rules are present, because it makes no sense to configure it without any rules.",
        "Issue Links": [
            "/jira/browse/NUTCH-2065"
        ]
    },
    "NUTCH-2190": {
        "Key": "NUTCH-2190",
        "Summary": "Protocol normalizer",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Dec/15 09:22",
        "Updated": "12/Jan/16 10:59",
        "Resolved": "12/Jan/16 10:34",
        "Description": "URL normalizer to normalize protocols for specified hosts/domains, e.g. normalizing http://www.apache.org/ to https://www.apache.org/",
        "Issue Links": [
            "/jira/browse/NUTCH-2065"
        ]
    },
    "NUTCH-2191": {
        "Key": "NUTCH-2191",
        "Summary": "Add protocol-htmlunit",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "protocol",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Markus Jelsma",
        "Created": "24/Dec/15 12:21",
        "Updated": "18/Apr/16 10:56",
        "Resolved": "18/Apr/16 09:49",
        "Description": "HtmlUnit is, opposed to other Javascript enabled headless browsers, a portable library and should therefore be better suited for very large scale crawls. This issue is an attempt to implement protocol-htmlunit.",
        "Issue Links": []
    },
    "NUTCH-2192": {
        "Key": "NUTCH-2192",
        "Summary": "Get rid of oro",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.15",
        "Fix Version/s": "2.4,                                            1.16",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "24/Dec/15 12:37",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "13/Oct/18 10:11",
        "Description": "Couple of classes still rely on oro, we should get rid of it.",
        "Issue Links": [
            "/jira/browse/NUTCH-1678",
            "/jira/browse/NUTCH-1014",
            "/jira/browse/NUTCH-1063",
            "https://github.com/apache/nutch/pull/389"
        ]
    },
    "NUTCH-2193": {
        "Key": "NUTCH-2193",
        "Summary": "Upgrade feed parser plugin to use rome 1.5",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.14",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "04/Jan/16 18:39",
        "Updated": "06/Apr/17 13:52",
        "Resolved": "06/Apr/17 13:28",
        "Description": "The class loader issue in the rome library (NUTCH-1494, [rometools #130) is fixed with rome 1.5. Time to upgrade.",
        "Issue Links": []
    },
    "NUTCH-2194": {
        "Key": "NUTCH-2194",
        "Summary": "Run IndexingFilterChecker as simple Telnet server",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Jan/16 14:26",
        "Updated": "15/Jan/16 11:54",
        "Resolved": "15/Jan/16 10:45",
        "Description": "We have used a customized IndexingFilterChecker running as server to be able to quickly test/check pages from web applications. I'll add this feature back by letting IndexingFilterChecker run optionally as a simple server.\nRun it with:\n\nexport NUTCH_HEAPSIZE=25 ;  bin/nutch indexchecker -normalize -dumpText -followRedirects -listen 1234\n\n\nThen perform a request over TCP:\n\necho \"http://apache.org/\" | nc localhost 1234",
        "Issue Links": [
            "/jira/browse/NUTCH-2195",
            "/jira/browse/NUTCH-2196"
        ]
    },
    "NUTCH-2195": {
        "Key": "NUTCH-2195",
        "Summary": "IndexingFilterChecker to optionally follow N redirects",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Jan/16 14:35",
        "Updated": "13/Jan/16 12:54",
        "Resolved": "13/Jan/16 12:17",
        "Description": "As mentioned in NUTCH-2194, we sometimes use it as a backend for a web application. If so, it should at least be able to follow N redirects.",
        "Issue Links": [
            "/jira/browse/NUTCH-2194"
        ]
    },
    "NUTCH-2196": {
        "Key": "NUTCH-2196",
        "Summary": "IndexingFilterChecker to optionally normalize",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Jan/16 14:38",
        "Updated": "13/Jan/16 13:54",
        "Resolved": "13/Jan/16 13:11",
        "Description": "As mentioned in NUTCH-2194, we sometimes use it as a backend for a web application. If so, then end users are obviously going to input bad URL's so having a normalizer running would smooth user satisfaction.",
        "Issue Links": [
            "/jira/browse/NUTCH-2194"
        ]
    },
    "NUTCH-2197": {
        "Key": "NUTCH-2197",
        "Summary": "Add solr5 solrcloud indexer support",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Jurian Broertjes",
        "Created": "07/Jan/16 10:49",
        "Updated": "13/Feb/18 19:24",
        "Resolved": "03/Feb/16 13:51",
        "Description": "Nutch cannot index to Solr5. Also proper SolrCloud support is missing.",
        "Issue Links": [
            "/jira/browse/NUTCH-2179",
            "/jira/browse/NUTCH-1662",
            "/jira/browse/NUTCH-1377"
        ]
    },
    "NUTCH-2198": {
        "Key": "NUTCH-2198",
        "Summary": "Indexing binary content by index-html causes Solr Exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "09/Jan/16 13:16",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "(reported by kalanya in NUTCH-2168)\nIf raw binary is indexed using the plugin index-html this may cause an exception in Solr:\n\n2016-01-05 12:28:00,152 INFO html.HtmlIndexingFilter - Html indexing for: http://ujiapps.uji.es/com/investigacio/img/ciencia11.jpg\n2016-01-05 12:28:00,163 INFO html.HtmlIndexingFilter - Html indexing for: http://ujiapps.uji.es/serveis/cd/bib/reservori/2015/e-llibres/\n2016-01-05 12:28:00,164 INFO solr.SolrIndexWriter - Adding 250 documents\n2016-01-05 12:28:00,531 INFO solr.SolrIndexWriter - Adding 250 documents\n2016-01-05 12:28:00,842 WARN mapred.LocalJobRunner - job_local1207147570_0001\njava.lang.Exception: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: [was class java.io.CharConversionException] Invalid UTF-8 character 0xfffe at char #137317, byte #139263)\n\n\nThe index-html plugin tries to treat any raw content as readable content converting it to a String based on the platform-dependent charset (cf. Scanner API docs):\nHtmlIndexingFilter.java\n            Scanner scanner = new Scanner(arrayInputStream);\n            scanner.useDelimiter(\"\\\\Z\");//To read all scanner content in one String\n            String data = \"\";\n            if (scanner.hasNext()) {\n                data = scanner.next();\n            }\n            doc.add(\"rawcontent\", StringUtil.cleanField(data));\n\n\nThe field \"rawcontent\" is of type \"string\":\nconf/schema.xml\n    <!-- fields for index-html plugin\n         Note: although raw document content may be binary,\n               index-html adds a String to the index field -->\n    <field name=\"rawcontent\" type=\"string\" stored=\"true\" indexed=\"false\"/>",
        "Issue Links": []
    },
    "NUTCH-2199": {
        "Key": "NUTCH-2199",
        "Summary": "Documentation for Nutch 2.X REST API",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "documentation,                                            REST_api",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Jan/16 14:50",
        "Updated": "24/Oct/18 09:10",
        "Resolved": "18/Oct/18 19:18",
        "Description": "The work done on NUTCH-1800 needs to be ported to 2.X branch. This is trivial, I thought I had already done it but obviously not.",
        "Issue Links": [
            "/jira/browse/NUTCH-2243"
        ]
    },
    "NUTCH-2200": {
        "Key": "NUTCH-2200",
        "Summary": "Establish process for publishing Docker containers",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "docker",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jan/16 20:27",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Docker containers exist for both trunk and 2.X however we don't formally publish them.\nWe should set up a publication mechanism which is built in to the release management cycle.",
        "Issue Links": []
    },
    "NUTCH-2201": {
        "Key": "NUTCH-2201",
        "Summary": "Remove loops program from webgraph package",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "18/Jan/16 20:22",
        "Updated": "21/Jan/16 15:54",
        "Resolved": "21/Jan/16 15:18",
        "Description": "Recently Dennis mentioned the loops program to be bad program. As developer of the package, he recommends not to use it.\n\n2. Crawl the pages for 1 shard.  Update the WebGraph and Linkrank as\n   described here.  https://wiki.apache.org/nutch/NewScoring. Don't use\n   Loops.  It was a bad program with a bad algorithm and I never should\n   have put it in.  Live and learn.\nSee: https://www.mail-archive.com/user@nutch.apache.org/msg14164.html",
        "Issue Links": []
    },
    "NUTCH-2202": {
        "Key": "NUTCH-2202",
        "Summary": "Integration of Anthelion (Focused Crawling Module) into Nutch",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parser,                                            scoring",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Robert Meusel",
        "Created": "19/Jan/16 09:08",
        "Updated": "25/Nov/19 03:32",
        "Resolved": null,
        "Description": "We have recently released anthelion, which is a focused crawler plugin for structured data which can be extracted with any23. (https://github.com/yahoo/anthelion) As proposed by Lewis (Lewis John McGibbney) we think the integration of the parser (any23) and the scoring function based on the online learner could be a good improvement for nutch.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/97"
        ]
    },
    "NUTCH-2203": {
        "Key": "NUTCH-2203",
        "Summary": "Suffix URL filter can't handle trailing/leading whitespaces",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Jurian Broertjes",
        "Created": "19/Jan/16 14:34",
        "Updated": "19/Jan/16 22:02",
        "Resolved": "19/Jan/16 14:53",
        "Description": "I ran into an issue where some lines in suffix-urlfilter.txt contained trailing whitespaces and caused the filtering to misbehave.",
        "Issue Links": []
    },
    "NUTCH-2204": {
        "Key": "NUTCH-2204",
        "Summary": "Remove junit lib from runtime",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "22/Jan/16 20:37",
        "Updated": "22/Jan/16 22:05",
        "Resolved": "22/Jan/16 21:32",
        "Description": "The junit library is shipped in the Nutch bin package as an unnecessary dependency (apache-nutch-1.11/lib/junit-3.8.1.jar). Unit tests use a different library version:\n\n% ls build/lib/junit* build/test/lib/junit*\nbuild/lib/junit-3.8.1.jar  build/test/lib/junit-4.11.jar",
        "Issue Links": []
    },
    "NUTCH-2205": {
        "Key": "NUTCH-2205",
        "Summary": "Nutch solrdedup error in solrcloud for larger docs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "VictorHu",
        "Created": "25/Jan/16 09:11",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "When the number of solr docs larger than 9000,the solrdedup of the nutch is broken.This is log: \nhttp://10.192.1.100:8080/solr/myEnterpriseCollection_shard2_replica2\n16/01/25 17:02:38 INFO solr.SolrDeleteDuplicates: SolrDeleteDuplicates: starting...\n16/01/25 17:02:38 INFO solr.SolrDeleteDuplicates: SolrDeleteDuplicates: Solr url: http://10.192.1.100:8080/solr/myEnterpriseCollection_shard2_replica2\n16/01/25 17:02:39 INFO client.RMProxy: Connecting to ResourceManager at master.Itble/10.192.1.100:8032\n16/01/25 17:02:43 INFO mapreduce.JobSubmitter: number of splits:1\n16/01/25 17:02:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1453104806095_0162\n16/01/25 17:02:44 INFO impl.YarnClientImpl: Submitted application application_1453104806095_0162\n16/01/25 17:02:44 INFO mapreduce.Job: The url to track the job: http://master.Itble:8088/proxy/application_1453104806095_0162/\n16/01/25 17:02:44 INFO mapreduce.Job: Running job: job_1453104806095_0162\n16/01/25 17:02:54 INFO mapreduce.Job: Job job_1453104806095_0162 running in uber mode : false\n16/01/25 17:02:54 INFO mapreduce.Job:  map 0% reduce 0%\n16/01/25 17:03:02 INFO mapreduce.Job: Task Id : attempt_1453104806095_0162_m_000000_0, Status : FAILED\nError: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:http://10.192.1.100:8080/solr/myEnterpriseCollection_shard2_replica2, http://10.192.1.101:8080/solr/myEnterpriseCollection_shard1_replica2, http://10.192.1.103:8080/solr/myEnterpriseCollection_shard2_replica1\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:554)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n        at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:91)\n        at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:301)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat.createRecordReader(SolrDeleteDuplicates.java:291)\n        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:492)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:735)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n16/01/25 17:03:12 INFO mapreduce.Job: Task Id : attempt_1453104806095_0162_m_000000_1, Status : FAILED\nError: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:http://10.192.1.100:8080/solr/myEnterpriseCollection_shard2_replica2, http://10.192.1.101:8080/solr/myEnterpriseCollection_shard1_replica2, http://10.192.1.103:8080/solr/myEnterpriseCollection_shard2_replica1, http://10.192.1.102:8080/solr/myEnterpriseCollection_shard1_replica1\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:554)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n        at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:91)\n        at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:301)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat.createRecordReader(SolrDeleteDuplicates.java:291)\n        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:492)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:735)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n16/01/25 17:03:22 INFO mapreduce.Job: Task Id : attempt_1453104806095_0162_m_000000_2, Status : FAILED\nError: org.apache.solr.client.solrj.impl.HttpSolrServer$RemoteSolrException: org.apache.solr.client.solrj.SolrServerException: No live SolrServers available to handle this request:http://10.192.1.100:8080/solr/myEnterpriseCollection_shard2_replica2, http://10.192.1.103:8080/solr/myEnterpriseCollection_shard2_replica1, http://10.192.1.102:8080/solr/myEnterpriseCollection_shard1_replica1\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.executeMethod(HttpSolrServer.java:554)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:210)\n        at org.apache.solr.client.solrj.impl.HttpSolrServer.request(HttpSolrServer.java:206)\n        at org.apache.solr.client.solrj.request.QueryRequest.process(QueryRequest.java:91)\n        at org.apache.solr.client.solrj.SolrServer.query(SolrServer.java:301)\n        at org.apache.nutch.indexer.solr.SolrDeleteDuplicates$SolrInputFormat.createRecordReader(SolrDeleteDuplicates.java:291)\n        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.<init>(MapTask.java:492)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:735)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:415)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)\n16/01/25 17:03:31 INFO mapreduce.Job:  map 100% reduce 100%\n16/01/25 17:03:31 INFO mapreduce.Job: Job job_1453104806095_0162 failed with state FAILED due to: Task failed task_1453104806095_0162_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n16/01/25 17:03:31 INFO mapreduce.Job: Counters: 8\n        Job Counters \n                Failed map tasks=4\n                Launched map tasks=4\n                Other local map tasks=4\n                Total time spent by all maps in occupied slots (ms)=30150\n                Total time spent by all reduces in occupied slots (ms)=0\n                Total time spent by all map tasks (ms)=30150\n                Total vcore-seconds taken by all map tasks=30150\n                Total megabyte-seconds taken by all map tasks=46310400",
        "Issue Links": []
    },
    "NUTCH-2206": {
        "Key": "NUTCH-2206",
        "Summary": "Provide example scoring.similarity.stopword.file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "plugin,                                            scoring",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Jan/16 01:52",
        "Updated": "17/Mar/16 07:14",
        "Resolved": "17/Mar/16 07:14",
        "Description": "The scoring-similarity plugin does not provide an example file for the property scoring.similarity.stopword.file.\nThis is an issue for a number of reasons, namely \n\nA user does not know what it is meant to look like, and\nWe always check of this file and will throw an exception if it is not found, this may not be picked up by the user until much later.\n\nI suggest a simple fix here, simply include the standard English stop words taken from Lucene's StopAnalyzer. The comments will help people to easily customize the list to whatever they require.",
        "Issue Links": []
    },
    "NUTCH-2207": {
        "Key": "NUTCH-2207",
        "Summary": "Remove class duplication and smarten-up scoring-similarity plugin",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            scoring",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Jan/16 02:01",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Right now it appears that DocumentVector.java is duplicated, there is also no license header on ScoringFilterModel.java. I think I've also spotted a number of places that imports are not being used. Finally, Javadoc is virtually non-existent for the scoring-similarity plugin at all. It would help to augment some documentation. \nIt would be very helpful if the SimilairittScoringFilter wiki page was cited.\nWe could also do with visiting the wiki page ensuring that all references are present.\nCC sujenshah",
        "Issue Links": []
    },
    "NUTCH-2208": {
        "Key": "NUTCH-2208",
        "Summary": "Fix 4 skipped tests in TestGenerator",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "generator,                                            test",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Jan/16 19:22",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Right now we @Ignore 4 tests within TestGenerator, with the following annotations\n\n @Ignore(\"GORA-240 Tests for MemStore\")\n\n\nIf I remove these annotations and run the tests I get the following\n\nTestcase: testGenerateDomainLimit took 3.724 sec\n\tFAILED\nexpected:<1> but was:<0>\njunit.framework.AssertionFailedError: expected:<1> but was:<0>\n\tat org.apache.nutch.crawl.TestGenerator.testGenerateDomainLimit(TestGenerator.java:217)\n\nTestcase: testGenerateOnlySitemap took 4.275 sec\nTestcase: testGenerateHostLimit took 1.929 sec\n\tFAILED\nexpected:<1> but was:<0>\njunit.framework.AssertionFailedError: expected:<1> but was:<0>\n\tat org.apache.nutch.crawl.TestGenerator.testGenerateHostLimit(TestGenerator.java:160)\n\nTestcase: testGenerateNoneSitemap took 11.104 sec\nTestcase: testFilter took 4.206 sec\n\tFAILED\nexpected:<3> but was:<0>\njunit.framework.AssertionFailedError: expected:<3> but was:<0>\n\tat org.apache.nutch.crawl.TestGenerator.testFilter(TestGenerator.java:278)\n\nTestcase: testGenerateHighest took 1.922 sec\n\tFAILED\nexpected:<2> but was:<0>\njunit.framework.AssertionFailedError: expected:<2> but was:<0>\n\tat org.apache.nutch.crawl.TestGenerator.testGenerateHighest(TestGenerator.java:96)\n\n\nWe need to investigate and address if this is truly related to GORA-240 or not!",
        "Issue Links": []
    },
    "NUTCH-2209": {
        "Key": "NUTCH-2209",
        "Summary": "Improved Tokenization for Similarity Scoring plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "scoring",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "31/Jan/16 02:32",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "12/Jun/18 19:50",
        "Description": "This patch would add Lucene based tokenization to the cosine similarity plugin and clean up the code currently present.",
        "Issue Links": []
    },
    "NUTCH-2210": {
        "Key": "NUTCH-2210",
        "Summary": "Upgrade to Tika 1.12",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "02/Feb/16 15:07",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "16/Feb/16 13:24",
        "Description": "Upgrade to Tika 1.12 when it is released. Keep in mind, <override module=\"rome\" rev=\"0.9\"/> in ivy.xml must be removed.",
        "Issue Links": [
            "/jira/browse/TIKA-1835",
            "/jira/browse/NUTCH-1233"
        ]
    },
    "NUTCH-2211": {
        "Key": "NUTCH-2211",
        "Summary": "Filter and normalizer checkers missing in bin/nutch",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "03/Feb/16 15:55",
        "Updated": "03/Feb/16 16:58",
        "Resolved": "03/Feb/16 16:03",
        "Description": "I am finally fed up typing their FQCN all the time. Will provide patch and get it in.",
        "Issue Links": []
    },
    "NUTCH-2212": {
        "Key": "NUTCH-2212",
        "Summary": "Decrease memory consumption by tuning stack size",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "03/Feb/16 16:25",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "18/Dec/17 17:27",
        "Description": "In today's environments it is common to see a default stack size (-Xss) of 1 MB. This is ridiculous for a fetcher running many fetcher.threads.fetch. The actual number of threads is much higher due to parsing and running it on YARN. \nWe can decrease stack usage by 75 %, 1 MB to a safe 256 kB. YARN will run out of stack size if we set it to 128 kB.",
        "Issue Links": []
    },
    "NUTCH-2213": {
        "Key": "NUTCH-2213",
        "Summary": "CommonCrawlDataDumper saves gzipped body in extracted form",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12",
        "Component/s": "commoncrawl,                                            dumpers",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Joris Rau",
        "Created": "10/Feb/16 10:26",
        "Updated": "01/Mar/16 03:44",
        "Resolved": "01/Mar/16 03:44",
        "Description": "I have downloaded a WARC file from the common crawl data. This file contains several gzipped responses which are stored plaintext (without the gzip encoding).\nI used warctools from Internet Archive to extract the responses out of the WARC file. However this tool expects the Content-Length field to match the actual length of the body in the WARC (See the issue on github). warctools uses a more up to date version of hanzo warctools which is recommended on the Common Crawl website under \"Processing the file format\".\nI have not been using Nutch and can therefore not say which versions are affected by this.\nAfter reading the official WARC draft I could not find out how gzipped content is supposed to be stored. However probably multiple WARC file parsers will have an issue with this.\nIt would be nice to know whether you consider this a bug and plan on fixing this and whether this is a major issue which concerns most WARC files of the Common Crawl data or only a small part.",
        "Issue Links": []
    },
    "NUTCH-2214": {
        "Key": "NUTCH-2214",
        "Summary": "Index clean to be flexible on what it deletes",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "10/Feb/16 11:16",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Nutch clean removes all useless records, but if Nutch is configured correctly (-deleteGone etc), the index should only contain duplicates, if existing. On a large index, this could result in Nutch sending millions of getById's to Solr, for records that don't exist in the first place.\nThis issue will make it configurable on what to delete, e.g. useless records (404, 30x) or duplicates.",
        "Issue Links": []
    },
    "NUTCH-2215": {
        "Key": "NUTCH-2215",
        "Summary": "Generator to restrict crawl to mime type",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "11/Feb/16 13:49",
        "Updated": "24/Feb/16 14:17",
        "Resolved": "24/Feb/16 14:17",
        "Description": "Large crawls fail to restrict crawling non-html via suffix filter alone, due to URL's hiding mime-types. This issue only passes records with a Content-Type that match a regex.",
        "Issue Links": [
            "/jira/browse/NUTCH-2231"
        ]
    },
    "NUTCH-2216": {
        "Key": "NUTCH-2216",
        "Summary": "db.ignore.*.links to optionally follow internal redirects",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.14",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Feb/16 11:01",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "18/Dec/17 16:32",
        "Description": "db.ignore.internal.links doesn't follow any internal hyperlinks or redirects. Together with db.ignore.external.links it helps to restrict the crawl to a predefined set of URL's, for example provided by a customer.\nIn many cases, a few of those URL's are redirects, which are not followed. This issue adds an option to optionally allow internal redirects despite db.ignore.internal.links being enabled.",
        "Issue Links": [
            "/jira/browse/NUTCH-2365",
            "/jira/browse/NUTCH-2221",
            "/jira/browse/NUTCH-2220"
        ]
    },
    "NUTCH-2217": {
        "Key": "NUTCH-2217",
        "Summary": "Crawl pages with specified language",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Dawid Wolski",
        "Created": "12/Feb/16 15:04",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Plugin to filter out the pages on languages other than specified. It bases on language returned by language-identifier plugin.",
        "Issue Links": [
            "/jira/browse/NUTCH-1663"
        ]
    },
    "NUTCH-2218": {
        "Key": "NUTCH-2218",
        "Summary": "Switch CrawlCompletion arg parsing to Commons CLI",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "util",
        "Assignee": "Michael Joyce",
        "Reporter": "Michael Joyce",
        "Created": "12/Feb/16 23:13",
        "Updated": "18/Feb/16 18:58",
        "Resolved": "18/Feb/16 18:04",
        "Description": "The current CrawlCompletion utility should be updated to use commons CLI instead of doing manual arg parsing and checking.",
        "Issue Links": []
    },
    "NUTCH-2219": {
        "Key": "NUTCH-2219",
        "Summary": "Criteria order to be configurable in DeduplicationJob",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Ron van der Vegt",
        "Created": "15/Feb/16 13:19",
        "Updated": "22/Feb/16 14:57",
        "Resolved": "22/Feb/16 14:41",
        "Description": "Current implementation:\n\"This command takes a path to a crawldb as parameter and finds duplicates based on the signature. If several entries share the same signature, the one with the highest score is kept. If the scores are the same, then the fetch time is used to determine which one to keep with the most recent one being kept. If their fetch times are the same we keep the one with the shortest URL.\"\nThe order in which the main document is selected is currently not changeable. Therefore I think this option would be nice:\n-compareOrder <score>,<fetchTime>,<urlLength>\nI have written a patch on trunk (rev 1730516). I'm looking forward for any peer review.",
        "Issue Links": []
    },
    "NUTCH-2220": {
        "Key": "NUTCH-2220",
        "Summary": "Rename db.* options used only by the linkdb to linkdb.*",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "linkdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "15/Feb/16 14:00",
        "Updated": "23/Feb/16 11:13",
        "Resolved": "23/Feb/16 10:23",
        "Description": "We need an option db.ignore.internal.links that operates in FetcherThread, just like db.ignore.external.links. It already exists but it only used by the LinkDB, and defaults to true, which is no good option for FetcherThread.\nI propose to make a clear distinction between which are used for LinkDB or not. Most options used by LinkDB already use the right prefix but db.ignore.*.links, db.max.inlinks and db.max.anchor.length not yet.\nThis patch will rename those options to linkdb.* prefixes so afterwards we can implement db.ignore.internal.links that operates in FetcherThread, just like db.ignore.external.links.\nThis will introduce a change in default parameters. Please comment.\nHow to upgrade from earlier releases\n\nreplace your old conf/nutch-default.xml with the conf/nutch-default.xml from Nutch 1.12 release\nif you use LinkDB (e.g. invertlinks) and modified parameters db.max.inlinks and/or db.max.anchor.length and/or db.ignore.internal.links, rename those parameters to linkdb.max.inlinks and linkdb.max.anchor.length and linkdb.ignore.internal.links\ndb.ignore.internal.links and db.ignore.external.links now operate on the CrawlDB only\nlinkdb.ignore.internal.links and linkdb.ignore.external.links now operate on the LinkDB only",
        "Issue Links": [
            "/jira/browse/NUTCH-2221",
            "/jira/browse/NUTCH-2216"
        ]
    },
    "NUTCH-2221": {
        "Key": "NUTCH-2221",
        "Summary": "Introduce db.ignore.internal.links to FetcherThread",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "15/Feb/16 14:29",
        "Updated": "23/Feb/16 11:56",
        "Resolved": "23/Feb/16 11:56",
        "Description": "FetcherThread has support for db.ignore.external.links. In config you can find db.ignore.internal.links as well, but it only operates on LinkDB, which is confusing. This patch will introduce db.ignore.internal.links to FetcherThread, similar to db.ignore.external.links. With both parameter set to true you can limit the crawl to the injected seed list.",
        "Issue Links": [
            "/jira/browse/NUTCH-2144",
            "/jira/browse/NUTCH-2220",
            "/jira/browse/NUTCH-2216"
        ]
    },
    "NUTCH-2222": {
        "Key": "NUTCH-2222",
        "Summary": "re-fetch deletes all  metadata except _csh_ and _rs_",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "crawldb",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Adnane B.",
        "Created": "16/Feb/16 19:50",
        "Updated": "01/Aug/18 18:43",
        "Resolved": "01/Aug/18 18:26",
        "Description": "This problem happens at the the second time I crawl a page\n\nbin/nutch inject urls/\nbin/nutch generate -topN 1000\nbin/nutch fetch  -all\nbin/nutch parse -force   -all\nbin/nutch updatedb  -all\n\n\nseconde time (re-fetch) : \n\nbin/nutch generate -topN 1000 --> batchid changes for all existing pages\nbin/nutch fetch  -all   -->  *** metadatas are delete for all pages already crawled  **\nbin/nutch parse -force   -all\nbin/nutch updatedb  -all\n\n\nI reproduce it with mongodb 2.6, mongodb 3.0, and hbase-0.98.8-hadoop2\nIt happens only if the page has not changed\nTo reproduce easily, please add to nutch-site.xml :\n\n<property>\n  <name>db.fetch.interval.default</name>\n  <value>60</value>\n  <description>The default number of seconds between re-fetches of a page (1 minute)\n</description>",
        "Issue Links": []
    },
    "NUTCH-2223": {
        "Key": "NUTCH-2223",
        "Summary": "Upgrade xercesImpl to 2.11.0 to fix hang on issue in tika mimetype detection",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Tien Nguyen Manh",
        "Created": "17/Feb/16 08:17",
        "Updated": "17/Feb/16 11:09",
        "Resolved": "17/Feb/16 10:40",
        "Description": "Stracktrace for the hang seems to be:\n\nat org.apache.xerces.impl.XMLScanner.scanExternalID(Unknown Source)\nat org.apache.xerces.impl.XMLDocumentScannerImpl.scanDoctypeDecl(Unknown Source)\nat org.apache.xerces.impl.XMLDocumentScannerImpl$PrologDispatcher.dispatch(Unknown Source)\nat org.apache.xerces.impl.XMLDocumentFragmentScannerImpl.scanDocument(Unknown Source)\nat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\nat org.apache.xerces.parsers.XML11Configuration.parse(Unknown Source)\nat org.apache.xerces.parsers.XMLParser.parse(Unknown Source)\nat org.apache.xerces.parsers.AbstractSAXParser.parse(Unknown Source)\nat org.apache.xerces.jaxp.SAXParserImpl$JAXPSAXParser.parse(Unknown Source)\nat org.apache.xerces.jaxp.SAXParserImpl.parse(Unknown Source)\nat javax.xml.parsers.SAXParser.parse(SAXParser.java:195)\nat org.apache.tika.detect.XmlRootExtractor.extractRootElement(XmlRootExtractor.java:54)\nat org.apache.tika.detect.XmlRootExtractor.extractRootElement(XmlRootExtractor.java:41)\nat org.apache.tika.mime.MimeTypes.getMimeType(MimeTypes.java:192)\nat org.apache.tika.mime.MimeTypes.detect(MimeTypes.java:439)\nat org.apache.tika.detect.CompositeDetector.detect(CompositeDetector.java:61)\nat org.apache.tika.cli.TikaCLI$10.process(TikaCLI.java:252)\nat org.apache.tika.cli.TikaCLI.process(TikaCLI.java:417)\nat org.apache.tika.cli.TikaCLI.main(TikaCLI.java:111)",
        "Issue Links": [
            "/jira/browse/TIKA-1154"
        ]
    },
    "NUTCH-2224": {
        "Key": "NUTCH-2224",
        "Summary": "Average bytes/second calculated incorrectly in fetcher",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Tien Nguyen Manh",
        "Created": "17/Feb/16 08:44",
        "Updated": "17/Feb/16 11:09",
        "Resolved": "17/Feb/16 09:56",
        "Description": "Currently we convert from bytes to kbits by\n(bytes.get() / 125l)\nI thinks it should be /128l",
        "Issue Links": []
    },
    "NUTCH-2225": {
        "Key": "NUTCH-2225",
        "Summary": "Parsed time calculated incorrectly",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Tien Nguyen Manh",
        "Created": "17/Feb/16 08:50",
        "Updated": "17/Feb/16 11:09",
        "Resolved": "17/Feb/16 09:51",
        "Description": "In ParseSegment we report parse time\nLOG.info(\"Parsed (\" + Long.toString(end - start) + \"ms):\" + url);\nBut the start time is the time after we parse so in log we see many \"0 ms\"",
        "Issue Links": []
    },
    "NUTCH-2226": {
        "Key": "NUTCH-2226",
        "Summary": "SOLR mismatch in deploy mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Steven W",
        "Created": "21/Feb/16 04:36",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "23/Jul/18 11:43",
        "Description": "I receive this error when indexing to SolrCloud in deploy mode on Hadoop 2.7.0:\nType 'org/apache/http/impl/client/DefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient'\nI'm assuming there's a version mismatch somewhere in the deploy JAR, but I don't know where to look. This is related to NUTCH-2197.",
        "Issue Links": [
            "/jira/browse/NUTCH-2267"
        ]
    },
    "NUTCH-2227": {
        "Key": "NUTCH-2227",
        "Summary": "RegexParseFilter",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "22/Feb/16 14:07",
        "Updated": "23/Feb/16 14:11",
        "Resolved": "23/Feb/16 12:59",
        "Description": "A parse filter that takes a regex and a field name. If regex matches via matcher.find() on the HTML. The field name is set to true in the CrawlDatum's metadata.\nCombined with the HostDB, it is easy to get a list of hosts that match some regex criteria.\n\n# Example configuration file for parsefilter-regex\n#\n# Parse metadata field <name> is set to true if the HTML matches the regex. The\n# source can either be html or text. If source is html, the regex is applied to\n# the entire HTML tree. If source is text, the regex is applied to the\n# extracted text.\n#\n# format: <name>\\t<source>\\t<regex>\\n",
        "Issue Links": []
    },
    "NUTCH-2228": {
        "Key": "NUTCH-2228",
        "Summary": "Plugin index-replace unit test broken on Java 8",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "22/Feb/16 16:21",
        "Updated": "23/Feb/16 11:13",
        "Resolved": "23/Feb/16 09:50",
        "Description": "------------- Standard Error -----------------\nSLF4J: Class path contains multiple SLF4J bindings.\nSLF4J: Found binding in [jar:file:/home/markus/projects/apache/nutch/trunk/build/test/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: Found binding in [jar:file:/home/markus/projects/apache/nutch/trunk/build/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]\nSLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\nSLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n------------- ---------------- ---------------\n\nTestcase: testGlobalAndUrlNotMatchesPattern took 1.052 sec\nTestcase: testGlobalReplacement took 0.149 sec\nTestcase: testReplacementsWithFlags took 0.105 sec\nTestcase: testUrlMatchesPattern took 0.116 sec\nTestcase: testReplacementsDifferentTarget took 0.099 sec\nTestcase: testReplacementsRunInSpecifedOrder took 0.1 sec\nTestcase: testInvalidPatterns took 0.078 sec\n        FAILED\nexpected:<With this []plugin, I control th...> but was:<With this [awesome ]plugin, I control th...>\njunit.framework.AssertionFailedError: expected:<With this []plugin, I control th...> but was:<With this [awesome ]plugin, I control th...>\n        at org.apache.nutch.indexer.replace.TestIndexReplace.testInvalidPatterns(TestIndexReplace.java:203)\n\nTestcase: testGlobalAndUrlMatchesPattern took 0.079 sec\nTestcase: testUrlNotMatchesPattern took 0.06 sec\nTestcase: testPropertyParse took 0.03 sec\n\n\nDoes the initial committer know what the outcome of the test should be?",
        "Issue Links": []
    },
    "NUTCH-2229": {
        "Key": "NUTCH-2229",
        "Summary": "Allow Jexl expressions on CrawlDatum's fixed attributes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Feb/16 15:00",
        "Updated": "03/Feb/17 11:04",
        "Resolved": "03/Feb/17 11:03",
        "Description": "CrawlDatum allows Jexl expressions on its metadata fields nicely, but it lacks the opportunity to select on attributes like fetchTime and modifiedTime.\nThis includes a rudimentary date parser only supporting the yyyy-MM-dd'T'HH:mm:ss'Z' format:\nDump everything with a modifiedTime higher than 2016-03-20T00:00:00Z\n\nbin/nutch readdb crawl/crawldb/ -dump out -format csv -expr \"(modifiedTime > 2016-03-20T00:00:00Z)\"\n\n\nDump everything that is an HTML file\n\nbin/nutch readdb crawl/crawldb/ -dump out -format csv -expr \"(Content_Type == 'text/html' || Content_Type == 'application/xhtml+xml')\"\n\n\nKeep in mind:\n\nJexl doesn't allow a hyphen/minus in field identifier, they are transformed to underscores\nstring literals must be in quotes, only surrounding qoute needs to be escaped by backslash",
        "Issue Links": []
    },
    "NUTCH-2230": {
        "Key": "NUTCH-2230",
        "Summary": "Nutch doesn't index all URLs found",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Aaron Cosand",
        "Created": "23/Feb/16 19:44",
        "Updated": "15/Nov/19 12:11",
        "Resolved": "15/Nov/19 12:11",
        "Description": "The initial query run by the generator task, against mongodb, doesn't force ordering by _id.  This causes an incorrect selection of ranges for successive map-reduce related queries.  The successive queries do appear to be getting run in the correct order since _id is always indexed, but they should also explicitly specify a sort, since you are not guaranteed a particular order otherwise.  I didn't dig deep enough to see if the root of the problem is with nutch or gora, and whether it only affected mongo or could affect other databases as well.",
        "Issue Links": []
    },
    "NUTCH-2231": {
        "Key": "NUTCH-2231",
        "Summary": "Jexl support in generator job",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Feb/16 12:55",
        "Updated": "11/May/16 08:21",
        "Resolved": "11/May/16 08:21",
        "Description": "Generator should support Jexl expressions. This would make it much easier to implement focussing crawlers that rely on information stored in the CrawlDB. With the HostDB it is possible to restrict the generator to select only interesting records but it is very cumbersome and involves domainblacklist-urlfiltering.\nWith Jexl support, it is no hassle!\nCrawl only english records:\n\nbin/nutch generate crawl/crawldb/ crawl/segments/ -expr \"(lang == 'en'')\"\n\n\nCrawl only HTML records:\n\nbin/nutch generate crawl/crawldb/ crawl/segments/ -expr \"(Content_Type == 'text/html' || Content_Type == 'application/xhtml+xml')\"\n\n\nKeep in mind:\n\nJexl doesn't allow a hyphen/minus in field identifier, they are transformed to underscores\nstring literals must be in quotes, only surrounding qoute needs to be escaped by backslash",
        "Issue Links": [
            "/jira/browse/NUTCH-1179",
            "/jira/browse/NUTCH-2215"
        ]
    },
    "NUTCH-2232": {
        "Key": "NUTCH-2232",
        "Summary": "DeduplicationJob should decode URL's before length is compared",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.12",
        "Component/s": "crawldb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Ron van der Vegt",
        "Created": "24/Feb/16 13:16",
        "Updated": "24/Feb/16 14:57",
        "Resolved": "24/Feb/16 14:13",
        "Description": "When certain documents have the same signature de deduplication script will elect one as duplicate. The urls are stored url encoded in the crawldb. When two urls are compared by url length, the urls are not first decoded. This could lead to misleading url length.",
        "Issue Links": []
    },
    "NUTCH-2233": {
        "Key": "NUTCH-2233",
        "Summary": "Index-basic incorrect assignment of next fetch time when using Mongodb as storage backend",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Pablo Torres",
        "Created": "24/Feb/16 13:50",
        "Updated": "20/Jan/17 02:12",
        "Resolved": "02/Mar/16 15:52",
        "Description": "This patch https://issues.apache.org/jira/browse/NUTCH-2045 does not work when using Mongodb as storage since date properties are stored as Longs in mongodb rather than objects, therefore the null date in this case is 0 which is accepted as valid by this patch. The system indexes 01/01/1970 as tstamp.\nI found this issue using Mongodb as storage and Elastic Search as index.",
        "Issue Links": []
    },
    "NUTCH-2234": {
        "Key": "NUTCH-2234",
        "Summary": "Upgrade to elasticsearch 2.3.3",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.13",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Tien Nguyen Manh",
        "Created": "24/Feb/16 15:08",
        "Updated": "29/Jun/16 15:08",
        "Resolved": "29/Jun/16 15:08",
        "Description": "Currently we use elasticsearch 1.x, We should upgrade to 2.x",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/118"
        ]
    },
    "NUTCH-2235": {
        "Key": "NUTCH-2235",
        "Summary": "Classpath discrepancy with protocol-selenium in deploy mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.14",
        "Component/s": "build,                                            plugin,                                            protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "25/Feb/16 23:21",
        "Updated": "22/Sep/17 18:17",
        "Resolved": "22/Sep/17 18:17",
        "Description": "when running with protocol-selenium in deploy mode I observe the following behaviour\n\nlmcgibbn@LMC-032857 /usr/local/nutch(master) $ ./runtime/deploy/bin/nutch parsechecker -dumpText \"http://www.jpl.nasa.gov\"\n16/02/25 15:22:08 INFO parse.ParserChecker: fetching: http://www.jpl.nasa.gov\n16/02/25 15:22:08 INFO plugin.PluginRepository: Plugins: looking in: /usr/local/hadoop-2.5.2/hd-tmp/hadoop-unjar6419843999522854503/classes/plugins\n16/02/25 15:22:09 INFO plugin.PluginRepository: Plugin Auto-activation mode: [true]\n16/02/25 15:22:09 INFO plugin.PluginRepository: Registered Plugins:\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tthe nutch core extension points (nutch-extensionpoints)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tBasic URL Normalizer (urlnormalizer-basic)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tHtml Parse Plug-in (parse-html)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tBasic Indexing Filter (index-basic)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tHttp Protocol Plug-in (protocol-selenium)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tSolrIndexWriter (indexer-solr)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tHTTP Framework (lib-http)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tRegex URL Filter (urlfilter-regex)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tPass-through URL Normalizer (urlnormalizer-pass)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tRegex URL Normalizer (urlnormalizer-regex)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tCyberNeko HTML Parser (lib-nekohtml)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tTika Parser Plug-in (parse-tika)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tOPIC Scoring Plug-in (scoring-opic)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tAnchor Indexing Filter (index-anchor)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tHTTP Framework (lib-selenium)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tRegex URL Filter Framework (lib-regex-filter)\n16/02/25 15:22:09 INFO plugin.PluginRepository: Registered Extension-Points:\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tNutch URL Normalizer (org.apache.nutch.net.URLNormalizer)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tNutch Protocol (org.apache.nutch.protocol.Protocol)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tNutch Segment Merge Filter (org.apache.nutch.segment.SegmentMergeFilter)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tNutch URL Filter (org.apache.nutch.net.URLFilter)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tNutch Index Writer (org.apache.nutch.indexer.IndexWriter)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tNutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tHTML Parse Filter (org.apache.nutch.parse.HtmlParseFilter)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tNutch Content Parser (org.apache.nutch.parse.Parser)\n16/02/25 15:22:09 INFO plugin.PluginRepository: \tNutch Scoring (org.apache.nutch.scoring.ScoringFilter)\n16/02/25 15:22:09 INFO protocol.RobotRulesParser: robots.txt whitelist not configured.\n16/02/25 15:22:09 INFO selenium.Http: http.proxy.host = null\n16/02/25 15:22:09 INFO selenium.Http: http.proxy.port = 8080\n16/02/25 15:22:09 INFO selenium.Http: http.proxy.exception.list = false\n16/02/25 15:22:09 INFO selenium.Http: http.timeout = 10000\n16/02/25 15:22:09 INFO selenium.Http: http.content.limit = -1\n16/02/25 15:22:09 INFO selenium.Http: http.agent = nutch_test/Nutch-1.12-SNAPSHOT\n16/02/25 15:22:09 INFO selenium.Http: http.accept.language = en-us,en-gb,en;q=0.7,*;q=0.3\n16/02/25 15:22:09 INFO selenium.Http: http.accept = text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\n16/02/25 15:22:09 ERROR selenium.Http: Failed to get protocol output\njava.lang.NoSuchFieldError: INSTANCE\n\tat org.apache.http.impl.io.DefaultHttpRequestWriterFactory.<init>(DefaultHttpRequestWriterFactory.java:52)\n\tat org.apache.http.impl.io.DefaultHttpRequestWriterFactory.<init>(DefaultHttpRequestWriterFactory.java:56)\n\tat org.apache.http.impl.io.DefaultHttpRequestWriterFactory.<clinit>(DefaultHttpRequestWriterFactory.java:46)\n\tat org.apache.http.impl.conn.ManagedHttpClientConnectionFactory.<init>(ManagedHttpClientConnectionFactory.java:72)\n\tat org.apache.http.impl.conn.ManagedHttpClientConnectionFactory.<init>(ManagedHttpClientConnectionFactory.java:84)\n\tat org.apache.http.impl.conn.ManagedHttpClientConnectionFactory.<clinit>(ManagedHttpClientConnectionFactory.java:59)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager$InternalConnectionFactory.<init>(PoolingHttpClientConnectionManager.java:493)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.<init>(PoolingHttpClientConnectionManager.java:149)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.<init>(PoolingHttpClientConnectionManager.java:138)\n\tat org.apache.http.impl.conn.PoolingHttpClientConnectionManager.<init>(PoolingHttpClientConnectionManager.java:114)\n\tat org.openqa.selenium.remote.internal.HttpClientFactory.getClientConnectionManager(HttpClientFactory.java:74)\n\tat org.openqa.selenium.remote.internal.HttpClientFactory.<init>(HttpClientFactory.java:57)\n\tat org.openqa.selenium.remote.internal.HttpClientFactory.<init>(HttpClientFactory.java:60)\n\tat org.openqa.selenium.remote.internal.ApacheHttpClient$Factory.getDefaultHttpClientFactory(ApacheHttpClient.java:251)\n\tat org.openqa.selenium.remote.internal.ApacheHttpClient$Factory.<init>(ApacheHttpClient.java:228)\n\tat org.openqa.selenium.remote.HttpCommandExecutor.getDefaultClientFactory(HttpCommandExecutor.java:96)\n\tat org.openqa.selenium.remote.HttpCommandExecutor.<init>(HttpCommandExecutor.java:70)\n\tat org.openqa.selenium.remote.HttpCommandExecutor.<init>(HttpCommandExecutor.java:58)\n\tat org.openqa.selenium.firefox.internal.NewProfileExtensionConnection.start(NewProfileExtensionConnection.java:97)\n\tat org.openqa.selenium.firefox.FirefoxDriver.startClient(FirefoxDriver.java:271)\n\tat org.openqa.selenium.remote.RemoteWebDriver.<init>(RemoteWebDriver.java:117)\n\tat org.openqa.selenium.firefox.FirefoxDriver.<init>(FirefoxDriver.java:216)\n\tat org.openqa.selenium.firefox.FirefoxDriver.<init>(FirefoxDriver.java:211)\n\tat org.openqa.selenium.firefox.FirefoxDriver.<init>(FirefoxDriver.java:207)\n\tat org.openqa.selenium.firefox.FirefoxDriver.<init>(FirefoxDriver.java:120)\n\tat org.apache.nutch.protocol.selenium.HttpWebClient.getDriverForPage(HttpWebClient.java:75)\n\tat org.apache.nutch.protocol.selenium.HttpWebClient.getHtmlPage(HttpWebClient.java:155)\n\tat org.apache.nutch.protocol.selenium.HttpResponse.readPlainContent(HttpResponse.java:244)\n\tat org.apache.nutch.protocol.selenium.HttpResponse.<init>(HttpResponse.java:168)\n\tat org.apache.nutch.protocol.selenium.Http.getResponse(Http.java:56)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:261)\n\tat org.apache.nutch.parse.ParserChecker.run(ParserChecker.java:136)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.parse.ParserChecker.main(ParserChecker.java:265)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)\nFetch failed with protocol status: exception(16), lastModified=0: java.lang.NoSuchFieldError: INSTANCE",
        "Issue Links": []
    },
    "NUTCH-2236": {
        "Key": "NUTCH-2236",
        "Summary": "Upgrade to Hadoop 2.7.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.13",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Tien Nguyen Manh",
        "Created": "29/Feb/16 00:24",
        "Updated": "29/Jun/16 15:09",
        "Resolved": "29/Jun/16 15:09",
        "Description": "Upgrade to Hadoop 2.7.1",
        "Issue Links": []
    },
    "NUTCH-2237": {
        "Key": "NUTCH-2237",
        "Summary": "DeduplicationJob: Add extra order criteria based on slug",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ron van der Vegt",
        "Created": "02/Mar/16 13:50",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Currently user can elect the main document when signatures are the same on score, url lenght and fetchtime. The quality of the slug, based mainly on the amount of meaningful characters, could give users more flexibility to make a difference between slugified urls and urls based on page id.",
        "Issue Links": []
    },
    "NUTCH-2238": {
        "Key": "NUTCH-2238",
        "Summary": "Indexer for Elasticsearch 2.x",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "plugin",
        "Assignee": "Pablo Torres",
        "Reporter": "Pablo Torres",
        "Created": "03/Mar/16 13:04",
        "Updated": "13/Apr/16 18:46",
        "Resolved": "13/Apr/16 18:32",
        "Description": "Add an additional plugin for Elasticsearch 2.x",
        "Issue Links": []
    },
    "NUTCH-2239": {
        "Key": "NUTCH-2239",
        "Summary": "Selenium Handlers for Ajax Patterns from Student submissions",
        "Type": "Improvement",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            protocol",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Raghav Bharadwaj Jayasimha Rao",
        "Created": "14/Mar/16 01:58",
        "Updated": "12/Jun/18 19:45",
        "Resolved": null,
        "Description": "Refactor student submissions from USC class of CSCI 572 to obtain a comprehensive set of selenium handlers for various Ajax Patterns",
        "Issue Links": []
    },
    "NUTCH-2240": {
        "Key": "NUTCH-2240",
        "Summary": "ava.lang.NoSuchFieldError: INSTANCE   selenium nutch",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "lq",
        "Created": "17/Mar/16 17:29",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "22/Nov/19 15:15",
        "Description": "java.lang.NoSuchFieldError: INSTANCE\n\tat org.apache.http.conn.ssl.SSLConnectionSocketFactory.<clinit>(SSLConnectionSocketFactory.java:144)\n\tat com.gargoylesoftware.htmlunit.HttpWebConnection.configureHttpsScheme(HttpWebConnection.java:597)\n\tat com.gargoylesoftware.htmlunit.HttpWebConnection.createHttpClient(HttpWebConnection.java:532)\n\tat com.gargoylesoftware.htmlunit.HttpWebConnection.getHttpClientBuilder(HttpWebConnection.java:494)\n\tat com.gargoylesoftware.htmlunit.HttpWebConnection.getResponse(HttpWebConnection.java:158)\n\tat org.apache.nutch.protocol.htmlunit.RegexHttpWebConnection.getResponse(RegexHttpWebConnection.java:63)\n\tat com.gargoylesoftware.htmlunit.WebClient.loadWebResponseFromWebConnection(WebClient.java:1321)\n\tat com.gargoylesoftware.htmlunit.WebClient.loadWebResponse(WebClient.java:1238)\n\tat com.gargoylesoftware.htmlunit.WebClient.getPage(WebClient.java:346)\n\tat com.gargoylesoftware.htmlunit.WebClient.getPage(WebClient.java:432)\n\tat org.apache.nutch.protocol.htmlunit.HttpWebClient.getPage(HttpWebClient.java:58)\n\tat org.apache.nutch.protocol.htmlunit.HttpWebClient.getHtmlPage(HttpWebClient.java:67)\n\tat org.apache.nutch.protocol.s2jh.HttpResponse.readPlainContentByHtmlunit(HttpResponse.java:345)\n\tat org.apache.nutch.protocol.s2jh.HttpResponse.<init>(HttpResponse.java:222)\n\tat org.apache.nutch.protocol.s2jh.Http.getResponse(Http.java:79)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:245)\n\tat org.apache.nutch.fetcher.FetcherReducer$FetcherThread.run(FetcherReducer.java:530)",
        "Issue Links": []
    },
    "NUTCH-2241": {
        "Key": "NUTCH-2241",
        "Summary": "Unstable Selenium plugin in Nutch. Fixed bugs and enhanced configuration",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.12",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Karanjeet Singh",
        "Created": "19/Mar/16 23:21",
        "Updated": "29/Mar/16 09:55",
        "Resolved": "20/Mar/16 00:43",
        "Description": "Issues:\n(a) Firefox browser doesn't close gracefully.\n(b) The property libselenium.page.load.delay is not working. No matter how much delay you give, the driver is not waiting for the page to load.\n(c) There is no timeout configured for the firefox binary.\n(d) A lot of selenium configuration is hard-coded which can be exposed through nutch-default.xml or nutch-site.xml\nAll these issues are part of \"lib-selenium\" plugin which is being used by two other protocols \"protocol-selenium\" and \"protocol-interactiveselenium\".",
        "Issue Links": []
    },
    "NUTCH-2242": {
        "Key": "NUTCH-2242",
        "Summary": "lastModified not always set",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.13",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "23/Mar/16 14:37",
        "Updated": "04/Nov/17 17:16",
        "Resolved": "23/Aug/16 08:45",
        "Description": "I observed two issues:\n\nWhen using the DefaultFetchSchedule, CrawlDatum's modifiedTime field is not updated on the first successful fetch.\nWhen a document modification is detected (protocol- or signature-wise), the modifiedTime isn't updated\n\nI can provide a patch later today.",
        "Issue Links": [
            "/jira/browse/NUTCH-2164",
            "https://github.com/apache/nutch/pull/238"
        ]
    },
    "NUTCH-2243": {
        "Key": "NUTCH-2243",
        "Summary": "Documentation for Nutch 2.X REST API",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "documentation,                                            REST_api",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "23/Mar/16 22:46",
        "Updated": "27/Jun/16 18:57",
        "Resolved": "27/Jun/16 18:23",
        "Description": "This issue should build on NUTCH-1769 with full Java documentation for all classes in the following packages:\norg.apache.nutch.api.*\nfor Nutch 2.x as done at NUTCH-1800 for Nutch 1.x",
        "Issue Links": [
            "/jira/browse/NUTCH-2199",
            "/jira/browse/NUTCH-1756",
            "https://github.com/apache/nutch/pull/123"
        ]
    },
    "NUTCH-2244": {
        "Key": "NUTCH-2244",
        "Summary": "Publish Protocol-Interactiveselenium to central maven repo",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Raghav Bharadwaj Jayasimha Rao",
        "Created": "30/Mar/16 16:45",
        "Updated": "09/Aug/22 07:04",
        "Resolved": "13/Jan/22 18:41",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-2292",
            "/jira/browse/NUTCH-2934"
        ]
    },
    "NUTCH-2245": {
        "Key": "NUTCH-2245",
        "Summary": "Developed the NGram Model on the existing Unigram Cosine Similarity Model",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12",
        "Component/s": "plugin,                                            scoring",
        "Assignee": "Sujen Shah",
        "Reporter": "Bhavya Sanghavi",
        "Created": "30/Mar/16 18:22",
        "Updated": "19/May/16 01:11",
        "Resolved": "26/Apr/16 17:33",
        "Description": "Built on the existing unigram cosine similarity model by adding the Ngram model, thus providing flexibility to the user to choose the window size for scoring the similarity between webpages and the gold standard.",
        "Issue Links": [
            "/jira/browse/NUTCH-2263"
        ]
    },
    "NUTCH-2246": {
        "Key": "NUTCH-1931 Apache Nutch 1.x REST service and crawler visualization",
        "Summary": "Refactor /seed endpoint for backward compatibility",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "03/Apr/16 02:33",
        "Updated": "23/Aug/16 08:43",
        "Resolved": "22/Aug/16 21:32",
        "Description": "Currently the seed endpoint allows you to create a seed list by providing a list of urls passed as an argument. \nAfter the first refactor here - https://issues.apache.org/jira/browse/NUTCH-2090. User could no longer provide a physical path to the seedlist. \nNutch should give both options to the user.\nAdditionally, once a seedlist is created by providing a list of urls (not a physical file), Nutch should store it like it does for the configurations.",
        "Issue Links": []
    },
    "NUTCH-2247": {
        "Key": "NUTCH-2247",
        "Summary": "Protocol resolver",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Apr/16 16:24",
        "Updated": "19/Aug/22 13:38",
        "Resolved": null,
        "Description": "Protocol resolver program capable of emitting rules for the urlnormalizer-protocol to ingest.",
        "Issue Links": []
    },
    "NUTCH-2248": {
        "Key": "NUTCH-2248",
        "Summary": "CSS parser plugin",
        "Type": "New Feature",
        "Status": "Reopened",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Joseph Naegele",
        "Created": "07/Apr/16 21:42",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "This plugin allows for collecting uri links from CSS (stylesheets). This is useful for collecting parent stylesheets, fonts, and images needed to display web pages as intended.\nParsed Outlinks do not have associated anchors, and no additional text/content is parsed from the stylesheet.",
        "Issue Links": []
    },
    "NUTCH-2249": {
        "Key": "NUTCH-2249",
        "Summary": "WordNet Integration for Cosine Similarity",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "plugin,                                            scoring",
        "Assignee": "Sujen Shah",
        "Reporter": "Bhavya Sanghavi",
        "Created": "12/Apr/16 22:15",
        "Updated": "22/Nov/19 15:13",
        "Resolved": null,
        "Description": "Integrated WordNet database to enhance the cosine similarity plugin. \nThis helps in reducing the size of the vectors for calculating the cosine similarity by mapping the synonymous words to the same entry in the vector. Consequently, it would increase the accuracy of the scores given to the webpages to be crawled.",
        "Issue Links": []
    },
    "NUTCH-2250": {
        "Key": "NUTCH-1949 Dump out the Nutch data into the Common Crawl format",
        "Summary": "CommonCrawlDumper : Invalid format + skipped parts",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10",
        "Fix Version/s": "1.12",
        "Component/s": "commoncrawl",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Thamme Gowda",
        "Created": "14/Apr/16 09:21",
        "Updated": "12/Jun/18 19:42",
        "Resolved": "17/Apr/16 22:32",
        "Description": "The following issues are found with CommonCrawlDumper;\n1. Documents get duplicated in dump files\nHow to reproduce \n\nbin/nutch commoncrawldump  -segment .../segments -outputDir testdump -SimpleDateFormat -epochFilename -jsonArray -reverseKey\n\n\nThe first ever written will contain 1 document.\nsecond file includes two documents\nthird file includes first three documents and this grows linearly.\n2.If a segment has many parts (part-00000, part-00001,...) only the first part (part-00000 ) is being dumped\nHow to reproduce ?\nCreate segment with two parts (part-00000 and part-00001)",
        "Issue Links": [
            "/jira/browse/NUTCH-2251"
        ]
    },
    "NUTCH-2251": {
        "Key": "NUTCH-1949 Dump out the Nutch data into the Common Crawl format",
        "Summary": "Make CommonCrawlFormatJackson instance reusable by properly handling object state",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "commoncrawl",
        "Assignee": null,
        "Reporter": "Thamme Gowda",
        "Created": "15/Apr/16 05:10",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "12/Jun/18 19:42",
        "Description": "The class `CommonCrawlFormatJackson` keeps appending the documents when it is used for more formatting more than one document. \nThis class shall be modified to handle states such that the same instance can be used instead of creating new one for each document being dumped.\nThis suggestion has been mentioned in the previous fix related to format issue : https://github.com/apache/nutch/pull/103",
        "Issue Links": [
            "/jira/browse/NUTCH-2250"
        ]
    },
    "NUTCH-2252": {
        "Key": "NUTCH-2252",
        "Summary": "Allow phantomjs as a browser for selenium options",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.12",
        "Component/s": "protocol",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Kim Whitehall",
        "Created": "16/Apr/16 16:08",
        "Updated": "09/May/16 20:56",
        "Resolved": "09/May/16 20:56",
        "Description": "Adding phantomjs libraries to lib-selenium so you can choose this as a browser with the selenium option",
        "Issue Links": []
    },
    "NUTCH-2253": {
        "Key": "NUTCH-2253",
        "Summary": "ProtocolFactory still not thread-safe",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.10,                                            1.11",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Leon Misakyan",
        "Created": "20/Apr/16 15:54",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "22/Nov/19 15:12",
        "Description": "Hi, as I can see in 1.11 release, ProtocolFactory class still has an issue in getProtocol method. This is because every fetcher thread has its own ProtocolFactory instance (this.protocolFactory = new ProtocolFactory(conf); in FetcherThread constructor.)\nSo have this method synchronized is useless, because each thread has its own monitor.\nIn our project we have issue of having multiple Protocol instances.\nIssue can be fixed if getProtocol method will use shared conf instance as lock object or by having one ProtocolFactory for all fetcher threads.",
        "Issue Links": [
            "/jira/browse/NUTCH-2625"
        ]
    },
    "NUTCH-2254": {
        "Key": "NUTCH-2254",
        "Summary": "Charset issues when using -addBinaryContent and -base64 options",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Federico Bonelli",
        "Created": "21/Apr/16 12:36",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "27/Apr/16 20:59",
        "Description": "The bug is reproducible with these steps:\n\nfind a site with cp1252 encoded pages like \"http://www.ilsole24ore.com/\" and characters with accents (byte representation >127, like [\u00e0\u00e8\u00e9\u00ec\u00f2\u00f9])\nstart a crawl on that site indexing on Solr with options -addBinaryContent -base64\nfind a document inside the newly indexed Solr collection with those accented characters\nget the base64 binary representation for said html page and decode it back to raw binary, save it\n\nThe file obtained will have invalid characters, which are neither UTF-8 nor cp1252.",
        "Issue Links": [
            "/jira/browse/NUTCH-1807",
            "/jira/browse/NUTCH-1785"
        ]
    },
    "NUTCH-2255": {
        "Key": "NUTCH-2255",
        "Summary": "WARCExporter to generate request records",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Julien Nioche",
        "Created": "27/Apr/16 12:46",
        "Updated": "27/Apr/16 12:50",
        "Resolved": null,
        "Description": "See discussion on http://lucene.472066.n3.nabble.com/Nutch-WARC-export-problems-td4270147.html\nWould be good to generate a warc info record at the beginning of the files but not sure of how doable this is given that we rely on a third party library for generating the WARC files. See https://github.com/ept/warc-hadoop/issues/4",
        "Issue Links": []
    },
    "NUTCH-2256": {
        "Key": "NUTCH-2256",
        "Summary": "Inconsistent log level practice",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2,                                            2.3,                                            1.11,                                            2.3.1",
        "Fix Version/s": "2.4,                                            1.12",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "songwanging",
        "Created": "28/Apr/16 22:02",
        "Updated": "29/Apr/16 17:06",
        "Resolved": "29/Apr/16 16:49",
        "Description": "In method \"run()\" of class: apache-nutch 2.3.1\\src\\java\\org\\apache\\nutch\\fetcher\\FetcherReducer.java\nThe log level is not correct, after checking \"LOG.isDebugEnabled()\", we should use \"LOG.debug(msg, e);\", while now we use \" LOG.info(msg, e);\". In this case, the log level is inconsistent and developers may lose debug messages because of this.\nThe related source code is as follows:\n if (LOG.isDebugEnabled()) {\n                  LOG.info(\"Crawl delay for queue: \" + fit.queueID\n                      + \" is set to \" + fiq.crawlDelay\n                      + \" as per robots.txt. url: \" + fit.url);\n}",
        "Issue Links": []
    },
    "NUTCH-2257": {
        "Key": "NUTCH-2257",
        "Summary": "apache-nutch-2.3.1-src.tar.gz can not be built",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Mirko Kaempf",
        "Created": "06/May/16 06:07",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "08/Jun/18 14:51",
        "Description": "The build fails for:\n   apache-nutch-2.3.1-src.tar.gz \nbut after replacing src folder from \n  apache-nutch-2.3.1-src.zip \nthe build works fine.\nError messages:\ncompile:\n[echo] Compiling plugin: indexer-solr\n[javac] Compiling 1 source file to /opt/examples/apache-nutch-2.3.1/build/indexer-solr/classes\n[javac] /opt/examples/apache-nutch-2.3.1/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrUtils.java:24: error: cannot find symbol\n[javac]     if (job.getBoolean(SolrConstants.USE_AUTH, false)) {\n[javac]                        ^\n[javac]   symbol:   variable SolrConstants\n[javac]   location: class SolrUtils\n[javac] /opt/examples/apache-nutch-2.3.1/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrUtils.java:25: error: cannot find symbol\n[javac]       String username = job.get(SolrConstants.USERNAME);\n[javac]                                 ^\n[javac]   symbol:   variable SolrConstants\n[javac]   location: class SolrUtils\n[javac] /opt/examples/apache-nutch-2.3.1/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrUtils.java:35: error: cannot find symbol\n[javac]               .get(SolrConstants.PASSWORD)));\n[javac]                    ^\n[javac]   symbol:   variable SolrConstants\n[javac]   location: class SolrUtils\n[javac] /opt/examples/apache-nutch-2.3.1/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrUtils.java:43: error: cannot find symbol\n[javac]     return new HttpSolrServer(job.get(SolrConstants.SERVER_URL), client);\n[javac]                                       ^\n[javac]   symbol:   variable SolrConstants\n[javac]   location: class SolrUtils\n[javac] 4 errors",
        "Issue Links": []
    },
    "NUTCH-2258": {
        "Key": "TINKERPOP-1301",
        "Summary": "Provide Javadoc for ScriptInput/OutputFormat's",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Done",
        "Affects Version/s": "3.2.0-incubating",
        "Fix Version/s": "3.1.3,                                            3.2.1",
        "Component/s": "documentation",
        "Assignee": "Stephen Mallette",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/May/16 23:32",
        "Updated": "16/Jun/16 12:00",
        "Resolved": "16/Jun/16 12:00",
        "Description": "Right now ScriptInputFormat and ScriptOutputFormat are not documented. Descriptions are however present over on some old TitanDB documentation which can be used to provide Javadoc level documentation for developers.",
        "Issue Links": [
            "https://github.com/apache/incubator-tinkerpop/pull/314",
            "https://github.com/apache/incubator-tinkerpop/pull/327"
        ]
    },
    "NUTCH-2259": {
        "Key": "NUTCH-2259",
        "Summary": "Nutch 2.x HBase Docker requires a logs folder to run exception free",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "docker",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/May/16 18:54",
        "Updated": "15/May/16 20:44",
        "Resolved": "14/May/16 19:02",
        "Description": "When running an inject\n\nhduser@028ce34179ee:/opt/nutch$ bin/nutch inject urls/seed.txt \nlog4j:ERROR setFile(null,true) call failed.\njava.io.FileNotFoundException: /opt/nutch/logs/hadoop.log (No such file or directory)\n    at java.io.FileOutputStream.open(Native Method)\n    at java.io.FileOutputStream.<init>(FileOutputStream.java:221)\n    at java.io.FileOutputStream.<init>(FileOutputStream.java:142)\n    at org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n    at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n    at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:223)\n    at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n    at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:172)\n    at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:104)\n    at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:842)\n    at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:768)\n    at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:648)\n    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:514)\n    at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:580)\n    at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n    at org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n    at org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:66)\n    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:277)\n    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:288)\n    at org.apache.nutch.crawl.InjectorJob.<clinit>(InjectorJob.java:64)\nlog4j:ERROR Either File or DatePattern options are not set for appender [DRFA].",
        "Issue Links": []
    },
    "NUTCH-2260": {
        "Key": "NUTCH-2260",
        "Summary": "JAVA_HOME and hbase-common dependency absent from hbase Docker image",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "build,                                            docker",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "15/May/16 20:56",
        "Updated": "16/May/16 23:53",
        "Resolved": "16/May/16 22:49",
        "Description": "Both JAVA_HOME (/usr) and the hbase-common transitive dependency are missing within the 2.x HBase Dockerfile. In order for the HBase stack to work out of the box we need to add both of these. Patch coming up.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/111"
        ]
    },
    "NUTCH-2261": {
        "Key": "NUTCH-2261",
        "Summary": "ParseSegment job does not pass metadata for content-level redirects",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11,                                            1.12,                                            1.13",
        "Fix Version/s": "1.20",
        "Component/s": "metadata,                                            parser",
        "Assignee": null,
        "Reporter": "David Astle",
        "Created": "16/May/16 18:54",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "When Fetcher runs in parsing mode, CrawlDatum metadata is properly passed to a new CrawlDatum for content-level redirects (HTML meta tag \"Refresh\").  If Fetcher runs in non-parsing mode, and ParseSegment is run as a separate step, then metadata other than \"repr\" is not passed to the new CrawlDatum.\nThis means that any filter relying on metadata, such as DepthScoringFilter and URLMetaScoringFilter, will not work.",
        "Issue Links": [
            "/jira/browse/NUTCH-685"
        ]
    },
    "NUTCH-2262": {
        "Key": "NUTCH-2262",
        "Summary": "Utilize parameterized logging notation across Fetcher",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.13",
        "Component/s": "fetcher",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "17/May/16 16:32",
        "Updated": "29/Jun/16 15:10",
        "Resolved": "29/Jun/16 15:10",
        "Description": "This issue was something I have had lying around for a wee while. It merely consists of implementing the parameterized logging for slf4j which improves logging speed. PR coming up.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/112"
        ]
    },
    "NUTCH-2263": {
        "Key": "NUTCH-2263",
        "Summary": "Support for mingram and maxgram at Unigram Cosine Similarity Model",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.12",
        "Component/s": "plugin,                                            scoring",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "19/May/16 01:11",
        "Updated": "20/May/16 01:56",
        "Resolved": "20/May/16 01:35",
        "Description": "Ngram model should support both mingram and maxgram. If maxgram is not defined, it should work as existing implementation (maxgram should be equal to mingram).",
        "Issue Links": [
            "/jira/browse/NUTCH-2245",
            "https://github.com/apache/nutch/pull/114"
        ]
    },
    "NUTCH-2264": {
        "Key": "NUTCH-2264",
        "Summary": "Check Forbidden APIs at Build",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "None",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "19/May/16 22:11",
        "Updated": "30/Aug/16 17:52",
        "Resolved": "30/Aug/16 16:55",
        "Description": "We should avoid forbidden calls  and check in the ant build for it.",
        "Issue Links": [
            "/jira/browse/NUTCH-1807",
            "https://github.com/apache/nutch/pull/148"
        ]
    },
    "NUTCH-2265": {
        "Key": "NUTCH-2265",
        "Summary": "Write A Test Package for Scoring Similarity",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "plugin,                                            scoring",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "21/May/16 22:43",
        "Updated": "22/Nov/19 15:04",
        "Resolved": null,
        "Description": "There is no test package for org.apache.nutch.scoring.similarity and it should be implemented.",
        "Issue Links": []
    },
    "NUTCH-2266": {
        "Key": "NUTCH-2266",
        "Summary": "Fix dead link in build.xml for javadoc",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "build",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "23/May/16 20:45",
        "Updated": "24/May/16 13:39",
        "Resolved": "24/May/16 12:42",
        "Description": "build.xml has a dead link for javadoc.link.lucene and should be fixed.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/117"
        ]
    },
    "NUTCH-2267": {
        "Key": "NUTCH-2267",
        "Summary": "Solr indexer fails at the end of the job with a java error message",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "kaveh minooie",
        "Created": "24/May/16 00:01",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "12/Jun/18 19:37",
        "Description": "this is was what I was getting first:\n16/05/23 13:52:27 INFO mapreduce.Job:  map 100% reduce 100%\n16/05/23 13:52:27 INFO mapreduce.Job: Task Id : attempt_1462499602101_0119_r_000000_0, Status : FAILED\nError: Bad return type\nException Details:\n  Location:\norg/apache/solr/client/solrj/impl/HttpClientUtil.createClient(Lorg/apache/solr/common/params/SolrParams;Lorg/apache/http/conn/ClientConnectionManager;)Lorg/apache/http/impl/client/CloseableHttpClient; @58: areturn\n  Reason:\n    Type 'org/apache/http/impl/client/DefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient' (from method signature)\n  Current Frame:\n    bci: @58\n    flags: { }\n    locals: \n{ 'org/apache/solr/common/params/SolrParams', 'org/apache/http/conn/ClientConnectionManager', 'org/apache/solr/common/params/ModifiableSolrParams', 'org/apache/http/impl/client/DefaultHttpClient' }\n    stack: \n{ 'org/apache/http/impl/client/DefaultHttpClient' }\n  Bytecode:\n    0x0000000: bb00 0359 2ab7 0004 4db2 0005 b900 0601\n    0x0000010: 0099 001e b200 05bb 0007 59b7 0008 1209\n    0x0000020: b600 0a2c b600 0bb6 000c b900 0d02 002b\n    0x0000030: b800 104e 2d2c b800 0f2d b0\n  Stackmap Table:\n    append_frame(@47,Object143)\n16/05/23 13:52:28 INFO mapreduce.Job:  map 100% reduce 0% \nas you can see the failed reducer gets re-spawned. then I found this issue: \nhttps://issues.apache.org/jira/browse/SOLR-7657 and I updated my hadoop config file. after that, the indexer seems to be able to finish ( I got the document in the solr, it seems ) but I still get the error message at the end of the job:\n16/05/23 16:39:26 INFO mapreduce.Job:  map 100% reduce 99%\n16/05/23 16:39:44 INFO mapreduce.Job:  map 100% reduce 100%\n16/05/23 16:39:57 INFO mapreduce.Job: Job job_1464045047943_0001 completed successfully\n16/05/23 16:39:58 INFO mapreduce.Job: Counters: 53\n\tFile System Counters\n\t\tFILE: Number of bytes read=42700154855\n\t\tFILE: Number of bytes written=70210771807\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=8699202825\n\t\tHDFS: Number of bytes written=0\n\t\tHDFS: Number of read operations=537\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=0\n\tJob Counters \n\t\tLaunched map tasks=134\n\t\tLaunched reduce tasks=1\n\t\tData-local map tasks=107\n\t\tRack-local map tasks=27\n\t\tTotal time spent by all maps in occupied slots (ms)=49377664\n\t\tTotal time spent by all reduces in occupied slots (ms)=32765064\n\t\tTotal time spent by all map tasks (ms)=3086104\n\t\tTotal time spent by all reduce tasks (ms)=1365211\n\t\tTotal vcore-milliseconds taken by all map tasks=3086104\n\t\tTotal vcore-milliseconds taken by all reduce tasks=1365211\n\t\tTotal megabyte-milliseconds taken by all map tasks=12640681984\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=8387856384\n\tMap-Reduce Framework\n\t\tMap input records=25305474\n\t\tMap output records=25305474\n\t\tMap output bytes=27422869763\n\t\tMap output materialized bytes=27489888004\n\t\tInput split bytes=15225\n\t\tCombine input records=0\n\t\tCombine output records=0\n\t\tReduce input groups=16061459\n\t\tReduce shuffle bytes=27489888004\n\t\tReduce input records=25305474\n\t\tReduce output records=230\n\t\tSpilled Records=54688613\n\t\tShuffled Maps =134\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=134\n\t\tGC time elapsed (ms)=88103\n\t\tCPU time spent (ms)=3361270\n\t\tPhysical memory (bytes) snapshot=144395186176\n\t\tVirtual memory (bytes) snapshot=751590166528\n\t\tTotal committed heap usage (bytes)=156232056832\n\tIndexerStatus\n\t\tindexed (add/update)=230\n\tShuffle Errors\n\t\tBAD_ID=0\n\t\tCONNECTION=0\n\t\tIO_ERROR=0\n\t\tWRONG_LENGTH=0\n\t\tWRONG_MAP=0\n\t\tWRONG_REDUCE=0\n\tSkippingTaskCounters\n\t\tMapProcessedRecords=25305474\n\t\tReduceProcessedGroups=16061459\n\tFile Input Format Counters \n\t\tBytes Read=8699187600\n\tFile Output Format Counters \n\t\tBytes Written=0\nException in thread \"main\" java.lang.VerifyError: Bad return type\nException Details:\n  Location:\n    org/apache/solr/client/solrj/impl/HttpClientUtil.createClient(Lorg/apache/solr/common/params/SolrParams;)Lorg/apache/http/impl/client/CloseableHttpClient; @57: areturn\n  Reason:\n    Type 'org/apache/http/impl/client/SystemDefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient' (from method signature)\n  Current Frame:\n    bci: @57\n    flags: { }\n    locals: \n{ 'org/apache/solr/common/params/SolrParams', 'org/apache/solr/common/params/ModifiableSolrParams', 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n    stack: \n{ 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n  Bytecode:\n    0x0000000: bb00 0359 2ab7 0004 4cb2 0005 b900 0601\n    0x0000010: 0099 001e b200 05bb 0007 59b7 0008 1209\n    0x0000020: b600 0a2b b600 0bb6 000c b900 0d02 00b8\n    0x0000030: 000e 4d2c 2bb8 000f 2cb0               \n  Stackmap Table:\n    append_frame(@47,Object143)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.<init>(HttpSolrClient.java:189)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.<init>(HttpSolrClient.java:162)\n\tat org.apache.nutch.indexwriter.solr.SolrUtils.getSolrClients(SolrUtils.java:54)\n\tat org.apache.nutch.indexwriter.solr.SolrIndexWriter.open(SolrIndexWriter.java:78)\n\tat org.apache.nutch.indexer.IndexWriters.open(IndexWriters.java:75)\n\tat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:148)\n\tat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:228)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:237)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
        "Issue Links": [
            "/jira/browse/NUTCH-2226",
            "/jira/browse/NUTCH-2270",
            "https://github.com/apache/nutch/pull/129"
        ]
    },
    "NUTCH-2268": {
        "Key": "NUTCH-2268",
        "Summary": "SolrIndexerJob: java.lang.RuntimeException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "narendra",
        "Created": "26/May/16 15:17",
        "Updated": "22/Nov/19 15:03",
        "Resolved": "22/Nov/19 15:03",
        "Description": "Could you please help out of this error \nSolrIndexerJob: java.lang.RuntimeException: job failed:name=apache-nutch-2.3.1.jar   \nwhen i run this commend \nlocal/bin/nutch solrindex http://localhost:8983/solr/ -all\nTried with Solr 4.10.3 but same error iam getting",
        "Issue Links": []
    },
    "NUTCH-2269": {
        "Key": "NUTCH-2269",
        "Summary": "Clean not working after crawl",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Francesco Capponi",
        "Created": "30/May/16 10:30",
        "Updated": "06/Apr/17 12:52",
        "Resolved": "06/Apr/17 12:23",
        "Description": "I'm have been having this problem for a while and I had to rollback using the old solr clean instead of the newer version. \nOnce it inserts/update correctly every document in Nutch, when it tries to clean, it returns error 255:\n\n2016-05-30 10:13:04,992 WARN  output.FileOutputCommitter - Output Path is null in setupJob()\n2016-05-30 10:13:07,284 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n2016-05-30 10:13:08,114 INFO  solr.SolrMappingReader - source: content dest: content\n2016-05-30 10:13:08,114 INFO  solr.SolrMappingReader - source: title dest: title\n2016-05-30 10:13:08,114 INFO  solr.SolrMappingReader - source: host dest: host\n2016-05-30 10:13:08,114 INFO  solr.SolrMappingReader - source: segment dest: segment\n2016-05-30 10:13:08,114 INFO  solr.SolrMappingReader - source: boost dest: boost\n2016-05-30 10:13:08,114 INFO  solr.SolrMappingReader - source: digest dest: digest\n2016-05-30 10:13:08,114 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2016-05-30 10:13:08,133 INFO  solr.SolrIndexWriter - SolrIndexer: deleting 15/15 documents\n2016-05-30 10:13:08,919 WARN  output.FileOutputCommitter - Output Path is null in cleanupJob()\n2016-05-30 10:13:08,937 WARN  mapred.LocalJobRunner - job_local662730477_0001\njava.lang.Exception: java.lang.IllegalStateException: Connection pool shut down\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: java.lang.IllegalStateException: Connection pool shut down\n\tat org.apache.http.util.Asserts.check(Asserts.java:34)\n\tat org.apache.http.pool.AbstractConnPool.lease(AbstractConnPool.java:169)\n\tat org.apache.http.pool.AbstractConnPool.lease(AbstractConnPool.java:202)\n\tat org.apache.http.impl.conn.PoolingClientConnectionManager.requestConnection(PoolingClientConnectionManager.java:184)\n\tat org.apache.http.impl.client.DefaultRequestDirector.execute(DefaultRequestDirector.java:415)\n\tat org.apache.http.impl.client.AbstractHttpClient.doExecute(AbstractHttpClient.java:863)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:106)\n\tat org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:57)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:480)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:241)\n\tat org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:230)\n\tat org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:150)\n\tat org.apache.solr.client.solrj.SolrClient.commit(SolrClient.java:483)\n\tat org.apache.solr.client.solrj.SolrClient.commit(SolrClient.java:464)\n\tat org.apache.nutch.indexwriter.solr.SolrIndexWriter.commit(SolrIndexWriter.java:190)\n\tat org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:178)\n\tat org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:115)\n\tat org.apache.nutch.indexer.CleaningJob$DeleterReducer.close(CleaningJob.java:120)\n\tat org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:237)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n2016-05-30 10:13:09,299 ERROR indexer.CleaningJob - CleaningJob: java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n\tat org.apache.nutch.indexer.CleaningJob.delete(CleaningJob.java:172)\n\tat org.apache.nutch.indexer.CleaningJob.run(CleaningJob.java:195)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.indexer.CleaningJob.main(CleaningJob.java:206)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/156"
        ]
    },
    "NUTCH-2270": {
        "Key": "NUTCH-2270",
        "Summary": "Solr indexer Failed i",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "narendra",
        "Created": "01/Jun/16 05:04",
        "Updated": "22/Nov/19 13:12",
        "Resolved": "15/Nov/19 12:10",
        "Description": "When i run this command\n bin/nutch solrindex http://localhost:8983/solr/#/gettingstarted crawl_Test1/crawldb -linkdb crawl_Test1/linkdb  crawl_Test1/segments/*\n16/05/31 22:21:47 WARN segment.SegmentChecker: The input path at * is not a segment... skipping\n16/05/31 22:21:47 INFO indexer.IndexingJob: Indexer: starting at 2016-05-31 22:21:47\n16/05/31 22:21:47 INFO indexer.IndexingJob: Indexer: deleting gone documents: false\n16/05/31 22:21:47 INFO indexer.IndexingJob: Indexer: URL filtering: false\n16/05/31 22:21:47 INFO indexer.IndexingJob: Indexer: URL normalizing: false\n16/05/31 22:21:47 INFO plugin.PluginRepository: Plugins: looking in: /tmp/hadoop-unjar8621976524622577403/classes/plugins\n16/05/31 22:21:47 INFO plugin.PluginRepository: Plugin Auto-activation mode: [true]\n16/05/31 22:21:47 INFO plugin.PluginRepository: Registered Plugins:\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tRegex URL Filter (urlfilter-regex)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tHtml Parse Plug-in (parse-html)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tHTTP Framework (lib-http)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tthe nutch core extension points (nutch-extensionpoints)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tBasic Indexing Filter (index-basic)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tAnchor Indexing Filter (index-anchor)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tTika Parser Plug-in (parse-tika)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tBasic URL Normalizer (urlnormalizer-basic)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tRegex URL Filter Framework (lib-regex-filter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tRegex URL Normalizer (urlnormalizer-regex)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tCyberNeko HTML Parser (lib-nekohtml)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tOPIC Scoring Plug-in (scoring-opic)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tPass-through URL Normalizer (urlnormalizer-pass)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tHttp Protocol Plug-in (protocol-http)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tSolrIndexWriter (indexer-solr)\n16/05/31 22:21:47 INFO plugin.PluginRepository: Registered Extension-Points:\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Content Parser (org.apache.nutch.parse.Parser)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch URL Filter (org.apache.nutch.net.URLFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tHTML Parse Filter (org.apache.nutch.parse.HtmlParseFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Scoring (org.apache.nutch.scoring.ScoringFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch URL Normalizer (org.apache.nutch.net.URLNormalizer)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Protocol (org.apache.nutch.protocol.Protocol)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch URL Ignore Exemption Filter (org.apache.nutch.net.URLExemptionFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Index Writer (org.apache.nutch.indexer.IndexWriter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Segment Merge Filter (org.apache.nutch.segment.SegmentMergeFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n16/05/31 22:21:47 INFO indexer.IndexWriters: Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n16/05/31 22:21:47 INFO indexer.IndexingJob: Active IndexWriters :\nSOLRIndexWriter\n\tsolr.server.url : URL of the SOLR instance\n\tsolr.zookeeper.hosts : URL of the Zookeeper quorum\n\tsolr.commit.size : buffer size when sending to SOLR (default 1000)\n\tsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n\tsolr.auth : use authentication (default false)\n\tsolr.auth.username : username for authentication\n\tsolr.auth.password : password for authentication\n16/05/31 22:21:47 INFO indexer.IndexerMapReduce: IndexerMapReduce: crawldb: crawl_Test1/crawldb\n16/05/31 22:21:47 INFO indexer.IndexerMapReduce: IndexerMapReduce: linkdb: crawl_Test1/linkdb\n16/05/31 22:21:48 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n16/05/31 22:21:48 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n16/05/31 22:21:54 INFO mapred.FileInputFormat: Total input paths to process : 2\n16/05/31 22:21:54 INFO mapreduce.JobSubmitter: number of splits:3\n16/05/31 22:21:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464692893405_0045\n16/05/31 22:21:55 INFO impl.YarnClientImpl: Submitted application application_1464692893405_0045\n16/05/31 22:21:55 INFO mapreduce.Job: The url to track the job: http://localhost:9046/proxy/application_1464692893405_0045/\n16/05/31 22:21:55 INFO mapreduce.Job: Running job: job_1464692893405_0045\n16/05/31 22:22:16 INFO mapreduce.Job: Job job_1464692893405_0045 running in uber mode : false\n16/05/31 22:22:16 INFO mapreduce.Job:  map 0% reduce 0%\n16/05/31 22:22:28 INFO mapreduce.Job:  map 100% reduce 0%\n16/05/31 22:22:33 INFO mapreduce.Job: Task Id : attempt_1464692893405_0045_r_000000_0, Status : FAILED\nError: Bad return type\nException Details:\n  Location:\n    org/apache/solr/client/solrj/impl/HttpClientUtil.createClient(Lorg/apache/solr/common/params/SolrParams;)Lorg/apache/http/impl/client/CloseableHttpClient; @57: areturn\n  Reason:\n    Type 'org/apache/http/impl/client/SystemDefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient' (from method signature)\n  Current Frame:\n    bci: @57\n    flags: { }\n    locals: \n{ 'org/apache/solr/common/params/SolrParams', 'org/apache/solr/common/params/ModifiableSolrParams', 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n    stack: { 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n  Bytecode:\n    0x0000000: bb00 0359 2ab7 0004 4cb2 0005 b900 0601\n    0x0000010: 0099 001e b200 05bb 0007 59b7 0008 1209\n    0x0000020: b600 0a2b b600 0bb6 000c b900 0d02 00b8\n    0x0000030: 000e 4d2c 2bb8 000f 2cb0               \n  Stackmap Table:\n    append_frame(@47,Object143)\n\n16/05/31 22:22:40 INFO mapreduce.Job: Task Id : attempt_1464692893405_0045_r_000000_1, Status : FAILED\nError: Bad return type\nException Details:\n  Location:\n    org/apache/solr/client/solrj/impl/HttpClientUtil.createClient(Lorg/apache/solr/common/params/SolrParams;)Lorg/apache/http/impl/client/CloseableHttpClient; @57: areturn\n  Reason:\n    Type 'org/apache/http/impl/client/SystemDefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient' (from method signature)\n  Current Frame:\n    bci: @57\n    flags: { }\n    locals: { 'org/apache/solr/common/params/SolrParams', 'org/apache/solr/common/params/ModifiableSolrParams', 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n    stack: \n{ 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n  Bytecode:\n    0x0000000: bb00 0359 2ab7 0004 4cb2 0005 b900 0601\n    0x0000010: 0099 001e b200 05bb 0007 59b7 0008 1209\n    0x0000020: b600 0a2b b600 0bb6 000c b900 0d02 00b8\n    0x0000030: 000e 4d2c 2bb8 000f 2cb0               \n  Stackmap Table:\n    append_frame(@47,Object143)\n16/05/31 22:22:46 INFO mapreduce.Job: Task Id : attempt_1464692893405_0045_r_000000_2, Status : FAILED\nError: Bad return type\nException Details:\n  Location:\n    org/apache/solr/client/solrj/impl/HttpClientUtil.createClient(Lorg/apache/solr/common/params/SolrParams;Lorg/apache/http/conn/ClientConnectionManager;)Lorg/apache/http/impl/client/CloseableHttpClient; @58: areturn\n  Reason:\n    Type 'org/apache/http/impl/client/DefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient' (from method signature)\n  Current Frame:\n    bci: @58\n    flags: { }\n    locals: \n{ 'org/apache/solr/common/params/SolrParams', 'org/apache/http/conn/ClientConnectionManager', 'org/apache/solr/common/params/ModifiableSolrParams', 'org/apache/http/impl/client/DefaultHttpClient' }\n    stack: \n{ 'org/apache/http/impl/client/DefaultHttpClient' }\n  Bytecode:\n    0x0000000: bb00 0359 2ab7 0004 4db2 0005 b900 0601\n    0x0000010: 0099 001e b200 05bb 0007 59b7 0008 1209\n    0x0000020: b600 0a2c b600 0bb6 000c b900 0d02 002b\n    0x0000030: b800 104e 2d2c b800 0f2d b0            \n  Stackmap Table:\n    append_frame(@47,Object143)\n16/05/31 22:22:53 INFO mapreduce.Job:  map 100% reduce 100%\n16/05/31 22:22:53 INFO mapreduce.Job: Job job_1464692893405_0045 failed with state FAILED due to: Task failed task_1464692893405_0045_r_000000\nJob failed as tasks failed. failedMaps:0 failedReduces:1\n16/05/31 22:22:54 INFO mapreduce.Job: Counters: 37\n\tFile System Counters\n\t\tFILE: Number of bytes read=0\n\t\tFILE: Number of bytes written=458051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=17460\n\t\tHDFS: Number of bytes written=0\n\t\tHDFS: Number of read operations=12\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=0\n\tJob Counters \n\t\tFailed reduce tasks=4\n\t\tLaunched map tasks=3\n\t\tLaunched reduce tasks=4\n\t\tData-local map tasks=3\n\t\tTotal time spent by all maps in occupied slots (ms)=56496\n\t\tTotal time spent by all reduces in occupied slots (ms)=30056\n\t\tTotal time spent by all map tasks (ms)=28248\n\t\tTotal time spent by all reduce tasks (ms)=15028\n\t\tTotal vcore-milliseconds taken by all map tasks=28248\n\t\tTotal vcore-milliseconds taken by all reduce tasks=15028\n\t\tTotal megabyte-milliseconds taken by all map tasks=28925952\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=15388672\n\tMap-Reduce Framework\n\t\tMap input records=184\n\t\tMap output records=184\n\t\tMap output bytes=15037\n\t\tMap output materialized bytes=15428\n\t\tInput split bytes=392\n\t\tCombine input records=0\n\t\tSpilled Records=184\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=0\n\t\tGC time elapsed (ms)=758\n\t\tCPU time spent (ms)=6200\n\t\tPhysical memory (bytes) snapshot=841703424\n\t\tVirtual memory (bytes) snapshot=5765849088\n\t\tTotal committed heap usage (bytes)=611319808\n\tFile Input Format Counters \n\t\tBytes Read=17068\n16/05/31 22:22:54 ERROR indexer.IndexingJob: Indexer: java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)\n\tat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n\tat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:228)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:237)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
        "Issue Links": [
            "/jira/browse/NUTCH-2267",
            "/jira/browse/NUTCH-2271"
        ]
    },
    "NUTCH-2271": {
        "Key": "NUTCH-2271",
        "Summary": "Solr indexer Failed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Bug",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": "Furkan Kamaci",
        "Reporter": "narendra",
        "Created": "01/Jun/16 05:05",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "05/Jun/16 20:59",
        "Description": "When i run this command\n  bin/nutch solrindex http://localhost:8983/solr/#/devel1 crawl_Test1/crawldb -linkdb crawl_Test1/linkdb  crawl_Test1/segments/*\n16/05/31 22:21:47 WARN segment.SegmentChecker: The input path at * is not a segment... skipping\n16/05/31 22:21:47 INFO indexer.IndexingJob: Indexer: starting at 2016-05-31 22:21:47\n16/05/31 22:21:47 INFO indexer.IndexingJob: Indexer: deleting gone documents: false\n16/05/31 22:21:47 INFO indexer.IndexingJob: Indexer: URL filtering: false\n16/05/31 22:21:47 INFO indexer.IndexingJob: Indexer: URL normalizing: false\n16/05/31 22:21:47 INFO plugin.PluginRepository: Plugins: looking in: /tmp/hadoop-unjar8621976524622577403/classes/plugins\n16/05/31 22:21:47 INFO plugin.PluginRepository: Plugin Auto-activation mode: [true]\n16/05/31 22:21:47 INFO plugin.PluginRepository: Registered Plugins:\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tRegex URL Filter (urlfilter-regex)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tHtml Parse Plug-in (parse-html)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tHTTP Framework (lib-http)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tthe nutch core extension points (nutch-extensionpoints)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tBasic Indexing Filter (index-basic)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tAnchor Indexing Filter (index-anchor)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tTika Parser Plug-in (parse-tika)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tBasic URL Normalizer (urlnormalizer-basic)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tRegex URL Filter Framework (lib-regex-filter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tRegex URL Normalizer (urlnormalizer-regex)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tCyberNeko HTML Parser (lib-nekohtml)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tOPIC Scoring Plug-in (scoring-opic)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tPass-through URL Normalizer (urlnormalizer-pass)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tHttp Protocol Plug-in (protocol-http)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tSolrIndexWriter (indexer-solr)\n16/05/31 22:21:47 INFO plugin.PluginRepository: Registered Extension-Points:\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Content Parser (org.apache.nutch.parse.Parser)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch URL Filter (org.apache.nutch.net.URLFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tHTML Parse Filter (org.apache.nutch.parse.HtmlParseFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Scoring (org.apache.nutch.scoring.ScoringFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch URL Normalizer (org.apache.nutch.net.URLNormalizer)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Protocol (org.apache.nutch.protocol.Protocol)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch URL Ignore Exemption Filter (org.apache.nutch.net.URLExemptionFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Index Writer (org.apache.nutch.indexer.IndexWriter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Segment Merge Filter (org.apache.nutch.segment.SegmentMergeFilter)\n16/05/31 22:21:47 INFO plugin.PluginRepository: \tNutch Indexing Filter (org.apache.nutch.indexer.IndexingFilter)\n16/05/31 22:21:47 INFO indexer.IndexWriters: Adding org.apache.nutch.indexwriter.solr.SolrIndexWriter\n16/05/31 22:21:47 INFO indexer.IndexingJob: Active IndexWriters :\nSOLRIndexWriter\n\tsolr.server.url : URL of the SOLR instance\n\tsolr.zookeeper.hosts : URL of the Zookeeper quorum\n\tsolr.commit.size : buffer size when sending to SOLR (default 1000)\n\tsolr.mapping.file : name of the mapping file for fields (default solrindex-mapping.xml)\n\tsolr.auth : use authentication (default false)\n\tsolr.auth.username : username for authentication\n\tsolr.auth.password : password for authentication\n16/05/31 22:21:47 INFO indexer.IndexerMapReduce: IndexerMapReduce: crawldb: crawl_Test1/crawldb\n16/05/31 22:21:47 INFO indexer.IndexerMapReduce: IndexerMapReduce: linkdb: crawl_Test1/linkdb\n16/05/31 22:21:48 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n16/05/31 22:21:48 INFO client.RMProxy: Connecting to ResourceManager at localhost/127.0.0.1:8032\n16/05/31 22:21:54 INFO mapred.FileInputFormat: Total input paths to process : 2\n16/05/31 22:21:54 INFO mapreduce.JobSubmitter: number of splits:3\n16/05/31 22:21:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464692893405_0045\n16/05/31 22:21:55 INFO impl.YarnClientImpl: Submitted application application_1464692893405_0045\n16/05/31 22:21:55 INFO mapreduce.Job: The url to track the job: http://localhost:9046/proxy/application_1464692893405_0045/\n16/05/31 22:21:55 INFO mapreduce.Job: Running job: job_1464692893405_0045\n16/05/31 22:22:16 INFO mapreduce.Job: Job job_1464692893405_0045 running in uber mode : false\n16/05/31 22:22:16 INFO mapreduce.Job:  map 0% reduce 0%\n16/05/31 22:22:28 INFO mapreduce.Job:  map 100% reduce 0%\n16/05/31 22:22:33 INFO mapreduce.Job: Task Id : attempt_1464692893405_0045_r_000000_0, Status : FAILED\nError: Bad return type\nException Details:\n  Location:\n    org/apache/solr/client/solrj/impl/HttpClientUtil.createClient(Lorg/apache/solr/common/params/SolrParams;)Lorg/apache/http/impl/client/CloseableHttpClient; @57: areturn\n  Reason:\n    Type 'org/apache/http/impl/client/SystemDefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient' (from method signature)\n  Current Frame:\n    bci: @57\n    flags: { }\n    locals: \n{ 'org/apache/solr/common/params/SolrParams', 'org/apache/solr/common/params/ModifiableSolrParams', 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n    stack: { 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n  Bytecode:\n    0x0000000: bb00 0359 2ab7 0004 4cb2 0005 b900 0601\n    0x0000010: 0099 001e b200 05bb 0007 59b7 0008 1209\n    0x0000020: b600 0a2b b600 0bb6 000c b900 0d02 00b8\n    0x0000030: 000e 4d2c 2bb8 000f 2cb0               \n  Stackmap Table:\n    append_frame(@47,Object143)\n\n16/05/31 22:22:40 INFO mapreduce.Job: Task Id : attempt_1464692893405_0045_r_000000_1, Status : FAILED\nError: Bad return type\nException Details:\n  Location:\n    org/apache/solr/client/solrj/impl/HttpClientUtil.createClient(Lorg/apache/solr/common/params/SolrParams;)Lorg/apache/http/impl/client/CloseableHttpClient; @57: areturn\n  Reason:\n    Type 'org/apache/http/impl/client/SystemDefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient' (from method signature)\n  Current Frame:\n    bci: @57\n    flags: { }\n    locals: { 'org/apache/solr/common/params/SolrParams', 'org/apache/solr/common/params/ModifiableSolrParams', 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n    stack: \n{ 'org/apache/http/impl/client/SystemDefaultHttpClient' }\n  Bytecode:\n    0x0000000: bb00 0359 2ab7 0004 4cb2 0005 b900 0601\n    0x0000010: 0099 001e b200 05bb 0007 59b7 0008 1209\n    0x0000020: b600 0a2b b600 0bb6 000c b900 0d02 00b8\n    0x0000030: 000e 4d2c 2bb8 000f 2cb0               \n  Stackmap Table:\n    append_frame(@47,Object143)\n16/05/31 22:22:46 INFO mapreduce.Job: Task Id : attempt_1464692893405_0045_r_000000_2, Status : FAILED\nError: Bad return type\nException Details:\n  Location:\n    org/apache/solr/client/solrj/impl/HttpClientUtil.createClient(Lorg/apache/solr/common/params/SolrParams;Lorg/apache/http/conn/ClientConnectionManager;)Lorg/apache/http/impl/client/CloseableHttpClient; @58: areturn\n  Reason:\n    Type 'org/apache/http/impl/client/DefaultHttpClient' (current frame, stack[0]) is not assignable to 'org/apache/http/impl/client/CloseableHttpClient' (from method signature)\n  Current Frame:\n    bci: @58\n    flags: { }\n    locals: \n{ 'org/apache/solr/common/params/SolrParams', 'org/apache/http/conn/ClientConnectionManager', 'org/apache/solr/common/params/ModifiableSolrParams', 'org/apache/http/impl/client/DefaultHttpClient' }\n    stack: \n{ 'org/apache/http/impl/client/DefaultHttpClient' }\n  Bytecode:\n    0x0000000: bb00 0359 2ab7 0004 4db2 0005 b900 0601\n    0x0000010: 0099 001e b200 05bb 0007 59b7 0008 1209\n    0x0000020: b600 0a2c b600 0bb6 000c b900 0d02 002b\n    0x0000030: b800 104e 2d2c b800 0f2d b0            \n  Stackmap Table:\n    append_frame(@47,Object143)\n16/05/31 22:22:53 INFO mapreduce.Job:  map 100% reduce 100%\n16/05/31 22:22:53 INFO mapreduce.Job: Job job_1464692893405_0045 failed with state FAILED due to: Task failed task_1464692893405_0045_r_000000\nJob failed as tasks failed. failedMaps:0 failedReduces:1\n16/05/31 22:22:54 INFO mapreduce.Job: Counters: 37\n\tFile System Counters\n\t\tFILE: Number of bytes read=0\n\t\tFILE: Number of bytes written=458051\n\t\tFILE: Number of read operations=0\n\t\tFILE: Number of large read operations=0\n\t\tFILE: Number of write operations=0\n\t\tHDFS: Number of bytes read=17460\n\t\tHDFS: Number of bytes written=0\n\t\tHDFS: Number of read operations=12\n\t\tHDFS: Number of large read operations=0\n\t\tHDFS: Number of write operations=0\n\tJob Counters \n\t\tFailed reduce tasks=4\n\t\tLaunched map tasks=3\n\t\tLaunched reduce tasks=4\n\t\tData-local map tasks=3\n\t\tTotal time spent by all maps in occupied slots (ms)=56496\n\t\tTotal time spent by all reduces in occupied slots (ms)=30056\n\t\tTotal time spent by all map tasks (ms)=28248\n\t\tTotal time spent by all reduce tasks (ms)=15028\n\t\tTotal vcore-milliseconds taken by all map tasks=28248\n\t\tTotal vcore-milliseconds taken by all reduce tasks=15028\n\t\tTotal megabyte-milliseconds taken by all map tasks=28925952\n\t\tTotal megabyte-milliseconds taken by all reduce tasks=15388672\n\tMap-Reduce Framework\n\t\tMap input records=184\n\t\tMap output records=184\n\t\tMap output bytes=15037\n\t\tMap output materialized bytes=15428\n\t\tInput split bytes=392\n\t\tCombine input records=0\n\t\tSpilled Records=184\n\t\tFailed Shuffles=0\n\t\tMerged Map outputs=0\n\t\tGC time elapsed (ms)=758\n\t\tCPU time spent (ms)=6200\n\t\tPhysical memory (bytes) snapshot=841703424\n\t\tVirtual memory (bytes) snapshot=5765849088\n\t\tTotal committed heap usage (bytes)=611319808\n\tFile Input Format Counters \n\t\tBytes Read=17068\n16/05/31 22:22:54 ERROR indexer.IndexingJob: Indexer: java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:865)\n\tat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n\tat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:228)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:237)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
        "Issue Links": [
            "/jira/browse/NUTCH-2270"
        ]
    },
    "NUTCH-2272": {
        "Key": "NUTCH-2272",
        "Summary": "Index checker server to optionally keep client connection open",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.13",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Jun/16 10:42",
        "Updated": "23/Jun/16 15:55",
        "Resolved": "03/Jun/16 13:03",
        "Description": "As the title says: for easier testing without having to start up the indexchecker JVM every time.\n\nbin/nutch org.apache.nutch.indexer.IndexingFiltersChecker -normalize -followRedirects -keepClientCnxOpen -listen 5000\n\n\nJust telnet to it an send URL's with line feed to get output fast.",
        "Issue Links": []
    },
    "NUTCH-2273": {
        "Key": "NUTCH-2273",
        "Summary": "Selenium and InteractiveSelenium Do Not Support HTTPS",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.15",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Brian Zhao",
        "Created": "02/Jun/16 16:09",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "23/May/18 16:22",
        "Description": "Both Selenium and InteractiveSelenium plugins do not have the https protocol specified in their plugin.xml, and will not fetch https links.\nTo fix for the Selenium plugin you should add: \n      <implementation id=\"org.apache.nutch.protocol.selenium.Http\"\n                      class=\"org.apache.nutch.protocol.selenium.Http\">\n         <parameter name=\"protocolName\" value=\"https\"/>\n      </implementation>\nto Selenium's plugin.xml (as a child element of the \"extension\" element)\nAn implementation already exists in protocol-http HttpResponse.java, and I've merged it into selenium's HttpResponse.java here: http://pastebin.com/ZAPfwee4\nThis should probably be similarly done for the InteractiveSelenium plugin.",
        "Issue Links": [
            "/jira/browse/NUTCH-2577"
        ]
    },
    "NUTCH-2274": {
        "Key": "NUTCH-2274",
        "Summary": "InteractiveSelenium Plugin's DefaultHandler Returns Null",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Brian Zhao",
        "Created": "02/Jun/16 16:13",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "23/Apr/20 11:10",
        "Description": "The Interactive Selenium plugin's DefaultHandler.java always returns null for its \"processDriver(WebDriver driver)\" method. \nIt should (probably?) instead return the body of the html:\n public String processDriver(WebDriver driver) \n{\n        return driver.findElement(By.tagName(\"body\")).getAttribute(\"innerHTML\");\n    }",
        "Issue Links": []
    },
    "NUTCH-2275": {
        "Key": "NUTCH-2275",
        "Summary": "MD5Signature by default doesn't take in account parse",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.20",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Francesco Capponi",
        "Created": "08/Jun/16 11:03",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "I'm testing Apache Nutch with the feed's plugin. I've noticed that for each page it generates the same digest/signature, therefore the dedup cleans everything up from the database.\nI'm wondering why the class MD5Signature is the default one instead of TextMD5Signature.\nAnyhow now I've modified a little bit the MD5Signature to let it work with the feed plugin",
        "Issue Links": []
    },
    "NUTCH-2276": {
        "Key": "NUTCH-2276",
        "Summary": "Tika Boilerpipe Parser in combo with RSS items doesn't work",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.11,                                            1.12",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Francesco Capponi",
        "Created": "08/Jun/16 22:06",
        "Updated": "08/Jun/16 22:06",
        "Resolved": null,
        "Description": "Sometimes it happens that the text (description) for an RSS item is too short or has characteristics that Tika with Boilerpipe decide to cut the entire text, resulting in an empty string.\nin fact when the feed plugin selects a parser uses the function:\n      Parser parser = parserFactory.getParsers(contentType, link)[0];\nthe content being a HTML returns the Tika Boilerpipe article extractor.\nSince the description text of an RSS as far as I know is always html, instead of asking for the contentType, we could set another mimetype for this specific case\n    String contentType = contentMeta.get(Response.CONTENT_TYPE);\n ->String contentType = \"text/html-short\";",
        "Issue Links": []
    },
    "NUTCH-2277": {
        "Key": "NUTCH-2277",
        "Summary": "Adding goldstandard.txt default file in conf",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Francesco Capponi",
        "Created": "09/Jun/16 01:08",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Since there is already the stopwords.txt example in the conf directory, instead the goldstandard is missing, it would be nice for newbies finding immediately that file in the directory without having to guess the position and how to create it.",
        "Issue Links": []
    },
    "NUTCH-2278": {
        "Key": "NUTCH-2278",
        "Summary": "Handle alpha-2 language codes consistently",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.20",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Fengtan",
        "Created": "11/Jun/16 02:56",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The language-identifier plugin provides two extraction policies: detect and identify.\nHowever the two policies handle alpha-2 codes differently:\n\n'identify' strips out the alpha-2 code e.g. if the identified language is 'en-US' then it will inject 'en' in the meta tags\n'detect' does not strip out the alpha-2 code e.g. if the detected language is 'en-US' then it will inject 'en-US' in the meta tags\n\nAny chance we can make this consistent and always strip out the alpha-2 code ?",
        "Issue Links": [
            "/jira/browse/NUTCH-1397",
            "/jira/browse/NUTCH-2449"
        ]
    },
    "NUTCH-2279": {
        "Key": "NUTCH-2279",
        "Summary": "LinkRank fails when using Hadoop MR output compression",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.16",
        "Component/s": "webgraph",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Joseph Naegele",
        "Created": "14/Jun/16 19:33",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "01/Oct/19 14:24",
        "Description": "When using MapReduce job output compression, i.e. mapreduce.output.fileoutputformat.compress=true, LinkRank can't read the results of its Counter MR job due to the additional, generated file extension.\nFor example, using the default compression codec (which appears to be DEFLATE), the counter file is written to crawl/webgraph/num_nodes/part-00000.deflate. Then, the LinkRank job attempts to manually read this file to obtain the number of links using the following code:\n\nFSDataInputStream readLinks = fs.open(new Path(numLinksPath, \"part-00000\"));\n\n\nwhich fails because the file part-00000 doesn't exist:\n\nLinkAnalysis: java.io.FileNotFoundException: File crawl/webgraph/_num_nodes_/part-00000 does not exist\n        at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:819)\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:596)\n        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)\n        at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.<init>(ChecksumFileSystem.java:140)\n        at org.apache.hadoop.fs.ChecksumFileSystem.open(ChecksumFileSystem.java:341)\n        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:767)\n        at org.apache.nutch.scoring.webgraph.LinkRank.runCounter(LinkRank.java:124)\n        at org.apache.nutch.scoring.webgraph.LinkRank.analyze(LinkRank.java:633)\n        at org.apache.nutch.scoring.webgraph.LinkRank.run(LinkRank.java:713)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.scoring.webgraph.LinkRank.main(LinkRank.java:680)\n\n\nTo reproduce, add -D mapreduce.output.fileoutputformat.compress=true to the properties for bin/nutch linkrank ...",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/478"
        ]
    },
    "NUTCH-2280": {
        "Key": "NUTCH-2280",
        "Summary": "HTTP Post form authentication CookiePolicy configuration",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Steve Yao",
        "Created": "15/Jun/16 11:40",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "24/Jul/16 06:30",
        "Description": "The protocol-httpclient plugin supports HTTP form authentication with form values post back to the assigned login URL and store the session cookie for following content retrieving.\nThe httpclient default CookiePolicy setting is in use. This default setting will reject cookie has domain set starting as \".\", for example domain=\".domain.com\". This kind of domain value could be accepted by most web browsers. \nI suggest to add an configurable option in conf/httpclient-auth.xml:\n\n<credentials authMethod=\"formMethod\" ...>\n...\n  <loginCookie>\n    <policy>DEFAULT | BROWSER_COMPATIBILITY | NETSCAPE RFC_2109 | RFC_2965</policy>\n  </loginCookie>\n</credentials>\n\nThen, the httpclient could take this Cookie policy value.\nI am working on a patch for this feature. But before i implement the configuration format change, i would like to hear any other suggestions or comments.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/134"
        ]
    },
    "NUTCH-2281": {
        "Key": "NUTCH-2281",
        "Summary": "Support non-default FileSystem",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Jun/16 13:31",
        "Updated": "17/Jan/18 10:13",
        "Resolved": "06/Apr/17 10:01",
        "Description": "If a path (input or output) does not belong to the configured default FileSystem various Nutch tools may raise an exception like\n\n  Exception in ... java.lang.IllegalArgumentException: Wrong FS: s3a://..., expected: hdfs://...\n\n\nThis is fixed by getting a reference to the FileSystem from the Path object\n\n  FileSystem fs = path.getFileSystem(getConf());\n\n\ninstead of\n\n  FileSystem fs = FileSystem.get(getConf());\n\n\nA given path (e.g., s3a://...) may not belong to the default file system (hdfs:// or file:// in local mode) and simple checks such as fs.exists(path) then will fail. Cf. FileSystem.checkPath(path), and FileSystem.get(conf) vs. FileSystem.get(URI,conf) which is called by Path.getFileSystem(conf).  \nNote that the FileSystem for input and output may be different, e.g., read from HDFS and write to S3.",
        "Issue Links": [
            "/jira/browse/NUTCH-2494",
            "https://github.com/apache/nutch/pull/119"
        ]
    },
    "NUTCH-2282": {
        "Key": "NUTCH-2282",
        "Summary": "Incorrect content-type returned in 4 API calls",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Giorgos Adam",
        "Created": "18/Jun/16 15:59",
        "Updated": "27/Jun/16 18:57",
        "Resolved": "27/Jun/16 18:17",
        "Description": "The REST API returns 'Content-type: application/json' instead of text/plain (at least) on the following calls:\n1. GET /admin/stop\n2. GET /config/:configId/:property\n3. POST /config/create\n4. POST /job/create",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/120"
        ]
    },
    "NUTCH-2283": {
        "Key": "NUTCH-2283",
        "Summary": "\"Bad substitution\" error when running cassandra docker scripts",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "docker",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Giorgos Adam",
        "Created": "19/Jun/16 09:45",
        "Updated": "20/Jun/16 02:40",
        "Resolved": "20/Jun/16 02:33",
        "Description": "\"Bad substitution\" error appears when running the following docker scripts:\n\ndocker/cassandra/bin/start.sh\ndocker/cassandra/bin/stop.sh\ndocker/cassandra/bin/restart.sh\n\ne.g. \n./bin/start.sh: 18: ./bin/start.sh: Bad substitution",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/121"
        ]
    },
    "NUTCH-2284": {
        "Key": "NUTCH-1756 Security layer for NutchServer",
        "Summary": "Basic Authentication Support for REST API",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "19/Jun/16 20:07",
        "Updated": "06/Jul/16 19:46",
        "Resolved": "06/Jul/16 19:34",
        "Description": "Add Basic Authentication for Nutch REST API.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/124"
        ]
    },
    "NUTCH-2285": {
        "Key": "NUTCH-1756 Security layer for NutchServer",
        "Summary": "Digest Authentication Support for REST API",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "19/Jun/16 20:19",
        "Updated": "13/Jul/16 20:41",
        "Resolved": "13/Jul/16 20:41",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/126",
            "https://github.com/apache/nutch/pull/132",
            "https://github.com/apache/nutch/pull/135"
        ]
    },
    "NUTCH-2286": {
        "Key": "NUTCH-2286",
        "Summary": "CrawlDbReader -stats to show fetch time and interval",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "23/Jun/16 14:52",
        "Updated": "02/Jul/16 10:18",
        "Resolved": "02/Jul/16 10:18",
        "Description": "An overview about fetch times and fetch intervals could be useful to configure a crawl.  CrawlDbReader could easily calculate min, max and average and show it as part of the statistics job (command-line option -stats):\n\n% bin/nutch readdb .../crawldb/ -stats\n...\nTOTAL urls:     544910\nshortest fetch interval:       7 days, 00:00:00\navg fetch interval:            7 days, 17:43:58\nlongest fetch interval:       10 days, 12:00:00\nearliest fetch time:   Wed May 25 11:42:00 CEST 2016\navg of fetch times:    Sun Jun 05 18:11:00 CEST 2016\nlatest fetch time:     Wed Jun 22 10:25:00 CEST 2016\n...",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/125"
        ]
    },
    "NUTCH-2287": {
        "Key": "NUTCH-2287",
        "Summary": "Indexer-elastic plugin should use Elasticsearch BulkProcessor and BackoffPolicy",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Joseph Naegele",
        "Created": "24/Jun/16 21:15",
        "Updated": "16/Jul/16 21:45",
        "Resolved": "16/Jul/16 21:40",
        "Description": "Elasticsearch's API (since at least v2.0) includes the BulkProcessor, which automatically handles flushing bulk requests given a max doc count and/or max bulk size. It also now (I believe since 2.2.0) offers a BackoffPolicy option, allowing the BulkProcessor/Client to retry bulk requests when the Elasticsearch cluster is saturated. Using the BulkProcessor was originally suggested here.\nRefactoring the indexer-elastic plugin to use the BulkProcessor will greatly simplify the existing plugin at the cost of slightly less debug logging. Additionally, it will allow the plugin to handle cluster saturation gracefully (rather than raising a RuntimeException and killing the reduce task), by using a configurable \"exponential back-off policy\".\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-api/2.3/java-docs-bulk-processor.html",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/131"
        ]
    },
    "NUTCH-2288": {
        "Key": "NUTCH-2288",
        "Summary": "Upgrade Restlet to 2.3.7",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "26/Jun/16 17:38",
        "Updated": "13/Jul/16 00:47",
        "Resolved": "13/Jul/16 00:04",
        "Description": "Currently we use restlet 2.2.3. We should upgrade restlet to 2.3.7. Changes can be seen at here: https://restlet.com/technical-resources/restlet-framework/misc/2.3/changes",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/127",
            "https://github.com/apache/nutch/pull/130"
        ]
    },
    "NUTCH-2289": {
        "Key": "NUTCH-1756 Security layer for NutchServer",
        "Summary": "SSL Support for REST API",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "26/Jun/16 20:11",
        "Updated": "14/Jul/16 23:45",
        "Resolved": "14/Jul/16 22:56",
        "Description": "Add SSL Authentication for Nutch REST API.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/128",
            "https://github.com/apache/nutch/pull/133",
            "https://github.com/apache/nutch/pull/136"
        ]
    },
    "NUTCH-2290": {
        "Key": "NUTCH-2290",
        "Summary": "Update licenses of bundled libraries",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.12",
        "Fix Version/s": "1.19",
        "Component/s": "deployment",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "29/Jun/16 11:05",
        "Updated": "19/Aug/22 13:42",
        "Resolved": "19/Aug/22 12:57",
        "Description": "The files LICENSE.txt and NOTICE.txt were last edited 5 years ago and should be updated to include all licenses of dependencies (and their dependencies) in accordance to Assembling LICENSE and NOTICE HOWTO:\n\ncheck for missing or obsolete licenses due to added or removed dependencies\nupdate year in NOTICE.txt \u2013 should be a range according to the licensing HOWTO\nbundled libraries are referenced with path and version number, e.g lib/icu4j-4_0_1.jar. This would require to update the LICENSE.txt with every dependency upgrade. A more generic reference (\"ICU4J\") would be easier to maintain but the HOWTO requires to \"specify the version of the dependency as licenses are sometimes changed\".\ntry to reduce the size of LICENSE.txt (currently 5800 lines). Mainly, according to the HOWTO there is no need to repeat the Apache license again and again.",
        "Issue Links": [
            "/jira/browse/ANY23-372",
            "https://github.com/apache/nutch/pull/743"
        ]
    },
    "NUTCH-2291": {
        "Key": "NUTCH-2291",
        "Summary": "Fix mrunit dependencies",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.13",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "30/Jun/16 10:05",
        "Updated": "01/Jul/16 14:00",
        "Resolved": "01/Jul/16 13:16",
        "Description": "The Jenkins builds fail with a NoClassDefFoundError, see build #3376 log. The missing class org/mockito/stubbing/Answer is part of build/test/lib/mockito-core-1.9.5.jar which was a dependency of mrunit (screenshot mrunit-deps-cached.png). After removing mrunit from my local ivy cache (rm -rf ~/.ivy2/cache/org.apache.mrunit/ mrunit lost mockito as dependency (screenshot mrunit-deps-new.png) and the build failure is reproducible.\nI don't understand what triggered the loss of the transitive dependency: the upgrade to Hadoop 2.7.2 (NUTCH-2236) or the addition of maven:classifier=\"hadoop2\" in commit 7956daee.",
        "Issue Links": []
    },
    "NUTCH-2292": {
        "Key": "NUTCH-2292",
        "Summary": "Mavenize the build for nutch-core and nutch-plugins",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Thamme Gowda",
        "Reporter": "Thamme Gowda",
        "Created": "03/Jul/16 20:23",
        "Updated": "09/Aug/22 06:50",
        "Resolved": "13/Jan/22 18:40",
        "Description": "Convert the build system of  nutch-core as well as plugins to Apache Maven.\nPlan :\nCreate multi-module maven project with the following structure\n\nnutch-parent\n  |-- pom.xml (POM)\n  |-- nutch-core\n  |       |-- pom.xml (JAR)\n  |       |--src    : sources\n  |-- nutch-plugins\n          |-- pom.xml (POM)\n          |-- plugin1\n          |    |-- pom.xml (JAR)\n          | .....\n          |-- pluginN\n               |-- pom.xml (JAR)\n\n\nNOTE: watch out for cyclic dependencies bwteen nutch-core and plugins, introduce another POM to break the cycle if required.",
        "Issue Links": [
            "/jira/browse/NUTCH-1371",
            "/jira/browse/NUTCH-2244",
            "/jira/browse/NUTCH-2293",
            "/jira/browse/NUTCH-2901",
            "/jira/browse/NUTCH-2638",
            "/jira/browse/NUTCH-2934"
        ]
    },
    "NUTCH-2293": {
        "Key": "NUTCH-2292 Mavenize the build for nutch-core and nutch-plugins",
        "Summary": "Make the unit tests which requires \"plugin.folders\" as integration tests",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Thamme Gowda",
        "Created": "05/Jul/16 23:32",
        "Updated": "09/Aug/22 06:50",
        "Resolved": "13/Jan/22 18:42",
        "Description": "The system property \"plugin.folders\" is heavily used in unit tests of nutch-core. \nSome of the utilities used by the tests in plugins also requires this property to be set.\nThese tests ought to be run after the package goal is executed, so configure the build to defer these tests for post-package (one solution is to make them as integration tests rather than unit tests)",
        "Issue Links": [
            "/jira/browse/NUTCH-2934",
            "/jira/browse/NUTCH-2292"
        ]
    },
    "NUTCH-2294": {
        "Key": "NUTCH-1756 Security layer for NutchServer",
        "Summary": "Authorization Support for REST API",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "18/Jul/16 21:19",
        "Updated": "20/Aug/16 02:45",
        "Resolved": "20/Aug/16 02:13",
        "Description": "Add authorization for Nutch REST API.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/142"
        ]
    },
    "NUTCH-2295": {
        "Key": "NUTCH-2295",
        "Summary": "Nutch master docker container broken",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "docker",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Jul/16 17:27",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "18/Dec/17 16:34",
        "Description": "Right now the Docker container at https://github.com/apache/nutch/blob/25e879afc9c48981e3daccb055b5389799fae464/docker/Dockerfile is broken. \nVarious links need updated. The base image could be updated to Ubuntu 16. Nutch is no longer held within SVN, etc.\nNeeds a bit of time put into resolving these issues.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/266"
        ]
    },
    "NUTCH-2296": {
        "Key": "NUTCH-2296",
        "Summary": "Elasticsearch Indexing Over Rest",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "indexer",
        "Assignee": "Brian Zhao",
        "Reporter": "Brian Zhao",
        "Created": "04/Aug/16 06:47",
        "Updated": "06/Apr/17 02:30",
        "Resolved": "06/Apr/17 02:27",
        "Description": "Open Elasticsearch to the option of REST-based indexing, via another indexing plugin implemeted using Jest, potentially allowing the use of https.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/139"
        ]
    },
    "NUTCH-2297": {
        "Key": "NUTCH-2297",
        "Summary": "CrawlDbReader -stats wrong values for earliest fetch time and shortest interval",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "08/Aug/16 10:06",
        "Updated": "14/Dec/17 15:14",
        "Resolved": "14/Dec/17 15:14",
        "Description": "NUTCH-2286 added min, max and average for fetch interval and fetch time.\nWhen running in distributed mode (not reproducible in local mode), the values for the minimum (earliest fetch time and shortest fetch interval) may be wrong with implausible values:\n\nTOTAL urls: 7180518032\n shortest fetch interval:    175 days, 00:00:00             <<<<<< ????\n avg fetch interval: 10 days, 08:01:36\n longest fetch interval:     15 days, 18:00:00\n earliest fetch time:        Thu Dec 20 05:30:00 UTC 3106   <<<<<< ????\n avg of fetch times: Fri Feb 19 00:07:00 UTC 2016\n latest fetch time:  Mon Jul 18 05:22:00 UTC 2016\n retry 0:    6907984913\n retry 1:    148125397\n retry 2:    82761892\n retry 3:    41645830\n min score:  0.0\n avg score:  0.014360981\n max score:  9.25\n ...",
        "Issue Links": [
            "/jira/browse/NUTCH-2474",
            "/jira/browse/NUTCH-2474"
        ]
    },
    "NUTCH-2298": {
        "Key": "NUTCH-2298",
        "Summary": "TestCrawlDbStates.testCrawlDbStatTransitionInject broken",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "test",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "11/Aug/16 13:45",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "06/Nov/17 22:12",
        "Description": "No idea what's happening on master:\n\nTestcase: testCrawlDbStatTransitionInject took 0.105 sec\n        Caused an ERROR\norg/mockito/stubbing/Answer\njava.lang.NoClassDefFoundError: org/mockito/stubbing/Answer\n        at org.apache.hadoop.mrunit.mapreduce.ReduceDriver.getContextWrapper(ReduceDriver.java:281)\n        at org.apache.hadoop.mrunit.mapreduce.ReduceDriver.run(ReduceDriver.java:257)\n        at org.apache.nutch.crawl.CrawlDbUpdateTestDriver.update(CrawlDbUpdateTestDriver.java:98)\n        at org.apache.nutch.crawl.TestCrawlDbStates.testCrawlDbStatTransitionInject(TestCrawlDbStates.java:233)\nCaused by: java.lang.ClassNotFoundException: org.mockito.stubbing.Answer\n        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)\n        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)",
        "Issue Links": []
    },
    "NUTCH-2299": {
        "Key": "NUTCH-2299",
        "Summary": "Remove obsolete properties protocol.plugin.check.*",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "fetcher,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Aug/16 09:19",
        "Updated": "16/Aug/16 18:47",
        "Resolved": "16/Aug/16 18:47",
        "Description": "There are two properties protocol.plugin.check.blocking and protocol.plugin.check.robots not used anymore since NUTCH-876. They can be removed from Fetcher and Protocol.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/140"
        ]
    },
    "NUTCH-2300": {
        "Key": "NUTCH-2300",
        "Summary": "Fetcher to optionally save robots.txt",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.13",
        "Component/s": "fetcher,                                            protocol,                                            segment",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/Aug/16 13:42",
        "Updated": "22/Aug/16 21:52",
        "Resolved": "22/Aug/16 21:51",
        "Description": "For debugging or archival purposes it may be useful to let Fetcher store the robots.txt response (content and HTTP status). Of course, this should be optional and not by default.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/141"
        ]
    },
    "NUTCH-2301": {
        "Key": "NUTCH-1756 Security layer for NutchServer",
        "Summary": "Create Tests for Security Layer of NutchServer",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "19/Aug/16 21:51",
        "Updated": "23/Aug/16 21:45",
        "Resolved": "23/Aug/16 21:37",
        "Description": "Create tests for security layer of NutchServer.",
        "Issue Links": [
            "/jira/browse/NUTCH-2302",
            "/jira/browse/NUTCH-2303",
            "https://github.com/apache/nutch/pull/146"
        ]
    },
    "NUTCH-2302": {
        "Key": "NUTCH-2302",
        "Summary": "RAMConfManager Could Be Constructed With Custom Configuration",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "20/Aug/16 12:58",
        "Updated": "23/Aug/16 09:59",
        "Resolved": "23/Aug/16 03:16",
        "Description": "RAMConfManager is intented to hold different configurations which can be accessible via a configuration id. However, it forces you to use a default configuration with a default id when you construct it. When RAMConfManager is used by any other classes they cannot set a custom configuration and it leads problem. i.e. test resources cannot be used when you test NutchServer due to it uses default configuration which is forced by RAMConfManager.",
        "Issue Links": [
            "/jira/browse/NUTCH-2301",
            "/jira/browse/NUTCH-2303",
            "/jira/browse/NUTCH-2306",
            "https://github.com/apache/nutch/pull/143"
        ]
    },
    "NUTCH-2303": {
        "Key": "NUTCH-2303",
        "Summary": "NutchServer Could Be Able To Select a Configuration to Use",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "20/Aug/16 14:37",
        "Updated": "23/Aug/16 16:49",
        "Resolved": "23/Aug/16 15:49",
        "Description": "RAMConfManager is intented to hold different configurations. However, currently NutchServer uses default config and it could be let to set an active configuration id when startup a NutchServer.",
        "Issue Links": [
            "/jira/browse/NUTCH-2301",
            "/jira/browse/NUTCH-2306",
            "/jira/browse/NUTCH-2302",
            "https://github.com/apache/nutch/pull/144"
        ]
    },
    "NUTCH-2304": {
        "Key": "NUTCH-2304",
        "Summary": "Fix Elasticsearch Rest Indexing Plugin's Dependencies",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Resolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Brian Zhao",
        "Created": "21/Aug/16 01:12",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "21/Apr/20 09:34",
        "Description": "Currently the Elaticsearch Rest Indexing Plugin has included 3 libraries into the root ivy.xml file. These need to be refactored into the plugin-specific ivy file.",
        "Issue Links": [
            "/jira/browse/NUTCH-2739",
            "/jira/browse/NUTCH-2755"
        ]
    },
    "NUTCH-2305": {
        "Key": "NUTCH-2305",
        "Summary": "generate.min.score doesn't work in 2.x",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Kiyonari Harigae",
        "Created": "22/Aug/16 02:56",
        "Updated": "22/Aug/16 21:46",
        "Resolved": "22/Aug/16 20:49",
        "Description": "The definition of \"generate.min.score\" is exist in GeneratorJob but,\nIt does not work even if described in nutch-site.conf.\n\"generate.min.score\" is necessary also 2.x",
        "Issue Links": []
    },
    "NUTCH-2306": {
        "Key": "NUTCH-2306",
        "Summary": "Id of Active Configuration Could Be Stored at NutchStatus and Exposed via REST API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "22/Aug/16 07:41",
        "Updated": "23/Aug/16 16:49",
        "Resolved": "23/Aug/16 15:50",
        "Description": "NutchStatus holds information about configuration it uses. However, it should also store the id of that configuration. Once NUTCH-2302 and NUTCH-2303 are merged, we will be able to store acitive configuration id and expose this information via REST API.",
        "Issue Links": [
            "/jira/browse/NUTCH-2302",
            "/jira/browse/NUTCH-2303",
            "https://github.com/apache/nutch/pull/145"
        ]
    },
    "NUTCH-2307": {
        "Key": "NUTCH-2307",
        "Summary": "Implement Missing NutchServer REST API Tests",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Furkan Kamaci",
        "Created": "23/Aug/16 21:48",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "TestAPI.java was all commented. Reason was indicated as:\n\nCURRENTLY DISABLED. TESTS ARE FLAPPING FOR NO APPARENT REASON.\nSHALL BE FIXED OR REPLACES BY NEW API IMPLEMENTATION\nSo, we should implement that missing tests based on new AbstractNutchAPITestBase.",
        "Issue Links": []
    },
    "NUTCH-2308": {
        "Key": "NUTCH-2308",
        "Summary": "Implement SSL Connection Test at TestNutchAPI",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "23/Aug/16 23:31",
        "Updated": "05/Jan/20 06:29",
        "Resolved": "30/Aug/16 16:51",
        "Description": "Currently, testing of SSL is ignored at TestNutchAPI. We should complete the implementation.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/147"
        ]
    },
    "NUTCH-2309": {
        "Key": "NUTCH-2309",
        "Summary": "Scoring-Similarity Plugin raises NullPointerException when error occurs in fetching URL",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Trivial",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            scoring",
        "Assignee": null,
        "Reporter": "Joey Hong",
        "Created": "24/Aug/16 21:09",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "When the Scoring-Similarity plugin is enabled, a NullPointerException is thrown, cancelling the crawl, when computing the Cosine Similarity for URLs where any kind of error occurred in fetching it. \nThe error occurs in line 77 in CosineSimilarity.java:\nfloat score = Float.parseFloat(parseData.getContentMeta().get(Nutch.SCORE_KEY));\nThis is probably because Nutch.SCORE_KEY is null for such URLs. It can be easily fixed by setting a default value for score.",
        "Issue Links": []
    },
    "NUTCH-2310": {
        "Key": "NUTCH-2310",
        "Summary": "Protocol-Selenium does not support HTTPS protocol",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.15",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Joey Hong",
        "Created": "24/Aug/16 21:18",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "23/May/18 16:22",
        "Description": "The protocol-selenium and protocol-interactiveselenium plugins raise errors whenever there is a URL with the HTTPS protocol.\n From the source code for those plugins, we can see that HTTP is the only scheme currently accepted, which makes Nutch unable to crawl HTTPS sites with JS using Selenium Webdrivers.",
        "Issue Links": [
            "/jira/browse/NUTCH-2577"
        ]
    },
    "NUTCH-2311": {
        "Key": "NUTCH-2311",
        "Summary": "Protocol-Selenium does not support HTTPS protocol",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Joey Hong",
        "Created": "24/Aug/16 21:18",
        "Updated": "24/Aug/16 21:31",
        "Resolved": "24/Aug/16 21:20",
        "Description": "The protocol-selenium and protocol-interactiveselenium plugins raise errors whenever there is a URL with the HTTPS protocol. From the source code for those plugins, we can see that HTTP is the only scheme currently accepted, which makes Nutch unable to crawl HTTPS sites with JS using Selenium Webdrivers.",
        "Issue Links": []
    },
    "NUTCH-2312": {
        "Key": "NUTCH-2312",
        "Summary": "Support PhantomJS as a WebDriver in protocol-selenium",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Incomplete",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Joey Hong",
        "Created": "24/Aug/16 22:12",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "12/Jun/18 19:39",
        "Description": "PhantomJS is a great parallelizable and headless browser to work with Nutch via protocol-selenium. It looks like the phantomjs JAR is already in the dependencies, and an empty initialization for the PhantomJSDriver exists in protocol-selenium source code.\nHowever, at its current state, protocol-selenium will not fetch any URLs with phantomjs, and configurations must be passed in via a DesiredCapabilities object. Also a parameter must be created to allow users to add a path to their phantomjs binary inside nutch-site.xml.",
        "Issue Links": []
    },
    "NUTCH-2313": {
        "Key": "NUTCH-2313",
        "Summary": "Error in Nutch 2.X WebApp Inject",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "kakou Denis",
        "Created": "01/Sep/16 21:30",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "when i define a crawl within the web, I have this ouput.\n16/09/01 20:58:51 INFO resource.PropertiesFactory: Loading properties files from jar:file:/tmp/hadoop-unjar2700707934366020491/lib/wicket-extensions-6.13.0.jar!/org/apache/wicket/extensions/Initializer.properties with loader org.apache.wicket.resource.IsoPropertiesFilePropertiesLoader@37dc175b\n16/09/01 20:59:10 WARN RequestCycleExtra: ********************************\n16/09/01 20:59:10 WARN RequestCycleExtra: Handling the following exception\norg.apache.wicket.core.request.mapper.StalePageException\n16/09/01 20:59:10 WARN RequestCycleExtra: ********************************\n16/09/01 20:59:10 WARN render.WebPageRenderer: The Buffered response should be handled by BufferedResponseRequestHandler\n16/09/01 20:59:22 ERROR impl.RemoteCommandExecutor: Remote command failed\njava.util.concurrent.TimeoutException\n        at java.util.concurrent.FutureTask.get(FutureTask.java:205)\n        at org.apache.nutch.webui.client.impl.RemoteCommandExecutor.executeRemoteJob(RemoteCommandExecutor.java:61)\n        at org.apache.nutch.webui.client.impl.CrawlingCycle.executeCrawlCycle(CrawlingCycle.java:58)\n        at org.apache.nutch.webui.service.impl.CrawlServiceImpl.startCrawl(CrawlServiceImpl.java:69)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\n        at org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\n        at org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:97)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n16/09/01 20:59:22 INFO impl.CrawlingCycle: Executed remote command data: INJECT status: FAILED",
        "Issue Links": []
    },
    "NUTCH-2314": {
        "Key": "NUTCH-2314",
        "Summary": "Use indexer-elastic2 Plugin for javadoc and eclipse Targets",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "plugin",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "02/Sep/16 14:52",
        "Updated": "03/Sep/16 06:22",
        "Resolved": "03/Sep/16 04:51",
        "Description": "indexer-elastic2 plugin is used at deploy and clean tasks of plugin/build.xml However, indexer-elastic plugin is used instead of indexer-elastic2 for javadoc and eclipse tasks at build.xml and gives error.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/150"
        ]
    },
    "NUTCH-2315": {
        "Key": "NUTCH-2315",
        "Summary": "UpdateDb jobs fails everytime (Nutch 2.3.1)",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Shubham Gupta",
        "Created": "15/Sep/16 11:43",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Hey,\nWhenever I run the update job, the following error occurs:\nINFO mapreduce.Job: Task Id : attempt_1473832356852_0107_m_000000_2, Status : FAILED\nError: java.net.MalformedURLException: no protocol: http%3A%2F%2Fwww.smh.com.au%2Fact-news%2Fcanberra-weather-warm-april-expected-after-record-breaking-march-temperatures-20160401-gnw2pg.html&title=Canberra+weather%3A+warm+April+expected+after+record+breaking+March+temperatures&source=The+Sydney+Morning+Herald&summary=Canberra+can+expect+warmer+than+average+temperatures+to+continue+for+April+after+enjoying+its+equal+second+warmest+March+on+record\n\tat java.net.URL.<init>(URL.java:586)\n\tat java.net.URL.<init>(URL.java:483)\n\tat java.net.URL.<init>(URL.java:432)\n\tat org.apache.nutch.util.TableUtil.reverseUrl(TableUtil.java:43)\n\tat org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:96)\n\tat org.apache.nutch.crawl.DbUpdateMapper.map(DbUpdateMapper.java:38)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n16/09/15 12:44:35 INFO mapreduce.Job:  map 100% reduce 100%\n16/09/15 12:44:36 INFO mapreduce.Job: Job job_1473832356852_0107 failed with state FAILED due to: Task failed task_1473832356852_0107_m_000000\nJob failed as tasks failed. failedMaps:1 failedReduces:0\n16/09/15 12:44:36 INFO mapreduce.Job: Counters: 8\n\tJob Counters \n\t\tFailed map tasks=4\n\t\tLaunched map tasks=4\n\t\tOther local map tasks=4\n\t\tTotal time spent by all maps in occupied slots (ms)=388304\n\t\tTotal time spent by all reduces in occupied slots (ms)=0\n\t\tTotal time spent by all map tasks (ms)=55472\n\t\tTotal vcore-seconds taken by all map tasks=55472\n\t\tTotal megabyte-seconds taken by all map tasks=198145984\nException in thread \"main\" java.lang.RuntimeException: job failed: name=[rss]update-table, jobid=job_1473832356852_0107\n\tat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:119)\n\tat org.apache.nutch.crawl.DbUpdaterJob.run(DbUpdaterJob.java:111)\n\tat org.apache.nutch.crawl.DbUpdaterJob.updateTable(DbUpdaterJob.java:140)\n\tat org.apache.nutch.crawl.DbUpdaterJob.run(DbUpdaterJob.java:174)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.crawl.DbUpdaterJob.main(DbUpdaterJob.java:178)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
        "Issue Links": []
    },
    "NUTCH-2316": {
        "Key": "NUTCH-2316",
        "Summary": "Library conflict with Parser-Tika Plugin and Lib Folder",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.14",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Christian Weber",
        "Created": "22/Sep/16 08:46",
        "Updated": "18/Aug/17 14:36",
        "Resolved": "18/Aug/17 14:36",
        "Description": "Hello Apache Nutch Team,\neverytime Nutch wants to parse a *.class file this Exception pops up:\n\njava.util.concurrent.ExecutionException: java.lang.IncompatibleClassChangeError: class org.apache.tika.parser.asm.XHTMLClassVisitor has interface org.objectweb.asm.ClassVisitor as super class\n\tat java.util.concurrent.FutureTask.report(FutureTask.java:122)\n\tat java.util.concurrent.FutureTask.get(FutureTask.java:206)\n\tat org.apache.nutch.parse.ParseUtil.runParser(ParseUtil.java:171)\n\tat org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:95)\n\tat org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:103)\n\tat org.apache.nutch.parse.ParseSegment.map(ParseSegment.java:45)\n\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)\n\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.IncompatibleClassChangeError: class org.apache.tika.parser.asm.XHTMLClassVisitor has interface org.objectweb.asm.ClassVisitor as super class\n\tat java.lang.ClassLoader.defineClass1(Native Method)\n\tat java.lang.ClassLoader.defineClass(ClassLoader.java:763)\n\tat java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)\n\tat java.net.URLClassLoader.defineClass(URLClassLoader.java:467)\n\tat java.net.URLClassLoader.access$100(URLClassLoader.java:73)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:368)\n\tat java.net.URLClassLoader$1.run(URLClassLoader.java:362)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat java.net.URLClassLoader.findClass(URLClassLoader.java:361)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n\tat org.apache.tika.parser.asm.ClassParser.parse(ClassParser.java:51)\n\tat org.apache.nutch.parse.tika.TikaParser.getParse(TikaParser.java:132)\n\tat de.qaware.qasearch.nutch.parse.SafetyTikaParser.getParse(SafetyTikaParser.java:73)\n\tat org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:35)\n\tat org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:24)\n\t... 4 more\nWhat I have found out is\n\nASM-Library is a library used inside TIKA Parser, and it has a breaking change with a certain version (3.x stated in TIKA-1240).\nparse-tika has the correct Version inside it's Plugin Folder (tika-parser 1.11 uses asm 5.0.4, see http://mvnrepository.com/artifact/org.apache.tika/tika-parsers/1.11).\ninside the nutch/lib folder is a asm jar with Version 3.3.1 which is incompatible\n\nI guess the Problem is that the JVM executing Nutch uses the asm library from inside the nutch/lib folder.\n(This Issue is related with NUTCH-2071. I hope it's okay that I opened a new Issue)",
        "Issue Links": [
            "/jira/browse/NUTCH-2378",
            "/jira/browse/NUTCH-2071"
        ]
    },
    "NUTCH-2317": {
        "Key": "NUTCH-2317",
        "Summary": "Plugin jars don't get added to classpath while running in local",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Sujen Shah",
        "Created": "24/Sep/16 17:10",
        "Updated": "05/Dec/17 12:11",
        "Resolved": "05/Dec/17 12:10",
        "Description": "Currently, plugin dependencies listed in the plugin's ivy.xml don't get added to the classpath while running nutch in local mode. \nLooking into the /bin/nutch code for adding jars to classpath these lines (https://github.com/apache/nutch/blob/master/src/bin/nutch#L161-L163) show an error in syntax. The PR tries to address this issue. \nLink to mail thread on dev list regarding this issue - http://www.mail-archive.com/dev@nutch.apache.org/msg22094.html",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/152",
            "https://github.com/apache/nutch/pull/152"
        ]
    },
    "NUTCH-2318": {
        "Key": "NUTCH-2318",
        "Summary": "Text extraction in HtmlParser adds too much whitespace.",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.3.1,                                            1.15",
        "Fix Version/s": "1.20",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Felix Zett",
        "Created": "29/Sep/16 16:07",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "In parse-html, org.apache.nutch.parse.html.HtmlParser will call DOMContentUtils.getText() to extract the text content. For every text node encountered in the document, the getTextHelper() function will first add a space character to the already extracted text and then the text content itself (stripped of excess whitespace). This means that parsing HTML such as\n<p>behavi<em>ou</em>r</p>\nwill lead to this extracted text:\nbehavi ou r\nI would have expected a parser not to add whitespace to content that visually (and actually) does not contain any in the first place. This applies to all similar semantic tags as well as <span>.\nMy naiive approach would be to remove the lines text = text.trim() and sb.append(' '), but I'm aware that this will lead to bad parsing of stuff like <p>foo</p><p>bar</p>.\nThis is not an issue in parse-tika, since tika removes all \"unimportant\" tags beforehand. However, I'd like to keep using parse-html because I need to keep the document reasonably intact for parse filters applied later.\nI know I could write a parse filter that will re-extract the text content, but this feels like a bug (or at least a shortcoming) in the ParseHtml.",
        "Issue Links": []
    },
    "NUTCH-2319": {
        "Key": "NUTCH-2319",
        "Summary": "Link with \"rel=alternate\" doesn't return in crawl",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Zuber",
        "Created": "01/Oct/16 12:10",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "06/Apr/17 09:23",
        "Description": "I am using nutch-1.4. I am getting the issue that the nutch doesn't return the URLs from the link rel=\"alternate\".\n For example, I am trying to crawl the URL  http://rssfeeds.azcentral.com/phoenix/asu which contains the  below link which I am not getting as result.\n<link rel=\"alternate\" type=\"application/atom+xml\" href=\"http://rssfeeds.azcentral.com/phoenix/asu&x=1\" title=\"Phoenix - ASU\">\nCould you please help",
        "Issue Links": []
    },
    "NUTCH-2320": {
        "Key": "NUTCH-2320",
        "Summary": "URLFilterChecker to run as TCP Telnet service",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "05/Oct/16 12:39",
        "Updated": "17/Dec/17 13:39",
        "Resolved": "17/Dec/17 13:39",
        "Description": "Allow testing URL filters for webapplications just like indexing filters checker.",
        "Issue Links": [
            "/jira/browse/NUTCH-2338",
            "/jira/browse/NUTCH-2477"
        ]
    },
    "NUTCH-2321": {
        "Key": "NUTCH-2321",
        "Summary": "Indexing filter checker leaks threads",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Oct/16 09:34",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "15/Jan/18 17:38",
        "Description": "Same issue as NUTCH-2320.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/272"
        ]
    },
    "NUTCH-2322": {
        "Key": "NUTCH-2322",
        "Summary": "URL not available for Jexl operations",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Oct/16 11:55",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "17/Dec/17 14:36",
        "Description": "In CrawlDatum.evaluate(), the records's URL is just missing.",
        "Issue Links": []
    },
    "NUTCH-2323": {
        "Key": "NUTCH-2323",
        "Summary": "ElasticSearch Indexer does not work on Nutch 2.3.1",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Joe Crane",
        "Created": "10/Oct/16 23:25",
        "Updated": "22/Nov/19 13:22",
        "Resolved": "22/Nov/19 13:22",
        "Description": "When attempting to Index to ElasticSearch, Nutch attempts to use SolrIndexerJob instead of ElasticSearch's indexer, despite the correct settings in ./runtime/local/conf/nutch-site.xml as well as ./ivy/ivy.xml.",
        "Issue Links": []
    },
    "NUTCH-2324": {
        "Key": "NUTCH-2324",
        "Summary": "Issue in setting default linkdb path",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.15",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Sachin",
        "Created": "12/Oct/16 05:29",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "10/Jan/18 01:50",
        "Description": "There is an extra if condition that prevents setting default linkdb path if we doesn't provide one in REST call.\nCheck this : https://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/indexer/IndexingJob.java#L272\nhttps://github.com/apache/nutch/pull/153\nPS : Don't know whether it is intentional. You may check!",
        "Issue Links": []
    },
    "NUTCH-2325": {
        "Key": "NUTCH-2325",
        "Summary": "Inject REST call to set overwrite and update parameters",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Sujan Kumar Suppala",
        "Created": "14/Oct/16 11:46",
        "Updated": "19/Oct/16 07:52",
        "Resolved": null,
        "Description": "INJECT REST uses the old method invocation which sets overwrite and update to false, which is wrong.\nhttps://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/crawl/Injector.java#L514\nThere should be an option to set the overwrite and update in the REST request.\neg:\n\n\nPOST /job/create\n {\n     \"type\":\"INJECT\",\n     \"confId\":\"default\", \n     \"crawlId\":\"TestCrawl\",\n     \"args\": {\n                 \"url_dir\":\"c:\\\\cygwin64\\\\tmp\\\\1475752235404-0\",\n                \"overwrite\":\"true\",\n                \"update\":\"true\"\n                }\n }",
        "Issue Links": []
    },
    "NUTCH-2326": {
        "Key": "NUTCH-2326",
        "Summary": "Implement InvertLinks job in webui package",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13",
        "Fix Version/s": "None",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "17/Oct/16 15:45",
        "Updated": "17/Oct/16 15:45",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2327": {
        "Key": "NUTCH-2327",
        "Summary": "Seeds injected in REST workflow must be ingested into HDFS",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "injector,                                            REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/Oct/16 06:38",
        "Updated": "25/Oct/16 16:56",
        "Resolved": "25/Oct/16 16:04",
        "Description": "Right now when one uses the REST POST /seed/create API, a directory is created within /var/some/path/here which is create if you are working locally with the Nutch server e.g. on one machine. It is however not suitable for using the REST API in distributed deployments where seeds needs to be present within HDFS. More documentation on this topic is available at \nhttps://wiki.apache.org/nutch/Nutch_1.X_RESTAPI#Seed_List_creation\nThere are also various mailing list threads regarding use of the REST and this injector url issue described above needs to be addressed.\nsujenshah CC for context.\nhttp://www.mail-archive.com/user%40nutch.apache.org/msg14922.html\nhttp://www.mail-archive.com/user%40nutch.apache.org/msg14921.html",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/155"
        ]
    },
    "NUTCH-2328": {
        "Key": "NUTCH-2328",
        "Summary": "GeneratorJob does not generate anything on second run",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.2,                                            2.3,                                            2.2.1,                                            2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Arthur B",
        "Created": "18/Oct/16 13:16",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Given a topN parameter (ie 10) the GeneratorJob will fail to generate anything new on the subsequent runs within the same process space.\nTo reproduce the issue submit the GeneratorJob twice one after another to the M/R framework. Second time will say it generated 0 URLs.\nThis issue is due to the usage of the static count field (org.apache.nutch.crawl.GeneratorReducer#count) to determine if the topN value has been reached.",
        "Issue Links": [
            "/jira/browse/NUTCH-2330"
        ]
    },
    "NUTCH-2329": {
        "Key": "NUTCH-2329",
        "Summary": "Update Slf4j logging for Java 8 and upgrade miredot plugin version",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "build,                                            REST_api",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/Oct/16 19:38",
        "Updated": "24/Oct/16 17:05",
        "Resolved": "24/Oct/16 17:05",
        "Description": "This issue was driven by the recent activity regarding the REST API in 1.X.\nWhen I attempted to create the Miredot documentation I was running into some errors, including one error regarding Slf4j and use of Object[]{} notation for tri-parameter logging entries. The kind folks over at Miredot provided us with a brand new license key which has been added to the mvn.template.\nPR coming up.",
        "Issue Links": []
    },
    "NUTCH-2330": {
        "Key": "NUTCH-2330",
        "Summary": "GeneratorJob does not generate anything on second run",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Arthur B",
        "Created": "18/Oct/16 12:48",
        "Updated": "09/Dec/16 10:52",
        "Resolved": "07/Dec/16 14:43",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-2328"
        ]
    },
    "NUTCH-2331": {
        "Key": "NUTCH-2331",
        "Summary": "REST API Fetch fails to retrieve HDFS path on distributed mode",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            REST_api",
        "Assignee": "Sujen Shah",
        "Reporter": "Sujen Shah",
        "Created": "20/Oct/16 22:40",
        "Updated": "22/Nov/19 13:22",
        "Resolved": null,
        "Description": "Currently in the REST API, if the user does not specify the absolute path of the segment to fetch and only the crawlId, then the fetcher would find the latest segment generated and use that. \nBut as of now, the above functionality will only work in local mode as per https://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/fetcher/Fetcher.java#L562-L573.\nNeed to update these lines to enable fetcher to read the directory and list files from an hdfs system.",
        "Issue Links": []
    },
    "NUTCH-2332": {
        "Key": "NUTCH-2332",
        "Summary": "Indexer-elastic2 plugin availability timeline",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Shaharia Azam",
        "Created": "01/Nov/16 11:19",
        "Updated": "15/Nov/19 11:57",
        "Resolved": "15/Nov/19 11:57",
        "Description": "Currently I am using Nutch v2.3. So is there any specific guide about how I can use indexer-elastic2 plugin binary in my v2.3?\nhttps://github.com/apache/nutch/tree/2.x/src/plugin/indexer-elastic2.\nThanks,\nShaharia",
        "Issue Links": []
    },
    "NUTCH-2333": {
        "Key": "NUTCH-2333",
        "Summary": "Indexer for RabbitMQ",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "02/Nov/16 20:54",
        "Updated": "16/Apr/17 09:41",
        "Resolved": "15/Apr/17 11:22",
        "Description": "A plugin to send the documents to a RabbitMQ server.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/168"
        ]
    },
    "NUTCH-2334": {
        "Key": "NUTCH-2334",
        "Summary": "Extension point for schedulers",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.20",
        "Component/s": "generator",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "03/Nov/16 20:51",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "With an extension point for schedulers, the users should be able to create new schedulers that meet to their own needs.",
        "Issue Links": []
    },
    "NUTCH-2335": {
        "Key": "NUTCH-2335",
        "Summary": "Injector not to filter and normalize existing URLs in CrawlDb",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "crawldb,                                            injector",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "28/Nov/16 10:05",
        "Updated": "17/Aug/17 10:56",
        "Resolved": "06/Apr/17 11:25",
        "Description": "With NUTCH-1712 the behavior of the Injector has changed in case new URLs are added to an existing CrawlDb:\n\nbefore only injected URLs were filtered and normalized\nnow filters and normalizers are applied to all URLs including those already in the CrawlDb\n\nThe default should be as before not to filter existing URLs. Filtering and normalizing may take long for large CrawlDbs and/or complex URL filters. If URL filter or normalizer rules are not changed there is no need to apply them anew every time new URLs are added. Of course, injected URLs should be filtered and normalized by default.",
        "Issue Links": [
            "/jira/browse/NUTCH-2409",
            "https://github.com/apache/nutch/pull/158"
        ]
    },
    "NUTCH-2336": {
        "Key": "NUTCH-2336",
        "Summary": "SegmentReader to implement Tool",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "segment",
        "Assignee": null,
        "Reporter": "Vincent Slot",
        "Created": "29/Nov/16 11:55",
        "Updated": "06/Apr/17 10:53",
        "Resolved": "01/Dec/16 11:01",
        "Description": "Let SegmentReader implement Tool for use on Hadoop",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/159"
        ]
    },
    "NUTCH-2337": {
        "Key": "NUTCH-2337",
        "Summary": "urlnormalizer-basic to strip empty port",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.12",
        "Fix Version/s": "2.4,                                            1.13",
        "Component/s": "plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Dec/16 10:59",
        "Updated": "13/Dec/16 13:56",
        "Resolved": "13/Dec/16 13:33",
        "Description": "Basic URL normalizer should strip an empty port from the URL, that's not the case at present:\n\necho \"http://example.com:/\" \\\n   | nutch plugin urlnormalizer-basic org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer\nhttp://example.com:/\n\n\nThe result should be http://example.com/",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/160"
        ]
    },
    "NUTCH-2338": {
        "Key": "NUTCH-2338",
        "Summary": "URLNormalizerChecker to run as TCP Telnet service",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Dec/16 17:17",
        "Updated": "17/Dec/17 13:38",
        "Resolved": "17/Dec/17 13:38",
        "Description": "Similar to NUTCH-2320, but then for normalizer checker.",
        "Issue Links": [
            "/jira/browse/NUTCH-2320"
        ]
    },
    "NUTCH-2339": {
        "Key": "NUTCH-2339",
        "Summary": "Nutch does not fetch documents with the -all argument",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "nutchNewbie",
        "Assignee": null,
        "Reporter": "Shubham Gupta",
        "Created": "16/Dec/16 04:41",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "I have deployed Nutch on the hadoop server. And whenever I check the count I get a humongous amount of docs with the status whereas very little amount of documents as compared to it with status 2.\nThe statistics are as follows:\n{ \"status\" : null, \"count\" : 16 }\n{ \"status\" : 1, \"count\" : 358437 }\n{ \"status\" : 2, \"count\" : 92021 }\n{ \"status\" : 3, \"count\" : 7354 }\n{ \"status\" : 4, \"count\" : 2807 }\n{ \"status\" : 5, \"count\" : 4042 }\n{ \"status\" : 34, \"count\" : 2767 }\n{ \"status\" : 38, \"count\" : 229 }\nFor successful fetching of status 1 documents, I have to run the command separately,then it starts fetching the status 1 documents. Is there any fix for this problem?",
        "Issue Links": []
    },
    "NUTCH-2340": {
        "Key": "NUTCH-2340",
        "Summary": "Can't install NUTCH from latest master branch.  resolve-default: [ivy:resolve] :: Apache Ivy 2.4.0 - 20141213170938 :: http://ant.apache.org/ivy/ :: [ivy:resolve] :: loading settings :: file = ...ivysettings.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Works for Me",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rajan Chandi",
        "Created": "17/Dec/16 08:00",
        "Updated": "18/Dec/16 02:40",
        "Resolved": "18/Dec/16 02:40",
        "Description": "Can't install Nutch on OS X. The Ant gets stuck here:\nresolve-default:\n[ivy:resolve] :: Apache Ivy 2.4.0 - 20141213170938 :: http://ant.apache.org/ivy/ ::\n[ivy:resolve] :: loading settings :: file = /Users/rajanchandi/WebstormProjects/crawl/nutch/ivy/ivysettings.xml",
        "Issue Links": []
    },
    "NUTCH-2341": {
        "Key": "NUTCH-2341",
        "Summary": "bin/crawl do not fetch batchId generated by bash script",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "bin",
        "Assignee": null,
        "Reporter": "YunXia",
        "Created": "27/Dec/16 09:02",
        "Updated": "15/Nov/19 11:57",
        "Resolved": "15/Nov/19 11:57",
        "Description": "I use bin/crawl to crawl url with no data returned, however use bin/nutch step by step fetch, it successed.\n\nbin/nutch generate -topN 10 -crawlId nutch -batchId  \"12345-123\"\nbin/nutch fetch \"1482737147-29630548\" -crawlId nutch -threads 20 # \"1482737147-29630548\" is generated by 'bin/nutch geneate', here if use batchId \"12345-123\" as bin/crawl do, then no data returned.",
        "Issue Links": []
    },
    "NUTCH-2342": {
        "Key": "NUTCH-2342",
        "Summary": "Inlinks are not being indexed as part of index-links plugin",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            linkdb",
        "Assignee": null,
        "Reporter": "Manish Bassi",
        "Created": "28/Dec/16 19:37",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "I have used index-links plugin along with other plugins to index both the inlinks and outlinks for a given page. But only the outlinks are getting indexed and not the inlinks.\nDue to this issue, even the anchor plugin is not working as expected.",
        "Issue Links": []
    },
    "NUTCH-2343": {
        "Key": "NUTCH-2343",
        "Summary": "Calling nutch extension points before custom plugin",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "nutchNewbie",
        "Assignee": null,
        "Reporter": "Shubham Gupta",
        "Created": "29/Dec/16 10:24",
        "Updated": "15/Nov/19 11:56",
        "Resolved": "15/Nov/19 11:56",
        "Description": "I want to use the language identifier parse plugin. The language identifier plugin works when the html parser works. I have also created a custom parser plugin which is called before the html parser plugin. I am unable to locate from where can this sequence be changed such that the language can be identified and inserted into Mongo along with other data.",
        "Issue Links": []
    },
    "NUTCH-2344": {
        "Key": "NUTCH-2344",
        "Summary": "Authentication Support for Web GUI",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "08/Jan/17 15:22",
        "Updated": "18/Jan/17 17:43",
        "Resolved": "18/Jan/17 17:13",
        "Description": "We should implement an authentication support for Web GUI.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/162",
            "https://github.com/apache/nutch/pull/163"
        ]
    },
    "NUTCH-2345": {
        "Key": "NUTCH-2345",
        "Summary": "FetchItemQueue logs are logged with wrong class name",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.11,                                            1.12",
        "Fix Version/s": "1.13",
        "Component/s": "fetcher",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Monika Gupta",
        "Created": "09/Jan/17 11:35",
        "Updated": "01/Feb/17 11:13",
        "Resolved": "01/Feb/17 11:13",
        "Description": "I ran bin/nutch fetch and notice that the log statements of class FetchItemQueue.java are logged in logs/hadoop.log with wrong file name as FetchItemQueues.java\nRefer the execution log:\n2017-01-06 15:31:25,562 INFO  fetcher.FetchItemQueues -   maxThreads    = 1\n2017-01-06 15:31:28,565 INFO  fetcher.FetchItemQueues -   inProgress    = 0\nIssue is in the logger for class FetchItemQueue.java. \nCurrently it is-\nprivate static final Logger LOG = LoggerFactory.getLogger(FetchItemQueues.class);\nCorrection: It should be-\nprivate static final Logger LOG = LoggerFactory.getLogger(FetchItemQueue.class);",
        "Issue Links": [
            "/jira/browse/NUTCH-2351",
            "/jira/browse/NUTCH-2352",
            "https://github.com/apache/nutch/pull/165"
        ]
    },
    "NUTCH-2346": {
        "Key": "NUTCH-2346",
        "Summary": "Check Types at Object Equality",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "generator,                                            metadata",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "09/Jan/17 13:59",
        "Updated": "28/Jan/17 00:43",
        "Resolved": "27/Jan/17 23:52",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/164",
            "https://github.com/apache/nutch/pull/176"
        ]
    },
    "NUTCH-2347": {
        "Key": "NUTCH-2347",
        "Summary": "Use Logger Instead of Printing Throwable",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "None",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "09/Jan/17 19:39",
        "Updated": "01/Feb/17 11:43",
        "Resolved": "01/Feb/17 11:27",
        "Description": "Loggers should be used instead of printing Throwable.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/166",
            "https://github.com/apache/nutch/pull/173"
        ]
    },
    "NUTCH-2348": {
        "Key": "NUTCH-2348",
        "Summary": "Close GZIPInputStream",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "tool",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "09/Jan/17 20:24",
        "Updated": "20/Jan/17 14:42",
        "Resolved": "20/Jan/17 14:42",
        "Description": "GZIPInputStream is not closed and it should be finally closed.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/167"
        ]
    },
    "NUTCH-2349": {
        "Key": "NUTCH-2349",
        "Summary": "urlnormalizer-basic NPE for ill-formed URL \"http:/\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4,                                            1.13",
        "Fix Version/s": "2.4,                                            1.13",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Jan/17 15:05",
        "Updated": "01/Feb/17 13:15",
        "Resolved": "01/Feb/17 11:09",
        "Description": "NUTCH-2337 introduced a potential (though rare) NullPointerException when an ill-formed URL (just the protocol followed by \":\", \":/\", \":////\" or even more slashes):\n\n% echo \"http://///\" \\\n  | runtime/local/bin/nutch org.apache.nutch.net.URLNormalizerChecker \\\n     -normalizer org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer \nChecking URLNormalizer org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer\nException in thread \"main\" java.lang.NullPointerException\n        at org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.normalize(BasicURLNormalizer.java:120)\n        at org.apache.nutch.net.URLNormalizerChecker.checkOne(URLNormalizerChecker.java:72)\n        at org.apache.nutch.net.URLNormalizerChecker.main(URLNormalizerChecker.java:110)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/169"
        ]
    },
    "NUTCH-2350": {
        "Key": "NUTCH-2350",
        "Summary": "Add Missing activeConfId Field to NutchStatus Object",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "web gui",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "15/Jan/17 20:04",
        "Updated": "18/Jan/17 17:43",
        "Resolved": "18/Jan/17 17:12",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/170"
        ]
    },
    "NUTCH-2351": {
        "Key": "NUTCH-2351",
        "Summary": "Log with Generic Class Name at Nutch 2.x",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "None",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "17/Jan/17 20:10",
        "Updated": "06/Apr/17 11:04",
        "Resolved": "19/Jan/17 22:12",
        "Description": "There are many mistakes when some reference code is copied and created a new class and a logger is used. We can log with a generic class name to avoid it as like:\n\nprivate static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n\n\n(cf. SOLR-8324)",
        "Issue Links": [
            "/jira/browse/NUTCH-2345",
            "https://github.com/apache/nutch/pull/171"
        ]
    },
    "NUTCH-2352": {
        "Key": "NUTCH-2352",
        "Summary": "Log with Generic Class Name at Nutch 1.x",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "None",
        "Assignee": "Furkan Kamaci",
        "Reporter": "Furkan Kamaci",
        "Created": "18/Jan/17 19:57",
        "Updated": "01/Feb/17 11:13",
        "Resolved": "19/Jan/17 21:48",
        "Description": "There are many mistakes when some reference code is copied and created a new class and a logger is used. We can log with a generic class name to avoid it as like:\n\nprivate static final Logger LOG = LoggerFactory.getLogger(MethodHandles.lookup().lookupClass());\n\n\n(cf. SOLR-8324)",
        "Issue Links": [
            "/jira/browse/NUTCH-2345",
            "https://github.com/apache/nutch/pull/172"
        ]
    },
    "NUTCH-2353": {
        "Key": "NUTCH-2353",
        "Summary": "Create seed file with metadata using the REST API",
        "Type": "Improvement",
        "Status": "Reopened",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.20",
        "Component/s": "injector,                                            REST_api",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "18/Jan/17 20:37",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "At the moment its not possible to create a seed file and specify any metadata when using the REST API. The file gets created but there is no option to add any metadata to the seed URLs.\nIf we use a payload like this:\n\n{\n    \"name\":\"name-of-seedlist\", \n    \"seedUrls\":[\n        {\n            \"url\" : \"http://example.com\",\n            \"metadata\" : {\n                \"key1\" : \"value1\",\n                \"key2\" : \"value2\",\n                \"key3\" : \"value3\"\n            }\n        }\n    ]\n}\n\n\nIt should be easy to specify the desired metadata. Also this should keep BC with the previous array syntax if we only want to specify the list of URLs without any metadata at all.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/175"
        ]
    },
    "NUTCH-2354": {
        "Key": "NUTCH-2354",
        "Summary": "Upgrade Hadoop dependencies to 2.7.4",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "injector",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Jan/17 13:23",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "15/Dec/17 19:38",
        "Description": "This wednesday we experienced trouble running the 1.12 injector on Hadoop 2.7.3. We operated 2.7.2 before and we had no trouble running a job.\n\n2017-01-18 15:36:53,005 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.Counter, but class was expected\n\tat org.apache.nutch.crawl.Injector$InjectMapper.map(Injector.java:216)\n\tat org.apache.nutch.crawl.Injector$InjectMapper.map(Injector.java:100)\n\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\n\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\n\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\n\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\n\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\nException in thread \"main\" java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.Counter, but class was expected\n        at org.apache.nutch.crawl.Injector.inject(Injector.java:383)\n        at org.apache.nutch.crawl.Injector.run(Injector.java:467)\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n        at org.apache.nutch.crawl.Injector.main(Injector.java:441)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n        at java.lang.reflect.Method.invoke(Method.java:498)\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)\n\n\nOur processes retried injecting for a few minutes until we manually shut it down. Meanwhile on HDFS, our CrawlDB was gone, thanks for snapshots and/or backups we could restore it, so enable those if you haven't done so yet.\nThese freak Hadoop errors can be notoriously difficult to debug but it seems we are in luck, recompile Nutch with Hadoop 2.7.3 instead 2.4.0. You are also in luck if your job file uses the old org.hadoop.mapred.* API, only jobs using the org.hadoop.mapreduce.* API seem to fail.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/261"
        ]
    },
    "NUTCH-2355": {
        "Key": "NUTCH-2355",
        "Summary": "Protocol plugins to set cookie if Cookie metadata field is present",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "31/Jan/17 10:19",
        "Updated": "25/Sep/17 10:55",
        "Resolved": "21/Feb/17 11:16",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-2432"
        ]
    },
    "NUTCH-2356": {
        "Key": "NUTCH-2356",
        "Summary": "Upgrade to Solr 6.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1,                                            1.12",
        "Fix Version/s": "2.5",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Cihad Guzel",
        "Created": "02/Feb/17 20:28",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Nutch 2.x branch support solr 4.6 [1] and nutch master branch support solr 5.5 [2]  according to ivy.xml of \"solr-indexer\" plugin .\n[1] https://github.com/apache/nutch/blob/2.x/src/plugin/indexer-solr/ivy.xml\n[2] https://github.com/apache/nutch/blob/master/src/plugin/indexer-solr/ivy.xml\nNutch should support Solr 6.x",
        "Issue Links": []
    },
    "NUTCH-2357": {
        "Key": "NUTCH-2357",
        "Summary": "Index metadata throw Exception because writable object cannot be cast to Text",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "indexer",
        "Assignee": "Chris A. Mattmann",
        "Reporter": "Eyeris Rodriguez Rueda",
        "Created": "07/Feb/17 13:53",
        "Updated": "14/Mar/17 23:51",
        "Resolved": "14/Mar/17 23:20",
        "Description": "Index Metadata plugin use this property(see below), to take keys from Datum and index it.\n<property>\n  <name>index.db.md</name>\n  <value></value>\n  <description>\n...\n  </description>\n</property>\nUsing any value from this property one Exception is thrown.\nThe problem occurs because Writable object can not be cast to Text see this line.\nhttps://github.com/apache/nutch/blob/master/src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataIndexer.java#L58\nA little change will fix it.\nThis is the Exception:\n**********************************\n2017-02-06 18:18:29,969 INFO  solr.SolrMappingReader - source: digest dest: digest\n2017-02-06 18:18:29,969 INFO  solr.SolrMappingReader - source: tstamp dest: tstamp\n2017-02-06 18:18:29,969 INFO  solr.SolrMappingReader - source: metatag.description dest: description\n2017-02-06 18:18:29,969 INFO  solr.SolrMappingReader - source: metatag.keywords dest: keywords\n2017-02-06 18:18:30,134 WARN  mapred.LocalJobRunner - job_local15168888_0001\njava.lang.Exception: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.Text\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.Text\n\tat org.apache.nutch.indexer.metadata.MetadataIndexer.filter(MetadataIndexer.java:58)\n\tat org.apache.nutch.indexer.IndexingFilters.filter(IndexingFilters.java:51)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:330)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:56)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n2017-02-06 18:18:30,777 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:836)\n\tat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:145)\n\tat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:228)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:237)\n******************************************",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/177"
        ]
    },
    "NUTCH-2358": {
        "Key": "NUTCH-2358",
        "Summary": "HostInjectorJob doesn't work",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "hostdb",
        "Assignee": null,
        "Reporter": "Kiyonari Harigae",
        "Created": "12/Feb/17 08:08",
        "Updated": "17/Dec/17 17:43",
        "Resolved": "17/Dec/17 17:14",
        "Description": "HostInjectorJob fails with NPE which causes\nHost#getMetadata returns null when instantiate Host with new Host().\nHowever, to run HostInjector completely, need to solve GORA-503",
        "Issue Links": [
            "/jira/browse/GORA-503"
        ]
    },
    "NUTCH-2359": {
        "Key": "NUTCH-2359",
        "Summary": "Parsefilter-regex raises IndexOutOfBoundsException when rules are ill-formed",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "plugin",
        "Assignee": "Markus Jelsma",
        "Reporter": "Laknath Semage",
        "Created": "13/Feb/17 13:14",
        "Updated": "18/Dec/17 14:22",
        "Resolved": "14/Feb/17 13:18",
        "Description": "This patch fixes:\n1) [Bug] Parsefilter-regex raises IndexOutOfBoundsException when rules are ill-formed\n2) Rules are split using any space character (\\s) instead tab (\\t) \n3) A detailed Readme for the plugin",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/178",
            "https://github.com/apache/nutch/pull/178"
        ]
    },
    "NUTCH-2360": {
        "Key": "NUTCH-2360",
        "Summary": "HTTP Basic Authentication in SolrIndexerPlugin is gone",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Resolved",
        "Affects Version/s": "1.12",
        "Fix Version/s": "None",
        "Component/s": "docker,                                            indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Patrick Schirch",
        "Created": "16/Feb/17 07:07",
        "Updated": "11/Oct/19 15:35",
        "Resolved": "11/Sep/18 07:49",
        "Description": "We upgraded Docker Nutch from 1.11 to 1.12. Now Nutch can't push to SSL HTTP Basic Auth protected Solr by SolrIndexerPlugin anymore. After some research we found the reason. The HTTP Basic Authentication was removed.\nhttps://svn.apache.org/viewvc/nutch/trunk/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrUtils.java?r1=1696506&r2=1728313&diff_format=h\nIs that intended?",
        "Issue Links": [
            "/jira/browse/NUTCH-2600"
        ]
    },
    "NUTCH-2361": {
        "Key": "NUTCH-2361",
        "Summary": "Deprecated nutch and solr integration documentation.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation,                                            wiki",
        "Assignee": null,
        "Reporter": "Omkar Reddy",
        "Created": "21/Feb/17 18:14",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "15/Nov/19 11:55",
        "Description": "I think the documentation here [0] is outdated and needs to be updated as solr's latest documentation[1] points solr has both managed schema and classic schema which can be used accordingly. \n[0] https://wiki.apache.org/nutch/NutchTutorial#Integrate_Solr_with_Nutch\n[1] https://cwiki.apache.org/confluence/display/solr/Schema+Factory+Definition+in+SolrConfig",
        "Issue Links": []
    },
    "NUTCH-2362": {
        "Key": "NUTCH-2362",
        "Summary": "Upgrade MaxMind GeoIP version in index-geoip",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Feb/17 06:06",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "15/Dec/17 20:48",
        "Description": "Current version of GeoIP dependency is 2.8.1, we should upgrade\nhttp://search.maven.org/#search|gav|1|g%3A%22com.maxmind.geoip2%22%20AND%20a%3A%22geoip2%22",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/262"
        ]
    },
    "NUTCH-2363": {
        "Key": "NUTCH-2363",
        "Summary": "Fetcher support for reading and setting cookies",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "28/Feb/17 12:06",
        "Updated": "19/Aug/22 13:38",
        "Resolved": null,
        "Description": "Patch adds basic support for cookies in the fetcher, and a scoring plugin that passes cookies to its outlinks, within the domain. Sub-domain or path based is not supported.\nThis is useful if you want to maintain sessions or need to get around a cookie wall.",
        "Issue Links": []
    },
    "NUTCH-2364": {
        "Key": "NUTCH-2364",
        "Summary": "http.agent.rotate: IllegalArgumentException / last element of agent names ignored",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.10,                                            1.11,                                            2.3.1,                                            1.12",
        "Fix Version/s": "2.4,                                            1.13",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "03/Mar/17 10:32",
        "Updated": "06/Mar/17 21:54",
        "Resolved": "06/Mar/17 21:22",
        "Description": "With http.agent.rotate == true and a one-element agent name list, the following exception is thrown:\n\n% cat .../conf/agents.txt\nmy-test-crawler/Nutch-1.13\n% .../bin/nutch parsechecker -Dhttp.agent.rotate=true http://nutch.apache.org/\n...\nFetch failed with protocol status: exception(16), lastModified=0: java.lang.IllegalArgumentException: bound must be positive\n% cat .../logs/hadoop.log\n...\n2017-03-03 11:17:19,750 ERROR http.Http - Failed to get protocol output\njava.lang.IllegalArgumentException: bound must be positive\n        at java.util.concurrent.ThreadLocalRandom.nextInt(ThreadLocalRandom.java:352)\n        at org.apache.nutch.protocol.http.api.HttpBase.getUserAgent(HttpBase.java:379)\n        at org.apache.nutch.protocol.http.HttpResponse.<init>(HttpResponse.java:180)\n...\n\n\nCaused by\n\nuserAgentNames.get(ThreadLocalRandom.current().nextInt(userAgentNames.size()-1));\n\n\nbut nextInt(...) is defined as: \"Returns a pseudorandom int value between zero (inclusive) and the specified bound (exclusive).\"",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/179"
        ]
    },
    "NUTCH-2365": {
        "Key": "NUTCH-2365",
        "Summary": "HTTP Redirects to SubDomains don't get crawled if db.ignore.external.links.mode == byDomain",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sriram Nookala",
        "Created": "09/Mar/17 15:01",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "18/Dec/17 16:31",
        "Description": "Crawling a domain  http://www.mercenarytrader.com which redirects to https://members.mercenarytrader.com which doesn't get followed by Nutch even though 'db.ignore.external.links' is set to 'true' and 'db.ignore.external.links.mode' is set to 'byDomain'. \n  The bug is in FetcherThread where the comparison is by host and not by domain\nString origHost = new URL(urlString).getHost().toLowerCase();\n>       String newHost = new URL(newUrl).getHost().toLowerCase();\n>       if (ignoreExternalLinks) {\n>         if (!origHost.equals(newHost)) {\n>           if (LOG.isDebugEnabled()) \n{\n>             LOG.debug(\" - ignoring redirect \" + redirType + \" from \"\n>                 + urlString + \" to \" + newUrl\n>                 + \" because external links are ignored\");\n>           }\n>           return null;\n>         }\n>       }",
        "Issue Links": [
            "/jira/browse/NUTCH-2216",
            "/jira/browse/NUTCH-2069",
            "https://github.com/apache/nutch/pull/264"
        ]
    },
    "NUTCH-2366": {
        "Key": "NUTCH-2366",
        "Summary": "Deprecated Job constructor in hostdb/ReadHostDb.java",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "build",
        "Assignee": "Markus Jelsma",
        "Reporter": "Omkar Reddy",
        "Created": "11/Mar/17 06:32",
        "Updated": "17/Mar/17 05:05",
        "Resolved": "15/Mar/17 12:05",
        "Description": "When we try to build ant using nutch we get the following warning : \nwarning: [deprecation] Job(Configuration,String) in Job has been deprecated\n[javac]     Job job = new Job(conf, \"ReadHostDb\");\nThis is because the constructor Job(Configuration conf, String jobName) has been deprecated and the reference can be found at [0].\n[0] http://hadoop.apache.org/docs/stable2/api/org/apache/hadoop/mapreduce/Job.html#getInstance%28org.apache.hadoop.conf.Configuration%29",
        "Issue Links": []
    },
    "NUTCH-2367": {
        "Key": "NUTCH-2367",
        "Summary": "Get single record from HostDB",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.13",
        "Component/s": "hostdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "14/Mar/17 13:21",
        "Updated": "16/Mar/17 10:52",
        "Resolved": "16/Mar/17 10:40",
        "Description": "Introduces:\n\nbin/nutch readhostdb crawl/hostdb/ -get www.apache.org",
        "Issue Links": []
    },
    "NUTCH-2368": {
        "Key": "NUTCH-2368",
        "Summary": "Variable generate.max.count and fetcher.server.delay",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.14",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "14/Mar/17 16:42",
        "Updated": "15/Dec/17 12:59",
        "Resolved": "26/Jul/17 11:21",
        "Description": "In some cases we need to use host specific characteristics in determining crawl speed and bulk sizes because with our (Openindex) settings we can just recrawl host with up to 800k urls.\nThis patch solves the problem by introducing the HostDB to the Generator and providing powerful Jexl expressions. Check these two expressions added to the Generator:\n\n-Dgenerate.max.count.expr='\nif (unfetched + fetched > 800000) {\n  return (conf.getInt(\"fetcher.timelimit.mins\", 12) * 60) / ((pct95._rs_ + 500) / 1000) * conf.getInt(\"fetcher.threads.per.queue\", 1)\n} else {\n  return conf.getDouble(\"generate.max.count\", 300);\n}'\n\n-Dgenerate.fetch.delay.expr='\nif (unfetched + fetched > 800000) {\n  return (pct95._rs_ + 500);\n} else {\n  return conf.getDouble(\"fetcher.server.delay\", 1000)\n}'\n\n\nFor each large host: select as many records as possible that are possible to fetch based on number of threads, 95th percentile response time of the fetch limit. Or: queueMaxCount = (timelimit / resonsetime) * numThreads.\nThe second expression just follows up to that, settings the crawlDelay of the fetch queue.",
        "Issue Links": [
            "/jira/browse/NUTCH-2461",
            "/jira/browse/NUTCH-2402",
            "/jira/browse/NUTCH-2481",
            "/jira/browse/NUTCH-2454",
            "/jira/browse/NUTCH-2455"
        ]
    },
    "NUTCH-2369": {
        "Key": "NUTCH-2369",
        "Summary": "Create a new GraphGenerator Tool for writing Nutch Records as a Full Web Graph",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "crawldb,                                            graphgenerator,                                            hostdb,                                            linkdb,                                            segment,                                            storage,                                            tool",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Mar/17 06:19",
        "Updated": "12/Jun/18 19:33",
        "Resolved": null,
        "Description": "I've been thinking for quite some time now that a new Tool which writes Nutch data out as full graph data would be an excellent addition to the codebase.\nMy thoughts involves writing data using Tinkerpop's ScriptInputFormat and ScriptOutputFormat's to create Vertex objects representing Nutch Crawl Records. \nhttp://tinkerpop.apache.org/javadocs/current/full/index.html?org/apache/tinkerpop/gremlin/hadoop/structure/io/script/ScriptInputFormat.html\nhttp://tinkerpop.apache.org/javadocs/current/full/index.html?org/apache/tinkerpop/gremlin/hadoop/structure/io/script/ScriptOutputFormat.html\nI envisage that each Vertex object would require the CrawlDB, LinkDB a Segment and possibly the HostDB in order to be fully populated. Graph characteristics e.g. Edge's would comes from those existing data structures as well.\nIt is my intention to propose this as a GSoC project for 2017 and I have already talked offline with a potential student omkar20895 about him participating as the student.\nEssentially, if we were able to create a Graph enabling true traversal, this could be a game changer for how Nutch Crawl data is interpreted. It is my feeling that this issue most likely also involved an entire upgrade of the Hadoop API's from mapred to mapreduce for the master codebase.",
        "Issue Links": []
    },
    "NUTCH-2370": {
        "Key": "NUTCH-2370",
        "Summary": "FileDumper: save JSON mapping file -> URL",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.14",
        "Component/s": "dumpers",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Madhav Sharan",
        "Created": "29/Mar/17 22:05",
        "Updated": "16/Jan/18 21:59",
        "Resolved": "17/Dec/17 15:16",
        "Description": "nutch dump [0] is a great tool to simply dump all the crawled files from nutch segments.\nAfter dump we loose information about URL from which this file was crawled. URL is used to name dumped file but that information is encrypted.\nIn `reverseUrlDirs` option one can figure out URL by checking the file path but even accessing file path is little complicated than simple mapping file.\nIn `flatdir` there is no way to know actual URL.\n\nI am submitting a PR which edits [0] and saves a json for each crawled segment which maps a file path to URL.\n[0] https://github.com/apache/nutch/blob/3e2d3d456489bf52bc586dae0e2e71fb7aad8fe7/src/java/org/apache/nutch/tools/FileDumper.java",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/180",
            "https://github.com/apache/nutch/pull/180"
        ]
    },
    "NUTCH-2371": {
        "Key": "NUTCH-2371",
        "Summary": "Injector to support noFilter and noNormalize",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "injector",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Apr/17 12:52",
        "Updated": "06/Apr/17 13:07",
        "Resolved": "06/Apr/17 13:07",
        "Description": "Injector does not support these switches, causing a simple injection of a few records to take an unreasonable amount of time.",
        "Issue Links": []
    },
    "NUTCH-2372": {
        "Key": "NUTCH-2372",
        "Summary": "Javadocs build failing.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2.1,                                            1.13",
        "Fix Version/s": "1.14",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Omkar Reddy",
        "Created": "10/Apr/17 14:52",
        "Updated": "15/Apr/17 11:31",
        "Resolved": "15/Apr/17 11:20",
        "Description": "When we build javadocs of nutch using the command : \"ant javadoc\" \nwe get a handful of errors and the build fails. This is because up to JDK 7, the Javadoc tool was pretty lenient. With JDK 8, a new part has been added to Javadoc called doclint and it changes that friendly behaviour. Warnings turned out into errors with JDK 8. \nThe error log can be found here : https://paste.apache.org/sVQ5",
        "Issue Links": []
    },
    "NUTCH-2373": {
        "Key": "NUTCH-2373",
        "Summary": "Indexer for Hbase",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.4",
        "Component/s": "indexer",
        "Assignee": "Kaidul Islam",
        "Reporter": "Kaidul Islam",
        "Created": "16/Apr/17 06:29",
        "Updated": "23/May/17 04:26",
        "Resolved": "22/May/17 21:02",
        "Description": "Some use-case involves storing the documents in some sort of database other than indexing search engines i.e. Solr, ElasticSearch.  This is a plugin to send the documents to Hbase storage.",
        "Issue Links": [
            "/jira/browse/NUTCH-2382"
        ]
    },
    "NUTCH-2374": {
        "Key": "NUTCH-2374",
        "Summary": "Upgrade Nutch 2.X to Gora 0.7",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4",
        "Component/s": "build,                                            storage",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Apr/17 16:55",
        "Updated": "04/Oct/17 18:44",
        "Resolved": "04/Oct/17 17:41",
        "Description": "We should make the upgrades before we release Nutch 2.X.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/183"
        ]
    },
    "NUTCH-2375": {
        "Key": "NUTCH-2375",
        "Summary": "Upgrade the code base from org.apache.hadoop.mapred to org.apache.hadoop.mapreduce",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "deployment",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Omkar Reddy",
        "Created": "20/Apr/17 10:58",
        "Updated": "19/Aug/22 09:03",
        "Resolved": "27/Feb/18 22:03",
        "Description": "Nutch is still using the deprecated org.apache.hadoop.mapred dependency which has been deprecated. It need to be updated to org.apache.hadoop.mapreduce dependency.",
        "Issue Links": [
            "/jira/browse/NUTCH-2517",
            "/jira/browse/NUTCH-2518",
            "/jira/browse/NUTCH-2550",
            "/jira/browse/NUTCH-2551",
            "/jira/browse/NUTCH-2569",
            "/jira/browse/NUTCH-2544",
            "/jira/browse/NUTCH-2552",
            "/jira/browse/NUTCH-2553",
            "/jira/browse/NUTCH-2597",
            "/jira/browse/NUTCH-2652",
            "/jira/browse/NUTCH-2535",
            "/jira/browse/NUTCH-2572",
            "/jira/browse/NUTCH-2590",
            "/jira/browse/NUTCH-2571",
            "/jira/browse/NUTCH-2717",
            "/jira/browse/NUTCH-2566",
            "/jira/browse/NUTCH-1380",
            "/jira/browse/NUTCH-1223",
            "/jira/browse/NUTCH-1224",
            "/jira/browse/NUTCH-1226",
            "/jira/browse/NUTCH-1783",
            "/jira/browse/NUTCH-1219",
            "https://github.com/apache/nutch/pull/188",
            "https://github.com/apache/nutch/pull/221"
        ]
    },
    "NUTCH-2376": {
        "Key": "NUTCH-2376",
        "Summary": "Improve configurability of HTTP Accept* header fields",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.13",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "20/Apr/17 12:27",
        "Updated": "19/May/17 15:10",
        "Resolved": "19/May/17 10:56",
        "Description": "There should be no differences between protocol-http and protocol-httpclient whether the HTTP header fields Accept, Accept-Language and Accept-Charset are configurable. The configured values should be used for both plugins. In addition,\n\nit should be possible to unset the default values (overwrite with empty value) so that no HTTP header field is sent\ndefault values should be contained in nutch-default.xml\n\nNote: Accept-Encoding should not be configurable as the protocol plugins must support the accepted compression codecs which may not be the case e.g. for Brotli.",
        "Issue Links": []
    },
    "NUTCH-2377": {
        "Key": "NUTCH-2377",
        "Summary": "Nutch can't parse relative links",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.3.1",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "hakim",
        "Created": "29/Apr/17 23:12",
        "Updated": "03/May/17 20:18",
        "Resolved": "03/May/17 20:18",
        "Description": "Testing with the following site: https://www.ouedkniss.com, nutch only parse links that does contain the base url. \nTried tika as parser, tried to update db.max.outlinks.per.page to -1, tried practically every comments about detecting all the links, doubted urlfilter or regex-normalizer so it was disabled but having the same results. \neach time I rebuild nutch and test the parser, it gives the same urls count arround 378. \nCan somebody help out to fix this.",
        "Issue Links": []
    },
    "NUTCH-2378": {
        "Key": "NUTCH-2378",
        "Summary": "ChildFirst plugin classloader",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.13",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Jurian Broertjes",
        "Created": "01/May/17 15:42",
        "Updated": "18/Aug/17 14:34",
        "Resolved": "18/Aug/17 14:34",
        "Description": "While working on upgrading the indexer-elastic plugin from 2.x to 5.x, I ran into several nasty runtime dependency issues (both local and on Hadoop). After seeking help on the mailing list, I still was unable to resolve these issues and after digging further, decided to try a different plugin classloader strategy. \nThe normal classloader delegates class loading requests to it's parent classloader. This can cause all sorts of nasty runtime dependency version conflicts (jar hell, version conflicts), since the plugin's own classloader gets queried last. The child-first classloader approach tries to load a class from the plugin's dependencies first and when unavailable, delegates to it's parent classloader. This fixed the issues I had.\nThe new approach can give runtime LinkageErrors, but these are easily resolvable (see the patch for a few examples)\nI've tested the new loader a bit and am curious about others' findings.",
        "Issue Links": [
            "/jira/browse/NUTCH-2316",
            "/jira/browse/NUTCH-2071",
            "/jira/browse/NUTCH-2380"
        ]
    },
    "NUTCH-2379": {
        "Key": "NUTCH-2379",
        "Summary": "crawl script dedup's crawldb update is slow",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.17",
        "Component/s": "bin",
        "Assignee": null,
        "Reporter": "Michael Coffey",
        "Created": "01/May/17 22:52",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "28/Apr/20 08:58",
        "Description": "In the standard crawl script, there is a _bin_nutch updatedb command and, soon after that, a _bin_nutch dedup command. Both of them launch hadoop jobs with \"crawldb /path/to/crawl/db\" in their names (in addition to the actual deduplication job).\nIn my situation, the \"crawldb\" job launched by dedup takes twice as long as the one launched by updatedb.\nI notice that the script passes $commonOptions to updatedb but not to dedup. I suspect that the crawldb update launched by dedup may not be compressing its output.",
        "Issue Links": []
    },
    "NUTCH-2380": {
        "Key": "NUTCH-2380",
        "Summary": "indexer-elastic version upgrade to 5.3.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "02/May/17 08:28",
        "Updated": "01/Jun/18 19:04",
        "Resolved": "18/Dec/17 16:25",
        "Description": "The current version of the indexer-elastic plugin is not compatible with ES 5.x. The patch bumps the ES lib version to 5.3 but also requires a Nutch classloader fix (NUTCH-2378) due to runtime dependency issues. \nI didn't test compatibility with ES 2.x, so not sure if that still works.\nPlease let me know what you think of the provided patch.",
        "Issue Links": [
            "/jira/browse/NUTCH-2378"
        ]
    },
    "NUTCH-2381": {
        "Key": "NUTCH-2381",
        "Summary": "In some situations the class TextProfileSignature gives different signatures for the same text \"profile\" page.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.16",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Rodrigo Joni Sestari",
        "Created": "02/May/17 11:53",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "30/Sep/19 11:32",
        "Description": "In some situations the class TextProfileSignature gives different signatures for the same text \"profile\" page.\nThe method TextProfileSignature.calculate uses a HashMap to salve the tokens, after some process, the tokens come sorted by decreasing frequency.\nFor some pages like \"http://curia.europa.eu/jcms/\" the text \"profile\" is the same but the signature come different for each fetch.\nIts happens because the tokens are sorted only by decreasing frequency. Tokens with the same frequency maybe not have the same order in different fetchs.\nThe HashMap no guarantees as to the order of the map and  not guarantee that the order will remain constant over time.\nMy suggestion is change the methods TokenComparator.compare  in order to sort by frequency and Name.\nRodrigo",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/473"
        ]
    },
    "NUTCH-2382": {
        "Key": "NUTCH-2382",
        "Summary": "indexer-hbase Nutch 1.x branch",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.20",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "02/May/17 12:47",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "I've ported the indexer-hbase for Nutch 2.x (https://github.com/apache/nutch/pull/184) to 1.x. Dit some basic tests. Patch is attached.",
        "Issue Links": [
            "/jira/browse/NUTCH-2373"
        ]
    },
    "NUTCH-2383": {
        "Key": "NUTCH-2383",
        "Summary": "Wrong FS exception in Fetcher",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.13",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "02/May/17 13:26",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "04/Nov/17 16:42",
        "Description": "Running bin/crawl on either Hadoop 2.7.2 or Hadoop 2.8, the Injector and Generator succeed, but the Fetcher throws: \n\njava.lang.IllegalArgumentException: Wrong FS: hdfs://localhost:9000/user/root/crawl/segments/20170430084337/crawl_fetch, expected: file:///\n\n.",
        "Issue Links": []
    },
    "NUTCH-2384": {
        "Key": "NUTCH-2384",
        "Summary": "nutch 2.3.1 job not properly interacting with hadoop 2.7.1",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "nutchNewbie",
        "Assignee": null,
        "Reporter": "Shubham Gupta",
        "Created": "03/May/17 12:17",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "11/Apr/18 11:38",
        "Description": "Hey, \nI am testing the Nutch crawler on local environment as well as on Hadoop cluster. \nThe script is able to fetch millions of documents but the apache job created after running the command \"ant clean runtime\" fails to do so.\nWhile testing in the local environment i.e using the following commands:\nbin/nutch fetch -all -crawlId <table-name>.\nIt ends up fetching all the URLs that are present in the queue. And I have been able to crawl over a 100,000 URLs. (5000 seed URLs)\nWhereas, when I run the same project on the Hadoop cluster, I am not able to reach even the 100,000 mark. It has only fetched a 45,000  URLs. (1100 seed URLs)\nWhen tested with 5000 seed URLs, then also it was able to fetch such amounts of data.\nThe plugins used in Nutch are as follows:\nprotocol-http|urlfilter-regex|parse-(html|tika|metatags)|index-(basic|anchor|metadata)|urlnormalizer-(pass|regex|basic)|scoring-opic\nThe settings I am using with the hadoop cluster are as follows:\nMAPRED-SITE.XML:\n<property>\n<name>mapreduce.map.memory.mb</name>\n<value>1024</value>\n</property>\n<property>\n<name>mapreduce.reduce.memory.mb</name>\n<value>2048</value>\n</property>\n<property>\n<name>mapreduce.reduce.java.opts</name>\n<value>-Xmx1800m</value>\n</property>\n<property>\n<name>mapreduce.map.java.opts</name>\n<value>-Xmx712m</value>\n</property>\n<property>\n<name>mapred.job.tracker.http.address</name>\n<value>master:50030</value>\n</property>\n<property>\n    <name>yarn.app.mapreduce.am.resource.mb</name>\n        <value>1024</value>\n        </property>\n        <property>\n            <name>yarn.app.mapreduce.am.command-opts</name>\n                <value>-Xmx800m</value>\n                </property>\nYARN-SITE.XML:\n<property>\n    <name>yarn.scheduler.minimum-allocation-mb</name>\n    <value>1024</value>\n   <description>minimum memory allcated to containers.</description>\n</property>\n<property>\n    <name>yarn.scheduler.maximum-allocation-mb</name>\n    <value>5120</value>\n   <description>maximum memory allcated to containers.</description>\n</property>\n<property>\n    <name>yarn.scheduler.minimum-allocation-vcores</name>\n    <value>1</value>\n</property>\n<property>\n    <name>yarn.scheduler.maximum-allocation-vcores</name>\n    <value>4</value>\n </property>\n<property>\n   <name>yarn.nodemanager.resource.memory-mb</name>\n   <value>12288</value>\n<description>max memory allcated to nodemanager.</description>\n</property>\n<property>\n <name>yarn.nodemanager.vmem-pmem-ratio</name>\n <value>2.1</value>\n</property>\n<property>\n  <name>yarn.scheduler.capacity.maximum-am-resource-percent</name>\n  <value>100</value>\n</property>\n<property>\n   <name>yarn.nodemanager.vmem-check-enabled</name>\n    <value>false</value>\n    <description>Whether virtual memory limits will be enforced for containers</description>\n  </property>\nThe RAM available to the system is 6 GB and Network Bandwidth available is 4 Mb/sec.",
        "Issue Links": []
    },
    "NUTCH-2385": {
        "Key": "NUTCH-2385",
        "Summary": "1.x Elasticsearch Indexer - path.home is not configured",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.13",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Steven W",
        "Created": "03/May/17 17:44",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "23/Apr/20 11:06",
        "Description": "Running Nutch 1.13 binaries, and configured to use indexer-elastic throws this error when indexing:\njava.lang.Exception: java.lang.IllegalStateException: path.home is not configured\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: java.lang.IllegalStateException: path.home is not configured\n        at org.elasticsearch.env.Environment.<init>(Environment.java:101)\n        at org.elasticsearch.node.internal.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:81)\n        at org.elasticsearch.node.Node.<init>(Node.java:140)\n        at org.elasticsearch.node.NodeBuilder.build(NodeBuilder.java:143)\n        at org.elasticsearch.node.NodeBuilder.node(NodeBuilder.java:150)\n        at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.makeClient(ElasticIndexWriter.java:141)\n        at org.apache.nutch.indexwriter.elastic.ElasticIndexWriter.open(ElasticIndexWriter.java:91)\n        at org.apache.nutch.indexer.IndexWriters.open(IndexWriters.java:77)\n        at org.apache.nutch.indexer.IndexerOutputFormat.getRecordWriter(IndexerOutputFormat.java:39)\n        at org.apache.hadoop.mapred.ReduceTask$OldTrackingRecordWriter.<init>(ReduceTask.java:484)\n        at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:414)\n        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n        at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n        at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)",
        "Issue Links": []
    },
    "NUTCH-2386": {
        "Key": "NUTCH-2386",
        "Summary": "BasicURLNormalizer does not encode curly braces",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "15/May/17 13:47",
        "Updated": "25/Oct/17 13:01",
        "Resolved": "25/Oct/17 13:01",
        "Description": "Causing:\n\n2017-05-15 13:23:33,474 ERROR [FetcherThread] org.apache.nutch.protocol.httpclient.Http: Failed to get protocol output\njava.lang.IllegalArgumentException: Invalid uri 'https://www.example.org/32/{{relative_url}}': escaped absolute path not valid\n\tat org.apache.commons.httpclient.HttpMethodBase.<init>(HttpMethodBase.java:222)\n\tat org.apache.commons.httpclient.methods.GetMethod.<init>(GetMethod.java:89)\n\tat org.apache.nutch.protocol.httpclient.HttpResponse.<init>(HttpResponse.java:76)\n\tat org.apache.nutch.protocol.httpclient.Http.getResponse(Http.java:181)\n\tat org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:261)\n\tat org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:295)",
        "Issue Links": []
    },
    "NUTCH-2387": {
        "Key": "NUTCH-2387",
        "Summary": "Nutch should not index document with \"noindex\" meta",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Eyeris Rodriguez Rueda",
        "Created": "18/May/17 18:50",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "30/Sep/19 13:43",
        "Description": "I'm using nutch 1.12 in local mode and solr 4.10.3.\nFor some reason i have detected that nutch index document with \"noindex\" robots meta.\n I have use nutch script for a complete cycle: \nbin/crawl -i urls/ crawl/ -2\nwith this url:\nhttps://humanos.uci.cu/category/humanos/comparte-tu-software/page/3/ \nAfter various testing the problem persist and aproximately 200 document with this robots meta are indexed incorrectly.\nI have read the method configure in IndexerMapReduce.java class and it has a line for that property but for some reason it is not doing appropiately.\nthis.deleteRobotsNoIndex =  job.getBoolean(INDEXER_DELETE_ROBOTS_NOINDEX,false);   (line 97)",
        "Issue Links": []
    },
    "NUTCH-2388": {
        "Key": "NUTCH-2388",
        "Summary": "bin/crawl indexing only webpages containing batchID instead of all in 2.x",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.4",
        "Component/s": "bin",
        "Assignee": "Kaidul Islam",
        "Reporter": "Kaidul Islam",
        "Created": "23/May/17 07:09",
        "Updated": "23/May/17 17:44",
        "Resolved": "23/May/17 16:48",
        "Description": "During each iteration, after generating, fetching, parsing and updating the current batch into DB, the indexer is supposed to index the current batch too. But its indexing all currently.\n\n__bin_nutch index $commonOptions -D solr.server.url=$SOLRURL -all -crawlId \"$CRAWL_ID\"\n\n\nIt should be like below i guess -\n\n__bin_nutch index $commonOptions -D solr.server.url=$SOLRURL $batchId -crawlId \"$CRAWL_ID\"",
        "Issue Links": []
    },
    "NUTCH-2389": {
        "Key": "NUTCH-2389",
        "Summary": "Precise data parsing using Jsoup CSS selectors",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.4",
        "Component/s": "parser",
        "Assignee": "Kaidul Islam",
        "Reporter": "Kaidul Islam",
        "Created": "23/May/17 09:30",
        "Updated": "31/Jul/17 06:27",
        "Resolved": "30/Jul/17 16:39",
        "Description": "As far as I know, currently Nutch 1.x and 2.x has no features to extract/parse exact contents for specific websites. I've developed a plugin parse-jsoup using Jsoup for my current project to extract precise content for site specific crawling using detailed XML configuration(field name, CSS-selector, attribute, extraction rules, data-type, default-value etc).\nPlease let me know if this feature seems relevant and currently not present in Nutch. I have also plan to export it into Nutch 1.x.",
        "Issue Links": []
    },
    "NUTCH-2390": {
        "Key": "NUTCH-2390",
        "Summary": "No documentation on pluggable indexing",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.15",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Jonathan Jackson",
        "Created": "23/May/17 15:13",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "26/Jul/18 12:27",
        "Description": "I'm currently struggling to figure out how to use the pluggable indexing feature effectively. Some brief docs might help.\nhttps://wiki.apache.org/nutch/bin/nutch%20index",
        "Issue Links": []
    },
    "NUTCH-2391": {
        "Key": "NUTCH-2391",
        "Summary": "Spurious Duplications for MD5",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.11",
        "Fix Version/s": "1.14",
        "Component/s": "commoncrawl",
        "Assignee": "Sebastian Nagel",
        "Reporter": "David Johnson",
        "Created": "05/Jun/17 18:08",
        "Updated": "05/Jul/17 16:06",
        "Resolved": "05/Jul/17 15:29",
        "Description": "We're seeing some incidence of a large number of documents being marked as duplicate in our crawl.\nWe traced it back to one of the crawl plugins returning an empty array for the content field.\nWe'd like to propose changing the MD5 signature generation from:\n\npublic byte[] calculate(Content content, Parse parse) {\n    byte[] data = content.getContent();\n    if (data == null)\n      data = content.getUrl().getBytes();\n    return MD5Hash.digest(data).getDigest();\n  }\n\n\nto:\n\npublic byte[] calculate(Content content, Parse parse) {\n    byte[] data = content.getContent();\n    if ((data == null) || (data.length == 0))\n      data = content.getUrl().getBytes();\n    return MD5Hash.digest(data).getDigest();\n  }\n\n\nto address the issue",
        "Issue Links": [
            "/jira/browse/NUTCH-2393"
        ]
    },
    "NUTCH-2392": {
        "Key": "NUTCH-2392",
        "Summary": "Get same pages multiple times if URL contains relative path",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.13",
        "Fix Version/s": "None",
        "Component/s": "commoncrawl",
        "Assignee": null,
        "Reporter": "Jayesh Shende",
        "Created": "07/Jun/17 12:18",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "18/Dec/17 17:27",
        "Description": "When websites have relative URL at different pages for same HTML document, for example on first depth I fetched contents of a page http://example.com/index.html, after few depths I got a link (constructed by Nutch from some relative path pattern in some anchor tag) http://example.com/Level1/Level2/../../index.html , in this case Nutch is fetching same HTML document two times considering both URLs are different but they are not.",
        "Issue Links": []
    },
    "NUTCH-2393": {
        "Key": "NUTCH-2393",
        "Summary": "2.x patch for MD5 duplication issue addressed in NUTCH-2391",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "commoncrawl",
        "Assignee": "Kaidul Islam",
        "Reporter": "Kaidul Islam",
        "Created": "10/Jun/17 05:12",
        "Updated": "05/Jul/17 15:55",
        "Resolved": "05/Jul/17 15:25",
        "Description": "Equivalent patch for 2.x for issue addressed in NUTCH-2391",
        "Issue Links": [
            "/jira/browse/NUTCH-2391"
        ]
    },
    "NUTCH-2394": {
        "Key": "NUTCH-2394",
        "Summary": "Possible bugs in the source code",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "AppChecker",
        "Created": "12/Jun/17 22:26",
        "Updated": "25/Oct/17 17:08",
        "Resolved": "25/Oct/17 15:01",
        "Description": "Hi!\nI've checked your project with static analyzer AppChecker and if found several suspicious code fragments:\n1) src/plugin/headings/src/java/org/apache/nutch/parse/headings/HeadingsParseFilter.java\n\nheading.trim();\n\n\nheading is not changed, because java.lang.String.trim returns new string.\nProbably, it should be:\n\nheading = heading.trim();\n\n\nsee also:\n\nsrc/plugin/urlnormalizer-host/src/java/org/apache/nutch/net/urlnormalizer/host/HostURLNormalizer.java#L78\nsrc/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java#L115\nsrc/java/org/apache/nutch/net/urlnormalizer/protocol/ProtocolURLNormalizer.java#L76\nsrc/java/org/apache/nutch/net/urlnormalizer/slash/SlashURLNormalizer.java#L78\nsrc/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java#L326\n\n2) src/java/org/apache/nutch/crawl/URLPartitioner.java#L84\n\nif (mode.equals(PARTITION_MODE_DOMAIN) && url != null)\n  ...\nelse if ..\n  ...\n  InetAddress address = InetAddress.getByName(url.getHost());\n  ...\n\n\nif url is null, method url.getHost() will be invoked, so NullPointerException wiil be thrown\n3) src/java/org/apache/nutch/tools/CommonCrawlDataDumper.java#L346\n\nString[] fullPathLevels = fullDir.split(File.separator);\n\n\nUsing File.separator in regular expressions may throws java.util.regex.PatternSyntaxException exceptions, because it is \"\\\" on Windows-based systems.\nPossible \tcorrection:\n\nString[] fullPathLevels = fullDir.split(Pattern.quote(File.separator));",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/234"
        ]
    },
    "NUTCH-2395": {
        "Key": "NUTCH-2395",
        "Summary": "Cannot run job worker! - error while running multiple crawling jobs in parallel",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "generator,                                            nutch server",
        "Assignee": null,
        "Reporter": "Vyacheslav Pascarel",
        "Created": "26/Jun/17 14:57",
        "Updated": "19/Jan/20 12:25",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Cannot run job worker! - error while running multiple crawling jobs in parallel\nUbuntu 16.04 64-bit\nOracle Java 8 64-bit\nNutch 2.3.1 (standalone deployment)\nMongoDB 3.4\nMy application is trying to execute multiple Nutch jobs in parallel using Nutch REST services. The application injects a seed URL and then repeats GENERATE/FETCH/PARSE/UPDATEDB sequence requested number of times to emulated continuous crawling (each step in the sequence is executed upon successful competition of the previous step then the whole sequence is repeated again). Here is a brief description of the jobs:\n\nNumber of parallel jobs: 7\nEach job has unique crawl id and MongoDB collection\nSeed URL for all jobs: http://www.cnn.com\nRegex URL filters for all jobs:\n\t\n\"-^.{1000,}$\" - exclude very long URLs\n\"+.\" - include the rest\n\n\n\nThe jobs are started as expected but at some point some of them fail with \"Cannot run job worker!\" error. For more details see job status and hadoop.log lines below.\nIn debugger during crash I noticed that a single instance of SelectorEntryComparator (definition is nested in GeneratorJob) is shared across multiple reducer tasks. The class is inherited from org.apache.hadoop.io.WritableComparator which has a few members unprotected for concurrent usage. At some point multiple threads may access those members in WritableComparator.compare call. I modified SelectorEntryComparator and it seems solved the problem but I am not sure if the change is appropriate and/or sufficient (covers GENERATE only?)\nOriginal code:\n\npublic static class SelectorEntryComparator extends WritableComparator {\n    public SelectorEntryComparator() {\n      super(SelectorEntry.class, true);\n    }\n}\n\n\nModified code:\n\npublic static class SelectorEntryComparator extends WritableComparator {\n    public SelectorEntryComparator() {\n      super(SelectorEntry.class, true);\n    }\n    \n    @Override\n    synchronized public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) {\n    \treturn super.compare(b1, s1, l1, b2, s2, l2);\n    }    \n}\n\n\nExample of failed job status:\n\n{\n\"id\" : \"parallel_0-65ff2f1b-382e-4eb2-a813-a0370b84d5b6-GENERATE-1961495833\",\n\"type\" : \"GENERATE\",\n\"confId\" : \"65ff2f1b-382e-4eb2-a813-a0370b84d5b6\",\n\"args\" : { \"topN\" : \"100\" },\n\"result\" : null,\n\"state\" : \"FAILED\",\n\"msg\" : \"ERROR: java.lang.RuntimeException: job failed: name=[parallel_0]generate: 1498059912-1448058551, jobid=job_local1142434549_0036\",\n\"crawlId\" : \"parallel_0\"\n}\n\n\nLines from hadoop.log\n\n2017-06-21 11:45:13,021 WARN  mapred.LocalJobRunner - job_local1142434549_0036\njava.lang.Exception: java.lang.RuntimeException: java.io.EOFException\n                at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n                at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: java.lang.RuntimeException: java.io.EOFException\n                at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:164)\n                at org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKeyValue(ReduceContextImpl.java:158)\n                at org.apache.hadoop.mapreduce.task.ReduceContextImpl.nextKey(ReduceContextImpl.java:121)\n                at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.nextKey(WrappedReducer.java:302)\n                at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:170)\n                at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n                at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n                at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n                at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n                at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n                at java.io.DataInputStream.readFully(DataInputStream.java:197)\n                at org.apache.hadoop.io.Text.readString(Text.java:466)\n                at org.apache.hadoop.io.Text.readString(Text.java:457)\n                at org.apache.nutch.crawl.GeneratorJob$SelectorEntry.readFields(GeneratorJob.java:92)\n                at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:158)\n                ... 12 more\n2017-06-21 11:45:13,058 WARN  mapred.LocalJobRunner - job_local1976432650_0038\njava.lang.Exception: java.lang.RuntimeException: java.io.EOFException\n                at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n                at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\nCaused by: java.lang.RuntimeException: java.io.EOFException\n                at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:164)\n                at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.compare(MapTask.java:1245)\n                at org.apache.hadoop.util.QuickSort.sortInternal(QuickSort.java:99)\n                at org.apache.hadoop.util.QuickSort.sortInternal(QuickSort.java:126)\n                at org.apache.hadoop.util.QuickSort.sort(QuickSort.java:63)\n                at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1575)\n                at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1462)\n                at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:700)\n                at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:770)\n                at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n                at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)\n                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n                at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n                at java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.EOFException\n                at java.io.DataInputStream.readByte(DataInputStream.java:267)\n                at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)\n                at org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)\n                at org.apache.hadoop.io.Text.readString(Text.java:464)\n                at org.apache.hadoop.io.Text.readString(Text.java:457)\n                at org.apache.nutch.crawl.GeneratorJob$SelectorEntry.readFields(GeneratorJob.java:92)\n                at org.apache.hadoop.io.WritableComparator.compare(WritableComparator.java:158)\n                ... 15 more\n\n\n\n2017-06-21 11:45:13,372 ERROR impl.JobWorker - Cannot run job worker!\njava.lang.RuntimeException: job failed: name=[parallel_0]generate: 1498059912-1448058551, jobid=job_local1142434549_0036\n                at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:120)\n                at org.apache.nutch.crawl.GeneratorJob.run(GeneratorJob.java:227)\n                at org.apache.nutch.api.impl.JobWorker.run(JobWorker.java:64)\n                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n                at java.lang.Thread.run(Thread.java:745)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/197"
        ]
    },
    "NUTCH-2396": {
        "Key": "NUTCH-2396",
        "Summary": "Cannot stop or abort fetch job via REST API",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.20",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Sergey",
        "Created": "30/Jun/17 12:36",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Case 1:\n1) Run fetch job via REST API.\n2) Send stop job  request.\n3) Request finished with code 200 and returned string 'false'.\n4) Job state changed to \"STOPPING\" and stopped only after finished all his work.\nCase 2:\n1) Run fetch job via REST API.\n2) Send abort job request.\n3) Request finished with code 200 and returned string 'false'.\n4) Job state changed  to \"KILLED\", but job continue working and stopped after finished all his work.",
        "Issue Links": []
    },
    "NUTCH-2397": {
        "Key": "NUTCH-2397",
        "Summary": "Parser to add paragraph line breaks",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.13",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "04/Jul/17 12:24",
        "Updated": "11/Sep/17 11:43",
        "Resolved": "11/Sep/17 10:59",
        "Description": "(initially reported with patch/pull-request by Vipul Behl, see #190)\nThe parser (parse-tika and parse-html) could be improved to add line breaks between paragraphs, instead of writing the whole document into a single line.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/198"
        ]
    },
    "NUTCH-2398": {
        "Key": "NUTCH-2398",
        "Summary": "Fetcher saving redirected robots.txt under redirect target URL",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "05/Jul/17 16:11",
        "Updated": "17/Jul/17 14:54",
        "Resolved": "17/Jul/17 14:31",
        "Description": "NUTCH-2300 lets the Fetcher store optionally the robots.txt response (content and HTTP status). If the '.../robots.txt' is redirected, the redirected content is also stored but with the redirect source URL as key. It should use the redirect target URL instead. Otherwise one of the responses is overwritten in the segments map file.",
        "Issue Links": []
    },
    "NUTCH-2399": {
        "Key": "NUTCH-2399",
        "Summary": "indexer-elastic does not index multi-value fields (only the first value is indexed)",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "09/Jul/17 11:52",
        "Updated": "06/Dec/17 12:51",
        "Resolved": "21/Aug/17 16:50",
        "Description": "Currently, if there is a NutchField with multiple values, only the first value is indexed (because this is what doc.getFieldValue returns). Pull request #200 checks if the NutchField has multiple values, and if so, they are added as an array (multivalue) field.\nhttps://github.com/apache/nutch/pull/200",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/200",
            "https://github.com/apache/nutch/pull/236"
        ]
    },
    "NUTCH-2400": {
        "Key": "NUTCH-2400",
        "Summary": "Solr 6.6.0 compatibility",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "indexer",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Jul/17 15:08",
        "Updated": "15/Aug/17 16:52",
        "Resolved": "15/Aug/17 15:51",
        "Description": "This issue relates to following mailing list thread http://www.mail-archive.com/user%40nutch.apache.org/msg15574.html\nThe schema.xml upgrade works with Solr 6.6.0, please try it out and let me know how things go.\nI've also updated the tutorial at https://wiki.apache.org/nutch/NutchTutorial so please check that out as well.",
        "Issue Links": []
    },
    "NUTCH-2401": {
        "Key": "NUTCH-2401",
        "Summary": "headings plugin does not trim values",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Thilo Haas",
        "Created": "19/Jul/17 10:00",
        "Updated": "19/Jul/17 13:24",
        "Resolved": "19/Jul/17 13:23",
        "Description": "The nutch headings plugin does trim the values but does not use the returned value of the trim function.",
        "Issue Links": []
    },
    "NUTCH-2402": {
        "Key": "NUTCH-2402",
        "Summary": "Fetcher variable missing for generate.max.count.expr and fetcher.server.delay.expr",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Jul/17 16:19",
        "Updated": "20/Jul/17 16:23",
        "Resolved": "20/Jul/17 16:23",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-2368"
        ]
    },
    "NUTCH-2403": {
        "Key": "NUTCH-2403",
        "Summary": "Nutch Selenium: Wrong documentation about PhantomJS",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "documentation,                                            plugin",
        "Assignee": "Moreno Feltscher",
        "Reporter": "Moreno Feltscher",
        "Created": "21/Jul/17 15:42",
        "Updated": "25/Jul/17 05:11",
        "Resolved": "24/Jul/17 20:55",
        "Description": "The Nutch Selenium documentation states that PhantomJS can be used as phantomJS for selenium.driver. The correct value would be phantomjs according to https://github.com/apache/nutch/blob/master/src/plugin/lib-selenium/src/java/org/apache/nutch/protocol/selenium/HttpWebClient.java#L124",
        "Issue Links": []
    },
    "NUTCH-2404": {
        "Key": "NUTCH-2404",
        "Summary": "Failed Jenkin Build #1588 error in unit test resolved",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.4",
        "Component/s": "test",
        "Assignee": "Kaidul Islam",
        "Reporter": "Kaidul Islam",
        "Created": "31/Jul/17 06:20",
        "Updated": "31/Jul/17 17:43",
        "Resolved": "31/Jul/17 16:52",
        "Description": "Fix for Jenkin Build #1588 after merging pull request #192 (NUTCH-2389).",
        "Issue Links": []
    },
    "NUTCH-2405": {
        "Key": "NUTCH-2405",
        "Summary": "jsoup-extractor structure correction, typo fixed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.4",
        "Component/s": "plugin",
        "Assignee": "Kaidul Islam",
        "Reporter": "Kaidul Islam",
        "Created": "06/Aug/17 09:08",
        "Updated": "09/Aug/17 17:43",
        "Resolved": "09/Aug/17 17:25",
        "Description": "Several bugs faced during testing with my project have been fixed\n1. Missed root tag <extractor> added in jsoup-extractor.xml like jsoup-extractor-example.xml\n2. jsoup API text() used instead of ownText() to get full contents under CSS selector\n3. <default> => <default-value> typo fixed",
        "Issue Links": []
    },
    "NUTCH-2406": {
        "Key": "NUTCH-2406",
        "Summary": "Sum up constants, make minor changes",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Kenneth McFarland",
        "Reporter": "Kenneth McFarland",
        "Created": "08/Aug/17 08:27",
        "Updated": "09/Aug/17 17:19",
        "Resolved": "09/Aug/17 17:19",
        "Description": "This change is very minor, spacing is added between imports to separate the logging imports from the other by a newline, minor English corrections, and the summation of integers that are for some reason in a strange form ( respectfully ).",
        "Issue Links": []
    },
    "NUTCH-2407": {
        "Key": "NUTCH-2407",
        "Summary": "Memory leak causing Nutch Server to run out of memory",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.3.1,                                            1.16",
        "Fix Version/s": "1.20",
        "Component/s": "nutch server",
        "Assignee": null,
        "Reporter": "Vyacheslav Pascarel",
        "Created": "11/Aug/17 21:19",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "My application is trying to perform continuous crawling using Nutch REST services. The application injects a seed URL and then repeats GENERATE/FETCH/PARSE/UPDATEDB sequence requested number of times (each step in the sequence is executed upon successful competition of the previous step then the whole sequence is repeated again). Here is a brief description of the job:\n\nNumber of GENERATE/FETCH/PARSE/UPDATEDB cycles per run: 50\n'topN' parameter value of GENERATE step in each cycle: 10\nSeed URL: http://www.cnn.com\nRegex URL filters for all jobs:\n\t\n\"-^.{1000,}$\" - exclude very long URLs\n\"+.\" - include the rest\n\n\n\nTo monitor Nutch server I use Java VisualVM that comes with Java SDK. After each run (50 cycles of GENERATE/FETCH/PARSE/UPDATEDB) I perform garbage collection using the mentioned tool and check memory usage. My observation is that Nutch Server leaks ~25MB per run.\nNOTES: I added custom HTTP DELETE services to clean job history in NutchServerPoolExecutor and remove all custom configurations from RAMConfManager after each run. So observed ~25MB memory leak is after job history/configuration cleanup.",
        "Issue Links": [
            "/jira/browse/NUTCH-1746"
        ]
    },
    "NUTCH-2408": {
        "Key": "NUTCH-2408",
        "Summary": "CrawlDb: allow update from unparsed segments",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "12/Aug/17 14:15",
        "Updated": "15/Aug/17 12:20",
        "Resolved": "15/Aug/17 12:20",
        "Description": "The command updatedb (class o.a.n.crawl.CrawlDb) does not allow to update the CrawlDb with fetch status only (from segment subdirectory crawl_fetch) without also reading crawl_parse (which contains outlinks but also scores, signatures and meta data). \nA workflow which does not require parsing of documents (e.g., because raw HTML content is exported to WARC files) is then unable to update the CrawlDb to store the fetch status.",
        "Issue Links": []
    },
    "NUTCH-2409": {
        "Key": "NUTCH-2409",
        "Summary": "Injector: complete command-line help and counters",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "injector",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Aug/17 10:52",
        "Updated": "11/Sep/17 11:59",
        "Resolved": "11/Sep/17 10:55",
        "Description": "See discussion in NUTCH-2335:\n\nadd counters for removed items from CrawlDb:\n\nInjector: Total urls removed from CrawlDb by filters: 2\nInjector: Total urls with status gone removed from CrawlDb (db.update.purge.404): 0\n\n\nadd -Ddb.update.purge.404=true to command-line help:\n\nUsage: Injector [-D...] <crawldb> <url_dir> [-overwrite|-update] [-noFilter] [-noNormalize] [-filterNormalizeAll]\n...\n -D...          set or overwrite configuration property (property=value)\n -Ddb.update.purge.404=true\n                remove URLs with status gone (404) from CrawlDb",
        "Issue Links": [
            "/jira/browse/NUTCH-2335",
            "https://github.com/apache/nutch/pull/215"
        ]
    },
    "NUTCH-2410": {
        "Key": "NUTCH-2410",
        "Summary": "Unit test for jsoup-extractor not to depend on external resources",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Aug/17 11:06",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The unit test of the plugin jsoup-extractor depends on a document hosted on youtube.com:\n\n  private static final String SAMPLE_URL = \"https://www.youtube.com/watch?v=pzMpwW4ppRM\";\n\n\nIt's nice to test the plugin on real-world example. However, the tests will fail without internet connection or unexpectedly when youtube changes the HTML layout.",
        "Issue Links": []
    },
    "NUTCH-2411": {
        "Key": "NUTCH-2411",
        "Summary": "Index-metadata to support indexing multiple values for a field",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "22/Aug/17 14:26",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "08/Mar/18 13:04",
        "Description": "<property>\n  <name>index.metadata.separator</name>\n  <value></value>\n  <description>\n   Separator to use if you want to index multiple values for a given field. Leave empty to\n   treat each value as a single value.\n  </description>\n</property>\n\n<property>\n  <name>index.metadata.multivalued.fields</name>\n  <value></value>\n  <description>\n    Comma separated list of fields that are multi valued.\n  </description>\n</property>",
        "Issue Links": []
    },
    "NUTCH-2412": {
        "Key": "NUTCH-2412",
        "Summary": "Exchange component for indexing job",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "24/Aug/17 13:25",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "28/Jun/18 07:47",
        "Description": "The exchange component acts in indexing job and decides which index writer a document should go to. It includes an extension point to allow developers to develop plugins with their own logic.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/340"
        ]
    },
    "NUTCH-2413": {
        "Key": "NUTCH-2413",
        "Summary": "Parsing fetcher to respect property \"parse.filter.urls\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "fetcher,                                            parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Marcos Bori",
        "Created": "25/Aug/17 12:15",
        "Updated": "26/Aug/17 09:53",
        "Resolved": "26/Aug/17 08:49",
        "Description": "In a situation when we want to:\n(1) Execute the fetch and parse together (\"fetcher.parse\" setting to \"true\")\n(2) Avoid applying the URL filters when executing this phase.\nCondition (2) can be configured when parsing is executed as a separate process by setting \"parse.filter.urls\" to \"false\".\nHowever, this setting (\"parse.filter.urls\") is ignored when we execute the fetch and parse phases together.",
        "Issue Links": []
    },
    "NUTCH-2414": {
        "Key": "NUTCH-2414",
        "Summary": "Allow LanguageIndexingFilter to actually filter documents by language.",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Yossi Tamari",
        "Created": "28/Aug/17 12:48",
        "Updated": "13/Dec/17 20:54",
        "Resolved": "13/Dec/17 20:54",
        "Description": "It is often useful to only index pages in select languages (e.g. only those languages that we intend to search in). At first glance it seems that this is done by LanguageIndexingFilter, but currently all the filter does is add the language as a field to the index.\nWe can add a configuration property to LanguageIndexingFilter that will allow it to only index languages specified in this property.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/217"
        ]
    },
    "NUTCH-2415": {
        "Key": "NUTCH-2415",
        "Summary": "Create a JEXL based IndexingFilter",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Yossi Tamari",
        "Created": "29/Aug/17 16:13",
        "Updated": "18/Dec/17 15:51",
        "Resolved": "18/Dec/17 15:51",
        "Description": "Following on NUTCH-2414 and NUTCH-2412, the requirement was raised for a IndexingFilter plugin which will decide whether to index a document based on a JEXL expression.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/219"
        ]
    },
    "NUTCH-2416": {
        "Key": "NUTCH-2416",
        "Summary": "Fetcher to log thread ID",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "fetcher",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "30/Aug/17 08:43",
        "Updated": "06/Jun/18 09:30",
        "Resolved": "06/Jun/18 08:39",
        "Description": "Better logging for the fetcher.",
        "Issue Links": []
    },
    "NUTCH-2417": {
        "Key": "NUTCH-2417",
        "Summary": "Support for variable fetch delay via FreeGenerator",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.20",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "30/Aug/17 11:57",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Same as NUTCH-2368",
        "Issue Links": []
    },
    "NUTCH-2418": {
        "Key": "NUTCH-2418",
        "Summary": "NPE in org.apache.hadoop.io.Text from FetcherThread",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.13",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "05/Sep/17 15:41",
        "Updated": "28/Sep/17 12:20",
        "Resolved": "28/Sep/17 12:19",
        "Description": "2017-09-05 15:28:54,539 INFO [FetcherThread] org.apache.nutch.fetcher.FetcherThread: FetcherThread 38 fetch of https://www.provinciegroningen.nl/fileadmin/user_upload/Documenten/Downloads/vanturfvntoervfol.pdf failed with: java.lang.NullPointerException\n\tat org.apache.hadoop.io.Text.encode(Text.java:450)\n\tat org.apache.hadoop.io.Text.encode(Text.java:431)\n\tat org.apache.hadoop.io.Text.writeString(Text.java:480)\n\tat org.apache.nutch.parse.ParseData.write(ParseData.java:168)\n\tat org.apache.nutch.parse.ParseImpl.write(ParseImpl.java:69)\n\tat org.apache.hadoop.io.GenericWritable.write(GenericWritable.java:142)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1157)\n\tat org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:610)\n\tat org.apache.nutch.fetcher.FetcherThread.output(FetcherThread.java:773)\n\tat org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:360)\n\n\nNever seen it before, no idea what's going on. Opening issue to track it.\nMore found: lots of fetches of this website throw this NPE:\n\n2017-09-25 13:55:08,103 INFO [FetcherThread] org.apache.nutch.fetcher.FetcherThread: FetcherThread 37 fetch of http://www.jabra.com.mx/c/fr/speak510-offert failed with: java.lang.NullPointerException\n\tat org.apache.hadoop.io.Text.encode(Text.java:450)\n\tat org.apache.hadoop.io.Text.encode(Text.java:431)\n\tat org.apache.hadoop.io.Text.writeString(Text.java:480)\n\tat org.apache.nutch.parse.ParseData.write(ParseData.java:168)\n\tat org.apache.nutch.parse.ParseImpl.write(ParseImpl.java:69)\n\tat org.apache.hadoop.io.GenericWritable.write(GenericWritable.java:142)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:98)\n\tat org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:82)\n\tat org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1157)\n\tat org.apache.hadoop.mapred.MapTask$OldOutputCollector.collect(MapTask.java:610)\n\tat org.apache.nutch.fetcher.FetcherThread.output(FetcherThread.java:773)\n\tat org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:360)",
        "Issue Links": []
    },
    "NUTCH-2419": {
        "Key": "NUTCH-2419",
        "Summary": "Some URL filters and normalizers do not respect command-line override for rule file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.17",
        "Component/s": "plugin,                                            urlfilter,                                            urlnormalizer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "06/Sep/17 10:06",
        "Updated": "09/Dec/22 14:49",
        "Resolved": "14/May/20 15:43",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-1691",
            "https://github.com/apache/nutch/pull/526"
        ]
    },
    "NUTCH-2420": {
        "Key": "NUTCH-2420",
        "Summary": "Bug in variable generate.max.count and fetcher.server.delay",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Sep/17 10:30",
        "Updated": "06/Nov/17 18:21",
        "Resolved": "06/Nov/17 16:09",
        "Description": "Feature added by NUTCH-2368 does not work for multiple hosts. Once a HostDatum has been read by getHostDatum(), the next host cannot be read. Apparantly i need to open and close the SequenceFile.Readers for every HostDatum it needs. Reader has no reset() method or whatsoever.",
        "Issue Links": []
    },
    "NUTCH-2421": {
        "Key": "NUTCH-2421",
        "Summary": "parse-html to prioritize HTML5 charset definitions",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Laurent Hervaud",
        "Created": "19/Sep/17 10:53",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "jira NUTCH-1733 add support to HTML5 charset definitions.\nIn some case web site declare multiple meta element with different charset :\n    <meta charset=\"utf-8\">\n    <meta http-equiv=\"Content-Type\" content=\"text/html; charset=iso-8859-1\"> \n(ex : http://www.edga.fr/)\nIn this case the second charset is detected (iso-8859-1).\nWhat about prioritize HTML5 charset definitions first ?",
        "Issue Links": []
    },
    "NUTCH-2422": {
        "Key": "NUTCH-2422",
        "Summary": "Update information about git repository",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Karl-Philipp Richter",
        "Created": "19/Sep/17 16:32",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "06/Nov/17 22:10",
        "Description": "https://github.com/apache/nutch claims to be a mirror of git://git.apache.org/nutch.git, but cloning fails due to `fatal: remote error: access denied or repository not exported: /nutch.git` and nutch isn't listed on https://git.apache.org/. That status isn't consistent.",
        "Issue Links": []
    },
    "NUTCH-2423": {
        "Key": "NUTCH-2423",
        "Summary": "Update contributor info page",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "documentation,                                            wiki",
        "Assignee": null,
        "Reporter": "Karl-Philipp Richter",
        "Created": "19/Sep/17 17:35",
        "Updated": "19/Aug/22 13:38",
        "Resolved": null,
        "Description": "The [contributor info page](https://wiki.apache.org/nutch/Becoming_A_Nutch_Developer) still mentions subversion as SCM which I assume is obsolete because there's git://git.apache.org/nutch.git. It should mention how the devs with write access deal with pull/merge requests in general or on different popular platforms (the information that they're not accepted is valuable as well).",
        "Issue Links": []
    },
    "NUTCH-2424": {
        "Key": "NUTCH-2424",
        "Summary": "Mirror git repository to gitlab.com",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Karl-Philipp Richter",
        "Created": "19/Sep/17 17:41",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "08/Jun/18 14:47",
        "Description": "GitLab is a free (as in speech) code hosting platform which has a continuous integration service and provides merge requests. An instance is provided at gitlab.com or a project can host its own instance.\nThe long term goal is to get a CI service working for Nutch. I already provided a `.gitlab-ci.yml` at https://gitlab.com/krichter/nutch/merge_requests/1 which reveals failure of crucial build commands, so this is definitely useful and necessary (and at https://gitlab.com/krichter/nutch/merge_requests/2 for Maven-based builds based on branch NUTCH-2292 and the corresponding issue).",
        "Issue Links": []
    },
    "NUTCH-2425": {
        "Key": "NUTCH-2425",
        "Summary": "Update GettingNutchRunningWithUbuntu wiki article",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation,                                            wiki",
        "Assignee": null,
        "Reporter": "Karl-Philipp Richter",
        "Created": "19/Sep/17 18:33",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "30/Apr/20 11:50",
        "Description": "https://wiki.apache.org/nutch/GettingNutchRunningWithUbuntu contains some errors (e.g. `echo 'http://lucene.apache.org/nutch/' > urls` where `urls` is a directory) and obsolete parts (`conf/crawl-urlfilter.txt` is `conf/regex-urlfilter.txt` in 2.x) and thus appear to be tested well.",
        "Issue Links": []
    },
    "NUTCH-2426": {
        "Key": "NUTCH-2426",
        "Summary": "Provide reason for job failure in job overview",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Karl-Philipp Richter",
        "Created": "19/Sep/17 20:32",
        "Updated": "15/Nov/19 10:51",
        "Resolved": "15/Nov/19 10:51",
        "Description": "If a crawl job fails and the status changes from `Crawling` to `Error` the user has no idea what the reason for the failure is or where to get this information.",
        "Issue Links": []
    },
    "NUTCH-2427": {
        "Key": "NUTCH-2427",
        "Summary": "Remove all the Hadoop wildcard imports.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Omkar Reddy",
        "Created": "20/Sep/17 09:06",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "13/Mar/18 23:03",
        "Description": "This improvement deals with removing the wildcard imports like \"import org.apache.hadoop.package.* \"",
        "Issue Links": [
            "/jira/browse/NUTCH-2516"
        ]
    },
    "NUTCH-2428": {
        "Key": "NUTCH-2428",
        "Summary": "Provide binary release for Nutch",
        "Type": "Wish",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3",
        "Fix Version/s": "2.5",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Karl-Philipp Richter",
        "Created": "20/Sep/17 09:29",
        "Updated": "15/Nov/19 10:50",
        "Resolved": "15/Nov/19 10:50",
        "Description": "I've been trying to setup a Nutch instance several times over the years, but never managed because it's so hard to configure the instance based on some parts of the 1.x tutorial and some parts of the 2.x tutorial which I had to figure out in trial and error mostly (and 2^# of configuration steps is a time consuming list of options to try). I already torpedoed the JIRA instance with a lot of issues, but now that I'm taking another approach, I don't think I can report them all and you might not want that.\nI'd be nice to have a binary release of Nutch which works with a default non-production configuration which can be changed via the web UI since editing configuration files is just overly painful and makes the software appear in some pre-alpha state in which it is not (download of binary archive, extracting, `bin/nutch start-web` and opening localhost:8080 should get you started).\nNutch is important and very smart software. I'd be nice if it was so hard to use for no apparent reason (except devs resources of course).\nhttps://issues.apache.org/jira/browse/NUTCH-728 covers releases, but it's been closed and there's no binary release available for download. Debian packages as suggested in https://issues.apache.org/jira/browse/NUTCH-1285 don't make sense because they're not very flexible; they should be low-priority. Things might be clearer after [migration to Maven](https://issues.apache.org/jira/browse/NUTCH-2292) is done.",
        "Issue Links": []
    },
    "NUTCH-2429": {
        "Key": "NUTCH-2429",
        "Summary": "Fix Plugin System to allow protocol plugins to bundle their URLStreamHandlers",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.19",
        "Component/s": "commoncrawl",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Hiran Chaudhuri",
        "Created": "22/Sep/17 10:22",
        "Updated": "19/May/22 09:21",
        "Resolved": "08/Jan/22 04:08",
        "Description": "While trying to use the protocol-smb plugin (which is not part of the Nutch distribution) I realized there are four steps to successfully make use of a protocol plugin:\n1 - put the artifact into the plugins directory\n2 - modify Nutch configuration files to allow smb:// urls plus include the plugin to the loaded list\n3 - extract jcifs.jar and place it on the system classpath\n4 - run nutch with the correct system property\nWhile steps 1 and 2 seem obvious, 3 and 4 require knowledge of plugin internals which does not feel right for nutch and plugin users. Even more, the jcifs.jar would exist twice on the classpath and could even cause further problems during runtime.",
        "Issue Links": [
            "/jira/browse/NUTCH-2936",
            "/jira/browse/NUTCH-2949",
            "/jira/browse/NUTCH-714",
            "/jira/browse/NUTCH-427",
            "https://github.com/apache/nutch/pull/222",
            "https://github.com/apache/nutch/pull/720",
            "https://lists.apache.org/thread.html/1ce4f5caa97d34dc8811433ed5cbc7e14016f7c93373292a4a62892e@%3Cuser.nutch.apache.org%3E"
        ]
    },
    "NUTCH-2430": {
        "Key": "NUTCH-2430",
        "Summary": "Complete plugin build configuration",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Sep/17 10:54",
        "Updated": "25/Sep/17 16:19",
        "Resolved": "25/Sep/17 14:57",
        "Description": "The build configuration around plugins isn't complete\n\nmissing plugin folders in the Eclipse target (see NUTCH-2135)\nnot all plugins included API docs / javadoc",
        "Issue Links": [
            "/jira/browse/NUTCH-2135",
            "https://github.com/apache/nutch/pull/223"
        ]
    },
    "NUTCH-2431": {
        "Key": "NUTCH-2431",
        "Summary": "URLFilterchecker to implement Tool-interface",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "25/Sep/17 10:07",
        "Updated": "18/Dec/17 13:05",
        "Resolved": "18/Dec/17 13:05",
        "Description": "The current implementation of the URLFilterChecker does not allow for commandline config overrides. It needs to implement the Tool interface for this. \nPlease see the attached patch",
        "Issue Links": [
            "/jira/browse/NUTCH-2477",
            "/jira/browse/NUTCH-2477"
        ]
    },
    "NUTCH-2432": {
        "Key": "NUTCH-2432",
        "Summary": "Protocol httpclient to disable cookies if http.enable.cookie.header is false",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "25/Sep/17 10:55",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "02/Jul/18 14:55",
        "Description": null,
        "Issue Links": [
            "/jira/browse/NUTCH-2355"
        ]
    },
    "NUTCH-2433": {
        "Key": "NUTCH-2433",
        "Summary": "Html Parser: keep htmltag where the outlinks are found",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Marcos Bori",
        "Created": "26/Sep/17 15:09",
        "Updated": "29/Sep/17 12:59",
        "Resolved": "29/Sep/17 11:49",
        "Description": "When parsing HTML pages, I need to know in which HTML tag the outlinks were found (for example, 'a', 'script', 'img', etc).\nI propose to add a new configuration value, \"parser.html.outlinks.htmlnode_metadata_name\".\nIf this configuration property is not empty, all found outlinks will be assigned a metadata with the name indicated in this configuration property with the html tag name where the outlink was found.\nI will now send the pull request with my code implementation.",
        "Issue Links": []
    },
    "NUTCH-2434": {
        "Key": "NUTCH-2434",
        "Summary": "Add methods to reset parameters HTMLMetaTags",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.17",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "27/Sep/17 09:51",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "05/May/20 09:31",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2435": {
        "Key": "NUTCH-2435",
        "Summary": "New configuration allowing to choose whether to store 'parse_text' directory or not.",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Marcos Bori",
        "Created": "27/Sep/17 11:05",
        "Updated": "19/Oct/17 21:29",
        "Resolved": "19/Oct/17 21:29",
        "Description": "Whenever a page is parsed, one of the outputs is the directory 'parse_text'.\nIt is intended to be used at the indexing phase so the page can be searched from a search engine such as Solr.\nIn my special crawling case, I don't need to index the page contents. Therefore, creating and filing the 'parse_text' is not required for me. To optimize performance, I don't want the crawler to store this information to the filesystem. \nI propose a new parameter \"parser.store.text\" allowing to choose whether to store 'parse_text' directory or not. Its default value, of course, is \"true\".",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/225"
        ]
    },
    "NUTCH-2436": {
        "Key": "NUTCH-2436",
        "Summary": "Remove empty comment, and redundant semicolon from CommandRunner",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Kenneth McFarland",
        "Reporter": "Kenneth McFarland",
        "Created": "27/Sep/17 21:47",
        "Updated": "28/Sep/17 20:43",
        "Resolved": "28/Sep/17 18:54",
        "Description": "CommandRunner has a set of empty comments and a redundant semicolon.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/226",
            "https://github.com/apache/nutch/pull/227"
        ]
    },
    "NUTCH-2437": {
        "Key": "NUTCH-2437",
        "Summary": "gora mongodb mapping file error",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.4",
        "Component/s": "storage",
        "Assignee": null,
        "Reporter": "Tulay Muezzinoglu",
        "Created": "04/Oct/17 02:41",
        "Updated": "04/Oct/17 18:44",
        "Resolved": "04/Oct/17 17:43",
        "Description": "conf/gora-mongodb-mapping.xml\n\n <field name=\"stmPriority\" family=\"stmPriority\" type=\"int32\"/>\n\n\nshould be\n\n \n <field name=\"stmPriority\" docfield=\"stmPriority\" type=\"int32\"/>\n\n\n Otherwise it is throwing exception.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/228"
        ]
    },
    "NUTCH-2438": {
        "Key": "NUTCH-2438",
        "Summary": "Upgrade Nutch 2.X to Gora 0.8",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.4",
        "Component/s": "storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Tulay Muezzinoglu",
        "Created": "04/Oct/17 19:22",
        "Updated": "13/Dec/17 21:46",
        "Resolved": "13/Dec/17 20:47",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/229",
            "https://github.com/apache/nutch/pull/258"
        ]
    },
    "NUTCH-2439": {
        "Key": "NUTCH-2439",
        "Summary": "Upgrade to Apache Tika 1.17",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "11/Oct/17 13:34",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "15/Dec/17 17:18",
        "Description": null,
        "Issue Links": [
            "/jira/browse/TIKA-2490",
            "https://github.com/apache/nutch/pull/259"
        ]
    },
    "NUTCH-2440": {
        "Key": "NUTCH-2440",
        "Summary": "DbResource does not accept crawlid",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3,                                            2.4",
        "Fix Version/s": "2.5",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Tulay Muezzinoglu",
        "Created": "12/Oct/17 00:12",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "DbResource is initiating DbReaders with null crawlids. This blocks querying correct table/collection if crawlid is set during fetch. \nFor example in mongodb, by default all data is stored in \"webpage\" collection. Let say you set crawlid as \"tech\" for fetch, then all data gets stored in \"tech_webpage\" collection. But during rest call to /db end point, since you cannot specify crawlid, it will query \"webpage\" collection.\nI am thinking either DBFilter can be changed to read in crawlid, or resource path can include crawlid. I am open to suggestions and then can make PR.",
        "Issue Links": []
    },
    "NUTCH-2441": {
        "Key": "NUTCH-2441",
        "Summary": "ARG_SEGMENT usage",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "metadata",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "16/Oct/17 14:31",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "18/Jan/18 17:51",
        "Description": "The class metadata/Nutch.java  public static final String ARG_SEGMENT = \"segment\" is not used correctly. In some cases Fetcher and ParseSegment it is interpreted as a single segmenet, in others CrawlDb, LinkDb, IndexingJob as an array of segments. Such misunderstanding leads to inconsistency of usage of the parameter.\nAfter a discussion with wastl-nagel  the proposed solution is to allow the usage of both array and a string in all cases. That gives an opportunity to not introduce the broken changes.\nA path is proposed.\n *The question left is refactoring, all these five components share the same code(two versions of the same code to be precise). Shouldn't we extract a method and reduce duplicates?  *",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/250"
        ]
    },
    "NUTCH-2442": {
        "Key": "NUTCH-2442",
        "Summary": "Injector to stop if job fails to avoid loss of CrawlDb",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "injector",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Oct/17 09:01",
        "Updated": "07/Nov/17 06:45",
        "Resolved": "06/Nov/17 21:43",
        "Description": "Injector does not check whether the MapReduce job is successful. Even if the job fails\n\ninstalls the CrawlDb\n\t\nmove current/ to old/\nreplace current/ with an empty or potentially incomplete version\n\n\nexits with code 0 so that scripts running the crawl workflow cannot detect the failure \u2013 if Injector is run a second time the CrawlDb is lost (both current/ and old/ are empty or corrupted)",
        "Issue Links": [
            "/jira/browse/NUTCH-2076",
            "https://github.com/apache/nutch/pull/239"
        ]
    },
    "NUTCH-2443": {
        "Key": "NUTCH-2443",
        "Summary": "Extract links from the video tag with the parse-html plugin",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "17/Oct/17 10:59",
        "Updated": "05/Nov/17 21:51",
        "Resolved": "05/Nov/17 21:04",
        "Description": "At the moment the parse-html extracts links from the tags a, area, form (configurable), frame, iframe, script, link, img. Since we allow extracting links to binary files (images) extracting links also from the video tag should be supported.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/230"
        ]
    },
    "NUTCH-2444": {
        "Key": "NUTCH-2444",
        "Summary": "HostDB CSV dumper to emit field header by default",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "hostdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Oct/17 09:07",
        "Updated": "23/Oct/17 13:50",
        "Resolved": "23/Oct/17 13:22",
        "Description": "Started to get annoyed by constantly having to look-u HostDatum for the field set.",
        "Issue Links": []
    },
    "NUTCH-2445": {
        "Key": "NUTCH-2445",
        "Summary": "Fetcher following outlinks to keep track of already fetched items",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Oct/17 12:42",
        "Updated": "23/Oct/17 14:54",
        "Resolved": "23/Oct/17 13:59",
        "Description": "When fetcher.follow.outlinks.depth is non-zero, fetcher follows outlinks. This patch keeps track of already fetched URL's and thus avoid fetching the same URL twice.\nA Set is used to keep track of them, hashcodes to reduce memory usage. This is not used if fetcher doesn't follow outlinks.",
        "Issue Links": []
    },
    "NUTCH-2446": {
        "Key": "NUTCH-2446",
        "Summary": "URLFiltersCheck fix",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "None",
        "Assignee": "Kenneth McFarland",
        "Reporter": "Kenneth McFarland",
        "Created": "23/Oct/17 07:22",
        "Updated": "23/Oct/17 08:52",
        "Resolved": "23/Oct/17 08:10",
        "Description": "Currently URLFilterChecker.checkAll() creates a URLFilters object repeatedly when conf does not change.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/231"
        ]
    },
    "NUTCH-2447": {
        "Key": "NUTCH-2447",
        "Summary": "Work-around SSLProtocolException: handshake alert: unrecognized_name",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "protocol",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Oct/17 11:34",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "27/Mar/18 15:50",
        "Description": "Nutch is unable to crawl some websites, regardless of protocol plugin you are using. The work-around you frequently find (-Djsse.enableSNIExtension=false) does not work at all, so the internet is clearly lying to us!\n\n\r\n2017-10-23 12:43:52,911 INFO  api.HttpRobotRulesParser - Couldn't get robots.txt for https://www.eidsiva.net/: javax.net.ssl.SSLProtocolException: handshake alert:  unrecognized_name\r\n2017-10-23 12:43:53,011 ERROR http.Http - Failed to get protocol output\r\njavax.net.ssl.SSLProtocolException: handshake alert:  unrecognized_name\r\n        at sun.security.ssl.ClientHandshaker.handshakeAlert(ClientHandshaker.java:1446)\r\n        at sun.security.ssl.SSLSocketImpl.recvAlert(SSLSocketImpl.java:2016)\r\n        at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1125)\r\n        at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1375)\r\n        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1403)\r\n        at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1387)\r\n        at org.apache.nutch.protocol.http.HttpResponse.<init>(HttpResponse.java:152)\r\n        at org.apache.nutch.protocol.http.Http.getResponse(Http.java:72)\r\n        at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:271)\r\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:327)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/305"
        ]
    },
    "NUTCH-2448": {
        "Key": "NUTCH-2448",
        "Summary": "Allow Sending an empty http.agent.version",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "fetcher,                                            protocol",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "23/Oct/17 14:25",
        "Updated": "24/Oct/17 19:54",
        "Resolved": "24/Oct/17 19:50",
        "Description": "http.agent.version defaults in nutch-default.xml to Nutch-1.14-SNAPSHOT (depending on the version of course).\nIf I want to override it to not send a version as part of the user-agent, there is nothing I can do in nutch-site.xml, since putting an empty string there causes the default to be taken, and putting any value there causes a slash to be appended to the http.agent.name.\nAs far as I can see, the only way to override it is to remove the value in nutch-default.xml, which is probably not the \u201ccorrect\u201d way, considering it contains a comment saying \u201cDo not modify this file directly\u201d.\nThe suggested solution is to treat a white-space-only value as empty.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/232"
        ]
    },
    "NUTCH-2449": {
        "Key": "NUTCH-2449",
        "Summary": "Usage of Tika LanguageIdentifier in language-identifier plugin",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.19",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "24/Oct/17 14:29",
        "Updated": "21/Dec/21 09:19",
        "Resolved": "18/Dec/21 04:11",
        "Description": "The language-identifier plugin uses org.apache.tika.language.LanguageIdentifier for extracting the language from the document text. There are two issues with that:\n\nLanguageIdentifier is deprecated in Tika.\nIt does not support CJK language (and I suspect a lot of other languages - https://wiki.apache.org/nutch/LanguageIdentifierPlugin#Implemented_Languages_and_their_ISO_636_Codes), and it doesn\u2019t even fail gracefully with them - in my experience Chinese was recognized as Italian.",
        "Issue Links": [
            "/jira/browse/NUTCH-2891",
            "/jira/browse/NUTCH-2278",
            "https://lists.apache.org/thread.html/0d5b77ef8070f24237abbb2d72cc52d9d33c66a4635fb2db22d8074d@%3Cuser.nutch.apache.org%3E",
            "https://github.com/apache/nutch/pull/233",
            "https://github.com/apache/nutch/pull/716"
        ]
    },
    "NUTCH-2450": {
        "Key": "NUTCH-2450",
        "Summary": "Remove FixMe in ParseOutputFormat",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Kenneth McFarland",
        "Reporter": "Kenneth McFarland",
        "Created": "24/Oct/17 21:54",
        "Updated": "19/Dec/17 04:08",
        "Resolved": "19/Dec/17 04:03",
        "Description": "ParseOutputFormat contains a few FixMe's that I've looked at. If a valid url is created, it will always return valid results. There is a spot in the code where the try catch is already done, so the predicate is satisfied and there is no need to keep checking it.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/235"
        ]
    },
    "NUTCH-2451": {
        "Key": "NUTCH-2451",
        "Summary": "protocol-ftp to resolve relative URL when following redirects",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Hiran Chaudhuri",
        "Created": "25/Oct/17 20:38",
        "Updated": "05/Dec/17 12:01",
        "Resolved": "05/Dec/17 11:12",
        "Description": "I tried running Nutch on my Synology NAS. As SMB protocol is not contained in Nutch, I turned on FTP service on the NAS and configured Nutch to crawl ftp://nas.\nThe experience gives me varying results which seem to point to problems within Nutch. However this may need further evaluation.\nAs some files could not be downloaded and I could not see a good error message I changed the method org.apache.nutch.protocol.ftp.FTP.getProtocolOutput(Text, CrawlDatum) to not only return protocol status but send the full exception and stack trace to the logs:\n{{    } catch (Exception e) {\n    \tLOG.warn(\"Could not get {}\", url, e);\n      return new ProtocolOutput(null, new ProtocolStatus(e));\n    }\n}}\nWith this modification I suddenly see such messages in the logfile:\n{{2017-10-25 22:09:31,865 TRACE org.apache.nutch.protocol.ftp.Ftp - fetching ftp://nas/MediaPC/usr/lib32/gconv/ARMSCII-8.so\n2017-10-25 22:09:32,147 WARN  org.apache.nutch.protocol.ftp.Ftp - Could not get ftp://nas/MediaPC/usr/lib32/gconv/ARMSCII-8.so\njava.net.MalformedURLException\n\tat java.net.URL.<init>(URL.java:627)\n\tat java.net.URL.<init>(URL.java:490)\n\tat java.net.URL.<init>(URL.java:439)\n\tat org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:145)\n\tat org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\nCaused by: java.lang.NullPointerException\n}}\nPlease mind the URL was not configured from me. Instead it was obtained by crawling my NAS. Also the URL looks perfectly fine to me. Even if the file did not exist I would not expect a MalformedURLException to occur. Even more, using Firefox and the same authentication data on the same URL retrieves the file successfully.\nHow come Nutch cannot get the file?",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/241"
        ]
    },
    "NUTCH-2452": {
        "Key": "NUTCH-2452",
        "Summary": "Problem retrieving encoded URLs via FTP?",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Hiran Chaudhuri",
        "Created": "25/Oct/17 20:46",
        "Updated": "11/Nov/17 23:27",
        "Resolved": "05/Nov/17 21:24",
        "Description": "I tried running Nutch on my Synology NAS. As SMB protocol is not contained in Nutch, I turned on FTP service on the NAS and configured Nutch to crawl ftp://nas.\nThe experience gives me varying results which seem to point to problems within Nutch. However this may need further evaluation.\nAs some files could not be downloaded and I could not see a good error message I changed the method org.apache.nutch.protocol.ftp.FTP.getProtocolOutput(Text, CrawlDatum) to not only return protocol status but send the full exception and stack trace to the logs:\n{{ } catch (Exception e) {\nLOG.warn(\"Could not get {}\", url, e);\nreturn new ProtocolOutput(null, new ProtocolStatus(e));\n}\n}}\nWith this modification I suddenly see such messages in the logfile:\n{{2017-10-25 14:14:37,254 TRACE org.apache.nutch.protocol.ftp.Ftp - fetching ftp://nas/silver-sda2/home/vivi/Desktop/Pictures/Kenya%20Pics/\n2017-10-25 14:14:37,512 WARN  org.apache.nutch.protocol.ftp.Ftp - Could not get ftp://nas/silver-sda2/home/vivi/Desktop/Pictures/Kenya%20Pics/\norg.apache.nutch.protocol.ftp.FtpError: Ftp Error: 404\n        at org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:151)\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\n}}\nPlease mind the URL was not configured from me. Instead it was obtained by crawling my NAS. Also the URL looks perfectly fine to me. Even more, using Firefox and the same authentication data on the same URL displays the directory successfully. Therefore I suspect the FTP client is unable to decode the URL such that the FTP server would understand it.",
        "Issue Links": []
    },
    "NUTCH-2453": {
        "Key": "NUTCH-2453",
        "Summary": "FTP protocol seems to have issues running multithreaded",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.20",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Hiran Chaudhuri",
        "Created": "25/Oct/17 20:58",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "I tried running Nutch on my Synology NAS. As SMB protocol is not contained in Nutch, I turned on FTP service on the NAS and configured Nutch to crawl ftp://nas. Also I wanted to increase crawl speed and thus configured fetcher.threads.per.queue=10 in nutch-site.xml.\nAs some files could not be downloaded and I could not see a good error message I changed the method org.apache.nutch.protocol.ftp.FTP.getProtocolOutput(Text, CrawlDatum) to not only return protocol status but send the full exception and stack trace to the logs:\n{{ } catch (Exception e) {\nLOG.warn(\"Could not get {}\", url, e);\nreturn new ProtocolOutput(null, new ProtocolStatus(e));\n}\n}}\nWith this setup I saw such messages in the logs:\n{{2017-10-25 22:52:54,699 WARN  org.apache.nutch.protocol.ftp.Ftp - ftp.client.login() failed: nas/192.168.178.43\n2017-10-25 22:52:54,718 WARN  org.apache.nutch.protocol.ftp.Ftp - Error:\njava.net.SocketException: Socket closed\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n        at java.net.SocketInputStream.read(SocketInputStream.java:171)\n        at java.net.SocketInputStream.read(SocketInputStream.java:141)\n        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n        at java.io.InputStreamReader.read(InputStreamReader.java:184)\n        at java.io.BufferedReader.fill(BufferedReader.java:161)\n        at java.io.BufferedReader.read(BufferedReader.java:182)\n        at org.apache.commons.net.io.CRLFLineReader.readLine(CRLFLineReader.java:58)\n        at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:310)\n        at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:290)\n        at org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:479)\n        at org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:552)\n        at org.apache.commons.net.ftp.FTP.user(FTP.java:698)\n        at org.apache.nutch.protocol.ftp.Client.login(Client.java:294)\n        at org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:190)\n        at org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:132)\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\n2017-10-25 22:52:54,721 WARN  org.apache.nutch.protocol.ftp.Ftp - Could not get ftp://nas/silver-sda2/home/hiran/Desktop/Segelclub.txt~\norg.apache.nutch.protocol.ftp.FtpException: java.net.SocketException: Socket closed\n        at org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:308)\n        at org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:132)\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\nCaused by: java.net.SocketException: Socket closed\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n        at java.net.SocketInputStream.read(SocketInputStream.java:171)\n        at java.net.SocketInputStream.read(SocketInputStream.java:141)\n        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n        at java.io.InputStreamReader.read(InputStreamReader.java:184)\n        at java.io.BufferedReader.fill(BufferedReader.java:161)\n        at java.io.BufferedReader.read(BufferedReader.java:182)\n        at org.apache.commons.net.io.CRLFLineReader.readLine(CRLFLineReader.java:58)\n        at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:310)\n        at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:290)\n        at org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:479)\n        at org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:552)\n        at org.apache.commons.net.ftp.FTP.user(FTP.java:698)\n        at org.apache.nutch.protocol.ftp.Client.login(Client.java:294)\n        at org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:190)\n        ... 2 more\n2017-10-25 22:52:54,730 WARN  org.apache.nutch.protocol.ftp.Ftp - Could not get ftp://nas/silver-sda2/home/hiran/svn/glib-2.2.3/tests/cxx-test.C\norg.apache.nutch.protocol.ftp.FtpException: java.net.SocketException: Socket closed\n        at org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:308)\n        at org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:132)\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\nCaused by: java.net.SocketException: Socket closed\n        at java.net.SocketInputStream.socketRead0(Native Method)\n        at java.net.SocketInputStream.socketRead(SocketInputStream.java:116)\n        at java.net.SocketInputStream.read(SocketInputStream.java:171)\n        at java.net.SocketInputStream.read(SocketInputStream.java:141)\n        at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n        at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n        at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n        at java.io.InputStreamReader.read(InputStreamReader.java:184)\n        at java.io.BufferedReader.fill(BufferedReader.java:161)\n        at java.io.BufferedReader.read(BufferedReader.java:182)\n        at org.apache.commons.net.io.CRLFLineReader.readLine(CRLFLineReader.java:58)\n        at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:310)\n        at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:290)\n        at org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:479)\n        at org.apache.commons.net.ftp.FTP.sendCommand(FTP.java:552)\n        at org.apache.commons.net.ftp.FTP.user(FTP.java:698)\n        at org.apache.nutch.protocol.ftp.Client.login(Client.java:294)\n        at org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:190)\n        ... 2 more\n2017-10-25 22:52:54,734 WARN  org.apache.nutch.protocol.ftp.Ftp - Could not get ftp://nas/MediaPC/usr/include/asm-generic/shmparam.h\norg.apache.nutch.protocol.ftp.FtpException: java.net.SocketException: Socket is not connected\n        at org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:308)\n        at org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:132)\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\nCaused by: java.net.SocketException: Socket is not connected\n        at java.net.Socket.getInputStream(Socket.java:905)\n        at org.apache.commons.net.SocketClient.connectAction(SocketClient.java:143)\n        at org.apache.commons.net.ftp.FTP.connectAction(FTP.java:374)\n        at org.apache.commons.net.SocketClient.connect(SocketClient.java:172)\n        at org.apache.commons.net.SocketClient.connect(SocketClient.java:266)\n        at org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:175)\n        ... 2 more\n2017-10-25 22:52:54,744 WARN  org.apache.nutch.protocol.ftp.Ftp - Could not get ftp://nas/MediaPC/home/hiran/.compiz/\norg.apache.nutch.protocol.ftp.FtpError: Ftp Error: 500\n        at org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:151)\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\n}}\nPlease note that all these problems vanished when I configured fetcher.threads.per.queue back to 1 (the default value).",
        "Issue Links": []
    },
    "NUTCH-2454": {
        "Key": "NUTCH-2454",
        "Summary": "REST API fix for usage of hostdb in generator",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "1.15",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "03/Nov/17 14:19",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "03/Jan/18 17:31",
        "Description": "NutchNUTCH-2368\nVariable generate.max.count and fetcher.server.delay",
        "Issue Links": [
            "/jira/browse/NUTCH-2368",
            "https://github.com/apache/nutch/pull/248"
        ]
    },
    "NUTCH-2455": {
        "Key": "NUTCH-2455",
        "Summary": "Speed up the merging of HostDb entries for variable fetch delay",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.20",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "06/Nov/17 16:10",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Citing Sebastian at NUTCH-2420:\nThe correct solution would be to use <host,score> pairs as keys in the Selector job, with a partitioner and secondary sorting so that all keys with same host end up in the same call of the reducer. If values can also hold a HostDb entry and the sort comparator guarantees that the HostDb entry (entries if partitioned by domain or IP) comes in front of all CrawlDb entries. But that would be a substantial improvement...",
        "Issue Links": [
            "/jira/browse/NUTCH-2924",
            "/jira/browse/NUTCH-2368",
            "https://github.com/apache/nutch/pull/254"
        ]
    },
    "NUTCH-2456": {
        "Key": "NUTCH-2456",
        "Summary": "Allow to index pages/URLs not contained in CrawlDb",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "06/Nov/17 17:53",
        "Updated": "23/Apr/18 08:29",
        "Resolved": "05/Dec/17 09:41",
        "Description": "If http.redirect.max is set to a positive value, the Fetcher will follow redirects, creating a new CrawlDatum.\nIf the redirected URL is fetched and parsed, during indexing for it we have a special case: dbDatum is null. This means that in https://github.com/apache/nutch/blob/6199492f5e1e8811022257c88dbf63f1e1c739d0/src/java/org/apache/nutch/indexer/IndexerMapReduce.java#L259 the document is not indexed, as it is assumed it only has inlinks (actually it has everything but dbDatum).\nI'm not sure what the correct fix is here. It seems to me the condition should use AND instead of OR anyway, but I may not understand the original intent. It is clear that it is too strict as is.\nHowever, the code following that line assumes all 4 objects are not null, so a patch would need to change more than just the condition.",
        "Issue Links": [
            "/jira/browse/NUTCH-2526",
            "/jira/browse/NUTCH-2184",
            "https://github.com/apache/nutch/pull/240"
        ]
    },
    "NUTCH-2457": {
        "Key": "NUTCH-2457",
        "Summary": "Embedded documents likely not correctly parsed by Tika",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Tim Allison",
        "Created": "06/Nov/17 21:54",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "30/Sep/19 11:31",
        "Description": "While working on TIKA-2490, I think I found that Nutch's current method of requesting a mime-specific parser for each file will fail to parse embedded files, e.g. https://github.com/apache/tika/blob/master/tika-server/src/test/resources/test_recursive_embedded.docx\nThe fix should be straightforward, and I'll submit a PR once I can get Nutch up and running in my dev environment.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/474"
        ]
    },
    "NUTCH-2458": {
        "Key": "NUTCH-2458",
        "Summary": "TikaParser doesn't work with tika-config.xml set",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "09/Nov/17 16:47",
        "Updated": "28/Nov/17 11:52",
        "Resolved": "10/Nov/17 09:58",
        "Description": "Well, it doesn't indeed. Thanks to Timothy Allison, its solved.",
        "Issue Links": []
    },
    "NUTCH-2459": {
        "Key": "NUTCH-2459",
        "Summary": "Nutch cannot download/parse some files via FTP",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.20",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Hiran Chaudhuri",
        "Created": "10/Nov/17 00:00",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "I tried running Nutch on my Synology NAS. As SMB protocol is not contained in Nutch, I turned on FTP service on the NAS and configured Nutch to crawl ftp://nas.\nThe experience gives me varying results which seem to point to problems within Nutch. However this may need further evaluation.\nAs some files could not be downloaded and I could not see a good error message I changed the method org.apache.nutch.protocol.ftp.FTP.getProtocolOutput(Text, CrawlDatum) to not only return protocol status but send the full exception and stack trace to the logs:\n{{ } catch (Exception e) {\nLOG.warn(\"Could not get {}\", url, e);\nreturn new ProtocolOutput(null, new ProtocolStatus(e));\n}\n}}\nWith this modification I suddenly see such messages in the logfile:\n{{2017-11-09 23:44:56,135 WARN  org.apache.nutch.protocol.ftp.Ftp - Error: \njava.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n        at java.util.LinkedList.checkElementIndex(LinkedList.java:555)\n        at java.util.LinkedList.get(LinkedList.java:476)\n        at org.apache.nutch.protocol.ftp.FtpResponse.getFileAsHttpResponse(FtpResponse.java:327)\n        at org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:267)\n        at org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:133)\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\n2017-11-09 23:44:56,135 ERROR org.apache.nutch.protocol.ftp.Ftp - Could not get protocol output for ftp://nas/MediaPC/boot/memtest86+.elf\norg.apache.nutch.protocol.ftp.FtpException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n\tat org.apache.nutch.protocol.ftp.FtpResponse.<init>(FtpResponse.java:309)\n\tat org.apache.nutch.protocol.ftp.Ftp.getProtocolOutput(Ftp.java:133)\n\tat org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:340)\nCaused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0\n\tat java.util.LinkedList.checkElementIndex(LinkedList.java:555)\n\tat java.util.LinkedList.get(LinkedList.java:476)\n\tat org.apache.nutch.protocol.ftp.FtpResponse.getFileAsHttpResponse(FtpResponse.java:327)\n}}\nI cannot tell what the URLs showing this problems have in common. They seem to be regular files, however a lot of other regular files can be fetched and parsed successfully. As far as I understand the source code, at least one outgoing link is expected:\n{{\nFTPFile ftpFile = (FTPFile) list.get(0);\n}}\nCan this be safely assumed for all files? Or should there rather be a check if outgoing links were found?",
        "Issue Links": []
    },
    "NUTCH-2460": {
        "Key": "NUTCH-2460",
        "Summary": "use the headless option of firefox and chrome in protocol-selenium",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "hussein Al_Ahmad",
        "Created": "11/Nov/17 12:03",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "23/Feb/19 23:12",
        "Description": "the --headless option is added to firefox in version 55 or later , and in chrome in version 59 or later ...\nthis is much better than relying on  xvfb and its associates .\nwe can add it as a property in the config file .\nI'm trying it on my local machine , and will create a pull request when I finish testing it .",
        "Issue Links": [
            "/jira/browse/NUTCH-2676",
            "https://github.com/apache/nutch/pull/245"
        ]
    },
    "NUTCH-2461": {
        "Key": "NUTCH-2461",
        "Summary": "Generate passes the data to when maxCount  == 0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "14/Nov/17 13:40",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "15/Jan/18 17:40",
        "Description": "The generator checks condition \nif (maxCount > 0) : line 421 and stop the generation when amount per host exceeds maxCount( continue : line 455)\nbut when  maxCount == 0 it goes directly to line 465 :output.collect(key, entry);\nIt is obviously not correct, the correct solution would be to add \nif(maxCount == 0)\n{\r\n        \tcontinue;\r\n}\nat line 380.",
        "Issue Links": [
            "/jira/browse/NUTCH-2368",
            "https://github.com/apache/nutch/pull/249"
        ]
    },
    "NUTCH-2462": {
        "Key": "NUTCH-2462",
        "Summary": "Cleanup Tika Boilerpipe patch",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.5",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "15/Nov/17 15:44",
        "Updated": "15/Nov/19 10:48",
        "Resolved": "15/Nov/19 10:48",
        "Description": "Remove unused imports and some generic (.*) imports\nApply the formatting rules\nRefactor configurations variables into the setConf method (for consistency)",
        "Issue Links": []
    },
    "NUTCH-2463": {
        "Key": "NUTCH-2463",
        "Summary": "Enable sampling CrawlDB",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "20/Nov/17 14:34",
        "Updated": "28/Nov/17 11:52",
        "Resolved": "28/Nov/17 10:49",
        "Description": "CrawlDB can grow to contain billions of records. When that happens readdb -dump is pretty useless, and readdb -topN can run for ages (and does not provide a statistically correct sample).\nWe should add a parameter -sample to readdb -dump which is followed by a number between 0 and 1, and only that fraction of records from the CrawlDB will be processed.\nThe sample should be statistically random, and all the other filters should be applied on the sampled records.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/243"
        ]
    },
    "NUTCH-2464": {
        "Key": "NUTCH-2464",
        "Summary": "Plugin headings: Headers That Contain HTML Elements Are Not Parsed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Cass Pallansch",
        "Created": "20/Nov/17 18:43",
        "Updated": "30/Nov/17 19:52",
        "Resolved": "30/Nov/17 19:20",
        "Description": "Nutch does not appear to traverse the HTML elements that may be contained within header elements (e.g., H1, H2, H3, etc. tags).  Many times there are anchors and/or <span> tags within these elements that contain the actual text nodes that should be picked up as the header value for indexing purposes.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/244"
        ]
    },
    "NUTCH-2465": {
        "Key": "NUTCH-2465",
        "Summary": "Broken Eclipse project. Classpaths and interactiveselenium should be fixed.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "27/Nov/17 10:52",
        "Updated": "30/Nov/17 19:52",
        "Resolved": "30/Nov/17 18:57",
        "Description": "With the latest version of develop the Eclipse project doesn't work anymore.\nThere are two sets of problem:\n1) Classpath problems \n2) Incorrect usage of org.apache.nutch.protocol.interactiveselenium in the code. Should be replaced by org.apache.nutch.protocol.interactiveselenium.handlers",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/251"
        ]
    },
    "NUTCH-2466": {
        "Key": "NUTCH-2466",
        "Summary": "Sitemap processor to follow redirects",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "28/Nov/17 12:18",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "31/Jan/18 13:54",
        "Description": "It does follow http > https, but not the following redirect, e.g. sitemap_index.xml that some websites have.",
        "Issue Links": []
    },
    "NUTCH-2467": {
        "Key": "NUTCH-2467",
        "Summary": "Sitemap type field can be null",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "28/Nov/17 13:11",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "06/Jan/18 08:44",
        "Description": "sitemap.isIndex() can return null for real sitemap indices, so there contents won't be added to the CrawlDB. Example, the indices https://www.reisenco.nl/sitemap_index.xml points to are not processed.",
        "Issue Links": [
            "/jira/browse/NUTCH-2490"
        ]
    },
    "NUTCH-2468": {
        "Key": "NUTCH-2468",
        "Summary": "should filter out invalid URLs by default",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.12",
        "Fix Version/s": "2.4,                                            1.14",
        "Component/s": "bin",
        "Assignee": null,
        "Reporter": "Michael Coffey",
        "Created": "30/Nov/17 19:38",
        "Updated": "11/May/18 14:36",
        "Resolved": "05/Dec/17 10:27",
        "Description": "Some Nutch components, by default, should reject invalid URLs. This was recently discussed in the users mailing list and has affected my work for a while. Although there may be some special-purpose needs to collect invalid URLs, they are not generally useful for crawling.",
        "Issue Links": [
            "https://lists.apache.org/thread.html/03eb71d788fac8cb1db1b349845f4bce728040890d0a8ea2ac40c5f3@%3Cuser.nutch.apache.org%3E"
        ]
    },
    "NUTCH-2469": {
        "Key": "NUTCH-2469",
        "Summary": "Documents not commited to solr in Sever mode",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Ninaad Joshi",
        "Created": "01/Dec/17 10:34",
        "Updated": "05/Dec/17 11:58",
        "Resolved": "05/Dec/17 11:43",
        "Description": "I found there is a discrepancy in execution paths when running Nutch in local standalone mode vis-\u00e0-vis server mode. \nI observed, in local standalone mode, when the indexing process is done the document along with its fields get indexed and committed in solr and is returned if queried immediately. However, the same when done through server mode, the document gets indexed but is not committed in solr, hence not returned if queried immediately. When we restart solr the indexed document is returned if queried.\nI browsed through the IndexingJob.java file to understand the cause for this. I found out:\n\nThere are two different entry paths for the local standalone mode and the server mode\n\t\nServer mode entry point: public Map<String, Object> run(Map<String, Object> args)\nStandalone mode entry point:\n\t\t\npublic int run(String[] args)\npublic void index(String batchId)\n\n\n\n\nThe local standalone mode path did extra stuff than the server mode\n\t\nThe public void index(String batchId) function initially calls the server mode path: public Map<String, Object> run(Map<String, Object> args)\nAnd then does this extra stuff\n\t\t\nGets IndexWriters\nUsing IndexWriters Describes\nUsing IndexWriters commits if COMMIT_INDEX=true is specified in the configuration\nThe aforementioned extra stuff is not done in the server mode\n\n\n\n\n\nI feel the execution paths for both the modes should be same and hence propose to:\n\nMove the extra stuff done using IndexWriters in public void index(String batchId) to the end of server mode execution path i.e public Map<String, Object> run(Map<String, Object> args) function\nCall public Map<String, Object> run(Map<String, Object> args) function directly from Standalone mode entry point: public int run(String[] args)\npublic int run(String[] args) becomes redundant and can be safely removed.\n\nI have attached the proposed patch along with this issue. Kindly go through the same and approve.",
        "Issue Links": []
    },
    "NUTCH-2470": {
        "Key": "NUTCH-2470",
        "Summary": "CrawlDbReader -stats to show quantiles of score",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "04/Dec/17 20:14",
        "Updated": "05/Dec/17 12:56",
        "Resolved": "05/Dec/17 12:23",
        "Description": "The command \"readdb -stats\" shows for the CrawlDatum score min., max. and average. Median and quartiles (quantiles, in general) would complete the statistics to get an impression how scores are distributed.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/252"
        ]
    },
    "NUTCH-2471": {
        "Key": "NUTCH-2471",
        "Summary": "Returning a bare string meant to be application/json doesn't properly quote the string",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "nutch server",
        "Assignee": null,
        "Reporter": "Ninaad Joshi",
        "Created": "06/Dec/17 10:49",
        "Updated": "15/Nov/19 10:48",
        "Resolved": "15/Nov/19 10:48",
        "Description": "Nutch Server resources are JAX-RS resources and that they proclaims to produce JSON even for a resource method that returns a String...... \n\n\r\n@Produces({ MediaType.APPLICATION_JSON })\r\npublic abstract class AbstractResource {\r\n.....\r\n}\r\n\r\n@Path(\"/config\")\r\npublic class ConfigResource extends AbstractResource {\r\n...\r\n@GET\r\n  @Path(\"/{configId}/{propertyId}\")\r\n  public String getProperty(@PathParam(\"configId\") String configId,\r\n      @PathParam(\"propertyId\") String propertyId) {\r\n    return configManager.getAsMap(configId).get(propertyId);\r\n  }\r\n}\r\n\n\nthe HTTP response indicates it is an application/json response, however the string returned is not properly quoted for JSON. We also get de-serialization errors on the client side.\n\nserver: Restlet-Framework/2.2.3\r\ndate: Wed, 06 Dec 2017 10:30:13 GMT\r\ncontent-type: application/json; charset=UTF-8\r\ncontent-length: 21\r\naccept-ranges: bytes\r\n\r\nThere was an error parsing JSON data\r\nUnexpected token B in JSON at position 0\r\n\n\nThe JAX-RS resources are configured to use JacksonJsonProvider for writing the JSON message body. JacksonJsonProvider.isWriteable() is indicating that it cannot write the value because _untouchables includes String.class. It appears String.class was put back into the _untouchables list as a result of a bug. That bug, however, appears to have been targeted at dealing with an XML issue, not a JSON issue. \nIt should have been ideally taken care by the Jackson providers. However, it's not and hence proposing this fix.\n\nCreate our own custom NutchJsonProvider based on JacksonJsonProvider and remove the String.class from the _untouchables\nRegister this NutchJsonProvider with NutchServer to be used while writing JSON message body\n\nI have attached the patch along with this issue\nThe other option is to JsonEscape the string and pad it with double quotes before returning. This has to be done at all places wherever string is returned as Json response and also is prone to errors in future if anybody misses this escaping and padding.\nThere already is : issue created on the JAX-RS",
        "Issue Links": []
    },
    "NUTCH-2472": {
        "Key": "NUTCH-2472",
        "Summary": "Sitemap processor does not honour db.ignore.external.links",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "sitemap",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "06/Dec/17 12:45",
        "Updated": "06/Dec/17 15:07",
        "Resolved": "06/Dec/17 13:32",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2473": {
        "Key": "NUTCH-2473",
        "Summary": "Elasticsearch REST Indexer broken due to wrong depenency",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Moreno Feltscher",
        "Created": "07/Dec/17 12:19",
        "Updated": "13/Dec/17 21:35",
        "Resolved": "13/Dec/17 21:31",
        "Description": "When trying to index into Elasticsearch using indexer-elastic-rest the following error is being thrown:\n\n\r\nException in thread \"main\" java.lang.LinkageError: loader constraint violation: when resolving method \"org.slf4j.impl.StaticLoggerBinder.getLoggerFactory()Lorg/slf4j/ILoggerFactory;\" the class loader (instance of org/apache/nutch/plugin/PluginClassLoader) of the current class, org/slf4j/LoggerFactory, and the class loader (instance of sun/misc/Launcher$AppClassLoader) for the method's defining class, org/slf4j/impl/StaticLoggerBinder, have different Class objects for the type org/slf4j/ILoggerFactory used in the signature\r\n    at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:418)\r\n    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:357)\r\n    at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\r\n    at org.apache.nutch.indexwriter.elasticrest.ElasticRestIndexWriter.<clinit>(ElasticRestIndexWriter.java:71)\r\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n    at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\r\n    at java.lang.Class.newInstance(Class.java:442)\r\n    at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:161)\r\n    at org.apache.nutch.indexer.IndexWriters.<init>(IndexWriters.java:57)\r\n    at org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:123)\r\n    at org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:230)\r\n    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n    at org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:239)\r\n\n\ne66d44d removed the runtime dependency on slf4j-api-1.7.21.jar everywhere but in indexer-elastic-rest.\nPossible fix: https://github.com/apache/nutch/pull/253",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/253"
        ]
    },
    "NUTCH-2474": {
        "Key": "NUTCH-2474",
        "Summary": "CrawlDbReader -stats fails with ClassCastException",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.14",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "08/Dec/17 09:17",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "14/Dec/17 15:13",
        "Description": "In distributed mode CrawlDbReader / readdb -stats fails with a ClassCastException in the combiner:\n\n17/12/08 04:57:13 INFO mapreduce.Job: Task Id : attempt_1512553291624_0022_m_000039_0, Status : FAILED\r\nError: java.lang.ClassCastException: org.apache.hadoop.io.FloatWritable cannot be cast to org.apache.hadoop.io.LongWritable\r\n        at org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner.reduce(CrawlDbReader.java:296)\r\n        at org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatCombiner.reduce(CrawlDbReader.java:222)\r\n        at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1639)\r\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.mergeParts(MapTask.java:1946)\r\n        at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1514)\r\n        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:466)\r\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\r\n\n\nFloatWritables are used since NUTCH-2470, so that's when this bug was introduced.",
        "Issue Links": [
            "/jira/browse/NUTCH-2297",
            "/jira/browse/NUTCH-2297",
            "https://github.com/apache/nutch/pull/255"
        ]
    },
    "NUTCH-2475": {
        "Key": "NUTCH-2475",
        "Summary": "If and else-if branches has the same condition",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.2,                                            1.14",
        "Fix Version/s": "2.4,                                            1.16",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "songwanging",
        "Created": "08/Dec/17 16:55",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/Jan/19 20:25",
        "Description": "Our tool DeepTect has detected a piece of buggy code snippet, in which the if and else branches has the same condition.\nPath: nutch/src/plugin/protocol-ftp/src/java/org/apache/nutch/protocol/ftp/Client.java\t\n\n\r\nprivate boolean _notBadReply(int reply) {\r\n\r\n    if (FTPReply.isPositiveCompletion(reply)) {\r\n      // do nothing\r\n ...\r\n    } else if (reply == 451) { // FTPReply.ACTION_ABORTED\r\n      // some ftp servers reply 451, e.g.,\r\n      // ProFTPD [ftp.kernel.org]\r\n      // there is second reply witing? no!\r\n      // getReply();\r\n    } else if (reply == 451) { // FTPReply.ACTION_ABORTED\r\n    } else {\r\n      // what other kind of ftp server out there?\r\n      return false;\r\n    }\r\n\r\n    return true;\r\n  }",
        "Issue Links": []
    },
    "NUTCH-2476": {
        "Key": "NUTCH-2476",
        "Summary": "Unable to see content in pdf file from crawldb, from crawldump output folder",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.13",
        "Fix Version/s": "None",
        "Component/s": "commoncrawl",
        "Assignee": null,
        "Reporter": "Vikas",
        "Created": "12/Dec/17 09:37",
        "Updated": "12/Dec/17 09:37",
        "Resolved": null,
        "Description": "fallowed below steps\nbin/nutch inject /path/to/your/crawl_dir/crawldb /path/to/your/urls_dir\nbin/nutch generate /path/to/your/crawldb /path/to/your/segment_dir\nbin/nutch fetch /path/to/your/segment_dir/2* (2* is a nuber generated by Nutch)\nbin/nutch parse /path/to/your/segment_dir/2*\nbin/nutch updatedb /path/to/your/crawldb /path/to/your/segment_dir.\nbin/nutch commoncrawldump -outputDir /path/to/output_dir -segment /path/to/segments\nafter above step in the output folder i can see pdf files but data in pdf file is not readble format\nplease help to resolve this issue.\ni need to see whole pdf content which crawled from website.\nsmple pdf file content is :\n\u00d9\u00d9\u00f7\u007fy\u00fd{\n  \"url\" : \"https://www.fda.gov/OHRMS/DOCKETS/98fr/052102d.pdf\",\n  \"timestamp\" : \"Tue, 21 May 2002 10:42:35 GMT\",\n  \"request\" : {\n    \"method\" : \"GET\",\n    \"client\" : {\n      \"hostname\" : \"ubuntu\",\n      \"address\" : \"127.0.1.1\",\n      \"software\" : \"Nutch-1.13-SNAPSHOT\",\n      \"robots\" : \"CLASSIC\",\n      \"contact\" : \n{\r\n        \"name\" : \"My Nutch Spider\",\r\n        \"email\" : \"\"\r\n      }\n    },\n    \"headers\" : \n{\r\n      \"Accept\" : \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\r\n      \"Accept-Encoding\" : \"\",\r\n      \"Accept-Language\" : \"en-us,en-gb,en;q=0.7,*;q=0.3\",\r\n      \"User-Agent\" : \"\"\r\n    }\n,\n    \"body\" : null\n  },\n  \"response\" : {\n    \"status\" : \"\",\n    \"server\" : \n{\r\n      \"hostname\" : \"www.fda.gov\",\r\n      \"address\" : \"\"\r\n    }\n,\n    \"headers\" : \n{\r\n      \"Content-Encoding\" : \"\",\r\n      \"Content-Type\" : \"application/pdf\",\r\n      \"Date\" : \"Mon, 20 Nov 2017 10:15:32 GMT\",\r\n      \"Server\" : \"Microsoft-IIS/7.0\",\r\n      \"Connection\" : \"close\",\r\n      \"Last-Modified\" : \"Tue, 21 May 2002 10:42:35 GMT\",\r\n      \"nutch.crawl.score\" : \"1.0\",\r\n      \"nutch.fetch.time\" : \"1511172932280\",\r\n      \"Accept-Ranges\" : \"bytes\",\r\n      \"nutch.segment.name\" : \"20171120021520\",\r\n      \"Strict-Transport-Security\" : \"max-age=31536000\",\r\n      \"Cache-Control\" : \"max-age=86195\",\r\n      \"ETag\" : \"\\\"b2c31038b40c21:0\\\"\",\r\n      \"Content-Length\" : \"41655\",\r\n      \"_fst_\" : \"33\",\r\n      \"X-Powered-By\" : \"ASP.NET\"\r\n    }\n,\n    \"body\" : \"%PDF-1.3\\n%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\\n1 0 obj\\n<< \\n/Type /Catalog \\n/Pages 2 0 R \\n>> \\nendobj\\n2 0 obj\\n<< \\n/Type /Pages \\n/Kids [ 29 0 R 4 0 R ] \\n/Count 2 \\n/MediaBox [ 0 0 612 792 ] \\n>> \\nendobj\\n3 0 obj\\n<< \\n/ModDate (D:20020521062302)\\n/Producer (Acrobat Distiller 4.0 for Windows)\\n/CreationDate (D:20020521062302)\\n/Author (U.S. Government Printing Office)\\n/Title (Document)\\n/Subject (Extracted Pages)\\n>> \\nendobj\\n4 0 obj\\n<< \\n/Type /Page \\n/Parent 2 0 R \\n/Resources 6 0 R \\n/Contents 5 0 R \\n/MediaBox [ 0 0 612 792 ] \\n/CropBox [ 0 0 612 792 ] \\n/Rotate 0 \\n>> \\nendobj\\n5 0 obj\\n<< /Length 4387 /Filter /FlateDecode >> \\nstream\\r\\nH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bdH\\u0012\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00d9\u20ac\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd3O\\u0006\u00ef\u00bf\u00bd@\\u00076\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d8\u2030\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd5#K\\u001E] \u00ef\u00bf\u00bd\u007f1\u00ef\u00bf\u00bd\u00df\u00bbY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>mD\u00ef\u00bf\u00bd\\u0018+U\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'OR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\\u0002\\u0014~\\u0003F\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdAa\\u0002\u00ef\u00bf\u00bd\u007fP\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\\u0003VE\u00cf\u20ac\\u0018z\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=JL|\u00ef\u00bf\u00bd~F\u00ef\u00bf\u00bdgT>\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd.\u00c7\u2020\\u0003\\u0006,\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+(\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd),6x\u00ef\u00bf\u00bd\\n\u007f\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\\u00023\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cb\u00b1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bdR\\u0017\\u000FX\\f{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\\u0017\u00ef\u00bf\u00bdc\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-K\\b8\u00ef\u00bf\u00bd9L3\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd>_T\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9L\u00ef\u00bf\u00bd\\u001D0\u00ef\u00bf\u00bd\\u001C\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bdiV\u00ef\u00bf\u00bd!/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00cf\u2014~\u00ef\u00bf\u00bd r\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00cb\u00ad\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\\u001E\\u0006U\u00ef\u00bf\u00bd!.!. {)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd)\\\\\\u0018\u00c4\u00b0mG\u00ef\u00bf\u00bd\u00dd\u00bfy\u00ef\u00bf\u00bd\\u0015/\u00ef\u00bf\u00bd\\u0019$Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\\u001Co-\u00ef\u00bf\u00bd|\\u0007\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<O\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\\\\\u00ef\u00bf\u00bd\\rO\u00ef\u00bf\u00bd\u00e2\u00bc\u00b5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\\u0018j\\u001E\u00ef\u00bf\u00bd\\u0001sb\u00ef\u00bf\u00bd\\u001E=\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd]k\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bdk(\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdoy\u00ef\u00bf\u00bdon\\u0002\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\\u0012c\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\\u000BqIZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E6<\\u000F+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\\u001D\u00ef\u00bf\u00bd+\\u000EI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00dc\u00bf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\\u001B>\\u0014\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\\\\\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00c4\u00b2\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(5j\\u0004\u00ef\u00bf\u00bd<\\u000F\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\\u0002G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00da\u00bf\\u0002\\r$\u00ef\u00bf\u00bd8\\u0014\u00ef\u00bf\u00bd$A\u00ef\u00bf\u00bd-\u00da\u2014*\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdbb\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW/\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR,\u00ef\u00bf\u00bd\\u001F\\b\u00d3\u00b2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMl\u00ef\u00bf\u00bdr5\u00ef\u00bf\u00bd\\b\\u0004$\\u001CK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\\u0016P \\n\u00ef\u00bf\u00bd+>\\u000Ec\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,m\u00cb\u0081&7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdd|S\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bdFf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV;Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\\u000B?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd|v\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bda\\u0012j\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6z1J1\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c7\u00af\u00ef\u00bf\u00bd\u00cd\u00ab$(\u00ef\u00bf\u00bd|\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\\u0019\u00ef\u00bf\u00bd9\\\"{\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda^\u00ef\u00bf\u00bd`\\u0010m\u00ef\u00bf\u00bd\\u0014)!\\u000FJ\\fp\u00ef\u00bf\u00bd{\u00db\u00a1\\u0014\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\\u0001\\u000FA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\\u001F'\u00ef\u00bf\u00bd\u00d6\u0090Q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd+\\u001A\u00db\u00a7\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00eb\u00ab\u2021'%\u00ef\u00bf\u00bdZH\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*Ui\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cf\u00b4\u00ef\u00bf\u00bd)8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Ez\u00ef\u00bf\u00bdLk:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdct\u00d3\u00ab[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bdp$\\b-|\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bdM x\\f\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bdu\\r\\u000B-\u00ef\u00bf\u00bd!V\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd'A\\u001A\u00ef\u00bf\u00bd$(\u00ef\u00bf\u00bd:G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%6\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\\u0017\u00ca\u00a0s~\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\\u0004S\u00ef\u00bf\u00bd!8\u00ef\u00bf\u00bd\\u001CC\u00ef\u00bf\u00bdUc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bdb\\u0018r\u00ef\u00bf\u00bd&B\u00ef\u00bf\u00bd! \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c8\u201a-~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdfH\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\\u00173\u00ef\u00bf\u00bdly\u00ef\u00bf\u00bd{\u007f\u00ef\u00bf\u00bd8;\u00ef\u00bf\u00bd0-\u00ef\u00bf\u00bdd\u007f\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd6_\u00ef\u00bf\u00bd<N\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\\f\\u000FG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00030\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd \\u0012\\\\\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd&p\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010!M\\u001EN\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bdK>R\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdG\\u001D\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|$I\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bdQCLXK`\u00ef\u00bf\u00bd{\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda4\\u0019!\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\\u0011\u00c7\u00b6M\u00ef\u00bf\u00bd\\u0006_F\u00db\u00b58'\\\\\u00e6\u2030\u00a2\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bdtO\u00ef\u00bf\u00bd:\\b\u007f_a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bdd\\u0012\u00ef\u00bf\u00bd@\\u001E\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bdSE\u00ef\u00bf\u00bd\u00ca\u00a7\u00ef\u00bf\u00bd ,i\\\\&<\\u0002=7&\u00ef\u00bf\u00bd\\f_\\u001Bq\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<$\\n<X\\u0010\u00ef\u00bf\u00bd~\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\\u0007]Y\\u0006\\u000F1J\u00d7\u00a0\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd-;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdVAM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\\u0015\u00d4\u00b0UN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdRP\\\"(\u00ef\u00bf\u00bd0Mb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdgZP(?<C\u00ef\u00bf\u00bd\u00c6\u00bf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdOD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\\u0000&_\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c7\u00b0\u00ef\u00bf\u00bd^P\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd*f\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00df\u00ab\\u0003\\u0017\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd_\\u0010E9\u00ef\u00bf\u00bdXT%\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU!\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00e1dH\u00ef\u00bf\u00bde\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd&B&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\\u001D*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014mc_+[\u00ef\u00bf\u00bd_%1\\u0011\\u0001\\u0001\\n*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00d8\u00b9\\u0014{w\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007ft\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdaP\\u0006I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdUI\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007fR\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\\tgw(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHkx\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq^\\u001F/\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dd\u00b6\\u0016u\u00ef\u00bf\u00bdr>\u00ef\u00bf\u00bdz;Jtq/8\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\\u001F\\u0019f$N\u00c3\u00a4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\\u0006\u00dc\u0152g\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\\u000B\u00c7\u201e$\u00dc\u00b2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\\u0005EB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+R\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.V\u00ef\u00bf\u00bdvB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~.\u00dc\u008d1\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9/wr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e7\u00b5\u00afT\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\\u0003f+\\u0007\\f\u00d1\u00a9\u00ef\u00bf\u00bdv9\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bdL3JxX\u00ef\u00bf\u00bdY\\u001A\u00ef\u00bf\u00bd0\\b\u00ef\u00bf\u00bd1\\u001F\\u0010\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bdFm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012hB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\\u0010HQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&Y\\u0010\u00ef\u00bf\u00bd-\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8l\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ \u00ef\u00bf\u00bdjd~c,\u00ef\u00bf\u00bd$Oy)\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd*>\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6|\u00ef\u00bf\u00bd\u00da\u00ab\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd*a\\u0006#t4\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00c9\u00beQ\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bdtK\u00e0\u00b3\u00bc\u00ef\u00bf\u00bduYn\u00ef\u00bf\u00bd./\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\\n\u00ef\u00bf\u00bd*{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bdsl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`J\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00dc\u00ae\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd9G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c9\u00ba\u00ef\u00bf\u00bd$\\u0012 \u00ef\u00bf\u00bd\\u001C_\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd9bhI\u00ef\u00bf\u00bdc5\u00ef\u00bf\u00bd\\r\\u0005|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv`X\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\\u001Fi\u00ef\u00bf\u00bdp\u00d3\u00b5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bdu*8\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d2\u00ab\\r1\\u001B\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bdU's,\u00ef\u00bf\u00bd\\u000E\\u0006E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00005*\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd4dP,\u007f3?$j\\rV\\u001F\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd*,\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d8\u00be\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\tP\u00ef\u00bf\u00bde\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\\u0014;b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007;\\ns\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000ED!t b\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS)/(\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd+\\u001CK\u00ef\u00bf\u00bd\u00e6\u2021\u00a6j\\u001CYQ\\u0007\\u0018c\u00e0\u00b8\u00a6\\u0018\u007f\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpP\u00dd\u0160w\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\\u001E\u00ee\u00a7- p\u00ef\u00bf\u00bd8\\u001C\u00ef\u00bf\u00bde8\u00ef\u00bf\u00bd\\u0015\u00d6\u0152W\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd#Q\u00ef\u00bf\u00bdGD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\\u0017OF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\\u0011u\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\u00d7\u2039VZI3\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl!4\\u001A<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn4xX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`:\u00ec\u00a0\u00a7\u00e1\u00b0\u00bb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00d9- \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\=g\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bdj]\\\"<\u00ef\u00bf\u00bd\\u00160\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdf\\u0002hC-/.JTv\\f\\u0018\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bdM&+\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bdR\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00dc\u201e\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg`\u00ef\u00bf\u00bdL8\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\\u0003X\\u0004i*f\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e2\u02c6\u00ab\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\\u000FD\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dd\u00a0P\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\\u001DMo~\u00d5\u201a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWp\u00ef\u00bf\u00bd\\n\\u000Eu\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bdl\u00d4\u017e\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\\\\\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c8\u00a2N\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bdw]\u00ef\u00bf\u00bd|b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdiV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bdq7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.P\u007f\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c5\u00aeQ\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdEE\u00cb\u00aaO\\f\u00ef\u00bf\u00bdtu\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+=\u00ef\u00bf\u00bd8mQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bdHj~|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\\u0018\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd.c\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0x\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$MKuZ\u00ef\u00bf\u00bdm\\u001C\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bdH\\u000E\u00ef\u00bf\u00bdG[[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bds\u00ef\u00bf\u00bdMX3\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ca\u00bdE\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\u00d2\u00b2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdmEi\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd$KW8V\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[]\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bdCy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJH+\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bdt5t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bdzY\u00ef\u00bf\u00bdN\u00c9\u00ae\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd^c!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR E\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bdm\u00c9\u017e{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\\u0019+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\\u0014K\\u001C~\u00ef\u00bf\u00bd\u00d7\u00b8\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00c3\u00be\u00ef\u00bf\u00bd\\u001Db\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\\u0014+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00d2\u00b2\u00ef\u00bf\u00bd&1\\rW\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd:\\u001C\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\\u0014\\u000E\u00ef\u00bf\u00bd@jz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00cc\u00ab\u007f\u00ef\u00bf\u00bd_m;\u00ef\u00bf\u00bd[Q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\\u0015~\\fR0>\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0 n\\u0002Z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cc\u0192\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c4\u009d\u00d8\u017dl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u007f\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bds|9\u00c6\u0081\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[GB\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\uD9BA\\uDF3D\u00ef\u00bf\u00bdZ3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00cb'[>nt\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\\u001D;:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$P`(\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bdQJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdjR;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D3\\u001D|\u00ef\u00bf\u00bd|V'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd3\\u000F\u00ef\u00bf\u00bdl2\\u0007S\\u00186\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\\u0019\u00ef\u00bf\u00bd\\u0016S\u00ef\u00bf\u00bdF\\u0014\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bds'9<\u00ef\u00bf\u00bd[\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\\u001DwPw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002F\u00ef\u00bf\u00bd\u00d0\u00ae\u00ef\u00bf\u00bd\\u0014N\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005B\\u0019\\u001Fo\u00ef\u00bf\u00bd\u00ea\u0192\u00a6\u00ef\u00bf\u00bd;0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\\u001B\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\\u000F\u00d3-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00c4\u00a5\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bdR\\u0016\u00cc\u00b0Q\\u00124x2\u00ef\u00bf\u00bdM\n{\u00cd\u00b6\u00ef\u00bf\u00bd\u00d3\u017e\u00ef\u00bf\u00bdy\\u001D\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv#vu\u00ef\u00bf\u00bdr\\u000FT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!9\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00c6\u0160K\u00ef\u00bf\u00bd,)\u00ef\u00bf\u00bd\\\\\\u0011\\u0001\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bdUjB\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd3)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\\\\q`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx]\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!Z\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd<\\u0014IV\\u0017Iy\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00c8\u00b9\u00ef\u00bf\u00bdQ\u00ce\u0081\\u0016\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bddk\\u001B|l\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(I\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd+m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\\u001E21M~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E&w\\u000FG\u00c7-\u00ef\u00bf\u00bdF\\u000E&\u00ef\u00bf\u00bd<<}\n9\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\\\\F\u00c9\u00af>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>AX\\u001D}\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bdr\\u0001y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\\u000E\\u0016,K:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl!<\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\\u0003\u00ef\u00bf\u00bd\u00d1\u00abv\u00ef\u00bf\u00bdk\\u0017\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!U\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#`\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bdAt\\u0004\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd :\u00ef\u00bf\u00bd/=\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007fLG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>D\\u001E\\u0003~\\u0002\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006I\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\\u0001\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\\u001D\\u0015\u00ef\u00bf\u00bdX=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00c9\u009d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u00b5\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bdqI\\u0017J*\u00ef\u00bf\u00bdL\\u001C\\u001BB\u00ef\u00bf\u00bd;\u00cc\u00b31\\u0004@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}6\\u000B\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd+\u00d0\u0192\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd^\u00e0\u00bf\u00ab\u00ef\u00bf\u00bd\\u001F;\\n\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd[I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd&e\\u0000\u00ef\u00bf\u00bdL\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d5\u02c6YSH_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dd\u00bc\u00ef\u00bf\u00bdf\u00d0\u00b1lX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\bf\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bdB7S\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\\u001E\\u00169L\\u0007J\u00ef\u00bf\u00bdm6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd=D \u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bdEv\u00ef\u00bf\u00bd]Cp@\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\\u0012\u00d5\u00b9\\u0001qcT)aQ\\u001F\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\\nK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd_6\u00ef\u00bf\u00bd\\u000B\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8}\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\\u0010\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'NKa\\\"*\u00ef\u00bf\u00bdHG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e7\u201e\u00be\\u0010\\u0011l\u00c2\u00bc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00da\u00a2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,[`_\u00ef\u00bf\u00bd*\u00c6\u00a0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cb\u2022\u00ef\u00bf\u00bd\\u000B_\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00d8\u00ac=\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00d1\u00b0P\u00ef\u00bf\u00bd4L\u00ef\u00bf\u00bd$*\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bdl\\nJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c3\u00b9\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00da...\u00ef\u00bf\u00bd\\u0014\n{7\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bdS\\u0002\u00d4\u0161h\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bdl*\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bdJA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd&\\u0011^\u00ef\u00bf\u00bdBy\u00ef\u00bf\u00bd.0\u00ef\u00bf\u00bd9:\u00ef\u00bf\u00bdji\u00c7\u00b0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bdrV\u00ef\u00bf\u00bd\\u0012'\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\\u001DP \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6(\\u001E\u00ef\u00bf\u00bdf]\u00ef\u00bf\u00bd\\b0H\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001$V\\u0002\u00ef\u00bf\u00bd0\u00d8\u00bc\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+H_6J\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_S\\u000F,iJ\\u0006\u00ef\u00bf\u00bdh\\u0011\u00ef\u00bf\u00bd(\\u0010\u00ef\u00bf\u00bd\\u001F6Z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00da\"Q2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdve\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8H\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpl_CE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\\\"\u00ef\u00bf\u00bdMA\u00ee\u203a\u0161^\u00ef\u00bf\u00bd@M\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<K\u00ef\u00bf\u00bdlA\\u0013T\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bdZ1r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u201e\u00ef\u00bf\u00bdc,\u00ef\u00bf\u00bd0-\u00d1\u009dE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019z\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd<^\u00ef\u00bf\u00bd\\u00109M0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd8P\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00d5\u00b8@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\\u0005CM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG@\u00ef\u00bf\u00bd\u00d9\u00a6\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd,\u00c8\u00a7\u00ef\u00bf\u00bdZ\u00ec\u0081\u00a7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[*y\\u000B\u00ef\u00bf\u00bd~\\u001Ex;\u00ef\u00bf\u00bd\u00cd\u0153\u00ef\u00bf\u00bd<`\\u0002\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bdr8\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdXq\\u000E(\u00d5\u009d\u00ef\u00bf\u00bd\u00df'\u00ef\u00bf\u00bdt0\\u0003\u00ef\u00bf\u00bd\\u000F\\u0002\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bdh\\u0007T\u00ef\u00bf\u00bd]7\\b\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d6\u00af\u00ef\u00bf\u00bd.\\u001AE%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh /i=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e9\u00a6\u017e!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]w\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP_\\u0000\\u001Ab\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u0153c\u00ef\u00bf\u00bdot\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00cc\u00a2\\u0000i\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$Z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSe\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca\u00a3\u00ef\u00bf\u00bdr.\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx;_#\u00ef\u00bf\u00bd<K\u00e3\u00b9\u2020\u00ef\u00bf\u00bdFE\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc \\u0002u\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\\t\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd@\\u0012lU\u00ef\u00bf\u00bd\\u0018\u00cc\u017e|*\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00db\u00a5\u00ef\u00bf\u00bd,\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdkZd4\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00cd-\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bdu\\u001BpI\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd o\\u0000\u00c5\u0192\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF_o\\u00174\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Ca-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;^>}\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bdiU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00d3\u0178\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^N\u00ef\u00bf\u00bd\\u0013N\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdVJ\\u00147\\u0010\u00ef\u00bf\u00bdu01fE\u00ef\u00bf\u00bd?t\u00d5\u00a0\u00ef\u00bf\u00bd\\u001E1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7K\u00ef\u00bf\u00bd\\u0001[\u00ef\u00bf\u00bd\u007fP\u00ef\u00bf\u00bd]m\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd`}U&k\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\\u001F\\\\\\u0000\u00ef\u00bf\u00bdi@`y\u0161a<\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdA5\u00c5\u00a0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017#.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\\u0005W\u00d9\u0160\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bdT<\\u0014'}q\\u00105\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001DO\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00cd\u00af\u00da\u00b8(\u00ef\u00bf\u00bdyL\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\\u000B\\u0019'/o\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bdUWKG\u00ef\u00bf\u00bd-q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u00ec- \u00b3\\fn\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000Bf\\u0010\\u001Cna\u00ef\u00bf\u00bdu\u00cc\u201e\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d9\u2021]\\bs\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bdMq\u00ef\u00bf\u00bd.o\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdYH\u00ef\u00bf\u00bd\u00d9\u00be\u00ef\u00bf\u00bd\u00cb\u00b6\\u001A \\t\u00ef\u00bf\u00bd4={zy}}y\n{\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd;;\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdPLqA\\u0000\u007f\u00ef\u00bf\u00bdZ=Yx=F\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\\u0012BfU6G\u00c4\u0152\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\\u0004\\u0017\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd('\\b\\u0012\\u0000\u007fb\u00ef\u00bf\u00bd\u00cf\u201e?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007X\\u0014#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d8\u00ac\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00d7\u0192\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bdW\u007f1\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd=\u00c3\u00aa\u00ef\u00bf\u00bdx|\u00ef\u00bf\u00bd~s>\u00ef\u00bf\u00bdN'\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdNMv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\\u001F/n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9O\u00ef\u00bf\u00bd\u00d7\u00bc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Di\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d1- \u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\\u0000ta&\u00ef\u00bf\u00bd\\nendstream\\nendobj\\n6 0 obj\\n<< \\n/ProcSet [ /PDF /Text ] \\n/Font << /F2 28 0 R /F4 27 0 R /F6 24 0 R /F7 23 0 R /F8 20 0 R /F9 17 0 R \\n/F10 12 0 R /F12 11 0 R /F16 8 0 R >> \\n/ExtGState << /GS1 7 0 R >> \\n>> \\nendobj\\n7 0 obj\\n<< \\n/Type /ExtGState \\n/SA false \\n/SM 0.02 \\n/TR2 /Default \\n>> \\nendobj\\n8 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/FirstChar 32 \\n/LastChar 181 \\n/Widths [ 278 333 402 556 556 889 667 227 333 333 556 606 278 333 278 278 556 \\n556 556 556 556 556 556 556 556 556 278 278 606 606 606 444 747 \\n778 667 667 722 611 611 778 778 389 389 722 611 944 778 778 611 \\n778 722 611 667 778 722 1000 722 722 667 333 606 333 606 500 333 \\n556 556 500 611 500 333 500 611 333 278 611 333 889 611 556 611 \\n556 444 444 333 611 556 833 556 556 500 310 606 310 606 278 278 \\n278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 \\n278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 \\n278 556 556 278 278 278 278 278 747 278 278 278 278 278 278 278 \\n606 278 278 278 611 ] \\n/Encoding /WinAnsiEncoding \\n/BaseFont /ONAEPM+Melior-Bold \\n/FontDescriptor 9 0 R \\n>> \\nendobj\\n9 0 obj\\n<< \\n/Type /FontDescriptor \\n/Ascent 745 \\n/CapHeight 692 \\n/Descent -252 \\n/Flags 262178 \\n/FontBBox [ -132 -252 1000 944 ] \\n/FontName /ONAEPM+Melior-Bold \\n/ItalicAngle 0 \\n/StemV 134 \\n/XHeight 472 \\n/CharSet (/S/parenright/acute/E/five/q/T/U/emdash/B/r/space/V/g/b/C/six/s/seven/W/\\\\\\nc/a/D/comma/t/eight/l/e/X/G/u/nine/f/I/H/period/v/colon/h/Z/J/P/w/F/i/L/\\\\\\nendash/d/y/n/zero/j/N/M/z/ampersand/one/k/O/A/two/m/quoteright/Q/x/three\\\\\\n/o/parenleft/R/question/K/p/four)\\n/FontFile3 10 0 R \\n>> \\nendobj\\n10 0 obj\\n<< /Filter /FlateDecode /Length 6823 /Subtype /Type1C >> \\nstream\\r\\nH\u00ef\u00bf\u00bd|TkP\\u0013Y\\u0016\u00ef\u00bf\u00bd&$\u00ef\u00bf\u00bdj\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\\u0013\u00ef\u00bf\u00bd\\u0006\\u0015\\u0007\\u0004Gp\\u0010\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010u\u00ef\u00bf\u00bd\\u0015\\u0005|\\u0004E\u00ef\u00bf\u00bdHL'\\u001D\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\\\"J\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdSXD\\u0011\u00ef\u00bf\u00bd5>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011u,kwF\u00ef\u00bf\u00bdN3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dd\u00a6\u00dc\u00aa\u00ef\u00bf\u00bd?n\u00dd\u00be\u00ef\u00bf\u00bd\u00de\u00ba\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\\u001C\u00c7\u00a9\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00da\"I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMIC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#&\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdNp\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ec'\"\\\\\\\"0rg\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\\u0012W\u00ef\u00bf\u00bduJ\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd'M\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00c7\u20ac\u00ef\u00bf\u00bd\u00c9\u00a9\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bdi)\\tIk7'\u00ef\u00bf\u00bdlt\u00d7\u00aes\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdiS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd9\u00da\"\u00ef\u00bf\u00bd\u00da\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003#$\u00ef\u00bf\u00bdH\\fs\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`X\u00ef\u00bf\u00bd\\u0018L\u00ef\u00bf\u00bda\\u0019\\u0018\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd|\\u0011\\u000Ef\u00ef\u00bf\u00bd\\u0011X\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdYx\\u0016~\\u001E\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bda\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd-Y)\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bdk_b\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd]\\u0016\\\"\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)d\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bdX1\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd|`\u00e4\u0161'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00d7\u017d!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdJN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZC5\u00ef\u00bf\u00bd\\u001A>*}\u00ef\u00bf\u00bd}z\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\\r\\u0017{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdcB\u00ef\u00bf\u00bd\\u0018T\\nU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:V]\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd[\\u0007\\u0013\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\\\\c^3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP!~1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\\u0019P\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdtA\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bdlJ\u00dd\u2122\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\\u0019K\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdr^w.\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP;\\u0004)\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ea\u00a6'\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdXgZg\\u0016'/Z\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u00a6m\u00da\u00b5\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\\u000FT\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~!\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdKd\u00ef\u00bf\u00bdfH)cD\u00ef\u00bf\u00bd\\u0017+}T\\u0001\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\#\u00ef\u00bf\u00bd\\u0014!\\b\u00ef\u00bf\u00bd\u00d0\u2020\\u0002!Py\\u001B&R\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd'-\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001DM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bduk\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bdd._\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdRK*>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bde0\u00ef\u00bf\u00bdFN\u00ef\u00bf\u00bd5KS\\u0011M\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\\u0004\\u0006\u00ef\u00bf\u00bd4\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd`_\\u001Ay\u00ef\u00bf\u00bd@\u00ce-<\u00ef\u00bf\u00bd\\u0004F\\u0005nY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdla\u00ef\u00bf\u00bd\\u001B\\u0004|\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd48sY\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\\u0005T~5\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd~z\u00ef\u00bf\u00bdzb\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdc&\\u0015hM;r*\\u00123Y\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bdx\\u001FL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\\u001E4x/xH\u00ef\u00bf\u00bd]/\u00e2\u00ab\u00af\u00ef\u00bf\u00bdjjN\u00ef\u00bf\u00bdZea\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd(U\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdeF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\\u0002Uz\\u001A\u00ef\u00bf\u00bd' \\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\\u0007\\u001A\u00ef\u00bf\u00bdEZ\\u001D\\nF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00d2\u20ac\u00ef\u00bf\u00bd\\u0015~\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018};V=\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd2\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\\u0015\u00ef\u00bf\u00bdU0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001*\\t\\u001C\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq+4\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c8...\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\\u0007\\r15\u00ef\u00bf\u00bd8D\u00ef\u00bf\u00bd$\\u0012]\u00ef\u00bf\u00bd\\rs\\u0017\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd2w\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bdB\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@.\u00ef\u00bf\u00bd^Pv\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007rO\u00d8\u0090Hd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \\u0003\\u0003DJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00d5\u00b5\\u0003*P \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\\u001A\u00ef\u00bf\u00bd0<h\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd.+,3\u00d7\u00b6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d9...K\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\\u001D-@\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/@\u00ef\u00bf\u00bd\\r\\f\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\\u0012I@\u00ef\u00bf\u00bdY 6\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd.\\u0012\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdOJ\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\\u0005\\u0005\u00ef\u00bf\u00bdT\u00de\u00b4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\\r\u00ef\u00bf\u00bd\\u0019\u00e3\u00a1\u201a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bdFn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000S\u00ef\u00bf\u00bd\u00c3\u203azp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bdr6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdlW\u00ef\u00bf\u00bd\\\\|\\u000B}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWzg2;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0y\u017e019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bdBS\u00ef\u00bf\u00bd1S9y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00eb\u00bc\u00ad\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bdoK\u00dc\u00bc\u00ef\u00bf\u00bd\u00d2\u0081\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\\f\u00ef\u00bf\u00bdi?\u00ef\u00bf\u00bd@\\u0013dAH\\u0011\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\\\"jsn\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd~-|\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdfs\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00034J\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdwR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd[a\\t\u00ef\u00bf\u00bdJ\u00de\u02c6M\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdiG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdDV\u00ca\u017dp\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd5\u00c5\u00a5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00179_\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd-L\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2]\u00ef\u00bf\u00bd^gd\\u0011\u00ef\u00bf\u00bd\\u0003\u00c7\u201e\\u00015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003|\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bdC4.ru\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5l\\u001E\u00ef\u00bf\u00bdH\u00e1\u0152\u00ac\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bdExf\u00ef\u00bf\u00bd(+~Ud\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd*a\\u0003}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>H\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bdx\\u0012\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\\\\4\\u0013%wj`\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u0160\\u001CWm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd^\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\\u001C~5\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Fa\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdEjd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\\\"\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~zW\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bdp\\u0010$\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00e4'\u02c6\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c8\u20ac%A\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c5\u00a07\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015r\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd~\\t\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u0081\u00ef\u00bf\u00bdl{\\u001D\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\\u0000o4\\n\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd&\u00ee\u00bc\u00a9+\u00ef\u00bf\u00bd!\\u0006B\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdog\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdah\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00dd\u00bc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdut,77,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdid\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\\nj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\u00c3\u20ac\u00ef\u00bf\u00bdL6A\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00da\u00b6[\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bdP\\u0014\u00ef\u00bf\u00bd>|\u00ef\u00bf\u00bd\u00d7\u00b5\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd2T(f D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_jT\\n=L\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:e=x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdBxI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00eb\u00bf\u00a7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bdp\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdf\\u0002Ug\u00ef\u00bf\u00bd9{\u00ef\u00bf\u00bd5.\u00ef\u00bf\u00bdB>>\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\\u0005\u00ef\u00bf\u00bdBL\\u001E+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\\u0004\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bdQB \\u00004\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\\u0018M\u00ef\u00bf\u00bd\\u0003\\u0003=\u00ef\u00bf\u00bdR\\u0012\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS*\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd@B\u00ef\u00bf\u00bdUy\\u000EH=x\\u0018a\\fu\\u001BH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ee\u00a8\u00adU\u00dd-\u00ef\u00bf\u00bd,nrJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019%\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\\n\\u000BJ\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\\bu\u00ef\u00bf\u00bd\u00e7\u00bb\u00af\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bdp\\u0003\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001AaG\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\\u000Fg>)p\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:X/\u00ef\u00bf\u00bd\\u0013^<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bdPM7\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdGR\u00ef\u00bf\u00bd=c\\u000Em\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bdqd\\\"\u00ef\u00bf\u00bd\u00c8- \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6S\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdA\\u0005\\u001Fh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ce\"n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdN>>\\n\\u0005,B\\u0013\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd-![\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u00af\u00ef\u00bf\u00bd\\u001Bm\\u0012\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!g-\\u001F\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\\n\u00ef\u00bf\u00bd+\\u001A`\u00ef\u00bf\u00bdKQ=\u00ef\u00bf\u00bd\\u001C\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd U\u00ef\u00bf\u00bdp\\rp\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bde(\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bdN$V\u00ef\u00bf\u00bdH2W\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Eh|\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\no\u00ef\u00bf\u00bds\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdUA\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00d6\u00a8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,l&$7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017!\u00ef\u00bf\u00bd\\\\|\\u0004\u00ef\u00bf\u00bd:#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\\u0015\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\\u001FU@\\u0001EPQDA\u00ef\u00bf\u00bd\\b(\u00ef\u00bf\u00bds\\u0003(\u00ef\u00bf\u00bd+\\t \u00ef\u00bf\u00bd\\bZk!Dq\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bdZT\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F3e\u00d6\u0161U\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bdYsnh\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9g\u00ef\u00bf\u00bdo\u007f\u00ef\u00bf\u00bd|{\u00ef\u00bf\u00bd\\u001BhY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdZI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bdP\\u0017\u00ef\u00bf\u00bdR\\u001B4W\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd`-\\fY\u00ef\u00bf\u00bd\\u0019\\t.\u00ef\u00bf\u00bd\\u001Fb4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007fs\u00ef\u00bf\u00bdR\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010}$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\\t\\u00118\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdEZw,\u00ef\u00bf\u00bd\u00d8\u01539U\u00ef\u00bf\u00bd\u00d3\u00be\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ea\"\u009d\\u0006\u00ef\u00bf\u00bd(\u007f w\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl%a\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\\u001D\\tK\u00ef\u00bf\u00bd\\t\\u0010\\b#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMdX\\rw\\u000Ff)mSgWN\u00d6'\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\\tI\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015u\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\\t\u007fN\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\nO\\u0005s\u00ef\u00bf\u00bd\u00d8\u2030\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d3'9\\u001B\u00ef\u00bf\u00bd\\u001FYi\u00ef\u00bf\u00bd\\u0011,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\\u0001\\u001B\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdO\u00d0\u00adc>\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ea\u203a\u00bba8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh`I:\\u0017\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd0\\u0014-D\u00ef\u00bf\u00bd$)\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp>\u00ef\u00bf\u00bd|;\\\\m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\\u001F\\fG\u00ef\u00bf\u00bd\\u001E\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2D\u00ef\u00bf\u00bdw`\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000Ep\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+J\u00d4\u201a[P\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c5\u00ba(8\u00ef\u00bf\u00bdD\\t:\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdiJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001DK<\\u0018\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\\u0016vlY\u00ef\u00bf\u00bd\\u0017\\u001Cd\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd/\\u001F\\u0016\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{q/g\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd.\\u0003\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e0\u00bb\u00a5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\\u0007\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<L\u00ef\u00bf\u00bd>\\b\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00c5\u2030p\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\\u0005\u00ef\u00bf\u00bd\u00d4\u0152%yI:2s3L\u00ef\u00bf\u00bd\\u00110\u00ef\u00bf\u00bd(\u00cc\u2021\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c9\u00ab\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\\u001D\u00ef\u00bf\u00bd\\u0006\\u0012/RIv\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSMBi3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd=^\\u0005F\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Ff\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdIE\u00ef\u00bf\u00bd8\u00da\"\u00ef\u00bf\u00bd0\\t\\u0007e\u00ef\u00bf\u00bdb\\u0007\u00ef\u00bf\u00bd8<\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd O\\\"^\u00ef\u00bf\u00bd|A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd|^`{L\u00ef\u00bf\u00bd}4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#)\u00ef\u00bf\u00bd/rvEY8\\u0012I\\u001A!\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdtF\\u0011\\u001D2x\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\\u000E{\u00db'S\\r^:\\u0007\\u0013=x!Z\u00ef\u00bf\u00bd\\u0000^9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdXf!o5\\u0011/\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd$P\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cc\u00b98jL#\\u0019\u00ef\u00bf\u00bd\\\\bC\u00ef\u00bf\u00bdDAd;\u00ef\u00bf\u00bd[\\u0019\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdyZ1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\\u000FT$\\u0003\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\\u0015\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\\u001Clb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bdcT\u00ef\u00bf\u00bdB3n\\u0016\u00ef\u00bf\u00bd#\u00c2\u00adjC\\u000B3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_{\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd+7\\u0015 \u007f|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\\u001AJ\u00ef\u00bf\u00bdi(IH\u00c4\u2030\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdgJ\u00ef\u00bf\u00bdOd\u00ef\u00bf\u00bdk'MR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd*J|I\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bdr(?\u00c7\u0160\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd!l\\to\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bdTok\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003;\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK<M\u00ef\u00bf\u00bdvE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00eb\u00a0\u00a9\u00ef\u00bf\u00bda\u00c9\u00a9}A\u00ef\u00bf\u00bdIDI8\\u0011m\\u0014R\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bdP4)\u00ef\u00bf\u00bdP\\\"NB\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\\u0007K\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\\\\j\u00ef\u00bf\u00bd\\u0014h\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw9q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2w\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\\u001A\\u0017\\u001F\\t\u00ef\u00bf\u00bd#b\u00ef\u00bf\u00bd\u00c2\u00a2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD##\\u001A\u00ef\u00bf\u00bdg\\u0014v\u00ef\u00bf\u00bd\\u0006\u00db...ZT\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\\u000B\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\\u001D(\u00ef\u00bf\u00bd)RQ\\u001AJ\u00ef\u00bf\u00bd?\\u0003L\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd3ZI\u00ef\u00bf\u00bd\\u0002\\u0007E(\u00ef\u00bf\u00bdVWAG\u00ef\u00bf\u00bd \u00c2\u0090\\\"\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0`\u00d1\u00b5y\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\\b-D4\u00d2\u00af\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd8\u00ef\u00bf\u00bd \\u0011[\u00ef\u00bf\u00bd\u00c7\u02c6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$8\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\\b\u00ef\u00bf\u00bd\\u0003\\u000F\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d0\u0192\\u000F;\u00ef\u00bf\u00bd}-\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd$#\\u0004\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\\u0006\u00ca\u00a6TI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00149\\u0006StU\u00ef\u00bf\u00bdbTYE\\u000F'\u00ef\u00bf\u00bdy\\t\u00ef\u00bf\u00bd*K\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013+&\u00ef\u00bf\u00bd7V}@4\\u00154 s\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c7\u20ac\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd67\u00ef\u00bf\u00bd\\\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpt\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bdl{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00074\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdjo\u00cd\u00aa\u00ef\u00bf\u00bd\u00df\u203a\u00ef\u00bf\u00bd{_\u00ef\u00bf\u00bd.u\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\\n\\b>\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd0G\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bdm\u00d9\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F3])\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/%\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd82\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\fh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd$>\u00ef\u00bf\u00bd}4\u00ef\u00bf\u00bd\\b4\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7(3\u007f\u00ef\u00bf\u00bd`M\\u001FX]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdPc\\u0017\u00ec\u00ba\u00be\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd{V\u00ef\u00bf\u00bd\\u0012N\u00ef\u00bf\u00bd\\u0001S\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdoK\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd,L'\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd-\u00d6\u00afF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\\u0014N\u00ef\u00bf\u00bdZ#q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdPb\u00cd\u2021\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHd\\u0005\\u0016V\u00ef\u00bf\u00bdvh57\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bds<\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd.!\u00ef\u00bf\u00bd4\\u0015s\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bdt(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpy;}/\u00ef\u00bf\u00bdu\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl0\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd%cb\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ce\u00a3\\u001Bc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bdkN\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk /\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bdy\u00ca\u009d_~\u00ef\u00bf\u00bd\u00db\u00a7\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f3A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\\u0019\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bdY6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bdzrr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u007f_I\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\\u000F\\u0006\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\\f\u00ef\u00bf\u00bdBH\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\\f\u00c8...\\u001DDQM\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bdW\\t\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^9\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQQA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;IU\\u0019,\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bdfX\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\\u001D\\\\\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd$H\u00ef\u00bf\u00bd#6*(ANQl\u00ef\u00bf\u00bd\u00d5\u00b8f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl;L\u00da\"$!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.ty\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bdvgS\\u0000Y\\u0001\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\\u001B?\u00ef\u00bf\u00bd9\\u0018\u00ef\u00bf\u00bd\u00c2\u00ac\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd]mHd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT3\u00ef\u00bf\u00bd\\nQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\\u0018\u00ef\u00bf\u00bdg-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca\u20ac\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy|\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c4\u00a2\u00ef\u00bf\u00bd8\\u0016\u00ef\u00bf\u00bd\\nt\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\\t\\u000B#H\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u00a9\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\\n\u00d3\u2020\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]mp2\u00ef\u00bf\u00bd-)\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~P\\u001C;K'U\u00ef\u00bf\u00bdC|\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\\u000B=\u00ef\u00bf\u00bdOi|\\u001B\u00ef\u00bf\u00bd2f2\\\"w3\u00ef\u00bf\u00bdJT$J)\\u001A\u00ef\u00bf\u00bd\u00c7\u0160\u00ef\u00bf\u00bdL(\\nA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00da\u0192\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B.\\u001F&\\u0006g\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\\b\\b\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \\n\\u000B(\\u000F\\u0001\\u0005\\u0002,\\u000E\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bdC+\u00ef\u00bf\u00bd\\u0014\\u0010\\u0004\\u0016\\u00140\\n>\\u0000\\u001F\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bdi\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\u00d4\u02c61\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ce\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u007f\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00da\u017d\\u0017\\u0012b_'\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00c8\u00af\\u0006\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\\u0004k:l\\u001F\\u0012[.\\u00021\u00ef\u00bf\u00bd\\u001F\\u001E)\u00ef\u00bf\u00bd&M\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00d3\u00a4\\\"o~\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd<4\u00ef\u00bf\u00bd\\u0002Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdqZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00cb\u00b5\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJJk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\u00d1\u2030J\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdcWX%\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dc\u02c6\u00ef\u00bf\u00bdUb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ce\u00a0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\\u0002S\u00ef\u00bf\u00bdw:8mW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bdI}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd)\\u0017p\u00ef\u00bf\u00bd,x\u00ef\u00bf\u00bdE\u00c7\u00a0\u00ef\u00bf\u00bde~\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd@/Z\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B,\u00ef\u00bf\u00bdu\\u0013\u00ef\u00bf\u00bd\\u0004\\u0001v\\b\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\\u0006\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bdr5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdGc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\\u001BX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[j\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003D\\u000EZr!0J\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1@\u00ef\u00bf\u00bd]}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd|s\\r8_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ce\u201a\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdgo])\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\\u0002\u00db\u00bb\u00ef\u00bf\u00bd{\\n\u00ef\u00bf\u00bd\u00c9\u00a7\u00ef\u00bf\u00bd9\\u0016\u00ef\u00bf\u00bd^L\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b><_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bdB_{\u00ef\u00bf\u00bd}\n)\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd$xI4\\u0011.\\u0011K5a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd.w\u00d6\u0153ne\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bdz:\u00ef\u00bf\u00bd@0\u00ef\u00bf\u00bd\\\\Dw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\\u0003\\u0002\u00ef\u00bf\u00bda-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\\u0016\u00ef\u00bf\u00bd\u00d8'\u00ef\u00bf\u00bd\\u0001\\u0019\\r_\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdc7\\t^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd':\u00d6\u00a0\u00ef\u00bf\u00bdVeI|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\\u0016$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd#m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdTL\u00ef\u00bf\u00bdJ\\u0013\u00ef\u00bf\u00bdi7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd \\u001E\\u0007\\u000BKx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002X\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c2\u201eh-\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSl9\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bdx!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\\u0018\u00ef\u00bf\u00bd!?\u00ef\u00bf\u00bdl\\u0005\\u0013E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\\u0005\\\"U\u00ef\u00bf\u00bd6\\b\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\\r\\u0012\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001AG7\\u0003\u00ef\u00bf\u00bde/x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010C9d_)\u00ef\u00bf\u00bd\\u000F\\t\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006/\u00ef\u00bf\u00bd\\u0010\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012;\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;e\u007fJ\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\\\"rb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd&G\u00ef\u00bf\u00bdH\\u0016\\u001A=\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bdYLr\u00ef\u00bf\u00bd2EB%gG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bdT\\u0014\\u001B\u00d9\u0153\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;r\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd,*+\\u0011\\u0017\u00ef\u00bf\u00bdj\u00cb\u00b35\u00ef\u00bf\u00bdX7\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000Fu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00c4\u00ab6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd>u\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQVV\u00ef\u00bf\u00bd+%\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdnAF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHg \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,i\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdyj'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\\u000B\u00ef\u00bf\u00bdh@q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdvI\u00ef\u00bf\u00bdf\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@}\n\u00ef\u00bf\u00bdz\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<G\\u00062G\\u0010\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\u00d6\u203a\u00ef\u00bf\u00bd\u00d3\u00a51\u00cc\u00ab\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00c7\u00b0\u00ef\u00bf\u00bdv\\u0002\u00ef\u00bf\u00bdX\\u0004\\f\u00ef\u00bf\u00bd\\u000Bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011e5\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\\u0012{\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006X\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO9[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d5\u2022v244mwXX\u00cd\u017e\u00ef\u00bf\u00bdd\\\"|8w\u00d3\u008f$\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd`r\u00c4\u00a8\u00db\u0090\u00ef\u00bf\u00bd\u00cd\u00a4\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c8\u0153x2\\u0012\\u0005\u00ef\u00bf\u00bdn\u00db\u0161\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bdU\\u001A\\u0006fT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bdb*\u00c2\"\u00ef\u00bf\u00bd'>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c8\u00b1\\u0001\u00ef\u00bf\u00bd[\\u0005\u00ef\u00bf\u00bdx~\\u000E\u00ef\u00bf\u00bd/~\u00de\u00aay:\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd4d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bdIb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdUM<\u00ef\u00bf\u00bd-h\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdM?~\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd] \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F]\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00c5\u00aab\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ee'\u02dc\u00ef\u00bf\u00bd\\u0004T\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd(rD\u00ef\u00bf\u00bd\\u0001[\u00c2\u00b6n\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d2\u00b9U0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZR\\u000B\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011!\u00ef\u00bf\u00bd($q\n{\u00ef\u00bf\u00bdxU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00c4'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK`LN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bdO/[E\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdIS\u00cf\u00a7\u00ef\u00bf\u00bd)k>\u00ef\u00bf\u00bdB$\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D(\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \\u0014\\\\\u00ef\u00bf\u00bd\u00db\u2039\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\\u0007\\\"\\u001B\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\\u001A\u00ef\u00bf\u00bdA\\u0013q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00d3\u00a2b\\u0019\u00ef\u00bf\u00bd\\\\\u00d8\u203a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bddCXZX\u00c8\u00ab&X\\u000F\u00ef\u00bf\u00bd\\u0002Nn\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bdf=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\\\\`\u00ef\u00bf\u00bd\\rQw/\\u001F\u00ef\u00bf\u00bd5T\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001(H\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdAb\u00ef\u00bf\u00bd>\u00d4\u00bf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdKx\u00ef\u00bf\u00bd\u00da\u201e\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u0152\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-i\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\\u0002Kh\u00ef\u00bf\u00bdd4v\u00dd\u00b8\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5w\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdAb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}G\u00ef\u00bf\u00bd+\\u001B9\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\\u001B<\u00cb\u00b1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd98\u00ef\u00bf\u00bd\\u001C:\\b\u00ef\u00bf\u00bdsR\\u0007.6Ax\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}Bj\u00ef\u00bf\u00bd\\u0002\n{\u00ef\u00bf\u00bdXSH-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdTu\u00ef\u00bf\u00bd*\\\\\\u0013\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd ;\u00dd\u2039\u00ef\u00bf\u00bd\u00dc\u2039\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004_\u00ef\u00bf\u00bd\\u001B|\u00ef\u00bf\u00bdfa2T\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdlcN\u00ef\u00bf\u00bd\\u0004>o#jA\\u0004y\\u000FP\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\\u0006\\u0002\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\\u0013y\u00ef\u00bf\u00bd9/_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdicc*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\\u001ED\u00ef\u00bf\u00bdy\\u0018\u00ef\u00bf\u00bd9]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d2\u008d\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd`\\tv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00e1\u00a5\u2014?\u00ef\u00bf\u00bdY=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSm,\u00ef\u00bf\u00bd\\u0012}\n]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@p\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u007fr#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\\u001E?\u00ef\u00bf\u00bd\u00c6\u00a2\\r\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd#\u00cd\u203aqy\u00ef\u00bf\u00bd`X\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\\u0010?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT2u\\u001A)8\u00ef\u00bf\u00bd/8\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007fh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd:D\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00d0\u00a0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bdx\u00cd\u00b1\u00ef\u00bf\u00bd:\u00c9- \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bdh \u00ef\u00bf\u00bd! 86)*\u00ef\u00bf\u00bd\\u0014W'o\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\nu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn1\\u0017NW\u00ef\u00bf\u00bdn/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$L\n{u\u00ef\u00bf\u00bd\\u001CHg/e\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00c4\u203a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn!\u00ef\u00bf\u00bd;4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014l\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bds$wP\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdY*\\u0018Bp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00c3\u009d\\f\u00ef\u00bf\u00bd6\\\\\\u001D\u00ef\u00bf\u00bda\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\\b!q\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\fmL\\u001B\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd%q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdaA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\\u0011\\u0016\u00ef\u00bf\u00bd\\u000B\\t7\u00ef\u00bf\u00bd!D\\u0016\u00ef\u00bf\u00bd-?\\u00137\u00ef\u00bf\u00bdD\\\\\u00ef\u00bf\u00bdY|\u00ef\u00bf\u00bdtN\u00ef\u00bf\u00bdo\u00e1\u00a6- \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc],g\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdUqo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb2\u00ef\u00bf\u00bd]\\u001E\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/0p\\u0006\u00c5' \u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd3\\u001Ff\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&#,h\\b\\u0007t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bdF\\t\u00ef\u00bf\u00bda\u00ef\u00bf\u00bdE`\u00ef\u00bf\u00bd\\u001A,\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bdmN\\u001C\u00ef\u00bf\u00bd'\u00de...\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\\u0007\\u0018\\u0017\\\\\u00ef\u00bf\u00bd\\u0005\\u001C\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdiKE%5\u00d5\u00a5-E\u00ef\u00bf\u00bd-H\u00ef\u00bf\u00bd\\\">[%\u00ef\u00bf\u00bdHy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd(*2\u00ef\u00bf\u00bdR`&\u00ef\u00bf\u00bdgWi*\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd*\\u0015y\\u0013+\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00c2\u017d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\\u001B@J\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI`|y\u00c9'3*ir9#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\\u001F\u00ef\u00bf\u00bd=\u00d6\u00ae\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bdt4:z^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca\u00a3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr><#N:\u00ef\u00bf\u00bdNz.~\u00ef\u00bf\u00bd\\u0001\\nEEA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(*\u00ef\u00bf\u00bd:q\u00c8\u00a8\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\\r\\u0002k\\u0002\u00ef\u00bf\u00bdZC\\\\\u00ef\u00bf\u00bdk/\u00ef\u00bf\u00bdOx(\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bdUP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZB\\u0012D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013a\\\\R\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010I\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u00ab\u00c9\u00a3\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd}\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdBV\u00ef\u00bf\u00bd+\u00d2\u2014\\u001A\u00ef\u00bf\u00bd<>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd _,\u00ef\u00bf\u00bd)\u00d8\u00af1\u00ef\u00bf\u00bd_;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\\\"\u007f\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx.\\u0004\u00dd\u00bc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdPs\u00ef\u00bf\u00bdX\u00d4\u0160\\u0012UI\u00ef\u00bf\u00bdDU\u00ef\u00bf\u00bd&Q\u00ef\u00bf\u00bd\u00c8\u008f|\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda5\\nM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd)S&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bdI%\\u0001\u00ef\u00bf\u00bd4I=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf6.M\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\\u001D\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd@18\\u0015\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bde\\u0014\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdy\u00c7\\u001D\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd(+\u00cf\u2022J^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015S\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:w\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bdw4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(a\u00ef\u00bf\u00bd}\\u00024@\u00ef\u00bf\u00bdg/\\u0001\\u0006\u00ef\u00bf\u00bd\\\\H\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e6\u00b6\u00ae\u00ef\u00bf\u00bdA@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd3}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\\u001D\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dc\u00ba\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u00ad\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bdVW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00de\u00bfu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd~/\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00d9\u00af\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\\u0015\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u00a9\u00ef\u00bf\u00bd\\u001D\\u001C\u00ef\u00bf\u00bd\\u0005\u00d9\u201e5\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\\n4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\\u001D\u00ef\u00bf\u00bd?:\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bde?V\u007f\u00ef\u00bf\u00bde|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u007fD\\u0017\u00ef\u00bf\u00bdZ6c\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!q\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bdy\\u0003k\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\\rP{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001FYS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN=4k\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\\u001F\\u0007D~&\u00ef\u00bf\u00bd\\u0002\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\\nendstream\\nendobj\\n11 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/Encoding /WinAnsiEncoding \\n/BaseFont /Helvetica \\n>> \\nendobj\\n12 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/FirstChar 1 \\n/LastChar 3 \\n/Widths [ 1000 278 820 ] \\n/Encoding 16 0 R \\n/BaseFont /ONAOEI+Gpospec5 \\n/FontDescriptor 14 0 R \\n/ToUnicode 13 0 R \\n>> \\nendobj\\n13 0 obj\\n<< /Filter /FlateDecode /Length 230 >> \\nstream\\r\\nH\u00ef\u00bf\u00bdTP\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd \\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\\u0013u\\u0000\u00ef\u00bf\u00bdj'\u00ef\u00bf\u00bd\u00c5'*\\u000Fi\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdl!\u00d5\u20ac\\u000E<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bdM\u00d5\u0081'\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bdFg\\u0013\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\\u0005\\u0017\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^U\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5&G7{\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\u00df\u00b3\\u0018\\u0013]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C|\\f\u00ef\u00bf\u00bd\\u001F\\u001F\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd+\\u0019$\u00ef\u00bf\u00bd\\u00168|4\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdqE\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`pf|8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\\u0004\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\\r\u00c6\u00a04\u00ef\u00bf\u00bdr\\u000BB/\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdXws\\\\\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\\u0010O\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD[\u00ef\u00bf\u00bd\\u0015D+\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJG+Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u00a5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bdux\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b0\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\\nendstream\\nendobj\\n14 0 obj\\n<< \\n/Type /FontDescriptor \\n/Ascent 0 \\n/CapHeight 0 \\n/Descent -183 \\n/Flags 4 \\n/FontBBox [ -13 -208 1013 627 ] \\n/FontName /ONAOEI+Gpospec5 \\n/ItalicAngle 0 \\n/StemV 1000 \\n/XHeight 504 \\n/CharSet (/space/b/l)\\n/FontFile3 15 0 R \\n>> \\nendobj\\n15 0 obj\\n<< /Length 234 /Subtype /Type1C >> \\nstream\\r\\n\\u0001\\u0000\\u0004\\u0001\\u0000\\u0001\\u0001\\u0001\\u0010ONAOEI+Gpospec5\\u0000\\u0001\\u0001\\u0001=\u00ef\u00bf\u00bd\\u000F\\u0000\u00ef\u00bf\u00bd\\u001B\\u0001\u00ef\u00bf\u00bd\\u001C\\u0002\u00ef\u00bf\u00bd\\u001C\\u0003\u00ef\u00bf\u00bd\\u0017\\u0004\u00ef\u00bf\u00bd\\u0019\\f\\u0003\u00ef\u00bf\u00bd\\f\\u0004\u00ef\u00bf\u00bd\\u001C\\f\\u0016~\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\\u0005\\u001E\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\\u0007\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\\u0002\\u000F\u00ef\u00bf\u00bd\\t\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\\u0012\\u0000\\u0002\\u0001\\u0001\\u0001\\tGpospec5\\u0000\\u0000\\u0000\\u0003 bl\\u0000\\u0000\\u0001\\u0000C\\u0000M\\u0000\\u0004\\u0001\\u0001\\u0004\\u0007+>\u00ef\u00bf\u00bd|\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd \\u0015\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bdr\\u0006\u00ef\u00bf\u00bdE^\\u0015\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\\u0018\\u0007\\u000E\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bdd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\\u0006\u00ef\u00bf\u00bd|\\u0006~r\u00ef\u00bf\u00bd\\u0006\\u000E\u00ef\u00bf\u00bd\\u001E\\u0018\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\\u001EI\\u001AX/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\\u0007\\u001E\u00ef\u00bf\u00bdT)G\u00ef\u00bf\u00bd\\f\\t\u00ef\u00bf\u00bd|\\n\u00ef\u00bf\u00bd|\\u000B\u00ef\u00bf\u00bd|\\f\\f\u00ef\u00bf\u00bd|\\f\\r\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\\u000F\\nendstream\\nendobj\\n16 0 obj\\n<< \\n/Type /Encoding \\n/Differences [ 1 /l /space /b ] \\n>> \\nendobj\\n17 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/Encoding 19 0 R \\n/BaseFont /Symbol \\n/ToUnicode 18 0 R \\n>> \\nendobj\\n18 0 obj\\n<< /Filter /FlateDecode /Length 253 >> \\nstream\\r\\nH\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd0\\f\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:$\u00ef\u00bf\u00bd\u00c2\u201e\\u0018\u00ef\u00bf\u00bdJ\\f\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\\na\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\\u0007zR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr>\u00ef\u00bf\u00bdv~\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\\u001D\u00ef\u00bf\u00bd}\nx+;\\f0j\u00ef\u00bf\u00bd<.v\u00ef\u00bf\u00bd\\u0012a\u00ef\u00bf\u00bdI\\u001B\u00ef\u00bf\u00bd\\u0005(-\u00ef\u00bf\u00bdqK*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dd\u00b6\\u0004\u00ef\u00bf\u00bd[3Z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\\u0007{\u007f\u00ef\u00bf\u00bdg`\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013A\u00ef\u00bf\u00bd:w\u00ef\u00bf\u00bd\\u0019M\\u0000\\u000EM\\u0003\\n\u00c7\u0152]o\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018h\\u0015.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{3!\u00ef\u00bf\u00bd<o\u00ef\u00bf\u00bdT\\r\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bdsY\u00ef\u00bf\u00bdW\\f\u00ef\u00bf\u00bd~=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b^\u00ef\u00bf\u00bd&\\u0002A \\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd)N\u00ef\u00bf\u00bd\\u0005'PR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bdc~l*b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#Z\u00ef\u00bf\u00bd\u00c3\u0090\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdM2\u00ef\u00bf\u00bd\\r>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\\u001Ft\u00ef\u00bf\u00bd_\\u0001\\u0006\\u0000\u00ef\u00bf\u00bd\\u000Fy\u00ef\u00bf\u00bd\\nendstream\\nendobj\\n19 0 obj\\n<< \\n/Type /Encoding \\n/Differences [ 1 /minute /mu /plusminus /space /underscore /bullet ] \\n>> \\nendobj\\n20 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/FirstChar 32 \\n/LastChar 181 \\n/Widths [ 278 333 500 556 556 1000 722 333 333 333 519 606 278 333 278 389 \\n556 556 556 556 556 556 556 556 556 556 278 278 606 606 606 444 \\n747 759 648 667 722 611 593 722 778 333 333 667 556 889 778 778 \\n611 778 667 611 667 778 722 944 722 722 667 333 606 333 606 500 \\n278 556 556 500 611 500 333 500 611 315 278 556 315 889 611 556 \\n611 556 389 444 333 611 519 759 556 519 500 333 606 333 606 278 \\n278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 \\n278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 278 \\n278 278 556 556 278 278 278 278 278 747 278 278 278 278 278 278 \\n278 606 278 278 278 611 ] \\n/Encoding /WinAnsiEncoding \\n/BaseFont /ONAFFC+Melior-Italic \\n/FontDescriptor 21 0 R \\n>> \\nendobj\\n21 0 obj\\n<< \\n/Type /FontDescriptor \\n/Ascent 745 \\n/CapHeight 692 \\n/Descent -252 \\n/Flags 98 \\n/FontBBoxy [ -110 -255 1072 927 ] \\n/FontName /ONAFFC+Melior-Italic \\n/ItalicAngle -12 \\n/StemV 82 \\n/XHeight 467 \\n/CharSet (/S/parenright/E/at/q/five/U/T/emdash/B/r/space/V/g/b/C/six/s/seven/W/c/a\\\\\\n/comma/D/t/l/eight/e/G/hyphen/u/Y/f/nine/I/H/period/v/colon/h/Z/J/P/slas\\\\\\nh/w/i/F/L/semicolon/d/y/n/zero/j/N/M/endash/z/one/k/O/equal/A/quoteright\\\\\\n/m/two/Q/x/three/o/parenleft/R/question/K/p/four)\\n/FontFile3 22 0 R \\n>> \\nendobj\\n22 0 obj\\n<< /Filter /FlateDecode /Length 7654 /Subtype /Type1C >> \\nstream\\r\\nH\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bdyP\u00ef\u00bf\u00bdh\\u001A\u00c6\"\\u000E\\tQ\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\\rBZ\u00ef\u00bf\u00bdFd<V\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd@\u00d4'Q\u00ef\u00bf\u00bduEQ\\u0010\\u0004\u00da\u00a3\\u0015\\u0006\\u0006Q\\u0014\\u0001\\u0005F\u00ef\u00bf\u00bd\\u001Al\\u0014EA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK<\\u0000\\u0005\\u0015\u00ef\u00bf\u00bd\\u0004\\u0005\\u0015\\u000FFXD\u00d4'\u00ef\u00bf\u00bd[gGk\u00ef\u00bf\u00bd0_o\u00cd\u2020a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d5\u0192cF\\u0012\\f\u00ef\u00bf\u00bdqf\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\\u0005\\u000B\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>P\\u001B\\u001Cj\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\\r\\u0018\u00d8\u02dc%\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\\u0012\\u0017FJ\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00cc\u00a5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI-\u00ef\u00bf\u00bd\\u0004\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<X\u00ef\u00bf\u00bd\u00c6\u00b0\u00ef\u00bf\u00bd\\u00144\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#G\u00df\u2022cF8n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdnU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd\\\\\\u0002\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d0\u0090\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd0m\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\\u001Dt0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007fS\\u0018.>\\u0018-\u00ef\u00bf\u00bdLL1\\u000E\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00cd\u00b5\u00c2\u201a1,\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv0#\u00ef\u00bf\u00bd\\u0018s\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8H\u00ef\u00bf\u00bdH\\u001E\\u0013\u00d3\u02c6|#k#o\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\\u001A2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdR;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\\u0010\u00d5\u0090\\u001BC}\n\u00ef\u00bf\u00bd>\\u001F\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd2NVl\u00ca\u02dc\u00e6\u2122\u00ad6\\u001Fc~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd;E\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00c3\u00bb\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd,oYqVwY%\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd0r\u00ef\u00bf\u00bd\u00c8\u00bb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Fu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bddN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\\u000FU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014k7k\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWP\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\\u0018+\u00d4\u0160\\\\\u00ef\u00bf\u00bd\\u0015\u00d4\u0152\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\\u0016o;\u00ef\u00bf\u00bd&T\u00ef\u00bf\u00bdqSIdI&\\u0017\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd~M!~\u00ef\u00bf\u00bd\\u001E!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00d0\u201e%\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\\r|\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00de\u00a7\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\\u000BH\u00ef\u00bf\u00bdSQU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cf\u008dp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00dc\u2030\u00ef\u00bf\u00bdy@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\\rd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b=\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd'6^\u00ef\u00bf\u00bdnj\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\\u0003>\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bdl\u00d9\u0153\u00ef\u00bf\u00bdE\\u0016.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00d5\u00aeJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd%\\u0011%\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\\u001E>\u007fY\\u0006\u00cf\u2021\u00ef\u00bf\u00bd\\u001DH#\u00ef\u00bf\u00bd\\u001Ax2Y\\u000FI\\u0014GE\u00ef\u00bf\u00bd'E\u00ef\u00bf\u00bd\\u00180\u00ef\u00bf\u00bd_\\u0003A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca\"\\u0015\u00ef\u00bf\u00bdJwd\\u0005\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\\u0019^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d0\u009d~m\\u000E^.\u00ef\u00bf\u00bd ^k\\u0019\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00d5\"\\u0001\u00ef\u00bf\u00bdO\\u000E)\u00ef\u00bf\u00bd)\\u0013:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d6\u2014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\\bC\u00ef\u00bf\u00bdOLsQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B/\u00ef\u00bf\u00bd9r\u00ef\u00bf\u00bd\\u0013V\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4L\u00ef\u00bf\u00bd\u007f\u00c4- \\u0017\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bdNy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\n{\u00ef\u00bf\u00bd>v\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\u0018\\u0016\\u0011\\u000F\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdFp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2!\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\\f=\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cc\u0090\u00ef\u00bf\u00bdq?\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSS\\f\u00ef\u00bf\u00bd!H\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdie\\bK\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010A\u00ca'\u00db...\u00ef\u00bf\u00bd38\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\\u0004`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\\u0014\\\\a1\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdfb<go\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&:}_\\u0006_\u00ef\u00bf\u00bdp0M\u007f\u00ef\u00bf\u00bdnH?S\u00db\u00a2\u00ef\u00bf\u00bd;\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHG~\u00ef\u00bf\u00bd\\fR0\u00ef\u00bf\u00bd\u00ce\u0081#\\u0019D\u00ef\u00bf\u00bdwDouS\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm?\u00ef\u00bf\u00bd\\u0018l_\\u0007\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMheZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,a'.\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cf\u0152\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdmAO\u00ef\u00bf\u00bd8\\u0018\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bdW)0Vwo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\u0014Dh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdsQI\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#|sj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\\u001E.\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bdc\u00d7'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`B\\u00034u\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd:\\u0002za!\\u00131;\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd}\n\\u0003s\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]N\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bdu`.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bdv!\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj 'S\u00c7\u00be\u00d4\u0192w^\\u0017\u00ef\u00bf\u00bdD\\u0019H\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00c3\u00a7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh=\u00ef\u00bf\u00bd\\u00005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\\n\\u001F+`L\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014R_\\u0013\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\\u0012s\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\\u001F\u00ef\u00bf\u00bd\\bd\\u0012=\u007f\\u0003R\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bdW\u00d8\u00acl\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bdh_d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00dc\u0161\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c4\u00b2c\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd'm\u00ef\u00bf\u00bd\\u0015\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd:x\\r\u00ef\u00bf\u00bd\\u0005o*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC!\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd[XU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\\u0018\u00c2\u20ac\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%W\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\\u0000\\u0007\\u0018\u00ef\u00bf\u00bdTL\u00ef\u00bf\u00bd>\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/]\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bdExx\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\\u0000D\u00ef\u00bf\u00bd\\n\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t?\\u0011\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd=jy\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8M\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004=e\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bdS \\u0007\\u000F0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\\u0019\u00ef\u00bf\u00bd-\\u0007\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd\u00c3\u00beRyk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\fO\u00ef\u00bf\u00bd\u00d6\u0160@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-gZzj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdP7g\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\\b\u00c2\u201e\u00ce\u00ac\u00ef\u00bf\u00bd\\u000F;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd#\\f\\u0011j$\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd<K\\u0003.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u2022o\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00130\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cf\u00bb\u00ef\u00bf\u00bd\\u0001\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\\u000BBP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2h\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bdae_\u00e3\u00ad\u00bc\u00ef\u00bf\u00bdJ>\u00ef\u00bf\u00bdBD\u00ef\u00bf\u00bd\\u0006d\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8~o\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdtT\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHv\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bdKpx\u00ef\u00bf\u00bd\\u0004lj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bdbA\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bdpn\u00ef\u00bf\u00bda(\u00ef\u00bf\u00bd\\u001Ex\u00ef\u00bf\u00bd\u00d6\u00afU`\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds}\n\\u0016O\u00ef\u00bf\u00bd\\u0011/\u00c5\u02dc\\u001F\\f\\u0013\\b\\u000139H\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006B\u00ef\u00bf\u00bdB\\fT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB}\\u0007\u00ef\u00bf\u00bd\u007f5p\u00ef\u00bf\u00bdL5\\b\\u0014U~\\u0005\u00ef\u00bf\u00bd_\\u0014yp[L7E\u00ee\"\u00bc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bdm~\\u001B6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=y\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bdLa\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdzAG\\bW\\u00049c\u00ef\u00bf\u00bd-\\u0016t\\u0014\u00ef\u00bf\u00bd4\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00d1\u00b4\u00ef\u00bf\u00bdP\\\"\\u0007# \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdLA\u00ef\u00bf\u00bdc\u00d0'\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u007f/\u00ef\u00bf\u00bd;q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdoc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:t\u00ef\u00bf\u00bdhF\\n\u007f\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00da\u00b2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d1\u00b2\u00ef\u00bf\u00bdY|0\u00ef\u00bf\u00bd&\\u0015\u00ef\u00bf\u00bd[P\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\\u0002h\u00cb\u0152,\\u0000U\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dd\u0153\\u0005\u00ef\u00bf\u00bdJ\u00c5\u00b3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003s\u00ef\u00bf\u00bd5;oV\u00ef\u00bf\u00bd\u00dd\u201a\\t}\nU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$^\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd.\u00c5...\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bdDF\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d3\u00bc_J\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca\u00ab\u00ef\u00bf\u00bdj\u00ce\u017e\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>hea.r\\u0000\\u0002M\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd!_\\u0014\\u00003\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\\u0006\\u000B0m\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c7\u00a4\u00ef\u00bf\u00bd\\bS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\\u0016\u00ef\u00bf\u00bd\\b\\u000F\\u00077\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00d0\u02dc\u00ef\u00bf\u00bd\u00d3\u0090\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e8\u00ac\u0153C)i)\u00ef\u00bf\u00bd|;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\\u0004\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012B\\u0011\\\\a\\n\u00d1\u00a2\u00ef\u00bf\u00bd\\u00191isb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\\n]er\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"I\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\\u000FD$\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00c6\u00ab4,\u00ef\u00bf\u00bd\u00ec\u0160\u00a6i\u00ef\u00bf\u00bdh3L\\u001B\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\\u0017\\u0012Q>T\\\\\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSAY\u00c2\u00a6\u00cd\u00ae\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bduU5(\u00ef\u00bf\u00bd\u00c2\u00a2(\\u001AV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bdvA@Qp\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bdFq\\u0003\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>&_\u00ef\u00bf\u00bdqf\\u0012\\rc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bds\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\\t\\u000F0O\\u001C=\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\feZ\\fV\u00ef\u00bf\u00bd\u00d5\u00b25\\\\\\rS\u00ef\u00bf\u00bd\u00d6\u02c6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdck\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd!\\u0018x\u00ef\u00bf\u00bd\\u0016jD\\u0015\\u0018\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cb\u00b5F\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bdQ|\\u0007{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\\r$\\u001y\u00bf19\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd!2\u00ef\u00bf\u00bd\u00cb\u0160T\u00ef\u00bf\u00bd\\u0005!D=W\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdsu\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU=KGg[+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU2B\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd,b\\u00171~\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdg\u00e3\u00a8\u2030mK\u00ef\u00bf\u00bd\u00db\u00b1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt%\\\\\\u0014~ rJ+\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd#u\u00ef\u00bf\u00bd)W\\\\\\u001E\u00ef\u00bf\u00bd\u00c2\u203a)^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdBj\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\\u0013^x\\u0002\u00ef\u00bf\u00bd\\u0005,@d\u00ef\u00bf\u00bd\\f\\r\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd1\\u0018@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bdt#^\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#.~\u00ef\u00bf\u00bdGQh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd d\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd75\\u0006\u00d9\u017d!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t\\u001Cs\\u000F7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdEZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00e1\u0178\u00a2Q{Qq\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd4{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\\u001C\u00ef\u00bf\u00bds\\u0017\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bde0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015X\\f#I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSh\u00ef\u00bf\u00bdj.3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cb\u201er\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00c5\u008dw\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd7<\\u001A\\r\\\\\u00ef\u00bf\u00bdJOB\u00ce\u02c6`\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bdj2A\u00cb\u00abMp\u00ef\u00bf\u00bd\\n\\n-{\\u000Bn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz-q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bdq g@\u00ce\u201a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bdu_[\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bdH\\n\u00ef\u00bf\u00bd\u00c5\u00af\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\\u0011[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5i\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ<\u00ef\u00bf\u00bdU~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd |LB\u00ef\u00bf\u00bdn5\u00ef\u00bf\u00bd$Z\n{!f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015o\u00dc\u00bf}\nm\\u0016\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\\u0016&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(C$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bdp!l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdVgF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[R\\u0002E\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00c6\u017d\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\\u001E\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000v\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(xz\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004gbM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\\u000B\u00cd\u00aa$\\u000Ff\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bdj`\u00ef\u00bf\u00bd%K\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bdE)\\u0006\u00ef\u00bf\u00bd6\\n\u00ef\u00bf\u00bd1\u00c9\u2039n!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u00bc\\t*\u00ef\u00bf\u00bd@&\u00ef\u00bf\u00bd\\u0015\\u0007\\u0017\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd2D\u00ef\u00bf\u00bdg`g2\\u000B9_\\u0012Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw@Cf_\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E_\u00ef\u00bf\u00bd/\\u0005)\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00c7\u00a7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx)\u00ef\u00bf\u00bdcy3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdYt\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u20302\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[N\\u0010Du\u00ef\u00bf\u00bd:l\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bdaM\n{s\u00ef\u00bf\u00bd\u00de\u00a1\u00ef\u00bf\u00bdwEn\u00ef\u00bf\u00bdWA)/\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bdN\\n\\u0007\\u0004\\u0005a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdLy\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@&P\u00ef\u00bf\u00bd\\u0012(b\\t\\u0014+^L\u00ef\u00bf\u00bdc\\u0016\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd1\\\"\\u000B\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bdl\\u001C\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdjX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u00ad\u007f \u00ef\u00bf\u00bd\\u0007:\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00c6\u2039\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\u00e3\u00bc\u00a4@p\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D8\\u0013\\u001F\u00ef\u00bf\u00bdIS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd'{|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\\n1\u00ef\u00bf\u00bd@\\u0014D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd65\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\\u0014>z.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D|\u00c2\"v\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd5m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\\f$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\\u001C\\u001F2\u00ef\u00bf\u00bd%L)\u00ef\u00bf\u00bd\\u0014\\u0019o-\u00ef\u00bf\u00bdZ^\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bdHa7\u00ef\u00bf\u00bd!r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\u001Dd\u00d5\u0160\\u0014\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018q\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\\u0005\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\\u0012I\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdduQk\u00ef\u00bf\u00bdrW\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d6\u00b1\\t6\u00ef\u00bf\u00bdL\u00dc\u2020\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\n{L\u00ef\u00bf\u00bd>n\u00ef\u00bf\u00bd,\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdXu\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdDnis\\u0001\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\\\"^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\\u0000g\\u0000c\\u0001\\u0017\u00ef\u00bf\u00bd[:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd^H;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\\u0014\u00d9\u00a1db\u00c6-\\u001D}(0\u00ef\u00bf\u00bd7\\u001Fd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\\u000B\u007f\u007f&\u00ef\u00bf\u00bdy\\u001F\\\"\\u0015|\u00ef\u00bf\u00bd&2p4P\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bdx6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd3\\\"R\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7N\u00ef\u00bf\u00bdZ?v\u00ef\u00bf\u00bdR\\fZ\u00ef\u00bf\u00bd\\u0019Yq0U\u00ef\u00bf\u00bd\u00d1'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\nL\u00ef\u00bf\u00bd\u00c3\u00a0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\\u00140\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00d2\u00bb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?j\u00ef\u00bf\u00bd~K%\u00ef\u00bf\u00bdQV\u00c5\u00b3\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdecm\\u001C-\u00ef\u00bf\u00bdc)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\\u0012K\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bdH:\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001FX!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(B\u00ef\u00bf\u00bd*b\\u0012\u00ef\u00bf\u00bd\u00cd\u00bf\u00ef\u00bf\u00bdW\u00d0\u00a6Sf\u00ef\u00bf\u00bdX\\u0004\\u0013\u00ef\u00bf\u00bdF0\\u0011\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd%X\u00ef\u00bf\u00bd\\u001B3\\f)\u00ef\u00bf\u00bd\u00e0\u00b1\u00bd\\u001A;GX\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bdJ\\u0016\u00ef\u00bf\u00bd3\\rq}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb0+\\r\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\\u0001\u00ef\u00bf\u00bdh@\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\\u0017_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018$\u00ef\u00bf\u00bd0\\u0007l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdtu\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd&`p\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdqS\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>HF!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg;\u00ef\u00bf\u00bdMO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\\u0014W\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\\t\u00ef\u00bf\u00bdhe\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00da\u00b5\u00ef\u00bf\u00bdih\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ea'\u00b2d\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd,a`~2\u00ef\u00bf\u00bd=\u00cb\u0081\u00ef\u00bf\u00bd;\u00e1\u2039\u00af\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI+\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdih'\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00db\u00b3\u00ef\u00bf\u00bdJ\u00d8\u00a0\u00ef\u00bf\u00bdf#\u00ef\u00bf\u00bdx?\u00ef\u00bf\u00bdDr\u00ef\u00bf\u00bdS\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015k\\u001B\u00ef\u00bf\u00bd\u00e6-\u2020\u00cb\u2021\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00cf\u00a5\u00ef\u00bf\u00bd6b\u00ef\u00bf\u00bd\u00ce\u00af\\\\\\u0015H\\u0005\u00d8\u00bf.\\r\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bdn+\u00ef\u00bf\u00bd\\u0002V2\u00c5-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=oH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'LWy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\\u0018(cT\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c9\u00b4\u00ef\u00bf\u00bd3T\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007i\u00ef\u00bf\u00bd\\u0007\\u000E&\u00ef\u00bf\u00bdpy\u00ef\u00bf\u00bd&\\u0003\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\\n!\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\\u0014~\\\"\u00ef\u00bf\u00bd\u007f$\u00ef\u00bf\u00bdyC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdjn\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd;q~P\\u0004\\u001D\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd^$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(J\\u0013\u00ef\u00bf\u00bdc5\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bdP\u00c7\"\u00ef\u00bf\u00bd\u00c6\u00a4s\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00d8\u201a\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u00ab\\u0014\\u000Ez^o(&[\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bdlL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bdrS\u00ef\u00bf\u00bd3+Y\u007f\u00c6\u2014\u00ef\u00bf\u00bda}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cc\u00a8\\t}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00da\u00b3}k\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd5O%rg\u00ef\u00bf\u00bd\u00c8\u203aA\u00ef\u00bf\u00bdAh\u00ef\u00bf\u00bd<\u00cb\u008d\u00ef\u00bf\u00bd\u00da\u2020\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdTgr\u00ef\u00bf\u00bd\u00d8\u00bf8\u00ef\u00bf\u00bd\\u0014e\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\\u0006\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\\u000F\\u001E\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\\u001F\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd`$\u00ef\u00bf\u00bdZ`\\u001A\\fEV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\\n_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bdd\u00d7\u00ad\u00ef\u00bf\u00bd'0-\\u0010}\\u0019$>\u00ef\u00bf\u00bd,\\u001Ej\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR&9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdKb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd0\\u0013\u00eb\u00bd\u008f\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013;\u00ef\u00bf\u00bd \u00ef\u00bf\u00bdZq\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u007f;^\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B^\\u0002\u00ef\u00bf\u00bdS)4\\tUD\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd 2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018Z\\u0010\u00de\u00b2\\u0010\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\\u0011[h\u00c3\u00bb# -ySt\u00ef\u00bf\u00bdi|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdvaL\\u0018+\\u000EN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd!T\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH)U\u00d2\u00b4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bdtf\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bdZ\\u001F\\u0003\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\":\u00ef\u00bf\u00bdPy\\u0005\u00ef\u00bf\u00bd\\u0000F\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b1!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMx\\b\\u0012\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010^! \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2J\\u0007\\u001F3Utl\u00ef\u00bf\u00bdR|\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd}]\u00ef\u00bf\u00bdYkN\u00ef\u00bf\u00bd\u00e9\u00af\u21225k\u00ef\u00bf\u00bd}~\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo`\\u000E~\\u00044\u00ef\u00bf\u00bd\u00c7\u00b9\\u001B\\u0018\u00ef\u00bf\u00bd\u00c7\u2039\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\\u0010.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u00b3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.}\\u001C\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd#m(\u00dc\u201a6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007fb\u00ef\u00bf\u00bd>\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\\u000F\\u00178m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dc\u00a4x\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bdS\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'h\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\\u001Eq\u00ef\u00bf\u00bd\\u001Fe\u00ef\u00bf\u00bd^\\u0010wK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm>\u00ef\u00bf\u00bdsW:\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1.c\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@]\u00ef\u00bf\u00bd^y\u00ef\u00bf\u00bd\u00de\u00a2\\f:\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\\u0010+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010E\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdzl\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdTz\u00ef\u00bf\u00bd`aq:\\u0013\u00ef\u00bf\u00bdD}\\u0002\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\\u0019\u00d1\u0161\\u0014%\\u0011d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d1- \u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\\uDB45\\uDFBBC\\u0018\u00ef\u00bf\u00bdXz9l\u00ef\u00bf\u00bd\u00c9\u00a0\\u001B\u00ef\u00bf\u00bd\\u000F?\u00ef\u00bf\u00bd\\u0002`\\u001DQ\u00ef\u00bf\u00bd^`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\\u0012L-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdcc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd)O\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00d1\u00a9+\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017i\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdDY\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bdH\\u001B\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9O\u00ef\u00bf\u00bd2X=t\u00ef\u00bf\u00bd9z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJY\u00ef\u00bf\u00bdT\u00ca\u00a4\u00ef\u00bf\u00bdRf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd09\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bdr\\n\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bds\\u0007\u00ef\u00bf\u00bd\u00d8\u2039(\u00ef\u00bf\u00bd\\u0017I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5U\u00ef\u00bf\u00bdd\\u0015.\u00ef\u00bf\u00bdPb\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdnj\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdjpm'\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdEp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\\u0010G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010d(22\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00d4'a\u00ef\u00bf\u00bd\\u00165\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00d4\u00ba\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u007f\\u001A\u00ef\u00bf\u00bdm@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bddJR\u00ef\u00bf\u00bdV\\n\u00ef\u00bf\u00bd\\u001A\u00d7\u00a2T\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{2\u00e2\u00bb\u00b1O\\u001AF\u00ef\u00bf\u00bd$\\u0006\\u001D!\u00dc\"\\u0017B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdzlL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00157\u00ef\u00bf\u00bd}\nM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\\u001F\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d8\u00bbn0\u00ef\u00bf\u00bdc]i\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo@qd\u00ef\u00bf\u00bd\u00e5\u00a2\u00a3\u00ef\u00bf\u00bdTE\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdsE\u00ef\u00bf\u00bd;O+\u00ef\u00bf\u00bdP\\u0010\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr B\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016u09\u00ef\u00bf\u00bdn\\u0005^\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bddm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bdd4\u00ca\u2030G{6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bdsK\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bdpUd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdDs\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\\n\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\\u001D\u00cd\u0152\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdAI\u00ef\u00bf\u00bdQ[YH\u00ef\u00bf\u00bd\\y\u00bcuD8EC\\uDE8B\u00ef\u00bf\u00bd\\u001Az\\uDAC7\\uDE1D\\u0001\u00ef\u00bf\u00bd\\u0012\u00c5\u00b9\u00ef\u00bf\u00bd\\rt9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bdG=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d1\u00a6\u00ef\u00bf\u00bd\\u000E3E\\u0002\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd[8?\\u0007&\u00ef\u00bf\u00bdu\n{B8\\b\\u001F\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdNA\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK_|\\u0018\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd64?\\\"\u00ef\u00bf\u00bdoD\\u0002W`@t25\u00ef\u00bf\u00bdf;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd8k\u00ef\u00bf\u00bd8D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#7\u00ef\u00bf\u00bdha\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-l\\u000B\u00ef\u00bf\u00bdWW\u00ef\u00bf\u00bd\\u0004\\u001DS\u00ef\u00bf\u00bda&9\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bdD+\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\\u0005]\u00ef\u00bf\u00bd&/\u00ef\u00bf\u00bdt\\u0011l\u00ef\u00bf\u00bd}\n\\u000EX\u00ef\u00bf\u00bd\u00ce\u00bbt\\u001B,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*h\\u0013\u00ef\u00bf\u00bd3;\u00ef\u00bf\u00bdzF]l\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00c9\u00a4\u00ef\u00bf\u00bdKE\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZf\u00ef\u00bf\u00bdz\u007f\\u001Fh\u00ef\u00bf\u00bd[\\u0015\u00ef\u00bf\u00bd\\u0016\\u0016+\u00ef\u00bf\u00bd\u00df\u00bc\u00ef\u00bf\u00bd7k\\\\WF4x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdERW\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(S\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdTpJ\u00ef\u00bf\u00bd_\\u001De\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd4\u00d6\u00a5\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00c2\u00b8n7\\b\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000r\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014T\\u001BQ\u00ef\u00bf\u00bde8\\\\\u00ef\u00bf\u00bdeL/I\\u000F>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\\u0015_@A\u00ef\u00bf\u00bdC\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006xK\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2;\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Au!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\\u0015S\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\\u0014\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd0K\\u0002\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bdNH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ca\u00bb\u00ef\u00bf\u00bdl^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY1&|\\u001A\\u00006\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u007f)\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdNWO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdbv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\\b\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0 5\u00ef\u00bf\u00bd \u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd J\u00ef\u00bf\u00bd^\\u001A\u00ef\u00bf\u00bd=\\u0017\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZPs8#A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdDiq\u00ef\u00bf\u00bdEPd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd_!x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012f\\u001B\u00ef\u00bf\u00bdB[v\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd4H\\b}\\u000B\u00ef\u00bf\u00bdR.:\u00ef\u00bf\u00bd}`\u00ef\u00bf\u00bd\u007f\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd&n\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd_\\u001C\\u0012\u00ef\u00bf\u00bd\\u000F1/'\u00db\u00a3U\u00c7'\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bdxg\u00e1'\u00ac\\bJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl/\u00d3\u00ab\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00c7\u00a2;N=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00c7\u0152\u00ef\u00bf\u00bd~H\\u00107?\u00ef\u00bf\u00bdn\\u001DeN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd~\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd2\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdPe\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc[j#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ce\u20ac\u00ef\u00bf\u00bd/\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5/H\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda:'\\u0015\u00ef\u00bf\u00bddDcWi\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd(\\\"P\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005*J\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\u0015k\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdE\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd3\u00e3\u00b3\u00a4\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016,\u00cb\u00b9\u00ef\u00bf\u00bd6 \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\\u0010\u00e9\u008d\u0161\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\\u0014k\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh^\u00ef\u00bf\u00bd*C\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bdl\\u000B\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00c2\u00b8\\u0002 ^n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd9\\u000Fo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000c\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\\u001CKd\u00ef\u00bf\u00bd\u007f\\u0014*\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u0192>\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017A\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd<M\u00ef\u00bf\u00bdJ.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd@l\u00ef\u00bf\u00bd_\\u000E8\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00cc\u0161\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\\u0004ka\\u0005\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd%j*\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\\r>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdm=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ce\u009dTj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00c5\u00b9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd!,\u00ef\u00bf\u00bdr/\u00eb\u02dc\u0160\u00ef\u00bf\u00bdx\\u0019=\u00d9\u0090\u00cb\u00aajHu\\rk3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdC\\u0005\u00ef\u00bf\u00bd\\\\\u007fB\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cf\u201e=\u00d0\u02c6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bdg\u00cb\u00ba\\n\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[Pm\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\\u0006\u007f!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd$Sa0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdtC\u00ef\u00bf\u00bdX8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}K\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdR\\u0014\u00ef\u00bf\u00bd\\bQiB\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\u007fqmw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\\u000FD\u00ef\u00bf\u00bd\\u001F\u00d5\u017e\\u0018zJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\\u0013^@\u00ef\u00bf\u00bdG\u007f\\\",\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT:v\\u001BS\u00ef\u00bf\u00bd6\u00ef\u00a3\u2020\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\\u000E\u00ef\u00bf\u00bdB\\u00033\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdjvP\u00ef\u00bf\u00bdke\u00ef\u00bf\u00bd\u00d2\u00a9j|\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bdFn\\u0000SjvQZ\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd]\u00d0\u21225\\u0005\\u001B\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHe\u00ef\u00bf\u00bdh4N\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F3V\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\\u0018\u00dc\u0178\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWN\u00ef\u00bf\u00bd\\b+\u00c6\u2020\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bdY\u00dc\u00a1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca'\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd$b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd=6\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/a\u00ef\u00bf\u00bd\\u001B<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdC\\u000B\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\\u0013f\u00c2\u00bag\u00ef\u00bf\u00bd\\u00006>_<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdVKE7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d5\u02c6\u00d8\u0160\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\n{N\u00ef\u00bf\u00bdz0\\f\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd>*\u00ef\u00bf\u00bdCr/\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdxq\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd?^2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bduV\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bdLA\u00ef\u00bf\u00bd2\\u0014\\u0014\u00ef\u00bf\u00bdV+6\u00ef\u00bf\u00bd\\u0015l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO8lL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c9\u00ae\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bdQ\u00cb\u00b8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6`q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdLf\\u0018\u00ef\u00bf\u00bd\\t2E@h\u00ef\u00bf\u00bdZ\\u000B}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\\u0002\\u0005m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\\u0005\\u0004\u00ef\u00bf\u00bdM\\u001E:Z^\u00ef\u00bf\u00bd<T\\u0004K\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdeK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~w\\u001E5;\u00ef\u00bf\u00bdf\u00c9'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00cd\u00b9\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\\u001C\u00ef\u00bf\u00bdZdl\\u0019u\u00ef\u00bf\u00bd\u00d9\u20ac\u007f\u00ef\u00bf\u00bd7\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u00e5\u2022...:\u00ef\u00bf\u00bd\n{\u007f\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd,\\u0017\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\\u0019\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\\u000FJ\u00ef\u00bf\u00bdVd\\u001EX\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bdwF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdV\\u0001\\rs\u00ef\u00bf\u00bdG\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00c5\u00bf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n6\u00ef\u00bf\u00bdg*\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\\u001A\\u0019P\u00ef\u00bf\u00bd\\u001CT\u00dc\u00ab\u00ef\u00bf\u00bdQ]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#4\u00da\u00b3\\u0015\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00d2-\\u000E\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bdg\u00df\u00ad\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00d7\u00a0j2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bdh\\u0002n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\u001F\\u001B\u00ef\u00bf\u00bd\\u0002f7cR}%\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd[\u00cc\u00ae/q`\u00ef\u00bf\u00bdF\u00e2\u2020\u00b5\u00ef\u00bf\u00bdM)5\u00ef\u00bf\u00bd\\u001E4\u00c8\u0160\u00ef\u00bf\u00bd\\u0019\u00dc\u0192\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd|\\u000FF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd[G\u00ef\u00bf\u00bd]\u00ce\u2039-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1P\\u0010\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00c6\u00a4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\\fc\\t\u00d2\u00a3d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\\u000BH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bdW$\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bddX\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN_\\u0013\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd]b\\u001AYh\u00ef\u00bf\u00bd\u00d2\u00bf\u00ef\u00bf\u00bdp\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00e9\u009d\u203a#\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)S\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bdtR\\u001E#]\\u0013\u00ef\u00bf\u00bd\\t\u00ef\u00a8\u00a8SQ\u00ef\u00bf\u00bd;B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\\tl\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd52\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd<}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn6\u00ef\u00bf\u00bd\\\"+$\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/Bp\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u00aa\u00ef\u00bf\u00bdh3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdGd\\u0016\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00de\u0161\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\\u0007oO\u00ef\u00bf\u00bdj\\u0002DEM\u00ef\u00bf\u00bd^O\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000Ez\u00ef\u00bf\u00bdw!N\\u0000Az\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\u00d0\u00b5*r\\u0015X\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00da\u00a3\u00ef\u00bf\u00bd\\u000Bj\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^+\u00ef\u00bf\u00bd7w\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\\u0016\u00ef\u00bf\u00bdp\\u0012\u00ef\u00bf\u00bd\u00db\u2030c\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018v\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSR\u00ef\u0178\u00a5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bddD\\u0005s\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdT\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c9\u00bdQ\\\\\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd,,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017B\n{\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd;z\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\\u001B&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00de\u00a1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00eb\u00a8\u00aa\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000Fk\u00ef\u00bf\u00bd_Co\u00ef\u00bf\u00bd2d1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c7\u00ba}\nwY\\n\u00ef\u00bf\u00bdx\\u0002\\u0007Z<p\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d9\u00bc\\u0006*\u00ef\u00bf\u00bd^jCt\\u000F\u00ef\u00bf\u00bdEIw\\u001C\\u000F!\u00ef\u00bf\u00bd\u00c8\u20ac\u00ef\u00bf\u00bd\u00da\u00b1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bdr9S\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-s\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd5w9\\\\%T;Z\u00df\u201a$>\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bdBB\\t\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bdl\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\\\"9\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ec\u008f\u0178\u00ef\u00bf\u00bdwv\\\\u{\u00c2\u00a9F\\\\3#6\\u0006#o\\fV]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u00aa)eFN\u00ef\u00bf\u00bdGi\\\"^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00c9\u00a2\\\\2\\u0013%g\\\"\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\\u0002~c\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\\t\u00ef\u00bf\u00bd\\u001188\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bdoT\\u001D\u00ef\u00bf\u00bd\\u000Fj\u00ef\u00bf\u00bd\\u001Dp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00103\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd)/\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012a#\u00ef\u00bf\u00bd_\\\"\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2`\u00ef\u00bf\u00bd\u00c2\u00ad;\u00ef\u00bf\u00bd_\u00dd\u00a0:<\\u001Dl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!Q[\\\"\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{N\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n lne\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\\u0007B\u00ef\u00bf\u00bdv~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0e\\u000E\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\\fn\\u001F> \u00ef\u00bf\u00bdL\\\"\u00ef\u00bf\u00bd]8\\u0018\u00ef\u00bf\u00bd\u00c3\u201aj\u00ef\u00bf\u00bd\\u0010,)\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bdME\u00da\u00a6^\\u0003]\u00ef\u00bf\u00bd`4x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>i\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd33\\u0003r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\\u000B\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\nG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdM0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e5\u00ac\u00b5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\\u0015\\ra\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!I'J\\t\u00ef\u00bf\u00bd\\u0014\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd6D\\u0004\\u0003\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdFI\\u0001\u00ef\u00bf\u00bd_]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bdr/\u00ef\u00bf\u00bd\\u000E\\u0011\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bduiZ\u00ef\u00bf\u00bd\\n1K\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00d0\u00b0\u00ef\u00bf\u00bdDO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.//1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00d9\u201eA(\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd{Old\\u0012\u00ef\u00bf\u00bdV\\u001F>6\\t\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~N\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bdg\\u0000G\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\nendstream\\nendobj\\n23 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/Encoding /WinAnsiEncoding \\n/BaseFont /Helyvetica-Bold \\n>> \\nendobj\\n24 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/FirstChar 32 \\n/LastChar 240 \\n/Widths [ 278 333 371 556 556 1000 667 208 333 333 556 606 278 333 278 389 \\n556 556 556 556 556 556 556 556 556 556 278 278 606 606 606 444 \\n747 778 667 667 722 611 611 722 778 333 333 722 611 944 778 778 \\n611 778 667 611 667 778 722 1000 722 722 667 333 606 333 606 500 \\n333 500 556 500 611 500 333 500 611 333 278 556 333 889 611 556 \\n611 556 389 444 333 611 556 833 556 556 500 333 606 333 606 278 \\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 \\n0 0 556 556 556 0 628 0 0 747 0 0 0 278 0 0 278 606 278 278 0 611 \\n278 278 278 278 278 0 0 278 0 0 0 0 0 278 0 278 278 0 0 0 278 0 \\n0 0 0 0 556 1000 0 0 278 278 0 278 0 0 167 0 0 0 0 0 0 0 0 0 0 0 \\n0 0 0 0 0 0 0 0 0 0 278 ] \\n/Encoding /MacRomanEncoding \\n/BaseFont /ONAFBK+Melior \\n/FontDescriptor 25 0 R \\n>> \\nendobj\\n25 0 obj\\n<< \\n/Type /FontDescriptor \\n/Ascent 745 \\n/CapHeight 692 \\n/Descent -252 \\n/Flags 34 \\n/FontBBox [ -135 -252 1000 927 ] \\n/FontName /ONAFBK+Melior \\n/ItalicAngle 0 \\n/StemV 82 \\n/XHeight 467 \\n/CharSet (/colon/S/parenright/h/semicolon/U/i/endash/asterisk/V/j/less/g/W/k/equal\\\\\\n/comma/K/X/m/l/hyphen/o/n/Y/question/period/fraction/Z/p/paragraph/at/sl\\\\\\nash/P/q/bracketleft/B/T/space/r/zero/C/A/one/s/section/D/bracketright/tw\\\\\\no/t/a/G/three/u/numbersign/I/H/N/x/v/four/E/J/dollar/five/w/quoteleft/F/\\\\\\nL/percent/emdash/six/y/d/b/M/ampersand/seven/z/c/O/quoteright/eight/e/Q/\\\\\\nnine/parenleft/f/R)\\n/FontFile3 26 0 R \\n>> \\nendobj\\n26 0 obj\\n<< /Filter /FlateDecode /Length 7679 /Subtype /Type1C >> \\nstream\\r\\nH\u00ef\u00bf\u00bdTT\\u000BT\\u0014G\\u0016\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\\u0018F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bdHT\\\\\\u00110\\u001A]A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\\uD97E\\uDE28\\u0003\\u0006\u00ef\u00bf\u00bdA\\u0018\u00ef\u00bf\u00bdg@\\u0001\\r\u00ef\u00bf\u00bd(\\n3\u00c3\u20ac\u00ef\u00bf\u00bd\u00c5\u20ac(\\f~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQPq!*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001AD\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\\u001AP1\u00ef\u00bf\u00bdr<y\u00ef\u00bf\u00bd\u00c5\u017e\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00d9\u00b3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00de\u00bbU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bdq\\\\\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd%n\u00ef\u00bf\u00bd6GG\u00ef\u00bf\u00bd\u00c5\u008f\u00ef\u00bf\u00bdx\\u000B\u00ef\u00bf\u00bd1A\u00ef\u00bf\u00bd\\u000BN\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdDc$\\\"\u00ef\u00bf\u00bdYb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ce\u02c6\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(%\\u0014x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdUc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\\\"\\u001Cw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(Ed\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00c4...+|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#7\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\\u0010\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007fIa6c0\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1,\\u000E\u00ef\u00bf\u00bd\\u00120L\u00ef\u00bf\u00bdaJs\u00ef\u00bf\u00bd%Fa\u00ef\u00bf\u00bd\\r~\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\\u000BA4S\\u0014*J\\u0012\\u0015\u00ef\u00bf\u00bd\u00ce\u0160\u00ef\u00bf\u00bd,g[\\u0016\\u0011\\u001ED\\u0006\u00ef\u00bf\u00bd\\f'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8R\u00ef\u00bf\u00bd\u00cb\u0178\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd4\u00db\u00a8l*m\u00ef\u00bf\u00bdJ\\r\u00d2\u00aeQI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdFO\\u001Bm\\u001C3sL\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{v\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00c7\u00be\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u00ba\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd|\u00ce\u201a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019y1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\\\\R\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bdpx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<A\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdIH\\u0013\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cf\u00b8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`K\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bdD~\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*2\\u0006\\u0005\\u0012\u00c8'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd(V\u00ef\u00bf\u00bd`A&\\\"[\\u0002\u00ef\u00bf\u00bd\\u0003\u007f'\u00ef\u00bf\u00bd 5\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd@M\\u0000k\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\\u0015#\\nN\\\"\\u000E\\u00161I#~,i4\u00ef\u00bf\u00bd)M?+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00df\u2030x\\u0002V\u00ef\u00bf\u00bd@>\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006S\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd5\u00c8\u0161E6\u00ef\u00bf\u00bd1o\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd4Ew\u00ef\u00bf\u00bdE\u00df\u017e\\u0002&\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\\f}\nI\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*?1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\\t\u00c4\u00b1\\u0017\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ.\u00ef\u00bf\u00bd\\b\\u0001\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\\flS\\\\\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\~bM\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bdN1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d2\u00b8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00e5\u00a2\u00bd\u00ef\u00bf\u00bd\\u0014\u00ce\u00a7\u00ef\u00bf\u00bd\u00c2\u008d\u00ef\u00bf\u00bdoG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\\u0017,\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\\u0011I7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E5\\u0011Jr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\\u0013!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007V\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c7\u008d\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\\u0001Y\u00ef\u00bf\u00bd.\\u001F\u00ef\u00bf\u00bdJ-g\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh5\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c9'\\u0019\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\\b\u00ef\u00bf\u00bd}\n\\u0004\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\u00c8\u00ae\u00ef\u00bf\u00bd]|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013L\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\\u0013a\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\\\\.7\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd\\n\\t\u00ef\u00bf\u00bd\\tPs\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd7/]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd|\\u0015e\u00ef\u00bf\u00bd\\u0016\\u001Bv\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bdC'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bds,\u00ef\u00bf\u00bd\u00d4\u00ba0\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\u00d5'\u00ef\u00bf\u00bdTSV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\\t\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00db\u02c6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u00a1\u00ef\u00bf\u00bdrk\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00c9'\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd,]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dc\u00aa\u00ef\u00bf\u00bdM\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd=M\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cc\u017e\u00ef\u00bf\u00bdDD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bd\u00df\u00aa\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012V\u00ef\u00bf\u00bda$\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bdAW\u00e8\u2022\u00b4\\uD957\\uDC0A\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd?\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm5\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd2T\\u0015\u00ef\u00bf\u00bdS,\\u001D\\u001E=\u00ef\u00bf\u00bd*\u00c3\u00bb\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\\u00184\u00ef\u00bf\u00bdF>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\\u000B~\u00ef\u00bf\u00bd`%L\u00ef\u00bf\u00bd\u00c9\u2022Q(y\u00ef\u00bf\u00bdQT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00e4\u2022\u0160J )\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bdT\\fLX\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bdD~,r\u00ef\u00bf\u00bdG)hB\\u0000\\u0017GARQ*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\\u000B\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\\u0010%\\u001F3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!X\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00d3\u203ah\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010=\u00ef\u00bf\u00bd\\u0004<\\b\\u001F\u00ef\u00bf\u00bd\\u0004y \u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf1x\u00ef\u00bf\u00bd!+\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bdV=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D<\u00ef\u00bf\u00bd\\u0011f~z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bdm\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00e6\"\u00b31h\u00ef\u00bf\u00bd:e\u00ef\u00bf\u00bd\\u0013D<\u00ef\u00bf\u00bdkA+\u00ef\u00bf\u00bd\\u000B\\u0010\u00ef\u00bf\u00bd<@Z_\u00ef\u00bf\u00bd\\fO\\u0007\\u0007\u00ef\u00bf\u00bd\\u000B,E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00c6\u2020\u00ef\u00bf\u00bd\\u001Bc\u00ef\u00bf\u00bdy)z4\\u0014Y\u00ef\u00bf\u00bdWA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\\u0000\\u0018\\u0006D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c2\u00a09Eh2\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd`.\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bdD\\u0014\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bdh\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00d8\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdGJ\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd|\\b~\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bdUB\u00ef\u00bf\u00bdM\\u001C\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd`B\u00ef\u00bf\u00bdV\\u0004y\u00c7\u2039\u00ef\u00bf\u00bdPw\\u000Bo]m\u00ef\u00bf\u00bd \\u001E\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdde\\\")\\u0017I=\u00cc\u00be\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}YwY\u00ef\u00bf\u00bd6RP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B%\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd%\\u0018M\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\\u0010\u00da\u00bf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd6w\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdvp\\u0007k\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bdrR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\\fN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd.O\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/o\u00ef\u00bf\u00bd\u00d8\u00b0\\u0014\u00ef\u00bf\u00bd \\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u00bb\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd%L\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A2}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8/\u00ef\u00bf\u00bd\\u0018m7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00da\u00b3\u00ef\u00bf\u00bd[%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00c3\u00abY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdkX\\u000Bw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdbC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd>4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018\\u001D\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bdx\\u001FC\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\\r\u00ef\u00bf\u00bd>U\u00dd\u009dy\u00ef\u00bf\u00bd7Ptt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2K\u00ef\u00bf\u00bdH.M{0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd03/O[\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdRv\u00ef\u00bf\u00bd\\fq\u00c7\u00be\u00ef\u00bf\u00bd \u007fvbL\u00ef\u00bf\u00bd{\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdRm%L\u00ef\u00bf\u00bdyu\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdqhy*\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd3(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd$P\u00cd\u00b0=\\u00035^d\u00ce'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\\r\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bdVwQ\u007fC2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd8t\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd_\\u0010=\\r\u00ef\u00bf\u00bd>C.\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bdoj\u00ef\u00bf\u00bd\u00eb'\u00b5<_\u007f\\tv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd3r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\\u001Bu\u00ef\u00bf\u00bd?d:\u00ef\u00bf\u00bd\\u001B\u00cf\u0153\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00c3\u00a2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bdt\u00c7\u0153\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'=\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq;p`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bdxZ\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\\u001A\\u0011\u00c9\u203a{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*( rM\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd{+\\u0012\u00ef\u00bf\u00bd\\n;\u00ef\u00bf\u00bdBm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd's\u00ef\u00bf\u00bdUd-G\u00ef\u00bf\u00bd(\u00e4\u201a\u00bc\u00ef\u00bf\u00bdO\\u000E\u00ef\u00bf\u00bd\\r\\u001AP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\\u0013\u00ef\u00bf\u00bd~Q\\u001BrX\\u0011\u00ef\u00bf\u00bd3,\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdoN\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cf\u00ba\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdvl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\\u0013\u00ef\u00bf\u00bdu\u00da\u00b3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT_,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\\\"a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\\u0002V\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\\r\u00ef\u00bf\u00bd\\b4\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010d(Kl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd- \u00ef\u00bf\u00bd!L\\u0005\\u0011x\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000=\\be\u00ef\u00bf\u00bd\u00c5\u00bc:\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd-L9sVX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\\u0005\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\u00da\u00a8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000EqkY\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd>d\\u0018\\u001Eb@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bdW!?z\\u001Ayzr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ce\u00bb\u00ef\u00bf\u00bdMR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\\u000BG\\u001F\u00ef\u00bf\u00bdL\\fY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d2\u2021\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\\u0010d\\u001F\u00ef\u00bf\u00bdiC\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c6\u203a\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd_\u00c6\u00ab5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\\u000B\\u0013Cr\u00ef\u00bf\u00bde\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdDs\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\\u0015\\u0015\\u0005\u00ef\u00bf\u00bd\\u0015\\u0011\u00c5\u00b1\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bdRa\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\" \\u000FA^I\u00ef\u00bf\u00bdM\\u0000\\u0011\\u0003X\u00ef\u00bf\u00bd\\u0018\\u0014,\\u0006\\f\\\"\\bC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017Z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd8\u00da\u2030\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZs\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdPiW\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~}\n{\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`>Fh\\u00148n\u00ef\u00bf\u00bd/N\u00ef\u00bf\u00bd!g\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00cd\u00acV%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdTVU\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bdS#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00c9\u00b3~\u00ef\u00bf\u00bd[a7\u00e4\u00b0\u00be\\u0012(\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSb~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e9\"'\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd~\u00c2\u009d[\u00ef\u00bf\u00bd~}\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\\u0006\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bdyG\\\"\\u0013/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bdt\\u0018<\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\\u0004\\bo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015o\u00ef\u00bf\u00bd\\\"\u00d7\u01522Q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\\u0016:\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bdBX#\u00ef\u00bf\u00bd%9KY\u00ef\u00bf\u00bd1U\\u0012H\\t\u00ef\u00bf\u00bd/GW\\u0018v\\u000E\u00ef\u00bf\u00bd\u00cf\u2030N\\n\u00ef\u00bf\u00bdXh\\u0015\u00ef\u00bf\u00bd\\u0018+\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bdm\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN~\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd/8~\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\\u000FJ\u00ef\u00bf\u00bd\\u0005\u00cc\u0192\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cb\u00b2's\u00ef\u00bf\u00bdgN\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdRU\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^)\u00ef\u00bf\u00bd\u00db\u008d<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca\u00a0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c3\u00a3<v\u00ef\u00bf\u00bdtf\\u0019\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00c8\u2039vW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdYZ\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00df\u0090n\\u0002\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:n\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bdJ!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\tN1\\u0019{:a\u00ef\u00bf\u00bdu\u00c5\u00a1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e9\u00b0\u0178QI\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\\u000F\\u0007\u00c9'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\\u001C\u00ef\u00bf\u00bd\u00d5\u00b9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00c3\u2030Z6\\\\\u00ef\u00bf\u00bd\\u000F7\u00c9\u0178\\u0003\\u0003\u00ef\u00bf\u00bd`2,\u00ef\u00bf\u00bd90\\b}p=\u00ef\u00bf\u00bd@O\\f\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\\u0013I\\u001E\\r\u00c3\u00af\\u0002u\u00ef\u00bf\u00bd])\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c9\u0152O]\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?,'\\u001F\\u0005\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\\u000BvHfYt\u00ef\u00bf\u00bdp[\\u0015\u00ef\u00bf\u00bd[S\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdPf\\u0016\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd_\u00ef\u0160\u008f\u00ef\u00bf\u00bdKM\\u001A\u00ef\u00bf\u00bd-&172G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdqu|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdTT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdDQ^in\\u0011\n{\\u0016\\u0016I S\u00da\u008d\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY[\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00c9\u00b9\\u0011-\\fF\u00ef\u00bf\u00bd\u00ea\u00af\u00ac\\u0017\\u000B%\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bdeL.\u00ef\u00bf\u00bd\\u0003\u00c7\u0153\u00ef\u00bf\u00bdd\u00d1\u017e^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\\\"v\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi#\\u000E\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E6%(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdhk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\u00cf\u00a8\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd8F\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>bR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\\u000E\u00ee\u00ad\u0161\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\\\"\u00ef\u00bf\u00bd\u007f6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdNA\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.0eb1\u00d2\u0160\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bdN\\td\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|TT\u00ef\u00bf\u00bd\\\\X\u00ef\u00bf\u00bd!\\u0016\u00ef\u00bf\u00bda-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdIa8U\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bdI|<\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\\u0018f\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bdbV\\u001B#i\\u0007O\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\\u0001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\\u001E)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[8\\u0002r\\u001A\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\\u000BK(\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdz\\u0001\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA?\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd9.\u00ef\u00bf\u00bdm\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\\u00187l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk#X\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\\f\u00ef\u00bf\u00bdQ_\u00ef\u00bf\u00bdz\\u001C\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\\u000EJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0x\u00ef\u00bf\u00bdE\u00c5\u00a7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00d4\u00b2\u00ef\u00bf\u00bd\\u001Ci\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd;g\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bdv\\u0003T\\u0019\u00ef\u00bf\u00bd@l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdkmtj\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e0\u00b8\u0152\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\\\\\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bdu)8l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\\u001B!3T\u00ea'\u0153\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\\u000FT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014h.\\r\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\\r|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c9\u00a4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd!DR\u00cd\u00af\u00c6\u20acf[\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,d_\u00ef\u00bf\u00bd^\\u0012\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e7\u201a\u00bb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.Z\\u0010\u00ea\u00ba\u2122\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bdar\\f\\r\u00ef\u00bf\u00bd\\u0011$\\u0012\\u001F4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdz`k\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;n\u00ef\u00bf\u00bdP\u00c8\u00baw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\\u0005\u00ef\u00bf\u00bd,T\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\\u0014_\u00ef\u00bf\u00bdU\\u001C\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00c4\u008dj\u00ef\u00bf\u00bd3\\u0017\u00db\u0153\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001,)\\u0019\\u0003t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\\b\\u001C\u00ef\u00bf\u00bd`1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$WQI-OT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015R\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bde,)f\\u000B\u00ef\u00bf\u00bdS\\u0017\u00ef\u00bf\u00bd5W^\u007f\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdLz\u00ef\u00bf\u00bdT\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdpr$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc/30\u00ef\u00bf\u00bd\\u0001^e\u00ef\u00bf\u00bds'\\u0010\u00ef\u00bf\u00bdQ\\u0004g\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\\n\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd0\\u001C\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\\u0007\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\\u0002\\u0011\u00ef\u00bf\u00bd\\u001B\\u0010\\u0003\\u001B\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\\u0010e\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\\u0010W\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd+G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9[jc\u00ef\u00bf\u00bd<\\u001EN{:\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdwv\u00cb\u00aa\u00ef\u00bf\u00bdhu4G\\u0016\\u001F\u00ef\u00bf\u00bdmrp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\\u0019\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX/\u00ef\u00bf\u00bdlp\\\"\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\\u001E)\\u0018!\u00ef\u00bf\u00bdV\\u0002n\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bdDh\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cb\u0160\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\\u001E\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\fv\\u0002\u00ef\u00bf\u00bd$\\\\0\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdvp\u00ef\u00bf\u00bd2k;V\\b\u00ef\u00bf\u00bdhuL\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\\u0001\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\\b\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\\u0015\\\"5\\u0014\u00ef\u00bf\u00bd\\u0017$\u00ef\u00bf\u00bd2?]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bdYc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHHLB\u00ef\u00bf\u00bdI\\f\u00ef\u00bf\u00bd^f \u00ef\u00bf\u00bd\\u0004JXi\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd~G\\b\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00de\u2014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\\u0001\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.\\u0014.Ib\\tM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX.\u00ef\u00bf\u00bd\\u0015M\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001AQ\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdgbad:\\u0003\\u0015=/ \\u0017\u00ef\u00bf\u00bdc\\u0011\\u0016y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bdc-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010RG\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\\u0004C\\\"`\\u001Ax\u00ef\u00bf\u00bdw\\u001DL\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdOK*k\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\\b(3\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00dc\u2014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr2b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bduH+Y?\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdzx\\u0006\\u0010\\u0013\u00ef\u00bf\u00bd\u00cd\u017e\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e8\u017d\"<\u00ef\u00bf\u00bdp8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00d5-\\u00115\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdgB\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bdE*\\fa\u00ef\u00bf\u00bd-\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/NQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0^\u00ef\u00bf\u00bd=13-\u00ef\u00bf\u00bd\u00c5\u00b9\u00ef\u00bf\u00bdSa1\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdgIw\u00ef\u00bf\u00bd$S\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:u7\\u000B\\u001FY9&;113A\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bdjr\\r\\u0000q.8\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2h\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005h\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\\u0016\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'$\u00c4\u2014\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd%M\\u001D\u00c7\"H;\\u000F\u00ef\u00bf\u00bdD6\u00ef\u00bf\u00bdv:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\bdf\u00ef\u00bf\u00bd=\\u0015\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4p\u00ef\u00bf\u00bd-\u00df\u00a2H\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+8=\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d5\u00b9;\\u000Em6\u00ef\u00bf\u00bd\\u0018q\\b\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00058\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd=o<\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u007fj\u00d8\u0152\\\"T\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd()I\u00ef\u00bf\u00bdM\\\"Q\\\"\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c5\u02c6\u00ef\u00bf\u00bd`4\u00ef\u00bf\u00bd\u00e9\u00a4\u00af\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bdCfuy\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdkn*\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\\u000B\\n\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd+=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bdp4\u00ef\u00bf\u00bdnG>\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd];\\u0013\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdyL\\u0003\u00ef\u00bf\u00bd\\u0001\u00d9\u0153\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQTm\u00ef\u00bf\u00bdp3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\\u00072\u00ef\u00bf\u00bdX\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\\nS\\u000E\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB-KPy\\u0011cZ_\\u0019\u00ef\u00bf\u00bdj3\u00ef\u00bf\u00bd+As\u00ef\u00bf\u00bd\\u0015Z(X\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bdD\\u0002\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\\u000B\\u0013\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+j\\u0015Q\u00ef\u00bf\u00bdZ\\uD85E\\uDCy*82\u00ef\u00bf\u00bd\\u0016\\u0011\\u0005\u00ef\u00bf\u00bd\\u0004\\u0011\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd*\\\"\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\\u0012\u00ef\u00bf\u00bd\\u0005\\u0002\u00ef\u00bf\u00bd?\\u0014\u00ef\u00bf\u00bdhuw\\u001D\u00ef\u00bf\u00bd.F\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bdv]\\u0015,=/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:;\u00ef\u00bf\u00bd{g2\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bds\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd='\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd9g\u00ef\u00bf\u00bd!Q\u00ef\u00bf\u00bd\\\\]\u00ef\u00bf\u00bd\u00cb\u017e\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdVup\\u000Be\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|p\u00ef\u00bf\u00bd\\u00160\u00ef\u00bf\u00bd\\u0003[d0L\\\\RU\u00ef\u00bf\u00bd\\u0003K\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u2122p:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;/\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\\u0011\\u0002\u00ef\u00bf\u00bdxo@3\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd;\u00e0\u00a0-\u00ef\u00bf\u00bd\\u001Ea^p\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bdj!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\\u0017\\u0017_/s\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdt\\u0014\\u0013MG\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bdqR\\u00105\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL~\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\\u001Es\u00ef\u00bf\u00bd]V6\\u0016\u00ef\u00bf\u00bd4\\u0018a\u00ef\u00bf\u00bd\\u0019a\\b\\u001FH\\u0005\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\\u0005\u00ef\u00bf\u00bd>+[he\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00dd\u2030\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c4\u00b9'UeG%M\u00ef\u00bf\u00bdh-\u00ef\u00bf\u00bd\\u001CF\u00ef\u00bf\u00bd1x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\\u0018\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd,{\u00ef\u00bf\u00bd)\\n\u00ef\u00bf\u00bdae\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{p?Z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bdo\\t\u00ef\u00bf\u00bdW\u00d6\u2122\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00c3\u0152\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\\u0018:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo`\u00ef\u00bf\u00bdcJ\u00ef\u00bf\u00bdWY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001EY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cf\u0081H#8a\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd=x)F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdzwTJ\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bdC,\u00ef\u00bf\u00bd\u00ce\u00b61x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\\u001B\\u001F\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\\u0016uf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E6\u00d1\u0081\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cc\u00a6E\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d2\u00af0\u00e3\u201e\u00a2\\u0013\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd=n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\\u0019(\u00ef\u00bf\u00bdPq8\u00ef\u00bf\u00bd!\\t\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd N^`\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cc\u00actD\u00ef\u00bf\u00bd@{GpM9\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd<\u00e6\u0090\u00aa\u00ef\u00bf\u00bd\u00cb\u00a9aE\\u001A\\\"\u00ef\u00bf\u00bd\\u000Ef\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdL0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~1\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00c4\u00a8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00c8\u008dg\u00ef\u00bf\u00bd;fY\u00ef\u00bf\u00bd\\u0018\u00cb\u00bay<\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8^P[Q'\\u0003G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd6\\u00079\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bdO]`b)\u00ef\u00bf\u00bdPFrf\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001CPg\\u001F\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u007f\u00c2\u00a1H\u00d7\u0153\u00d7-\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bdi9\u00ef\u00bf\u00bd\\u0017\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH(\u00ef\u00bf\u00bd\\ra\u00da\u00b9Wm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dd\u0153\u00ef\u00bf\u00bd-!7bt\\u0013W\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw7Q\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd30A-\u00ef\u00bf\u00bd4k\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdVI\u00ef\u00bf\u00bd7\\u0010\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bdYoz\\u0017M\u00ef\u00bf\u00bd\\u001A;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\\u000Bc'Xh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\nn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6O\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd6\\u000E,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bdF(I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n>\\u0018$d\\u0018Y\\b\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bd\\u0019\\u001E\\u0016\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bdQ\\u0019\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\\u0016W\\u0013\u00ef\u00bf\u00bd-c/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdwT\u00ef\u00bf\u00bdlQlf3\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bdKg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd?@\\u001F\\u001C\\u0012\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|dq]1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00de\u00a8\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\\u001B\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdICx\\u0015d\\u0015\u00dc\"\u00ef\u00bf\u00bd.\\u0002\u00c6\u2021\u00ef\u00bf\u00bd\\\"\\u001B\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdkn\\u00165=\u00ef\u00bf\u00bd\u00da'\\u0014@\u00ef\u00bf\u00bdfh\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd S#xh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA!x\\u0018S\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00da\u0090W\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00d9'\\u0018\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bdw\\\\\u00cd\u00bb$\u00ef\u00bf\u00bdy\u00cd\u00b8y\\u0014\u00c3\u0152\\u0010\\u0003\u00ef\u00bf\u00bd\\u0017\\u0018\u00ef\u00bf\u00bd\\u001C\\u0019E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<;\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd%Ld\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd$\\f\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f&\u00ef\u00bf\u00bd&eh8\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\\u0010\u00ef\u00bf\u00bd/\\n\u00ef\u00bf\u00bdi0C\u00ef\u00bf\u00bdQ$GB)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\\u0012\u00ef\u00bf\u00bdI[\\u0015\\\\L]\\u001E\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdr9\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c5\u203a<\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D<\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004.\u00ef\u00bf\u00bdM\\t\\u001An.\\u000F\u00d3\u00aew\\u0010*\\f\u00ef\u00bf\u00bd\\u0005n>\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00db\u00b5f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg,L\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd25,\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm/\\uDB0E\\uDC85\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\\u0007$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00c8\u20ac:\u00ef\u00bf\u00bd,SgvV\\u0003\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSF\n{l2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\\u001E\u00ef\u00bf\u00bd\\u000E\\\"t\u00ef\u00bf\u00bdZ#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)$\\u000F\\ta\\n\u00ef\u00bf\u00bd\\u0014?QU\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdts\u00cb\u00a5\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT,\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdbc\u00ef\u00bf\u00bd.C24\u00ef\u00bf\u00bd\\t?'\u00ef\u00bf\u00bd^}\n\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMfR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4)\\\"i\\u0010\\r\u00ef\u00bf\u00bd\u00cd\u00a3J\u00ef\u00bf\u00bdMm\\u0001u\u00ef\u00bf\u00bd\\u0000H\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd]T\u00ef\u00bf\u00bdldU\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bds\u00d6\u00b5Jem\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdNfh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\\n\u00ef\u00bf\u00bd~a\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>X\u00ef\u00bf\u00bd2W\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`;h=?\u00ef\u00bf\u00bdi\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#7(x\\u0000\\u0013Z\\bl\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00de\u00bb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd#nUw15;D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\\u0012\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u00bb\u00ef\u00bf\u00bdv\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd.\\\"\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\\u0005l\\u0010/I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\\t\u00ef\u00bf\u00bdK)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006qidx\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bdddS\\u001CE>9\u00ef\u00bf\u00bd\u00e8\u2014\u00b9F\u00ef\u00bf\u00bd+\\n\u00da\u2122\\u0010\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd,`\u00ca\u0081\\u0007\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004 >\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\\u0011\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+{\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd/:\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00da-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bdi4\u00ef\u00bf\u00bdS\\u0007\u00ef\u00bf\u00bdE\\u0018g\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006!\u00ef\u00bf\u00bd\\u0007=b\\u0010\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00de\"\u00ef\u00bf\u00bdRF(\u00ef\u00bf\u00bd(D0\u00ef\u00bf\u00bd<\\u0015\u00ef\u00bf\u00bd\u00c3\u00a2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdkO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\\n\u00ef\u00bf\u00bd'gz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007fe_GfV#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\\u0014,]\\f\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00c2\u00ac\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdrb\u00ef\u00bf\u00bd2m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00d8\u017d\\u000E=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdnV\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd8\u00c5\u203a\u00ef\u00bf\u00bd\u00eb\u203a\u203a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\\u0015U\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e2\u00ab\u0081\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\\\\d \u00ef\u00bf\u00bd\\nV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bdNop\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR7\\b\u00ef\u00bf\u00bd%\\\\\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd)}\np\u00c9\u00a8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd!:\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#4\\ba\u00ef\u00bf\u00bd\u007f\\u0012\\u0015k\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Ax\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX7t\u00ef\u00bf\u00bd\u00ef\u2020\u00b4\u00c4\u00b6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\\f3\u007f\\u00189{\\u0011,U\\u000Euh\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u00be\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\\\"\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000FF\\u0007\u00de\u0152\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd-xa\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@`\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\\u001A|y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7'\\u001B\u00ef\u00bf\u00bd \\u0002\u00d8\u00ac\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bdm\\fv\\u0013\\u000F\n{Y^\\u0005\\u001Em}\n\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd$(\u00ef\u00bf\u00bd`\\u0003sR<i\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d0\u008d)hy\\u0005O%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO<>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u007fi\u00ef\u00bf\u00bdZ\\u0019\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\u00d9\u00a61\u00ef\u00bf\u00bdV^\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bdx\\u0011\u00ef\u00bf\u00bd9u\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\\b\u00d9\u0178cBJ>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u0161\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdzC\u00c4\u02c6\\u0006;\u00ef\u00bf\u00bd4@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ec\u201e\u008d<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdfg\u007f\u00ef\u00bf\u00bds\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\\u0013\u007f\u00ef\u00bf\u00bdG\u00de\u2030\\u001E\u00ef\u00bf\u00bdyd\u00ef\u00bf\u00bd7U7\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd:&\u00ef\u00bf\u00bdfF\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQl#^3|i\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~2\u00ef\u00bf\u00bd\\f\\u001D\u00ef\u00bf\u00bdP'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\\u001F#\\u000F\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\\u0015V\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd<\\u0005g\\txx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdXK\u00ef\u00bf\u00bdT4\u00ef\u00bf\u00bdiz\u00ef\u00bf\u00bdT4\\tM\u00ef\u00bf\u00bd\\u0019\\u001D\\u0019.\\u001A\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\\u0002T\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdfn\u00ef\u00bf\u00bdfw\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\\u001Cs\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F,w67'\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\\u0010\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\\u000F\\u000B\u00ef\u00bf\u00bd6Q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u00a1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<=\u00ef\u00bf\u00bd\u00ca\"Zx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\ns\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd}LSW\\u0018\u00ef\u00bf\u00bdW\u00cb\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM5\\\\.+\u00ef\u00bf\u00bd\u00c9\u00bd\u00ef\u00bf\u00bd\u00d4\u00a9!:\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\\u0017t\u00ef\u00bf\u00bdR\\u0019\u00ef\u00bf\u00bd*\\u001F~T(\\b\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\\u000EZE @\u00ef\u00bf\u00bd\\u0016Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\\n\u00d2\u00a9\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00046\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bddc\u00db\u00b9xj\u00ef\u00bf\u00bdS\u00e7-\u00b9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bdU\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\\u0005#\u00e2\u00b2\u00b0\u00ef\u00bf\u00bd=\\u0005\u00ef\u00bf\u00bd\\u001D4|5u\\u0011\\\\\\n\\u0017^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00003x\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\\u000F\\u000B\\\"#Vk\\u000B\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bdaD9\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015,\\u0004\u00ef\u00bf\u00bd'/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0#\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bdhv\\u0006\u00cc\u00bb\u00ef\u00bf\u00bd\\b%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;ju\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\\u0018\\b+\u00ef\u00bf\u00bd\u00d1\u0153\\u001Aa\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bdzj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bdtHw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d8\u00a5\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\\u001Ep\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH^\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bdvX3\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd'@\u00ef\u00bf\u00bd?u\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdRv\u00ef\u00bf\u00bdTt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\\u00106\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC@\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdd\\u000F\u00ef\u00bf\u00bd\\u001Dq\u00ef\u00bf\u00bdJ\\u0006O\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bdlK\\r\u00ef\u00bf\u00bd\\u0007B\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006#\\u0012\\u0002G\\u0003E,\u00ef\u00bf\u00bd\\u001A(\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh%\u00ef\u00bf\u00bd\\u001A\\u000E\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd \\u00066\\u0012\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd \u00ef\u00bf\u00bdA6\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdzK\u00e2\u00af\u00be~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u0153p\\u0015\u00ef\u00bf\u00bd1M\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00c4\u201a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f(\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\\u001D\u00ef\u00bf\u00bd4:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd?S\u00ef\u00bf\u00bdxhL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd.\\u0016\u00ef\u00bf\u00bd\\u0011d\u00ef\u00bf\u00bdjy\u00ef\u00bf\u00bd\\u0006(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\fN3\u00ef\u00bf\u00bd\\u000FnR\\u0010w-\u00ef\u00bf\u00bd54\\u0011+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c3\u02c6\\f\\u0001\u00ef\u00bf\u00bdE7\u00ef\u00bf\u00bd\u00ce\u009d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\\u0010\\u0003\u00ee\u00bb\u201a\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1n\\\"W\u00ef\u00bf\u00bdk@\u00c7\u02c6\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\u00d2\u00b1\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\\u0010-\u00ef\u00bf\u00bdot5:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\\u001AL\\u00027!\u00ef\u00bf\u00bdg\\u001A\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-n\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\\u00187\u00cf\u0192^\\u0000]|\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bdTe\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\\r\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010w\u00d4\u0161\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdIxJ<\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\\u0013\\u0002~H\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\\u0003\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1Z\\u001B\u00ef\u00bf\u00bd!r\u00ef\u00bf\u00bd9\\u0006t\u00ef\u00bf\u00bdrC\u00ef\u00bf\u00bdpY.\u00d1\u00ac\u00ef\u00bf\u00bd1\u00c9\u008d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\nm\u00ef\u00bf\u00bd\\u001Ax]\\fq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c4\u00a18Dc\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd 8M=\\u0000\u00ef\u00bf\u00bd]^w\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd>\u00df\u0090\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd2e\u00ef\u00bf\u00bdRul\u00ef\u00bf\u00bdvq,\\u0010a6u\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#A\u00ef\u00bf\u00bd^nD5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\\u0014$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00da\u0161\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C\u00c2\u2030\\u0012\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG5\\f\u00ef\u00bf\u00bdE__\u00ef\u00bf\u00bd\\u0005b\u00ef\u00bf\u00bd}\\u0018\u00d5'_Q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdcp#z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJZu\u00ef\u00bf\u00bd\u00d9\u0192](,\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\\u000779\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00cd\u009d\u00ef\u00bf\u00bd\\u0003st;^\u00ef\u00bf\u00bd7\\u0003&$\u00da\u00a5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bdamg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00d1\u009d`\u00ef\u00bf\u00bd}/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\\t\u00d8\u0153}\\u0016DM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdcT<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMDm\u00ef\u00bf\u00bd\\u00195\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00ef\u00bf\u00bdW\\u0010\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\\r>.\u00c6\u00a7pG\u00ef\u00bf\u00bd\\u001A\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdIn?\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c6\u2039\u00ef\u00bf\u00bd4H\u00ef\u00bf\u00bda \u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdiD|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\np\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\\u0017\\f\u00ef\u00bf\u00bd\u00dc\u00bdF\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd~|4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002l\\u000F\\u001E\\u0012?Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00dd'7\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001r\\u001C$\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bdlU\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\\u001B[\\\\v\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}qW\u00e1\u00ad\u0192\u00ef\u00bf\u00bd$2eYA\\u0001\u00ef\u00bf\u00bdv/V\\u0011G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\\u0007S%o\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd_\\u001B\u00ef\u00bf\u00bd2fw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdxP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^0;q\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bds\u00ef\u00bf\u00bdY@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[?K\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9%\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\\b\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\\u000E\\u000B{$hh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd}\n3\\r\u00ef\u00bf\u00bdt;\\u0013-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdLG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010C6m?t8\u00ef\u00bf\u00bd=I\u00ef\u00bf\u00bd\u00c3\u00ac\u00ef\u00bf\u00bd,F\\u0004ro\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\\u0007\\u0015\u00ef\u00bf\u00bd's~\\\\\\u0014\\u0007_\u00ef\u00bf\u00bdqk\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\\u0007\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\\u000FF\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bdX\u00e8\u00b5\u008d2\\u0000|\u00db\u02c6\u00ef\u00bf\u00bd9+\u00ef\u00bf\u00bd\u00c4\u00ban\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq*AS2\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\\u0015\\u001FtD$?V\u00ef\u00bf\u00bd D\u00ef\u00bf\u00bd6Nf\\u0003\u00ef\u00bf\u00bdlW\\u001C67\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u007f\\u000E\\u0000\u00ef\u00bf\u00bd#{m\\nendstream\\nendobj\\n27 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/FirstChar 32 \\n/LastChar 182 \\n/Widths [ 278 333 371 556 556 1000 667 208 333 333 556 606 278 333 278 389 \\n556 556 556 556 556 556 556 556 556 556 278 278 606 606 606 444 \\n747 778 667 667 722 611 611 722 778 333 333 722 611 944 778 778 \\n611 778 667 611 667 778 722 1000 722 722 667 333 606 333 606 500 \\n333 500 556 500 611 500 333 500 611 333 278 556 333 889 611 556 \\n611 556 389 444 333 611 556 833 556 556 500 333 606 333 606 0 0 \\n0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 278 278 0 0 0 556 1000 0 0 0 0 0 \\n0 0 0 278 0 556 556 0 0 0 556 0 747 0 0 0 333 0 0 0 606 0 0 0 611 \\n628 ] \\n/Encoding /WinAnsiEncoding \\n/BaseFont /ONAFBK+Melior \\n/FontDescriptor 25 0 R \\n>> \\nendobj\\n28 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/Encoding /WinAnsiEncoding \\n/BaseFont /Helvetica-Bold \\n>> \\nendobj\\n29 0 obj\\n<< \\n/Type /Page \\n/Parent 2 0 R \\n/Resources 31 0 R \\n/Contents 30 0 R \\n/MediaBox [ 0 0 612 792 ] \\n/CropBox [ 0 0 612 792 ] \\n/Rotate 0 \\n>> \\nendobj\\n30 0 obj\\n<< /Length 4803 /Filter /FlateDecode >> \\nstream\\r\\nH\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWs\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012~\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd#>\u00ef\u00bf\u00bd\u00c7\u00a3\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bdr\\b\\u0018\u00db\u00a9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\\u0007Y\\u001A`6Bb%a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001|I\u00ea\u00a4'][\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00eb\u00af\u00bf\u00ef\u00bf\u00bdp\u00d5\u00a1\u00ef\u00bf\u00bd\\u0001(\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd0\u00d8\u201aAa\\f_\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd:\\u0017WS\\u0003\\u0016y\u00ef\u00bf\u00bd\\u0000\\u0001\\u001D\u00ef\u00bf\u00bd\\u0006\u00d7\u00b2`\u00d5\u00b1\\u001DW\u00ef\u00bf\u00bd\\u0014w\u00ef\u00bf\u00bd\\u001DJL<R\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00de\u00bd\u00ef\u00bf\u00bdu.F\u00ef\u00bf\u00bd\\u0003\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMa\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\\u0017\u00ef\u00bf\u00bdo\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl;]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd4\u00c6\u00bf\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\u00c6\u017e0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\\u00051\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bdPJ]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d2\u00bd8;7\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdiL\u00ef\u00bf\u00bdq\n{0I\\t\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdy?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\\nv=\\u0018\\u0007;`F\\u000F\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\\u0016\\\"\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\u00ef\u00bf\u00bd\u00cb\u00ad\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0M\u00ef\u00bf\u00bd@$\\\"Y@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\\u0013a\u00ef\u00bf\u00bd#H3Xg\u00ef\u00bf\u00bd:\\u0013\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\\u001DH\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdi\u00ef\u00bf\u00bd\\n\\n\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd$-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u007fu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd?!_\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\\u0005\u00ef\u00bf\u00bd<\\\\\u00d9\u2022\u00ef\u00bf\u00bd]o\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD>-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\\u0003t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca...a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd e\u00ef\u00bf\u00bd\u00eb'\u017e\\u001B\u00ef\u00bf\u00bdm\u00d3\u00b5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd [\\u0004\\u0019/`L`\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd+\u00cd\u00bd\u00ef\u00bf\u00bd\\u001A\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<OC\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd ]\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<S\\u0011\u00dc\u00a5\u00ef\u00bf\u00bd\\bwD+iu\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E/\\t\\t`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u2014\u2022/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019Jl\u00ef\u00bf\u00bd)_\u00ef\u00bf\u00bd?g0\\u0007q5\\u00121\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdZo\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\\u001Db'X}-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\tU-G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\\u0015\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bdcZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Fo&W0\u00ef\u00bf\u00bd\\u001D^\u00ef\u00bf\u00bde8\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\\u0013A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdAPC;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd(N\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\\u0003\\u000Es\u00ef\u00bf\u00bd'i\u00ef\u00bf\u00bd\\u0019f\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bdnq\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\\f/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\\u0006\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd]C\u007f2<\u00ef\u00bf\u00bd&\u00d7\u0178\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\tL/\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\\f.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd`\u00ef\u00bf\u00bdm\\u0016\u00d0\u008fV\u00d8\u201e\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdKm\\u0007)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\nJ\u00ef\u00bf\u00bdZR\\\\\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\\u0014(Nd\u00ef\u00bf\u00bd0\\u001D\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00de\u00b6\u00ef\u00bf\u00bd~g\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdFDA\\u0012r\u00ef\u00bf\u00bd\\u00057I\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZOD:\\u0007\\u0015\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bdM(].\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u007f\u00d7\u00b0E;\\rO\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd?\\u0005\\\"\\u000E\\u001EE,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\\tS.\u00ef\u00bf\u00bd\u00c4\u00b2h\u00ef\u00bf\u00bd'u\\u001A?\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\\\\\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bdb\u007f\u00ef\u00bf\u00bd\\u0001_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00e1-\u017d_O\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdnn'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d8\u00bes`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd'F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\\u001D\u00dc\u009d~\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bdtu\u00ef\u00bf\u00bd7&t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bdL\u00c3\u0152\u00ef\u00bf\u00bdlx\u00e8\u017d\u2020\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\\u00109\\u001EM\u00ef\u00bf\u00bdM\\u0012\u00ca\u00a1U\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\\u0004-\u00ef\u00bf\u00bd\\u0000\\\"N\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bdO\\u001C| \\nI\u00ef\u00bf\u00bd\\u001A-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdCJ\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c7\u2039\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00d5\u2020\u00ef\u00bf\u00bd!\u00d8\u00ab\\u0000M\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bdA\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd':Q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bdv;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00d4\"\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)_\\f\\u000BG\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8oxX$\\u0005O\\\"\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd6IG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd%1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00182\u00ef\u00bf\u00bd\\u00037\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdeQ\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd2Ya+Y\u00ef\u00bf\u00bd.i'G\\u0016\u007f0\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\\b \\b\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bd&AV\u00ef\u00bf\u00bd\u00cf\u2039\\\"o\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\\u001Ds\\u0019Ny\u00a8sy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\u00de...j*W\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd<\u00e6...\u00bc\u00ef\u00bf\u00bdtt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\\u0016\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd4F\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bdc\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bdb\\u001A\u00ef\u00bf\u00bd~\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?+\u00ef\u00bf\u00bdP\u00ef\u00bf\u00bd\u00c9\u017dOL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd4\u00d3- v\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\\u0012\\u0005l3Q ,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\\u000F\u00ef\u00bf\u00bd,MDX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\\u001A\u00ef\u00bf\u00bdCTR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\\u001EnW\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd`\\u0006\u00c5\u0160\u00ef\u00bf\u00bd\\u00161\u00ef\u00bf\u00bdL\\u0019\u00ef\u00bf\u00bd\u00d9\u0161\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bdM<*`\u00ef\u00bf\u00bd\u00d8\u017e\u00ef\u00bf\u00bd\\u001E\\u0004\u00ef\u00bf\u00bdL\u00ef\u00bf\u00bdM|\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd()\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\\t\\u0016\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\\u0013K,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002J\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdxA\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bdE\u00cb\u00a5v4&\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdz\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d9\u017dIQ\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001C>\\u0006\\t\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\\u00029\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dc\u00a3\u00ef\u00bf\u00bdO\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~<DI\u00ef\u00bf\u00bd\u00d9\u0152@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdSX\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00dd'y\\u0014\u00ef\u00bf\u00bdE\u00ef\u00bf\u00bdt\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQQ6i\u00d3\u2014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]k\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd.H\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9v>oX\u00ef\u00bf\u00bd\\u001FEB\\u00122L\u00ef\u00bf\u00bd9/v\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bdj%A\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015M]\u00d5\u20ac\u00ef\u00bf\u00bd'j\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c4\u02c6\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd\\u000BC\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdA)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00d7\u00a0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdvGz8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\\u000B\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd7\\\\\\u001BeI\\\\\u00ef\u00bf\u00bd`\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\\b\u00ef\u00bf\u00bdb6i\u00c6\u02c6oR\u00ef\u00bf\u00bd9lMqV\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx~\\u001ED\\u0011.\u00ef\u00bf\u00bdr\\u0011\\f\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bdeBqV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000W\u00ef\u00bf\u00bd\\u001C\\u001Bb\\u0019H\u00ef\u00bf\u00bdSU\\u0010\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00da\u00a5\u00ef\u00bf\u00bd}u\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd@\\u0010\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\\u0004\\bd\u00dd\u203aru\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001oI\u00ef\u00bf\u00bd\\u0015J\\t\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00df\u0192\u00d1\u00b4?!\u00c8\u00bb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00eb\u00a5\u0152\u00ef\u00bf\u00bd4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdVG\\u0012~\u00ef\u00bf\u00bd\\u001D,\u00ef\u00bf\u00bd\\u0006%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bds\\u001C\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdgRO\u00ce\"\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00c5\u2122UV\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r]\u00ef\u00bf\u00bdg0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\\u001Be\\u0004}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007fp\u00ef\u00bf\u00bd0\\r\\u0002\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bdr\\u0018\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bds\\u0017\u00ef\u00bf\u00bd\\u0013I\u00ef\u00bf\u00bd7\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd:\\u0006\u00ef\u00bf\u00bdF\\r\\u0016S\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bdb8o\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000BC\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd9_P2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00e7\u00a6...\u00ef\u00bf\u00bdvo&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq_i\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bdd86F`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011\u00ef\u00bf\u00bdR\\u000B\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\\u0016f\u00ef\u00bf\u00bdqT\u00ef\u00bf\u00bd- \\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\\u0015\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdRg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\\u0018\\u0003\u00ef\u00bf\u00bd\\u001A\u00c4'\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bdb8ul\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdLE^\\u0007l:H \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd}w\\u0000\\u0004\u00d3\u00acz\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd3\\u0016E\u00ef\u00bf\u00bd\u00e4\u02c6\u0090\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001CE\\u001D2\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ca\u0161\u00d6\u008du\\u0013\\\"I\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdX\\\"d\u00ef\u00bf\u00bd)p\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u00b4_ \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#Z?I\u00ef\u00bf\u00bd\u00da\u00a4=\u00ef\u00bf\u00bdu\\\\\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bdzSwjm\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd;ju\\u000B!`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ea'\u0152\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd:6\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd\\u001B5\u00e2\u0152\u00a6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|.St\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\\u001Ef\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdP\\u0014c\u00ef\u00bf\u00bd5\\u0003)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\\u0010\u00ef\u00bf\u00bdZ\u00da\u00a2N\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00104z\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd*w\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd.d\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bdS\\u0004\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS@\u00ef\u00bf\u00bd)H\\u000E\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bd\\u000BtW\u00ef\u00bf\u00bd\\u001B\\u000FJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd+\\u000F<t\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00c8\"\\f\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bdM\\u0007\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWC\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-k\\u0007x\u00ef\u00bf\u00bd\\u0012{\u00ef\u00bf\u00bd-\u00d4\u201a\\r>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw9\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM^\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bdm\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdliE)\\u0010\u00ef\u00bf\u00bd\\f\\u0003\u00ef\u00bf\u00bd]\\u000BG\\u0002hyu\u00ef\u00bf\u00bdcy\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd?Z\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd0j\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd{^U\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?_\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bdn7\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bdR\\u001B\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdt(z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdUk\\u0005\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdPs\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd]>\u00ef\u00bf\u00bd=t\\u001D\u00ef\u00bf\u00bdJUo\\u0019H\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bdaX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;X\\u0013x\u00ef\u00bf\u00bdDJ\\\\9\u00ef\u00bf\u00bd\u00da\u00a3\\u0017)\u00ef\u00bf\u00bd\u00c4\u0178\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bdQ: \u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\\u0002)Q[\u00ef\u00bf\u00bd^c!1\\u000B\\u0014n\u00ef\u00bf\u00bdNE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u203a\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5v\u00ef\u00bf\u00bdz \u007f\\b\\u0016\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$_\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd~kai\u00ef\u00bf\u00bd\u00c6'\u00c4'*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c8\u017e\u00ef\u00bf\u00bd\\u0011Lp\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\\u0002Tej\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00cf\u00b1\\u0004\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\rR\u00c8'\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bd\u00df\u00ae=\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bdxa\\u000E\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bdk\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\\u0001\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bdv^g\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\\t!\\u0015\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bdIK<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bdm\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bd\u00c8\u00ab\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,\\u0016\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bde`i\\fK\\u0019 \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c4\u0152H\\n$e\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdf7\u00ef\u00bf\u00bd%Y\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd`\\u0006c\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bdZn\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f\u00ef\u00bf\u00bd=O]oyU\u00ef\u00bf\u00bd\\u0001\u00ca\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d4\u008d\\u0019\\rZ\u00ef\u00bf\u00bdzQs\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd_Te\u00ce\u00bb\u00ef\u00bf\u00bd\\\\]V\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd}X.\\u0006\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bd;S\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017 ?_\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bdvl'\u00ef\u00bf\u00bd\\u000F\u007f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\\t\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c4\u00b8\u00ef\u00bf\u00bd`U\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bd~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdjZ[\\u001DI\u00ef\u00bf\u00bd7\u00d0\u00baQQ@rb*\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9[g\u00ef\u00bf\u00bd0\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bdai\\u00133k\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00df\u2014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bdD\u00c2\u00a8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bdO'\u00ef\u00bf\u00bdiI\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdj\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u007f_\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\\u0000^ `4_e\u00ef\u00bf\u00bd\\u001AM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd9\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,a\u00ef\u00bf\u00bd\\\"\\u0007t\u00ef\u00bf\u00bd&c/\\u0018#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bdeE\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdEh\u00ef\u00bf\u00bd\\\\FU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdEh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+^\u00d1\u00b6\u00ef\u00bf\u00bd\u00e6\u00b9\u00a2\u00ef\u00bf\u00bdx|\\u0018O\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\\u001A4\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV\u00ef\u00bf\u00bd\u00d2\u2020\u00ef\u00bf\u00bdN%\u00ef\u00bf\u00bd\u00c9-\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdzw\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bdlY\u00ef\u00bf\u00bd[\u00d2\u00b8ic?a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cb\u00b4\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\\u001C\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd*n0<l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00db\u00aa\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\\u0005)\\nCfd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd,\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bdJN\u00ef\u00bf\u00bd\u00dd\u201e)\\u0013\\t\\u0000\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\\b!k\u00ef\u00bf\u00bd\\u0010m\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd3lA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd\u007f#qI\u00ef\u00bf\u00bd\\n=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd0\\u000B\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd-W\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdMZ\u00ef\u00bf\u00bd\\u001A\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012/\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3z\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdlY\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd--l\\u0017N[C\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bdf\\u0016m\\u0010z\\u0004\u00ef\u00bf\u00bd\u00cc\u00a3W=/B\u00ef\u00bf\u00bd[\\u001DV\u00ef\u00bf\u00bdjl\u00ef\u00bf\u00bd\\u0017YZ\\u001BGk\\u0012P+\\u0013C\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\\u001E\\u0005G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd])\u00ef\u00bf\u00bdN$\u00ef\u00bf\u00bd03Wr\u00ef\u00bf\u00bd\\u000Ep\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'j\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0014\\u001D\u00ef\u00bf\u00bd\u007fg\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd&\u00d5- 6\\u0015\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd[/\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdz77\\u000F5<\u00ef\u00bf\u00bdv1\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd##\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\\u0000G(\u00c7\u0160-q\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd#ns\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bdsZ\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(3\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\\u001B8\u00ef\u00bf\u00bd\\\\@RE\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd`\u00ef\u00bf\u00bd\\u001EZ\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00d8\"8\\u001E0f\u00ef\u00bf\u00bd \\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\\u0016\\u0000>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT[I\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd$\u00cd\u00bbV\u00ef\u00bf\u00bd\\u0002\u00dc\"\u00ef\u00bf\u00bd\\\"<\\u0011\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bd\\u0017 %\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0016N\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ce\u2122\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00d2\u00a15\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\\u0007>\u00ef\u00bf\u00bd\u00c6\u00a0ml\\u0018\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdyA\u00ef\u00bf\u00bd\\u000E\\u0006XSUi\u00ef\u00bf\u00bdN\u00ef\u00bf\u00bd\u00cd\u201e\\\\q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd.\u00ef\u00bf\u00bdV4\\u0016\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3Bu\\u0007\\\"\\u0010!\u00ef\u00bf\u00bdU\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdWkI\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bdw\u00ef\u00bf\u00bd{[\u00ee\u20ac\u00a3J\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd%\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bdr\\u001B\\u001Ea\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd1'/T\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdu\u00ef\u00bf\u00bdY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx&\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdsYh\u00c2\u0192\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\\u0004\u00ef\u00bf\u00bd\u00d8\u0178\u00c3\u00a4\u00ef\u00bf\u00bd\u00e1\u00a9\u00b0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5(!\\\"\u00ef\u00bf\u00bd+\\r\u00ef\u00bf\u00bd;b<Z\u00ef\u00bf\u00bd_i\\u000E\\u0004&\\u001FB]\u00ef\u00bf\u00bdPVgmd\\u0004\\u001A@m\\u0003,*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bddkt.\u00ef\u00bf\u00bd\\u0002F\u00ef\u00bf\u00bd{\\\"qoj\u00ef\u00bf\u00bd-\\u0018%\u00ef\u00bf\u00bd4+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\\u001C\u00ef\u00bf\u00bdsG\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdF\u00ef\u00bf\u00bdJi\u00ef\u00bf\u00bd9a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0017Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd?j\u00ef\u00bf\u00bd|\\u0010\u00ef\u00bf\u00bd0ApA\u00ef\u00bf\u00bd5\\f\n{Ub^d\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdKv~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdh#\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\\t\u00ef\u00bf\u00bd+HD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u00185\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bdl.>~\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdvV\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdZ$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd,_>\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0018jB\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bdy\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bd+c\u00ef\u00bf\u00bdQ$l\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd|}\n*]:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\u00ef\u00bf\u00bd@\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd%N\u00ef\u00bf\u00bd'\\u0005\u00ef\u00bf\u00bd\\u0014\u00cd\u00b0iSG\u00ef\u00bf\u00bd\\u0010&Kf\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdFe\\f\u00ef\u00bf\u00bd[\u00ef\u00bf\u00bdIT@\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bd>U\\br\\u0005\\r\u00ef\u00bf\u00bdSIy\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd1\\u0017\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0019\\u0018\u00d3- C\u00ef\u00bf\u00bd!\u00ef\u00bf\u00bd\\u0003N\u00ef\u00bf\u00bdx\u00ef\u00bf\u00bdK\\u000E\\u001F\u00ef\u00bf\u00bd\\u001E\u00c7\u00b3\u00ef\u00bf\u00bdxfD\\u001B\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\n{\u00d6\u0090\\u0006\\u0010}\n\u00ef\u00bf\u00bd~_(\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"_XpX\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}+\\f\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bdt\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ\u00ef\u00bf\u00bd\\u0003[\\u00135\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5y:YNP\u00d9\u2022\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\\u0016\u00ef\u00bf\u00bd\n{\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\u00df\u00a5Ly\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdn\\u001E\u00ef\u00bf\u00bd-\\u0006\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\\u000B\\u0003\u00ef\u00bf\u00bdaD\u00ef\u00bf\u00bdFk#\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bdv4&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU%Z\u00ef\u00bf\u00bdX.\u00ef\u00bf\u00bdEb\u00c5-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0002\u00ef\u00bf\u00bdOt\\f>\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd]\u00ef\u00bf\u00bdJ#\u00ef\u00bf\u00bd\\u0005N1\\u001A\u00ef\u00bf\u00bd;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdIc\\u001D\\u0007\u00ef\u00bf\u00bd2\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT4E\u00ef\u00bf\u00bd4=\\u0014,\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\\u0010\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdhM\u00ef\u00bf\u00bd\\\"\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006?v9\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdE\\u0012\\u0011eU\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\\u0015\\\"AVf\u00ef\u00bf\u00bdx\\u000F\\u0014b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bda\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001AI\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bdORt>\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\\u0018/\\u0010WE)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd;\u00d1\u00a3\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\u00c8\u203ar\u00ef\u00bf\u00bd|;`\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ce\u00a1\u00ef\u00bf\u00bdGW\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\\u0000>^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdIw\\u000F\u00ef\u00bf\u00bd\\u0002Pp\\u0002\u00d4\u2039\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdB\\u000F\u00ef\u00bf\u00bdV\\u0013\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\\\u00ef\u00bf\u00bdny\\u0004q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\\u001C~Y\u00ef\u00bf\u00bde\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd@\\u001D\\u0001\u00d6...4\u00ef\u00bf\u00bd\\nW\u00ef\u00bf\u00bd*\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\u00ef\u00bf\u00bdd\\u001A\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\\u0004\\u0000\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bdkx\u00ef\u00bf\u00bd>l]\\u0018\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'}\nS{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\\u0006\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHu\u007f\u00cf\u201e\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003{\u00ef\u00bf\u00bdml%\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdwEa\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005X!\u00ef\u00bf\u00bd\u007f1Xh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006D'-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd)\n{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00de\u00b0\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bdA\\u001B\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bdn\\u0018\\u000E\u00ef\u00bf\u00bd#\u00ef\u00bf\u00bd\\b\\u000Eoa;\u00ef\u00bf\u00bdV~\\u000B\u00c7\u00bc\u00c6']\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000F\u00ef\u00bf\u00bdn\u007fY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd>\u00ef\u00bf\u00bd\\u0015)\u00ef\u00bf\u00bde~\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bdp\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bds\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00dd\u017d/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr::\u00ef\u00bf\u00bd\\u001Cn\u007f\u00ef\u00bf\u00bd\\\\N\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdJ\u00ef\u00bf\u00bd/\u007f\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdT6\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdK\u00ef\u00bf\u00bd{\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c3\u00b1=\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#[M\\b\\u0006\\\\<u\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdHN\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00eb\u017d\u00a3y\\rC^V=\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdx\\u000E.\\u0013X\u00ef\u00bf\u00bd&\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdA\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdq\u00ef\u00bf\u00bd-W\u00df\"\u00ef\u00bf\u00bd\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd#\u00c9\u02dc\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\b\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:^\\u0001\u00ef\u00bf\u00bd)\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdb\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0000\\u0003\\u0014n\u00ef\u00bf\u00bd\\u0019gE\u00ef\u00bf\u00bd?\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0011-t|[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00c2\u203a\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001BR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\u00ef\u00bf\u00bdC\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\\u000F\u00c7\u00adS\\u0005<\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdh\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd\u00cf\u00b4\u00ef\u00bf\u00bdq0\\u0007\u00ca\u0090.\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw_\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\\u001F\\\".c\u00dd\u2030\u00ef\u00bf\u00bd\n{SaG508\u00ef\u00bf\u00bdoy\\u0012\\f\u00ef\u00bf\u00bd/-\\u0018NF\u00ef\u00bf\u00bdn\u00ef\u00bf\u00bdf\u00ef\u00bf\u00bd7\u00ef\u00bf\u00bdq2\u00ef\u00bf\u00bd|4\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r,'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\\u0016\u00ef\u00bf\u00bd3\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bdff\u00ef\u00bf\u00bdL\\u0006^h:\\r3\u00ef\u00bf\u00bdpH0\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\\u0019Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdt\\u001E\u00ef\u00bf\u00bdg\\u000E\u00ef\u00bf\u00bdIt\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\u00ef\u00bf\u00bdH\u00ef\u00bf\u00bdu+\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd(\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdI\u00ef\u00bf\u00bdl\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd}\n/D\u00ef\u00bf\u00bd\\u0018%\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\u00ef\u00bf\u00bd\\u00185\u00db\u00aau~\u00ef\u00bf\u00bd\\u0004\u00ef\u00bf\u00bdBG\u00ef\u00bf\u00bdd\u00ef\u00bf\u00bdF^e\\u000F?\u00ef\u00bf\u00bd\\n\\u0012\\u000EO\\u001FL\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0010\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001D\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdM\u00ef\u00bf\u00bdh/\u00ef\u00bf\u00bdRa\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd?(\u00c7\u00b5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E[\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdV$\u00ef\u00bf\u00bd$\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bd\u007fs\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\\u0007\\u0003+\u00ef\u00bf\u00bd9\u00ce...\u00ef\u00bf\u00bdr\u00ef\u00bf\u00bdvm!\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdo\\rsB\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdzp]\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00d7\u017e\u00ef\u00bf\u00bd\\ryTO=\u00ef\u00bf\u00bd-\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd/\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bdo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd:\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd5\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdAo\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdr'\u00ef\u00bf\u00bdXO/\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdg\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\\u0000\u00ef\u00bf\u00bdjvn\\nendstream\\nendobj\\n31 0 obj\\n<< \\n/ProcSet [ /PDF /Text ] \\n/Font << /F2 28 0 R /F4 27 0 R /F6 24 0 R /F7 23 0 R /F8 20 0 R /F12 11 0 R \\n/F16 8 0 R /F18 32 0 R >> \\n/ExtGState << /GS1 7 0 R >> \\n>> \\nendobj\\n32 0 obj\\n<< \\n/Type /Font \\n/Subtype /Type1 \\n/FirstChar 1 \\n/LastChar 1 \\n/Widths [ 833 ] \\n/Encoding 36 0 R \\n/BaseFont /ONCHBA+AScChar \\n/FontDescriptor 34 0 R \\n/ToUnicode 33 0 R \\n>> \\nendobj\\n33 0 obj\\n<< /Filter /FlateDecode /Length 213 >> \\nstream\\r\\nH\u00ef\u00bf\u00bdT\u00ef\u00bf\u00bd1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd0\\f\u00ef\u00bf\u00bdw~\u00ef\u00bf\u00bdGN\\u001D\\u0012n\u00e9\u201a\u0090N\\\\\\u0007\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdU\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\\u0012C#\\u0015'2a\u00ef\u00bf\u00bdoB[N7\u00d8'\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd=G\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdwC6\u00ef\u00bf\u00bd8\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd-\\u0006\u00ef\u00bf\u00bd-\\u0019\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00cd\u00ac\\u0011:\\u001C,A\u00ef\u00bf\u00bd\\t\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\uD829\\uDDAEG\u00ef\u00bf\u00bdAD\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdO\\u0001\u00c7\u2020z\\u0007e\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdc\\u0014\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw\u00c8\u00bfZ]_\\u0015o\u00ef\u00bf\u00bd\\u0007\u00ef\u00bf\u00bd_6\u00c8-\\u0006\u00ef\u00bf\u00bdO\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\\u001F\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001B\u00ef\u00bf\u00bdH\\u0001$T\\u0015\\u0018\u00ef\u00bf\u00bd3Q\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdQ#\u00ef\u00bf\u00bdX\u00ef\u00bf\u00bdE(^\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdFV4 \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\r\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdD\u00ef\u00bf\u00bd?\u00c7\u00bf\u00ef\u00bf\u00bdR\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\\"\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0012\u00ef\u00bf\u00bdnY\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001Cs-\\u0007/\u00ef\u00bf\u00bdR\\u0006K\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdw>Y\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\\u0002\\f\\u0000\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdiS\\nendstream\\nendobj\\n34 0 obj\\n<< \\n/Type /FontDescriptor \\n/Ascent 698 \\n/CapHeight 811 \\n/Descent 427 \\n/Flags 4 \\n/FontBBox [ -67 -226 1040 829 ] \\n/FontName /ONCHBA+AScChar \\n/ItalicAngle 0 \\n/StemV 32 \\n/XHeight 438 \\n/CharSet (/asciitilde)\\n/FontFile3 35 0 R \\n>> \\nendobj\\n35 0 obj\\n<< /Length 230 /Subtype /Type1C >> \\nstream\\r\\n\\u0001\\u0000\\u0004\\u0001\\u0000\\u0001\\u0001\\u0001\\u000FONCHBA+AScChar\\u0000\\u0001\\u0001\\u0001=\u00ef\u00bf\u00bd\\u000F\\u0000\u00ef\u00bf\u00bd\\u001B\\u0001\u00ef\u00bf\u00bd\\u001C\\u0002\u00ef\u00bf\u00bd\\u0003\u00ef\u00bf\u00bd\\u0017\\u0004\u00ef\u00bf\u00bd\\u0019\\f\\u0003\u00ef\u00bf\u00bd\\f\\u0004\u00ef\u00bf\u00bd\\u001C\\f\\u0016H\u00ef\u00bf\u00bdv\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0005\\u001E\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001E\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\\u0007\u00ef\u00bf\u00bd\\u0018\\u0010\u00ef\u00bf\u00bd\\u001B\\u000F\u00ef\u00bf\u00bd\\u001E\\u0011\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdW\\u0012\\u0000\\u0002\\u0001\\u0001\\u001E%Generated by Fontographer 3.5AScChar\\u0000\\u0000\\u0000\\u0001~\\u0000\\u0000_\\u0000\\u0002\\u0001\\u0001\\u00044\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0001\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0015\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd+\u00ef\u00bf\u00bd\\u001B\u00d6\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u001F\u00ef\u00bf\u00bd\\u0007R^PiB#1\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0007\\u001B@\\\\rVW\\u001F\\u000E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bdS\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd_\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\u0006\\u001E\u00ef\u00bf\u00bd\\\"rs\u00ef\u00bf\u00bd\\f\\t\u00ef\u00bf\u00bd\\n\u00ef\u00bf\u00bd\\u000B\u00ef\u00bf\u00bd\\f\\f\u00ef\u00bf\u00bd\\f\\r\\u001E\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\\f\\u000F\\nendstream\\nendobj\\n36 0 obj\\n<< \\n/Type /Encoding \\n/Differences [ 1 /asciitilde ] \\n>> \\nendobj\\nxref\\n0 37 \\n0000000000 00000 f \\n0000000016 00000 n \\n0000000069 00000 n \\n0000000167 00000 n \\n0000000384 00000 n \\n0000000534 00000 n \\n0000004995 00000 n \\n0000005187 00000 n \\n0000005264 00000 n \\n0000006052 00000 n \\n0000006549 00000 n \\n0000013464 00000 n \\n0000013568 00000 n \\n0000013764 00000 n \\n0000014068 00000 n \\n0000014310 00000 n \\n0000014614 00000 n \\n0000014688 00000 n \\n0000014798 00000 n \\n0000015125 00000 n \\n0000015236 00000 n \\n0000016028 00000 n \\n0000016541 00000 n \\n0000024287 00000 n \\n0000024396 00000 n \\n0000025251 00000 n \\n0000025873 00000 n \\n0000033644 00000 n \\n0000034348 00000 n \\n0000034457 00000 n \\n00000y\u00a034610 00000 n \\n0000039488 00000 n \\n0000039670 00000 n \\n0000039856 00000 n \\n0000040143 00000 n \\n0000040386 00000 n \\n0000040686 00000 n \\ntrailer\\n<<\\n/Size 37\\n/Info 3 0 R \\n/Root 1 0 R \\n/ID[<365ed9e306e111423dc58eaa09fc93f8><365ed9e306e111423dc58eaa09fc93f8>]\\n>>\\nstartxref\\n40759\\n%%EOF\\n\"\n  },\n  \"key\" : \"https://www.fda.gov/OHRMS/DOCKETS/98fr/052102d.pdf\",\n  \"imported\" : \"Mon, 20 Nov 2017 10:15:32 GMT\"\n}\u00ff",
        "Issue Links": []
    },
    "NUTCH-2477": {
        "Key": "NUTCH-2477",
        "Summary": "Refactor *Checker classes to use base class for common code",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "12/Dec/17 15:51",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "17/Dec/17 13:22",
        "Description": "The various Checker class implementations have quite a bit of duplicated code in them. This should be refactored for cleanliness and maintainability.",
        "Issue Links": [
            "/jira/browse/NUTCH-2431",
            "/jira/browse/NUTCH-2431",
            "/jira/browse/NUTCH-2320",
            "https://github.com/apache/nutch/pull/256"
        ]
    },
    "NUTCH-2478": {
        "Key": "NUTCH-2478",
        "Summary": "// is not a valid base URL",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "12/Dec/17 15:51",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "17/Dec/17 11:34",
        "Description": "This test fails:\n\n\r\n  @Test\r\n  public void testBadResolver() throws Exception {\r\n    URL base = new URL(\"//www.example.org/\");\r\n    String target = \"index/produkt/kanaly/\";\r\n    \r\n    URL abs = URLUtil.resolveURL(base, target);\r\n    Assert.assertEquals(\"http://www.example.org/index/produkt/kanaly/\", abs.toString());\r\n  }\r\n\n\nand has to fail because of invalid base URL, so the current URL is used. If current URL is not /, its path will be prepended, resulting in 404 being crawled.\nThis ticket must allow // as base, and resolve the protocol.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/263",
            "https://github.com/apache/nutch/pull/263"
        ]
    },
    "NUTCH-2479": {
        "Key": "NUTCH-2479",
        "Summary": "urlmeta plugin port from 1.x to 2.x",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "nutch server,                                            plugin,                                            REST_api",
        "Assignee": null,
        "Reporter": "Ninaad Joshi",
        "Created": "13/Dec/17 08:43",
        "Updated": "15/Nov/19 10:47",
        "Resolved": "15/Nov/19 10:47",
        "Description": "I have ported urlmeta plugin available in 1.x to 2.x\nIt is designed to do two things:\n\nMeta Tags that are supplied with your Crawl URLs, during injection either through seed.txt or through REST API, will be propagated throughout the out-links of those Crawl URLs\nWhen you index your URLs, the meta tags that you specified with your URLs will be indexed alongside those URLs--and can be directly queried, assuming you have done everything else correctly.\n\nI have also added support through the NutchServer REST-API. Have Attached patch along with this issue.",
        "Issue Links": []
    },
    "NUTCH-2480": {
        "Key": "NUTCH-2480",
        "Summary": "Upgrade crawler-commons dependency to 0.9",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "build,                                            deployment",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Dec/17 10:47",
        "Updated": "18/Dec/17 16:53",
        "Resolved": "15/Dec/17 19:36",
        "Description": "Crawler-commons 0.9 is relased. We should upgrade the dependency: there are significant improvements in the sitemap parser, also crawler-commons 0.9 depends on Tika 1.16 which minimizes the gap to Tika 1.17 (NUTCH-2439).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/260"
        ]
    },
    "NUTCH-2481": {
        "Key": "NUTCH-2481",
        "Summary": "HostDatum deltas(previous step statistics) and Metadata expressions",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "hostdb",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "15/Dec/17 12:58",
        "Updated": "15/Nov/19 10:47",
        "Resolved": null,
        "Description": "To allow the usage of previous step statistics(deltas of fetched,unfetced etc) in hostdb. The motivation is usage of this statistics in generate with maxCount expressions.\n\u00a0\nThe solution allows to fill in metadata of hostdatum based on custom JEXL expression using two hostdatum: before update(previousHostDatum) and after update(currentHostDatum)..\nFor example to fill in difference in quantity of fetched at round t and t-1 we can use the following expression\n<property>\n <name>hostdb.deltaExpression</name>\n <value>{return new (\"javafx.util.Pair\",\"FetchedDelta\", currentHostDatum.fetched - previousHostDatum.fetched);}</value>\n</property>\nA pull request will be provided shortly.",
        "Issue Links": [
            "/jira/browse/NUTCH-2368",
            "https://github.com/apache/nutch/pull/278",
            "https://github.com/apache/nutch/pull/278/commits/97083fa17234194ab2a35e9fbf7f11de42d87bef"
        ]
    },
    "NUTCH-2482": {
        "Key": "NUTCH-2482",
        "Summary": "index-geoip not to add null values to document fields",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.16",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Dec/17 15:32",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "30/Sep/19 07:24",
        "Description": "The plugin index-geoip may add null values to document fields which then cause further errors, here a NPE in IndexingFiltersChecker when toString() is called on null:\n\n$ bin/nutch indexchecker -Dstore.ip.address=true -Dindex.geoip.usage=cityDatabase \\\r\n     -Dplugin.includes=\"protocol-http|parse-html|index-(basic|geoip)\" http://www.example.com/\r\n...\r\nException in thread \"main\" java.lang.NullPointerException\r\n        at org.apache.nutch.indexer.IndexingFiltersChecker.fetch(IndexingFiltersChecker.java:340)\r\n        at org.apache.nutch.indexer.IndexingFiltersChecker.run(IndexingFiltersChecker.java:127)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.nutch.indexer.IndexingFiltersChecker.main(IndexingFiltersChecker.java:370)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/476"
        ]
    },
    "NUTCH-2483": {
        "Key": "NUTCH-2483",
        "Summary": "Remove/replace indirect dependencies to org.json",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.14",
        "Component/s": "deployment",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "16/Dec/17 17:19",
        "Updated": "18/Dec/17 17:52",
        "Resolved": "18/Dec/17 17:12",
        "Description": "As indirect transitive dependency we ship with Nutch 1.x binary packages a jar file of org.json which license is since one year among the category x licenses (see also license faq).\nWe should check whether the library is mandatory and the exclude or replace it.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/265"
        ]
    },
    "NUTCH-2484": {
        "Key": "NUTCH-2484",
        "Summary": "Extend indexer-elastic-rest to support languages",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.14",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Dec/17 18:51",
        "Updated": "01/Jun/18 19:04",
        "Resolved": "16/Dec/17 18:53",
        "Description": "This issue is being retrospectively created to track https://github.com/apache/nutch/pull/257",
        "Issue Links": []
    },
    "NUTCH-2485": {
        "Key": "NUTCH-2485",
        "Summary": "ParserFactory swallows exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "18/Dec/17 15:14",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "02/Jul/18 14:31",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2486": {
        "Key": "NUTCH-2486",
        "Summary": "Compiler Warning: Unchecked / unsafe operations in MimeTypeIndexingFilter",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Moreno Feltscher",
        "Created": "19/Dec/17 14:49",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "19/Dec/17 14:57",
        "Description": "When compiling Nutch source, the following warning is being shown:\n\nNote: src/plugin/mimetype-filter/src/java/org/apache/nutch/indexer/filter/MimeTypeIndexingFilter.java uses unchecked or unsafe operations.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/267"
        ]
    },
    "NUTCH-2487": {
        "Key": "NUTCH-2487",
        "Summary": "Fetcher thread stopped due to constraint violation",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "dhirajforyou",
        "Created": "26/Dec/17 05:48",
        "Updated": "26/Dec/17 09:51",
        "Resolved": "26/Dec/17 06:41",
        "Description": "FetcherThread 42 fetch of https://nutch.apache.org/ failed with: java.lang.LinkageError: loader constraint violation: when resolving method \"org.slf4j.impl.StaticLoggerBinder.getLoggerFactory()Lorg/slf4j/ILoggerFactory;\" the class loader (instance of org/apache/nutch/plugin/PluginClassLoader) of the current class, org/slf4j/LoggerFactory, and the class loader (instance of sun/misc/Launcher$AppClassLoader) for the method's defining class, org/slf4j/impl/StaticLoggerBinder, have different Class objects for the type org/slf4j/ILoggerFactory used in the signature\n\tat org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:299)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:269)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:281)\n\tat ir.co.bayan.simorq.zal.extractor.nutch.ExtractorParser.<clinit>(ExtractorParser.java:29)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat java.lang.Class.newInstance(Class.java:442)\n\tat org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:161)\n\tat org.apache.nutch.parse.ParserFactory.getParsers(ParserFactory.java:138)\n\tat org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:80)\n\tat org.apache.nutch.fetcher.FetcherThread.output(FetcherThread.java:631)\n\tat org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:375)",
        "Issue Links": []
    },
    "NUTCH-2488": {
        "Key": "NUTCH-2488",
        "Summary": "Please use SSL (https) for KEYS, sigs, hashes",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebb",
        "Created": "27/Dec/17 19:24",
        "Updated": "08/Jan/18 13:27",
        "Resolved": "08/Jan/18 12:24",
        "Description": "As the subject says: Please use https (SSL) for links to KEYS, hashes, sigs.\nAlso the GPG verification instructions are wrong.\nThe command line should be (for example):\n$ gpg --verify apache-nutch-X.Y.Z-src.tar.gz.asc apache-nutch-X.Y.Z-src.tar.gz\nit's important that both the sig file (which must be the first parameter) and the target to be check are both specified, see:\nSee:\nhttps://www.apache.org/info/verification.html#CheckingSignatures",
        "Issue Links": []
    },
    "NUTCH-2489": {
        "Key": "NUTCH-2489",
        "Summary": "Dependency collision with lucene-analyzers-common in scoring-similarity plugin",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "scoring",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "28/Dec/17 16:57",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "08/Feb/18 05:17",
        "Description": "After updating to Master branch of 1.14, we get a few compile errors in LuceneTokenizer.java and LuceneAnalyzerUtil.java:\n\n\r\nType mismatch: cannot convert from org.apache.lucene.analysis.CharArraySet to org.apache.lucene.analysis.util.CharArraySet\r\n\n\nThis seems to be caused by the fact that scoring-similarity compiles with lucene-analyzers-common-5.5.0.jar (from ivy.xml), but with lucene-core-6.4.1 instead of the matching 5.5.0.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/284"
        ]
    },
    "NUTCH-2490": {
        "Key": "NUTCH-2490",
        "Summary": "Sitemap processing: Sitemap index files not working",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Moreno Feltscher",
        "Reporter": "Moreno Feltscher",
        "Created": "02/Jan/18 22:50",
        "Updated": "16/Jun/22 02:22",
        "Resolved": "03/Jan/18 17:41",
        "Description": "The sitemap processing feature does not properly handle sitemap index files due to a unnecessary conditional.",
        "Issue Links": [
            "/jira/browse/NUTCH-2467",
            "https://github.com/apache/nutch/pull/269",
            "https://github.com/apache/nutch/pull/735"
        ]
    },
    "NUTCH-2491": {
        "Key": "NUTCH-2491",
        "Summary": "Integrate sitemap processing and HostDB into crawl script",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Moreno Feltscher",
        "Reporter": "Moreno Feltscher",
        "Created": "03/Jan/18 14:14",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "03/Jan/18 17:35",
        "Description": "Add three new steps to the crawl bash script:\n1. Generate HostDB from CrawlDB\n2. Inject URLs from sitemaps URLs found in hosts from HostDb\n3. If given, inject sitemap URLs specified in a configuration file / in configuration files",
        "Issue Links": [
            "/jira/browse/NUTCH-2493",
            "https://github.com/apache/nutch/pull/270"
        ]
    },
    "NUTCH-2492": {
        "Key": "NUTCH-2492",
        "Summary": "Add more configuration parameters to crawl script",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Moreno Feltscher",
        "Reporter": "Moreno Feltscher",
        "Created": "03/Jan/18 23:30",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "08/Jan/18 11:50",
        "Description": "Instead of having to copy and adjust the crawl script in order to specify the following configuration options allow the user to pass them in using arguments:\n\nnumSlaves\nnumTasks\nsizeFetchlist\ntimeLimitFetch\nnumThreads",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/271"
        ]
    },
    "NUTCH-2493": {
        "Key": "NUTCH-2493",
        "Summary": "Add configuration parameter for sitemap processing to crawler script",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Moreno Feltscher",
        "Reporter": "Moreno Feltscher",
        "Created": "09/Jan/18 00:23",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "10/Jan/18 16:15",
        "Description": "While using the crawler script with the sitemap processing feature introduced in NUTCH-2491 I encountered some performance issues when working with large sitemaps.\nTherefore one should be able to specify if sitemap processing based on HostDB should take place and if so how frequently it should be done.",
        "Issue Links": [
            "/jira/browse/NUTCH-2491",
            "https://github.com/apache/nutch/pull/273"
        ]
    },
    "NUTCH-2494": {
        "Key": "NUTCH-2494",
        "Summary": "Fetcher: java.lang.IllegalArgumentException: Wrong FS: s3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "fetcher,                                            parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Ashraful Islam",
        "Created": "11/Jan/18 10:17",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "29/Jan/18 15:41",
        "Description": "We are using nutch 1.14 in AWS EMR Cluster (Hadoop 2.2.7).  trying to use S3 as main storage. \nWe are using the below command.\n\n\r\nbin/crawl -s s3://nutch-emr-cluster/test/crawl/urls s3://nutch-emr-cluster/test/crawl 1\r\n\n\nInjector and Generator completed successfully without any error and data written perfectly into S3. But in the Fetcher and Parser steps we are getting IllegalArgumentException\nFull stacktrace \n\n\r\n18/01/11 07:16:52 ERROR fetcher.Fetcher: Fetcher: java.lang.IllegalArgumentException: Wrong FS: s3://nutch-emr-cluster/test/crawl/segments/20180111071602/crawl_fetch, expected: hdfs://ip-172-31-26-180.eu-west-1.compute.internal:8020\r\n\tat org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:653)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:194)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)\r\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1430)\r\n\tat org.apache.nutch.fetcher.FetcherOutputFormat.checkOutputSpecs(FetcherOutputFormat.java:55)\r\n\tat org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:268)\r\n\tat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:139)\r\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\r\n\tat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\r\n\tat org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\r\n\tat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:575)\r\n\tat org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:570)\r\n\tat java.security.AccessController.doPrivileged(Native Method)\r\n\tat javax.security.auth.Subject.doAs(Subject.java:422)\r\n\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1698)\r\n\tat org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:570)\r\n\tat org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:561)\r\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:870)\r\n\tat org.apache.nutch.fetcher.Fetcher.fetch(Fetcher.java:486)\r\n\tat org.apache.nutch.fetcher.Fetcher.run(Fetcher.java:521)\r\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n\tat org.apache.nutch.fetcher.Fetcher.main(Fetcher.java:495)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)\r\n\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
        "Issue Links": [
            "/jira/browse/NUTCH-2281",
            "https://github.com/apache/nutch/pull/274"
        ]
    },
    "NUTCH-2495": {
        "Key": "NUTCH-2495",
        "Summary": "Use -deleteGone instead of clean job in crawler script while indexing",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.17",
        "Component/s": "bin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Moreno Feltscher",
        "Created": "12/Jan/18 23:22",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "30/Apr/20 09:09",
        "Description": "Instead of running bin/nutch clean after indexing the documents run bin/nutch index with the -deleteGone flag which instead of just deleting gone and duplicated documents also deletes redirects from the index.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/275",
            "https://github.com/apache/nutch/pull/517"
        ]
    },
    "NUTCH-2496": {
        "Key": "NUTCH-2496",
        "Summary": "Speed up link inversion step in crawling script",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.17",
        "Component/s": "linkdb",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Moreno Feltscher",
        "Created": "12/Jan/18 23:32",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "09/Jun/20 10:47",
        "Description": "While working on a project where I have to index a huge number of URLs I encountered an issue with the link inversion step of the crawling script. A while ago Ian Lopata stumbled upon the same issue as described here: http://lucene.472066.n3.nabble.com/InvertLinks-Performance-Nutch-1-6-td4183004.html\n\nI am running the invertlinks step in my Nutch 1.6 based crawl process on a \nsingle node.  I run invertlinks only because I need the Inlinks in the \nindexer step so as to store them with the document.  I do not need the \nanchor text and I am not scoring.  I am finding that invertlinks (and more \nspecifically the merge of the linkdb) takes a long time - about 30 minutes \nfor a crawl of around 150K documents.  I am looking for ways that I might \nshorten this processing time.  Any suggestions? \nBack then wastl-nagel suggested turning off the normalizers and filters during the inversion step which speeds up the process a bunch.\nIn my case however I kind of depend on those so this is no real solution.\nI opened this issue here in order to get some feedback on how we could improve things in a crawl script and speed up the process.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/527"
        ]
    },
    "NUTCH-2497": {
        "Key": "NUTCH-2497",
        "Summary": "Elastic REST Indexer: Allow multiple hosts",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Moreno Feltscher",
        "Reporter": "Moreno Feltscher",
        "Created": "13/Jan/18 01:10",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "18/Jan/18 17:48",
        "Description": "Allow specifying a list of Elasticsearch hosts to index documents to. This would be especially helpful when working with a Elasticsearch cluster which contains of multiple nodes.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/276"
        ]
    },
    "NUTCH-2498": {
        "Key": "NUTCH-2498",
        "Summary": "Docker files are outdated",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.5",
        "Component/s": "docker",
        "Assignee": null,
        "Reporter": "dhirajforyou",
        "Created": "13/Jan/18 08:57",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Docker file for hbase is outdated. It uses java 7 but, nutch requires java 8.\nCasandra docker file refers to meabed/debian-jdk, which is also based on java7.",
        "Issue Links": []
    },
    "NUTCH-2499": {
        "Key": "NUTCH-2499",
        "Summary": "Elastic REST Indexer: Duplicate values",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Moreno Feltscher",
        "Created": "16/Jan/18 21:42",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "23/Jan/18 17:58",
        "Description": "Due to a change in https://github.com/apache/nutch/commit/160758023e3de83894ae4fe654c17fde62aba50e#diff-408fd2f17bc9791dcbf531ffe6574a6a the Elastic REST indexer does not work with HashSets for values anymore but instead saves duplicated values as arrays.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/277"
        ]
    },
    "NUTCH-2500": {
        "Key": "NUTCH-2500",
        "Summary": "Add pull-reqest template to github",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Jan/18 11:03",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "24/May/18 12:36",
        "Description": "Github allows to add [pull request templates](https://help.github.com/articles/creating-a-pull-request-template-for-your-repository/). For contributors already familiar with github from other projects that's probably the best place to show a check list which helps us to get pull requests merged more quickly. Here's a draft:\n\nThanks for your contribution to [Apache Nutch](http://nutch.apache.org/)! Your help is appreciated!\r\n\r\nBefore opening the pull request, please verify that\r\n* there is an open issue on the [Nutch issue tracker](https://issues.apache.org/jira/projects/NUTCH) which describes the problem or the improvement. We cannot accept pull requests without an issue because the change wouldn't be listed in the release notes.\r\n* the issue ID (`NUTCH-XXXX`)\r\n\u00a0 - is referenced in the title of the pull request\r\n\u00a0 - and placed in front of your commit messages\r\n* commits are squashed into a single one (or few commits for larger changes)\r\n* Java source code follows [Nutch Eclipse Code Formatting rules](https://github.com/apache/nutch/blob/master/eclipse-codeformat.xml)\r\n* Nutch builds and unit tests pass by running `ant clean runtime test`",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/333"
        ]
    },
    "NUTCH-2501": {
        "Key": "NUTCH-2501",
        "Summary": "allow to set Java heap size when using crawl script in distributed mode",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.17",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Moreno Feltscher",
        "Created": "22/Jan/18 22:30",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "28/Apr/20 08:49",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/279",
            "https://github.com/apache/nutch/pull/513"
        ]
    },
    "NUTCH-2502": {
        "Key": "NUTCH-2502",
        "Summary": "Any23 Plugin: Add Content-Type filtering",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Moreno Feltscher",
        "Created": "23/Jan/18 13:28",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "23/Jan/18 18:01",
        "Description": "It should be possible to filter based on a document's Content-Type when using Any23 extractors.",
        "Issue Links": []
    },
    "NUTCH-2503": {
        "Key": "NUTCH-2503",
        "Summary": "Add option to run tests for a single plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Moreno Feltscher",
        "Reporter": "Moreno Feltscher",
        "Created": "23/Jan/18 16:06",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "23/Jan/18 17:52",
        "Description": "Sometimes it makes sense to just run tests for a single plugin instead of building all plugins and running all tests at once.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/281"
        ]
    },
    "NUTCH-2504": {
        "Key": "NUTCH-2504",
        "Summary": "Results of maxCountExpr and fetchDelayExpr should be stored in memory in Generate",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "25/Jan/18 14:54",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "With NUTCH-2455 the expressions\u00a0maxCountExpr and fetchDelayExpr are calculated for each value. That slows the process, instead we can store the results for each host in\u00a0hostDomainCounts.\u00a0\nThat will take only 2 x sizeof(long) extra memory per host.",
        "Issue Links": []
    },
    "NUTCH-2505": {
        "Key": "NUTCH-2505",
        "Summary": "nutch does not delete the .locked file, when the generator partition got an exception",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Ajoy Lian",
        "Created": "27/Jan/18 07:56",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "07/Jun/18 19:23",
        "Description": "nutch does not delete the\u00a0.locked file\u00a0when the generator partition got an\u00a0exception.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/282"
        ]
    },
    "NUTCH-2506": {
        "Key": "NUTCH-2506",
        "Summary": "host is not available for filtering on the JEXL indexing plugin",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "30/Jan/18 12:55",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The host\u00a0attribute is not available for filtering on the JexlIndexingFilter. Take a look at the documentation on https://github.com/apache/nutch/blob/master/conf/nutch-default.xml#L1653-L1667. \nThis could be quite useful, although the url.",
        "Issue Links": []
    },
    "NUTCH-2507": {
        "Key": "NUTCH-2507",
        "Summary": "NutchTutorial wiki pages as a lot of outdated command line calls when it starts with the solr interaction",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.17",
        "Component/s": "documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "artodeto",
        "Created": "31/Jan/18 11:14",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "30/Apr/20 11:30",
        "Description": "h2.\u00a0Section \"Step-by-Step: Indexing into Apache Solr\"\nreplace:\n\n\r\nExample: bin/nutch index http://localhost:8983/solr crawl/crawldb/ -linkdb crawl/linkdb/ crawl/segments/20131108063838/ -filter -normalize -deleteGone\n\nwith:\n\n\r\nExample: bin/nutch index -Dsolr.server.url=http://localhost:8983/solr/nutch ${NUTCH_RUNTIME_HOME}/crawl\r\n/crawldb/ -linkdb ${NUTCH_RUNTIME_HOME}/crawl\r\n/linkdb/ ${NUTCH_RUNTIME_HOME}/crawl\r\n/segments/20131108063838\r\n/ -filter -normalize -deleteGo\n\n\u00a0\nSection \"Step-by-Step: Deleting Duplicates\"\nreplace:\n\n\r\n     Usage: bin/nutch dedup <solr url>\r\n     Example: /bin/nutch dedup http://localhost:8983/solr\r\n\n\nwith:\n\n\r\n     Usage: bin/nutch dedup <path to the crawldb> <solr url>\r\n     Example: /bin/nutch dedup ${NUTCH_RUNTIME_HOME}/crawl/crawldb/ http://localhost:8983/sol\r\n\n\nSection \"Step-by-Step: Cleaning Solr\"\nreplace:\n\n\r\n     Usage: bin/nutch clean -Dsolr.server.url=<solr index url> <crawldb>\r\n     Example: /bin/nutch clean -Dsolr.server.url=http://localhost:8983/solr/nutch crawl/crawldb/\r\n\n\nwith:\n\n\r\n     Usage: bin/nutch clean -Dsolr.server.url=<solr index url> <crawldb>\r\n     Example: /bin/nutch clean -Dsolr.server.url=http://localhost:8983/solr/nutch ${NUTCH_RUNTIME_HOME}/crawl/crawldb/",
        "Issue Links": []
    },
    "NUTCH-2508": {
        "Key": "NUTCH-2508",
        "Summary": "Misleading documentation about http.proxy.exception.list",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Moreno Feltscher",
        "Reporter": "Moreno Feltscher",
        "Created": "31/Jan/18 22:38",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "31/Jan/18 22:59",
        "Description": "The description about http.proxy.exception.list states that domains as well as URLs can be configured to be excluded from being routed through a pre-configured proxy. This is misleading since only hosts are being checked when using this feature.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/283"
        ]
    },
    "NUTCH-2509": {
        "Key": "NUTCH-2509",
        "Summary": "Inconsistent behavior in SitemapProcessor",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "sitemap",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "13/Feb/18 16:51",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "02/Apr/18 16:09",
        "Description": "There are two inconsistent behaviors in\u00a0SitemapProcessor:\n\nThere is a member variable\u00a0maxRedir that is supposed to limit the number of redirections on sitemap URLs, and it is initialized from config property\u00a0sitemap.redir.max, but it is ignored in the code because a local variable with the same name is defined in the relevant method, and is always set to 3.\nWhen a sitemap URL goes through redirect, it is filtered and normalized. However, if a sitemap URL comes from a sitemapindex, it is not. This seems inconsistent, as in both cases we have a URL from an outside source.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/301"
        ]
    },
    "NUTCH-2510": {
        "Key": "NUTCH-2510",
        "Summary": "Crawl script modification. HostDb : generate, optional usage and description",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "bin",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "14/Feb/18 10:33",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "02/Jul/18 14:28",
        "Description": "Script crawl now includes hostdb update as a part of crawling cycle, but :\n1) There is no hostdb parameter for generate\n2) Generation of hostdb is not optional, therefore hostdb is generated each step without asking of user. It should be an optional parameter.\n3) Description of 1 and 2.",
        "Issue Links": [
            "/jira/browse/NUTCH-2524",
            "https://github.com/apache/nutch/pull/286"
        ]
    },
    "NUTCH-2511": {
        "Key": "NUTCH-2511",
        "Summary": "SitemapProcessor limited by http.content.limit",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.17",
        "Component/s": "sitemap",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "19/Feb/18 16:35",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "15/Oct/19 12:03",
        "Description": "Because\u00a0SitemapProcessor uses the\u00a0HTTP protocol plugin, which limits the size of a response to\u00a0http.content.limit (64KB by default), it can only handle sitemaps smaller than that size.\u00a0\nI don't believe that is the intent of the users by setting\u00a0http.content.limit - they want to limit document size, not sitemap size. The spec specifically says that sitemaps can be up to\u00a050MB.",
        "Issue Links": [
            "/jira/browse/NUTCH-2666"
        ]
    },
    "NUTCH-2512": {
        "Key": "NUTCH-2512",
        "Summary": "Nutch does not build under JDK9",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.19",
        "Component/s": "build,                                            injector",
        "Assignee": null,
        "Reporter": "Ralf",
        "Created": "22/Feb/18 14:23",
        "Updated": "21/Mar/21 15:32",
        "Resolved": "21/Mar/21 15:32",
        "Description": "Nutch 1.14 (Source) does not compile properly under JDK 9\nNutch 1.14 (Binary) does not function under Java 9\n\u00a0\nWhen trying to Nuild Nutch, Ant complains about missing Sonar files then exits with:\n\"BUILD FAILED\n/home/nutch/nutch/build.xml:79: Unparseable date: \"01/25/1971 2:00 pm\" \"\n\u00a0\nOnce having commented out the \"offending code\" the Build finishes but the resulting Binary fails to function (as well as the Apache Compiled Binary distribution), Both exit with:\n\u00a0\nInjecting seed URLs\n/home/nutch/nutch2/bin/nutch inject searchcrawl//crawldb urls/\nInjector: starting at 2018-02-21 02:02:16\nInjector: crawlDb: searchcrawl/crawldb\nInjector: urlDir: urls\nInjector: Converting injected urls to crawl db entries.\nWARNING: An illegal reflective access operation has occurred\nWARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/home/nutch/nutch2/lib/hadoop-auth-2.7.4.jar) to method sun.security.krb5.Config.getInstance()\nWARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\nWARNING: All illegal access operations will be denied in a future release\nInjector: java.lang.NullPointerException\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getBlockIndex(FileInputFormat.java:444)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:413)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.lib.input.DelegatingInputFormat.getSplits(DelegatingInputFormat.java:115)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:301)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:318)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/java.security.AccessController.doPrivileged(Native Method)\n\u00a0 \u00a0 \u00a0 \u00a0 at java.base/javax.security.auth.Subject.doAs(Subject.java:423)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.crawl.Injector.inject(Injector.java:417)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.crawl.Injector.run(Injector.java:563)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.crawl.Injector.main(Injector.java:528)\n\u00a0\nError running:\n\u00a0 /home/nutch/nutch2/bin/nutch inject searchcrawl//crawldb urls/\nFailed with exit value 255.",
        "Issue Links": [
            "/jira/browse/NUTCH-2857"
        ]
    },
    "NUTCH-2513": {
        "Key": "NUTCH-2513",
        "Summary": "ant eclipse target fails with \"protocol switch unsafe\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.14",
        "Fix Version/s": "2.4,                                            1.15",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "dhirajforyou",
        "Created": "23/Feb/18 08:48",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "08/May/18 11:32",
        "Description": "ant eclipse failed as the url to sourceforge was\u00a0getting redirected to https from http.\n\u00a0\nant-eclipse-download:\n[get] Getting: http://downloads.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2\n[get] To: /home/govind/apache/nutch/build/ant-eclipse-1.0.bin.tar.bz2\n[get] http://downloads.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2 moved to https://excellmedia.dl.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2\nTarget 'ant-eclipse-download' failed with message 'Redirection detected from http to https. Protocol switch unsafe, not allowed.'.\nCannot execute 'eclipse' - 'ant-eclipse-download' failed or was not executed.\nBUILD FAILED\n/home/govind/apache/nutch/build.xml:1006: Redirection detected from http to https. Protocol switch unsafe, not allowed.",
        "Issue Links": []
    },
    "NUTCH-2514": {
        "Key": "NUTCH-2514",
        "Summary": "Segmentation Fault issue  while running crawl job.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Incomplete",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "crawldb,                                            fetcher,                                            indexer,                                            parser",
        "Assignee": null,
        "Reporter": "Kshitij Shukla",
        "Created": "23/Feb/18 09:40",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "10/May/18 10:59",
        "Description": "Error occurs while running crawl job in on fetching, parsing and indexing phase. error posting below:-\nExitCodeException exitCode=139: /bin/bash: line 1: 68684 Segmentation fault      /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.161.x86_64/bin/java -Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN -Xmx13312m -Djava.io.tmpdir=/tmp/hadoop-root/nm-local-dir/usercache/root/appcache/application_1519286094099_0016/container_1519286094099_0016_01_000003/tmp -Dlog4j.configuration=container-log4j.properties -Dyarn.app.container.log.dir=/home/c1/hadoop-2.5.2/logs/userlogs/application_1519286094099_0016/container_1519286094099_0016_01_000003 -Dyarn.app.container.log.filesize=0 -Dhadoop.root.logger=INFO,CLA org.apache.hadoop.mapred.YarnChild 95.142.101.139 35714 attempt_1519286094099_0016_r_000000_0 3 > /home/c1/hadoop-2.5.2/logs/userlogs/application_1519286094099_0016/container_1519286094099_0016_01_000003/stdout 2> /home/c1/hadoop-2.5.2/logs/userlogs/application_1519286094099_0016/container_1519286094099_0016_01_000003/stderr\n\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:538)\n\tat org.apache.hadoop.util.Shell.run(Shell.java:455)\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:702)\n\tat org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:195)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:300)\n\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:81)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1152)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:622)\n\tat java.lang.Thread.run(Thread.java:748)",
        "Issue Links": []
    },
    "NUTCH-2515": {
        "Key": "NUTCH-2515",
        "Summary": "Bad return type error(Stack map does not match) while running crawl job.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Kshitij Shukla",
        "Created": "23/Feb/18 09:48",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "Error occurs while running crawl job in indexing phase. Below mentioning error:-\nError: Stack map does not match the one at exception handler 105 Exception Details: Location: org/apache/nutch/indexer/solr/SolrDeleteDuplicates$SolrInputFormat.createRecordReader(Lorg/apache/hadoop/mapreduce/InputSplit;Lorg/apache/hadoop/mapreduce/TaskAttemptContext;)Lorg/apache/hadoop/mapreduce/RecordReader; @105: astore Reason: Type 'org/apache/solr/client/solrj/impl/HttpSolrServer' (current frame, locals[4]) is not assignable to 'org/apache/solr/client/solrj/SolrServer' (stack map, locals[4]) Current Frame: bci: @93 flags: { } locals: \n{ 'org/apache/nutch/indexer/solr/SolrDeleteDuplicates$SolrInputFormat', 'org/apache/hadoop/mapreduce/InputSplit', 'org/apache/hadoop/mapreduce/TaskAttemptContext', 'org/apache/hadoop/conf/Configuration', 'org/apache/solr/client/solrj/impl/HttpSolrServer', 'org/apache/nutch/indexer/solr/SolrDeleteDuplicates$SolrInputSplit', integer, 'org/apache/solr/client/solrj/SolrQuery' }\n stack: \n{ 'org/apache/solr/client/solrj/SolrServerException' } Stackmap Frame: bci: @105 flags: { } locals: { 'org/apache/nutch/indexer/solr/SolrDeleteDuplicates$SolrInputFormat', 'org/apache/hadoop/mapreduce/InputSplit', 'org/apache/hadoop/mapreduce/TaskAttemptContext', 'org/apache/hadoop/conf/Configuration', 'org/apache/solr/client/solrj/SolrServer', 'org/apache/nutch/indexer/solr/SolrDeleteDuplicates$SolrInputSplit', integer, 'org/apache/solr/client/solrj/SolrQuery' } stack: { 'org/apache/solr/client/solrj/SolrServerException' }\n Bytecode: 0000000: 2cb9 0018 0100 4e2d b800 043a 042b c000 0000010: 153a 0519 05b6 0019 8836 06bb 0005 5912 0000020: 06b7 0007 3a07 1907 07bd 0008 5903 1209 0000030: 5359 0412 1a53 5905 121b 5359 0612 1c53 0000040: b600 0a57 1907 1905 b600 1db8 000b b600 0000050: 1e57 1907 1506 b800 0bb6 000c 5719 0419 0000060: 07b6 000d 3a08 a700 0f3a 09bb 000f 5919 0000070: 09b7 0010 bf19 08b6 0011 3a09 bb00 1f59 0000080: 1909 1506 b700 20b0 Exception Handler Table: bci [93, 102] => handler: 105 Stackmap Table: full_frame(@105,\n{Object[#69],Object[#92],Object[#93],Object[#71],Object[#72],Object[#94],Integer,Object[#73]}\n,\n{Object[#74]}\n) append_frame(@117,Object75)",
        "Issue Links": []
    },
    "NUTCH-2516": {
        "Key": "NUTCH-2516",
        "Summary": "Hadoop imports use wildcards",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Feb/18 23:15",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "27/Mar/18 13:49",
        "Description": "Right now the Hadoop imports use wildcards all over the place. \nWe wanted to address this during NUTCH-2375 but didn't get around to it.\nWe should address it in a new issue as it is still important.",
        "Issue Links": [
            "/jira/browse/NUTCH-2427",
            "https://github.com/apache/nutch/pull/295"
        ]
    },
    "NUTCH-2517": {
        "Key": "NUTCH-2517",
        "Summary": "mergesegs corrupts segment data",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "segment",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Marco Ebbinghaus",
        "Created": "04/Mar/18 14:03",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "26/Apr/18 11:09",
        "Description": "The problem probably occurs since commit https://github.com/apache/nutch/commit/54510e503f7da7301a59f5f0e5bf4509b37d35b4\nHow to reproduce:\n\ncreate container from apache/nutch image (latest)\nopen terminal in that container\nset http.agent.name\ncreate crawldir and urls file\nrun bin/nutch inject (bin/nutch inject mycrawl/crawldb urls/urls)\nrun bin/nutch generate (bin/nutch generate mycrawl/crawldb mycrawl/segments 1)\n\t\nthis results in a segment (e.g. 20180304134215)\n\n\nrun bin/nutch fetch (bin/nutch fetch mycrawl/segments/20180304134215 -threads 2)\nrun bin/nutch parse (bin/nutch parse mycrawl/segments/20180304134215 -threads 2)\n\t\nls in the segment folder -> existing folders: content, crawl_fetch, crawl_generate, crawl_parse, parse_data, parse_text\n\n\nrun bin/nutch updatedb (bin/nutch updatedb mycrawl/crawldb mycrawl/segments/20180304134215)\nrun bin/nutch mergesegs (bin/nutch mergesegs mycrawl/MERGEDsegments mycrawl/segments/* -filter)\n\t\nconsole output: `SegmentMerger: using segment data from: content crawl_generate crawl_fetch crawl_parse parse_data parse_text`\nresulting segment: 20180304134535\n\n\nls in mycrawl/MERGEDsegments/segment/20180304134535 -> only existing folder: crawl_generate\nrun bin/nutch invertlinks (bin/nutch invertlinks mycrawl/linkdb -dir mycrawl/MERGEDsegments) which results in a consequential error\n\t\nconsole output: `LinkDb: adding segment: file:/root/nutch_source/runtime/local/mycrawl/MERGEDsegments/20180304134535\n LinkDb: org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/root/nutch_source/runtime/local/mycrawl/MERGEDsegments/20180304134535/parse_data\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:323)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:265)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:59)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:387)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:301)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:318)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)\n \u00a0\u00a0 \u00a0at java.security.AccessController.doPrivileged(Native Method)\n \u00a0\u00a0 \u00a0at javax.security.auth.Subject.doAs(Subject.java:422)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)\n \u00a0\u00a0 \u00a0at org.apache.nutch.crawl.LinkDb.invert(LinkDb.java:224)\n \u00a0\u00a0 \u00a0at org.apache.nutch.crawl.LinkDb.run(LinkDb.java:353)\n \u00a0\u00a0 \u00a0at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n \u00a0\u00a0 \u00a0at org.apache.nutch.crawl.LinkDb.main(LinkDb.java:313)`\n\n\n\nSo as it seems mapreduce corrupts the segment folder during mergesegs command.\n\u00a0\nPay attention to the fact that this issue is not related on trying to merge a single segment like described above. As you can see on the attached screenshot that problem also appears when executing multiple bin/nutch generate/fetch/parse/updatedb commands before executing mergesegs - resulting in a segment count > 1.",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/293",
            "https://github.com/apache/nutch/pull/321"
        ]
    },
    "NUTCH-2518": {
        "Key": "NUTCH-2518",
        "Summary": "Must check return value of job.waitForCompletion()",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "crawldb,                                            fetcher,                                            generator,                                            hostdb,                                            linkdb",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "05/Mar/18 11:16",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "04/Apr/18 10:59",
        "Description": "The return value of job.waitForCompletion() of the new MapReduce API (NUTCH-2375) must always be checked. If it's not true, the job has been failed or killed. Accordingly, the program\n\nshould not proceed with further jobs/steps\nmust clean-up temporary data, unlock CrawlDB, etc.\nexit with non-zero exit value, so that scripts running the crawl workflow can handle the failure\n\nCf. NUTCH-2076, NUTCH-2442, NUTCH-2375 PR #221.",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "/jira/browse/NUTCH-2161",
            "/jira/browse/NUTCH-1783",
            "https://github.com/apache/nutch/pull/299",
            "https://github.com/apache/nutch/pull/307"
        ]
    },
    "NUTCH-2519": {
        "Key": "NUTCH-2519",
        "Summary": "Log mapreduce job counters in local mode",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.14",
        "Fix Version/s": "2.4,                                            1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "05/Mar/18 12:26",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "06/Mar/18 08:20",
        "Description": "A simple change in the log4j.properties would make the Hadoop job counters appear in the hadoop.log also in local mode:\n\nlog4j.logger.org.apache.hadoop.mapreduce.Job=INFO\r\n\n\nThis may provide useful information for debugging, esp. if counters are not explicitly logged by tools (see @user). This would make the output also more similar to (pseudo)distributed mode (Nutch is called via hadoop jar) Job counters and progress info are obligatorily logged.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/287"
        ]
    },
    "NUTCH-2520": {
        "Key": "NUTCH-2520",
        "Summary": "Wrong Accept-Charset sent when http.accept.charset is not defined",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4,                                            1.14",
        "Fix Version/s": "2.4,                                            1.15",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "05/Mar/18 12:38",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "06/Mar/18 08:44",
        "Description": "When the property http.accept.charset is not defined, instead of the hard-wired default value utf-8,iso-8859-1;q=0.7,*;q=0.7 that of the \"Accept\" field is used. Introduced by NUTCH-2376 (HttpBase).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/288"
        ]
    },
    "NUTCH-2521": {
        "Key": "NUTCH-2521",
        "Summary": "SitemapProcessor to use property sitemap.redir.max",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "05/Mar/18 12:45",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "06/Mar/18 08:45",
        "Description": "SitemapProcessor isn't actually using the property sitemap.redir.max (NUTCH-2466), instead the maximum number of redirects is hardwired (=3).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/289"
        ]
    },
    "NUTCH-2522": {
        "Key": "NUTCH-2522",
        "Summary": "Bidirectional URL exemption filter",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "05/Mar/18 13:05",
        "Updated": "16/Mar/18 18:05",
        "Resolved": null,
        "Description": "The current Nutch Url Exemption plugin exempts based on toUrl only, the new plugin uses both fromUrl and toUrl and after the regex transformation, exempts based on condition regex(fromUrl) == regex(toUrl).\nThis approach allows us to perform more complex url exemption filter checks, such as allow links:\nhttp://[www.website.com/|http://www.website.com/]home ->\u00a0http://[website.com/a|http://www.website.com/]bout ( with/without www).",
        "Issue Links": [
            "/jira/browse/NUTCH-2537",
            "/jira/browse/NUTCH-2538",
            "https://github.com/apache/nutch/pull/290"
        ]
    },
    "NUTCH-2523": {
        "Key": "NUTCH-2523",
        "Summary": "UpdateHostDB blocks usage of plugins unintentionally",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "hostdb",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "05/Mar/18 13:19",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "19/Mar/18 14:29",
        "Description": "UpdateHostDB blocks the use of\u00a0urlnormalizer-host and\u00a0urlfilter-domainblacklist (it check if they are configured and throws an exception) without any good reason.\nQuoting Markus: \"I simply reused the job setup code and forgot to remove that check. You can safely remove that check in HostDB.\"",
        "Issue Links": [
            "https://lists.apache.org/thread.html/1bfede50999344d20c3b6172bdddb6615f4bf5555293c4d91431bf1d@%3Cuser.nutch.apache.org%3E"
        ]
    },
    "NUTCH-2524": {
        "Key": "NUTCH-2524",
        "Summary": "bin/crawl: fix check for HostDb in distributed mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "bin",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "06/Mar/18 10:09",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "12/Mar/18 11:58",
        "Description": "In crawl script you can find something like\u00a0\nif [[ -d \"$CRAWL_PATH\"/hostdb ]]; then\n echo \"Processing sitemaps based on hosts in HostDB\"\n __bin_nutch sitemap \"$CRAWL_PATH\"/crawldb -hostdb \"$CRAWL_PATH\"/hostdb -threads $NUM_THREADS\n fi\nif [[ -d \"$CRAWL_PATH\"/hostdb ]]; doesnt work for HDFS only for local mode.",
        "Issue Links": [
            "/jira/browse/NUTCH-2510",
            "https://github.com/apache/nutch/pull/291"
        ]
    },
    "NUTCH-2525": {
        "Key": "NUTCH-2525",
        "Summary": "Metadata indexer cannot handle uppercase parse metadata",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Mar/18 11:32",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "19/Jan/20 20:31",
        "Description": "MetadataIndexer lowercases keys for parse metadata, making it impossible to index metadata containing uppercase.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/493"
        ]
    },
    "NUTCH-2526": {
        "Key": "NUTCH-2526",
        "Summary": "NPE in scoring-opic when indexing document without CrawlDb datum",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "parser,                                            scoring",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Yash Thenuan",
        "Created": "07/Mar/18 13:30",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "26/Apr/18 10:59",
        "Description": "I was trying to write a parse filter plugin whose work was to parse internal links as a separate document.what I did basically is,breaking the page into multiple parseResults each parseResult having ParseText and ParseData corresponding to the InternalLinks. I was successfully able to parse them separately. But at the time of Scoring Some Error occurred.\nI am attaching the Logs for Indexing.\n\u00a0\n\u00a02018-03-07 15:41:52,327 INFO  indexer.IndexerMapReduce - IndexerMapReduce: crawldb: crawl/crawldb\n2018-03-07 15:41:52,327 INFO  indexer.IndexerMapReduce - IndexerMapReduce: linkdb: crawl/linkdb\n2018-03-07 15:41:52,327 INFO  indexer.IndexerMapReduce - IndexerMapReduces: adding segment: crawl/segments/20180307130959\n2018-03-07 15:41:53,677 INFO  anchor.AnchorIndexingFilter - Anchor deduplication is: off\n2018-03-07 15:41:54,861 INFO  indexer.IndexWriters - Adding org.apache.nutch.indexwriter.elasticrest.ElasticRestIndexWriter\n2018-03-07 15:41:55,168 INFO  client.AbstractJestClient - Setting server pool to a list of 1 servers: http://localhost:9200\n2018-03-07 15:41:55,170 INFO  client.JestClientFactory - Using multi thread/connection supporting pooling connection manager\n2018-03-07 15:41:55,238 INFO  client.JestClientFactory - Using default GSON instance\n2018-03-07 15:41:55,238 INFO  client.JestClientFactory - Node Discovery disabled...\n2018-03-07 15:41:55,238 INFO  client.JestClientFactory - Idle connection reaping disabled...\n2018-03-07 15:41:55,282 INFO  elasticrest.ElasticRestIndexWriter - Processing remaining requests [docs = 1, length = 210402, total docs = 1]\n2018-03-07 15:41:55,361 INFO  elasticrest.ElasticRestIndexWriter - Processing to finalize last execute\n2018-03-07 15:41:55,458 INFO  elasticrest.ElasticRestIndexWriter - Previous took in ms 175, including wait 97\n2018-03-07 15:41:55,468 WARN  mapred.LocalJobRunner - job_local1561152089_0001\njava.lang.Exception: java.lang.NullPointerException\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\nCaused by: java.lang.NullPointerException\n\tat org.apache.nutch.scoring.opic.OPICScoringFilter.indexerScore(OPICScoringFilter.java:171)\n\tat org.apache.nutch.scoring.ScoringFilters.indexerScore(ScoringFilters.java:120)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:296)\n\tat org.apache.nutch.indexer.IndexerMapReduce.reduce(IndexerMapReduce.java:57)\n\tat org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)\n\tat org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)\n\tat org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n2018-03-07 15:41:55,510 ERROR indexer.IndexingJob - Indexer: java.io.IOException: Job failed!\n\tat org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:873)\n\tat org.apache.nutch.indexer.IndexingJob.index(IndexingJob.java:147)\n\tat org.apache.nutch.indexer.IndexingJob.run(IndexingJob.java:230)\n\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\n\tat org.apache.nutch.indexer.IndexingJob.main(IndexingJob.java:239)",
        "Issue Links": [
            "/jira/browse/NUTCH-2456",
            "https://github.com/apache/nutch/pull/324",
            "https://lists.apache.org/thread.html/%3CCADfrkit5BGSP08jC2a5GkGAeBEx1j8EPQm2947Ce0cm8V2xwJw@mail.gmail.com%3E"
        ]
    },
    "NUTCH-2527": {
        "Key": "NUTCH-2527",
        "Summary": "URL filter: provide rules to exclude localhost and private address spaces",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "2.3.1,                                            1.14",
        "Fix Version/s": "2.4,                                            1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "07/Mar/18 16:43",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "26/Apr/18 10:43",
        "Description": "While checking the log files of a large web crawl, I've found hundreds of (luckily failed) requests of local or private content:\n\n2018-02-18 04:48:34,022 INFO [FetcherThread] org.apache.nutch.fetcher.Fetcher: fetching http://127.0.0.42/ ...\r\n018-02-18 04:48:34,022 INFO [FetcherThread] org.apache.nutch.fetcher.Fetcher: fetch of http://127.0.0.42/ failed with: java.net.ConnectException: Connection refused (Connection refused)\r\n\n\nURLs pointing to localhost, loop-back addresses, private address spaces should be blocked for a wider web crawl where links are not controlled to avoid that information is leaked by links or redirects pointing to web interfaces of services running on the crawling machines (e.g., HDFS, Hadoop YARN).\nOf course, this must be optional. For testing it's quite common to crawl your local machine.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/292"
        ]
    },
    "NUTCH-2528": {
        "Key": "NUTCH-2528",
        "Summary": "Getting error :-Invalid Utf-8 character while running a crawl job at indexer phase.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Kshitij Shukla",
        "Created": "12/Mar/18 09:58",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "INFO [main] org.apache.hadoop.mapred.MapTask: Ignoring exception during close for org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector@4eb14055\norg.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server ..... [com.ctc.wstx.exc.WstxLazyException] Invalid UTF-8 character 0xffff at char #20261885, byte #20308993)\n at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:575)\n at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:241)\n at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:230)\n at org.apache.solr.client.solrj.SolrRequest.process(SolrRequest.java:150)\n at org.apache.solr.client.solrj.SolrClient.add(SolrClient.java:107)\n at org.apache.solr.client.solrj.SolrClient.add(SolrClient.java:72)\n at org.apache.solr.client.solrj.SolrClient.add(SolrClient.java:86)\n at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:97)\n at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:114)\n at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:54)\n at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.close(MapTask.java:647)\n at org.apache.hadoop.mapred.MapTask.closeQuietly(MapTask.java:1990)\n at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:774)\n at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)\n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)\n at java.security.AccessController.doPrivileged(Native Method)\n at javax.security.auth.Subject.doAs(Subject.java:421)\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
        "Issue Links": []
    },
    "NUTCH-2529": {
        "Key": "NUTCH-2529",
        "Summary": "\"ant runtime\" warns? about \"Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Krzysztof Madejski",
        "Created": "12/Mar/18 16:15",
        "Updated": "15/Nov/19 10:44",
        "Resolved": "15/Nov/19 10:44",
        "Description": "What am I missing? Is this a warning? An error?\n\n\r\n/opt/apache-nutch-2.3.1$ ant runtime\r\nBuildfile: /opt/apache-nutch-2.3.1/build.xml\r\nTrying to override old definition of task javac\r\n[taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\r\n\r\nivy-probe-antlib:\r\n\r\nivy-download:\r\n[taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\r\n\n\n\u00a0\nI've been following:\u00a0https://wiki.apache.org/nutch/Nutch2Tutorial",
        "Issue Links": []
    },
    "NUTCH-2530": {
        "Key": "NUTCH-2530",
        "Summary": "Rename property db.max.anchor.length > linkdb.max.anchor.length",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "12/Mar/18 16:31",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "08/Jun/18 09:49",
        "Description": "The property db.max.anchor.length affects the LinkDb (not CrawlDb) and should be named linkdb.max.anchor.length, see NUTCH-2220. Also:\n\nmove into LinkDb \"section\" in nutch-default.xml\ncheck/improve description\nadd warning about the new name on top of the release notes (CHANGES.txt)\n\nSee  discussion @user",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/343"
        ]
    },
    "NUTCH-2531": {
        "Key": "NUTCH-2531",
        "Summary": "Unclear steps in Nutch2 Tutorial",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "None",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Krzysztof Madejski",
        "Created": "12/Mar/18 16:57",
        "Updated": "13/Nov/19 15:15",
        "Resolved": "12/Nov/19 15:08",
        "Description": "I was trying to install Nutch based on this tutorial https://wiki.apache.org/nutch/Nutch2Tutorial:\n\u00a0\nIssues I've found:\nIn\u00a0Obtaining Software and Configuration:\n\n\"Specify the [...]\u00a0along with all of the other Configuration options suggested within the\u00a0Nutch 1.x tutorial.\"\n\u00a0 It would be better to copy necessary configuration. I don't have idea which settings exactly should be copied.\n\n2. \"In addition add the missing hbase-common-0.98.8-hadoop2.jar transitive dependency, this is a bug in gora-hbase 0.6.1 as described\u00a0here. This bug is removed in current Gora development.\"\n\u00a0\u00a0__\u00a0\u00a0What does this step require from me? Should I add something to the dependencies? In which file? This point is written in an informative manner. Should be either deleted or clear instruction should be given.\n3. \"N.B.\u00a0It's probably worth checking and setting all your usual configuration settings within $NUTCH_HOME/conf/nutch-site.xml etc. before progressing.\"\n\u00a0 \u00a0I'ts my first install. There is no such thing as \"usual configuration\"..\nIn \"Invoke Nutch\":\n\n\"nutch readdb\" doesn't return anything meaningful apart from Usage.\u00a0\n./nutch readdb\nUsage: WebTableReader (-stats | -url [url] | -dump <out_dir> [-regex regex]) \n[-crawlId <id>] [-content] [-headers] [-links] [-text]\n -crawlId <id> - the id to prefix the schemas to operate on, \n (default: storage.crawl.id)\n -stats [-sort] - print overall statistics to System.out\n[-sort] - list status sorted by host\n -url <url> - print information on <url> to System.out\n -dump <out_dir> [-regex regex] - dump the webtable to a text file in \n <out_dir>\n -content - dump also raw content\n -headers - dump protocol headers\n -links - dump links\n -text - dump extracted text\n[-regex] - filter on the URL of the webtable entry",
        "Issue Links": []
    },
    "NUTCH-2532": {
        "Key": "NUTCH-2532",
        "Summary": "Throw error if HBase is not available while running nutch commands.",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Krzysztof Madejski",
        "Created": "12/Mar/18 16:58",
        "Updated": "22/Nov/19 15:29",
        "Resolved": "12/Nov/19 15:08",
        "Description": "If HBase is not running `nutch inject` and `./nutch readdb -stats` both hangs - don't return anything.\u00a0\u00a0In such case an ConnectionTimeoutError or other error should be thrown.",
        "Issue Links": []
    },
    "NUTCH-2533": {
        "Key": "NUTCH-2533",
        "Summary": "Injector: NullPointerException if seed URL dir contains non-file entries",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "2.3.1,                                            1.14",
        "Fix Version/s": "2.4,                                            1.15",
        "Component/s": "injector",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Krzysztof Madejski",
        "Created": "12/Mar/18 17:00",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "12/Apr/18 08:28",
        "Description": "I'm following\u00a0https://wiki.apache.org/nutch/Nutch2Tutorial\n\u00a0\nI've run `./nutch inject /` and I've got the following error:\n\nInjectorJob: starting at 2018-03-12 11:59:05\r\nInjectorJob: Injecting urlDir: /\r\nInjectorJob: Using class org.apache.gora.hbase.store.HBaseStore as the Gora storage class.\r\nInjectorJob: java.lang.NullPointerException\r\nat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getBlockIndex(FileInputFormat.java:442)\r\nat org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:411)\r\nat org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:493)\r\nat org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:510)\r\nat org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:394)\r\nat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\r\nat org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\r\nat java.security.AccessController.doPrivileged(Native Method)\r\nat javax.security.auth.Subject.doAs(Subject.java:422)\r\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\r\nat org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\r\nat org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)\r\nat org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:115)\r\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:231)\r\nat org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:252)\r\nat org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:275)\r\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/312",
            "https://github.com/apache/nutch/pull/313"
        ]
    },
    "NUTCH-2534": {
        "Key": "NUTCH-2534",
        "Summary": "CrawlDbReader -stats: make score quantiles configurable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "15/Mar/18 12:33",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "27/Mar/18 15:00",
        "Description": "Since NUTCH-2470 the CrawlDbReader statistics shows the distribution of score values using a fixed set of quantiles. Would be nice to make the quantiles shown configurable to adapt to the size of the CrawlDb and the range of scores.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/297"
        ]
    },
    "NUTCH-2535": {
        "Key": "NUTCH-2535",
        "Summary": "CrawlDbReader -stats: ClassCastException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Mar/18 12:54",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "16/Mar/18 08:58",
        "Description": "java.lang.Exception: java.lang.ClassCastException: org.apache.nutch.crawl.NutchWritable cannot be cast to org.apache.hadoop.io.LongWritable\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:522)\r\nCaused by: java.lang.ClassCastException: org.apache.nutch.crawl.NutchWritable cannot be cast to org.apache.hadoop.io.LongWritable\r\n        at org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer.reduce(CrawlDbReader.java:254)\r\n        at org.apache.nutch.crawl.CrawlDbReader$CrawlDbStatReducer.reduce(CrawlDbReader.java:237)\r\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/296"
        ]
    },
    "NUTCH-2536": {
        "Key": "NUTCH-2536",
        "Summary": "GeneratorReducer.count is a static variable",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Ben Vachon",
        "Created": "16/Mar/18 14:36",
        "Updated": "27/Mar/18 19:45",
        "Resolved": "27/Mar/18 19:04",
        "Description": "The count field of the GeneratorReducer class is a static field. This means that if the\u00a0GeneratorJob\u00a0is\u00a0run multiple times within the same JVM, it will\u00a0count all the\u00a0webpages generated across all batches.\nThe count field is\u00a0checked against the GeneratorJob's topN configuration variable, which is described as:\n\"top threshold for maximum number of URLs permitted in a batch\"\nI understand this to mean that\u00a0EACH batch should be capped at the topN value, not ALL batches.\nThis isn't a problem with the way that Nutch is typically used because the script starts a new JVM each time. I'm not using the script, I'm calling the java classes directly (using the ToolRunner) within an existing JVM, so I'm categorizing this as an SDK issue.\nChanging the field to be non-static will not affect the behavior of the class as its run by the script.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/298"
        ]
    },
    "NUTCH-2537": {
        "Key": "NUTCH-2537",
        "Summary": "Logical OR instead of AND in UrlExemptionFilters",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "16/Mar/18 18:01",
        "Updated": "16/Mar/18 18:05",
        "Resolved": null,
        "Description": "With NUTCH-2522 another urlexemptionfilter is added, therefore\u00a0now we can combine the filters.\nWe should use more reasonable\u00a0combination of ExemptionFilters based on OR, instead of AND.\nThe following code should be modified\u00a0\nURLExemptionFilters.java : 66\n for (int i = 0; i < this.filters.length && exempted; i++) \n{\r\n exempted = this.filters[i].filter(fromUrl, toUrl);\r\n }",
        "Issue Links": [
            "/jira/browse/NUTCH-2522"
        ]
    },
    "NUTCH-2538": {
        "Key": "NUTCH-2538",
        "Summary": "Refactoring of Regex Url Normalizer and Bidirectional Url ExemptionFilter",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "16/Mar/18 18:04",
        "Updated": "16/Mar/18 18:05",
        "Resolved": null,
        "Description": "NUTCH-2522 uses the same regex logic as\u00a0RegxUrlNormalizer.\u00a0\nThese plugins can be refactored to the same base class.",
        "Issue Links": [
            "/jira/browse/NUTCH-2522"
        ]
    },
    "NUTCH-2539": {
        "Key": "NUTCH-2539",
        "Summary": "Not correct naming of db.url.filters and db.url.normalizers in nutch-default.xml",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Semyon Semyonov",
        "Created": "19/Mar/18 13:18",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "10/Apr/18 22:54",
        "Description": "There is a mismatch between config and code.\nIn code,\u00a0\n In CrawlDbFilter line 41:43\n> public static final String URL_FILTERING = \"crawldb.url.filters\";\n> public static final String URL_NORMALIZING = \"crawldb.url.normalizers\";\n> public static final String URL_NORMALIZING_SCOPE = \"crawldb.url.normalizers.scope\";\n\u00a0\nIn nutch-default.xml\n> <property>\n> <name>db.url.normalizers</name>\n> <value>false</value>\n> <description>Normalize urls when updating crawldb</description>\n> </property>\n>\n> <property>\n> <name>db.url.filters</name>\n> <value>false</value>\n> <description>Filter urls when updating crawldb</description>\n> </property>\nThese properties should be\u00a0in line with code.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/300"
        ]
    },
    "NUTCH-2540": {
        "Key": "NUTCH-2540",
        "Summary": "Support Generic Deduplication in Nutch 2.x",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Ben Vachon",
        "Created": "20/Mar/18 19:31",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "Currently, deduplication in 2.x exists only as a utility for the Solr index.\nMy use-case for Nutch required\u00a0deduplication so I wrote\u00a0custom code that checks for duplicates based on digest and deletes them at index time. I figured I'd port the change so that others could use it as well.\nThis is a very simple approach to Deduplication. There's plenty of room to improve it.\nThis change adds a new DataStore for Duplicate entries that are just lists of urls with signatures as keys.\nA DeduplicatorJob can be run between the DbUpdatorJob and IndexingJob to map WebPages into the Duplicate DataStore.\nSince the key of\u00a0the Duplicate store is the digest field of the WebPage store entries, duplicate matching can be configured via extension of the\u00a0Signature abstract class.\nA new \"-deduplicate\" argument\u00a0is added to the IndexingJob (false by default). If this flag is used, then the IndexingJob will check the Duplicate DataStore for duplicate URLs, run pluggable DuplicateFilters to determine which URL belongs to the original WebPage, and\u00a0skip the WebPage if it is not the original, and delete (from the index) the other pages if the WebPage is the original.\nI've also added a BasicDuplicateFilter plugin class that considers the URL with the shortest path to be the original.\nEventually, it would be best to consider things like score and fetch time when determining which WebPage to keep and which to remove.",
        "Issue Links": []
    },
    "NUTCH-2541": {
        "Key": "NUTCH-2541",
        "Summary": "Non-ASCII characters in the URL path are not properly escaped by the protocol-httpclient plugin",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "2.3.1,                                            1.14",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "21/Mar/18 13:06",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "As reported on [1] \nWhen trying to crawl some URLs with Arabic characters Nutch will complain due to an InvalidArgumentException. This happens because the HTTP client library is using internally the java.net.URI which does not support this characters unless they're properly escaped.\n[1] https://stackoverflow.com/questions/49379007/apache-nutch-2-3-1-fetcher-giving-invalid-uri-exception/49395225?noredirect=1#comment85798974_49395225",
        "Issue Links": []
    },
    "NUTCH-2542": {
        "Key": "NUTCH-2542",
        "Summary": "Sync URL filter and normalizer plugins with 1.x",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "22/Mar/18 08:37",
        "Updated": "13/Oct/19 22:35",
        "Resolved": "13/Oct/19 22:35",
        "Description": "The behavior of some URL filter and normalizer plugins has been diverged between the 1.x and 2.x branches, cf. NUTCH-2541. Since the interfaces are identical there should be no reason why bug fixes and improvements are not applied to both branches. This issue shall update all 2.x URL filter and normalizer plugins so that they behave same as the 1.x counter parts.",
        "Issue Links": []
    },
    "NUTCH-2543": {
        "Key": "NUTCH-2543",
        "Summary": "readdb & readlinkdb to implement AbstractChecker",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "crawldb,                                            linkdb",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "22/Mar/18 13:20",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "27/Mar/18 14:41",
        "Description": "Implement AbstractChecker in LinkDbReader & CrawlDbReader classes, so we can expose them via TCP.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/303"
        ]
    },
    "NUTCH-2544": {
        "Key": "NUTCH-2544",
        "Summary": "Nutch 1.15 no longer compatible with AWS EMR and S3",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "fetcher,                                            generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Steven W",
        "Created": "23/Mar/18 19:39",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "26/Apr/18 10:57",
        "Description": "Nutch 1.14 is working OK with AWS EMR and S3 storage, but NUTCH-2375 appears to have broken this.\nGenerator partitioning fails with\u00a0Error: java.lang.NullPointerException at org.apache.nutch.crawl.URLPartitioner.getPartition(URLPartitioner.java:75)",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/320"
        ]
    },
    "NUTCH-2545": {
        "Key": "NUTCH-2545",
        "Summary": "Upgrade to Any23 2.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "any23,                                            plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "27/Mar/18 15:46",
        "Updated": "02/Apr/18 16:53",
        "Resolved": "02/Apr/18 16:11",
        "Description": "We recently released Any23 2.2. I would like to update the Any23 plugin to this newest version.\nPR coming up.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/306"
        ]
    },
    "NUTCH-2546": {
        "Key": "NUTCH-2546",
        "Summary": "parse-(metatags|html) plugin - \"meta property\" not extracted only \"meta name\"",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Irinel",
        "Created": "27/Mar/18 17:22",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The parse-(metatags|html) plugin \"extracts\" meta tags like \"<meta property=\", but tags like \"<meta\u00a0name=\" are not processed.\nHTML e.g.:\n<meta property=\"og:title\" content=\"Content in this property...\"/> -\u00a0not extracted\n <meta name=\"description\" content=\"Content in this meta...\"/> - OK\n\u00a0\nWhen using parse-tika plugin for parsing, meta property fields are processed.\n<name>plugin.includes</name>\n<value>parse-(html|tika|metatags)...</value>",
        "Issue Links": []
    },
    "NUTCH-2547": {
        "Key": "NUTCH-2547",
        "Summary": "urlnormalizer-basic fails on special characters in path/query",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "29/Mar/18 10:07",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "28/Jun/18 11:08",
        "Description": "If a URL contains one of the characters |\"<>^` or a single % (not followed by a 2-characther hex-value), BasicURLNormalizer fails to normalize the URL path (here: remove /c/..):\n\n% for c in \"\" $(echo '|%\"^<>`' | grep -o .); do\r\n    echo \"http://www.example.com/a/c/../b/search?q=foobar$c\"\r\n  done \\\r\n  | nutch normalizerchecker -normalizer urlnormalizer-basic -stdin\r\nChecking combination of these URLNormalizers: BasicURLNormalizer \r\nhttp://www.example.com/a/b/search?q=foobar\r\nhttp://www.example.com/a/c/../b/search?q=foobar|\r\nhttp://www.example.com/a/c/../b/search?q=foobar%\r\nhttp://www.example.com/a/c/../b/search?q=foobar\"\r\nhttp://www.example.com/a/c/../b/search?q=foobar^\r\nhttp://www.example.com/a/c/../b/search?q=foobar<\r\nhttp://www.example.com/a/c/../b/search?q=foobar>\r\nhttp://www.example.com/a/c/../b/search?q=foobar`\r\n\n\nThe reason is that these characters (should check for more, including control characters) are not valid as part of a URI (cf. RFC3986). BasicURLNormalizer normalizes the path by converting the URL to a URI and calling normalize().\nThere are two possible solutions:\n\ndo not use java.net.URI\nensure that every URL returned (or used internally) by urlnormalizer-basic is a valid URI (resp. its String representation).\n\nI would opt for #2 because the class URI is used practically everywhere in Nutch and libraries (e.g. HttpClient). Any thoughts?",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/353"
        ]
    },
    "NUTCH-2548": {
        "Key": "NUTCH-2548",
        "Summary": "Compressed content skipped. Content of size 78 was truncated to 74",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rustam",
        "Created": "03/Apr/18 15:52",
        "Updated": "09/Apr/18 10:43",
        "Resolved": "09/Apr/18 10:00",
        "Description": "gzip or deflate compressed content fails to parse with a message like:\nWARN\u00a0 parse.ParserJob - https://rustyx.org/temp/index%20bbb skipped. Content of size 78 was truncated to 74\nThe root cause is that the original (compressed) Content-Length is stored in the headers, while the\u00a0content is stored uncompressed. Subsequently the Content-Length doesn't match the stored content size.\nSee attached patch that fixed the issue by removing Content-Length from the headers if it contains compressed value.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/308"
        ]
    },
    "NUTCH-2549": {
        "Key": "NUTCH-2549",
        "Summary": "protocol-http does not behave the same as browsers",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Gerard Bouchar",
        "Created": "06/Apr/18 14:39",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "12/Jun/18 19:15",
        "Description": "We identified the following issues in protocol-http (a plugin implementing the HTTP protocol):\n\nIt fails if an url's path does not start with '/'\n\t\nExample: http://news.fx678.com?171 (browsers correctly rewrite the url as http://news.fx678.com/?171, while nutch tries to send an invalid HTTP request starting with GET ?171 HTTP/1.0.\n\n\nIt advertises its requests as being HTTP/1.0, but sends an\u00a0Accept-Encoding request header, that is defined only in HTTP/1.1. This confuses some web servers\n\t\nExample: http://www.hansamanuals.com/main/english/none/theconf___987/manuals/version___82/hwconvindex.htm\n\n\nIf a server sends a redirection (3XX status code, with a Location header), protocol-http tries to parse the HTTP response body anyway. Thus, if an error occurs while decoding the body, the redirection is not followed and the information is lost. Browsers follow the redirection and close the socket soon as they can.\n\t\nExample: http://www.webarcelona.net/es/blog?page=2\n\n\nSome servers invalidly send an HTTP body directly without a status line or headers. Browsers handle that, protocol-http doesn't:\n\t\nExample: https://app.unitymedia.de/\n\n\nSome servers invalidly add colons after the HTTP status code in the status line (they can send HTTP/1.1 404: Not found instead of HTTP/1.1 404 Not found for instance). Browsers can handle that.\nSome servers invalidly send headers that span over multiple lines. In that case, browsers simply ignore the subsequent lines, but protocol-http throws an error, thus preventing us from fetching the contents of the page.\nThere is no limit over the size of the HTTP headers it reads. A bogus server could send an infinite stream of different HTTP headers and cause the fetcher to go out of memory, or send the same HTTP header repeatedly and cause the fetcher to timeout.\nThe same goes for the HTTP status line: no check is made concerning its size.\nWhile reading chunked content, if the content size becomes larger than http.getMaxContent(), instead of just stopping, it tries to read a new chunk before having read the previous one completely, resulting in a 'bad chunk length' error.\n\nAdditionally (and that concerns protocol-httpclient as well), when reading http headers, for each header, the SpellCheckedMetadata class computes a Levenshtein distance between it and every\u00a0 known header in the HttpHeaders interface. Not only is that slow, non-standard, and non-conform to browsers' behavior, but it also causes bugs and prevents us from accessing the real headers sent by the HTTP server.\n\nExample: http://www.taz.de/!443358/ . The server sends a Client-Transfer-Encoding: chunked header, but SpellCheckedMetadata corrects it to Transfer-Encoding: chunked. Then, HttpResponse (in protocol-http) tries to read the HTTP body as chunked, whereas it is not.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/347"
        ]
    },
    "NUTCH-2550": {
        "Key": "NUTCH-2550",
        "Summary": "Fetcher fails to follow redirects",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Hans Brende",
        "Created": "07/Apr/18 23:16",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "10/Apr/18 22:52",
        "Description": "As I detailed in this github comment, it appears that PR #221 broke redirects. The fetcher will repeatedly fetch the original url rather than the one it's supposed to be redirecting to until http.redirect.max is exceeded, and then end with STATUS_FETCH_GONE.\nI noticed this issue when I was trying to crawl a site with a 301 MOVED PERMANENTLY status code.\nShould be pretty easy to fix though: I was able to get redirects working again simply by inserting the code \n\nurl = fit.url\n\n here and here.",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/309"
        ]
    },
    "NUTCH-2551": {
        "Key": "NUTCH-2551",
        "Summary": "NullPointerException in generator",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Hans Brende",
        "Created": "08/Apr/18 05:38",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "12/Apr/18 13:03",
        "Description": "A NullPointerException is thrown during the crawl generate stage when I deploy to a hadoop cluster (but for some reason, it works fine locally).\nIt looks like this is caused because the URLPartitioner class still has the old configure() method in there (which is never called, causing the normalizers field to remain null), rather than implementing the Configurable interface as detailed in the newer mapreduce API's Partitioner spec.\nStack trace:\n\n\r\njava.lang.NullPointerException\r\n at org.apache.nutch.crawl.URLPartitioner.getPartition(URLPartitioner.java:76)\r\n at org.apache.nutch.crawl.URLPartitioner.getPartition(URLPartitioner.java:40)\r\n at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:716)\r\n at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)\r\n at org.apache.hadoop.mapreduce.lib.map.WrappedMapper$Context.write(WrappedMapper.java:112)\r\n at org.apache.nutch.crawl.Generator$SelectorInverseMapper.map(Generator.java:553)\r\n at org.apache.nutch.crawl.Generator$SelectorInverseMapper.map(Generator.java:546)\r\n at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\r\n at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)\r\n at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\r\n at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175)\r\n at java.security.AccessController.doPrivileged(Native Method)\r\n at javax.security.auth.Subject.doAs(Subject.java:422)\r\n at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)\r\n at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:169)\r\n\n\n\u00a0\nOh and it might also be because a static URLPartitioner instance is being used in the Generator.Selector class... but it's only initialized in the setup() method of the Generator.Selector.SelectorMapper class! So that whole setup looks pretty weird...",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/316"
        ]
    },
    "NUTCH-2552": {
        "Key": "NUTCH-2552",
        "Summary": "CrawlDbReader -topN fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Apr/18 08:20",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "21/Apr/18 16:24",
        "Description": "% bin/nutch readdb crawldb -topN 50 crawldb_topn\r\nCrawlDb topN: starting (topN=50, min=0.0)\r\nCrawlDb db: crawl/crawldb\r\nCrawlDb topN: collecting topN scores.\r\nCrawlDbReader job did not succeed, job status:FAILED, reason: NA\r\nException in thread \"main\" java.lang.RuntimeException: CrawlDbReader job did not succeed, job status:FAILED, reason: NA\r\n        at org.apache.nutch.crawl.CrawlDbReader.processTopNJob(CrawlDbReader.java:853)\r\n\n\nThe hadoop.log shows the reason\n\n2018-04-09 10:04:16,435 WARN  mapred.LocalJobRunner - job_local1653923841_0002\r\njava.lang.Exception: java.lang.NumberFormatException: null\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\r\nCaused by: java.lang.NumberFormatException: null\r\n        at java.lang.Integer.parseInt(Integer.java:542)\r\n        at java.lang.Integer.parseInt(Integer.java:615)\r\n        at org.apache.nutch.crawl.CrawlDbReader$CrawlDbTopNReducer.setup(CrawlDbReader.java:370)\r\n        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:168)\r\n        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\r\n\n\nCaused by NUTCH-2375: the property mapred.job.reduces must be updated by mapreduce.job.reduces.\nNote: Should check all occurrences of this property and similars ones (mapred.job.*).",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/315"
        ]
    },
    "NUTCH-2553": {
        "Key": "NUTCH-2553",
        "Summary": "Fetcher not to modify URLs to be fetched",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Apr/18 08:26",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "21/Apr/18 16:36",
        "Description": "Fetcher modifies the URLs being fetched (introduced with NUTCH-2375 in c93d908:\n\nFetcherThread 22 fetching http://nutch.apache.org:-1/ (queue crawl delay=5000ms)\r\n\n\nwhich makes it hard to trace the URLs in the log files and likely causes other issues because URLs in CrawlDb and segments do not match (http://nutch.apache.org/ in CrawlDb and http://nutch.apache.org:-1/ in segment).",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/317"
        ]
    },
    "NUTCH-2554": {
        "Key": "NUTCH-2554",
        "Summary": "parserchecker can't fetch some URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 08:48",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "09/Apr/18 13:45",
        "Description": "The parserchecker (org.apache.nutch.parse.ParserChecker) calls URLUtil.toASCII on the url it is given, reencoding already percent-encoded URLs.\nFor instance, let's say we want to query http://example.com, passing a GET parameter with name 'q' and value '/'. '/' is a special character, and thus has to be encoded before being sent.\n If we pass 'http://example.com/?q=/' to the parserchecker, then it doesn't encode the '/', and tries to fetch the URL as is, which is invalid.\n If we try to encode the parameter beforehand, and call the parsechecker with 'http://example.com/?q=%2F', then it encodes the '%' sign to '%25', and thus fetches 'http://example.com/?q=%252F'.\nThis actually makes it impossible to fetch the correct URL (http://example.com/?q=%2F) from the parserchecker.",
        "Issue Links": [
            "/jira/browse/NUTCH-2012"
        ]
    },
    "NUTCH-2555": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "URL normalization problem: path not starting with a '/'",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 14:32",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "12/Jun/18 19:05",
        "Description": "When an URL does not have a path but has GET parameters (for instance 'http://example.com?a=1') it should be normalized to add a '/' at the beginning of the path (giving http://example.com/?a=1). Our logs show that non-normalized URLs reach protocol-http, which then uses URL::getFile() to get the path, and tries to send an invalid HTTP request:\nGET ?a=1 HTTP/1.0\ninstead of\nGET /?a=1 HTTP/1.0\n\u00a0\nExample URL for which this poses a problem: http://news.fx678.com?171",
        "Issue Links": []
    },
    "NUTCH-2556": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http makes invalid HTTP/1.0 requests",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 14:34",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "12/Jun/18 19:06",
        "Description": "protocol-http advertises its requests as being HTTP/1.0, but sends an\u00a0Accept-Encoding request header, that is defined only in HTTP/1.1. This confuses some web servers\n\nExample: http://www.hansamanuals.com/main/english/none/theconf___987/manuals/version___82/hwconvindex.htm",
        "Issue Links": []
    },
    "NUTCH-2557": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http fails to follow redirections when an HTTP response body is invalid",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 14:40",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "12/Jun/18 19:07",
        "Description": "If a server sends a redirection (3XX status code, with a Location header), protocol-http tries to parse the HTTP response body anyway. Thus, if an error occurs while decoding the body, the redirection is not followed and the information is lost. Browsers follow the redirection and close the socket soon as they can.\n\nExample: this page is a redirection to its https version, with an HTTP body containing invalidly gzip encoded contents. Browsers follow the redirection, but nutch throws an error:\n\n\n\n\nhttp://www.webarcelona.net/es/blog?page=2\n\n\n\n\u00a0\nThe HttpResponse::getContent class can already return null. I think it should at least return null when parsing the HTTP response body fails.\nIdeally, we would adopt the same behavior as browsers, and not even try parsing the body when the headers indicate a redirection.",
        "Issue Links": []
    },
    "NUTCH-2558": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http cannot handle a missing HTTP status line",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 14:41",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "12/Jun/18 19:07",
        "Description": "Some servers invalidly send an HTTP body directly without a status line or headers. Browsers handle that, protocol-http doesn't:\n\nExample: https://app.unitymedia.de/",
        "Issue Links": []
    },
    "NUTCH-2559": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http cannot handle colons after the HTTP status code",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 14:43",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "12/Jun/18 19:08",
        "Description": "Some servers invalidly add colons after the HTTP status code in the status line (they can send HTTP/1.1 404: Not found instead of HTTP/1.1 404 Not found for instance). Browsers can handle that.",
        "Issue Links": []
    },
    "NUTCH-2560": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http throws an error when an http header spans over multiple lines",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 14:44",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "12/Jun/18 19:12",
        "Description": "Some servers invalidly send headers that span over multiple lines. In that case, browsers simply ignore the subsequent lines, but protocol-http throws an error, thus preventing us from fetching the contents of the page.",
        "Issue Links": []
    },
    "NUTCH-2561": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http can be made to read arbitrarily large HTTP responses",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 14:47",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "12/Jun/18 19:13",
        "Description": "protocol-http limits the size of the HTTP response body. However\n\nThere is no limit over the size of the HTTP headers it reads. A bogus server could send an infinite stream of different HTTP headers and cause the fetcher to go out of memory, or send the same HTTP header repeatedly and cause the fetcher to timeout.\nThe same goes for the HTTP status line: no check is made concerning its size.\n\nThis can be both a performance and a security problem.\nJoined is an example python implementation of a server that makes protocol-http receive huge amounts of data and use a lot of CPU (because of NUTCH-2563), without being stopped by http.getTimeout() nor http.getMaxContent().",
        "Issue Links": []
    },
    "NUTCH-2562": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http fails to read large chunked HTTP responses",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 14:58",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "02/Jun/18 12:08",
        "Description": "While reading chunked content, if the content size becomes larger than http.getMaxContent(), instead of just stopping and truncate the content, it tries to read a new chunk before having read the previous one completely, resulting in a 'bad chunk length' error.\n\u00a0\nSee: https://github.com/apache/nutch/blob/master/src/plugin/protocol-http/src/java/org/apache/nutch/protocol/http/HttpResponse.java#L440-L442",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/329"
        ]
    },
    "NUTCH-2563": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "HTTP header spellchecking issues",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "09/Apr/18 15:05",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "12/Jun/18 19:14",
        "Description": "When reading http headers, for each header, the SpellCheckedMetadata class computes a Levenshtein distance between it and every\u00a0 known header in the HttpHeaders interface. Not only is that slow, non-standard, and non-conform to browsers' behavior, but it also causes bugs and prevents us from accessing the real headers sent by the HTTP server.\n\nExample: http://www.taz.de/!443358/ . The server sends a Client-Transfer-Encoding: chunked header, but SpellCheckedMetadata corrects it to Transfer-Encoding: chunked. Then, HttpResponse (in protocol-http) tries to read the HTTP body as chunked, whereas it is not.\n\nI personally think that HTTP header spell checking is a bad idea, and that this logic should be completely removed. But if it were to be kept, the threshold (SpellCheckedMetadata.TRESHOLD_DIVIDER) should be higher (we internally set it to 5 as a temporary fix for this issue)",
        "Issue Links": []
    },
    "NUTCH-2564": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http throws an error when the content-length header is not a number",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "10/Apr/18 08:44",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "12/Jun/18 19:14",
        "Description": "When a server sends an invalid Content-Length header (one that is not a valid number) with a plain-text http body, browsers simply ignore it, but protocol-http has a strange approach: if the header is composed only of white spaces, it ignores it, but if it contains other characters, it throws an error, preventing us from doing anything with the page.\nIt should simply ignore invalid Content-Length headers.\n\u00a0\nRelevant code: https://github.com/apache/nutch/blob/master/src/plugin/protocol-http/src/java/org/apache/nutch/protocol/http/HttpResponse.java#L354-L359",
        "Issue Links": []
    },
    "NUTCH-2565": {
        "Key": "NUTCH-2565",
        "Summary": "MergeDB incorrectly handles unfetched CrawlDatums",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "10/Apr/18 10:59",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "21/Jun/18 14:43",
        "Description": "I ran into this issue when merging a crawlDB originating from sitemaps into our normal crawlDB. CrawlDatums are merged based on output of AbstractFetchSchedule::calculateLastFetchTime(). When CrawlDatums are unfetched, this can overwrite fetchTime or other stuff.\nI assume this is a bug and have a simple fix for it that checks if CrawlDatum has status db_unfetched.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/311"
        ]
    },
    "NUTCH-2566": {
        "Key": "NUTCH-2566",
        "Summary": "Fix exception log messages",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Apr/18 10:11",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "11/Apr/18 12:38",
        "Description": "Some log messages use erroneously \"{}\" in combination with a Throwable object. That's not placed in the log message by org.slf4j.Logger but added after the message:\n\nCrawlDbMerge job failed {}\r\norg.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:...crawldb/current",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/314"
        ]
    },
    "NUTCH-2567": {
        "Key": "NUTCH-2567",
        "Summary": "parse-metatags writes all meta tags twice",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.17",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Gerard Bouchar",
        "Created": "11/Apr/18 15:57",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "09/Jun/20 09:39",
        "Description": "Using nutch witch the following configuration, MetaTagsParser writes HTML meta tags to the metadata twice:\n\n\r\n    <property>\r\n        <name>plugin.includes</name>\r\n        <value>protocol-http|parse-(tika|metatags)</value>\r\n    </property>\r\n\n\nThe problem seems to come from MetaTagsParser.java#L104-L111 :\nBoth the meta tags from the existing ParseResult and from the HTMLMetaTags are added to the metadata with a \"metatag.\" prefix. But the ParseResult object already contains the HTML meta tags, because they have been added by TikaParser here: TikaParser.java#L198-L206\n\u00a0\n This bug is concerning, because it makes the segments uselessly big, especially if we want to store all metatags (by default, only metatag.description and metatag.keywords are stored, and thus duplicated).\nI would also suggest making the output of Metadata::toString more readable(for instance by adding a newline before each new metadata value). It would have made this bug way easier to spot inside the output of the parsechecker.",
        "Issue Links": [
            "/jira/browse/NUTCH-1559",
            "/jira/browse/NUTCH-1559"
        ]
    },
    "NUTCH-2568": {
        "Key": "NUTCH-2568",
        "Summary": "Caught exception is immediately rethrown",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "util",
        "Assignee": null,
        "Reporter": "Hans Brende",
        "Created": "13/Apr/18 00:31",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "21/Apr/18 16:44",
        "Description": "NutchJob.cleanupAfterFailure() catches an IOException and immediately rethrows it without logging it. Either remove the try-catch block, or do something with the exception, e.g., log it.\nRelevant line of code:\nhttps://github.com/apache/nutch/blob/4c3fb71208a29e1e49201002bda08499a14b443d/src/java/org/apache/nutch/util/NutchJob.java#L50",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/318"
        ]
    },
    "NUTCH-2569": {
        "Key": "NUTCH-2569",
        "Summary": "ClassNotFoundException when running in (pseudo-)distributed mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Apr/18 19:23",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "26/Apr/18 11:12",
        "Description": "The CrawlDb / updatedb job fails in pseudo-distributed mode with a ClassNotFoundException:\n\n18/04/22 19:24:49 INFO mapreduce.Job: Task Id : attempt_1524395182329_0018_m_000000_0, Status : FAILED\r\nError: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.nutch.crawl.CrawlDbFilter not found\r\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2369)\r\n        at org.apache.hadoop.mapreduce.task.JobContextImpl.getMapperClass(JobContextImpl.java:186)\r\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:745)\r\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)\r\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1836)\r\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:169)\r\nCaused by: java.lang.ClassNotFoundException: Class org.apache.nutch.crawl.CrawlDbFilter not found\r\n        at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2273)\r\n        at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2367)\r\n\n\nMust define the job jar by calling job.setJarByClass(...). This affects also other jobs.",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/322"
        ]
    },
    "NUTCH-2570": {
        "Key": "NUTCH-2570",
        "Summary": "Deduplication job fails to install deduplicated CrawlDb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Apr/18 19:57",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "26/Apr/18 11:14",
        "Description": "The DeduplicationJob (\"nutch dedup\") fails to install the deduplicated CrawlDb and leaves only the \"old\" crawldb (if \"db.preserve.backup\" is true):\n\n% tree crawldb\r\ncrawldb\r\n\u251c\u2500\u2500 current\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 part-r-00000\r\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data\r\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 index\r\n\u2514\u2500\u2500 old\r\n\u2514\u2500\u2500 part-r-00000\r\n\u251c\u2500\u2500 data\r\n\u2514\u2500\u2500 index\r\n% bin/nutch dedup crawldb\r\nDeduplicationJob: starting at 2018-04-22 21:48:08\r\nDeduplication: 6 documents marked as duplicates\r\nDeduplication: Updating status of duplicate urls into crawl db.\r\nException in thread \"main\" java.io.FileNotFoundException: File file:/tmp/crawldb/1742327020 does not exist\r\nat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\r\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\r\nat org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\r\nat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)\r\nat org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)\r\nat org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:374)\r\nat org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:613)\r\nat org.apache.nutch.util.FSUtils.replace(FSUtils.java:58)\r\nat org.apache.nutch.crawl.CrawlDb.install(CrawlDb.java:212)\r\nat org.apache.nutch.crawl.CrawlDb.install(CrawlDb.java:225)\r\nat org.apache.nutch.crawl.DeduplicationJob.run(DeduplicationJob.java:366)\r\nat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\nat org.apache.nutch.crawl.DeduplicationJob.main(DeduplicationJob.java:379)\r\n\r\n% tree crawldb\r\ncrawldb\r\n\u2514\u2500\u2500 old\r\n\u2514\u2500\u2500 part-r-00000\r\n\u251c\u2500\u2500 data\r\n\u2514\u2500\u2500 index\r\n\n\nIn pseudo-distributed mode it's even worse: only the \"old\" CrawlDb is left without any error.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/323"
        ]
    },
    "NUTCH-2571": {
        "Key": "NUTCH-2571",
        "Summary": "SegmentReader -list fails to read segment",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "segment",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "23/Apr/18 11:25",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "26/Apr/18 11:16",
        "Description": "The -list command of SegmentReader fails to read data from segments:\n\n% bin/nutch readseg -list crawl/segments/20180409100315/ \r\nException in thread \"main\" java.io.IOException: wrong value class:  is not class org.apache.nutch.crawl.CrawlDatum\r\n        at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:2379)\r\n        at org.apache.nutch.segment.SegmentReader.getStats(SegmentReader.java:524)\r\n        at org.apache.nutch.segment.SegmentReader.list(SegmentReader.java:482)\r\n        at org.apache.nutch.segment.SegmentReader.run(SegmentReader.java:670)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.nutch.segment.SegmentReader.main(SegmentReader.java:736)",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/325"
        ]
    },
    "NUTCH-2572": {
        "Key": "NUTCH-2572",
        "Summary": "HostDb: updatehostdb does not set values",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "hostdb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "23/Apr/18 11:56",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "26/Apr/18 11:18",
        "Description": "% bin/nutch readdb crawl/crawldb -stats -sort\r\n...\r\nstatus 1 (db_unfetched):        3\r\n   nutch.apache.org :   3\r\nstatus 2 (db_fetched):  2\r\n   nutch.apache.org :   2\r\nstatus 6 (db_notmodified):      34\r\n   nutch.apache.org :   34\r\nCrawlDb statistics: done\r\n\r\n% bin/nutch updatehostdb -hostdb  crawl/hostdb -crawldb crawl/crawldb\r\nUpdateHostDb: hostdb: crawl/hostdb\r\nUpdateHostDb: crawldb: crawl/crawldb\r\nUpdateHostDb: starting at 2018-04-23 13:50:33\r\nUpdateHostDb: finished at 2018-04-23 13:50:35, elapsed: 00:00:01\r\n\r\n% bin/nutch readhostdb crawl/hostdb -get nutch.apache.org\r\nReadHostDb: get: nutch.apache.org\r\n0       0       0       0       0       0       0       0       0       0       0.0     1970-01-01 01:00:00\r\n\n\nAlthough a HostDb record is added for \"nutch.apache.org\", all expected values (number of fetched/unfetched/... pages, fetch time min/max/average/percentiles, etc.) are empty or zero.",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/326"
        ]
    },
    "NUTCH-2573": {
        "Key": "NUTCH-2573",
        "Summary": "Suspend crawling if robots.txt fails to fetch with 5xx status",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.19",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "26/Apr/18 14:14",
        "Updated": "18/Jan/22 08:43",
        "Resolved": "18/Jan/22 07:24",
        "Description": "Fetcher should optionally (by default) suspend crawling by a configurable interval when fetching the robots.txt fails with a server errors (HTTP status code 5xx, esp. 503) following Google's spec:\n5xx (server error)\nServer errors are seen as temporary errors that result in a \"full disallow\" of crawling. The request is retried until a non-server-error HTTP result code is obtained. A 503 (Service Unavailable) error will result in fairly frequent retrying. To temporarily suspend crawling, it is recommended to serve a 503 HTTP result code. Handling of a permanent server error is undefined.\nSee also the draft robots.txt RFC, section \"Unreachable status\".\nCrawler-commons robots rules already provide isDeferVisits to store this information (must be set from RobotRulesParser).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/724"
        ]
    },
    "NUTCH-2574": {
        "Key": "NUTCH-2574",
        "Summary": "Generator: hostCount >= maxCount comparison wrong",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.13",
        "Fix Version/s": "1.15",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Michael Coffey",
        "Created": "28/Apr/18 00:04",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "13/Jun/18 08:39",
        "Description": "In the Generator.Selector.reduce function, there is a comparison of hostCount[1] to maxCount, to determine whether or not to push the current URL to the next segment. The purpose is to honor\u00a0generate.max.count.\nSebastian noticed that it should test\u00a0if (hostCount[1] > maxCount) rather than \">=\".\u00a0 As it stands, the code sometimes puts one less url into a segment than it should.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/344"
        ]
    },
    "NUTCH-2575": {
        "Key": "NUTCH-2549 protocol-http does not behave the same as browsers",
        "Summary": "protocol-http does not respect the maximum content-size for chunked responses",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "30/Apr/18 09:54",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "10/May/18 21:03",
        "Description": "There is a bug in HttpResponse::readChunkedContent that prevents it to stop reading content when it exceeds the maximum allowed size.\nThere is a variable contentBytesRead that is used to check how much content has been read, but it is never updated, so it always stays null, and the size check always returns false (unless a single chunk is larger than the maximum allowed content size).\nThis allows any server to cause out-of-memory errors on our size.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/327"
        ]
    },
    "NUTCH-2576": {
        "Key": "NUTCH-2576",
        "Summary": "HTTP protocol plugin based on okhttp",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.15",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/May/18 11:47",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "12/Jun/18 15:41",
        "Description": "Okhttp is an Apache2-licensed http library which supports HTTP/2. jnioche's implementation storm-crawler#443 proves that it should be straightforward to implement a Nutch protocol plugin using okhttp. A recent HTTP protocol implementation should also fix (most of) the issues reported in NUTCH-2549.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/328"
        ]
    },
    "NUTCH-2577": {
        "Key": "NUTCH-2577",
        "Summary": "protocol-selenium can't handle https",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "hussein Al_Ahmad",
        "Created": "15/May/18 13:27",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "23/May/18 16:21",
        "Description": "fetch of\u00a0any https page\u00a0is failing with: org.apache.nutch.protocol.ProtocolNotFound: protocol not found for url=https\nat org.apache.nutch.protocol.ProtocolFactory.getProtocol(ProtocolFactory.java:83)\nat org.apache.nutch.fetcher.Fetcher$FetcherThread.run(Fetcher.java:687)",
        "Issue Links": [
            "/jira/browse/NUTCH-2273",
            "/jira/browse/NUTCH-2310",
            "https://github.com/apache/nutch/pull/330"
        ]
    },
    "NUTCH-2578": {
        "Key": "NUTCH-2578",
        "Summary": "Avoid lock by MimeUtil in constructor of protocol.Content",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/May/18 12:16",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "13/Jun/18 08:38",
        "Description": "The constructor of the class o.a.n.protocol.Content instantiates a new MimeUtil object. That's not cheap as it always creates a new Tika object and there is a lock on the job/jar file when config files are read:\n\n\"FetcherThread\" #146 daemon prio=5 os_prio=0 tid=0x00007f70523c3800 nid=0x1de2 waiting for monitor entry [0x00007f70193a8000]\r\n   java.lang.Thread.State: BLOCKED (on object monitor)\r\n        at java.util.zip.ZipFile.getEntry(ZipFile.java:314)\r\n        - waiting to lock <0x00000005e0285758> (a java.util.jar.JarFile)\r\n        at java.util.jar.JarFile.getEntry(JarFile.java:240)\r\n        at java.util.jar.JarFile.getJarEntry(JarFile.java:223)\r\n        at sun.misc.URLClassPath$JarLoader.getResource(URLClassPath.java:1042)\r\n        at sun.misc.URLClassPath$JarLoader.findResource(URLClassPath.java:1020)\r\n        at sun.misc.URLClassPath$1.next(URLClassPath.java:267)\r\n        at sun.misc.URLClassPath$1.hasMoreElements(URLClassPath.java:277)\r\n        at java.net.URLClassLoader$3$1.run(URLClassLoader.java:601)\r\n        at java.net.URLClassLoader$3$1.run(URLClassLoader.java:599)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at java.net.URLClassLoader$3.next(URLClassLoader.java:598)\r\n        at java.net.URLClassLoader$3.hasMoreElements(URLClassLoader.java:623)\r\n        at sun.misc.CompoundEnumeration.next(CompoundEnumeration.java:45)\r\n        at sun.misc.CompoundEnumeration.hasMoreElements(CompoundEnumeration.java:54)\r\n        at java.util.Collections.list(Collections.java:5239)\r\n        at org.apache.tika.config.ServiceLoader.identifyStaticServiceProviders(ServiceLoader.java:325)\r\n        at org.apache.tika.config.ServiceLoader.loadStaticServiceProviders(ServiceLoader.java:352)\r\n        at org.apache.tika.config.ServiceLoader.loadServiceProviders(ServiceLoader.java:274)\r\n        at org.apache.tika.detect.DefaultEncodingDetector.<init>(DefaultEncodingDetector.java:45)\r\n        at org.apache.tika.config.TikaConfig.getDefaultEncodingDetector(TikaConfig.java:92)\r\n        at org.apache.tika.config.TikaConfig.<init>(TikaConfig.java:248)\r\n        at org.apache.tika.config.TikaConfig.getDefaultConfig(TikaConfig.java:386)\r\n        at org.apache.tika.Tika.<init>(Tika.java:116)\r\n        at org.apache.nutch.util.MimeUtil.<init>(MimeUtil.java:69)\r\n        at org.apache.nutch.protocol.Content.<init>(Content.java:83)\r\n        at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:316)\r\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:341)\r\n\n\nIf there are many Fetcher threads this may cause a significant bottleneck, running a Fetcher with 120 threads I've found up to 50 threads waiting for this lock:\n\n# pid 7195 is a Fetcher map task\r\n% sudo -u yarn jstack 7195 \\\r\n      | grep -A25 'waiting to lock' \\\r\n      | grep -F 'org.apache.tika.Tika.<init>' \\\r\n      | wc -l\r\n49\r\n\n\nAs MimeUtil is thread-safe including the called Tika detector, the best solution seems to cache the MimeUtil object in the actual protocol implementation as it is done in Nutch 2.x (lib-http HttpBase, line #151).",
        "Issue Links": [
            "/jira/browse/TIKA-2645",
            "https://github.com/apache/nutch/pull/338"
        ]
    },
    "NUTCH-2579": {
        "Key": "NUTCH-2579",
        "Summary": "Fetcher to use parsed URL to call ProtocolFactory.getProtocol(url)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "fetcher,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "18/May/18 16:16",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "13/Jun/18 08:20",
        "Description": "The call of ProtocolFactory.getProtocol(url) is synchronized and causes waits for the lock in a multi-threaded fetcher. It uses the URL string, although it would be more efficient to use the parsed URL hold in the FetchItem. The lock could be released faster. In addition, parsing the URL also causes a lock in the URL stream handler:\n\n\"FetcherThread\" #37 daemon prio=5 os_prio=0 tid=0x00007f21edea2000 nid=0x5c20 waiting for monitor entry [0x00007f21bacb4000]\r\n   java.lang.Thread.State: BLOCKED (on object monitor)\r\n        at java.util.Hashtable.get(Hashtable.java:363)\r\n        - waiting to lock <0x00000005e01b5840> (a java.util.Hashtable)\r\n        at java.net.URL.getURLStreamHandler(URL.java:1135)\r\n        at java.net.URL.<init>(URL.java:599)\r\n        at java.net.URL.<init>(URL.java:490)\r\n        at java.net.URL.<init>(URL.java:439)\r\n        at org.apache.nutch.protocol.ProtocolFactory.getProtocol(ProtocolFactory.java:74)\r\n        - locked <0x00000005fc5f4fb8> (a org.apache.nutch.protocol.ProtocolFactory)\r\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:299)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/334"
        ]
    },
    "NUTCH-2580": {
        "Key": "NUTCH-2580",
        "Summary": "Improvements for Rabbitmq support",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "21/May/18 14:20",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "02/Jun/18 11:26",
        "Description": "This one includes:\n\nCreation of lib-rabbitmq for common functionalities (publish-rabbitmq and indexer-rabbit).\nUpdate of the RabbitMQ's library version.\nHeaders selection from NutchDocument's fields (for indexer-rabbit).\nOptional binding.\nA single or multiple documents into each message.\nOptions for the creation of exchange, queue\u00a0and binding.\nSimplify the configuration options.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/335"
        ]
    },
    "NUTCH-2581": {
        "Key": "NUTCH-2581",
        "Summary": "Caching of redirected robots.txt may overwrite correct robots.txt rules",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.14",
        "Fix Version/s": "2.4,                                            1.15",
        "Component/s": "fetcher,                                            robots",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/May/18 13:03",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "08/Jun/18 09:52",
        "Description": "Redirected robots.txt rules are also cached for the target host. That may cause that the correct robots.txt rules are never fetched. E.g., http://wyomingtheband.com/robots.txt redirects to https://www.facebook.com/wyomingtheband/robots.txt. Because fetching fails with a 404 bots are allowed to crawl wyomingtheband.com. The rules is erroneously also cached for the redirect target host www.facebook.com which is clear regarding their robots.txt rules and does not allow crawling.\nNutch may cache redirected robots.txt rules only if the path part (in doubt, including the query) of the redirect target URL is exactly /robots.txt.",
        "Issue Links": [
            "/jira/browse/NUTCH-2646",
            "/jira/browse/NUTCH-731",
            "https://github.com/apache/nutch/pull/331",
            "https://github.com/apache/nutch/pull/342"
        ]
    },
    "NUTCH-2582": {
        "Key": "NUTCH-2582",
        "Summary": "Set pool size of XML SAX parsers used for MIME detection in Tika 1.19",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.18",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/May/18 15:55",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "18/Nov/20 11:27",
        "Description": "See NUTCH-2578. Tika 1.19 will use a pool of SAX parser to avoid the bottleneck while creating a new one (see NUTCH-2578/TIKA-2645). Fetcher should adjust the size of the pool to the number of Fetcher threads (or a fraction of it because most threads are likely to be busy fetching content).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/554"
        ]
    },
    "NUTCH-2583": {
        "Key": "NUTCH-2583",
        "Summary": "Upgrading Nutch's dependencies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Ralf",
        "Created": "24/May/18 13:47",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "02/Jun/18 11:38",
        "Description": "Hi,\n\u00a0\nIt would be nice to be able to upgrade all of Nutch's dependencies to the latest possible available.\nI've attached an Ivy.xml with the latest possible dependencies without breaking the compile. I've tested it with a few runs of the \"crawl script\", so far it seems to work, it generates, it fetches, it parses, it indexes to Solr..... Increasing any of this dependencies breaks the compile.\n\u00a0\nPS: I haven't touched any of the Hadoop stuff and don't remember if I touched the testing part or not.",
        "Issue Links": []
    },
    "NUTCH-2584": {
        "Key": "NUTCH-2584",
        "Summary": "Upgrade parse-tika to use Tika 1.18",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "24/May/18 14:51",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "02/Jun/18 11:52",
        "Description": "Tika 1.18 is released and NUTCH-2583 includes and upgrade of tika-core.\nSee howto_upgrade_tika.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/336"
        ]
    },
    "NUTCH-2585": {
        "Key": "NUTCH-2585",
        "Summary": "NPE in TrieStringMatcher",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "25/May/18 14:32",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/May/19 15:20",
        "Description": "Stumbled on this one just now:\n\n\r\n2018-05-25 14:29:31,844 INFO [FetcherThread] org.apache.nutch.fetcher.FetcherThread: FetcherThread 42 fetch of http://www.ndcmediagroep.nl/wp-content/uploads/2017/03/Leaflet-Noflik-Wenje.pdf failed with: java.lang.NullPointerException\r\n\tat org.apache.nutch.util.TrieStringMatcher$TrieNode.getChild(TrieStringMatcher.java:107)\r\n\tat org.apache.nutch.util.SuffixStringMatcher.shortestMatch(SuffixStringMatcher.java:74)\r\n\tat org.apache.nutch.urlfilter.suffix.SuffixURLFilter.filter(SuffixURLFilter.java:164)\r\n\tat org.apache.nutch.net.URLFilters.filter(URLFilters.java:43)\r\n\tat org.apache.nutch.fetcher.FetcherThread.handleRedirect(FetcherThread.java:487)\r\n\tat org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:404)\r\n\n\nEdit - added on 1 may 2019, i got a slightly different strack trace and using PrefixURLFilter this time:\n\n\r\n2019-05-01 08:50:07,282 INFO [FetcherThread] org.apache.nutch.fetcher.FetcherThread: FetcherThread 38 fetch of https://kanaalstreek.nl/fzh/2018/06/04/vijf-maal-goud-voor-pegasus-op-nk failed with: java.lang.NullPointerException\r\n\tat org.apache.nutch.util.TrieStringMatcher$TrieNode.getChild(TrieStringMatcher.java:107)\r\n\tat org.apache.nutch.util.PrefixStringMatcher.shortestMatch(PrefixStringMatcher.java:79)\r\n\tat org.apache.nutch.urlfilter.prefix.PrefixURLFilter.filter(PrefixURLFilter.java:73)\r\n\tat org.apache.nutch.net.URLFilters.filter(URLFilters.java:43)\r\n\tat org.apache.nutch.fetcher.FetcherThread.handleRedirect(FetcherThread.java:487)\r\n\tat org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:404)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/452",
            "https://github.com/apache/nutch/pull/452"
        ]
    },
    "NUTCH-2586": {
        "Key": "NUTCH-2586",
        "Summary": "Add a fallback mechanism for missing meta tags",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "metadata,                                            plugin",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "28/May/18 14:29",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "While using nutch, we faced the following issue: some web pages miss a \"description\"  meta tag, but include an \"og:description\" meta (using the open graph protocol).\nHere are two examples: \n\nhttp://imagenesdelavirgenmaria.com/17-imagenes-de-la-virgen-maria-de-guadalupe/\nhttp://mixcdsource.com/product/dj-arson-dj-sin-cerothe-hit-list-18-5-reggaeton-edition/\n\nIt would be nice to have a configurable list of fallback meta tags to use when the main meta tag is absent. Something that would allow us to specify, in the configuration, \"when the 'description' meta is missing, use 'og:description', when 'title' is missing, use 'og:title', etc...\" .",
        "Issue Links": []
    },
    "NUTCH-2587": {
        "Key": "NUTCH-2587",
        "Summary": "Tests do not pass",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "28/May/18 15:15",
        "Updated": "29/May/18 08:46",
        "Resolved": "29/May/18 08:46",
        "Description": "The test TestMetatagParser in the plugin parse-metatags does not pass.",
        "Issue Links": []
    },
    "NUTCH-2588": {
        "Key": "NUTCH-2588",
        "Summary": "Getting status code x01 (unfetched) on more than 80% crawled urls",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "crawldb,                                            fetcher",
        "Assignee": null,
        "Reporter": "Usama Tahir",
        "Created": "29/May/18 06:28",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "30/May/18 10:29",
        "Description": "when i run nucth with external links enabled, seed of 10 urls and number of rounds 5 using command\u00a0\nbin/crawl <seed_path> <db>\u00a0 [<solr url>] <number of rounds>\ni have default topN value which is 50000\nthe process completes execution in 11 to 12 hours and generated urls rows of about 280000.\nwhen we analyze hbase table and check status codes of all urls we got round about\u00a0242000 urls having status code of x01 [un fetched]\u00a0\nit means 242000 urls out of 280000 which nutch extracted remains unfetched.\nafter some debugging of nutch and analyzing its logs i found that those urls which have status code of x01 are not even tried to fetch.\nis this the bug of nutch or something configuration issue?\n kindly resolve my issue as soon as possible.",
        "Issue Links": []
    },
    "NUTCH-2589": {
        "Key": "NUTCH-2589",
        "Summary": "HTML redirections are not followed when using parse-tika",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "29/May/18 16:00",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "02/Jun/18 11:54",
        "Description": "Html redirections using meta tags are supported in nutch. They work well when using parse-html to parse files. However, when using parse-tika, they are not detected.\nThis is because of https://issues.apache.org/jira/browse/TIKA-2652\nTika emits redirection meta tags as :\n\n\r\n<meta name=\"refresh\" content=\"0; url=http://example.com\"/>\r\n\n\nwhereas org.apache.nutch.parse.tika.HTMLMetaProcessor expects meta tags having the following format :\n\n\r\n<meta http-equiv=\"refresh\" content=\"0; url=http://example.com\">\r\n\n\nThe bug can be reproduced with the following nutch-site.xml:\n\n\r\n<?xml version=\"1.0\"?>\r\n<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\r\n\r\n<!-- Put site-specific property overrides in this file. -->\r\n\r\n<configuration>\r\n    <property>\r\n        <name>plugin.includes</name>\r\n        <value>protocol-http|parse-tika</value>\r\n    </property>\r\n    <property>\r\n        <name>http.agent.name</name>\r\n        <value>blah</value>\r\n    </property>\r\n</configuration>\r\n\n\nfetching this url: http://www.google.com/policies/technologies/ads/\nThe resulting status is \n\nsuccess(1,0)\n\n whereas using parse-html, the resulting status is \n\nsuccess(1,100), args[0]=https://policies.google.com/technologies/ads, args[1]=0",
        "Issue Links": []
    },
    "NUTCH-2590": {
        "Key": "NUTCH-2590",
        "Summary": "SegmentReader -get fails",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "segment",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "31/May/18 15:36",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "02/Jun/18 12:18",
        "Description": "SegmentReader -get fails in local and (pseudo-)distributed mode:\n\n% bin/nutch readseg -get crawl/segments/20180531124348/ \"https://nutch.apache.org/\"\r\nSegmentReader: get 'https://nutch.apache.org/'\r\nException:\r\njava.io.FileNotFoundException: File file:.../crawl/segments/20180531124348/crawl_generate/part-r-00000/data does not exist\r\n        at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\r\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\r\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\r\n        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428)\r\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1820)\r\n        at org.apache.hadoop.io.MapFile$Reader.createDataFileReader(MapFile.java:456)\r\n        at org.apache.hadoop.io.MapFile$Reader.open(MapFile.java:429)\r\n        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:399)\r\n        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:408)\r\n        at org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getReaders(MapFileOutputFormat.java:98)\r\n        at org.apache.nutch.segment.SegmentReader.getSeqRecords(SegmentReader.java:443)\r\n        at org.apache.nutch.segment.SegmentReader.access$200(SegmentReader.java:72)\r\n        at org.apache.nutch.segment.SegmentReader$3.run(SegmentReader.java:341)\r\nException:\r\njava.io.FileNotFoundException: File file:.../crawl/segments/20180531124348/crawl_parse/part-r-00000/data does not exist\r\n        at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:611)\r\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)\r\n        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)\r\n        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:428)\r\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1820)\r\n        at org.apache.hadoop.io.MapFile$Reader.createDataFileReader(MapFile.java:456)\r\n        at org.apache.hadoop.io.MapFile$Reader.open(MapFile.java:429)\r\n        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:399)\r\n        at org.apache.hadoop.io.MapFile$Reader.<init>(MapFile.java:408)\r\n        at org.apache.hadoop.mapreduce.lib.output.MapFileOutputFormat.getReaders(MapFileOutputFormat.java:98)\r\n        at org.apache.nutch.segment.SegmentReader.getSeqRecords(SegmentReader.java:443)\r\n        at org.apache.nutch.segment.SegmentReader.access$200(SegmentReader.java:72)\r\n        at org.apache.nutch.segment.SegmentReader$4.run(SegmentReader.java:353)",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/337"
        ]
    },
    "NUTCH-2591": {
        "Key": "NUTCH-2591",
        "Summary": "Can not import org.json.simple.JSONObject in Nutch 2.3.1",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Usama Tahir",
        "Created": "01/Jun/18 09:16",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Jun/18 10:37",
        "Description": "I am modifying Nutch 2.3.1 and integrating CLD2 language detector in FetcherReducer.java class. I made a separate class name LangInfo.java in org.apache.nutch.fetcher package.\nIn this LangInfo.java class i tried to write import org.json.simple.JSONObject;\nand org.json.simple.parser;\nbut when i build it using ant runtime command in Nutch directory it fails with error\n\u00a0\nsrc/java/org/apache/nutch/fetcher/LangInfo.java:20: error: package org.json.simple.JSONObject does not exist\n\u00a0\nsrc/java/org/apache/nutch/fetcher/LangInfo.java:21: error: package org.json.simple.parser does not exist**\n\u00a0\nBut when i open it in eclipse, eclipse does not shows error on this import. and if i go to project properties > Libraries there i found json-simple-1.1.0.jar. it means this jar file already exists. but why i am always getting errors due to this import?\ni have attached two screenshots with this issue:\n1- jar.png (this is eclipse screenshot showing json-simple-1.1.0.jar exists)\n2- error.png (this is terminal build error log. when i run ant runtime command)\nKindly help me ASAP.\nTIA.",
        "Issue Links": []
    },
    "NUTCH-2592": {
        "Key": "NUTCH-2592",
        "Summary": "Fetcher to log reason of failed fetches",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "04/Jun/18 09:10",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "06/Jun/18 08:09",
        "Description": "By a stupid mistake introduced with NUTCH-2553 (it's my fault) Fetcher does not log the reason for a failed fetch:\n\n2018-06-04 09:34:40,522 INFO  fetcher.FetcherThread - FetcherThread 23 fetch of http://localhost/index.html failed with:",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/339"
        ]
    },
    "NUTCH-2593": {
        "Key": "NUTCH-2593",
        "Summary": "Single mode doesn't work in RabbitMQ indexer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "05/Jun/18 02:18",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "06/Jun/18 07:48",
        "Description": "Switching from multiple to simple has no effect. This is because the commit.mode parameter is used instead of commit.commit in the RabbitMQ indexer.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/341"
        ]
    },
    "NUTCH-2594": {
        "Key": "NUTCH-2594",
        "Summary": "Documentation for indexer plugins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Resolved",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "06/Jun/18 00:09",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "28/Jun/18 15:59",
        "Description": "A NutchTutorial wiki page for indexer plugins, to document the structure proposed\u00a0in\u00a0GitHub Pull Request #218",
        "Issue Links": []
    },
    "NUTCH-2595": {
        "Key": "NUTCH-2595",
        "Summary": "Upgrade crawler-commons dependency to 0.10",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "08/Jun/18 10:11",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "12/Jun/18 15:40",
        "Description": "See CHANGES",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/345"
        ]
    },
    "NUTCH-2596": {
        "Key": "NUTCH-2512 Nutch does not build under JDK9",
        "Summary": "Upgrade from org.mortbay.jetty to org.eclipse.jetty",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.20",
        "Component/s": "nutch server,                                            test",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Jun/18 12:52",
        "Updated": "17/Mar/23 16:38",
        "Resolved": "17/Mar/23 15:55",
        "Description": "The old org.mortbay.jetty libs (not maintained since 2008) are still used for the Nutch server and multiple unit tests. Nutch should be upgraded to the maintained org.eclipse.jetty libs/packages.\nThe old dependency causes the unit tests of the HTTP protocol plugins to fail when built with Java 9 or 10 with the following error while compiling JSP classes (see NUTCH-2512):\n\n2018-06-06 11:03:02,335 ERROR mortbay.log (Slf4jLog.java:warn(87)) - /basic-http.jsp\r\njava.lang.ClassCastException: java.base/jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to java.base/java.net.URLClassLoader\r\n        at org.apache.jasper.compiler.JspRuntimeContext.<init>(JspRuntimeContext.java:94)\r\n        at org.apache.jasper.servlet.JspServlet.init(JspServlet.java:100)\r\n        at org.mortbay.jetty.servlet.ServletHolder.initServlet(ServletHolder.java:440)\r\n        at org.mortbay.jetty.servlet.ServletHolder.getServlet(ServletHolder.java:339)\r\n        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:487)\r\n        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401)\r\n        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)\r\n        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)\r\n        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)\r\n        at org.mortbay.jetty.Server.handle(Server.java:326)\r\n        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)\r\n        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)\r\n        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)\r\n        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)\r\n        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)\r\n        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)\r\n        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)\r\n2018-06-06 11:03:02,356 WARN  mortbay.log (Slf4jLog.java:warn(76)) - /basic-http.jsp: javax.servlet.UnavailableException: java.lang.ClassCastException: java.base/jdk.internal.loader.ClassLoaders$AppClassLoader cannot be cast to java.base/java.net.URLClassLoader\r\n2018-06-06 11:03:02,386 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped SelectChannelConnector@127.0.0.1:47504\r\n...\r\nHTTP Status Code for http://127.0.0.1:47504/basic-http.jsp expected:<200> but was:<500>\r\njunit.framework.AssertionFailedError: HTTP Status Code for http://127.0.0.1:47504/basic-http.jsp expected:<200> but was:<500>\r\n        at org.apache.nutch.protocol.http.TestProtocolHttp.fetchPage(TestProtocolHttp.java:130)\r\n        at org.apache.nutch.protocol.http.TestProtocolHttp.testStatusCode(TestProtocolHttp.java:80)",
        "Issue Links": [
            "/jira/browse/NUTCH-2984",
            "https://github.com/apache/nutch/pull/574",
            "https://github.com/apache/nutch/pull/758"
        ]
    },
    "NUTCH-2597": {
        "Key": "NUTCH-2597",
        "Summary": "NPE in updatehostdb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "hostdb",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "13/Jun/18 10:57",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "21/Jun/18 14:38",
        "Description": "I get an NPE on updatehostdb. I start with a clean crawlDB & hostDB. After an inject, I do an updatehostdb with -checkAll and get the following stacktrace:\n\n\r\n2018-06-13 10:45:21,958 WARN hostdb.ResolverThread - java.lang.NullPointerException\r\n at org.apache.hadoop.io.SequenceFile$Writer.checkAndWriteSync(SequenceFile.java:1359)\r\n at org.apache.hadoop.io.SequenceFile$Writer.append(SequenceFile.java:1400)\r\n at org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat$1.write(SequenceFileOutputFormat.java:83)\r\n at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)\r\n at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)\r\n at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)\r\n at org.apache.nutch.hostdb.ResolverThread.run(ResolverThread.java:82)\r\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n at java.lang.Thread.run(Thread.java:748)\r\n\n\nIs this related to NUTCH-2375?\nIf further testing is needed, please let me know!",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/349"
        ]
    },
    "NUTCH-2598": {
        "Key": "NUTCH-2598",
        "Summary": "URLNormalizerChecker fails on invalid URLs in input",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "13/Jun/18 16:18",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "01/Sep/19 10:06",
        "Description": "I use the URLNormalizerChecker (urlnormalizer-regex and urlnormalizer-basic) to normalize URLs before further processing them. If one of the used normalizers throws a MalformedURLException when the URLNormalizer.normalize(...) method is called, this isn't caught and causes the checker to exit:\n\nException in thread \"main\" java.net.MalformedURLException: For input string: \"???120810002\"\r\n        at java.net.URL.<init>(URL.java:627)\r\n        at java.net.URL.<init>(URL.java:490)\r\n        at java.net.URL.<init>(URL.java:439)\r\n        at org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.normalize(BasicURLNormalizer.java:100)\r\n        at org.apache.nutch.net.URLNormalizers.normalize(URLNormalizers.java:319)\r\n        at org.apache.nutch.net.URLNormalizerChecker.process(URLNormalizerChecker.java:75)\r\n        at org.apache.nutch.util.AbstractChecker.processStdin(AbstractChecker.java:97)\r\n        at org.apache.nutch.util.AbstractChecker.run(AbstractChecker.java:77)\r\n        at org.apache.nutch.net.URLNormalizerChecker.run(URLNormalizerChecker.java:71)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.nutch.net.URLNormalizerChecker.main(URLNormalizerChecker.java:80)\r\nCaused by: java.lang.NumberFormatException: For input string: \"???120810002\"\r\n        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\r\n        at java.lang.Integer.parseInt(Integer.java:580)\r\n        at java.lang.Integer.parseInt(Integer.java:615)\r\n        at java.net.URLStreamHandler.parseURL(URLStreamHandler.java:222)\r\n        at java.net.URL.<init>(URL.java:622)\r\n        ... 10 more\r\n\n\nThe URLNormalizer interface declares the MalformedURLException, it should be caught in the normalizer checker:\n\nlog the error\nreturn/output empty string",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/435"
        ]
    },
    "NUTCH-2599": {
        "Key": "NUTCH-2599",
        "Summary": "charset detection issue with parse-tika",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "15/Jun/18 13:52",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Here is an example page that is displayed correctly in web browsers, but is decoded with the wrong charset in nutch : https://gerardbouchar.github.io/html-encoding-example/index.html\n\u00a0\nThis page's contents are encoded in UTF-8, it is served with HTTP headers indicating that it is in UTF-8, but it contains a bogus HTML meta tag indicating that is encoded in ISO-8859-1.\n\u00a0\nThis is a tricky case, but there is a W3C specification about how to handle it. It clearly states that the HTTP header (transport layer information) should have precedence over the HTML meta tag (obtained in byte stream prescanning). Browsers do respect the spec, but the tika parser doesn't.\n\u00a0\nLooking at the source code, it looks like the charset information is not even extracted from the HTTP headers.\n\u00a0\n\n\r\nHTTP/1.1 200 OK\r\nContent-Type: text/html; charset=utf-8\r\n\r\n\r\n<!doctype html>\r\n<html>\r\n\u00a0 <head>\r\n\u00a0\u00a0\u00a0 <meta charset=\"iso-8859-1\">\r\n\u00a0 </head>\r\n\u00a0 <body>\r\n\u00a0\u00a0\u00a0 <a href=\"/\">fran\u00e7ais</a>\r\n\u00a0 </body>\r\n</html>",
        "Issue Links": [
            "/jira/browse/TIKA-2671"
        ]
    },
    "NUTCH-2600": {
        "Key": "NUTCH-2600",
        "Summary": "Refactoring indexer-solr",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "18/Jun/18 01:49",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "21/Jun/18 14:50",
        "Description": "indexer-solr includes an interface to define the parameters used by the plugin, however, in some cases this interface is not used and a string is used instead. Also, in index-writers.xml file, the commit.index parameter is never used in the code and the collection parameter is not included in index-writers.xml file.\nOn the other hand, according to the configuration of indexer-solr plugin, it seems to support Basic Authentication but the username and password are never used. I don't know the reason for this, but I believe that it could be a good feature for Nutch and besides I think we should update Solrj library.",
        "Issue Links": [
            "/jira/browse/NUTCH-2360",
            "/jira/browse/NUTCH-1220",
            "https://github.com/apache/nutch/pull/351"
        ]
    },
    "NUTCH-2601": {
        "Key": "NUTCH-2601",
        "Summary": "Elasticsearch Rest and Amazon CloudSearch have the same implementation class in indexer-writers.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "18/Jun/18 01:52",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "21/Jun/18 14:53",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/350"
        ]
    },
    "NUTCH-2602": {
        "Key": "NUTCH-2602",
        "Summary": "Configuration values in the description of index writers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "18/Jun/18 01:58",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "27/Sep/18 19:47",
        "Description": "Since\u00a0GitHub Pull Request #218\u00a0when you have 2+ different configuration of the same index writers (the same implementation class), the index command print the same description\u00a0several times. I\u00a0propose\u00a0the describe() method show the values of its own configuration and not a generic one.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/356"
        ]
    },
    "NUTCH-2603": {
        "Key": "NUTCH-2603",
        "Summary": "Bring back legacy pre-Tika parsers and use them as back up parsers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Arkadi Kosmynin",
        "Created": "20/Jun/18 06:18",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "12/Nov/19 14:50",
        "Description": "There are cases when legacy parsers successfully parse documents on which Tika fails. I am attaching a list of examples of such documents. Nutch allows use of more than one parser on a document, in a sequence, until the document has been parsed successfully. Thus, old parsers can be combined with Tika to achieve better parsing success rate, at least until Tika is perfect.",
        "Issue Links": []
    },
    "NUTCH-2604": {
        "Key": "NUTCH-2604",
        "Summary": "The lines defining catch-all (*) parser in parse-plugins.xml are ignored",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Arkadi Kosmynin",
        "Created": "20/Jun/18 06:32",
        "Updated": "20/Jun/18 06:32",
        "Resolved": null,
        "Description": "The lines defining catch-all  plugin in parse-plugins.xml are not effective, because they are ignored, as long as there is at least one plugin claiming * in its plugin.xml file. In some cases, Nutch assigns * capability to plugins that don\u2019t even claim it. For example, I can\u2019t understand, why Arch content blocking plugin gets it.",
        "Issue Links": []
    },
    "NUTCH-2605": {
        "Key": "NUTCH-2605",
        "Summary": "The Feed plugin causes a NumberFormatException",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Cannot Reproduce",
        "Affects Version/s": "1.14",
        "Fix Version/s": "None",
        "Component/s": "indexer,                                            parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Arkadi Kosmynin",
        "Created": "20/Jun/18 06:35",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "26/Sep/19 16:26",
        "Description": "The Feed plugin seems to have a major problem. The line 102 in\u00a0 FeedIndexingFilter.java generated a NumberFormatException (which caused the failure of the entire crawling process!) because it was trying to parse a date in string format, not a number. Given that this metadata piece was generated by the feed parser (same plugin), it seems that the plugin is in disagreement with itself.",
        "Issue Links": []
    },
    "NUTCH-2606": {
        "Key": "NUTCH-2606",
        "Summary": "MIME detection is wrong for plain-text documents send as Content-Type \"application/msword\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "20/Jun/18 10:03",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "19/Nov/18 20:53",
        "Description": "Plain-text documents send as Content-Type \"application/msword\" are tried to parse as Word documents. The MIME detection should be fixed, so that these are correctly identified as plain-text documents. See NUTCH-2603 and https://www.atnf.csiro.au/computing/software/gipsy/doc/update.doc",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/392"
        ]
    },
    "NUTCH-2607": {
        "Key": "NUTCH-2607",
        "Summary": "ParserChecker should call ScoringFilters.passScoreAfterParsing() on all parses",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Jun/18 09:23",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "28/Jun/18 09:57",
        "Description": "A ParseResult may contain multiple parses, e.g., the feed parser adds one for every item in the RSS/Atom feed. The tool ParseSegment calls the method ScoringFilters.passScoreAfterParsing() for every parse of the ParseResult. ParserChecker does this only for the parse identified by the fetched URL. It should behave same as ParseSegment.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/352"
        ]
    },
    "NUTCH-2608": {
        "Key": "NUTCH-2608",
        "Summary": "Reduce size of Nutch job file and package",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "deployment",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "21/Jun/18 09:56",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The Nutch 1.15 binary package and the Nutch job file will reach or even exceed 300 MB. A huge job file isn't ideal as it needs to be distributed in the Hadoop cluster. There are several reasons for this:\n\nmissing exclusion of unneeded indirect dependencies, e.g.:\n\t\nindexer-elastic includes many Lucene libraries related to query processing\n\n\nhuge plugins. That can be hardly avoided as these plugins provide also \"huge\" functionality, e.g., parse-tika is able to pass a long list of document formats. However, it should be possible to avoid overlaps between plugins (in case dependency upgrades can be made at the same time). Hot candidates are:\n\t\nany23 includes tika-parsers as does parse-tika\nlib-selenium and lib-htmlunit both use selenium and phantomjsdriver libs\n\n\n\nOf course, users can easily build smaller packages by excluding unused plugins in src/plugin/build.xml. But would be good, at least, to try to reduce the size of the Nutch package.",
        "Issue Links": []
    },
    "NUTCH-2609": {
        "Key": "NUTCH-2609",
        "Summary": "urlnormalizer-basic to normalize path of file: URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Jun/18 15:25",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "28/Jun/18 11:09",
        "Description": "% echo \"file:/var/www/html/foo/../bar/index.html\" \\\r\n\u00a0 | nutch normalizerchecker -normalizer urlnormalizer-basic -stdin\r\nChecking combination of these URLNormalizers: BasicURLNormalizer \r\nfile:/var/www/html/foo/../bar/index.html",
        "Issue Links": []
    },
    "NUTCH-2610": {
        "Key": "NUTCH-2610",
        "Summary": "How to exclude specific domains from Nutch crawling",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "injector,                                            plugin",
        "Assignee": null,
        "Reporter": "Usama Tahir",
        "Created": "22/Jun/18 11:35",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "22/Jun/18 13:35",
        "Description": "I am using nutch for crawling sites. i want to use a blacklisting concept.\nFor example if i add a domain in black list, none of its document should be in my crawl.\ncan you guide me how to do that?\nI came to know it can be done by regex-urlfilter.txt file in which we write domain with - sign like:\n\n-jang.com.pk\n\nis there better way to do that?\nor is there any way that we write all our blacklist domains into a separate file and include it in regex-urlfilter file to exclude those blacklisted domains?\nplease guide me as soon as possible.\nTIA",
        "Issue Links": []
    },
    "NUTCH-2611": {
        "Key": "NUTCH-2611",
        "Summary": "Add line-breaks when parsing HTML block-level elements",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.15",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "25/Jun/18 11:38",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "28/Jun/18 09:08",
        "Description": "Currently, the HTML and Tika parser only add newlines following text-nodes that contain only whitespaces (e.g </span> <span>), but not based on what the tags are, so for example a </div><div> will not add a new line.\nWhile some applications do not differentiate between a space and a new line, many others see the semantic difference (two following words in the same sentence are \"near\", but in separate sentences they are not).\nI believe adding newlines after block-level HTML elements, while not a panacea, will be an improvement on the current behavior.\nNUTCH-2318 is related to this.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/354"
        ]
    },
    "NUTCH-2612": {
        "Key": "NUTCH-2612",
        "Summary": "Support for sitemap processing by hostname",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "sitemap",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "26/Jun/18 13:47",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "09/Sep/19 13:01",
        "Description": "Add support to sitemap processor for processing just hostnames. Similar to the mapper eating sitemap URL's, but then with BaseRobotRules finding the sitemap URL's itself.\nWill upload patch soon.",
        "Issue Links": []
    },
    "NUTCH-2613": {
        "Key": "NUTCH-2613",
        "Summary": "Documentation for exchange component",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "documentation,                                            indexer",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "28/Jun/18 15:54",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "26/Nov/18 16:52",
        "Description": "After GitHub Pull Request #340 a NutchTutorial wiki page for exchange component is necessary.",
        "Issue Links": []
    },
    "NUTCH-2614": {
        "Key": "NUTCH-2614",
        "Summary": "NPE in CrawlDbReader -stats on empty CrawlDb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14,                                            1.15",
        "Fix Version/s": "1.15",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "03/Jul/18 12:22",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "11/Jul/18 10:40",
        "Description": "Got this in master:\n\n\r\nException in thread \"main\" java.lang.NullPointerException\r\n        at org.apache.nutch.crawl.CrawlDbReader.processStatJob(CrawlDbReader.java:555)\r\n        at org.apache.nutch.crawl.CrawlDbReader.run(CrawlDbReader.java:914)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.nutch.crawl.CrawlDbReader.main(CrawlDbReader.java:980)\r\n\n\nNot sure why it happens or which commit caused the problem.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/357",
            "https://github.com/apache/nutch/pull/357"
        ]
    },
    "NUTCH-2615": {
        "Key": "NUTCH-2615",
        "Summary": "Publisher for Telegram",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "publisher",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "04/Jul/18 14:31",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "Publisher plugin for Telegram",
        "Issue Links": []
    },
    "NUTCH-2616": {
        "Key": "NUTCH-2616",
        "Summary": "Review routing of deletions by Exchange component",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jul/18 08:31",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "19/Jul/18 12:52",
        "Description": "If the exchange component (NUTCH-2412) is enabled it must also route deletions (404, etc.) to the configured index writers. Deletions are done alone using the document ID (URL), there is no NutchDocument (or it's null) which needs to handled to avoid an NPE in the Exchanges class or the exchange plugins.\nNUTCH-2412 has added a new delete method in the IndexWriters class:\n\ndelete(String, NutchDocument) is now called from the indexing job (bin/nutch index ... -deleteGone). However, the NutchDocument is always null in case of deletions, see IndexerMapReduce.DELETE_ACTION.\ndelete(String) is now a no-op but is still called from CleaningJob (bin/nutch clean ...)\n\nWe could (roannel, are there better options?)\n\nsend deletions to all index writers. This causes a certain overhead (could be critical if deletion lists are long).\npass a document containing only a single field (the document ID / URL) to the exchange component.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/363"
        ]
    },
    "NUTCH-2617": {
        "Key": "NUTCH-2617",
        "Summary": "Disable Exchange component by default",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jul/18 08:37",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "10/Jul/18 08:51",
        "Description": "The Exchange component is now enabled by default using the sample exchange.xml configuration, installed/instantiated from exchange.xml.template. It should be off by default: it's only a sample and users should not be forced to touch it if they use only a single indexer (Solr, or anything else).",
        "Issue Links": []
    },
    "NUTCH-2618": {
        "Key": "NUTCH-2618",
        "Summary": "protocol-okhttp not to use http.timeout for max duration to fetch document",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Jul/18 09:29",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "19/Jul/18 12:58",
        "Description": "Protocol-okhttp (NUTCH-2576) uses the HTTP network timeout (http.timeout) as time limit for the max duration to fetch a document. The timeout value (default = 10 sec.) is usually to small to fetch larger documents. The max fetch duration should be separately configurable, e.g., by a property http.time.limit (similar to http.content.limit).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/360"
        ]
    },
    "NUTCH-2619": {
        "Key": "NUTCH-2619",
        "Summary": "protocol-okhttp: allow to keep partially fetched docs as truncated",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Jul/18 10:18",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "19/Jul/18 12:59",
        "Description": "Sometimes fetching a larger document times out after some content has already been downloaded. For some use cases it may be better to save this partially fetched document and mark it as truncated, instead of retrying the fetch later (may fail for the same reason again).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/361"
        ]
    },
    "NUTCH-2620": {
        "Key": "NUTCH-2620",
        "Summary": "urlfilter-validator incorrectly assumes that top-level domains are not longer than 4 characters",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gareth",
        "Created": "12/Jul/18 19:33",
        "Updated": "01/Oct/19 14:30",
        "Resolved": "16/Jul/18 10:24",
        "Description": "The code assumes a TLD can be max 4 in length. But this is wrong.\n\u00a0\nhttps://github.com/apache/nutch/blob/f011b2193c24e031aafbcfa1e66fc2bcbb59098b/src/plugin/urlfilter-validator/src/java/org/apache/nutch/urlfilter/validator/UrlValidator.java#L280",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/362"
        ]
    },
    "NUTCH-2621": {
        "Key": "NUTCH-2290 Update licenses of bundled libraries",
        "Summary": "Generate report of third-party licenses",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "20/Jul/18 11:40",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "17/Aug/18 10:02",
        "Description": "Would be good to have the possibility to generate a list of third-party licenses of all dependent libraries shipped with the binary 1.x package. There is the ant report target which includes also licenses but a list or tabular view would be much easier to process than a graph or HTML report. Cf. the Maven license plugin.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/365"
        ]
    },
    "NUTCH-2622": {
        "Key": "NUTCH-2290 Update licenses of bundled libraries",
        "Summary": "Unbundle LGPL-licensed jars from binary release",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "build,                                            deployment",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "20/Jul/18 12:23",
        "Updated": "01/Oct/19 14:29",
        "Resolved": "25/Jul/18 13:39",
        "Description": "The tool commoncrawldump depends on LGPL-licensed libs to create WARC files. The LGPL license is not compatible with the Apache license (aka. category x), the jars cannot be distributed together with Nutch (see 1).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/366"
        ]
    },
    "NUTCH-2623": {
        "Key": "NUTCH-2623",
        "Summary": "Fetcher to guarantee delay for same host/domain/ip independent of http/https protocol",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "23/Jul/18 13:28",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "07/Oct/18 19:12",
        "Description": "Fetcher uses a combination of protocol and host/domain/ip as ID for fetch item queues, see FetchItem.java. This inhibits a guaranteed delay, in case both http:// and https:// URLs are fetched from the same host/domain/ip, e.g. here with a large delay of 30 sec.:\n\n2018-07-23 14:54:39,834 INFO fetcher.FetcherThread - FetcherThread 24 fetching http://nutch.apache.org/ (queue crawl delay=30000ms)\r\n2018-07-23 14:54:39,846 INFO fetcher.FetcherThread - FetcherThread 23 fetching https://nutch.apache.org/ (queue crawl delay=30000ms)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/369"
        ]
    },
    "NUTCH-2624": {
        "Key": "NUTCH-2624",
        "Summary": "protocol-okhttp resource leak",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.15",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "23/Jul/18 15:21",
        "Updated": "01/Oct/19 14:28",
        "Resolved": "25/Jul/18 13:31",
        "Description": "The protocol-okhttp leaks response streams as logged by okhttp3.ConnectionPool:\n\nWARNING: A connection to ... was leaked. Did you forget to close a response body?\r\n\n\nThis has been introduced in commit f598db7 - need to catch the IOException and close the response, then re-throw so that FetcherThread can handle the exception.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/367"
        ]
    },
    "NUTCH-2625": {
        "Key": "NUTCH-2625",
        "Summary": "ProtocolFactory.getProtocol(url) may create multiple plugin instances",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "24/Jul/18 14:18",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "20/Oct/18 17:13",
        "Description": "The method ProtocolFactory.getProtocol(URL url) may create unnecessarily multiple instances of protocol plugins given the same configuration. The following snippets from a Fetcher using 100 FetcherThreads show that the setConf(conf) method of the protocol-okhttp plugin is called 100 times (once for each thread):\n\n2018-07-12 12:04:32,811 INFO [main] org.apache.nutch.fetcher.FetcherThread: FetcherThread 1 Using queue mode : byHost\r\n... (skipped 98 repeated messages)\r\n2018-07-12 12:04:33,136 INFO [main] org.apache.nutch.fetcher.FetcherThread: FetcherThread 1 Using queue mode : byHost\r\n\r\n...\r\n\r\n2018-07-12 12:04:37,493 INFO [FetcherThread] org.apache.nutch.protocol.RobotRulesParser: robots.txt whitelist not configured.\r\n2018-07-12 12:04:37,493 INFO [FetcherThread] org.apache.nutch.protocol.okhttp.OkHttp: http.proxy.host = null\r\n...\r\n2018-07-12 12:04:37,494 INFO [FetcherThread] org.apache.nutch.protocol.okhttp.OkHttp: http.enable.cookie.header = false\r\n\r\n... (skipped 98 blocks of repeated messages)\r\n\r\n2018-07-12 12:04:39,080 INFO [FetcherThread] org.apache.nutch.protocol.RobotRulesParser: robots.txt whitelist not configured.\r\n2018-07-12 12:04:39,080 INFO [FetcherThread] org.apache.nutch.protocol.okhttp.OkHttp: http.proxy.host = null\r\n...\r\n2018-07-12 12:04:39,080 INFO [FetcherThread] org.apache.nutch.protocol.okhttp.OkHttp: http.enable.cookie.header = false\r\n\n\nThe method ProtocolFactory.getProtocol(URL url) is synchronized, however each FetcherThread holds its own instance of the ProtocolFactory.",
        "Issue Links": [
            "/jira/browse/NUTCH-2253",
            "https://github.com/apache/nutch/pull/368"
        ]
    },
    "NUTCH-2626": {
        "Key": "NUTCH-2626",
        "Summary": "bin/crawl: remove option -noParsing from fetch command",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "bin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "26/Jul/18 12:20",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/May/19 15:30",
        "Description": "The script bin/crawl still uses the option -noParsing in the fetch command which has been removed by NUTCH-1102. Should be removed to avoid confusion.",
        "Issue Links": []
    },
    "NUTCH-2627": {
        "Key": "NUTCH-2627",
        "Summary": "Fetcher to optionally filter URLs",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Jul/18 11:51",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "22/Feb/19 14:51",
        "Description": "When running a large web crawl it happens that a webadmin requests to immediately stop crawling a certain domain. The default Nutch workflow applies URL filters only to seeds and outlinks. Applying filters during fetch list generation is expensive with a large CrawlDb (fetch lists are usually much shorter). Allowing the fetcher to optionally filter URLs would allow to apply changed filter rules to the next launched fetcher job even if the the segment has been already created (esp., if multiple segments are generated in one turn).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/370"
        ]
    },
    "NUTCH-2628": {
        "Key": "NUTCH-2628",
        "Summary": "Fetcher: optionally generate signature of unparsed content",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Jul/18 13:31",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/Jan/19 19:40",
        "Description": "To generate a document signature (MD5 digest) of the binary content requires that documents are parsed during the parse or fetch step. The signature is required for deduplication and detection of unmodified content and should be always available, also in a workflow which does not require that documents are parsed, e.g., because HTML content is exported to WARC files.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/371"
        ]
    },
    "NUTCH-2629": {
        "Key": "NUTCH-2629",
        "Summary": "Documentation for CSV Index Writer",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "30/Jul/18 14:37",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "21/Jan/19 11:04",
        "Description": "The CSV index writer was\u00a0introduced by\u00a0NUTCH-1541. Now, we have to document its usage.",
        "Issue Links": []
    },
    "NUTCH-2630": {
        "Key": "NUTCH-2630",
        "Summary": "Fetcher to log skipped records by robots.txt",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "01/Aug/18 11:34",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "14/Nov/18 12:05",
        "Description": "To analyze problems it would be helpful if fetcher logs URLs which are disallowed in the robots.txt - see discussion on user mailing list.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/387"
        ]
    },
    "NUTCH-2631": {
        "Key": "NUTCH-2631",
        "Summary": "KafkaIndexWriter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Ayal Ciobotaru",
        "Created": "01/Aug/18 16:38",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "26/Mar/19 01:04",
        "Description": "There is no current way to index directly into Kafka in order to have a full message based system controlled by Kafka. Created a KafkaIndexWriter in order to\u00a0produce the crawled documents into Kafka and have Kafka distribute the messages as necessary.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/372",
            "https://github.com/apache/nutch/pull/373"
        ]
    },
    "NUTCH-2632": {
        "Key": "NUTCH-2632",
        "Summary": "protocol-okhttp doesn't accept proxy authentication",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Steven W",
        "Created": "09/Aug/18 03:31",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "17/Aug/18 09:56",
        "Description": "protocol-okhttp doesn't accept proxy authentication. I believe the username/password can be passed in to the OkHttpClient.Builder (https://square.github.io/okhttp/3.x/okhttp/okhttp3/OkHttpClient.Builder.html#proxyAuthenticator-okhttp3.Authenticator-)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/375"
        ]
    },
    "NUTCH-2633": {
        "Key": "NUTCH-2633",
        "Summary": "Fix deprecation warnings when building Nutch master branch under JDK 10.0.2+13",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.16",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "09/Aug/18 16:04",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "11/Aug/18 00:45",
        "Description": "I just got around to making a dev upgrade to >= JDK 10.\nWhen building master using environment JDK\nI get several compile time deprecations which are reflected in the attached build log. \nAdditionally, I get some issues with Ivy... see below\n\n\r\nWARNING: An illegal reflective access operation has occurred\r\nWARNING: Illegal reflective access by org.apache.ivy.util.url.IvyAuthenticator (file:/Users/lmcgibbn/.ant/lib/ivy-2.3.0.jar) to field java.net.Authenticator.theAuthenticator\r\nWARNING: Please consider reporting this to the maintainers of org.apache.ivy.util.url.IvyAuthenticator\r\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\r\nWARNING: All illegal access operations will be denied in a future release\r\n[ivy:resolve] :: problems summary ::\r\n[ivy:resolve] :::: ERRORS\r\n[ivy:resolve] \tunknown resolver null\r\n[ivy:resolve] \tunknown resolver null\r\n[ivy:resolve] \tunknown resolver null",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/374"
        ]
    },
    "NUTCH-2634": {
        "Key": "NUTCH-2634",
        "Summary": "Some links marked as \"nofollow\" are followed anyway.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Gerard Bouchar",
        "Created": "14/Aug/18 07:47",
        "Updated": "22/Jan/23 21:49",
        "Resolved": "08/Jan/23 19:10",
        "Description": "In order to check if an outlink in an <a> tag can be followed, nutch checks whether the value of its rel attribute is the exact string string \"nofollow\".\nHowever, the rel attribute can contain a list of link types, all of which should be respected.\nSo nutch rightfully doesn't follow a link like:\n\n\r\n<a href='top-secret.html' rel=\"nofollow\">DO NOT FOLLOW THIS LINK</a>\r\n\n\nbut wrongfully follows :\n\n\r\n<a href='top-secret.html' rel=\"nofollow noreferrer\">DO NOT FOLLOW THIS LINK</a>\r\n\n\nBecause of the code duplication in nutch's html parsers, this should be fixed in two places:\n\nparse/html/DOMContentUtils.java\nparse/tika/DOMContentUtils.java",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/751"
        ]
    },
    "NUTCH-2635": {
        "Key": "NUTCH-2635",
        "Summary": "Generator writes unneeded temporary output",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "16/Aug/18 19:16",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "07/Oct/18 18:45",
        "Description": "Generator writes the temporary output of the Selector job/step twice (see line 516). Not a big issue when generating small fetch lists but may be when working on large data. The temporary output looks like:\n\n% tree -h generate-temp-fc27fe85-9ddc-4926-b6ba-dcd0066d5007/\r\nenerate-temp-fc27fe85-9ddc-4926-b6ba-dcd0066d5007/\r\n|-- [4.0K]  fetchlist-1\r\n|   `-- [ 25M]  part-r-00000\r\n`-- [ 77M]  part-r-00000\r\n\r\n1 directory, 2 files\r\n\r\n% file generate-temp-fc27fe85-9ddc-4926-b6ba-dcd0066d5007/part-r-00000 \r\ngenerate-temp-fc27fe85-9ddc-4926-b6ba-dcd0066d5007/part-r-00000: ASCII text\r\n\r\n% file generate-temp-fc27fe85-9ddc-4926-b6ba-dcd0066d5007/fetchlist-1/part-r-00000 \r\ngenerate-temp-fc27fe85-9ddc-4926-b6ba-dcd0066d5007/fetchlist-1/part-r-00000: Apache Hadoop Sequence file version 6\r\n\n\nThe unneeded output is plain-text which explains its larger size compared to the Hadoop Sequence file.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/376"
        ]
    },
    "NUTCH-2636": {
        "Key": "NUTCH-2636",
        "Summary": "protocol-okhttp: http.proxy.exclusion.list does not work if http.proxy.username",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Aug/18 09:44",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "(see sjwoodard's comment in PR #375)\nIf square/okhttp#3995 is fixed (and the fix is released):\n\nupgrade okhttp dependency\nremove the limitations in OkHttp",
        "Issue Links": []
    },
    "NUTCH-2637": {
        "Key": "NUTCH-2637",
        "Summary": "Number of fetcher reducers is misconfigured when the arg not passed",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3,                                            2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Fumio Nakajima",
        "Created": "18/Aug/18 05:59",
        "Updated": "12/Sep/18 09:43",
        "Resolved": "12/Sep/18 09:06",
        "Description": "I'm kind a new to this, so sorry if i'm wrong.\n The thing is the number of fetcher reducers are currently set to the value of \"mapreduce.job.maps\" when the arg not passed. It should be \"mapreduce.job.reduces\".\n\u00a0\nhttps://github.com/apache/nutch/blob/2.x/src/java/org/apache/nutch/fetcher/FetcherJob.java#L216\nLine: 216, branch-2.X\n\n\r\nif (numTasks == null || numTasks < 1) {\r\ncurrentJob.setNumReduceTasks(currentJob.getConfiguration().getInt(\r\n\"mapreduce.job.maps\", currentJob.getNumReduceTasks()));\r\n} else {\r\ncurrentJob.setNumReduceTasks(numTasks);\r\n}",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/381"
        ]
    },
    "NUTCH-2638": {
        "Key": "NUTCH-2638",
        "Summary": "Publish plugins in Maven",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rustam Abdullaev",
        "Created": "29/Aug/18 09:54",
        "Updated": "09/Aug/22 06:50",
        "Resolved": "13/Jan/22 18:41",
        "Description": "The Nutch core is available in Maven central, but its plugins aren't.\nPlease publish the plugins in Maven as well.",
        "Issue Links": [
            "/jira/browse/NUTCH-2934",
            "/jira/browse/NUTCH-2292"
        ]
    },
    "NUTCH-2639": {
        "Key": "NUTCH-2639",
        "Summary": "bin/nutch fails to set native library path on Cygwin causing jobs to fail with UnsatisfiedLinkError",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4,                                            1.15",
        "Fix Version/s": "2.4,                                            1.16",
        "Component/s": "bin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Rustam Abdullaev",
        "Created": "04/Sep/18 10:59",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "11/Sep/18 08:40",
        "Description": "It is impossible to run nutch under Cygwin, even when hadoop.dll is properly available in the PATH.\nThe issue is two-fold:\n1.\u00a0JAVA_PLATFORM detects as\u00a0\"Windows_NT-amd64-64\\r\" (notice the trailing \\r)\n2. A non-existent directory lib/native/Windows_NT-amd64-64\\r\u00a0is being set as java.library.path, making Java ignore the system PATH when looking for hadoop.dll.\n\u00a0\nAs a result, the following exception is thrown on any Gora backend access:\n\njava.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n        at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n        at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:570)\r\n        at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:977)\r\n        at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:173)\r\n        at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:160)\r\n        at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:94)\r\n        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:285)\r\n        at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:344)\r\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:150)\r\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:131)\r\n        at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathForWrite(LocalDirAllocator.java:115)\r\n        at org.apache.hadoop.mapred.LocalDistributedCacheManager.setup(LocalDistributedCacheManager.java:131)\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.<init>(LocalJobRunner.java:163)\r\n        at org.apache.hadoop.mapred.LocalJobRunner.submitJob(LocalJobRunner.java:731)\r\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:432)\r\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1285)\r\n        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1282)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1614)\r\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1282)\r\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1303)\r\n        at org.apache.nutch.util.NutchJob.waitForCompletion(NutchJob.java:115)\r\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:249)\r\n        at org.apache.nutch.crawl.InjectorJob.inject(InjectorJob.java:270)\r\n        at org.apache.nutch.crawl.InjectorJob.run(InjectorJob.java:293)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.nutch.crawl.InjectorJob.main(InjectorJob.java:302)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/378"
        ]
    },
    "NUTCH-2640": {
        "Key": "NUTCH-2640",
        "Summary": "Typo: DbUpdaterJob: updatinging all",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.4",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Rustam Abdullaev",
        "Created": "04/Sep/18 11:48",
        "Updated": "11/Sep/18 08:44",
        "Resolved": "11/Sep/18 08:40",
        "Description": "There is a typo in \"DbUpdaterJob: updating*ing* all\"",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/379"
        ]
    },
    "NUTCH-2641": {
        "Key": "NUTCH-2641",
        "Summary": "ClassCastException in webui",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4,                                            1.15",
        "Fix Version/s": "2.4,                                            1.16",
        "Component/s": "web gui",
        "Assignee": null,
        "Reporter": "Rustam Abdullaev",
        "Created": "04/Sep/18 14:21",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "07/Oct/18 18:42",
        "Description": "webui 2.x constantly logs this exception whenever the status of a crawl changes:\n\njava.lang.ClassCastException: org.apache.nutch.webui.client.model.JobInfo$State cannot be cast to [Ljava.lang.Object;\r\nat java.text.MessageFormat.format(MessageFormat.java:865)\r\nat java.text.Format.format(Format.java:157)\r\nat org.apache.nutch.webui.client.impl.RemoteCommand.toString(RemoteCommand.java:72)\r\nat org.slf4j.helpers.MessageFormatter.safeObjectAppend(MessageFormatter.java:305)\r\nat org.slf4j.helpers.MessageFormatter.deeplyAppendParameter(MessageFormatter.java:277)\r\nat org.slf4j.helpers.MessageFormatter.arrayFormat(MessageFormatter.java:231)\r\nat org.slf4j.helpers.MessageFormatter.format(MessageFormatter.java:124)\r\nat org.slf4j.impl.Log4jLoggerAdapter.info(Log4jLoggerAdapter.java:322)\r\nat org.apache.nutch.webui.client.impl.CrawlingCycle.executeCrawlCycle(CrawlingCycle.java:63)\r\nat org.apache.nutch.webui.service.impl.CrawlServiceImpl.startCrawl(CrawlServiceImpl.java:71)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\nat java.lang.reflect.Method.invoke(Method.java:498)\r\nat org.springframework.aop.support.AopUtils.invokeJoinpointUsingReflection(AopUtils.java:317)\r\nat org.springframework.aop.framework.ReflectiveMethodInvocation.invokeJoinpoint(ReflectiveMethodInvocation.java:190)\r\nat org.springframework.aop.framework.ReflectiveMethodInvocation.proceed(ReflectiveMethodInvocation.java:157)\r\nat org.springframework.aop.interceptor.AsyncExecutionInterceptor$1.call(AsyncExecutionInterceptor.java:97)\r\nat java.util.concurrent.FutureTask.run(FutureTask.java:266)\r\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nat java.lang.Thread.run(Thread.java:748)\n\nThe issue is\u00a0in RemoteCommand,\u00a0line 72. The arguments to MessageFormat.format() should be in an array, even\u00a0when there is only one.\n\n\r\nstatusInfo = new MessageFormat(\"{0}\", Locale.ROOT).format(new Object[] {jobInfo.getState()});",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/380"
        ]
    },
    "NUTCH-2642": {
        "Key": "NUTCH-2642",
        "Summary": "MoreIndexingFilter parses ISO 8601 UTC dates in local time zone",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1,                                            1.14,                                            1.15",
        "Fix Version/s": "2.4,                                            1.16",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "John Lacey",
        "Created": "05/Sep/18 08:25",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "07/Oct/18 17:10",
        "Description": "The ISO 8601\u00a0pattern in MoreIndexingFilter.getTime is\u00a0\"yyyy-MM-dd'T'HH:mm:ss'Z'\".\u00a0Note the literal Z.\nhttps://github.com/apache/nutch/blob/b834b81/src/plugin/index-more/src/java/org/apache/nutch/indexer/more/MoreIndexingFilter.java#L142\nApache commons-lang's DateUtils uses the local time zone by default when parsing, and can't tell that\u00a0a string matching this pattern is specifying an offset because the pattern doesn't have an offset, just a literal \"Z\":\nhttps://github.com/apache/commons-lang/blob/b610707/src/main/java/org/apache/commons/lang3/time/DateUtils.java#L370\nSo, when parsing a date string such as \"2018-09-04T12:34:56Z\", the time is returned as a local time:\nDateUtils.parseDate(\"2018-09-04T12:34:56Z\", new String[] { \"yyyy-MM-dd'T'HH:mm:ss'Z'\" })\n=> Tue Sep 04 12:34:56 PDT 2018 (1536089696000)\nI think a reasonable fix would be to specify an offset pattern instead of a literal \"Z\": \"yyyy-MM-dd'T'HH:mm:ssXXX\". That would also allow arbitrary offsets, as well as \"Z\":\nDateUtils.parseDate(\"2018-09-04T12:34:56Z\", new String[] { \"yyyy-MM-dd'T'HH:mm:ssXXX\" })\n=>\u00a0Tue Sep 04 05:34:56 PDT 2018 (1536064496000)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/385",
            "https://github.com/apache/nutch/pull/386"
        ]
    },
    "NUTCH-2643": {
        "Key": "NUTCH-2643",
        "Summary": "ant target \"resolve-default\" to depend on \"init\"",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "12/Sep/18 09:02",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "07/Oct/18 18:54",
        "Description": "If ant resolve-default (resolve library dependencies) is called on a clean Nutch source tree, it fails because the ant ivy library is not installed (it's installed by \"ivy-init\" or \"init\"). The target \"resolve-test\" which installs the test dependencies depends on \"init\", so this should be also the case for \"resolve-default\".\n\n% ant resolve-default\r\n...\r\nresolve-default:\r\n\r\nBUILD FAILED\r\n/mnt/data/wastl/proj/crawler/nutch/git/trunk/build.xml:532: Problem: failed to create task or type antlib:org.apache.ivy.ant:resolve\r\n...\r\nThis appears to be an antlib declaration. \r\nAction: Check that the implementing library exists in one of:\r\n        -/usr/share/ant/lib\r\n        -/home/wastl/.ant/lib\r\n        -a directory added on the command line with the -lib argument\r\n\n\nNote that the build does not fail if the ivy lib is present in the system or user ant library folder.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/382"
        ]
    },
    "NUTCH-2644": {
        "Key": "NUTCH-2644",
        "Summary": "CrawlDbReader -dump ignores filter options",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "12/Sep/18 13:02",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "07/Oct/18 19:09",
        "Description": "The CrawlDbReader ignores the filter options -status and -expr when dumping a crawldb:\n\n% bin/nutch readdb crawldb/ -dump cdb.dump -status 'db_fetched' -expr 'status == \"db_fetched\"'\r\n...\r\n% grep '^Status:' cdb.dump/part-r-00000 | sort | uniq -c\r\n     10 Status: 1 (db_unfetched)\r\n     28 Status: 2 (db_fetched)\r\n      1 Status: 3 (db_gone)\r\n      1 Status: 4 (db_redir_temp)\r\n      3 Status: 7 (db_duplicate)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/383"
        ]
    },
    "NUTCH-2645": {
        "Key": "NUTCH-2645",
        "Summary": "Webgraph tools ignore command-line options",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "webgraph",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "13/Sep/18 10:07",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "07/Oct/18 19:10",
        "Description": "Some webgraph jobs/tools do not properly set command-line options in the job configuration (see NUTCH-2644 for a similar problem in CrawlDbReader).",
        "Issue Links": []
    },
    "NUTCH-2646": {
        "Key": "NUTCH-2646",
        "Summary": "CLONE - Caching of redirected robots.txt may overwrite correct robots.txt rules",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.3.1,                                            1.14",
        "Fix Version/s": "None",
        "Component/s": "fetcher,                                            robots",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Chang Fan",
        "Created": "17/Sep/18 15:59",
        "Updated": "17/Sep/18 16:23",
        "Resolved": "17/Sep/18 16:23",
        "Description": "Redirected robots.txt rules are also cached for the target host. That may cause that the correct robots.txt rules are never fetched. E.g., http://wyomingtheband.com/robots.txt redirects to https://www.facebook.com/wyomingtheband/robots.txt. Because fetching fails with a 404 bots are allowed to crawl wyomingtheband.com. The rules is erroneously also cached for the redirect target host www.facebook.com which is clear regarding their robots.txt rules and does not allow crawling.\nNutch may cache redirected robots.txt rules only if the path part (in doubt, including the query) of the redirect target URL is exactly /robots.txt.",
        "Issue Links": [
            "/jira/browse/NUTCH-2581",
            "/jira/browse/NUTCH-731",
            "https://github.com/apache/nutch/pull/331",
            "https://github.com/apache/nutch/pull/342"
        ]
    },
    "NUTCH-2647": {
        "Key": "NUTCH-2647",
        "Summary": "Skip TLS certificate checks in protocol-http plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Sep/18 10:27",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "28/Sep/18 10:45",
        "Description": "Taken from protocol-httpclient, protocol-http now has support for ignoring certificate checks.",
        "Issue Links": []
    },
    "NUTCH-2648": {
        "Key": "NUTCH-2648",
        "Summary": "Make configurable whether TLS/SSL certificates are checked by protocol plugins",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "08/Oct/18 15:45",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "09/Oct/18 13:03",
        "Description": "(see discussion in NUTCH-2647)\nIt should be possible to enable/disable TLS/SSL certificate validation centrally for all http/https protocol plugins by a single configuration property.\nSome use cases (eg. crawl a site to detect insecure pages) may require that TLS/SSL certificates are checked. Also a broader, unrestricted web crawl may skip sites with invalid certificates as this is can be an indicator for the quality of a site.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/388"
        ]
    },
    "NUTCH-2649": {
        "Key": "NUTCH-2649",
        "Summary": "Optionally skip TLS/SSL certificate validation for protocol-selenium and protocol-htmlunit",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.17",
        "Component/s": "protocol",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Oct/18 08:28",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "23/Jan/20 13:12",
        "Description": "NUTCH-2648 adds a property to enable/disable the TLS/SSL certificate validation for protocol-http, protocol-httpclient and protocol-okhttp. It should be also supported by remaining protocol plugins:\n\nprotocol-selenium,\nprotocol-interactiveselenium and\nprotocol-htmlunit",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/496"
        ]
    },
    "NUTCH-2650": {
        "Key": "NUTCH-2650",
        "Summary": "-addBinaryContent -base64 flags are causing \"String length must be a multiple of four\" error in IndexingJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "asmita",
        "Created": "11/Oct/18 11:43",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "24/May/19 13:39",
        "Description": "I am running Nutch crawl command as follows, in distributed mode:\n\n\r\nruntime/deploy/bin/crawl -i -D solr.server.url=http://my-solr:8983/solr/my-collection -D solr.server.type=cloud -D solr.zookeeper.url=http://my-solr:9983\u00a0 -s /user/my-user/seed\u00a0 /user/my-user/crawl 1\r\n\n\nThe IndexingJob fails with the following error:\n\u00a0\n\n\r\norg.apache.solr.common.SolrException: ERROR: [doc=3b9a9fb7fd92d32287da1b2f3df5f8a1] Error adding field 'binaryContent'='.......==\r\n' msg=String length must be a multiple of four.\r\n\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:208)\r\n\tat org.apache.solr.update.AddUpdateCommand.getLuceneDocument(AddUpdateCommand.java:101)\r\n\tat org.apache.solr.update.DirectUpdateHandler2.updateDocument(DirectUpdateHandler2.java:963)\r\n\tat org.apache.solr.update.DirectUpdateHandler2.updateDocOrDocValues(DirectUpdateHandler2.java:954)\r\n\tat org.apache.solr.update.DirectUpdateHandler2.doNormalUpdate(DirectUpdateHandler2.java:334)\r\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc0(DirectUpdateHandler2.java:271)\r\n\tat org.apache.solr.update.DirectUpdateHandler2.addDoc(DirectUpdateHandler2.java:221)\r\n\tat org.apache.solr.update.processor.RunUpdateProcessor.processAdd(RunUpdateProcessorFactory.java:67)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.doLocalAdd(DistributedUpdateProcessor.java:950)\r\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.versionAdd(DistributedUpdateProcessor.java:1163)\r\n\tat org.apache.solr.update.processor.DistributedUpdateProcessor.processAdd(DistributedUpdateProcessor.java:633)\r\n\tat org.apache.solr.update.processor.LogUpdateProcessorFactory$LogUpdateProcessor.processAdd(LogUpdateProcessorFactory.java:103)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.AddSchemaFieldsUpdateProcessorFactory$AddSchemaFieldsUpdateProcessor.processAdd(AddSchemaFieldsUpdateProcessorFactory.java:475)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.FieldMutatingUpdateProcessor.processAdd(FieldMutatingUpdateProcessor.java:118)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.FieldMutatingUpdateProcessor.processAdd(FieldMutatingUpdateProcessor.java:118)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.FieldMutatingUpdateProcessor.processAdd(FieldMutatingUpdateProcessor.java:118)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.FieldMutatingUpdateProcessor.processAdd(FieldMutatingUpdateProcessor.java:118)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.FieldNameMutatingUpdateProcessorFactory$1.processAdd(FieldNameMutatingUpdateProcessorFactory.java:75)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.FieldMutatingUpdateProcessor.processAdd(FieldMutatingUpdateProcessor.java:118)\r\n\tat org.apache.solr.update.processor.UpdateRequestProcessor.processAdd(UpdateRequestProcessor.java:55)\r\n\tat org.apache.solr.update.processor.AbstractDefaultValueUpdateProcessorFactory$DefaultValueUpdateProcessor.processAdd(AbstractDefaultValueUpdateProcessorFactory.java:92)\r\n\tat org.apache.solr.handler.loader.JavabinLoader$1.update(JavabinLoader.java:98)\r\n\tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readOuterMostDocIterator(JavaBinUpdateRequestCodec.java:188)\r\n\tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readIterator(JavaBinUpdateRequestCodec.java:144)\r\n\tat org.apache.solr.common.util.JavaBinCodec.readObject(JavaBinCodec.java:311)\r\n\tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:256)\r\n\tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec$1.readNamedList(JavaBinUpdateRequestCodec.java:130)\r\n\tat org.apache.solr.common.util.JavaBinCodec.readObject(JavaBinCodec.java:276)\r\n\tat org.apache.solr.common.util.JavaBinCodec.readVal(JavaBinCodec.java:256)\r\n\tat org.apache.solr.common.util.JavaBinCodec.unmarshal(JavaBinCodec.java:178)\r\n\tat org.apache.solr.client.solrj.request.JavaBinUpdateRequestCodec.unmarshal(JavaBinUpdateRequestCodec.java:195)\r\n\tat org.apache.solr.handler.loader.JavabinLoader.parseAndLoadDocs(JavabinLoader.java:109)\r\n\tat org.apache.solr.handler.loader.JavabinLoader.load(JavabinLoader.java:55)\r\n\tat org.apache.solr.handler.UpdateRequestHandler$1.load(UpdateRequestHandler.java:97)\r\n\tat org.apache.solr.handler.ContentStreamHandlerBase.handleRequestBody(ContentStreamHandlerBase.java:68)\r\n\tat org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:195)\r\n\tat org.apache.solr.core.SolrCore.execute(SolrCore.java:2503)\r\n\tat org.apache.solr.servlet.HttpSolrCall.execute(HttpSolrCall.java:711)\r\n\tat org.apache.solr.servlet.HttpSolrCall.call(HttpSolrCall.java:517)\r\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:384)\r\n\tat org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:330)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1637)\r\n\tat org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:311)\r\n\tat org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:265)\r\n\tat org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1629)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:533)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143)\r\n\tat org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:548)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:190)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:1595)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:188)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1253)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:168)\r\n\tat org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:473)\r\n\tat org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:1564)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:166)\r\n\tat org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1155)\r\n\tat org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)\r\n\tat org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:219)\r\n\tat org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:126)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\r\n\tat org.eclipse.jetty.rewrite.handler.RewriteHandler.handle(RewriteHandler.java:335)\r\n\tat org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132)\r\n\tat org.eclipse.jetty.server.Server.handle(Server.java:530)\r\n\tat org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:347)\r\n\tat org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:256)\r\n\tat org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:279)\r\n\tat org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:102)\r\n\tat org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:124)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.doProduce(EatWhatYouKill.java:247)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.produce(EatWhatYouKill.java:140)\r\n\tat org.eclipse.jetty.util.thread.strategy.EatWhatYouKill.run(EatWhatYouKill.java:131)\r\n\tat org.eclipse.jetty.util.thread.ReservedThreadExecutor$ReservedThread.run(ReservedThreadExecutor.java:382)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:708)\r\n\tat org.eclipse.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:626)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.lang.IllegalArgumentException: String length must be a multiple of four.\r\n\tat org.apache.solr.common.util.Base64.base64ToByteArray(Base64.java:102)\r\n\tat org.apache.solr.schema.BinaryField.createField(BinaryField.java:101)\r\n\tat org.apache.solr.schema.FieldType.createFields(FieldType.java:317)\r\n\tat org.apache.solr.update.DocumentBuilder.addField(DocumentBuilder.java:66)\r\n\tat org.apache.solr.update.DocumentBuilder.toDocument(DocumentBuilder.java:159)\r\n\t... 84 more\r\n\n\n\u00a0Solr version: 7.3.1\nIndexing command from logs:\n\n\r\nruntime/deploy/bin/nutch index -Dsolr.zookeeper.url=http://my-solr:9983 -Dsolr.server.type=cloud -Dsolr.server.url=http://my-solr:8983/solr/my-collection /user/my-user/crawl/crawldb -linkdb /user/my-user/crawl/linkdb /user/my-user/crawl/segments/20181011040457 -addBinaryContent -base64\r\n\n\n\u00a0\n(removed huge binary content from the log)",
        "Issue Links": [
            "/jira/browse/NUTCH-2706"
        ]
    },
    "NUTCH-2651": {
        "Key": "NUTCH-2651",
        "Summary": "Upgrade to Tika 1.19.1 (from 1.18)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "parser,                                            protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "12/Oct/18 11:46",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "20/Oct/18 17:31",
        "Description": "Tika 1.19.1 has been released recently. Among all the other improvements and fixes (including those of 1.19) It contains one important performance fix (TIKA-2645, cf. NUTCH-2578) affecting the MIME-/Content-Type detector.",
        "Issue Links": [
            "/jira/browse/NUTCH-2665",
            "https://github.com/apache/nutch/pull/391"
        ]
    },
    "NUTCH-2652": {
        "Key": "NUTCH-2652",
        "Summary": "Fetcher launches more fetch tasks than fetch lists",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/18 10:27",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "20/Oct/18 17:40",
        "Description": "Fetcher may launch more fetcher tasks than there are fetch lists:\n\n18/10/15 07:27:26 INFO input.FileInputFormat: Total input paths to process : 128\r\n18/10/15 07:27:26 INFO mapreduce.JobSubmitter: number of splits:187\r\n\n\nThat's one design principle of Nutch as a MapRecude-based crawler: to ensure politeness and a guaranteed delay between requests to the same host/domain/ip all items of one host/domain/ip are put by Generator into the same fetch list. A fetch list may not be split because that would violate the politeness constraints - multiple fetcher tasks processing the splits of one fetch list then may send requests to the same host/domain/ip in parallel. See ab's chapter about Nutch in Hadoop the definitive guide (3rd edition).",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/394"
        ]
    },
    "NUTCH-2653": {
        "Key": "NUTCH-2653",
        "Summary": "ProtocolFactory.getProtocol(url) creates separate plugin instances for http/https",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher,                                            protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/18 12:16",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "18/Jan/19 15:16",
        "Description": "Fetcher creates two instances of the protocol-okhttp plugin, one to handle http requests, another for https. The plugin properties are logged during plugin instantiation when calling setConf(...):\n\n2018-10-11 13:28:34,417 INFO [FetcherThread] org.apache.nutch.fetcher.FetcherThread: FetcherThread 40 fetching http://...\r\n...\r\n2018-10-11 13:28:35,099 INFO [FetcherThread] org.apache.nutch.protocol.okhttp.OkHttp: http.proxy.host = null\r\n2018-10-11 13:28:35,100 INFO [FetcherThread] org.apache.nutch.protocol.okhttp.OkHttp: http.proxy.port = 8080\r\n...\r\n2018-10-11 13:28:36,864 INFO [FetcherThread] org.apache.nutch.fetcher.FetcherThread: FetcherThread 87 fetching https://...\r\n...\r\n2018-10-11 13:28:36,864 INFO [FetcherThread] org.apache.nutch.protocol.okhttp.OkHttp: http.proxy.host = null\r\n2018-10-11 13:28:36,864 INFO [FetcherThread] org.apache.nutch.protocol.okhttp.OkHttp: http.proxy.port = 8080\r\n\n\nThe question is whether this is the correct behavior for plugins supporting multiple protocols (http and https)? It may cause that connection pooling and other network optimizations do not work as expected. Of course, it's correct if different plugins are required, e.g., for ftp or the local file system.\n(seen while reviewing the behavior of fetcher with fix for NUTCH-2625 applied)",
        "Issue Links": [
            "/jira/browse/NUTCH-2678"
        ]
    },
    "NUTCH-2654": {
        "Key": "NUTCH-2654",
        "Summary": "Remove obsolete index-writer configuration in conf/",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/18 12:49",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "19/Sep/19 13:12",
        "Description": "The configuration folder conf/ still contains stuff obsolete after NUTCH-1480:\n\nproperties to configure indexer plugins in nutch-default.xml\nsolrindex-mapping.xml (looks like obsolete)\n(still read) elasticsearch.conf\n\nAll obsolete files and properties should be removed.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/468"
        ]
    },
    "NUTCH-2655": {
        "Key": "NUTCH-2655",
        "Summary": "Update Solr schema.xml for Solr 7.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/18 12:57",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "14/Nov/18 09:12",
        "Description": "The Solr schema.xml is not compatible with Solr 7.x which is used by Nutch 1.15. I've tested Solr 7.3.1 and 7.5.0: when using the current schema.xml, Solr fails and complains about unknown field types:\n\n2018-10-15 12:55:24.484 ERROR (qtp102617125-17) [ x:nutch] o.a.s.h.RequestHandlerBase org.apache.solr.common.SolrException: Error CREATEing SolrCore 'nutch': Unable to create core [nutch] Caused by: fieldType 'pdates' not found in the schema",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/395"
        ]
    },
    "NUTCH-2656": {
        "Key": "NUTCH-2656",
        "Summary": "Update description to configure Solr 7.x in tutorial",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/18 13:27",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/Jan/19 20:45",
        "Description": "(reported byTimeka Cobb, see discussion on the user mailing list)\nThe description in the tutorial how to setup Solr 6 and 7 needs to be updated.",
        "Issue Links": []
    },
    "NUTCH-2657": {
        "Key": "NUTCH-2657",
        "Summary": "Protocol-http to store HTTP response header with \"\\r\\n\"",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/18 14:27",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "06/Jan/19 11:52",
        "Description": "The plugins protocol-http and protocol-okhttp allow to store the HTTP request and/or response headers in the response metadata. However, there is no consensus which line breaks (\"\\r\\n\" or \"\\n\") are used between header lines and whether there is a trailing second line break at the end of the headers: while request headers are stored by both plugins with \"\\r\\n\" and two trailing \"\\r\\n\",  the response headers are stored by protocol-http with \"\\n\" and a single trailing line break. This is difficult to handle if the headers are required to be stored uniformly (I've created such a nasty bug writing WARC files).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/422"
        ]
    },
    "NUTCH-2658": {
        "Key": "NUTCH-2658",
        "Summary": "Add README file to all plugins in src/plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "documentation,                                            plugin",
        "Assignee": null,
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "17/Oct/18 08:43",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/Jan/19 11:20",
        "Description": "Since we've migrated a good portion of our workflow to Github we could consider adding a README.md file to the root of each plugin in src/plugins. \nThis is a good place to have plugin-specific documentation. Wich fields the plugin adds to the indexer, which configuration options, etc. Also, since the README.md is rendered by Github automatically is a good link to point users.\nI think that a good example is the indexer-cloudsearch plugin, on top of that it's a good source of information to point users when asking questions regarding a specific plugin.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/398"
        ]
    },
    "NUTCH-2659": {
        "Key": "NUTCH-2659",
        "Summary": "Add missing Apache license headers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Oct/18 12:23",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "07/Dec/18 16:50",
        "Description": "Should add Apache license headers to source files (at least, *.java) - some files lack the license header.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/396"
        ]
    },
    "NUTCH-2660": {
        "Key": "NUTCH-2660",
        "Summary": "Unit tests of plugins parse-js, headings, index-jexl-filter to be executed during build",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "build,                                            test",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Oct/18 12:36",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "20/Oct/18 17:17",
        "Description": "The unit tests of the plugins \"parse-js\", \"headings\" and \"index-jexl-filter\" are not executed during build.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/397"
        ]
    },
    "NUTCH-2661": {
        "Key": "NUTCH-2661",
        "Summary": "Move TestOutlinks to the proper path",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "17/Oct/18 16:06",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "15/Nov/18 10:42",
        "Description": "Initially, I placed the TestOutlinks class in the index-links plugin, although this was when I found the bug with the hashCode. Now I realised that this test is best to have in the test/org/apache/nutch/nutch/parse directory. \nEven more because since this test is not covering any plugin-specific code.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/399"
        ]
    },
    "NUTCH-2662": {
        "Key": "NUTCH-2662",
        "Summary": "index-jexl-filter plugin throws a RuntimeException if its enabled but not configured",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.20",
        "Component/s": "plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "18/Oct/18 11:22",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "If the index-jexl-filter plugin is enabled but no configuration is provided in the index.jexl.filter property the plugin throws a RuntimeException. In the same exception message, we advise to either set true or false to index all/none. \nThis is a case where we can just select a sane default and log a warning, but not stop the entire process. I think this is more consistent with how we approach configuration in general: Only fail if there is an actual error in the configuration (i.e parse error on the expression).",
        "Issue Links": []
    },
    "NUTCH-2663": {
        "Key": "NUTCH-2663",
        "Summary": "Improve index-jexl-filter syntax for scripts",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.16",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Jorge Luis Betancourt Gonzalez",
        "Reporter": "Jorge Luis Betancourt Gonzalez",
        "Created": "18/Oct/18 13:12",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "18/Jan/19 15:24",
        "Description": "JEXL scripts need to be written using the array syntax to get the actual value (for instance, example extracted from the tests):\n\n\r\ndoc.lang[0]=='en'\r\n\n\nIdeally, this would only be required if the actual value is really an array, and not for single value elements.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/400"
        ]
    },
    "NUTCH-2664": {
        "Key": "NUTCH-2664",
        "Summary": "WebApp for Nutch running in deploy Mode Creates Seed Directory in local FileSystem",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "REST_api,                                            web gui",
        "Assignee": null,
        "Reporter": "Gajanan Watkar",
        "Created": "20/Oct/18 12:22",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "When creating crawl jobs using nutch webapp, seed directory gets created in temp (/tmp on linux) directory in local filesystem. This prevents crawl job to inject urls. As injection of url fails, no further phases of crawl can be executed. Seed Directory needs to be created on HDFS in case of Nutch running in deploy mode.",
        "Issue Links": []
    },
    "NUTCH-2665": {
        "Key": "NUTCH-2665",
        "Summary": "Upgrade to Apache Tika 1.19.1",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "23/Oct/18 11:19",
        "Updated": "28/Dec/18 11:45",
        "Resolved": "27/Dec/18 16:11",
        "Description": "Borrowing from wastl-nagel's efforts on NUTCH-2651, 2.x can be upgraded to Apache Tika 1.19.1 as well.",
        "Issue Links": [
            "/jira/browse/NUTCH-2667",
            "/jira/browse/NUTCH-2667",
            "/jira/browse/NUTCH-2651"
        ]
    },
    "NUTCH-2666": {
        "Key": "NUTCH-2666",
        "Summary": "Increase default value for http.content.limit / ftp.content.limit / file.content.limit",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Marco Ebbinghaus",
        "Created": "23/Oct/18 14:27",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "10/Apr/19 11:40",
        "Description": "The default value for\u00a0http.content.limit in nutch-default.xml (The length limit for downloaded content using the http://\n protocol, in bytes. If this value is nonnegative (>=0), content longer\n than it will be truncated; otherwise, no truncation at all. Do not\n confuse this setting with the file.content.limit setting.) is set to 64kb. Maybe this default value should be increased as many pages today are greater than 64kb.\nThis fact hit me when trying to crawl a single website whose pages are much greater than 64kb and because of that with every crawl cycle the count of db_unfetched urls decreased until it hit zero and the crawler became inactive (because the first 64 kB contained always the same set of navigation links)\nThe description might also be updated as this is not only the case for the http protocol, but also for https.",
        "Issue Links": [
            "/jira/browse/NUTCH-2511",
            "https://github.com/apache/nutch/pull/427"
        ]
    },
    "NUTCH-2667": {
        "Key": "NUTCH-2667",
        "Summary": "Update Tika and Commons Collections 4",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4",
        "Fix Version/s": "2.4",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "24/Oct/18 05:17",
        "Updated": "04/Jan/19 15:51",
        "Resolved": "04/Jan/19 15:51",
        "Description": "Tika and Commons Collections 4 need to be updated. This issue needs to address them.",
        "Issue Links": [
            "/jira/browse/NUTCH-2665",
            "/jira/browse/NUTCH-2665",
            "https://github.com/apache/nutch/pull/403",
            "https://github.com/apache/nutch/pull/423"
        ]
    },
    "NUTCH-2668": {
        "Key": "NUTCH-2668",
        "Summary": "Integrate OWASP dependency checks as ant target",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4,                                            1.16",
        "Fix Version/s": "2.4,                                            1.16",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "24/Oct/18 08:42",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "07/Dec/18 17:01",
        "Description": "OWASP provides the ant tool \"dependency-check\" which lists potential vulnerabilities of library dependencies. We should integrate the generation of vulnerability reports into our build system as an optional task/target recommended to be run from time to time and especially shortly before releases are prepared.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/401",
            "https://github.com/apache/nutch/pull/404",
            "https://github.com/apache/nutch/pull/401"
        ]
    },
    "NUTCH-2669": {
        "Key": "NUTCH-2669",
        "Summary": "Reliable solution for javax.ws packaging.type",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4,                                            1.16",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "24/Oct/18 11:32",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "17/Aug/20 13:49",
        "Description": "The upgrade of Tika to v1.19.1 (NUTCH-2651, NUTCH-2665, NUTCH-2667) raises an ant/ivy issue during build when resolving/fetching dependencies:\n\n[ivy:resolve] \t\t[FAILED     ] javax.ws.rs#javax.ws.rs-api;2.1!javax.ws.rs-api.${packaging.type}:  (0ms)\r\n[ivy:resolve] \t==== local: tried\r\n[ivy:resolve] \t  /home/jenkins/.ivy2/local/javax.ws.rs/javax.ws.rs-api/2.1/${packaging.type}s/javax.ws.rs-api.${packaging.type}\r\n[ivy:resolve] \t==== maven2: tried\r\n[ivy:resolve] \t  http://repo1.maven.org/maven2/javax/ws/rs/javax.ws.rs-api/2.1/javax.ws.rs-api-2.1.${packaging.type}\r\n[ivy:resolve] \t==== apache-snapshot: tried\r\n[ivy:resolve] \t  https://repository.apache.org/content/repositories/snapshots/javax/ws/rs/javax.ws.rs-api/2.1/javax.ws.rs-api-2.1.${packaging.type}\r\n[ivy:resolve] \t==== sonatype: tried\r\n[ivy:resolve] \t  http://oss.sonatype.org/content/repositories/releases/javax/ws/rs/javax.ws.rs-api/2.1/javax.ws.rs-api-2.1.${packaging.type}\r\n[ivy:resolve] \t\t::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] \t\t::              FAILED DOWNLOADS            ::\r\n[ivy:resolve] \t\t:: ^ see resolution messages for details  ^ ::\r\n[ivy:resolve] \t\t::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] \t\t:: javax.ws.rs#javax.ws.rs-api;2.1!javax.ws.rs-api.${packaging.type}\r\n[ivy:resolve] \t\t::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] :::: ERRORS\r\n...\r\nBUILD FAILED\r\n\n\nMore information about this issue is linked on jax-rs#576. \nA work-around is to define a property packaging.type and set it to jar. This can be done\n\nin command-line ant -Dpackaging.type=jar ...\nin default.properties\nin ivysettings.xml\nThe last work-around is active in current master/1.x. However, there are still Jenkins builds failing while few succeed:\n\n\n\n#build\nstatus jax-rs\nmachine\nwork-around\n\n\n3578\nsuccess\nH28\nivysettings.xml\n\n\n3577\nfailed\nH28\nivysettings.xml\n\n\n3576\nfailed\nH33\nivysettings.xml\n\n\n3575\nsuccess\nubuntu-4\nivysettings.xml\n\n\n3574\nfailed\nubuntu-4\n-Dpackaging.type=jar + default.properties\n\n\n3571\nfailed\n?\n-Dpackaging.type=jar + default.properties\n\n\n3568\nfailed\n?\n-Dpackaging.type=jar + default.properties\n\n\n\n\n\nBuilds which failed for other reasons are left away. The only pattern I see is that only the second build on every of the Jenkins machines succeeds. A possible reason could be that the build environments on the machines persist state (the Nutch build directory, local ivy cache, etc.). If this is the case, it may take some time until all Jenkins machines will succeed.\nThe ivysettings.xml work-around was the first which succeeded on a Jenkins build but it may be the case that all three work-arounds apply.\nThe issue is supposed to be resolved (without work-arounds) by IVY-1577. However, it looks like it isn't:\n\nget rc2 of ivy 2.5.0 (the URL may change):\n\n% wget -O ivy/ivy-2.5.0-rc2-test.jar \\\r\n    https://builds.apache.org/job/Ivy/lastSuccessfulBuild/artifact/build/artifact/org.apache.ivy_2.5.0.cr2_20181023065327.jar\r\n\n\nedit default properties and set ivy.version=2.5.0-rc2-test\nremove work-around in ivysettings.xml (or default.properties)\nrun ant clean runtime and check for failure resp. whether javax.ws lib is in place: ls build/lib/javax.ws.rs-api*.jar\nThis solution fails for ivy-2.5.0-rc1.jar and the mentioned rc2 jar as of 2018-10-23. But maybe the procedure is wrong, I'll contact the ant/ivy team to solve this.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/467"
        ]
    },
    "NUTCH-2670": {
        "Key": "NUTCH-2670",
        "Summary": "org.apache.nutch.indexer.IndexerMapReduce does not read the value of \"indexer.delete\" from nutch-site.xml",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.14,                                            1.15",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Junqiang Zhang",
        "Created": "29/Oct/18 11:02",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "07/Jan/19 10:11",
        "Description": "Inside org.apache.nutch.indexer.IndexerMapReduce.IndexerReducer, the setup() function should read the value of \"indexer.delete\" from nutch-site.xml, and assign the value to the variable of \"delete\". See the following line of code.\n(line 201)      delete = conf.getBoolean(INDEXER_DELETE, false);\nHowever, the value of \"indexer.delete\" set in nutch-site.xml and nutch-default.xml is not assigned to the variable, \"delete\". I put the following setting in one of nutch-site.xml and nutch-default.xml, or in both of them. The variable of \"delete\" remains false.\n<property>\n  <name>indexer.delete</name>\n  <value>true</value>\n  <description>Whether the indexer will delete documents GONE or REDIRECTS by indexing filters\n  </description>\n</property>\nI also changed the line of code to\ndelete = conf.getBoolean(INDEXER_DELETE, true);\nWhatever value of \"indexer.delete\" is set in nutch-site.xml or nutch-default.xml, the value of \"delete\" remains false.",
        "Issue Links": []
    },
    "NUTCH-2671": {
        "Key": "NUTCH-2669 Reliable solution for javax.ws packaging.type",
        "Summary": "Upgrade ant ivy library",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4,                                            1.16",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "29/Oct/18 12:36",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "17/Aug/20 13:48",
        "Description": "Upgrade the ant ivy library to latest release (2.5.0-rc1) to address NUTCH-2669.",
        "Issue Links": [
            "/jira/browse/NUTCH-2672",
            "/jira/browse/NUTCH-2697",
            "https://github.com/apache/nutch/pull/405",
            "https://github.com/apache/nutch/pull/406"
        ]
    },
    "NUTCH-2672": {
        "Key": "NUTCH-2669 Reliable solution for javax.ws packaging.type",
        "Summary": "Ant build erronously installs *-test.jar instead *.jar for target \"nightly\"",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "30/Oct/18 14:42",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "17/Aug/20 13:51",
        "Description": "The Jenkins build (1.x/trunk only) failed because instead of the \"normal\" jars the *-test.jar versions are installed in build/lib/ which causes a compiler error (\"cannot find symbol\"):\n\nresolve-test:\r\n[ivy:resolve] :: Apache Ivy 2.5.0-rc1 - 20180412005306 :: http://ant.apache.org/ivy/ ::\r\n[ivy:resolve] :: loading settings :: file = /home/jenkins/jenkins-slave/workspace/Nutch-trunk/ivy/ivysettings.xml\r\n...\r\n\r\nresolve-default:\r\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-auth/2.7.4/hadoop-auth-2.7.4-tests.jar ...\r\n[ivy:resolve] ...... (99kB)\r\n[ivy:resolve] .. (0kB)\r\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-auth;2.7.4!hadoop-auth.jar(test-jar) (181ms)\r\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.6/zookeeper-3.4.6-tests.jar ...\r\n[ivy:resolve] ........................................................................ (526kB)\r\n[ivy:resolve] .. (0kB)\r\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.zookeeper#zookeeper;3.4.6!zookeeper.jar(test-jar) (35ms)\r\n[ivy:resolve] downloading http://repo1.maven.org/maven2/org/apache/hadoop/hadoop-yarn-common/2.7.4/hadoop-yarn-common-2.7.4-tests.jar ...\r\n[ivy:resolve] ........................... (258kB)\r\n[ivy:resolve] .. (0kB)\r\n[ivy:resolve] \t[SUCCESSFUL ] org.apache.hadoop#hadoop-yarn-common;2.7.4!hadoop-yarn-common.jar(test-jar) (181ms)\r\n[ivy:resolve] downloading http://repo1.maven.org/maven2/com/j256/ormlite/ormlite-core/5.1/ormlite-core-5.1-tests.jar ...\r\n[ivy:resolve] ...................................................................................... (643kB)\r\n[ivy:resolve] .. (0kB)\r\n[ivy:resolve] \t[SUCCESSFUL ] com.j256.ormlite#ormlite-core;5.1!ormlite-core.jar (321ms)\r\n  [taskdef] Could not load definitions from resource org/sonar/ant/antlib.xml. It could not be found.\r\n\r\ncopy-libs:\r\n\r\ncompile-core:\r\n    [javac] Compiling 298 source files to /home/jenkins/jenkins-slave/workspace/Nutch-trunk/build/classes\r\n    [javac] /home/jenkins/jenkins-slave/workspace/Nutch-trunk/src/java/org/apache/nutch/webui/model/SeedList.java:32: error: cannot find symbol\r\n    [javac] import com.j256.ormlite.field.ForeignCollectionField;\r\n    [javac]                              ^\r\n    [javac]   symbol:   class ForeignCollectionField\r\n    [javac]   location: package com.j256.ormlite.field\r\n    [javac] /home/jenkins/jenkins-slave/workspace/Nutch-trunk/src/java/org/apache/nutch/webui/model/SeedUrl.java:29: error: cannot find symbol\r\n    [javac] import com.j256.ormlite.field.DatabaseField;\r\n    [javac]                              ^\r\n\n\nThe problem is reproducible by:\n\ncleaning the local ivy cached\n\nrm -rf ~/.ivy2/cache/com.j256.ormlite/\r\n\n\nand running ant clean nightly or ant clean compile-core-test\n\nIt's not reproducible\n\nwhen rolling back NUTCH-2671 and using ivy 2.4.0\nwhen running ant clean resolve-default or ant clean resolve-test",
        "Issue Links": [
            "/jira/browse/NUTCH-2671",
            "/jira/browse/NUTCH-2697",
            "/jira/browse/IVY-1586"
        ]
    },
    "NUTCH-2673": {
        "Key": "NUTCH-2673",
        "Summary": "EOFException protocol-http",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "07/Nov/18 11:25",
        "Updated": "12/Aug/20 23:02",
        "Resolved": "07/Jan/19 11:28",
        "Description": "Got an EOFException for some URL:\n\n\r\n2018-11-07 12:23:18,463 INFO  indexer.IndexingFiltersChecker - fetching: https://www.misdaadjournalist.nl/2018/11/politie-kraakt-server-van-blackbox-265-000-criminele-berichten-onderschept/\r\n2018-11-07 12:23:18,704 INFO  protocol.RobotRulesParser - robots.txt whitelist not configured.\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.proxy.host = null\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.proxy.port = 8080\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.proxy.exception.list = false\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.timeout = 30000\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.content.limit = 32554432\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.agent = Mozilla/5.0 (compatible; OpenindexSpider; +https://www.openindex.io/saas/about-our-spider/)\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.accept.language = en-us,en-gb,en;q=0.7,*;q=0.3\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.accept = text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\r\n2018-11-07 12:23:18,704 INFO  http.Http - http.enable.cookie.header = false\r\n2018-11-07 12:23:18,911 ERROR http.Http - Failed to get protocol output\r\njava.io.EOFException\r\n        at org.apache.nutch.protocol.http.HttpResponse.readLine(HttpResponse.java:591)\r\n        at org.apache.nutch.protocol.http.HttpResponse.parseStatusLine(HttpResponse.java:482)\r\n        at org.apache.nutch.protocol.http.HttpResponse.<init>(HttpResponse.java:249)\r\n        at org.apache.nutch.protocol.http.Http.getResponse(Http.java:72)\r\n        at org.apache.nutch.protocol.http.api.HttpBase.getProtocolOutput(HttpBase.java:276)\r\n        at org.apache.nutch.indexer.IndexingFiltersChecker.getProtocolOutput(IndexingFiltersChecker.java:270)\r\n        at org.apache.nutch.indexer.IndexingFiltersChecker.process(IndexingFiltersChecker.java:141)\r\n        at org.apache.nutch.util.AbstractChecker.processSingle(AbstractChecker.java:86)\r\n        at org.apache.nutch.indexer.IndexingFiltersChecker.run(IndexingFiltersChecker.java:111)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)\r\n        at org.apache.nutch.indexer.IndexingFiltersChecker.main(IndexingFiltersChecker.java:275)",
        "Issue Links": []
    },
    "NUTCH-2674": {
        "Key": "NUTCH-2674",
        "Summary": "HostDb: dump shows wrong column headers",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "hostdb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "08/Nov/18 15:12",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "07/Dec/18 17:00",
        "Description": "The header line of the HostDb dump is wrong, it should show the fields returned by HostDatum.toString() used for the dump. In detail, the column headers redirSum and ok should be replaced by notModified.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/407"
        ]
    },
    "NUTCH-2675": {
        "Key": "NUTCH-2675",
        "Summary": "Give parsers the capability to read and write CrawlDatum",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Won't Do",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Junqiang Zhang",
        "Created": "09/Nov/18 16:50",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "06/Jan/19 20:43",
        "Description": "Parsers are called inside org.apache.nutch.parse.ParseSegment,\n(Line 127 for version 1.15)        parseResult = parseUtil.parse(content);\nand inside org.apache.nutch.fetcher.FetcherThread.\n(Line 640 for version 1.15)            parseResult = this.parseUtil.parse(content);\nThe current version of Nutch does not give parsers the capability to access CrawlDatum. If users want to customize the parsing process using some metadata of CrawlDatum, it is difficult to read the required metadata. \nOn the other side, if users want to save metadata generated during parsing, the metadata can only be saved as parseMeta of org.apache.nutch.parse.ParseData, and those of parseMeta selected by db.parsemeta.to.crawldb in nutch-site.xml can be added to CrawlDatum inside org.apache.nutch.parse.ParseOutputFormat and org.apache.nutch.crawl.CrawlDbReducer. If parsers have direct access to CrawlDatum, the metadata generated during parsing can be added to CrawlDatum directly by parsers.\nI use Nutch to fetch and parse web pages. To read required metadata from CrawlDatum during parsing, I do the following steps to work around.\n(1) During web page fetching, inside org.apache.nutch.protocol.http.api.HttpBase of lib-http plugin, read the required metadata from CrawlDatum, and save the required metadata together with the Headers metadata of org.apache.nutch.net.protocols.Response to the metadata of org.apache.nutch.protocol.Content. This can be done at line 334 of the code by replacing \"response.getHeaders()\" by a new metadata containing both the required metadata from CrawlDatum and the Headers metadata.\nThe code need to be modified inside org.apache.nutch.protocol.http.api.HttpBase of lib-http plugin is\n(Line 332 for version 1.15)      Content c = new Content(u.toString(), u.toString(),\n(Line 333 for version 1.15)           (content == null ? EMPTY_CONTENT : content),\n(Line 334 for version 1.15)           response.getHeader(\"Content-Type\"), response.getHeaders(), mimeTypes);\n(2) During html page parsing, inside org.apache.nutch.parse.html.HtmlParser of parse-html plugin, read the required metadata from the metadata of org.apache.nutch.protocol.Content, and customize the parsing process using the required metadata.\nIf parsers have direct access to CrawlDatum, the above workaround is not needed. To give parsers the capacity to directly read and write CrawlDatum, I would like to suggest adding a new method \"public ParseResult parse(Content content, CrawlDatum datum)\" to org.apache.nutch.parse.ParseUtil in future versions of Nutch.\nTo be compatible with current 1.15 and previous versions, I would like to suggest adding a new configuration property to nutch-default.xml. The default of the configuration property can be use the current method \"public ParseResult parse(Content content)\". If users want to use \"public ParseResult parse(Content content, CrawlDatum datum)\", they can change the property in nutch-site.xml.",
        "Issue Links": []
    },
    "NUTCH-2676": {
        "Key": "NUTCH-2676",
        "Summary": "Update to the latest selenium and add code to use chrome and firefox headless mode with the remote web driver",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Stas Batururimi",
        "Created": "15/Nov/18 07:47",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "23/Feb/19 23:10",
        "Description": "Selenium needs to be updated\nmissing remote web driver for chrome\u00a0\nnecessity to add headless mode for both remote WebDriverBase Firefox & Chrome\nuse case with Selenium grid using\u00a0docker (1 hub docker container, several nodes in different docker containers, Nutch in another docker container, streaming to Apache Solr in docker container, that is at least 4 different docker containers)",
        "Issue Links": [
            "/jira/browse/NUTCH-2460"
        ]
    },
    "NUTCH-2677": {
        "Key": "NUTCH-2677",
        "Summary": "Update Jest client in indexer-elastic-rest plugin",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Nov/18 22:30",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "21/Apr/20 09:31",
        "Description": "We should really upgrade the dependency to a more recent version\nhttps://github.com/apache/nutch/blob/master/src/plugin/indexer-elastic-rest/ivy.xml\nWe are using 2.0.1, the most recent is 6.3.1\nhttps://search.maven.org/artifact/io.searchbox/jest/6.3.1/jar",
        "Issue Links": [
            "/jira/browse/NUTCH-2739",
            "/jira/browse/NUTCH-2755"
        ]
    },
    "NUTCH-2678": {
        "Key": "NUTCH-2678",
        "Summary": "Allow for per-host configurable protocol plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "10/Dec/18 14:01",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "18/Jan/19 12:28",
        "Description": "Introduces new configuration file for mapping protocol plugins to hostnames.\n\n\r\n# This file defines a hostname to protocol plugin mapping. Each line takes a\r\n# host name followed by a tab, followed by the ID of the protocol plugin. You\r\n# can find the ID in the protocol plugin's plugin.xml file.\r\n# \r\n# <hostname>\\t<plugin_id>\\n\r\n# nutch.apache.org      org.apache.nutch.protocol.httpclient.Http\r\n# tika.apache.org       org.apache.nutch.protocol.http.Http\r\n#",
        "Issue Links": [
            "/jira/browse/NUTCH-2653",
            "/jira/browse/NUTCH-2126",
            "https://github.com/apache/nutch/pull/431"
        ]
    },
    "NUTCH-2679": {
        "Key": "NUTCH-2679",
        "Summary": "\"ant eclipse\" failed as eclipse binary is moved",
        "Type": "Test",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "dhirajforyou",
        "Created": "12/Dec/18 10:40",
        "Updated": "12/Dec/18 16:59",
        "Resolved": "12/Dec/18 16:59",
        "Description": "curl -I \"https://downloads.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2\"\nHTTP/1.1 302 Found\nServer: nginx/1.13.12\nDate: Wed, 12 Dec 2018 10:32:28 GMT\nContent-Type: text/html; charset=UTF-8\nConnection: keep-alive\nContent-Disposition: attachment; filename=\"ant-eclipse-1.0.bin.tar.bz2\"\nSet-Cookie: sf_mirror_attempt=\"ant-eclipse:liquidtelecom:ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2\"; Max-Age=120; Path=/; expires=Wed, 12-Dec-2018 10:34:28 GMT\nLocation: https://liquidtelecom.dl.sourceforge.net/project/ant-eclipse/ant-eclipse/1.0/ant-eclipse-1.0.bin.tar.bz2\n\u00a0\nso the eclipse binary src need to be changed.\n\u00a0\n@\u00a0wastl-nagel\u00a0@\u00a0lewismc\nlast time we changed http to https and this time url got changed.\ncan you suggest the best to overcome this.",
        "Issue Links": []
    },
    "NUTCH-2680": {
        "Key": "NUTCH-2680",
        "Summary": "Documentation: https supported by multiple protocol plugins not only httpclient",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "documentation,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Dec/18 09:24",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "18/Jan/19 15:26",
        "Description": "nutch-default.xml still states:\nIn order to use HTTPS please enable protocol-httpclient, but be aware of possible intermittent problems with the underlying commons-httpclient library.\nNow https is supported by most protocol plugins and there is no need to activate protocol-httpclient to fetch https:// pages.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/426"
        ]
    },
    "NUTCH-2681": {
        "Key": "NUTCH-2681",
        "Summary": "ClassCastException - Apache Nutch 1.x, Selenium v2.48.2, firefox 31.4.0",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Venkata Madhusudhana Rao",
        "Created": "21/Dec/18 15:03",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "23/Apr/20 11:01",
        "Description": "Fetching of Ajax content using protocol-selenium, with the specified selenium and firefox versions, while executing bin/nutch fetch, below ClassCastException thrown\nCaused by: org.openqa.selenium.WebDriverException: java.lang.ClassCastException: org.apache.xerces.jaxp.DocumentBuilderFactoryImpl cannot be cast to javax.xml.parsers.DocumentBuilderFactory\nBuild info: version: '2.48.2', revision: '41bccdd10cf2c0560f637404c2d96164b67d9d67', time: '2015-10-09 13:08:06'\nSystem info: host: '24labs', ip: '10.0.10.24', os.name: 'Linux', os.arch: 'amd64', os.version: '3.10.0-327.13.1.el7.x86_64', java.version: '1.8.0_191'\nDriver info: driver.version: FirefoxDriver\n\u00a0at org.openqa.selenium.firefox.internal.FileExtension.readIdFromInstallRdf(FileExtension.java:142)\n\u00a0at org.openqa.selenium.firefox.internal.FileExtension.writeTo(FileExtension.java:61)\n\u00a0at org.openqa.selenium.firefox.internal.ClasspathExtension.writeTo(ClasspathExtension.java:64)\n\u00a0at org.openqa.selenium.firefox.FirefoxProfile.installExtensions(FirefoxProfile.java:443)\n\u00a0at org.openqa.selenium.firefox.FirefoxProfile.layoutOnDisk(FirefoxProfile.java:421)\n\u00a0at org.openqa.selenium.firefox.internal.NewProfileExtensionConnection.start(NewProfileExtensionConnection.java:95)\n... 12 more\nCaused by: java.lang.ClassCastException: org.apache.xerces.jaxp.DocumentBuilderFactoryImpl cannot be cast to javax.xml.parsers.DocumentBuilderFactory\n\u00a0at javax.xml.parsers.DocumentBuilderFactory.newInstance(Unknown Source)\n\u00a0at org.openqa.selenium.firefox.internal.FileExtension.readIdFromInstallRdf(FileExtension.java:95)\nAlso tried with below firefox versions (Firefox: 60.3 oesr (64 bit), Selenium : v3.4.0,\u00a0 Geckodriver: 0.23.0 ( 2018-10-04)), ended with same casting exception.",
        "Issue Links": []
    },
    "NUTCH-2682": {
        "Key": "NUTCH-2682",
        "Summary": "Upgrade to Tika 1.20",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Dec/18 16:07",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "21/Jan/19 15:37",
        "Description": "Tika 1.20 has been released, 1.x should be upgraded.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/424"
        ]
    },
    "NUTCH-2683": {
        "Key": "NUTCH-2683",
        "Summary": "DeduplicationJob: add option to prefer https:// over http://",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "07/Jan/19 08:04",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "10/Apr/19 11:35",
        "Description": "The deduplication job allows to keep the shortest URLs as the \"best\" URL of a set of duplicates, marking all longer ones as duplicates. Recently search engines started to penalize non-https pages by giving https pages a higher rank and marking http as insecure.\nIf URLs are identical except for the protocol the deduplication job should be able to prefer https:// over http:// URLs, although the latter ones are shorter by one character. Of course, this should be configurable and in addition to existing preferences (length, score and fetch time) to select the \"best\" URL among duplicates.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/425"
        ]
    },
    "NUTCH-2684": {
        "Key": "NUTCH-2658 Add README file to all plugins in src/plugin",
        "Summary": "Add README.md file to all indexer writers plugins",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "documentation,                                            indexer",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "15/Jan/19 21:38",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "22/Feb/19 15:38",
        "Description": "Adding the README.md file with plugin-specific documentation to all indexer writers plugins.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/436"
        ]
    },
    "NUTCH-2685": {
        "Key": "NUTCH-2658 Add README file to all plugins in src/plugin",
        "Summary": "Add README.md file to all exchange plugins",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "documentation,                                            indexer",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "15/Jan/19 21:42",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "22/Jan/19 16:26",
        "Description": "Adding the README.md file with plugin-specific documentation to all exchange plugins.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/429"
        ]
    },
    "NUTCH-2686": {
        "Key": "NUTCH-2686",
        "Summary": "Separate field for mime types mapped by index-more plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "15/Jan/19 23:30",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "22/Jan/19 15:44",
        "Description": "Since NUTCH-1262, several mime types can be mapped to a different value. By default, the behavior is to replace the original value with the new one. But if we want to keep the original mime type too? This issue pretends to accomplish this requirement.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/428"
        ]
    },
    "NUTCH-2687": {
        "Key": "NUTCH-2687",
        "Summary": "Regex for reading title from Content-Disposition is wrong",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Jan/19 14:44",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "18/Jan/19 10:38",
        "Description": "Given URL: https://www.amuse-project.org/file/download/default/E6D0537647AF1204656076943F4729B0/Koopstra2016_5fOntologically%20classifying%20ERP%20feature,%20the%20NEXT%20method_5fFinal.pdf\nAnd regex: bfilename=['\\\"](.+)['\\\"]\nWe get the following title:\nKoopstra2016_Ontologically classifying ERP feature, the NEXT method_Final.pdf\"; filename*=utf-8'\nChanged regex to: bfilename=['\\\"]([^\\\"]+) fixes it",
        "Issue Links": []
    },
    "NUTCH-2688": {
        "Key": "NUTCH-2688",
        "Summary": "Unify the licence headers",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "17/Jan/19 15:54",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "06/May/19 14:52",
        "Description": "Sometimes the license headers are written in .java classes in a javadoc comment (/** license */), sometimes in a block comment (/* license */), and sometimes is a javadoc comment but with several * (/**** license */). The idea is to reach an understanding on how the license headers should be written on .java files.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/444"
        ]
    },
    "NUTCH-2689": {
        "Key": "NUTCH-2689",
        "Summary": "Speed up urlfilter-regex and urlfilter-automaton",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Jan/19 13:45",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "29/Jan/19 10:32",
        "Description": "The unit tests of urlfilter-regex and urlfilter-automaton include a benchmark. After playing and benchmarking modifications the following changes seem to significantly improve the performance:\n\ndo not extract host and domain name from the URL if not needed (no host/domain-specific rules used, cf. NUTCH-1838)\nuse non-capturing groups if possible\nuse (?i) to make the patterns case insensitive and remove uppercase variants",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/432"
        ]
    },
    "NUTCH-2690": {
        "Key": "NUTCH-2690",
        "Summary": "Configurable and fast URL filter",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Jan/19 15:48",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/May/19 15:32",
        "Description": "This improvement introduces a new URL filter plugin \"urlfilter-fast\" (naming debatable) which is in use at Common Crawl since 2013 to apply a long list of filters. \n\nan exact (suffix) match against the host name is done to retrieve host/domain-specific regex rules\napplies a regular expression against the path (and query) component of the URL\n\nWhat makes it faster than urlfilter-regex for common cases:\n\nregexes are selected by host name or it's domain suffix, so there are usually fewer rules to be checked. That's similar to NUTCH-1838 but any domain suffix can be matched including subdomain.domain.com, com or . for global rules. The selection by host name suffix is considerably fast.\nregexes are applied only to the path component (optionally including the query) and not the entire URL.\n  Matching against a shorter string can make a huge difference for more complex regular expressions.\nthe rule to deny everything from a host or domain gets special treatment to be fast\n\nMore details about the rule format are found in the plugin's README.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/433"
        ]
    },
    "NUTCH-2691": {
        "Key": "NUTCH-2691",
        "Summary": "Improve logging from scoring-depth plugin",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "scoring",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "22/Jan/19 15:56",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "29/Jan/19 10:19",
        "Description": "Currently the scoring-depth plugin emits a \"Missing depth, removing all outlinks from url\" log message for every page that failed parsing (and does not have outlinks anyway).\nWill provide a patch that exits immediately when there\u00a0are no outlinks.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/434"
        ]
    },
    "NUTCH-2692": {
        "Key": "NUTCH-2692",
        "Summary": "Subcollection to support case-insensitive white and black lists",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "28/Jan/19 09:40",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "22/Feb/19 15:49",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2693": {
        "Key": "NUTCH-2693",
        "Summary": "Misspelled configuration property names in documentation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "07/Feb/19 10:58",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "22/Feb/19 14:58",
        "Description": "Some configuration properties in documentation (description of properties in nutch-default.xml or Java comments) are misspelled or haven't been updated when property names have been changed. For example, the property \"generate.count.mode\" is referenced in the description of another property as \"generator.count.mode\".",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/437"
        ]
    },
    "NUTCH-2694": {
        "Key": "NUTCH-2694",
        "Summary": "HostDB to aggregate by long instead of integer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "hostdb",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "11/Feb/19 11:11",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/May/19 14:50",
        "Description": "Last week we got Pinterest in our database, it has a neat set of sitemaps, and a lot of entries, over 2 billion. When first making HostDatum i foolishly used ints instead of longs, which shows in -1.9 billion records for Pinterest.\nI propose a simple move from int to long with an upgrade note mentioning the databases are not compatible and the suggestion to delete any existing HostDB. Agreed?",
        "Issue Links": []
    },
    "NUTCH-2695": {
        "Key": "NUTCH-2695",
        "Summary": "Fix some alerts raised by LGTM",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Malcolm Taylor",
        "Created": "13/Feb/19 14:05",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "22/Feb/19 14:25",
        "Description": "I work for Semmle, the company behind LGTM. Our analysis of Nutch raised some alerts, and this issue is to fix a few of the more straightforward ones. (https://lgtm.com/projects/g/apache/nutch/alerts/?mode=tree).\n\n`Wrong NaN comparison` in Generator\n`Type mismatch on container modification` in NutchServerPoolExecutor\n`Missing format argument` in CrawlDbReader",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/438"
        ]
    },
    "NUTCH-2696": {
        "Key": "NUTCH-2696",
        "Summary": "Nutch SegmentReader does not dump non-ASCII characters with Hadoop 3.x",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "segment",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Laurent Hervaud",
        "Created": "21/Feb/19 09:57",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "01/Sep/19 15:49",
        "Description": "All Nutch tasks work properly with Hadoop 3.x. (except SegmentReader)\n SegmentReader with -get option work fine.\n SegmentReader with -dump option replace non-ascii character by ?\nExemple url : http://www.wikipedia.fr/index.php\n\u00a0\n\n\r\ncommand : ./runtime/deploy/bin/nutch readseg -dump /user/nutch/crawl1.15/segments/20190221093756 /tmp/dump1.15 -nocontent -nogenerate -noparse -noparsedata\r\nParseText::\r\n Wikipedia.fr - Portail de recherche sur les projets Wikim?dia\r\n Chercher sur Wikip?dia en fran?ais\r\n L?encyclop?die librement r?utilisable que chacun peut am?liorer.\r\n\n\n\u00a0\n\u00a0\n\n\r\ncommand : ./runtime/deploy/bin/nutch readseg -get /user/nutch/crawl1.15/segments/20190221093756 http://www.wikipedia.fr/index.php -nocontent -nogenerate -noparse -noparsedata\r\nParseText::\r\n Wikipedia.fr - Portail de recherche sur les projets Wikim\u00e9dia\r\n Chercher sur Wikip\u00e9dia en fran\u00e7ais\r\n L\u2019encyclop\u00e9die librement r\u00e9utilisable que chacun peut am\u00e9liorer.\r\n\n\n\u00a0\nI try to build with hadoop 3.0.0 dependencies in ivy.xml but i have the same result\nIt's work fine in local mode.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/440"
        ]
    },
    "NUTCH-2697": {
        "Key": "NUTCH-2697",
        "Summary": "Upgrade Ivy to fix the issue of an unset packaging.type property",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Chris Gavin",
        "Created": "25/Feb/19 12:21",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "17/Aug/20 13:48",
        "Description": "Currently Nutch fails to build from a clean checkout due to packaging.type not being set (even with the current workaround in ivysettings.xml).\n\n\r\n[ivy:resolve] :: problems summary ::\r\n[ivy:resolve] :::: WARNINGS\r\n[ivy:resolve] [FAILED ] javax.ws.rs#javax.ws.rs-api;2.1.1!javax.ws.rs-api.${packaging.type}: (0ms)\r\n[ivy:resolve] ==== local: tried\r\n[ivy:resolve] /opt/work/.ivy2/local/javax.ws.rs/javax.ws.rs-api/2.1.1/${packaging.type}s/javax.ws.rs-api.${packaging.type}\r\n[ivy:resolve] ==== maven2: tried\r\n[ivy:resolve] http://repo1.maven.org/maven2/javax/ws/rs/javax.ws.rs-api/2.1.1/javax.ws.rs-api-2.1.1.${packaging.type}\r\n[ivy:resolve] ==== apache-snapshot: tried\r\n[ivy:resolve] https://repository.apache.org/content/repositories/snapshots/javax/ws/rs/javax.ws.rs-api/2.1.1/javax.ws.rs-api-2.1.1.${packaging.type}\r\n[ivy:resolve] ==== sonatype: tried\r\n[ivy:resolve] http://oss.sonatype.org/content/repositories/releases/javax/ws/rs/javax.ws.rs-api/2.1.1/javax.ws.rs-api-2.1.1.${packaging.type}\r\n[ivy:resolve] ::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] :: FAILED DOWNLOADS ::\r\n[ivy:resolve] :: ^ see resolution messages for details ^ ::\r\n[ivy:resolve] ::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] :: javax.ws.rs#javax.ws.rs-api;2.1.1!javax.ws.rs-api.${packaging.type}\r\n[ivy:resolve] ::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve] \r\nBUILD FAILED\n\nThis issue has been fixed in the latest version of Ivy so upgrading will cause the build to work correctly again.",
        "Issue Links": [
            "/jira/browse/NUTCH-2671",
            "/jira/browse/NUTCH-2672",
            "https://github.com/apache/nutch/pull/441",
            "https://github.com/apache/nutch/pull/442",
            "https://github.com/apache/nutch/pull/550"
        ]
    },
    "NUTCH-2698": {
        "Key": "NUTCH-2698",
        "Summary": "Remove sonar build task from build.xml",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "02/Mar/19 18:52",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "05/Mar/19 21:05",
        "Description": "build.xml currently has the following content\n\n\r\n  <!-- ================================================================== -->\r\n  <!-- SONAR targets                                                      -->\r\n  <!-- ================================================================== -->\r\n\r\n  <!-- Define the Sonar task if this hasn't been done in a common script -->\r\n  <taskdef uri=\"antlib:org.sonar.ant\" resource=\"org/sonar/ant/antlib.xml\">\r\n    <classpath path=\"${ant.library.dir}\"/>\r\n    <classpath path=\"${mysql.library.dir}\"/>\r\n  </taskdef>\r\n\r\n  <!-- Add the target -->\r\n  <target name=\"sonar\" description=\"--> run SONAR analysis\">\r\n\r\n    <!-- list of mandatory source directories (required) -->\r\n    <property name=\"sonar.sources\" value=\"${src.dir}\"/>\r\n\r\n    <!-- list of properties (optional) -->\r\n    <property name=\"sonar.projectName\" value=\"Nutch Trunk 1.4 Sonar Analysis\" />\r\n    <property name=\"sonar.binaries\" value=\"${build.dir}/classes\" />\r\n    <property name=\"sonar.binaries\" value=\"${build.dir}/plugins\" />\r\n    <property name=\"sonar.tests\" value=\"${test.src.dir}\" />\r\n\r\n    <sonar:sonar workDir=\"${base.dir}\" key=\"org.apache.nutch:trunk\"\r\n     version=\"1.4-SNAPSHOT\" xmlns:sonar=\"antlib:org.sonar.ant\"/>\r\n  </target>\r\n\n\nWe should simply remove as it is defunct.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/443"
        ]
    },
    "NUTCH-2699": {
        "Key": "NUTCH-2699",
        "Summary": "Protocol-okhttp: needless loops to increment requested bytes counter when more content is already buffered",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "13/Mar/19 15:41",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "12/Apr/19 11:27",
        "Description": "The okhttp library used by the plugin protocol-okhttp buffers content internal and often has already buffered more content than has been requested. The plugin should immediately set the request count to the size of the buffered content to avoid needless loops when the buffered size comes close to the content limit (the increment steps are too small):\n\n2019-03-11 14:56:36,642 DEBUG okhttp.OkHttpResponse - http://localhost/large.pdf - http/1.1 200 OK\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 8192, buffered = 16088\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 16384, buffered = 24280\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 24576, buffered = 32472\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 32768, buffered = 40664\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 40960, buffered = 48856\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 49152, buffered = 57048\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 57344, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 57638, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 57932, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 58226, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 58520, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 58814, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 59108, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 59402, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 59696, buffered = 65240\r\n2019-03-11 14:56:36,643 DEBUG okhttp.OkHttpResponse - total bytes requested = 59990, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 60284, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 60578, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 60872, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 61166, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 61460, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 61754, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 62048, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 62342, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 62636, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 62930, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 63224, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 63518, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 63812, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 64106, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 64400, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 64694, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 64988, buffered = 65240\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - total bytes requested = 65282, buffered = 73432\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - content limit reached\r\n2019-03-11 14:56:36,644 DEBUG okhttp.OkHttpResponse - copied 65534 bytes out of 73432 buffered, remaining buffer contains 7898 bytes\r\n2019-03-11 14:56:36,645 DEBUG okhttp.OkHttpResponse - HTTP content truncated to 65534 bytes (reason: LENGTH)\r\n2019-03-11 14:56:36,661 INFO parse.ParseSegment - http://localhost/large.pdf skipped. Content of size 366578 was truncated to 65534\r\n2019-03-11 14:56:36,661 WARN parse.ParserChecker - Content is truncated, parse may fail!",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/445"
        ]
    },
    "NUTCH-2700": {
        "Key": "NUTCH-2700",
        "Summary": "Indexchecker: improve command-line help",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "13/Mar/19 20:11",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "11/Apr/19 10:46",
        "Description": "The command-line help of the indexchecker tool is incomplete:\n\nUsage: IndexingFiltersChecker [-normalize] [-followRedirects] [-dumpText] [-md key=value] (-stdin | -listen <port> [-keepClientCnxOpen])\r\n\n\nIt does not\n\nshow the possibility to pass the URL as argument\nmention the property -DdoIndex=true which makes it send the document to the indexes\n\nIt should follow the help shown by parsechecker.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/446"
        ]
    },
    "NUTCH-2701": {
        "Key": "NUTCH-2701",
        "Summary": "Fetcher: log dates and times also in human-readable form",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Mar/19 12:57",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "10/Apr/19 11:44",
        "Description": "Fetcher logs dates and times as epoch milliseconds. It should log it also in a human-readable format, e.g. in case of the timelimit:\n\n19/01/11 17:57:56 INFO fetcher.Fetcher: Fetcher Timelimit set for : 1547246036104",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/447"
        ]
    },
    "NUTCH-2702": {
        "Key": "NUTCH-2702",
        "Summary": "Fetcher: suppress stack for frequent exceptions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Mar/19 13:09",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "05/Aug/19 13:45",
        "Description": "Fetcher logs the entire Java stack when a fetch fails with an Exception. To reduce the size of the fetcher logs, frequent exceptions with an obvious reason should be logged without stack, at least, for the default log level (INFO).\nFrequent and obvious exceptions are:\n\njava.net.UnknownHostException\njava.net.SocketTimeoutException\njava.net.ConnectException\njavax.net.ssl.SSLHandshakeException\n\nClass name and message should be sufficient by default. For debugging the fetch can be retried with a more verbose log level.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/456"
        ]
    },
    "NUTCH-2703": {
        "Key": "NUTCH-2703",
        "Summary": "parse-tika: Boilerpipe should not run for non-(X)HTML pages",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Markus Jelsma",
        "Reporter": "Hany Shehata",
        "Created": "18/Mar/19 14:08",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "11/Apr/19 10:34",
        "Description": "Boilerpipe is running for non-(X)html pages which is require more resources.\nIn my testing scenario, I've large PDFs in my websites and by enabling\u00a0Boilerpipe I have to\u00a0assign 8500MB for\u00a0JAVA Heap to finish the crawl job without issues.\nDisabling\u00a0Boilerpipe allow me to minimize the JVM Heap to 500MB with no issues.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/449"
        ]
    },
    "NUTCH-2704": {
        "Key": "NUTCH-2704",
        "Summary": "Upgrade crawler-commons dependency to 1.0",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "25/Mar/19 09:50",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "12/Apr/19 11:29",
        "Description": "Crawler-commons 1.0 has been released. We should upgrade.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/448"
        ]
    },
    "NUTCH-2705": {
        "Key": "NUTCH-2705",
        "Summary": "urlfilter-validator rejects IPv6 URLs",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "26/Mar/19 13:19",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "The plugin urlfilter-validator rejects URLs with an IPv6 address as hostname/authority (given according to RFC 2732:\n\n% echo \"http://[2010:836B:4179::836B:4179]/\" \\\r\n    | bin/nutch filterchecker -filterName urlfilter-validator -stdin\r\nChecking combination of these URLFilters: UrlValidator \r\n-http://[2010:836B:4179::836B:4179]/\r\n\n\nWe should also consider to use the class UrlValidator from commons-validator directly instead of a modified copy. This would help to get updates and improvements with little effort - IPv6 is already supported, see the class implementation.",
        "Issue Links": []
    },
    "NUTCH-2706": {
        "Key": "NUTCH-2706",
        "Summary": "-addBinaryContent flag can cause \"String length must be a multiple of four\" error in IndexingJob",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Prajeeth Emanuel",
        "Created": "04/Apr/19 11:52",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "24/May/19 13:38",
        "Description": "When using the following\u00a0crawling command:\nbin/crawl -i -s /user/xxxx/seed /user/xxxx/test-crawl-8 3\u00a0\nwith the index command in the crawl script with -addBinaryContent and -base64.\nThe error I get is:\n2019-04-04 04:10:43,702 svnNumber= clientHw=\"\" userId=\"\" actionKpi=\"\" [main] WARN org.apache.hadoop.mapred.YarnChild - Exception running child : org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException:\u00a0ERROR: [doc=73ad5e05e49054efa258e7c54ae9b9ee] Error adding field 'binaryContent'='PCFET0NUWVBFIGh0bWw+DQo8aHRtbCBsYW5nPSJlbiI+DQo8aGVhZD4NCgk8bWV0YSBodHRwLWVx...\n\u00a0\n...\n\u00a0\nmsg=String length must be a multiple of four. at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:559) at\u00a0 at org.apache.nutch.indexer.IndexWriters.commit(IndexWriters.java:251) at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:47) at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.close(ReduceTask.java:550) at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:629) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1693) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)\n\u00a0\nI see this\u00a0https://issues.apache.org/jira/browse/NUTCH-2186\u00a0as well. Opening a new ticket as mentioned in the comments because I have a different environment.",
        "Issue Links": [
            "/jira/browse/NUTCH-2650",
            "https://github.com/apache/nutch/pull/453"
        ]
    },
    "NUTCH-2707": {
        "Key": "NUTCH-2707",
        "Summary": "protocol-okhttp fails to decompress content if Content-Encoding header is wrong",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "05/Apr/19 13:14",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The plugin protocol-okhttp does not decompress the returned gzipped content for some rare pages.  Looks like that happens because the response HTTP header does not specify Content-Type: gzip but zlib,gzip,deflate.\n\n% bin/nutch parsechecker -Dplugin.includes='protocol-okhttp|parse-tika' \\\r\n      -Dstore.http.headers=true -Dstore.http.request=true \\\r\n      http://24310.gr/afroditi-42426.html\r\nfetching: http://24310.gr/afroditi-42426.html \r\n...\r\ncontentType: application/gzip\r\n...\r\nContent Metadata: Transfer-Encoding=chunked ... Content-Encoding=zlib,gzip,deflate ... _request_=GET /afroditi-42426.html HTTP/1.1\r\n...\r\nAccept-Encoding: gzip\r\n\r\n _response.headers_=HTTP/1.1 200 OK\r\n...\r\nContent-Encoding: zlib,gzip,deflate\r\n...\r\nTransfer-Encoding: chunked\r\nConnection: keep-alive\r\n\n\nThe plugin protocol-http requests Accept-Encoding: x-gzip, gzip, deflate and gets the correct response header:\n\n% bin/nutch parsechecker -Dplugin.includes='protocol-http|parse-tika' \\\r\n       -Dstore.http.headers=true -Dstore.http.request=true http://24310.gr/afroditi-42426.html\r\n...\r\ncontentType: application/xhtml+xml\r\n...\r\nContent Metadata: ... Content-Encoding=gzip ... _request_=GET /afroditi-42426.html HTTP/1.1\r\nHost: 24310.gr\r\nAccept-Encoding: x-gzip, gzip, deflate\r\n...\r\n\n\nSimilar for Firefox which sends Accept-Encoding: gzip, deflate.\nI will report the issue to upstream okhttp. But it would be also possible to handle the content encoding in the protocol implementation: if the Accept-Encoding header is set, the okhttp library will not decompress the content and expects that it's handled in the calling code.",
        "Issue Links": []
    },
    "NUTCH-2708": {
        "Key": "NUTCH-2708",
        "Summary": "urlfilter-automaton: update library dependency (dk.brics.automaton)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "plugin,                                            urlfilter",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Apr/19 11:57",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/May/19 16:33",
        "Description": "A new version of the dk.brics.automaton library (1.12-1) is available.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/450"
        ]
    },
    "NUTCH-2709": {
        "Key": "NUTCH-2709",
        "Summary": "Remove unused properties and code related to HTTP protocol",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "16/Apr/19 12:33",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "06/May/19 15:26",
        "Description": "Remove the unused properties http.verbose and http.max.delays from nutch-default.xml. Also remove code commented out which has used http.verbose.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/451"
        ]
    },
    "NUTCH-2710": {
        "Key": "NUTCH-2710",
        "Summary": "Normalize outlinks before checking for internal or external links",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "25/Apr/19 13:54",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "We have a normalizer that transforms external URLs back to internal URLs. But those URLs are never passed to the normalizer, because they have already been filtered out by internal and/or external host/domain checks in parseOutputFormat.filterNormalize().\nThis patch proposes to move the normalizers above the checks for internal/external hosts/domains.",
        "Issue Links": []
    },
    "NUTCH-2711": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2712": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2713": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2714": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2715": {
        "Key": "NUTCH-2715",
        "Summary": "WARCExporter fails on large records",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Yossi Tamari",
        "Created": "06/May/19 12:20",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "24/May/19 13:28",
        "Description": "com.martinkl.warc.WARCRecord throws an\u00a0IllegalStateException when a single line is over 10,000\u00a0bytes. Since this exception is not caught in WARCExporter, it fails the whole export.\nI doubt that validity of the limitation in WARCRecord, but regardless, I think WARCExporter should catch the exception and skip to the next record.\n(See also\u00a0https://github.com/ept/warc-hadoop/issues/5)",
        "Issue Links": []
    },
    "NUTCH-2716": {
        "Key": "NUTCH-2716",
        "Summary": "protocol-http: Response headers are not stored for a compressed response",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Yossi Tamari",
        "Created": "06/May/19 13:17",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "24/May/19 13:27",
        "Description": "Even when\u00a0store.http.headers=true, the HTTP headers are not saved for a gzipped or deflated response, because they may contain an incorrect content-length header.\nThis causes WARCExporter to generate \"resource\" (headerless) entries instead of \"response\" entries.\nWhile I can see why reporting the wrong content-encoding and length may be a bug, removing all the headers is not a fix.\nI am not submitting a patch yet since I'm not sure what the best fix is, but I guess the best patch is to remove those two header lines and store the rest of the headers. If there is no objection, I can submit a patch that does this. Otherwise, what would be a better fix?",
        "Issue Links": []
    },
    "NUTCH-2717": {
        "Key": "NUTCH-2717",
        "Summary": "Generator cannot open hostDB",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "generator,                                            hostdb",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "16/May/19 15:58",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "24/May/19 13:30",
        "Description": "During generate, the hostDB cannot be opened anymore, see:\n2019-05-16 15:53:50,134 ERROR crawl.Generator - Error reading HostDB because File file:<path>/hostdb/current/part-r-00000/data does not exist\nPR: https://github.com/apache/nutch/pull/455",
        "Issue Links": [
            "/jira/browse/NUTCH-2375",
            "https://github.com/apache/nutch/pull/455"
        ]
    },
    "NUTCH-2718": {
        "Key": "NUTCH-2718",
        "Summary": "Names of index writers and exchanges configuration files to be configurable",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Sebastian Nagel",
        "Created": "22/May/19 14:12",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "01/Sep/19 04:53",
        "Description": "The file names of configuration files should be configurable by a property to make it possible to select another file without the need to repackage Nutch. For example this is achieved by the properties: urlnormalizer.regex.file or urlfilter.regex.file.\nAlso the file name for the index writer (default: index-writers.xml) and exchanges configuration (default: exchanges.xml) should be configurable.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/466"
        ]
    },
    "NUTCH-2719": {
        "Key": "NUTCH-2719",
        "Summary": "NPE if exchanges.xml uses index writer not available",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Sebastian Nagel",
        "Created": "22/May/19 15:03",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "01/Sep/19 04:53",
        "Description": "If the exchanges.xml uses an index writer not present (e.g., because the plugin is not in plugin.includes) the index job fails with a NPE:\n\njava.lang.Exception: java.lang.NullPointerException\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:462)\r\n        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:529)\r\nCaused by: java.lang.NullPointerException\r\n        at org.apache.nutch.indexer.IndexWriters.write(IndexWriters.java:222)\r\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:62)\r\n        at org.apache.nutch.indexer.IndexerOutputFormat$1.write(IndexerOutputFormat.java:47)\r\n        at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)\r\n\n\nThis should be caught gracefully with a message about an inconsistent configuration.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/465"
        ]
    },
    "NUTCH-2720": {
        "Key": "NUTCH-2720",
        "Summary": "ROBOTS metatag ignored when capitalized",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            robots",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Felix Zett",
        "Created": "23/May/19 15:09",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "09/Jun/20 10:45",
        "Description": "As discussed on the mailing list, index-metadata fails to ignore a webpage with a capitalized robots metatag such as <META NAME=\"ROBOTS\" CONTENT=\"NOINDEX, FOLLOW\">. This only applies when parse-tika is used. parse-html will \"decapitalize\"\nParsing the attached noindex.html leads to the following results:\nparse-html:\n\n\r\nbin/nutch parsechecker -Dplugin.includes=\"protocol-httpclient|parse-(html|metatags)|index-metadata\" -Dindexer.delete.robots.noindex=\"true\" -Dmetatags.names=\"robots\" -Dindex.parse.md=\"metatag.robots\" http://localhost:8080/noindex.html\r\n\r\nParse Metadata:\u00a0[...] metatag.robots=noindex,nofollow robots=noindex,nofollow\n\nparse-tika:\n\n\r\nbin/nutch parsechecker -Dplugin.includes=\"protocol-httpclient|parse-(tika|metatags)|index-metadata\" -Dindexer.delete.robots.noindex=\"true\" -Dmetatags.names=\"robots\" -Dindex.parse.md=\"metatag.robots\" http://localhost:8080/noindex.html\r\n\r\nParse Metadata: metatag.robots=NOINDEX,NOFOLLOW\u00a0 [...] ROBOTS=NOINDEX,NOFOLLOW [...]\n\n\u00a0\nThe field being named \"ROBOTS\" and not \"robots\" leads to parseData.getMeta(\"robots\") being\u00a0null in https://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/indexer/IndexerMapReduce.java#L257.",
        "Issue Links": []
    },
    "NUTCH-2721": {
        "Key": "NUTCH-2721",
        "Summary": "Make the plugin lib-htmlunit depend on lib-selenium",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.20",
        "Component/s": "build,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "24/May/19 13:11",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Try to reduce the number of dependency jars copied and shipped twice by making lib-htmlunit depend on lib-selenium. See discussion on github in NUTCH-2716.",
        "Issue Links": []
    },
    "NUTCH-2722": {
        "Key": "NUTCH-2722",
        "Summary": "Fetch dependencies via https",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.4,                                            2.5,                                            1.16",
        "Fix Version/s": "2.4,                                            1.16",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "18/Jun/19 12:12",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "11/Jul/19 15:59",
        "Description": "Dependencies need to be fetched via https, see https://central.sonatype.org/articles/2019/Apr/30/http-access-to-repo1mavenorg-and-repomavenapacheorg-is-being-deprecated/",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/457"
        ]
    },
    "NUTCH-2723": {
        "Key": "NUTCH-2723",
        "Summary": "Indexer Solr not to decode URLs before deletion",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.14",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "19/Jun/19 11:40",
        "Updated": "12/Jul/19 10:46",
        "Resolved": "12/Jul/19 10:10",
        "Description": "URLs are indexed in their raw encoded form. But indexer-solr incorrectly decodes them just before they are sent for deletion, leading to a state where a bunch of URLs are never deleted.",
        "Issue Links": []
    },
    "NUTCH-2724": {
        "Key": "NUTCH-2724",
        "Summary": "Metadata indexer not to emit empty values",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "24/Jun/19 15:13",
        "Updated": "15/Jul/19 10:55",
        "Resolved": "15/Jul/19 10:26",
        "Description": "This patch adds a trimmed length check before emitting a field.",
        "Issue Links": []
    },
    "NUTCH-2725": {
        "Key": "NUTCH-2725",
        "Summary": "Plugin lib-http to support per-host configurable cookies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "protocol",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "25/Jul/19 14:20",
        "Updated": "29/Jul/19 11:45",
        "Resolved": "29/Jul/19 10:46",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2726": {
        "Key": "NUTCH-2726",
        "Summary": "Upgrade to Tika 1.22",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.16",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "06/Aug/19 10:07",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "26/Aug/19 15:05",
        "Description": "Tika 1.22 has been released and we should upgrade master/1.x (from 1.20).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/459"
        ]
    },
    "NUTCH-2727": {
        "Key": "NUTCH-2727",
        "Summary": "Upgrade Hadoop dependencies to 2.9.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "06/Aug/19 11:00",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "27/Aug/19 07:13",
        "Description": "The latest upgrade of the Hadoop dependency dates back to Dec 2017 (NUTCH-2354). We might upgrade to the latest version of Hadoop 2.x (2.9.2).\nNote: Nutch 1.15 (or master) built with Hadoop 2.7.4 runs seamlessly on Hadoop 3.x. This should be also the case for 2.9.4 (to be tested), so we still might wait for the final upgrade to Hadoop 3.x to ensure backward-compatibility.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/460"
        ]
    },
    "NUTCH-2728": {
        "Key": "NUTCH-2728",
        "Summary": "protocol-okhttp: upgrade okhttp dependency to 3.14.2",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Aug/19 17:03",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "27/Aug/19 09:10",
        "Description": "Upgrade the okhttp library dependency to 3.14.2 (for now not to 4.0.1 which adds the Kotlin standard lib as a further dependency).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/461"
        ]
    },
    "NUTCH-2729": {
        "Key": "NUTCH-2729",
        "Summary": "protocol-okhttp: fix marking of truncated content",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "13/Aug/19 15:43",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "01/Sep/19 08:47",
        "Description": "The plugin protocol-okhttp marks content as \"truncated\" including the reason for the truncation - content limit or time limit exceeded, network disconnect during fetch.\nThe detection of truncation by content limit has one bug: if the fetched content is exactly the size of the content limit the loop to request more content is exited. It should be continued by requesting one byte more to reliably detect whether content is truncated or not.\nNote that the Content-Length header cannot be used to determine truncation reliably: it does not indicate the real content length for compressed or chunked content or it might be wrong.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/462"
        ]
    },
    "NUTCH-2730": {
        "Key": "NUTCH-2730",
        "Summary": "SitemapProcessor to treat sitemap URLs as Set instead of List",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.18",
        "Component/s": "sitemap",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Aug/19 10:42",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "14/Jul/20 10:49",
        "Description": "https://archive.epa.gov/robots.txt lists 160k sitemap URLs, absurd! Almost 160k of them are duplicates, no friendly words to describe this astonishing fact.\nAnd although our Nutch locally chews through this list in 22s, for some weird reason the big job on Hadoop fails, although it is also working on a lot more.\nMaybe this is not a problem, maybe it is. Nevertheless, treating them as Set and not List makes sense.",
        "Issue Links": [
            "/jira/browse/NUTCH-2796",
            "https://github.com/apache/nutch/pull/535"
        ]
    },
    "NUTCH-2731": {
        "Key": "NUTCH-2731",
        "Summary": "Solr Cleanup Step Fails when Authentication is Required",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Dean Pearce",
        "Created": "22/Aug/19 17:46",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "26/Aug/19 10:06",
        "Description": "When the refactoring for the new index-writer was complete, the SolrIndexWriter.java plugin class was updated for pushing documents, but not for deleting documents. This is causing a failure on the clean step when running bin/crawl as we are unable to authenticate to Solr.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/464"
        ]
    },
    "NUTCH-2732": {
        "Key": "NUTCH-2732",
        "Summary": "Ignored and tracked configuration files by git",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "build",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "01/Sep/19 19:58",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "30/Sep/19 07:15",
        "Description": "In folder conf/ there are files that are ignored and tracked by git at the same time. A way to solve this is creating *.template files for those files.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/469",
            "https://github.com/apache/nutch/pull/475"
        ]
    },
    "NUTCH-2733": {
        "Key": "NUTCH-2733",
        "Summary": "protocol-okhttp: add support for Brotli compression (Content-Encoding)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.17",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Sep/19 11:45",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "19/Jan/20 20:40",
        "Description": "Okhttp 4.1.0 adds support for Brotli compression. We might upgrade the okhttp dependency to enable support for the \"brotli\" Content-Encoding.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/492"
        ]
    },
    "NUTCH-2734": {
        "Key": "NUTCH-2734",
        "Summary": "Upgrade 2.x to use Tika 1.22",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.4",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Sep/19 16:06",
        "Updated": "23/Sep/19 08:40",
        "Resolved": "23/Sep/19 08:40",
        "Description": "Tika 1.22 has been released and we should also upgrade 2.x (from 1.20), cf. NUTCH-2726 which did the upgrade for 1.x. This is also a first step to get rid of the javax.ws.rs-api dependency (address NUTCH-2669).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/470"
        ]
    },
    "NUTCH-2735": {
        "Key": "NUTCH-2654 Remove obsolete index-writer configuration in conf/",
        "Summary": "Update the indexer-solr documentation about the schema.xml usage",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "documentation",
        "Assignee": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Reporter": "Roannel Fern\u00e1ndez Hern\u00e1ndez",
        "Created": "12/Sep/19 17:35",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "31/Oct/19 12:01",
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2736": {
        "Key": "NUTCH-2736",
        "Summary": "Upgrade Dockerfile to be based on recent Ubuntu LTS version",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.16",
        "Component/s": "build,                                            test",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "25/Sep/19 08:22",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "27/Sep/19 09:18",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/472"
        ]
    },
    "NUTCH-2737": {
        "Key": "NUTCH-2737",
        "Summary": "Generator: count and log reason of rejections during selection",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "30/Sep/19 08:01",
        "Updated": "21/Jun/22 10:53",
        "Resolved": "01/Oct/19 14:19",
        "Description": "During the map phase of the selection step, the generator rejects many (usually most of) items for various reasons:\n\nnot yet time for a refetch (returned by the fetch scheduler)\ngenerator score too low\nstatus does not match restrict status\nJexl expression not matched\n\nand some more. It would be useful if the reasons are counted and logged, esp. when the CrawlDb gets bigger and multiple options to restrict the selection are used.",
        "Issue Links": [
            "/jira/browse/NUTCH-2951",
            "https://github.com/apache/nutch/pull/477"
        ]
    },
    "NUTCH-2738": {
        "Key": "NUTCH-2738",
        "Summary": "Generator: document property generate.restrict.status",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "30/Sep/19 09:08",
        "Updated": "28/Jan/21 13:55",
        "Resolved": "01/Oct/19 14:19",
        "Description": "(seen while working on NUTCH-2737)\nThe property generate.restrict.status should be documented in nutch-default.xml.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/477"
        ]
    },
    "NUTCH-2739": {
        "Key": "NUTCH-2739",
        "Summary": "indexer-elastic: Upgrade ES and migrate to REST client",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "30/Sep/19 11:55",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "22/Nov/19 13:32",
        "Description": "The indexer-elastic plugin is based on 5.3.0 and should be upgraded to the most recent Elasticsearch version (7.3.0 or upwards).\nTransportClient has been deprecated in ES 7.x and will be removed in 8.x. We should migrate to using the REST client and also check whether this would obsolete the indexer-elastic-rest plugin.",
        "Issue Links": [
            "/jira/browse/NUTCH-2304",
            "/jira/browse/NUTCH-2677",
            "https://github.com/apache/nutch/pull/484"
        ]
    },
    "NUTCH-2740": {
        "Key": "NUTCH-2740",
        "Summary": "Generator: generate.max.count overflow not logged",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.16",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "30/Sep/19 12:28",
        "Updated": "28/Jan/21 13:56",
        "Resolved": "01/Oct/19 14:20",
        "Description": "The logging of generate.max.count overflows (max. number of URLs per host) has been broken in NUTCH-2574/646932a. Should be fixed also to get correct counts in NUTCH-2737.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/477"
        ]
    },
    "NUTCH-2741": {
        "Key": "NUTCH-2741",
        "Summary": "Remove ivy/ivy-2.2.0.jar",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Auto Closed",
        "Affects Version/s": "2.3.1",
        "Fix Version/s": "2.5",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "01/Oct/19 08:57",
        "Updated": "13/Oct/19 22:36",
        "Resolved": "13/Oct/19 22:36",
        "Description": "The Nutch 2.x branch still contains an ivy jar (ivy/ivy-2.2.0.jar) under version control. It should be remove as the ivy jar is downloaded automatically when the ant build is run first.",
        "Issue Links": []
    },
    "NUTCH-2742": {
        "Key": "NUTCH-2742",
        "Summary": "Unable to parse specific pdf file",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "nutchNewbie,                                            parser",
        "Assignee": null,
        "Reporter": "M A",
        "Created": "06/Oct/19 13:15",
        "Updated": "06/Oct/19 15:44",
        "Resolved": "06/Oct/19 15:44",
        "Description": "It appears that the Tika plugin is not parsing some PDF files.\nWhen I completed a dump of the segment data there is no content\nEDIT: See attached for output and crawl log",
        "Issue Links": []
    },
    "NUTCH-2743": {
        "Key": "NUTCH-2743",
        "Summary": "Add list of Nutch properties (nutch-default.xml) to documentation",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.17",
        "Component/s": "documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Oct/19 15:25",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "30/Apr/20 09:16",
        "Description": "The file nutch-default.xml lists all Nutch properties. It should become part of the documentation similar as done for Hadoop (eg. mapred-default.xml), including the XSL (configuration.xsl) required to render the file into a table.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/520"
        ]
    },
    "NUTCH-2744": {
        "Key": "NUTCH-2744",
        "Summary": "CrawlDbReader: improved reporting of syntactic errors in Jexl expression",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.20",
        "Component/s": "crawldb",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Oct/19 19:12",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "CrawlDbReader reports syntactic errors in Jexl expressions only in task logs (hadoop.log in local mode) and continues as if there where no Jexl expression set. It should report it more verbosely and probably also fail the job, at least, if the error can be checked at job start.\nIn my case a trivial error (score > .9 instead of score > 0.9), the crawlDb was just left unfiltered.",
        "Issue Links": []
    },
    "NUTCH-2745": {
        "Key": "NUTCH-2745",
        "Summary": "Solr schema.xml not shipped in binary release",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "build,                                            indexer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/19 11:04",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "20/Dec/19 12:38",
        "Description": "The binary release packages of Nutch 1.16 do not contain the Solr schema.xml - it should be shipped as part of the package.\nFor now users of the binary package must take the schema.xml from the source package or download it from the source repositories: \nhttps://github.com/apache/nutch/blob/release-1.16/src/plugin/indexer-solr/schema.xml",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/488"
        ]
    },
    "NUTCH-2746": {
        "Key": "NUTCH-2746",
        "Summary": "Basic URL normalizer to normalize Unicode domain names",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "plugin,                                            urlnormalizer",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "15/Oct/19 14:03",
        "Updated": "09/Dec/22 14:43",
        "Resolved": "22/Nov/19 17:54",
        "Description": "The BasicURLNormalizer (plugin urlnormalizer-basic) lacks the possibility to normalize IDNs (Unicode host/domain names).",
        "Issue Links": [
            "/jira/browse/NUTCH-1321",
            "https://github.com/apache/nutch/pull/480"
        ]
    },
    "NUTCH-2747": {
        "Key": "NUTCH-2747",
        "Summary": "Replace remaining o.a.commons.logging by org.slf4j",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Oct/19 18:33",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "07/Nov/19 08:02",
        "Description": "A few classes still use logging classes of the package org.apache.commons.logging, they should rely on org.slf4j instead, see NUTCH-1078. The commons-logging lib is included as a transitive dependency. But Nutch classes should not use it. Five classes are found via git grep -F org.apache.commons.logging.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/482"
        ]
    },
    "NUTCH-2748": {
        "Key": "NUTCH-2748",
        "Summary": "Fetch status gone (redirect exceeded) not to overwrite existing items in CrawlDb",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "crawldb,                                            fetcher",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "18/Oct/19 13:59",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "02/Dec/19 11:52",
        "Description": "If fetcher is following redirects and the max. number of redirects in a redirect chain (http.max.redirect) is reached, fetcher stores a CrawlDatum item with status \"fetch_gone\" and protocol status \"redir_exceeded\". During the next CrawlDb update the \"gone\" item will set the status of existing items (including \"db_fetched\") with \"db_gone\". It shouldn't as there has been no fetch of the final redirect target and indeed nothing is know about it's status. An wrong db_gone may then cause that a page gets deleted from the search index.\nThere are two possible solutions:\n1. ignore protocol status \"redir_exceeded\" during CrawlDb update\n2. when http.redirect.max is hit the fetcher stores nothing or a redirect status instead of a fetch_gone\nSolution 2. seems easier to implement and it would be possible to make the behavior configurable:\n\nstore the redirect target as outlink, i.e. same behavior as if http.redirect.max == 0\nstore \"fetch_gone\" (current behavior)\nstore nothing, i.e. ignore those redirects - this should be the default as it's close to the current behavior without the risk to accidentally set successful fetches to db_gone",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/485",
            "https://github.com/apache/nutch/pull/485"
        ]
    },
    "NUTCH-2749": {
        "Key": "NUTCH-2749",
        "Summary": "Fetcher and scoring-opic: transfer score to redirects",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.20",
        "Component/s": "fetcher,                                            plugin,                                            scoring",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "18/Oct/19 16:20",
        "Updated": "19/Aug/22 13:38",
        "Resolved": null,
        "Description": "See the discussion \"Score value lost after two successive redirects\" dating back to 2012.\nRedirects should be enabled to pass scores to the targets. This is mandatory for reliable scoring, otherwise scores often get lost when a link target is redirected. Eg. when the target site has moved from http:// to https://, incoming links to http:// pages are usually redirected to https:// (on the target site), and the incoming score is lost. If the migration to https:// happened recently the scores for this site might just become zero.\nI aggree with markus17's comment in the mentioned discussion @user that \"it cannot be a good idea to just copy over the score\". Instead redirects should have the same effect as a page containing a single href link.\nThis would require the following change(s):\n1. in Fetcher (class FetcherThread): the score should be passed forward to the redirect target\n\nbecause the method distributeScoreToOutlinks(...) cannot be called for redirects (no content is parsed) we would need a dedicated hook\n distributeScoreToRedirect(Text fromUrl, Text toUrl, CrawlDatum source, CrawlDatum target)\nto be called both for \"recorded\" and followed redirects (depending on http.max.redirect)\nscoring strategies can be implemented there, eg. apply \"db.score.link.{internal,external}\"\nto be implemented as default method which avoids that existing scoring filter plugins are broken\n\n2. during CrawlDb update (class CrawlDbReducer), there are different cases to consider:\na. URL not yet in CrawlDb: nothing to do if the score has been already passed forward (step 1)\nb. URL already in CrawlDb, redirects not followed in fetcher (htt.redirect.max == 0): the redirect target has been stored as db_outlink, so it will be used in the scoring method updateDbScore(...) -> nothing to do\nc. URL already in CrawlDb, fetcher follows redirects: to get the same behavior as for incoming links we would need to mark fetches stemming from a followed redirect and use them in a modified updateDbScore(...)\nBeing pragmatic I would address in this issue only point 1 and (implicitely 2a and 2b). Point 2c would require significant changes and isn't easy to control in the worst case, if there are multiple redirects followed all ending in the same target",
        "Issue Links": []
    },
    "NUTCH-2750": {
        "Key": "NUTCH-2750",
        "Summary": "Improve CrawlDbReader & LinkDbReader reader handling",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "crawldb,                                            linkdb",
        "Assignee": null,
        "Reporter": "Jurian Broertjes",
        "Created": "24/Oct/19 12:02",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "12/Nov/19 14:38",
        "Description": "The current implementation in the\u00a0CrawlDbReader re-opens readers for every URL. This is not very efficient. I've implemented a modification time check that only re-opens readers on updated crawlDB.\n\u00a0PR: https://github.com/apache/nutch/pull/483",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/483"
        ]
    },
    "NUTCH-2751": {
        "Key": "NUTCH-2751",
        "Summary": "nutch clean does not work with secured solr cloud",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Daniel Hammling",
        "Created": "01/Nov/19 14:11",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "28/Apr/20 07:45",
        "Description": "I am calling nutch clean to remove 404 entries from Solr, but fail with exception below.\nAdding and updating entries is working fine. Hence, index-writer config seems to be correct in general.\nIdentical behaviour in 1.15 and 1.16, although SolrIndexWriter.java has been modified for delete case.\nNo more ideas, where to look at....\n\u00a0\n2019-11-01 14:45:55,664 INFO solr.SolrIndexWriter - SolrIndexer: deleting 14/14 documents\n2019-11-01 14:45:55,768 ERROR impl.CloudSolrClient - Request to collection [www-int.replaceddomain.de] failed due to (0) org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity.\n, retry? 0\n2019-11-01 14:45:55,780 ERROR impl.CloudSolrClient - Request to collection [www-int.replaceddomain.de] failed due to (0) org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity.\n, retry? 1\n2019-11-01 14:45:55,858 ERROR impl.CloudSolrClient - Request to collection [www-int.replaceddomain.de] failed due to (0) org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity.\n, retry? 2\n2019-11-01 14:45:55,887 ERROR impl.CloudSolrClient - Request to collection [www-int.replaceddomain.de] failed due to (0) org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity.\n, retry? 3\n2019-11-01 14:45:55,903 ERROR impl.CloudSolrClient - Request to collection [www-int.replaceddomain.de] failed due to (0) org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity.\n, retry? 4\n2019-11-01 14:45:55,938 ERROR impl.CloudSolrClient - Request to collection [www-int.replaceddomain.de] failed due to (0) org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity.\n, retry? 5\n2019-11-01 14:45:55,938 DEBUG concurrent.ExecutorHelper - afterExecute in thread: pool-4-thread-1, runnable type: java.util.concurrent.FutureTask\n2019-11-01 14:45:55,940 INFO mapred.LocalJobRunner - reduce task executor complete.\n2019-11-01 14:45:55,941 WARN mapred.LocalJobRunner - job_local2086525572_0001\njava.lang.Exception: org.apache.solr.client.solrj.impl.CloudSolrClient$RouteException: IOException occured when talking to server at: http://10.10.0.96:10983/solr/www-int.replaceddomain.de_shard1_replica_n5\n at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:491)\n at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:558)\nCaused by: org.apache.solr.client.solrj.impl.CloudSolrClient$RouteException: IOException occured when talking to server at: http://10.10.0.96:10983/solr/www-int.replaceddomain.de_shard1_replica_n5\n at org.apache.solr.client.solrj.impl.CloudSolrClient.directUpdate(CloudSolrClient.java:553)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.sendRequest(CloudSolrClient.java:1014)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:885)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:947)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:947)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:947)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:947)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.requestWithRetryOnStaleState(CloudSolrClient.java:947)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.request(CloudSolrClient.java:818)\n at org.apache.solr.client.solrj.SolrClient.request(SolrClient.java:1219)\n at org.apache.nutch.indexwriter.solr.SolrIndexWriter.push(SolrIndexWriter.java:270)\n at org.apache.nutch.indexwriter.solr.SolrIndexWriter.commit(SolrIndexWriter.java:214)\n at org.apache.nutch.indexwriter.solr.SolrIndexWriter.close(SolrIndexWriter.java:205)\n at org.apache.nutch.indexer.IndexWriters.close(IndexWriters.java:257)\n at org.apache.nutch.indexer.CleaningJob$DeleterReducer.cleanup(CleaningJob.java:115)\n at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:179)\n at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)\n at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)\n at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:346)\n at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n at java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.solr.client.solrj.SolrServerException: IOException occured when talking to server at: http://10.10.0.96:10983/solr/www-int.replaceddomain.de_shard1_replica_n5\n at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:657)\n at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:255)\n at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:244)\n at org.apache.solr.client.solrj.impl.LBHttpSolrClient.doRequest(LBHttpSolrClient.java:483)\n at org.apache.solr.client.solrj.impl.LBHttpSolrClient.request(LBHttpSolrClient.java:413)\n at org.apache.solr.client.solrj.impl.CloudSolrClient.lambda$directUpdate$0(CloudSolrClient.java:528)\n at java.util.concurrent.FutureTask.run(FutureTask.java:266)\n at org.apache.solr.common.util.ExecutorUtil$MDCAwareThreadPoolExecutor.lambda$execute$0(ExecutorUtil.java:188)\n ... 3 more\nCaused by: org.apache.http.client.ClientProtocolException\n at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:187)\n at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)\n at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)\n at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:542)\n ... 10 more\nCaused by: org.apache.http.client.NonRepeatableRequestException: Cannot retry request with a non-repeatable request entity.\n at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:226)\n at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:185)\n at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)\n at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:111)\n at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)\n ... 13 more",
        "Issue Links": [
            "/jira/browse/NUTCH-2752",
            "/jira/browse/NUTCH-2780"
        ]
    },
    "NUTCH-2752": {
        "Key": "NUTCH-2752",
        "Summary": "indexer-solr: Upgrade to latest Solr version",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "04/Nov/19 12:07",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "28/Apr/20 07:44",
        "Description": "Upgrade the indexer-solr plugin to the latest Solr version, also to address NUTCH-2751.",
        "Issue Links": [
            "/jira/browse/NUTCH-2751",
            "/jira/browse/NUTCH-2780",
            "/jira/browse/NUTCH-2780"
        ]
    },
    "NUTCH-2753": {
        "Key": "NUTCH-2753",
        "Summary": "Add -listen option to command-line help of CrawlDbReader and LinkDbReader",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.15",
        "Fix Version/s": "1.17",
        "Component/s": "crawldb,                                            linkdb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Nov/19 10:02",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "05/May/20 14:00",
        "Description": "The tools CrawlDbReader and LinkDbReader extend AbstractChecker but do not show `-listen <port> [-keepClientCnxOpen]` as available option(s).",
        "Issue Links": []
    },
    "NUTCH-2754": {
        "Key": "NUTCH-2754",
        "Summary": "fetcher.max.crawl.delay ignored if exceeding 5 min. / 300 sec.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "fetcher,                                            robots",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "13/Nov/19 13:51",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "23/Dec/19 10:57",
        "Description": "Sites specifying a Crawl-Delay of more than 5 minutes (301 seconds or more) are always ignored, even if fetcher.max.crawl.delay is set to a higher value.\nWe need to pass a higher value of fetcher.max.crawl.delay to crawler-commons' robots.txt parser otherwise it will use the internal default value of 300 sec. and disallow all sites specifying a longer Crawl-Delay in their robots.txt.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/487"
        ]
    },
    "NUTCH-2755": {
        "Key": "NUTCH-2755",
        "Summary": "Remove obsolete plugin indexer-elastic-rest",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Nov/19 13:29",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "21/Apr/20 09:30",
        "Description": "With NUTCH-2739 the plugin indexer-elastic uses the REST client instead of the deprecated TransportClient. This obsoletes the separate REST-based plugin indexer-elastic-rest.",
        "Issue Links": [
            "/jira/browse/NUTCH-2757",
            "/jira/browse/NUTCH-2677",
            "/jira/browse/NUTCH-2304"
        ]
    },
    "NUTCH-2756": {
        "Key": "NUTCH-2756",
        "Summary": "Segment Part problem with HDFS on distibuted mode",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Resolved",
        "Affects Version/s": "1.15",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Lucas Pauchard",
        "Created": "05/Dec/19 12:41",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "20/Dec/19 14:54",
        "Description": "During the parsing, it happens sometimes that parts of the data on the HDFS is missing after the parsing.\nWhen I take a look at our HDFS, I've got this file with 0 bytes (see attachments).\nAfter that the CrawlDB complains about this specific (corrupted?) part:\nlog_crawl\n2019-12-04 22:25:57,454 INFO mapreduce.Job: Task Id : attempt_1575479127636_0047_m_000017_2, Status : FAILED\nError: java.io.EOFException: hdfs://jobmaster:9000/user/hadoop/crawlmultiokhttp/segment/20191204221308/crawl_parse/part-r-00004 not a SequenceFile\n        at org.apache.hadoop.io.SequenceFile$Reader.init(SequenceFile.java:1964)\n        at org.apache.hadoop.io.SequenceFile$Reader.initialize(SequenceFile.java:1923)\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1872)\n        at org.apache.hadoop.io.SequenceFile$Reader.<init>(SequenceFile.java:1886)\n        at org.apache.hadoop.mapreduce.lib.input.SequenceFileRecordReader.initialize(SequenceFileRecordReader.java:54)\n        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:560)\n        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:798)\n        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:347)\n        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)\n        at java.security.AccessController.doPrivileged(Native Method)\n        at javax.security.auth.Subject.doAs(Subject.java:422)\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)\n        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)~\n\nWhen I check the namenode logs, I don't see any error during the writing of the segment part but one hour later, I've got the following log:\nlog_namenode\n2019-12-04 23:23:13,750 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_attempt_1575479127636_0046_r_000004_1_1307945884_1, pending creates: 2], src=/user/hadoop/crawlmultiokhttp/segment/20191204221308/parse_data/part-r-00004/index\n2019-12-04 23:23:13,750 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file /user/hadoop/crawlmultiokhttp/segment/20191204221308/parse_data/part-r-00004/index closed.\n2019-12-04 23:23:13,750 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Recovering [Lease.  Holder: DFSClient_attempt_1575479127636_0046_r_000004_1_1307945884_1, pending creates: 1], src=/user/hadoop/crawlmultiokhttp/segment/20191204221308/crawl_parse/part-r-00004\n2019-12-04 23:23:13,750 WARN org.apache.hadoop.hdfs.StateChange: BLOCK* internalReleaseLease: All existing blocks are COMPLETE, lease removed, file /user/hadoop/crawlmultiokhttp/segment/20191204221308/crawl_parse/part-r-00004 closed.\n\nThis issue is hard to reproduce and I can't figure out what are the preconditions. It seems that it just happens randomly.\nMaybe the problem is coming from a bad management when we close the file.",
        "Issue Links": []
    },
    "NUTCH-2757": {
        "Key": "NUTCH-2757",
        "Summary": "indexer-elastic: add authentication options",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "06/Dec/19 16:03",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "19/Apr/20 09:36",
        "Description": "The plugin [indexer-elastic-rest](https://github.com/apache/nutch/blob/ac9c435db2c9b1317fc195a762fa84d4e79fd97c/src/plugin/indexer-elastic-rest/README.md) allows to configure authentication properties (mainly user and password). This should be ported to the indexer-elastic plugin which is now REST-based (NUTCH-2739).",
        "Issue Links": [
            "/jira/browse/NUTCH-2755",
            "https://github.com/apache/nutch/pull/508"
        ]
    },
    "NUTCH-2758": {
        "Key": "NUTCH-2758",
        "Summary": "Add plugin READMEs to binary release packages",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "build,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Dec/19 10:57",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "05/May/20 14:01",
        "Description": "Almost 20 plugins have a README (.md or .txt) which explains how to use and configure the plugin. The READMEs should be included in the binary release packages.",
        "Issue Links": []
    },
    "NUTCH-2759": {
        "Key": "NUTCH-2759",
        "Summary": "bin/crawl: Rename option --num-slaves",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "bin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Dec/19 12:55",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "19/Jan/20 20:34",
        "Description": "The option --num-slaves of the script bin/crawl it isn't really precise: there is no need to set it to the actual number of nodes in the Hadoop cluster. In addition, Hadoop now uses the term \"worker\" , see HADOOP-13209.\nRenaming the option to --num-fetcher-tasks or --num-fetcher-workers would be more precise.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/491"
        ]
    },
    "NUTCH-2760": {
        "Key": "NUTCH-2760",
        "Summary": "protocol-okhttp: properly record HTTP version in request message header",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "13/Dec/19 11:35",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "09/Jan/20 10:15",
        "Description": "The HTTP version in the request message tracked by the plugin protocol-okhttp (store.http.request=true) is not the version sent in the request but that received from the response.\nNote that the HTTP version sent in the request may differ from that sent back in the response. One example (tracked using wget):\n\n> wget -d https://www.kp.ru/daily/27061/4129507/\r\n...\r\n---request begin---\r\nGET /daily/27061/4129507/ HTTP/1.1\r\nUser-Agent: Wget/1.20.3 (linux-gnu)\r\nAccept: */*\r\nAccept-Encoding: identity\r\nHost: www.kp.ru\r\nConnection: Keep-Alive\r\n\r\n---request end---\r\nHTTP request sent, awaiting response... \r\n---response begin---\r\nHTTP/1.0 200 OK\r\n...\r\n\n\nprotocol-http uses the response version (\"HTTP/1.0\") also for the request:\n\n> bin/nutch parsechecker -Dstore.http.headers=true -Dstore.http.request=true \\\r\n     -Dplugin.includes='protocol-okhttp|parse-html' https://www.kp.ru/daily/27061/4129507/\r\n...\r\n_request_=GET /daily/27061/4129507/ HTTP/1.0\r\n...\r\n_response.headers_=HTTP/1.0 200 OK\r\n...\r\n\n\nThe protocol-http tracks the versions correctly:\n\n> bin/nutch parsechecker -Dstore.http.headers=true -Dstore.http.request=true \\\r\n     -Dplugin.includes='protocol-http|parse-html' https://www.kp.ru/daily/27061/4129507/\r\n...\r\n_request_=GET /daily/27061/4129507/ HTTP/1.1\r\n...\r\n_response.headers_=HTTP/1.0 200 OK\r\n...",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/489"
        ]
    },
    "NUTCH-2761": {
        "Key": "NUTCH-2761",
        "Summary": "ivy jar fails to download",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Jan/20 12:30",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "17/Jan/20 13:54",
        "Description": "Building from scratch fails with\n\n      [get] Can't get http://repo2.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar to .../ivy/ivy-2.4.0.jar\r\n\n\nNeed switch to https://repo1.maven.org/maven2/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/494"
        ]
    },
    "NUTCH-2762": {
        "Key": "NUTCH-2762",
        "Summary": "Replace http:// URLs by https:// (build files and documentation)",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "build,                                            documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Jan/20 13:36",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "23/Jan/20 13:40",
        "Description": "To avoid errors such as in NUTCH-2761, all http:// URLs in build files and documentation should be replaced by https:// if the https:// exists and there are no SSL errors.\nBut not for\n\nlicense headers: the Apache license (see https://www.apache.org/licenses/LICENSE-2.0) still uses http:// to reference the full version of the license. Changing the URL could cause legal issues.\nXML namespaces\nunit tests\nexamples\nhistoric references (eg. in CHANGES.txt)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/495"
        ]
    },
    "NUTCH-2763": {
        "Key": "NUTCH-2763",
        "Summary": "protocol-okhttp (store.http.headers): add whitespace in status line after status code also when message is empty",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "20/Jan/20 13:26",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "27/Feb/20 11:09",
        "Description": "RFC 7230 describes the HTTP response status line as:\n\nstatus-line = HTTP-version SP status-code SP reason-phrase CRLF\r\n\n\nThe \"reason-phrase\" is allowed to be empty, but the white space between status-code and reason-phrase is mandatory. The  protocol-okhttp, when storing the HTTP response header (store.http.headers = true), does not add a white space when the message is empty, even if the original response header contained a white space after the status code.\nNote: protocol-http add the status line literally.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/498"
        ]
    },
    "NUTCH-2764": {
        "Key": "NUTCH-2764",
        "Summary": "Weird build error javax.javax.measure#unit-api",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Information Provided",
        "Affects Version/s": "1.15,                                            1.16",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "21/Jan/20 12:32",
        "Updated": "15/Nov/20 15:27",
        "Resolved": "15/Nov/20 15:27",
        "Description": "In weird cases Nutch is unable to build due to a dependency problem in the TikaParser. The error report:\n\n\r\n[ivy:resolve]   ==== sonatype: tried\r\n[ivy:resolve]     https://oss.sonatype.org/content/repositories/releases/javax/measure/unit-api/working@localhost/unit-api-working@localhost.pom\r\n[ivy:resolve]     -- artifact javax.measure#unit-api;working@localhost!unit-api.jar:\r\n[ivy:resolve]     https://oss.sonatype.org/content/repositories/releases/javax/measure/unit-api/working@localhost/unit-api-working@localhost.jar\r\n[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve]           ::          UNRESOLVED DEPENDENCIES         ::\r\n[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::\r\n[ivy:resolve]           :: javax.measure#unit-api;working@localhost: not found\r\n[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::\r\n\n\nCleaning the local ~/.ivy cache solves the problem. Cleaning ~/.ivy2/cache/javax.measure/unit-api/ can also work but i have not tested it.",
        "Issue Links": []
    },
    "NUTCH-2765": {
        "Key": "NUTCH-2765",
        "Summary": "Unify and cleanup X509TrustManager",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.20",
        "Component/s": "protocol",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "23/Jan/20 13:03",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Multiple plugins use a separate implementation of the javax.net.ssl.X509TrustManager to optionally skip TLS certificate checks (NUTCH-2647).\n\nprotocol-httpclient, protocol-http, and with NUTCH-2649 also protocol-selenium, protocol-interactiveselenium, protocol-htmlunit use identical copies of DummyX509TrustManager\n 1. there should be a single class implementation either in the plugin lib-http or in the core package org.apache.nutch.net.protocols\n 2. the DummyX509TrustManager needs a clean up: it still keeps a lot old stuff relating to the sun.net.ssl.X509TrustManager interface\nprotocol-okhttp includes a minimalistic anonymous class implementing X509TrustManager",
        "Issue Links": []
    },
    "NUTCH-2766": {
        "Key": "NUTCH-2766",
        "Summary": "Update selenium-based protocol plugins to be in sync with protocol-http",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.20",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "23/Jan/20 13:11",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The selenium-based protocol plugins (protocol-selenium, protocol-interactiveselenium, protocol-htmlunit) are partial copies of protocol-http to sniff HTTP headers in order to determine the content type and if it's not HTML to implement a fall-back protocol to download the content.\nWhile there have been significant improvements and fixes in protocol-http (NUTCH-2549 and related), these haven't been transferred to the selenium-based protocol plugins. They should, or even better the code for the fall-back protocol implementation should be shared.",
        "Issue Links": []
    },
    "NUTCH-2767": {
        "Key": "NUTCH-2767",
        "Summary": "Fetcher to stop filling queues skipped due to repeated exceptions",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/Feb/20 14:31",
        "Updated": "08/Jan/23 19:42",
        "Resolved": "27/Feb/20 10:30",
        "Description": "Since NUTCH-769 the fetcher skips URLs from queues which already got more exceptions than configured by \"fetcher.max.exceptions.per.queue\". Such queues are emptied when the threshold is reached. However, the QueueFeeder may still feeding queues and add again URLs to the queues which are already over the exception threshold. The first URL in the queue is then fetched, consecutive ones are eventually removed if the next exception is observed.\nHere one example:\n\n2020-02-19 06:26:48,877 INFO [FetcherThread] o.a.n.fetcher.FetchItemQueues: * queue: ww.example.com >> removed 61 URLs from queue because 40 exceptions occurred\r\n2020-02-19 06:26:53,551 INFO [FetcherThread] o.a.n.fetcher.FetcherThread: FetcherThread 172 fetching https://www.example.com/... (queue crawl delay=5000ms)\r\n2020-02-19 06:26:54,073 INFO [FetcherThread] o.a.n.fetcher.FetcherThread: FetcherThread 172 fetch of https://www.example.com/... failed with: ...\r\n2020-02-19 06:26:58,766 INFO [FetcherThread] o.a.n.fetcher.FetcherThread: FetcherThread 111 fetching https://www.example.com/... (queue crawl delay=5000ms)\r\n2020-02-19 06:26:59,290 INFO [FetcherThread] o.a.n.fetcher.FetcherThread: FetcherThread 111 fetch of https://www.example.com/... failed with: ...\r\n2020-02-19 06:27:03,960 INFO [FetcherThread] o.a.n.fetcher.FetcherThread: FetcherThread 103 fetching https://www.example.com/... (queue crawl delay=5000ms)\r\n2020-02-19 06:27:04,482 INFO [FetcherThread] o.a.n.fetcher.FetcherThread: FetcherThread 103 fetch of https://www.example.com/... failed with: ...\r\n2020-02-19 06:27:04,484 INFO [FetcherThread] o.a.n.fetcher.FetchItemQueues: * queue: www.example.com >> removed 1 URLs from queue because 41 exceptions occurred\r\n... (fetching again 30 URLs, all failed)\r\n2020-02-19 06:28:23,578 INFO [FetcherThread] org.apache.nutch.fetcher.FetchItemQueues: * queue: www.example.com >> removed 1 URLs from queue because 42 exceptions occurred\r\n\n\nQueueFeeder should check whether the exception threshold is already reached and if yes not add further URLs to the queue.",
        "Issue Links": [
            "/jira/browse/NUTCH-1687",
            "https://github.com/apache/nutch/pull/497"
        ]
    },
    "NUTCH-2768": {
        "Key": "NUTCH-2768",
        "Summary": "FetcherThread: unnecessary usage of class casts",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "21/Feb/20 15:09",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "27/Feb/20 10:57",
        "Description": "The member variables fetchQueues (instance of FetchItemQueues) and spinWaiting (instance of AtomicInteger) are declared as \"Object\" which then requires to cast them. The variables are assigned in the constructor, they should be declared with the correct type to avoid unnecessary casts.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/499"
        ]
    },
    "NUTCH-2769": {
        "Key": "NUTCH-2769",
        "Summary": "parse-html unable to parse certain outlinks",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.15,                                            1.16",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "Prajeeth Emanuel",
        "Created": "26/Feb/20 09:37",
        "Updated": "28/Feb/20 18:57",
        "Resolved": null,
        "Description": "Nutch is unable to parse certain outlinks in pages.\u00a0\nFor example:\nCrawling\u00a0http://d4fdot.com/pbfdot/PBC-North_index.asp\u00a0does not parse the outlinks:\u00a0\ncongress_avenue_lighting_improvements.asp\nblue_heron_boulevard_bridge_fender_replacement.asp\nindiantown_road_intersection_improvements.asp\n\u00a0\nCrawling\u00a0http://www.d4fdot.com/pbfdot/index.asp\u00a0however, parses\u00a0congress_avenue_lighting_improvements.asp\u00a0correctly even though the Anchor element is structured similarly.\u00a0\n\u00a0\nURL filters and normalizers have been modified to barely operate and no URLs or outlinks are being ignored in the current config and the error still occurs.",
        "Issue Links": []
    },
    "NUTCH-2770": {
        "Key": "NUTCH-2770",
        "Summary": "Subcollection logic allows empty string as a whitelist value, thus matching every incoming document.",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Jason Grey",
        "Created": "26/Feb/20 22:43",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "13/Mar/20 08:33",
        "Description": "If subcollections.xml whitelist element contains empty lines at the end (ie: because the XML was formatted nicely) those lines can become an empty string in the string matching logic. That logic uses String.contains, and that in turn returns TRUE for an empty string as input.\nThis then causes that subcollection to be tagged on EVERY incoming document.\nHere is a POC to show the issue in isolation, since I do not yet have a dev environment setup for nutch yet.\n\n\r\n/**\r\nThis is a snippet that does the same logic as Subcollection.java in nutch.\r\nhttps://github.com/apache/nutch/blob/fdee94d8e0894384f1fca7c9f16c7593a5bc928c/src/plugin/subcollection/src/java/org/apache/nutch/collection/Subcollection.java\r\n**/\r\n\r\nimport java.lang.Math; \r\nimport java.util.StringTokenizer;\r\npublic class HelloWorld\r\n{\r\n  public static void main(String[] args)\r\n  {\r\n    String urlToTest = \"https://www.example.com/test/url/here\";\r\n    String text = \"\\r\\n\\t//research.xyz.com/\\r\\n\\t/research/\\r\\n\\t\";\r\n    StringTokenizer st = new StringTokenizer(text, \"\\n\\r\");\r\n    while (st.hasMoreElements()) {\r\n      String line = ((String) st.nextElement()).trim();\r\n      boolean matched = urlToTest.contains(line);\r\n      System.out.println(\"line: [\" + line + \"] = \" + matched);\r\n    }\r\n  }\r\n}\r\n\r\n\r\n/**\r\noutput:\r\nline: [//research.xyz.com/] = false\r\nline: [/research/] = false\r\nline: [] = true\r\nas we can see, for the text in our XML config, it's outputting an extra line which is matching on EVERYTHING...\r\n**/\t\r\n\n\nThere is a workaround, if you collapse the whitespace in the XML file, but I think we should fix this anyway. I will try to do so and submit a patch soon which will filter out empty string.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/503"
        ]
    },
    "NUTCH-2771": {
        "Key": "NUTCH-2771",
        "Summary": "Tests in nightly builds: speed up long runners",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.20",
        "Component/s": "build,                                            test",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Feb/20 09:11",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The Nutch tests run by \"ant test\" or \"ant nightly\") take rather long to run. Although all tests are implemented as JUnit tests, some tests are more integration tests, eg. launching a Jetty web server and fetching documents from it. It's nice to have also higher level tests, and they are expected to long runner than a simple unit test. However, some of the test classes take really long to run (times taken from https://builds.apache.org/job/Nutch-trunk/3663/consoleText):\n\n    [junit] Running org.apache.nutch.segment.TestSegmentMergerCrawlDatums\r\n    [junit] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 133.898 sec\r\n\r\n    [junit] Running org.apache.nutch.segment.TestSegmentMerger\r\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 101.026 sec\r\n\r\n    [junit] Running org.apache.nutch.crawl.TestGenerator\r\n    [junit] Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 46.03 sec\r\n\r\n    [junit] Running org.apache.nutch.fetcher.TestFetcher\r\n    [junit] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 17.805 sec\r\n\r\n    [junit] Running org.apache.nutch.urlfilter.fast.TestFastURLFilter\r\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.36 sec\r\n\r\n    [junit] Running org.apache.nutch.parse.tika.TestPdfParser\r\n    [junit] Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 11.974 sec\r\n\r\n    [junit] Running org.apache.nutch.parse.tika.TestImageMetadata\r\n    [junit] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.113 sec\r\n\r\n    [junit] Running org.apache.nutch.parse.feed.TestFeedParser\r\n    [junit] Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.369 sec\r\n\r\n    [junit] Running org.apache.nutch.crawl.TestInjector\r\n    [junit] Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 6.15 sec\r\n\n\nWe could try to speed up at least some of these long-running tests.",
        "Issue Links": []
    },
    "NUTCH-2772": {
        "Key": "NUTCH-2772",
        "Summary": "Debugging parse filter to show serialized DOM tree",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Feb/20 16:13",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "30/Apr/20 08:29",
        "Description": "A tool to show the DOM tree (eg. serialized as XML/HTML) might be helpful for debugging, eg., see NUTCH-2769. The DOM tree is available in the parse plugins and is also passed to the HtmlParseFilter plugins. We could provide a parsefilter-debug plugin which logs the DOM tree and add the serialized string representation to the parse data.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/500"
        ]
    },
    "NUTCH-2773": {
        "Key": "NUTCH-2773",
        "Summary": "SegmentReader (-dump or -get): show HTML content as UTF-8",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "segment",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "28/Feb/20 10:07",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "13/Mar/20 09:09",
        "Description": "SegmentReader dumps resp. the output shown by -get is first converted to Java strings and then shown using UTF-8 as output encoding. The HTML page content is hold by the container class \"Content\" as byte[] and if another charset than UTF-8 is used as original page encoding, the output of SegmentReader may look flawed. The reader could use the encoding already detected by the parser (if available) and try to properly recode the HTML page content to UTF-8.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/501"
        ]
    },
    "NUTCH-2774": {
        "Key": "NUTCH-2774",
        "Summary": "Annotate methods implementing the Hadoop API by @Override",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "28/Feb/20 18:26",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "13/Mar/20 08:47",
        "Description": "Methods which implement or override the Hadoop API should always be annotated using @Override. This will help to avoid that methods are not called accidentally because the method name or signature do not match the current API.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/502"
        ]
    },
    "NUTCH-2775": {
        "Key": "NUTCH-2775",
        "Summary": "Fetcher to guarantee minimum delay even if robots.txt defines shorter Crawl-delay",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "fetcher,                                            robots",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "29/Feb/20 18:06",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "10/Apr/20 11:44",
        "Description": "Fetcher uses the amount of seconds defined by \"fetcher.server.delay\" to delay between successive requests to the same server. Servers can request a longer delay using the Crawl-Delay directive in the robots.txt. This was thought to allow servers to set a longer delay. However, I've recently seen a server requesting \"Crawl-Delay: 1\". The delay is shorter than the default delay and Nutch may indeed now request one page per second. Later this server responds with \"HTTP 429 Too Many Request\". Stupid. What about ignoring Crawl-Delay values shorter than the configured default delay or a configurable minimum delay?\nI've already seen the same issue using a different crawler architecture.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/506"
        ]
    },
    "NUTCH-2776": {
        "Key": "NUTCH-2776",
        "Summary": "Fetcher to temporarily deduplicate followed redirects",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "20/Mar/20 18:52",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "30/Apr/20 08:43",
        "Description": "If fetcher follows redirect (http.redirect.max > 0), it may happen that many redirects of a site point to the same URL. In this situation, it might be good if fetcher could temporarily (for a configurable time period) deduplicate the redirect targets and skip all redirects except the first one. Typical examples of duplicated redirect targets are:\n\ninstead of responding with HTTP status 404:\n\n/\r\n/resource-not-found\r\n/search/\r\n/404\r\n/error/not-found\r\n/err/notfound.html\n\na page to accept/decline cookies\n\n/cookie_usage.php",
        "Issue Links": [
            "/jira/browse/NUTCH-1150",
            "https://github.com/apache/nutch/pull/505"
        ]
    },
    "NUTCH-2777": {
        "Key": "NUTCH-2777",
        "Summary": "Upgrade to Hadoop 3.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "build",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "23/Mar/20 06:58",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "10/Apr/20 11:45",
        "Description": "Hadoop 3.0.0 has been released in December 2017, the number of \"legacy\" clusters running on Hadoop 2.x should go down now\na user reported that Nutch and Hadoop 3.1.3 work well on Windows",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/507"
        ]
    },
    "NUTCH-2778": {
        "Key": "NUTCH-2778",
        "Summary": "indexer-elastic to properly log errors",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "15/Apr/20 20:16",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "28/Apr/20 15:40",
        "Description": "While verifying the solution of NUTCH-2757, it turned out that no errors are indicated if authentication fails because of wrong credentials. The indexing job succeeds and no errors are logged.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/509"
        ]
    },
    "NUTCH-2779": {
        "Key": "NUTCH-2779",
        "Summary": "Upgrade to Tika 1.24.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Apr/20 12:04",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "24/Apr/20 07:08",
        "Description": "Tika 1.24.1 should be released soon. I've upgraded Nutch to use the release candidate: all unit tests pass and processing PDFs, MP3s, etc. works. I'll open a PR but we need to wait for the final release of 1.24.1",
        "Issue Links": []
    },
    "NUTCH-2780": {
        "Key": "NUTCH-2780",
        "Summary": "Upgrade index-solr to use Solr 8.5.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Apr/20 12:12",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "28/Apr/20 07:35",
        "Description": "The indexer-solr plugin should be upgraded to be based on the latest Solr version (currently, 8.5.1)",
        "Issue Links": [
            "/jira/browse/NUTCH-2751",
            "/jira/browse/NUTCH-2752",
            "/jira/browse/NUTCH-2752"
        ]
    },
    "NUTCH-2781": {
        "Key": "NUTCH-2781",
        "Summary": "Increase default Java heap size",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "runtime",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "23/Apr/20 07:18",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "28/Apr/20 08:04",
        "Description": "The Nutch run script (bin/nutch) sets a \"conservative\" Java heap size of 1000 MB. This default was defined 15 years ago. It's probably safe to increase the heap size to a value suitable to process more pages or larger documents. What about 4096 MB?\nNote this overlaps with NUTCH-2501 (Java heap size defined via mapred.child.java.opts in distributed mode).",
        "Issue Links": []
    },
    "NUTCH-2782": {
        "Key": "NUTCH-2782",
        "Summary": "protocol-http / lib-http: support TLSv1.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.18",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "24/Apr/20 05:48",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "14/Jul/20 10:52",
        "Description": "TLSv1.3 (since 2018) is not included in the list of supported protocols in lib-http (HttpBase.java, line 311). It should be added. Also the list of supported ciphers needs to be updated accordingly.",
        "Issue Links": [
            "/jira/browse/NUTCH-2794"
        ]
    },
    "NUTCH-2783": {
        "Key": "NUTCH-2783",
        "Summary": "Use (more) parametrized logging",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "24/Apr/20 13:49",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "28/Apr/20 07:54",
        "Description": "Nutch uses slf4j's Logger (since NUTCH-851), but there are still many places where parametrized logging is not used or parameters are needless converted to strings before the call. This issue aims to improve the situation for commonly used tools and plugins in order to simplify code and improve the logging performance. A PR is on the way.",
        "Issue Links": []
    },
    "NUTCH-2784": {
        "Key": "NUTCH-2784",
        "Summary": "Add tool to list Nutch and Hadoop properties",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.17",
        "Component/s": "tool",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Apr/20 08:47",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "30/Apr/20 09:14",
        "Description": "Nutch properties are defined in nutch-default.xml but can be redefined (overridden) in nutch-site.xml or from command-line (-Dproperty=value). In addition, property definitions can include other properties (${property.name}) which makes it sometimes hard to figure out what the actual value of a property is.\nIn short, a command-line tool which lists all properties and the configured values could be useful.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/518"
        ]
    },
    "NUTCH-2785": {
        "Key": "NUTCH-2785",
        "Summary": "FreeGenerator: command-line option to define number of generated fetch lists",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "28/Apr/20 17:50",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "05/May/20 13:57",
        "Description": "While Generator allows to specify the number of generated fetch lists using the \"-numFetchers\" command-line option, FreeGenerator does not provide such an option. It uses the value of \"mapreduce.job.maps\" (2 by default), also in local mode where Generator always creates only one single fetch list.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/519"
        ]
    },
    "NUTCH-2786": {
        "Key": "NUTCH-2786",
        "Summary": "TrustManager methods do not have certificate validation logic",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.16",
        "Fix Version/s": "None",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Md Mahir Asef Kabir",
        "Created": "04/May/20 03:20",
        "Updated": "19/Aug/22 12:51",
        "Resolved": "19/Aug/22 12:51",
        "Description": "Vulnerability Description: In \u201csrc/plugin/protocol-httpclient/src/java/org/apache/nutch/protocol/httpclient/DummyX509TrustManager.java\u201d overridden TrustManager methods (i.e. checkClientTrusted and checkServerTrusted) do not have validation logic for certificates.\n\n\nReason it\u2019s vulnerable: It is vulnerable because DummyX509TrustManager implements X509TrustManager and it overrides the standard TrustManager methods (i.e. checkClientTrusted and checkServerTrusted) to do nothing but returning hard-coded true. Certificate validation is expected to be handled by these methods. Doing nothing means no verification.\n\n\nSuggested Fix: Adding necessary certificate verification logic in the overridden methods. This is an example code showing a format that can be used and modified appropriately to implement the certificate validation logic - https://paste.ubuntu.com/p/jWtH2yTNR8/ .\n\n\nFeedback: Please select any of the options down below to help us get an idea about how you felt about the suggestion -\n\n\nLiked it and will make the suggested changes\nLiked it but happy with the existing version\nDidn\u2019t find the suggestion helpful",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/524"
        ]
    },
    "NUTCH-2787": {
        "Key": "NUTCH-2787",
        "Summary": "CrawlDb JSON dump does not export metadata primitive data types correctly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.17",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Patrick M\u00e9zard",
        "Created": "04/Jun/20 13:28",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "10/Jun/20 18:35",
        "Description": "To reproduce:\n\nActivate scoring-depth plugin\nCreate a new crawldb from a seed URL:\nDump the crawldb as json\nLook at the json\n\n\n\r\n$ nutch inject crawl/crawldb seeds.txt\r\n$ rm -rf out; nutch readdb crawl/crawldb -dump out -format json\r\n$ cat out/part-r-00000 | head -1 | python -m json.tool\r\n{\r\n    \"url\": \"http://example.com/\",\r\n    \"statusCode\": 1,\r\n    \"statusName\": \"db_unfetched\",\r\n    \"fetchTime\": \"Thu Jun 04 15:19:02 CEST 2020\",\r\n    \"modifiedTime\": \"Thu Jan 01 01:00:00 CET 1970\",\r\n    \"retriesSinceFetch\": 0,\r\n    \"retryIntervalSeconds\": 2592000,\r\n    \"retryIntervalDays\": 30,\r\n    \"score\": 1.0,\r\n    \"signature\": \"null\",\r\n    \"metadata\": {\r\n        \"_depth_\": {},\r\n        \"_maxdepth_\": {}\r\n    }\r\n}\n\nKO => `_depth` and `maxdepth_` are not integer.\nThe fields are correct in the crawldb, as shown by a CSV dump:\n\n\r\n$ rm -rf out; nutch readdb crawl/crawldb -dump out -format csv\r\n$ cat out/part-r-00000 \r\nUrl,Status code,Status name,Fetch Time,Modified Time,Retries since fetch,Retry interval seconds,Retry interval days,Score,Signature,Metadata\r\n\"http://example.com/\",1,\"db_unfetched\",Thu Jun 04 15:19:02 CEST 2020,Thu Jan 01 01:00:00 CET 1970,0,2592000.0,30.0,1.0,\"null\",\"_depth_:1|||_maxdepth_:5|||\" \n\nCode is here:\nhttps://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/crawl/CrawlDbReader.java#L269\nI do not know Java very well but I think it comes from IntWritable & co not being POJO types (or at least not the way we want them).\nOne fix might be to:\n\nMap all primitive type Writable classes to some function casting the base interface and calling \"get\" (may boxing the value as well).\nCall that in the metadata conversion loop.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/531"
        ]
    },
    "NUTCH-2788": {
        "Key": "NUTCH-2788",
        "Summary": "ParseData: improve presentation of Metadata in method toString()",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "metadata,                                            parser",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Jun/20 09:28",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "10/Jun/20 18:43",
        "Description": "See NUTCH-2567:\nI would also suggest making the output of Metadata::toString more readable(for instance by adding a newline before each new metadata value). It would have made this bug way easier to spot inside the output of the parsechecker.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/529"
        ]
    },
    "NUTCH-2789": {
        "Key": "NUTCH-2789",
        "Summary": "Documentation: update links to point to cwiki",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "docker,                                            documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "09/Jun/20 10:05",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "10/Jun/20 18:44",
        "Description": "The Docker README still contains links pointing to the old Nutch wiki and should point to the new Apache Nutch cwiki.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/530"
        ]
    },
    "NUTCH-2790": {
        "Key": "NUTCH-2790",
        "Summary": "CSVIndexWriter does not escape leading quotes properly",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.17",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Patrick M\u00e9zard",
        "Created": "09/Jun/20 14:52",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "10/Jun/20 18:28",
        "Description": "There is an off-by-one error in https://github.com/apache/nutch/blob/master/src/plugin/indexer-csv/src/java/org/apache/nutch/indexwriter/csv/CSVIndexWriter.java#L408\nThe nextQuoteChar==0 case is skipped and the value serialized verbatim instead of being escaped.\nI will send a PR after the ticket creation.",
        "Issue Links": []
    },
    "NUTCH-2791": {
        "Key": "NUTCH-2791",
        "Summary": "domainstats, protocolstats and crawlcomplete do not handle GCS URLs",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "util",
        "Assignee": null,
        "Reporter": "Patrick M\u00e9zard",
        "Created": "09/Jun/20 15:35",
        "Updated": "28/Jan/21 13:16",
        "Resolved": "11/Jun/20 11:26",
        "Description": "I am running Nutch in GCP Dataproc. The domainstats, protocolstats and crawlcomplete commands do not resolve crawldb paths correctly from GCS URLs.\nAlso:\n\nprotocolstats has an off-by-one error when resolving the numReducers argument\ncrawlcomplete -inputDirs is inconsistent with other command: where domainstats expect directories containing a \"current\" path, crawlcomplete looks for directories containing a \"crawldb/current\" path.\n\nI will send a PR after creating the issue.",
        "Issue Links": []
    },
    "NUTCH-2792": {
        "Key": "NUTCH-2792",
        "Summary": "nutch index -params is only used in Solr indexer",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.20",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Patrick M\u00e9zard",
        "Created": "10/Jun/20 08:22",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "`nutch index` help displays:\n\n\r\n General options:\r\n...\r\n -params k1=v1&k2=v2... parameters passed to indexer plugins\r\n (via property indexer.additional.params)\n\nThe option does nothing when used with CSV or dummy indexers. Looking at the code, the property is defined in:\nhttps://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/indexer/IndexerMapReduce.java#L78\nwhich is only used in:\nhttps://github.com/apache/nutch/blob/master/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrIndexWriter.java#L141\nSeveral possibilities:\n\nDrop the parameter from the help. Does not break backward compatibility.\nMove the -params handling in IndexWriters.java and add them to IndexWriterParams of every indexer. Not too impactful but not super clean either: the parameters are not \"namespaced\" per indexer, if someone uses multiple indexers there may be parameter collisions.\nRefactor the way these parameters are passed, to prefix them with target indexer. Would break backward compatibility. In that case, it would be good to change the format completely: turn -params into -param, allow multiple values to be passed and forget the '=/&' syntax (which does not handle escaping anyway).\n\nNot sure how much this parameter is used. I would have used it to configure the output path for indexer-csv or indexer-dummy.",
        "Issue Links": []
    },
    "NUTCH-2793": {
        "Key": "NUTCH-2793",
        "Summary": "CSV indexer does not work in distributed mode",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Patrick M\u00e9zard",
        "Created": "10/Jun/20 12:01",
        "Updated": "25/Nov/22 13:58",
        "Resolved": null,
        "Description": "Reasons are discussed in https://issues.apache.org/jira/browse/NUTCH-1541?focusedCommentId=13797768&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-13797768 and following comments.\nTo summarize, the indexer interface is not aware of tasks so it cannot generate unique output name per reducers.\nBut it seems achievable because IndexWriters initialize each writer with calls to 2 open functions:\n\nOne passing the general configuration and a \"name\"\nThe second to pass indexer parameters\n\nhttps://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/indexer/IndexWriters.java#L214\nFortunately, \"name\" is generated by calling getUniqueFile which does exactly what we want:\nhttps://github.com/apache/nutch/blob/master/src/java/org/apache/nutch/indexer/IndexerOutputFormat.java#L43\nI propose we use it instead of \"nutch.csv\" as CSVIndexWriter output file name. This is a breaking change because it modifies the output name but allows the indexer to work in distributed mode.\nPR will follow the ticket creation.",
        "Issue Links": []
    },
    "NUTCH-2794": {
        "Key": "NUTCH-2794",
        "Summary": "Add additional ciphers to HTTP base's default cipher suite",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.17",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "16/Jun/20 12:47",
        "Updated": "28/Jan/21 13:15",
        "Resolved": "17/Jun/20 11:23",
        "Description": "More sites switch to stronger cipher suites to support and lib-http stays behind./\nThis ticket adds some cipher suites and enables protocol-http to crawl affected sites.",
        "Issue Links": [
            "/jira/browse/NUTCH-2782"
        ]
    },
    "NUTCH-2795": {
        "Key": "NUTCH-2795",
        "Summary": "CrawlDbReader: compress CrawlDb dumps if configured",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.19",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "06/Jul/20 11:40",
        "Updated": "21/Aug/22 11:47",
        "Resolved": "21/Aug/22 10:38",
        "Description": "The dumps of CrawlDbReader (text, CSV, JSON) are not compressed given the configured file output compression. E.g., if running\n\n$> bin/nutch readdb \\\r\n       -Dmapreduce.output.fileoutputformat.compress=true  \\\r\n       -Dmapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.BZip2Codec \\\r\n       crawldb/ -dump crawldb.dump -format json\r\n\n\nthe output should be compressed using bzip2.\nSee the Hadoop class FileOutputFormat and the implementation in TextOutputFormat.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/746"
        ]
    },
    "NUTCH-2796": {
        "Key": "NUTCH-2796",
        "Summary": "Upgrade to crawler-commons 1.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.18",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "06/Jul/20 11:53",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "14/Jul/20 10:49",
        "Description": "Crawler-commons 1.1 has been released. We should upgrade.",
        "Issue Links": [
            "/jira/browse/NUTCH-2730"
        ]
    },
    "NUTCH-2797": {
        "Key": "NUTCH-2797",
        "Summary": "Update Miredot license for REST API documentation creation",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.20",
        "Component/s": "build,                                            REST_api",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "06/Jul/20 12:45",
        "Updated": "21/Aug/22 10:32",
        "Resolved": null,
        "Description": "The Miredot REST API documentation (see NUTCH-1800) failed to build for Nutch 1.17 because of \"License key expired on 30 Oct 2019\". Should be fixed by updating the license. The 1.17 REST docs should be created and linked on the javadoc page. Note that the REST API hasn't changed recently and the 1.16 docs can also be used for 1.17.",
        "Issue Links": []
    },
    "NUTCH-2798": {
        "Key": "NUTCH-2798",
        "Summary": "Nutch v2.4 Not Able to crawl after javax.faces.viewstate",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Trivial",
        "Resolution": "Not A Bug",
        "Affects Version/s": "2.4",
        "Fix Version/s": "None",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Mihir Sharma",
        "Created": "06/Jul/20 14:53",
        "Updated": "15/Jul/20 08:39",
        "Resolved": "15/Jul/20 08:39",
        "Description": "Nutch v2.4 Not crawling The html page After input tag with name javax.faces.viewstate it is crawling before this tag but unable to go ahead after this javax viewstate which is having a lot special character.\nThis page is having different tabs, Current crawler is fetching information till date(\nDate Published: 06/30/2020 09:00 PM) After that it is unable to fetch from\u00a0Assembly Bill No. 103\u00a0which is title\ni m crawling this site:\u00a0http://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=201920200AB103\n\u00a0\n\n\u00a0\nThis is the output i am getting after crawling.",
        "Issue Links": []
    },
    "NUTCH-2799": {
        "Key": "NUTCH-2799",
        "Summary": "Add .asf.yaml file",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "08/Jul/20 14:29",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "02/Aug/20 11:09",
        "Description": "See https://cwiki.apache.org/confluence/display/INFRA/git+-+.asf.yaml+features",
        "Issue Links": []
    },
    "NUTCH-2800": {
        "Key": "NUTCH-2800",
        "Summary": "Outdated information in documentation about catch all user agent",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "documentation",
        "Assignee": null,
        "Reporter": "Jay",
        "Created": "09/Jul/20 02:25",
        "Updated": "09/Jul/20 03:02",
        "Resolved": null,
        "Description": "It's mentioned on this page\u00a0http://nutch.apache.org/bot.html\u00a0that all Nutch based crawlers will respond to the user agent name \"Nutch\" irrespective of what the actual user agent name(s) have been set through the conf (nutch-site.xml).\u00a0\nThe page recommends that a webmaster can ban all Nutch based crawlers by simply putting this in robots.txt file.\nUser-agent: Nutch\nDisallow: /\nI tested crawling a site with Nutch 1.6 variant (common crawl fork) with another user agent name with a site (\u00a0https://store.stockcharts.com/robots.txt\u00a0) containing this in robots.txt and Nutch allowed me to fetch the page so this catch-all type user agent isn't working and the documentation should be updated to reflect this change in behavior.",
        "Issue Links": []
    },
    "NUTCH-2801": {
        "Key": "NUTCH-2801",
        "Summary": "RobotsRulesParser command-line checker to use http.robots.agents as fall-back",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "checker,                                            robots",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jul/20 13:13",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "03/Aug/20 19:07",
        "Description": "The RobotsRulesParser command-line tool, used to check a list of URLs against one robots.txt file, should use the value of the property http.robots.agents as fall-back if no user agent names are explicitly given as command-line argument. In this case it should behave same as the robots.txt parser, looking first for http.agent.name, then for other names listed in http.robots.agents, finally picking the rules for User-agent: *\n\n$> cat robots.txt\r\nUser-agent: Nutch\r\nAllow: /\r\nUser-agent: *\r\nDisallow: /\r\n\r\n$> bin/nutch org.apache.nutch.protocol.RobotRulesParser \\\r\n      -Dhttp.agent.name=mybot \\\r\n      -Dhttp.robots.agents='nutch,goodbot' \\\r\n      robots.txt urls.txt \r\nTesting robots.txt for agent names: mybot,nutch,goodbot\r\nnot allowed:    https://www.example.com/\r\n\n\nThe log message \"Testing ... for ...: mybot,nutch,goodbot\" is misleading. Only the name \"mybot\" is actually checked.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/537"
        ]
    },
    "NUTCH-2802": {
        "Key": "NUTCH-2802",
        "Summary": "Replace blacklist/whitelist by more inclusive and precise terminology",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "configuration,                                            plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Jul/20 17:33",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The terms blacklist and whitelist should be replaced by a more inclusive and more precise terminology, see the proposal and discussion on the @dev mailing list (1, 2).\nThis is an umbrella issue, subtasks to be opened for individual plugins and configuration properties.",
        "Issue Links": []
    },
    "NUTCH-2803": {
        "Key": "NUTCH-2802 Replace blacklist/whitelist by more inclusive and precise terminology",
        "Summary": "Rename property http.robot.rules.whitelist",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.18",
        "Component/s": "configuration,                                            robots",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jul/20 17:45",
        "Updated": "19/Aug/22 12:54",
        "Resolved": "19/Aug/22 12:54",
        "Description": "As part of NUTCH-2802 the property http.robot.rules.whitelist should be renamed.\nSee the definition of http.robot.rules.whitelist:\nComma separated list of hostnames or IP addresses to ignore robot rules parsing for. Use with care and only if you are explicitly allowed by the site owner to ignore the site's robots.txt!",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/539"
        ]
    },
    "NUTCH-2804": {
        "Key": "NUTCH-2802 Replace blacklist/whitelist by more inclusive and precise terminology",
        "Summary": "Rename blacklist/whitelist in configuration of subcollection plugin",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jul/20 17:57",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "As part of NUTCH-2802 the element names (\"blacklist\"/\"whitelist\") in the file conf/subcollection.xml to configure inclusion/exclusion of documents by URL into a \"subcollection\" should be renamed. Also variables names in the Java classes should reflect this change of terminology.",
        "Issue Links": []
    },
    "NUTCH-2805": {
        "Key": "NUTCH-2802 Replace blacklist/whitelist by more inclusive and precise terminology",
        "Summary": "Rename plugin urlfilter-domainblacklist",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.18",
        "Component/s": "plugin,                                            urlfilter",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jul/20 18:06",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "18/Aug/20 09:52",
        "Description": "As part of NUTCH-2802 the plugin urlfilter-domainblacklist should be renamed including variable names in Java classes and the file names of configuration files.",
        "Issue Links": []
    },
    "NUTCH-2806": {
        "Key": "NUTCH-2806",
        "Summary": "Nutch can't parse links",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Won't Fix",
        "Affects Version/s": "2.4",
        "Fix Version/s": "None",
        "Component/s": "parser",
        "Assignee": null,
        "Reporter": "lina dziri",
        "Created": "10/Jul/20 22:09",
        "Updated": "21/Jun/22 11:06",
        "Resolved": "21/Jun/22 11:06",
        "Description": "Testing with the following site: https://www.algeriahome.com , nutch only parse links that does contain the base url. \n Tried tika as parser, tried to update db.max.outlinks.per.page to -1, tried practically every comments about detecting all the links, doubted urlfilter or regex-normalizer so it was disabled but having the same results. \n each time I rebuild nutch and test the parser, it gives the same urls count arround 378. \n Can somebody help out to fix this.",
        "Issue Links": []
    },
    "NUTCH-2807": {
        "Key": "NUTCH-2807",
        "Summary": "SitemapProcessor to warn that ignoring robots.txt affects detection of sitemaps",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "robots,                                            sitemap",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Jul/20 12:04",
        "Updated": "17/Dec/21 09:09",
        "Resolved": "17/Dec/21 08:59",
        "Description": "Ignoring the robots.txt causes as a site effect that no sitemaps can be detected via robots.txt.\nSitemapProcessor should log a warning if robots.txt is ignored by configuration (NUTCH-1927/NUTCH-2803).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/710"
        ]
    },
    "NUTCH-2808": {
        "Key": "NUTCH-2808",
        "Summary": "Document side effects of ignoring robots.txt",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "documentation,                                            robots",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Jul/20 12:05",
        "Updated": "17/Dec/21 09:09",
        "Resolved": "17/Dec/21 08:57",
        "Description": "(see NUTCH-1927 and NUTCH-2803)\nThe aim of NUTCH-1927 was to make it possible to ignore the robots.txt for a defined set of hosts/domains. Ignoring the robots.txt entirely has some site effects which should be documented:\n\nundesired content (duplicates, private pages, etc.) may get indexed\nthe Crawl-Delay is ignored\nno sitemaps are detected (cf. NUTCH-2807)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/711"
        ]
    },
    "NUTCH-2809": {
        "Key": "NUTCH-2290 Update licenses of bundled libraries",
        "Summary": "Upgrade any23 plugin dependency to 2.4",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Jul/20 10:21",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "18/Nov/20 03:10",
        "Description": "Any23 2.4 has been released, the plugin any23 should be upgraded. This will also eliminate the a transitive dependency incompatible with the Apache license (see ANY23-372).",
        "Issue Links": [
            "/jira/browse/NUTCH-2830",
            "https://github.com/apache/nutch/pull/541",
            "https://github.com/apache/nutch/pull/553"
        ]
    },
    "NUTCH-2810": {
        "Key": "NUTCH-2810",
        "Summary": "FreeGenerator to actually apply configured number of fetch lists",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "generator,                                            segment",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "27/Jul/20 09:49",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "03/Aug/20 19:08",
        "Description": "NUTCH-2785 added a command-line option to set the number of fetch lists created by FreeGenerator included a check which resets the number of fetch lists to a single one in local mode. Actually this does not work, see https://github.com/apache/nutch/pull/519#issuecomment-664127212",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/542"
        ]
    },
    "NUTCH-2811": {
        "Key": "NUTCH-2811",
        "Summary": "Setup Github workflows for PR",
        "Type": "New Feature",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            test",
        "Assignee": null,
        "Reporter": "Shashanka Balakuntala Srinivasa",
        "Created": "31/Jul/20 05:21",
        "Updated": "28/Jan/21 14:04",
        "Resolved": "03/Aug/20 19:50",
        "Description": "Create a workflow[1] in the git repository for incoming PR merging with the master. As reference we can have this[2] which was setup for GORA.\u00a0\n\u00a0\n[1] -\u00a0https://docs.github.com/en/actions/reference/workflow-syntax-for-github-actions\n[2] -\u00a0https://github.com/apache/gora/tree/master/.github/workflows",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/543"
        ]
    },
    "NUTCH-2812": {
        "Key": "NUTCH-2815 Add Spotbugs target to build and address detected \"bugs\"",
        "Summary": "Methods returning array may expose internal representation",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "01/Aug/20 04:10",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "Returning a reference to a mutable object value stored in one of the object's fields exposes the internal representation of the object.  If instances are accessed by untrusted code, and unchecked changes to the mutable object would compromise security or other important properties, you will need to do something different. Returning a new copy of the object is better approach in many situations.\nFor example org.apache.nutch.fetcher.FetchNode.getOutlinks() may expose internal representation by returning FetchNode.outlinks\nThere are 11 such occurrences of this bug in the codebase.",
        "Issue Links": []
    },
    "NUTCH-2813": {
        "Key": "NUTCH-2813",
        "Summary": "MoreIndexingFilter - can't parse erroneous date - 2019-07-03T10:28:14",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Jakob Berlin",
        "Created": "03/Aug/20 13:42",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "16/Aug/20 19:08",
        "Description": "Especially when parsing images of type jpg I got:\nlogs/hadoop.log.2020-07-30:2020-07-30 05:54:56,952 WARN more.MoreIndexingFilter - https://www.url.com/image.jpg: can't parse erroneous date: 2019-07-03T10:28:14\nAdd mentioned format.\nSomeone should reactivate\u00a0NUTCH-1190 I think...",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/544"
        ]
    },
    "NUTCH-2814": {
        "Key": "NUTCH-2814",
        "Summary": "HttpDateFormat's internal time zone may change after parsing a date",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "07/Aug/20 16:05",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "18/Aug/20 09:43",
        "Description": "In the Common Crawl WARC files I've observed that the If-modified-since header is sent in varying time zones:\n\nIf-Modified-Since: Tue, 25 Feb 2020 03:33:21 MSK\r\nIf-Modified-Since: Sun, 22 Sep 2019 04:41:48 GMT\r\nIf-Modified-Since: Mon, 18 Nov 2019 12:06:19 KRAT\r\nIf-Modified-Since: Tue, 21 Jan 2020 02:10:22 UTC\r\nIf-Modified-Since: Fri, 18 Oct 2019 20:23:57 BST\r\nIf-Modified-Since: Sun, 20 Oct 2019 08:39:26 CEST\r\nIf-Modified-Since: Fri, 15 Nov 2019 12:56:38 EST\r\nIf-Modified-Since: Mon, 30 Mar 2020 09:10:33 GMT\r\nIf-Modified-Since: Mon, 30 Mar 2020 05:18:36 GMT\r\nIf-Modified-Since: Fri, 28 Feb 2020 03:09:16 PST\r\nIf-Modified-Since: Thu, 21 Nov 2019 10:16:19 YEKT\r\nIf-Modified-Since: Thu, 14 Nov 2019 18:01:05 EET\r\nIf-Modified-Since: Thu, 14 Nov 2019 16:46:43 UTC\r\nIf-Modified-Since: Sun, 17 Nov 2019 13:14:28 UTC\r\nIf-Modified-Since: Tue, 25 Feb 2020 21:46:10 GMT\r\nIf-Modified-Since: Wed, 16 Oct 2019 19:03:31 UTC\r\nIf-Modified-Since: Thu, 14 Nov 2019 09:07:13 EST\r\nIf-Modified-Since: Thu, 09 Apr 2020 12:21:53 EEST\r\nIf-Modified-Since: Sat, 28 Mar 2020 19:08:52 CET\r\nIf-Modified-Since: Sun, 23 Feb 2020 12:22:46 CET\r\nIf-Modified-Since: Mon, 21 Oct 2019 03:18:16 PDT\r\nIf-Modified-Since: Fri, 15 Nov 2019 05:41:44 UTC\r\nIf-Modified-Since: Thu, 09 Apr 2020 21:01:32 CEST\r\nIf-Modified-Since: Wed, 11 Dec 2019 11:18:28 KRAT\r\nIf-Modified-Since: Tue, 22 Oct 2019 18:55:54 GMT\r\n\n\nThis actually happens because the time zone of HttpDateFormat's internal SimpleDateFormatter may change when a date is parsed. The next formatting uses the time zone of the last parsed date.\nThe usage of \"GMT\" as time zone is specified in sec. 7.1.1.1 of RFC 7231.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/546"
        ]
    },
    "NUTCH-2815": {
        "Key": "NUTCH-2815",
        "Summary": "Add Spotbugs target to build and address detected \"bugs\"",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.20",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "08/Aug/20 08:16",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "(thanks to lewismc for the suggestion and the patch to run Spotbugs)\n\nadd Spotbugs static code analysis to the Nutch build scripts (called on-demand as ant target \"spotbugs)\n\n\nfix detected \"bugs\" (at least, those classified as high priority). Sub-tasks to be opened to address groups of similar \"bugs\".",
        "Issue Links": [
            "/jira/browse/NUTCH-1807"
        ]
    },
    "NUTCH-2816": {
        "Key": "NUTCH-2815 Add Spotbugs target to build and address detected \"bugs\"",
        "Summary": "Add Spotbugs target to ant build",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "08/Aug/20 08:20",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "11/Aug/20 07:38",
        "Description": "Add Spotbugs static code analysis to the Nutch build scripts.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/547"
        ]
    },
    "NUTCH-2817": {
        "Key": "NUTCH-2815 Add Spotbugs target to build and address detected \"bugs\"",
        "Summary": "Avoid check for equality of URL path and file part using ==/!=",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "08/Aug/20 08:46",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "11/Aug/20 07:39",
        "Description": "In the plugins protocol-file and protocol-ftp Spotbugs warns about \"Comparison of String objects using == or !=\". This is actual a false positive since path and file point to the same object for URLs with no query part (a query is not supported for file:// and ftp://). To check for existence of the query part would be better to understand and also avoids the Spotbugs warning.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/548"
        ]
    },
    "NUTCH-2818": {
        "Key": "NUTCH-2818",
        "Summary": "Ant build: upgrade Apache Rat report task",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Aug/20 16:40",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "18/Aug/20 09:49",
        "Description": "The Ant target \"rat-sources\" which creates a report about license usage in Nutch sources should be upgraded to use the latest version of the Apache Rat Ant Task Library .",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/549"
        ]
    },
    "NUTCH-2819": {
        "Key": "NUTCH-2815 Add Spotbugs target to build and address detected \"bugs\"",
        "Summary": "Move spotbugs \"installation\" directory to avoid that spotbugs is shipped in Nutch runtime",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": "Shashanka Balakuntala Srinivasa",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Aug/20 16:57",
        "Updated": "01/Feb/21 00:35",
        "Resolved": "01/Feb/21 00:06",
        "Description": "With NUTCH-2816 the Spotbugs tool is \"installed\" in lib/. However, files in lib/ are copied to build/ and runtime/. To avoid that the spotbugs jars are shipped in runtime and eventually also releases, the spotbugs installation folder should be moved into a different directory.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/565"
        ]
    },
    "NUTCH-2820": {
        "Key": "NUTCH-2820",
        "Summary": "Review sample files used in any23 unit tests",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.20",
        "Component/s": "plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Aug/20 16:57",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The sample files used by unit tests of the any23 plugin include content not applicable to the Apache license. These should removed or stripped to a minimal snippet (mostly HTML markup).",
        "Issue Links": []
    },
    "NUTCH-2821": {
        "Key": "NUTCH-2821",
        "Summary": "Deduplicate licenses in LICENSE.txt file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Aug/20 16:59",
        "Updated": "19/Aug/22 12:56",
        "Resolved": "19/Aug/22 12:56",
        "Description": "The LICENSE.txt contains duplicate licenses (esp. the Apache license) which should be removed. Thanks @jmclean for the hint. Cf. NUTCH-723 which already discussed the topic.",
        "Issue Links": []
    },
    "NUTCH-2822": {
        "Key": "NUTCH-2822",
        "Summary": "Split the LICENSE.txt file into two files for source resp. binary releases",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Aug/20 17:00",
        "Updated": "19/Aug/22 13:42",
        "Resolved": "19/Aug/22 12:57",
        "Description": "According to the https://infra.apache.org/licensing-howto.html#binary the LICENSE.txt file needs to be split into two files for source resp. binary releases. Steps:\n\ncopy LICENSE.txt into LICENSE-binary (needs a review anyway, see NUTCH-2290)\nstrip licenses of dependencies (jar files) not included in the source release from LICENSE.txt\nadapt the build scripts for packaging license files as necessary",
        "Issue Links": []
    },
    "NUTCH-2823": {
        "Key": "NUTCH-2823",
        "Summary": "IllegalStateException in IndexWriters.describe() when validating url param for SolrIndexer",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16,                                            1.17",
        "Fix Version/s": "1.18",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Joe Gilvary",
        "Created": "13/Aug/20 11:08",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "14/Sep/20 12:06",
        "Description": "The string validation for the IndexWriters.describe() fails when the value in index-writers.xml is too long.\nI encountered the exception when using three comma-separated URL values in a config that worked for Nutch 1.15.The schema doesn't allow multiple values, but the documentation says a comma-separated list works.\nIndexing ran without the exception when I changed to use only one host's URL (Solr Cloud). Sebastian duplicated the error with a long string value for the param, so it's not directly due to the comma separated values.\nWhile googling I found this thread in the archives where Markus encountered it going from 1.15 to 1.16:\nmail-archives.apache.org/mod_mbox/nutch-user/201910.mbox/<05eda22b-14b2-309f-3bc7-d6d85c218235@googlemail.com>\nI also found a change in 1.16 that might be relevant: NUTCH-2602\nhttps://issues.apache.org/jira/browse/NUTCH-2602\nMy stack trace:\njava.lang.Exception: java.lang.IllegalStateException: text width is less than 1, was <-26>\n {{ at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492)}}\n {{ at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:559)}}\nCaused by: java.lang.IllegalStateException: text width is less than 1, was <-26>\n {{ at org.apache.commons.lang3.Validate.validState(Validate.java:829)}}\n {{ at de.vandermeer.skb.interfaces.transformers.textformat.Text_To_FormattedText.transform(Text_To_FormattedText.java:215)}}\n {{ at de.vandermeer.asciitable.AT_Renderer.renderAsCollection(AT_Renderer.java:250)}}\n {{ at de.vandermeer.asciitable.AT_Renderer.render(AT_Renderer.java:128)}}\n {{ at de.vandermeer.asciitable.AsciiTable.render(AsciiTable.java:191)}}\n {{ at org.apache.nutch.indexer.IndexWriters.describe(IndexWriters.java:326)}}\n {{ at org.apache.nutch.indexer.IndexerOutputFormat.getRecordWriter(IndexerOutputFormat.java:45)}}\n {{ at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.<init>(ReduceTask.java:542)}}\n {{ at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:615)}}\n {{ at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390)}}\n {{ at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:347)}}\n {{ at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)}}\n {{ at java.util.concurrent.FutureTask.run(FutureTask.java:266)}}\n {{ at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}\n {{ at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}\n {{ at java.lang.Thread.run(Thread.java:748)}}\n\u00a0\n\u00a0Thanks,\n\u00a0Joe",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/551"
        ]
    },
    "NUTCH-2824": {
        "Key": "NUTCH-2824",
        "Summary": "urlnormalizer-basic to unescape percent-encoded host names",
        "Type": "Bug",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "plugin,                                            urlnormalizer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Aug/20 14:12",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "14/Sep/20 12:27",
        "Description": "BasicURLNormalizer should unescape percent-encoded characters in host names, similar as done in web browsers. Examples: https://example%2Ecom/ or https://www.0251-sachverst%c3%a4ndiger.de/",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/552"
        ]
    },
    "NUTCH-2825": {
        "Key": "NUTCH-2825",
        "Summary": "lib-selenium:  property webdriver.chrome.driver overwritten by selenium.grid.binary",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "21/Aug/20 15:30",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "(see https://stackoverflow.com/questions/63456514/nutch-selenium-interactive-plugin-ignores-the-chromedriver-configuration/63472368#63472368)\nThe property webdriver.chrome.driver is overwritten by the value of selenium.grid.binary\u00a0- is this the expected behavior? Needs documentation in any case as both properties are listed in nutch-default.xml, cf. 8f421a4.",
        "Issue Links": []
    },
    "NUTCH-2826": {
        "Key": "NUTCH-2826",
        "Summary": "Migrate Nutch Site from Apache CMS to Hugo",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "documentation,                                            site,                                            website",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "16/Sep/20 10:42",
        "Updated": "09/Jan/22 12:54",
        "Resolved": "24/Nov/21 17:07",
        "Description": "We need to migrate the Nutch site because the Apache CMS is reaching end-of-life, see the discussion about Nutch site migration on the @dev mailing list.\nWe decided to use Hugo to build the new site.\nSubtasks to be added for migrations steps.",
        "Issue Links": [
            "/jira/browse/NUTCH-2928",
            "/jira/browse/NUTCH-1999"
        ]
    },
    "NUTCH-2827": {
        "Key": "NUTCH-2826 Migrate Nutch Site from Apache CMS to Hugo",
        "Summary": "Migrate site repository",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "16/Sep/20 10:52",
        "Updated": "21/Jun/22 12:06",
        "Resolved": "21/Jun/22 12:06",
        "Description": "The new site repository is created (https://github.com/apache/nutch-site resp. https://gitbox.apache.org/repos/asf?p=nutch-site.git) and needs to be filled with content from the old SVN repository (https://svn.apache.org/repos/asf/nutch/cms_site).\nTo be decided:\n\nkeep commit history (in doubt: yes)\nif not: skip content not required for new site (eg. Java docs of older Nutch versions)",
        "Issue Links": []
    },
    "NUTCH-2828": {
        "Key": "NUTCH-2828",
        "Summary": "Nightly builds fail with ivy error \"Multiple artifacts of the module * are retrieved to the same file\"",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "10/Nov/20 10:52",
        "Updated": "28/Jan/21 14:03",
        "Resolved": "01/Dec/20 12:33",
        "Description": "Nightly builds fail since few weeks with an error while resolving ivy-managed dependencies: \n\n/home/jenkins/jenkins-agent/workspace/Nutch/Nutch-trunk/src/plugin/build-plugin.xml:230: impossible to ivy retrieve: java.lang.RuntimeException: problem during retrieve of org.apache.nutch#lib-htmlunit: java.lang.RuntimeException: Multiple artifacts of the module com.squareup.okio#okio;1.14.0 are retrieved to the same file! Update the retrieve pattern to fix this error.\r\n\n\nThe error is not reproducible:\n\nGithub PR builds (NUTCH-2811) succeed\nlocal builds on my dev machine also succeed",
        "Issue Links": []
    },
    "NUTCH-2829": {
        "Key": "NUTCH-2828 Nightly builds fail with ivy error \"Multiple artifacts of the module * are retrieved to the same file\"",
        "Summary": "Fix ant target \"clean-cache\"",
        "Type": "Sub-task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "10/Nov/20 11:05",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "13/Nov/20 18:13",
        "Description": "The ant target \"clean-cache\" (cleans the ivy cache) is broken:\n\n$> ant clean-cache\r\n...\r\nclean-cache:\r\n\r\nBUILD FAILED\r\n.../build.xml:1005: Problem: failed to create task or type antlib:org.apache.ivy.ant:cleancache\r\nCause: The name is undefined.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/555"
        ]
    },
    "NUTCH-2830": {
        "Key": "NUTCH-2830",
        "Summary": "Upgrade any23 to v2.4",
        "Type": "Task",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "any23",
        "Assignee": null,
        "Reporter": "Mauro Asprea",
        "Created": "11/Nov/20 13:59",
        "Updated": "17/Sep/21 11:22",
        "Resolved": "12/Nov/20 13:45",
        "Description": "As you can see here http://mail-archives.apache.org/mod_mbox/any23-user/202009.mbox/%3cpony-b7497055821405926d63668ab1112e0f108e2346-5e8ec124fb4d03c37875c1e6d7cf2ecf1cf1de38@user.any23.apache.org%3e, the Any23 v2.2 has issues extracting json-ld.\nCould this plugin get the latest release of the Any23 jar?",
        "Issue Links": [
            "/jira/browse/NUTCH-2809"
        ]
    },
    "NUTCH-2831": {
        "Key": "NUTCH-2831",
        "Summary": "Elastic indexer does not support SSL",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.19",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Gareth Owenson",
        "Created": "20/Nov/20 11:53",
        "Updated": "21/Jun/22 11:08",
        "Resolved": "21/Jun/22 11:08",
        "Description": "Using SSL for elastic is now mandatory for production, support for SSL and specifying the CA is not present at the moment.\nExamples here:\nhttps://www.elastic.co/guide/en/elasticsearch/client/java-rest/current/_encrypted_communication.html",
        "Issue Links": [
            "/jira/browse/NUTCH-2903"
        ]
    },
    "NUTCH-2832": {
        "Key": "NUTCH-2832",
        "Summary": "Create tutorial on sending Nutch logs to Elasticsearch",
        "Type": "New Feature",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "configuration,                                            deployment",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Nov/20 21:12",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "A while back I used to use Chukwa for log aggregation and analysis. Chukwa is now retired. \nI a bit of research into directly logging Log4j2 into Elasticsearch and came across log4j2-elasticsearch which looks pretty simple.\nI'm going to have a crack at implementing this functionality as a configuration option.",
        "Issue Links": []
    },
    "NUTCH-2833": {
        "Key": "NUTCH-2833",
        "Summary": "Upgrade to Tika 1.25",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "build,                                            parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "01/Dec/20 12:35",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "08/Dec/20 12:55",
        "Description": "Tika 1.25 is released, time to upgrade the Nutch dependencies.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/556"
        ]
    },
    "NUTCH-2834": {
        "Key": "NUTCH-2834",
        "Summary": "Deduplication mode via command line in crawl script",
        "Type": "Wish",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.18",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Jakob Berlin",
        "Created": "08/Dec/20 10:54",
        "Updated": "17/Dec/20 21:56",
        "Resolved": "17/Dec/20 21:56",
        "Description": "Add possibility to have a command line parameter in crawl script which controls deduplication mode.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/557"
        ]
    },
    "NUTCH-2835": {
        "Key": "NUTCH-2835",
        "Summary": "Upgrade commons-jexl from 2 --> 3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.18",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Dec/20 04:05",
        "Updated": "14/Jan/21 04:12",
        "Resolved": "17/Dec/20 16:56",
        "Description": "I am running into an issue with commons-jexl, which is basically described at \nhttps://stackoverflow.com/questions/54684259/java-lang-classnotfoundexception-org-apache-commons-jexl2-jexlcontext-while-run\nThe difference is that I experiencing this attempting to run Nutch on Tez instead of MapReduce. I've decided to upgrade commons-jexl from 2 --> 3. PR coming up!",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/558"
        ]
    },
    "NUTCH-2836": {
        "Key": "NUTCH-2836",
        "Summary": "Upgrade various commons dependencies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Dec/20 05:17",
        "Updated": "08/Jan/21 05:36",
        "Resolved": "08/Jan/21 04:43",
        "Description": "I ended up taking a look at some other dependencies. Specifically the following\n\n\r\n\t\t<dependency org=\"org.apache.commons\" name=\"commons-lang3\" rev=\"3.8.1\" conf=\"*->default\" />\r\n\t\t<dependency org=\"org.apache.commons\" name=\"commons-collections4\" rev=\"4.2\" conf=\"*->master\" />\r\n\t\t<dependency org=\"org.apache.httpcomponents\" name=\"httpclient\" rev=\"4.5.6\" conf=\"*->master\" />\r\n\t\t<dependency org=\"commons-codec\" name=\"commons-codec\" rev=\"1.11\" conf=\"*->default\" />\r\n\t\t<dependency org=\"org.apache.commons\" name=\"commons-compress\" rev=\"1.18\" conf=\"*->default\" />\r\n\n\nSome of them can be upgraded... I'll go ahead and do that right now.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/559"
        ]
    },
    "NUTCH-2837": {
        "Key": "NUTCH-2837",
        "Summary": "Update multiple dependencies",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "20/Dec/20 16:39",
        "Updated": "10/Jan/21 04:22",
        "Resolved": "08/Jan/21 18:02",
        "Description": "Began with a trivial upgrade of slf4j-api and slf4j-log4j12 dependencies to most recent stable. The upgrades then spread so I augmented the PR.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/560"
        ]
    },
    "NUTCH-2838": {
        "Key": "NUTCH-2838",
        "Summary": "Apache Tez integration",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "deployment,                                            runtime,                                            tez",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Dec/20 18:23",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "This is a parent epic under which all Tez integration tasks can be nested.",
        "Issue Links": [
            "/jira/browse/TEZ-4370",
            "https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=170267159"
        ]
    },
    "NUTCH-2839": {
        "Key": "NUTCH-2838 Apache Tez integration",
        "Summary": "Implement Tez counters in Injector job",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "injector,                                            tez",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Dec/20 18:40",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "When running the Injector job on Tez, counters are not populated. This makes sense as all existing counters are created using MapReduce framework Context objects. This presents a major issue however. Counters are a requirement as they are key to regular inspections of ongoing crawls, finding errors and debugging. The org.apache.tez.common.counters  package may offer a equivalent replacement. This issue will be investigated in this ticket.",
        "Issue Links": [
            "/jira/browse/TEZ-4371"
        ]
    },
    "NUTCH-2840": {
        "Key": "NUTCH-2840",
        "Summary": "Fix 'report-vulnerabilities' ant target in build.xml",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "08/Jan/21 07:40",
        "Updated": "01/Feb/21 00:35",
        "Resolved": "01/Feb/21 00:07",
        "Description": "I recently noticed (FOR THE FIRST TIME) the report-vulnerabilities target in build.xml\nWhen I invoked it, it was broken so I decided to fix it.\nPR coming up.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/561"
        ]
    },
    "NUTCH-2841": {
        "Key": "NUTCH-2841",
        "Summary": "Upgrade xercesImpl dependency",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.18",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Jan/21 18:42",
        "Updated": "13/Jan/21 19:13",
        "Resolved": "13/Jan/21 18:56",
        "Description": "Trivial update <dependency org=\"xerces\" name=\"xercesImpl\" rev=\"2.12.1\" />",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/562",
            "https://github.com/apache/nutch/pull/563"
        ]
    },
    "NUTCH-2842": {
        "Key": "NUTCH-2842",
        "Summary": "Fix Javadoc warnings, errors and add Javadoc check to Github Action and Jenkins",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "15/Jan/21 00:54",
        "Updated": "11/Feb/21 18:32",
        "Resolved": "11/Feb/21 17:28",
        "Description": "When attempting to push the 1.18 release candidate I see that Javadoc is again in need of some attention.\nThis time I would like to add a Javadoc check to Github Action and Jenkins such that incoming pull requests are failed if they raise or introduce new Javadoc warnings.\n\n\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:28: error: package org.apache.any23 does not exist\r\n  [javadoc] import org.apache.any23.Any23;\r\n  [javadoc]                        ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:29: error: package org.apache.any23.extractor does not exist\r\n  [javadoc] import org.apache.any23.extractor.ExtractionException;\r\n  [javadoc]                                  ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:30: error: package org.apache.any23.writer does not exist\r\n  [javadoc] import org.apache.any23.writer.BenchmarkTripleHandler;\r\n  [javadoc]                               ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:31: error: package org.apache.any23.writer does not exist\r\n  [javadoc] import org.apache.any23.writer.NTriplesWriter;\r\n  [javadoc]                               ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:32: error: package org.apache.any23.writer does not exist\r\n  [javadoc] import org.apache.any23.writer.TripleHandler;\r\n  [javadoc]                               ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:33: error: package org.apache.any23.writer does not exist\r\n  [javadoc] import org.apache.any23.writer.TripleHandlerException;\r\n  [javadoc]                               ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:80: error: cannot find symbol\r\n  [javadoc]     Any23Parser(String url, String htmlContent, String contentType, String... extractorNames) throws TripleHandlerException {\r\n  [javadoc]                                                                                                      ^\r\n  [javadoc]   symbol:   class TripleHandlerException\r\n  [javadoc]   location: class Any23Parser\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:100: error: cannot find symbol\r\n  [javadoc]     private void parse(String url, String htmlContent, String contentType, String... extractorNames) throws URISyntaxException, IOException, TripleHandlerException {\r\n  [javadoc]                                                                                                                                              ^\r\n  [javadoc]   symbol:   class TripleHandlerException\r\n  [javadoc]   location: class Any23Parser\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:52: error: package com.rometools.rome.feed.synd does not exist\r\n  [javadoc] import com.rometools.rome.feed.synd.SyndCategory;\r\n  [javadoc]                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:53: error: package com.rometools.rome.feed.synd does not exist\r\n  [javadoc] import com.rometools.rome.feed.synd.SyndContent;\r\n  [javadoc]                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:54: error: package com.rometools.rome.feed.synd does not exist\r\n  [javadoc] import com.rometools.rome.feed.synd.SyndEntry;\r\n  [javadoc]                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:55: error: package com.rometools.rome.feed.synd does not exist\r\n  [javadoc] import com.rometools.rome.feed.synd.SyndFeed;\r\n  [javadoc]                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:56: error: package com.rometools.rome.feed.synd does not exist\r\n  [javadoc] import com.rometools.rome.feed.synd.SyndPerson;\r\n  [javadoc]                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:57: error: package com.rometools.rome.io does not exist\r\n  [javadoc] import com.rometools.rome.io.SyndFeedInput;\r\n  [javadoc]                             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:218: error: cannot find symbol\r\n  [javadoc]   private void addToMap(ParseResult parseResult, SyndFeed feed,\r\n  [javadoc]                                                  ^\r\n  [javadoc]   symbol:   class SyndFeed\r\n  [javadoc]   location: class FeedParser\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:219: error: cannot find symbol\r\n  [javadoc]       String feedLink, SyndEntry entry, Content content) {\r\n  [javadoc]                        ^\r\n  [javadoc]   symbol:   class SyndEntry\r\n  [javadoc]   location: class FeedParser\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:286: error: cannot find symbol\r\n  [javadoc]   private static String stripTags(SyndContent c) {\r\n  [javadoc]                                   ^\r\n  [javadoc]   symbol:   class SyndContent\r\n  [javadoc]   location: class FeedParser\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:302: error: cannot find symbol\r\n  [javadoc]       SyndFeed feed, SyndEntry entry) {\r\n  [javadoc]       ^\r\n  [javadoc]   symbol:   class SyndFeed\r\n  [javadoc]   location: class FeedParser\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java:302: error: cannot find symbol\r\n  [javadoc]       SyndFeed feed, SyndEntry entry) {\r\n  [javadoc]                      ^\r\n  [javadoc]   symbol:   class SyndEntry\r\n  [javadoc]   location: class FeedParser\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPIndexingFilter.java:35: error: package com.maxmind.geoip2 does not exist\r\n  [javadoc] import com.maxmind.geoip2.DatabaseReader;\r\n  [javadoc]                          ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPIndexingFilter.java:36: error: package com.maxmind.geoip2 does not exist\r\n  [javadoc] import com.maxmind.geoip2.WebServiceClient;\r\n  [javadoc]                          ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPIndexingFilter.java:122: error: cannot find symbol\r\n  [javadoc]   WebServiceClient client = null;\r\n  [javadoc]   ^\r\n  [javadoc]   symbol:   class WebServiceClient\r\n  [javadoc]   location: class GeoIPIndexingFilter\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPIndexingFilter.java:124: error: cannot find symbol\r\n  [javadoc]   DatabaseReader reader = null;\r\n  [javadoc]   ^\r\n  [javadoc]   symbol:   class DatabaseReader\r\n  [javadoc]   location: class GeoIPIndexingFilter\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:25: error: package com.maxmind.geoip2 does not exist\r\n  [javadoc] import com.maxmind.geoip2.DatabaseReader;\r\n  [javadoc]                          ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:26: error: package com.maxmind.geoip2 does not exist\r\n  [javadoc] import com.maxmind.geoip2.WebServiceClient;\r\n  [javadoc]                          ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:27: error: package com.maxmind.geoip2.exception does not exist\r\n  [javadoc] import com.maxmind.geoip2.exception.GeoIp2Exception;\r\n  [javadoc]                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:28: error: package com.maxmind.geoip2.model does not exist\r\n  [javadoc] import com.maxmind.geoip2.model.InsightsResponse;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:29: error: package com.maxmind.geoip2.model does not exist\r\n  [javadoc] import com.maxmind.geoip2.model.CityResponse;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:30: error: package com.maxmind.geoip2.model does not exist\r\n  [javadoc] import com.maxmind.geoip2.model.ConnectionTypeResponse;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:31: error: package com.maxmind.geoip2.model does not exist\r\n  [javadoc] import com.maxmind.geoip2.model.CountryResponse;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:32: error: package com.maxmind.geoip2.model does not exist\r\n  [javadoc] import com.maxmind.geoip2.model.DomainResponse;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:33: error: package com.maxmind.geoip2.model does not exist\r\n  [javadoc] import com.maxmind.geoip2.model.IspResponse;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:34: error: package com.maxmind.geoip2.record does not exist\r\n  [javadoc] import com.maxmind.geoip2.record.City;\r\n  [javadoc]                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:35: error: package com.maxmind.geoip2.record does not exist\r\n  [javadoc] import com.maxmind.geoip2.record.Continent;\r\n  [javadoc]                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:36: error: package com.maxmind.geoip2.record does not exist\r\n  [javadoc] import com.maxmind.geoip2.record.Country;\r\n  [javadoc]                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:37: error: package com.maxmind.geoip2.record does not exist\r\n  [javadoc] import com.maxmind.geoip2.record.Location;\r\n  [javadoc]                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:38: error: package com.maxmind.geoip2.record does not exist\r\n  [javadoc] import com.maxmind.geoip2.record.Postal;\r\n  [javadoc]                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:39: error: package com.maxmind.geoip2.record does not exist\r\n  [javadoc] import com.maxmind.geoip2.record.RepresentedCountry;\r\n  [javadoc]                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:40: error: package com.maxmind.geoip2.record does not exist\r\n  [javadoc] import com.maxmind.geoip2.record.Subdivision;\r\n  [javadoc]                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:41: error: package com.maxmind.geoip2.record does not exist\r\n  [javadoc] import com.maxmind.geoip2.record.Traits;\r\n  [javadoc]                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:74: error: cannot find symbol\r\n  [javadoc]       NutchDocument doc, WebServiceClient client) throws UnknownHostException,\r\n  [javadoc]                          ^\r\n  [javadoc]   symbol:   class WebServiceClient\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:75: error: cannot find symbol\r\n  [javadoc]       IOException, GeoIp2Exception {\r\n  [javadoc]                    ^\r\n  [javadoc]   symbol:   class GeoIp2Exception\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:132: error: cannot find symbol\r\n  [javadoc]       NutchDocument doc, WebServiceClient client) throws UnknownHostException,\r\n  [javadoc]                          ^\r\n  [javadoc]   symbol:   class WebServiceClient\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:133: error: cannot find symbol\r\n  [javadoc]       IOException, GeoIp2Exception {\r\n  [javadoc]                    ^\r\n  [javadoc]   symbol:   class GeoIp2Exception\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:140: error: cannot find symbol\r\n  [javadoc]       NutchDocument doc, WebServiceClient client) throws UnknownHostException,\r\n  [javadoc]                          ^\r\n  [javadoc]   symbol:   class WebServiceClient\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:141: error: cannot find symbol\r\n  [javadoc]       IOException, GeoIp2Exception {\r\n  [javadoc]                    ^\r\n  [javadoc]   symbol:   class GeoIp2Exception\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:147: error: cannot find symbol\r\n  [javadoc]       NutchDocument doc, DatabaseReader reader) throws UnknownHostException,\r\n  [javadoc]                          ^\r\n  [javadoc]   symbol:   class DatabaseReader\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:148: error: cannot find symbol\r\n  [javadoc]       IOException, GeoIp2Exception {\r\n  [javadoc]                    ^\r\n  [javadoc]   symbol:   class GeoIp2Exception\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:159: error: cannot find symbol\r\n  [javadoc]       NutchDocument doc, DatabaseReader reader) throws UnknownHostException,\r\n  [javadoc]                          ^\r\n  [javadoc]   symbol:   class DatabaseReader\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:160: error: cannot find symbol\r\n  [javadoc]       IOException, GeoIp2Exception {\r\n  [javadoc]                    ^\r\n  [javadoc]   symbol:   class GeoIp2Exception\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:168: error: cannot find symbol\r\n  [javadoc]       NutchDocument doc, DatabaseReader reader) throws UnknownHostException,\r\n  [javadoc]                          ^\r\n  [javadoc]   symbol:   class DatabaseReader\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:169: error: cannot find symbol\r\n  [javadoc]       IOException, GeoIp2Exception {\r\n  [javadoc]                    ^\r\n  [javadoc]   symbol:   class GeoIp2Exception\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:178: error: cannot find symbol\r\n  [javadoc]       NutchDocument doc, DatabaseReader reader) throws UnknownHostException,\r\n  [javadoc]                          ^\r\n  [javadoc]   symbol:   class DatabaseReader\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-geoip/src/java/org/apache/nutch/indexer/geoip/GeoIPDocumentCreator.java:179: error: cannot find symbol\r\n  [javadoc]       IOException, GeoIp2Exception {\r\n  [javadoc]                    ^\r\n  [javadoc]   symbol:   class GeoIp2Exception\r\n  [javadoc]   location: class GeoIPDocumentCreator\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:45: error: package com.amazonaws.regions does not exist\r\n  [javadoc] import com.amazonaws.regions.RegionUtils;\r\n  [javadoc]                             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:46: error: package com.amazonaws.services.cloudsearchdomain does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchdomain.AmazonCloudSearchDomainClient;\r\n  [javadoc]                                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:47: error: package com.amazonaws.services.cloudsearchdomain.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchdomain.model.ContentType;\r\n  [javadoc]                                                      ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:48: error: package com.amazonaws.services.cloudsearchdomain.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchdomain.model.UploadDocumentsRequest;\r\n  [javadoc]                                                      ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:49: error: package com.amazonaws.services.cloudsearchdomain.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchdomain.model.UploadDocumentsResult;\r\n  [javadoc]                                                      ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:50: error: package com.amazonaws.services.cloudsearchv2 does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchv2.AmazonCloudSearchClient;\r\n  [javadoc]                                            ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:51: error: package com.amazonaws.services.cloudsearchv2.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchv2.model.DescribeDomainsRequest;\r\n  [javadoc]                                                  ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:52: error: package com.amazonaws.services.cloudsearchv2.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchv2.model.DescribeDomainsResult;\r\n  [javadoc]                                                  ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:53: error: package com.amazonaws.services.cloudsearchv2.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchv2.model.DescribeIndexFieldsRequest;\r\n  [javadoc]                                                  ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:54: error: package com.amazonaws.services.cloudsearchv2.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchv2.model.DescribeIndexFieldsResult;\r\n  [javadoc]                                                  ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:55: error: package com.amazonaws.services.cloudsearchv2.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchv2.model.DomainStatus;\r\n  [javadoc]                                                  ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:56: error: package com.amazonaws.services.cloudsearchv2.model does not exist\r\n  [javadoc] import com.amazonaws.services.cloudsearchv2.model.IndexFieldStatus;\r\n  [javadoc]                                                  ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:57: error: package com.amazonaws.util.json does not exist\r\n  [javadoc] import com.amazonaws.util.json.JSONException;\r\n  [javadoc]                               ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:58: error: package com.amazonaws.util.json does not exist\r\n  [javadoc] import com.amazonaws.util.json.JSONObject;\r\n  [javadoc]                               ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:73: error: cannot find symbol\r\n  [javadoc]   private AmazonCloudSearchDomainClient client;\r\n  [javadoc]           ^\r\n  [javadoc]   symbol:   class AmazonCloudSearchDomainClient\r\n  [javadoc]   location: class CloudSearchIndexWriter\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:44: error: package org.apache.http.impl.nio.client does not exist\r\n  [javadoc] import org.apache.http.impl.nio.client.HttpAsyncClientBuilder;\r\n  [javadoc]                                       ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:52: error: package org.elasticsearch.action.bulk does not exist\r\n  [javadoc] import org.elasticsearch.action.bulk.BulkResponse;\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:53: error: package org.elasticsearch.action does not exist\r\n  [javadoc] import org.elasticsearch.action.DocWriteRequest;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:54: error: package org.elasticsearch.action.bulk does not exist\r\n  [javadoc] import org.elasticsearch.action.bulk.BackoffPolicy;\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:55: error: package org.elasticsearch.action.bulk does not exist\r\n  [javadoc] import org.elasticsearch.action.bulk.BulkProcessor;\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:56: error: package org.elasticsearch.action.bulk does not exist\r\n  [javadoc] import org.elasticsearch.action.bulk.BulkRequest;\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:57: error: package org.elasticsearch.action.delete does not exist\r\n  [javadoc] import org.elasticsearch.action.delete.DeleteRequest;\r\n  [javadoc]                                       ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:58: error: package org.elasticsearch.action.index does not exist\r\n  [javadoc] import org.elasticsearch.action.index.IndexRequest;\r\n  [javadoc]                                      ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:59: error: package org.elasticsearch.client does not exist\r\n  [javadoc] import org.elasticsearch.client.RestClient;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:60: error: package org.elasticsearch.client does not exist\r\n  [javadoc] import org.elasticsearch.client.RestClientBuilder;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:61: error: package org.elasticsearch.client does not exist\r\n  [javadoc] import org.elasticsearch.client.RestHighLevelClient;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:62: error: package org.elasticsearch.common.unit does not exist\r\n  [javadoc] import org.elasticsearch.common.unit.ByteSizeUnit;\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:63: error: package org.elasticsearch.common.unit does not exist\r\n  [javadoc] import org.elasticsearch.common.unit.ByteSizeValue;\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:64: error: package org.elasticsearch.common.unit does not exist\r\n  [javadoc] import org.elasticsearch.common.unit.TimeValue;\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:65: error: package org.elasticsearch.client.RestClientBuilder does not exist\r\n  [javadoc] import org.elasticsearch.client.RestClientBuilder.HttpClientConfigCallback;\r\n  [javadoc]                                                  ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:66: error: package org.elasticsearch.common.xcontent does not exist\r\n  [javadoc] import org.elasticsearch.common.xcontent.XContentBuilder;\r\n  [javadoc]                                         ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:67: error: package org.elasticsearch.common.xcontent does not exist\r\n  [javadoc] import org.elasticsearch.common.xcontent.XContentFactory;\r\n  [javadoc]                                         ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:68: error: package org.elasticsearch.client does not exist\r\n  [javadoc] import org.elasticsearch.client.RequestOptions;\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:102: error: cannot find symbol\r\n  [javadoc]   private RestHighLevelClient client;\r\n  [javadoc]           ^\r\n  [javadoc]   symbol:   class RestHighLevelClient\r\n  [javadoc]   location: class ElasticIndexWriter\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:103: error: cannot find symbol\r\n  [javadoc]   private BulkProcessor bulkProcessor;\r\n  [javadoc]           ^\r\n  [javadoc]   symbol:   class BulkProcessor\r\n  [javadoc]   location: class ElasticIndexWriter\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:167: error: cannot find symbol\r\n  [javadoc]   protected RestHighLevelClient makeClient(IndexWriterParams parameters)\r\n  [javadoc]             ^\r\n  [javadoc]   symbol:   class RestHighLevelClient\r\n  [javadoc]   location: class ElasticIndexWriter\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:211: error: package BulkProcessor does not exist\r\n  [javadoc]   protected BulkProcessor.Listener bulkProcessorListener() {\r\n  [javadoc]                          ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-kafka/src/java/org/apache/nutch/indexwriter/kafka/KafkaIndexWriter.java:39: error: package org.apache.kafka.clients.producer does not exist\r\n  [javadoc] import org.apache.kafka.clients.producer.KafkaProducer;\r\n  [javadoc]                                         ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-kafka/src/java/org/apache/nutch/indexwriter/kafka/KafkaIndexWriter.java:40: error: package org.apache.kafka.clients.producer does not exist\r\n  [javadoc] import org.apache.kafka.clients.producer.ProducerConfig;\r\n  [javadoc]                                         ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-kafka/src/java/org/apache/nutch/indexwriter/kafka/KafkaIndexWriter.java:41: error: package org.apache.kafka.clients.producer does not exist\r\n  [javadoc] import org.apache.kafka.clients.producer.ProducerRecord;\r\n  [javadoc]                                         ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-kafka/src/java/org/apache/nutch/indexwriter/kafka/KafkaIndexWriter.java:52: error: package org.apache.kafka.clients.producer does not exist\r\n  [javadoc]   private org.apache.kafka.clients.producer.Producer<String, JsonNode> producer;\r\n  [javadoc]                                            ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-kafka/src/java/org/apache/nutch/indexwriter/kafka/KafkaIndexWriter.java:53: error: cannot find symbol\r\n  [javadoc]   private ProducerRecord<String, JsonNode> data;\r\n  [javadoc]           ^\r\n  [javadoc]   symbol:   class ProducerRecord\r\n  [javadoc]   location: class KafkaIndexWriter\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-kafka/src/java/org/apache/nutch/indexwriter/kafka/KafkaIndexWriter.java:67: error: cannot find symbol\r\n  [javadoc]   private List<ProducerRecord<String, JsonNode>> inputDocs = null;\r\n  [javadoc]                ^\r\n  [javadoc]   symbol:   class ProducerRecord\r\n  [javadoc]   location: class KafkaIndexWriter\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrIndexWriter.java:37: error: package org.apache.solr.client.solrj does not exist\r\n  [javadoc] import org.apache.solr.client.solrj.SolrClient;\r\n  [javadoc]                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrIndexWriter.java:38: error: package org.apache.solr.client.solrj does not exist\r\n  [javadoc] import org.apache.solr.client.solrj.SolrServerException;\r\n  [javadoc]                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrIndexWriter.java:39: error: package org.apache.solr.client.solrj.impl does not exist\r\n  [javadoc] import org.apache.solr.client.solrj.impl.CloudSolrClient;\r\n  [javadoc]                                         ^\r\n  [javadoc] Standard Doclet version 1.8.0_221\r\n  [javadoc] Building tree for all the packages and classes...\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/java/org/apache/nutch/indexer/IndexerMapReduce.java:67: error: reference not found\r\n  [javadoc]  * {@link org.apache.nutch.indexer.IndexerMapReduce#initMRJob(Path, Path, Collection, JobConf, boolean)}\r\n  [javadoc]           ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/java/org/apache/nutch/indexer/IndexWriter.java:60: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                            ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/java/org/apache/nutch/indexer/IndexWriter.java:60: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/java/org/apache/nutch/indexer/IndexWriter.java:60: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/java/org/apache/nutch/indexer/IndexWriter.java:60: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23IndexingFilter.java:39: error: unexpected content\r\n  [javadoc]  * @see {@link org.apache.nutch.any23.Any23ParseFilter}.\r\n  [javadoc]    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:51: error: malformed HTML\r\n  [javadoc]  * <code><http://www.bbc.co.uk/news/scotland/> <http://iptc.org/std/rNews/2011-10-07#datePublished> \"2014/03/31 13:53:03\"@en-gb .</code>\r\n  [javadoc]          ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:51: error: bad use of '>'\r\n  [javadoc]  * <code><http://www.bbc.co.uk/news/scotland/> <http://iptc.org/std/rNews/2011-10-07#datePublished> \"2014/03/31 13:53:03\"@en-gb .</code>\r\n  [javadoc]                                              ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:51: error: malformed HTML\r\n  [javadoc]  * <code><http://www.bbc.co.uk/news/scotland/> <http://iptc.org/std/rNews/2011-10-07#datePublished> \"2014/03/31 13:53:03\"@en-gb .</code>\r\n  [javadoc]                                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/any23/src/java/org/apache/nutch/any23/Any23ParseFilter.java:51: error: bad use of '>'\r\n  [javadoc]  * <code><http://www.bbc.co.uk/news/scotland/> <http://iptc.org/std/rNews/2011-10-07#datePublished> \"2014/03/31 13:53:03\"@en-gb .</code>\r\n  [javadoc]                                                                                                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataIndexer.java:40: error: unknown tag: value\r\n  [javadoc]  * <value>key1,key2,key3</value>.\r\n  [javadoc]    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/index-metadata/src/java/org/apache/nutch/indexer/metadata/MetadataIndexer.java:40: error: unknown tag: value\r\n  [javadoc]  * <value>key1,key2,key3</value>.\r\n  [javadoc]                         ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:350: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                            ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:350: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:350: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-cloudsearch/src/java/org/apache/nutch/indexwriter/cloudsearch/CloudSearchIndexWriter.java:350: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-csv/src/java/org/apache/nutch/indexwriter/csv/CSVIndexWriter.java:328: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                            ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-csv/src/java/org/apache/nutch/indexwriter/csv/CSVIndexWriter.java:328: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-csv/src/java/org/apache/nutch/indexwriter/csv/CSVIndexWriter.java:328: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-csv/src/java/org/apache/nutch/indexwriter/csv/CSVIndexWriter.java:328: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-dummy/src/java/org/apache/nutch/indexwriter/dummy/DummyIndexWriter.java:131: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                            ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-dummy/src/java/org/apache/nutch/indexwriter/dummy/DummyIndexWriter.java:131: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-dummy/src/java/org/apache/nutch/indexwriter/dummy/DummyIndexWriter.java:131: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-dummy/src/java/org/apache/nutch/indexwriter/dummy/DummyIndexWriter.java:131: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:300: error: malformed HTML\r\n  [javadoc]    *         <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]              ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:300: error: malformed HTML\r\n  [javadoc]    *         <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:300: error: bad use of '>'\r\n  [javadoc]    *         <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-elastic/src/java/org/apache/nutch/indexwriter/elastic/ElasticIndexWriter.java:300: error: bad use of '>'\r\n  [javadoc]    *         <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                      ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-rabbit/src/java/org/apache/nutch/indexwriter/rabbit/RabbitIndexWriter.java:217: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                            ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-rabbit/src/java/org/apache/nutch/indexwriter/rabbit/RabbitIndexWriter.java:217: error: malformed HTML\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-rabbit/src/java/org/apache/nutch/indexwriter/rabbit/RabbitIndexWriter.java:217: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-rabbit/src/java/org/apache/nutch/indexwriter/rabbit/RabbitIndexWriter.java:217: error: bad use of '>'\r\n  [javadoc]    * @return The values of each row. It must have the form <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                                                                    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrIndexWriter.java:301: error: malformed HTML\r\n  [javadoc]    *         <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]              ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrIndexWriter.java:301: error: malformed HTML\r\n  [javadoc]    *         <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrIndexWriter.java:301: error: bad use of '>'\r\n  [javadoc]    *         <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                     ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/indexer-solr/src/java/org/apache/nutch/indexwriter/solr/SolrIndexWriter.java:301: error: bad use of '>'\r\n  [javadoc]    *         <KEY,<DESCRIPTION,VALUE>>.\r\n  [javadoc]                                      ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/lib-rabbitmq/src/java/org/apache/nutch/rabbitmq/RabbitMQClient.java:112: error: self-closing element not allowed\r\n  [javadoc]    *                         <br />\r\n  [javadoc]                              ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/lib-rabbitmq/src/java/org/apache/nutch/rabbitmq/RabbitMQClient.java:120: error: self-closing element not allowed\r\n  [javadoc]    *                         <br />\r\n  [javadoc]                              ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/lib-rabbitmq/src/java/org/apache/nutch/rabbitmq/RabbitMQClient.java:130: error: self-closing element not allowed\r\n  [javadoc]    *                         <br />\r\n  [javadoc]                              ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/protocol-interactiveselenium/src/java/org/apache/nutch/protocol/interactiveselenium/handlers/DefaultClickAllAjaxLinksHandler.java:34: error: unknown attribute: hfer\r\n  [javadoc]  * This handler clicks all the <a hfer=\"javascript:void(null);\"> tags\r\n  [javadoc]                                   ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/protocol-interactiveselenium/src/java/org/apache/nutch/protocol/interactiveselenium/handlers/DefaultClickAllAjaxLinksHandler.java:34: error: element not closed: a\r\n  [javadoc]  * This handler clicks all the <a hfer=\"javascript:void(null);\"> tags\r\n  [javadoc]                                ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/scoring-orphan/src/java/org/apache/nutch/scoring/orphan/OrphanScoringFilter.java:71: error: @param name not found\r\n  [javadoc]    * @param inLinks\r\n  [javadoc]             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/scoring-orphan/src/java/org/apache/nutch/scoring/orphan/OrphanScoringFilter.java:73: error: invalid use of @return\r\n  [javadoc]    * @return void\r\n  [javadoc]      ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-ajax/src/java/org/apache/nutch/net/urlnormalizer/ajax/AjaxURLNormalizer.java:58: error: @param name not found\r\n  [javadoc]    * @param String urlString\r\n  [javadoc]             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-ajax/src/java/org/apache/nutch/net/urlnormalizer/ajax/AjaxURLNormalizer.java:83: error: @param name not found\r\n  [javadoc]    * @param String urlString\r\n  [javadoc]             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-ajax/src/java/org/apache/nutch/net/urlnormalizer/ajax/AjaxURLNormalizer.java:112: error: @param name not found\r\n  [javadoc]    * @param String urlString\r\n  [javadoc]             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-ajax/src/java/org/apache/nutch/net/urlnormalizer/ajax/AjaxURLNormalizer.java:170: error: @param name not found\r\n  [javadoc]    * @param String fragmentPart\r\n  [javadoc]             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-ajax/src/java/org/apache/nutch/net/urlnormalizer/ajax/AjaxURLNormalizer.java:186: error: @param name not found\r\n  [javadoc]    * @param String fragmentPart\r\n  [javadoc]             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-ajax/src/java/org/apache/nutch/net/urlnormalizer/ajax/AjaxURLNormalizer.java:221: error: @param name not found\r\n  [javadoc]    * @param Configuration conf\r\n  [javadoc]             ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-basic/src/java/org/apache/nutch/net/urlnormalizer/basic/package-info.java:32: error: block element not allowed within inline element <code>: ul\r\n  [javadoc]  * <ul>\r\n  [javadoc]    ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-basic/src/java/org/apache/nutch/net/urlnormalizer/basic/package-info.java:28: error: element not closed: code\r\n  [javadoc]  * <code>https://www.example.org/a/../b//./select%2Dlang.php?lang=espa\u00f1ol#anchor<code>\r\n  [javadoc]                                                                                 ^\r\n  [javadoc] /Users/lmcgibbn/Downloads/nutch_release/src/plugin/urlnormalizer-basic/src/java/org/apache/nutch/net/urlnormalizer/basic/package-info.java:28: error: element not closed: code\r\n  [javadoc]  * <code>https://www.example.org/a/../b//./select%2Dlang.php?lang=espa\u00f1ol#anchor<code>\r\n  [javadoc]    ^\r\n  [javadoc] Building index for all the packages and classes...\r\n  [javadoc] Building index for all classes...\r\n  [javadoc] Generating /Users/lmcgibbn/Downloads/nutch_release/build/release/javadoc/help-doc.html...",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/568"
        ]
    },
    "NUTCH-2843": {
        "Key": "NUTCH-2843",
        "Summary": "Duplicate declaration of dependencies in ivy.xml",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "19/Jan/21 20:31",
        "Updated": "21/Aug/22 11:47",
        "Resolved": "21/Aug/22 10:40",
        "Description": "When pushing the 1.18 release candidate Maven reported the following issues\n\n\r\n[artifact:mvn] [WARNING] Some problems were encountered while building the effective model for org.apache.nutch:nutch:jar:1.18\r\n[artifact:mvn] [WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.mortbay.jetty:jetty:jar -> duplicate declaration of version 6.1.26 @ line 457, column 17\r\n[artifact:mvn] [WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.commons:commons-collections4:jar -> version 4.4 vs 4.1 @ line 463, column 17\r\n[artifact:mvn] [WARNING] 'dependencies.dependency.(groupId:artifactId:type:classifier)' must be unique: org.apache.httpcomponents:httpclient:jar -> version 4.5.13 vs 4.5.5 @ line 559, column 17",
        "Issue Links": []
    },
    "NUTCH-2844": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2845": {
        "Key": "NUTCH-2845",
        "Summary": "Update urlfilter-suffix rules",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "plugin,                                            urlfilter",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "25/Jan/21 16:04",
        "Updated": "01/Feb/21 10:34",
        "Resolved": "01/Feb/21 10:02",
        "Description": "The rules of urlfilter-suffix should be update to include recent file formats of\n\nimages\n   .icns (Apple Icon Image Format)\n   .tif (TIFF, alternate pattern)\n   .webp (WebP)\narchive and software package formats\n   .apk\n   .bz2\n   .xz\nvideos\n   .mp4\n   .webm\n   .m4v\n   .qt (QuickTime)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/564"
        ]
    },
    "NUTCH-2846": {
        "Key": "NUTCH-2815 Add Spotbugs target to build and address detected \"bugs\"",
        "Summary": "Fix various bugs spotted by NUTCH-2815",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "28/Jan/21 16:21",
        "Updated": "01/Feb/21 21:29",
        "Resolved": "01/Feb/21 20:27",
        "Description": "This issue addresses various bugs spotted by Spotbugs (NUTCH-2815):\n\nuse static method Integer.parseInt(...)\nuse integer arithmetic instead of floating point with rounding floats afterwards\nerroneous declaration of constructor in BasicURLNormalizer\nfix bracketing when calculating hash code of CrawlDatum",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/566"
        ]
    },
    "NUTCH-2847": {
        "Key": "NUTCH-2847",
        "Summary": "HttpDateFormat: Simplify based on new Java 8 DateTime API",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "28/Jan/21 19:38",
        "Updated": "01/Feb/21 21:29",
        "Resolved": "01/Feb/21 20:28",
        "Description": "The class HttpDateFormat is still based on SimpleDateFormat and requires \"synchronized\" code blocks to avoid race conditions. Using the new Java 8 Date and Time API would allow to simplify the code.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/567"
        ]
    },
    "NUTCH-2848": {
        "Key": "NUTCH-2848",
        "Summary": "Consider use of StringUtil#isEmpty",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "util",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "07/Feb/21 00:56",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "We should consider 'standardizing' the use of StringUtil#isEmpty() across the codebase.\n\n\r\n  /**\r\n   * Checks if a string is empty (ie is null or empty).\r\n   */\r\n  public static boolean isEmpty(String str) {\r\n    return (str == null) || (str.equals(\"\"));\r\n  }\r\n\n\nSo far the impact is as follows\n\n\r\ngrep -lr \".equals(\\\"\\\")\" .\r\n./plugin/urlnormalizer-protocol/src/java/org/apache/nutch/net/urlnormalizer/protocol/ProtocolURLNormalizer.java\r\n./plugin/parse-ext/src/java/org/apache/nutch/parse/ext/ExtParser.java\r\n./plugin/urlnormalizer-host/src/java/org/apache/nutch/net/urlnormalizer/host/HostURLNormalizer.java\r\n./plugin/parsefilter-regex/src/java/org/apache/nutch/parsefilter/regex/RegexParseFilter.java\r\n./plugin/feed/src/java/org/apache/nutch/parse/feed/FeedParser.java\r\n./plugin/parsefilter-naivebayes/src/java/org/apache/nutch/parsefilter/naivebayes/Train.java\r\n./plugin/language-identifier/src/test/org/apache/nutch/analysis/lang/TestHTMLLanguageParser.java\r\n./plugin/urlnormalizer-slash/src/java/org/apache/nutch/net/urlnormalizer/slash/SlashURLNormalizer.java\r\n./java/org/apache/nutch/tools/FileDumper.java\r\n./java/org/apache/nutch/net/URLNormalizers.java\r\n./java/org/apache/nutch/util/StringUtil.java\r\n./java/org/apache/nutch/util/domain/DomainStatistics.java\r\n./java/org/apache/nutch/util/MimeUtil.java\r\n\n\nWe may wish to also consider the following implementation as well \n\n\r\n    public static boolean isEmpty(String str) {  \r\n            return str.length == 0;  \r\n        }  \r\n\n\nAny comments?",
        "Issue Links": []
    },
    "NUTCH-2849": {
        "Key": "NUTCH-2849",
        "Summary": "Replace remaining package.html files with package-info.java",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "11/Feb/21 17:27",
        "Updated": "16/Feb/21 19:35",
        "Resolved": "16/Feb/21 18:40",
        "Description": "Following on from NUTCH-2842, this issue will implement solution # 2 in snagel's comment.\nOnce the warnings are gone, we can turn on the failonwarning=\"true\" flag in the javadoc tasks.\nI'll get to work on this one.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/569"
        ]
    },
    "NUTCH-2850": {
        "Key": "NUTCH-2815 Add Spotbugs target to build and address detected \"bugs\"",
        "Summary": "Method ignores exceptional return value",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "dumpers",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/Feb/21 02:04",
        "Updated": "19/Feb/21 12:47",
        "Resolved": "18/Feb/21 15:21",
        "Description": "In class org.apache.nutch.tools.FileDumper\nIn method org.apache.nutch.tools.FileDumper.dump(File, File, String[], boolean, boolean, boolean)\nCalled method java.io.File.mkdirs()\nAt FileDumper.java:[line 237]\nExceptional return value of java.io.File.mkdirs() ignored in org.apache.nutch.tools.FileDumper.dump(File, File, String[], boolean, boolean, boolean)\nThis method returns a value that is not checked. The return value should be checked since it can indicate an unusual or unexpected function execution. For example, the File.delete() method returns false if the file could not be successfully deleted (rather than throwing an Exception). If you don't check the result, you won't notice if the method invocation signals unexpected behavior by returning an atypical return value.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/570"
        ]
    },
    "NUTCH-2851": {
        "Key": "NUTCH-2815 Add Spotbugs target to build and address detected \"bugs\"",
        "Summary": "Random object created and used only once",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "dmoz,                                            generator,                                            indexer,                                            segment",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/Feb/21 02:22",
        "Updated": "19/Feb/21 12:47",
        "Resolved": "18/Feb/21 15:21",
        "Description": "In class org.apache.nutch.crawl.Generator\nIn method org.apache.nutch.crawl.Generator.partitionSegment(Path, Path, int)\nCalled method java.util.Random.nextInt()\nAt Generator.java:[line 1016]\nRandom object created and used only once in org.apache.nutch.crawl.Generator.partitionSegment(Path, Path, int)\nThis code creates a java.util.Random object, uses it to generate one random number, and then discards the Random object. This produces mediocre quality random numbers and is inefficient. If possible, rewrite the code so that the Random object is created once and saved, and each time a new random number is required invoke a method on the existing Random object to obtain it.\nIf it is important that the generated Random numbers not be guessable, you must not create a new Random for each random number; the values are too easily guessable. You should strongly consider using a java.security.SecureRandom instead (and avoid allocating a new SecureRandom for each random number needed).\nThis bad practice also affects the following\norg.apache.nutch.indexer.IndexingJob since first historized release\norg.apache.nutch.segment.SegmentReader since first historized release\norg.apache.nutch.tools.DmozParser$RDFProcessor since first historized release",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/571"
        ]
    },
    "NUTCH-2852": {
        "Key": "NUTCH-2815 Add Spotbugs target to build and address detected \"bugs\"",
        "Summary": "Method invokes System.exit(...) 9 bugs",
        "Type": "Sub-task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/Feb/21 15:25",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "org.apache.nutch.indexer.IndexingFiltersChecker since first historized release\nIn class org.apache.nutch.indexer.IndexingFiltersChecker\nIn method org.apache.nutch.indexer.IndexingFiltersChecker.run(String[])\nAt IndexingFiltersChecker.java:[line 96]\nAnother occurrence at IndexingFiltersChecker.java:[line 129]\norg.apache.nutch.indexer.IndexingFiltersChecker.run(String[]) invokes System.exit(...), which shuts down the entire virtual machine\nInvoking System.exit shuts down the entire Java virtual machine. This should only been done when it is appropriate. Such calls make it hard or impossible for your code to be invoked by other code. Consider throwing a RuntimeException instead.\nAlso occurs in\n   org.apache.nutch.net.URLFilterChecker since first historized release\n   org.apache.nutch.net.URLNormalizerChecker since first historized release\n   org.apache.nutch.parse.ParseSegment since first historized release\n   org.apache.nutch.parse.ParserChecker since first historized release\n   org.apache.nutch.service.NutchServer since first historized release\n   org.apache.nutch.tools.CommonCrawlDataDumper since first historized release\n   org.apache.nutch.tools.DmozParser since first historized release\n   org.apache.nutch.util.AbstractChecker since first historized release",
        "Issue Links": []
    },
    "NUTCH-2853": {
        "Key": "NUTCH-2853",
        "Summary": "bin/nutch: remove deprecated commands solrindex, solrdedup, solrclean",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "bin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "26/Feb/21 09:58",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The commands \"solrindex\", \"solrdedup\" and \"solrclean\" are deprecated since 7 years and should be removed to avoid any confusions (one example: https://stackoverflow.com/questions/66376609/nutch-solr-index-is-failing).",
        "Issue Links": []
    },
    "NUTCH-2854": {
        "Key": "NUTCH-2854",
        "Summary": "Address ALL security vulnerabilities indicated by report-vulnerabilities ant target",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Critical",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "05/Mar/21 18:16",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "NUTCH-2840 uncovered lots of security issues for us to work on. This is simply a parent issue to contain all work relating to addressing those issues.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/572"
        ]
    },
    "NUTCH-2855": {
        "Key": "NUTCH-2854 Address ALL security vulnerabilities indicated by report-vulnerabilities ant target",
        "Summary": "Update org.elasticsearch.client",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Randall Williams",
        "Reporter": "Randall Williams",
        "Created": "05/Mar/21 18:27",
        "Updated": "10/Jun/21 08:23",
        "Resolved": "01/Apr/21 15:57",
        "Description": "Update from\u00a0org.elasticsearch.client to newest version. It is listed as a HIGH risk vulnerability on the dependency report. Change it from 7.0.3 to 7.11.1 (newest version as of 3/5/21).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/572",
            "https://github.com/apache/nutch/pull/577"
        ]
    },
    "NUTCH-2856": {
        "Key": "NUTCH-2856",
        "Summary": "Implement a protocol-smb plugin based on hierynomus/smbj",
        "Type": "New Feature",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "external,                                            plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Hiran Chaudhuri",
        "Created": "08/Mar/21 21:48",
        "Updated": "01/Mar/23 04:42",
        "Resolved": null,
        "Description": "The plugin protocol-smb advertized on https://cwiki.apache.org/confluence/display/NUTCH/PluginCentral actually refers to the JCIFS library. According to this library's homepage https://www.jcifs.org/:\nIf you're looking for the latest and greatest open source Java SMB library, this is not it. JCIFS has been in maintenance-mode-only for several years and although what it does support works fine (SMB1, NTLMv2, midlc, MSRPC and various utility classes), jCIFS does not support the newer SMB2/3 variants of the SMB protocol which is slowly becoming required (Windows 10 requires SMB2/3). JCIFS only supports SMB1 but Microsoft has deprecated SMB1 in their products. So if SMB1 is disabled on your network, JCIFS' file related operations will NOT work.\nLooking at https://en.wikipedia.org/wiki/Server_Message_Block#SMB_/CIFS/_SMB1:\nMicrosoft added SMB1 to the Windows Server 2012 R2 deprecation list in June 2013. Windows Server 2016 and some versions of Windows 10 Fall Creators Update do not have SMB1 installed by default.\nAs a conclusion, the chances that SMB1 protocol is installed and/or configured are getting vastly smaller. Therefore some migration towards SMB2/3 is required. Luckily the JCIFS homepage lists alternatives:\n\njcifs-codelibs\njcifs-ng\nsmbj",
        "Issue Links": [
            "/jira/browse/NUTCH-427"
        ]
    },
    "NUTCH-2857": {
        "Key": "NUTCH-2857",
        "Summary": "Upgrade from JDK1.8 --> JDK11",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Mar/21 03:12",
        "Updated": "21/Mar/21 16:28",
        "Resolved": "21/Mar/21 15:31",
        "Description": "I kicked the JDK1.8 --> JDK11 conversation off on dev@nutch.a.o.\nThis issue allows us to flesh it out more and to share some experiences alongside some pull requests.",
        "Issue Links": [
            "/jira/browse/NUTCH-2512",
            "https://github.com/apache/nutch/pull/573"
        ]
    },
    "NUTCH-2858": {
        "Key": "NUTCH-2858",
        "Summary": "urlnormalizer-protocol: URL port is lost during normalization",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "plugin,                                            urlnormalizer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "27/Mar/21 11:04",
        "Updated": "06/Apr/21 15:38",
        "Resolved": "06/Apr/21 14:52",
        "Description": "If a URL includes a port, e.g.\u00a0http://example.com:8080/ or https://example.com:8443/, the port is removed when normalizing using the protocol-urlnormalizer.\nInstead, if the port is set,\n\nthe port should be kept as is and\nthe protocol should be unchanged\n\t\nkeeping the port and changing the protocol might result in a connection failure\nunlike the default port mappings (80 (http) <> 443 (https)), non-default port mappings (8080 <> 8443) are risky and unlikely to work on every server setup.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/575"
        ]
    },
    "NUTCH-2859": {
        "Key": "NUTCH-2859",
        "Summary": "urlnormalizer-protocol: allow to normalize domains",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "plugin,                                            urlnormalizer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "29/Mar/21 14:31",
        "Updated": "06/Apr/21 15:38",
        "Resolved": "06/Apr/21 14:53",
        "Description": "The plugin urlnormalizer-protocol normalizes the URL protocol/scheme for a given list of hosts to the desired \"normal\" protocol (usually one of http or https). It would be handy to allow to specify domain names as well, so that all hosts/subdomains in a given domain are normalized.\nIn order to stay backward-compatible this could be done by matching *.example.org as a pattern for all hosts or subdomains of the domain example.org.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/576"
        ]
    },
    "NUTCH-2860": {
        "Key": "NUTCH-2860",
        "Summary": "Upgrade to Tika 1.26",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "1.18",
        "Fix Version/s": "None",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "19/Apr/21 08:08",
        "Updated": "01/Dec/21 12:08",
        "Resolved": "01/Dec/21 12:08",
        "Description": "Apache Tika 1.26 is out, we should upgrade our dependency.\nSee howto_upgrade_tika.",
        "Issue Links": [
            "/jira/browse/NUTCH-2891"
        ]
    },
    "NUTCH-2861": {
        "Key": "NUTCH-2861",
        "Summary": "Remove parse-swf",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/Apr/21 08:35",
        "Updated": "19/Aug/22 13:42",
        "Resolved": "19/Aug/22 12:56",
        "Description": "We should consider to remove the Shockwafe Flash parser plugin (parse-swf):\n\nShockwave/Adobe Flash reached end-of-life\nmajor browsers now block playing Flash content\nthe plugin is based on 15-year old library (javaswf), not maintained anymore and not available on Maven repository\nit's shipped in binary form also in the source package which contradicts the Apache release policy\n\nNotes:\n\nshould place a notice about the removal in the release not, as parse-tika is not able to extract textual content from *.swf files\ndo not forget to unregister the plugin in parse-plugins.xml",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/742"
        ]
    },
    "NUTCH-2862": {
        "Key": "NUTCH-2862",
        "Summary": "Do not include Ivy jar in source release package",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/Apr/21 08:39",
        "Updated": "20/Oct/21 10:41",
        "Resolved": "20/Oct/21 09:22",
        "Description": "The source release package includes the ivy/ivy-*.jar, it should not be packaged because\n\nit is automatically downloaded during build\nthere shouldn't be any binaries (compiled jar files) in the source package following the Apache release policy",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/699",
            "https://github.com/apache/nutch/pull/699"
        ]
    },
    "NUTCH-2863": {
        "Key": "NUTCH-2863",
        "Summary": "Injector to parse command-line flags case-insensitive",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "injector",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "07/May/21 12:39",
        "Updated": "21/Aug/22 11:47",
        "Resolved": "21/Aug/22 10:22",
        "Description": "Injector parses command-line arguments case-sensitive and flags such as -noNormalize need to be spelled accordingly. Unluckily, the correct case is not consistently used in the command-line help:\n\nUsage: Injector [-D...] <crawldb> <url_dir> [-overwrite|-update] [-noFilter] [-noNormalize] [-filterNormalizeAll]\r\n\r\n...\r\n\r\n -nonormalize   Do not normalize URLs before injecting\r\n -nofilter      Do not apply URL filters to injected URLs\r\n\n\nMaybe better to just ignore case? But the command-line help should be consistent.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/745",
            "https://github.com/apache/nutch/pull/745"
        ]
    },
    "NUTCH-2864": {
        "Key": "NUTCH-2864",
        "Summary": "Upgrade Dockerfile to use JDK 11",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "build,                                            docker",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "20/May/21 10:01",
        "Updated": "04/Jun/21 08:28",
        "Resolved": "03/Jun/21 20:15",
        "Description": "With NUTCH-2857 the Nutch Dockerfile needs to be update to use JDK 11 and a more recent base image.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/647"
        ]
    },
    "NUTCH-2865": {
        "Key": "NUTCH-2865",
        "Summary": "WARC exporter support for metadata and dropping empty responses",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/May/21 10:05",
        "Updated": "22/Nov/21 14:31",
        "Resolved": "22/Nov/21 13:53",
        "Description": "WARCExporter is a handy tool to dump the segments. Unfortunately it also emits WARC records for status' other than success of notmodified, which accounts for a decent number in each crawl cycle. It also doesn't emit parsed metadata or extracted text. It does now.\n\u00a0\nThis patch adds three switches:\n\n-includeOnlySuccessfulResponses to only emit records of success or notmodified\n-includeParseData to also emit parse metadata as WARC metadata record\n-includeParseText to also emit extracted text as WARC metadata\n\nBoth metadata objects are stored in the same WARC metadata record to save space.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/706"
        ]
    },
    "NUTCH-2866": {
        "Key": "NUTCH-2866",
        "Summary": "MetaData.toString() should return \"key=value ...\"",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.17",
        "Fix Version/s": "1.19",
        "Component/s": "metadata",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "31/May/21 15:52",
        "Updated": "01/Jun/21 15:37",
        "Resolved": "01/Jun/21 14:40",
        "Description": "The default implementation of Metadata.toString() returns key1 value1=key2 value2. This should be key1=value1 key2=value2, of course. Introduced with NUTCH-2788 (commit e3f7725) - , seen while reviewing NUTCH-2865.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/648"
        ]
    },
    "NUTCH-2867": {
        "Key": "NUTCH-2867",
        "Summary": "Support for custom HostDb aggregators",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "hostdb",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "09/Jun/21 12:20",
        "Updated": "22/Nov/21 15:28",
        "Resolved": "22/Nov/21 14:12",
        "Description": "HostDB needs support for custom per-host statistic aggregators. This gives users a simple tool to calculate their own statistics just by implementing a simple interface, and configurating that class as a hostdb.crawldatum.processor.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/705"
        ]
    },
    "NUTCH-2868": {
        "Key": "NUTCH-2868",
        "Summary": "urlnormalizer-protocol fails with StringIndexOutOfBoundsException when reading invalid line in configuration file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "plugin,                                            urlnormalizer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jun/21 09:41",
        "Updated": "14/Jun/21 15:28",
        "Resolved": "14/Jun/21 14:29",
        "Description": "When reading a invalid line in the configuration file, the protocol urlnormalizer may fail with a StringIndexOutOfBoundsException:\n\n2021-06-10 05:10:41,877 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.StringIndexOutOfBoundsException: String index out of range: -1\r\n        at java.lang.String.substring(String.java:1967)\r\n        at org.apache.nutch.net.urlnormalizer.protocol.ProtocolURLNormalizer.readConfiguration(ProtocolURLNormalizer.java:95)\r\n        at org.apache.nutch.net.urlnormalizer.protocol.ProtocolURLNormalizer.setConf(ProtocolURLNormalizer.java:182)\r\n\n\nThe invalid line should be logged and skipped without causing the job to fail.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/649"
        ]
    },
    "NUTCH-2869": {
        "Key": "NUTCH-2869",
        "Summary": "Add @Override annotations to Nutch plugins",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "10/Jun/21 13:06",
        "Updated": "14/Jun/21 07:58",
        "Resolved": "12/Jun/21 10:00",
        "Description": "All plugins implement interfaces. Ideally, the overriden methods should be marked using the \"@Override\" annotation.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/650"
        ]
    },
    "NUTCH-2870": {
        "Key": "NUTCH-2870",
        "Summary": "fireant upgrade dependency junit in ivy/ivy.xml from 4.13.1 to 4.13.2",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build,                                            fireant",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Jun/21 02:54",
        "Updated": "14/Jun/21 02:58",
        "Resolved": "14/Jun/21 02:58",
        "Description": "fireant upgrade dependency junit in ivy/ivy.xml from 4.13.1 to 4.13.2",
        "Issue Links": []
    },
    "NUTCH-2871": {
        "Key": "NUTCH-2871",
        "Summary": "TEST",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jun/21 03:01",
        "Updated": "16/Jun/21 03:02",
        "Resolved": "16/Jun/21 03:02",
        "Description": "this is a Fireant test",
        "Issue Links": []
    },
    "NUTCH-2872": {
        "Key": "NUTCH-2872",
        "Summary": "TEST",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jun/21 03:17",
        "Updated": "16/Jun/21 03:17",
        "Resolved": "16/Jun/21 03:17",
        "Description": "this is a Fireant test",
        "Issue Links": []
    },
    "NUTCH-2873": {
        "Key": "NUTCH-2873",
        "Summary": "TEST",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jun/21 03:30",
        "Updated": "16/Jun/21 03:30",
        "Resolved": "16/Jun/21 03:30",
        "Description": "this is a Fireant test",
        "Issue Links": []
    },
    "NUTCH-2874": {
        "Key": "NUTCH-2874",
        "Summary": "TEST",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            fireant",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jun/21 03:36",
        "Updated": "09/Aug/22 06:52",
        "Resolved": "09/Aug/22 06:52",
        "Description": "this is a Fireant test",
        "Issue Links": []
    },
    "NUTCH-2875": {
        "Key": "NUTCH-2875",
        "Summary": "TEST",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            fireant",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Jun/21 01:54",
        "Updated": "09/Aug/22 06:53",
        "Resolved": "09/Aug/22 06:53",
        "Description": "TEST",
        "Issue Links": []
    },
    "NUTCH-2876": {
        "Key": "NUTCH-2876",
        "Summary": "TEST",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            fireant",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Jun/21 01:55",
        "Updated": "09/Aug/22 06:53",
        "Resolved": "09/Aug/22 06:53",
        "Description": "TEST",
        "Issue Links": []
    },
    "NUTCH-2877": {
        "Key": "NUTCH-2877",
        "Summary": "fireant upgrade dependency t-digest in ivy/ivy.xml from 3.2 to 3.3",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            fireant",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Jun/21 01:59",
        "Updated": "09/Aug/22 07:15",
        "Resolved": "09/Aug/22 07:15",
        "Description": "fireant upgrade dependency t-digest in ivy/ivy.xml from 3.2 to 3.3",
        "Issue Links": []
    },
    "NUTCH-2878": {
        "Key": "NUTCH-2878",
        "Summary": "fireant upgrade dependency hadoop-common in ivy/ivy.xml from 3.1.3 to 3.3.1",
        "Type": "Improvement",
        "Status": "Closed",
        "Priority": "Major",
        "Resolution": "Invalid",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            fireant",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Jun/21 02:00",
        "Updated": "09/Aug/22 07:15",
        "Resolved": "09/Aug/22 07:15",
        "Description": "fireant upgrade dependency hadoop-common in ivy/ivy.xml from 3.1.3 to 3.3.1",
        "Issue Links": []
    },
    "NUTCH-2879": {
        "Key": "NUTCH-2879",
        "Summary": "fireant upgrade dependency hadoop-hdfs in ivy/ivy.xml from 3.1.3 to 3.3.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "build,                                            fireant",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Jun/21 02:00",
        "Updated": "19/Aug/22 13:33",
        "Resolved": "19/Aug/22 13:33",
        "Description": "fireant upgrade dependency hadoop-hdfs in ivy/ivy.xml from 3.1.3 to 3.3.1",
        "Issue Links": [
            "/jira/browse/NUTCH-2952"
        ]
    },
    "NUTCH-2880": {
        "Key": "NUTCH-2880",
        "Summary": "parse-html/tika: update/complete HTML elements to extract outlinks from",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "18/Jun/21 08:52",
        "Updated": "19/Aug/22 13:38",
        "Resolved": null,
        "Description": "The list of HTML elements used to extract outlinks from (in DOMContentUtils (parse-html) and DOMContentUtils (parse-tika)) needs to be updated/completed to include HTML elements common in HTML5. Cf. a related question on stackoverflow about the <object> element\nA (mostly?) up-to-date list of HTML elements could be taken from the extractor of iipc/webarchiv-commons.",
        "Issue Links": []
    },
    "NUTCH-2881": {
        "Key": "NUTCH-2881",
        "Summary": "bug in 'nutch' symlink in docker container",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "docker",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "25/Jun/21 08:01",
        "Updated": "27/Jun/21 02:44",
        "Resolved": "27/Jun/21 02:05",
        "Description": "I am in the process of delivering a Helm chart for Nutch webserver which would allow us to build apps on top of Nutch webserver (farming jobs to EMR) running within K8s.\nI will provide an architecture diagram later to show why running Nutch webserver is useful.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/689"
        ]
    },
    "NUTCH-2882": {
        "Key": "NUTCH-2882",
        "Summary": "Configure NutchUiServer for DEPLOYMENT and improve logging",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Jun/21 01:20",
        "Updated": "29/Jun/21 13:39",
        "Resolved": "28/Jun/21 16:12",
        "Description": "Right now the web app runs in a development mode. It should instead run in a deployment mode.\nImprovements can also be made such that NutchServer and the NutchUIServer log properly.\nPatch coming up...",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/690"
        ]
    },
    "NUTCH-2883": {
        "Key": "NUTCH-2883",
        "Summary": "Provide means to run server as a persistent service in Docker container",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "docker,                                            nutch server,                                            web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Jun/21 02:44",
        "Updated": "11/Sep/22 10:47",
        "Resolved": "11/Sep/22 10:08",
        "Description": "Some conditional logic and some Docker build arguments would allow a user to run the nutch server alone or both the nutch server and webapp as long running services.\nThis could be useful for users who want to remotely interact with Nutch via REST where jobs may actually be executed on yet another remote deployment.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/691",
            "https://github.com/apache/nutch/pull/748"
        ]
    },
    "NUTCH-2884": {
        "Key": "NUTCH-2884",
        "Summary": "Security Layer for 1.x",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "nutch server,                                            REST_api,                                            web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Jul/21 01:47",
        "Updated": "09/Aug/22 07:47",
        "Resolved": null,
        "Description": "We should implement the equivalent of NUTCH-1756 in 1.x.",
        "Issue Links": []
    },
    "NUTCH-2885": {
        "Key": "NUTCH-2885",
        "Summary": "Upgrade to Log4j2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "build,                                            logging",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "10/Jul/21 15:23",
        "Updated": "05/Aug/21 11:25",
        "Resolved": "04/Aug/21 17:02",
        "Description": "log4j2 offers significant optimizations over log4j. I propose to upgrade. This will NOT drop the use of slf4j... just upgrade the underlying logging implementation.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/692"
        ]
    },
    "NUTCH-2886": {
        "Key": "NUTCH-2886",
        "Summary": "Move Nutch WebApp to separate repository",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "build,                                            web gui",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Jul/21 15:32",
        "Updated": "14/Jul/21 00:32",
        "Resolved": "13/Jul/21 23:20",
        "Description": "I propose to move the Nutch WebApp out of the primary Nutch repository and into a new repository https://github.com/apache/nutch-webapp. This work has already been done. The task is now to essentially remove the WebApp code from the canonical Nutch repository at https://github.com/apache/nutch.\nPR coming up...",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/693"
        ]
    },
    "NUTCH-2887": {
        "Key": "NUTCH-2887",
        "Summary": "Migrate to JUnit 5 Jupiter",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "test",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "14/Jul/21 03:42",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "This effort is a bit of a beast. See the JUnit migration tips for general guidance. A general grep for junit in src produces the following\n\n\r\n./test/nutch-site.xml\r\n./test/org/apache/nutch/tools/TestCommonCrawlDataDumper.java\r\n./test/org/apache/nutch/net/TestURLNormalizers.java\r\n./test/org/apache/nutch/net/protocols/TestHttpDateFormat.java\r\n./test/org/apache/nutch/net/TestURLFilters.java\r\n./test/org/apache/nutch/util/TestStringUtil.java\r\n./test/org/apache/nutch/util/TestSuffixStringMatcher.java\r\n./test/org/apache/nutch/util/TestEncodingDetector.java\r\n./test/org/apache/nutch/util/TestMimeUtil.java\r\n./test/org/apache/nutch/util/TestPrefixStringMatcher.java\r\n./test/org/apache/nutch/util/DumpFileUtilTest.java\r\n./test/org/apache/nutch/util/TestNodeWalker.java\r\n./test/org/apache/nutch/util/WritableTestUtils.java\r\n./test/org/apache/nutch/util/TestTableUtil.java\r\n./test/org/apache/nutch/util/TestURLUtil.java\r\n./test/org/apache/nutch/util/TestGZIPUtils.java\r\n./test/org/apache/nutch/parse/TestParseText.java\r\n./test/org/apache/nutch/parse/TestOutlinks.java\r\n./test/org/apache/nutch/parse/TestParseData.java\r\n./test/org/apache/nutch/parse/TestOutlinkExtractor.java\r\n./test/org/apache/nutch/parse/TestParserFactory.java\r\n./test/org/apache/nutch/segment/TestSegmentMerger.java\r\n./test/org/apache/nutch/segment/TestSegmentMergerCrawlDatums.java\r\n./test/org/apache/nutch/plugin/TestPluginSystem.java\r\n./test/org/apache/nutch/fetcher/TestFetcher.java\r\n./test/org/apache/nutch/protocol/TestProtocolFactory.java\r\n./test/org/apache/nutch/protocol/TestContent.java\r\n./test/org/apache/nutch/protocol/AbstractHttpProtocolPluginTest.java\r\n./test/org/apache/nutch/crawl/TestCrawlDbFilter.java\r\n./test/org/apache/nutch/crawl/TestTextProfileSignature.java\r\n./test/org/apache/nutch/crawl/TestCrawlDbStates.java\r\n./test/org/apache/nutch/crawl/TestGenerator.java\r\n./test/org/apache/nutch/crawl/TestAdaptiveFetchSchedule.java\r\n./test/org/apache/nutch/crawl/TODOTestCrawlDbStates.java\r\n./test/org/apache/nutch/crawl/TestSignatureFactory.java\r\n./test/org/apache/nutch/crawl/ContinuousCrawlTestUtil.java\r\n./test/org/apache/nutch/crawl/TestInjector.java\r\n./test/org/apache/nutch/crawl/TestLinkDbMerger.java\r\n./test/org/apache/nutch/crawl/TestCrawlDbMerger.java\r\n./test/org/apache/nutch/service/TestNutchServer.java\r\n./test/org/apache/nutch/metadata/TestMetadata.java\r\n./test/org/apache/nutch/metadata/TestSpellCheckedMetadata.java\r\n./test/org/apache/nutch/indexer/TestIndexingFilters.java\r\n./test/org/apache/nutch/indexer/TestIndexerMapReduce.java\r\n./bin/nutch\r\n./plugin/scoring-orphan/src/test/org/apache/nutch/scoring/orphan/TestOrphanScoringFilter.java\r\n./plugin/index-basic/src/test/org/apache/nutch/indexer/basic/TestBasicIndexingFilter.java\r\n./plugin/urlfilter-domaindenylist/build.xml\r\n./plugin/urlfilter-domaindenylist/src/test/org/apache/nutch/urlfilter/domaindenylist/TestDomainDenylistURLFilter.java\r\n./plugin/protocol-imaps/plugin.xml\r\n./plugin/protocol-imaps/ivy.xml\r\n./plugin/protocol-imaps/lib/junit-4.13.jar\r\n./plugin/protocol-imaps/lib/greenmail-junit4-1.6.0.jar\r\n./plugin/protocol-imaps/lib/greenmail-1.6.0.jar\r\n./plugin/protocol-imaps/src/test/org/apache/nutch/protocol/imaps/TestImaps.java\r\n./plugin/protocol-file/build.xml\r\n./plugin/protocol-file/src/test/org/apache/nutch/protocol/file/TestProtocolFile.java\r\n./plugin/urlnormalizer-regex/build.xml\r\n./plugin/urlnormalizer-regex/src/test/org/apache/nutch/net/urlnormalizer/regex/TestRegexURLNormalizer.java\r\n./plugin/build-plugin.xml\r\n./plugin/creativecommons/src/test/org/creativecommons/nutch/TestCCParseFilter.java\r\n./plugin/urlnormalizer-basic/src/test/org/apache/nutch/net/urlnormalizer/basic/TestBasicURLNormalizer.java\r\n./plugin/urlnormalizer-protocol/build.xml\r\n./plugin/urlnormalizer-protocol/src/test/org/apache/nutch/net/urlnormalizer/protocol/TestProtocolURLNormalizer.java\r\n./plugin/urlfilter-prefix/src/test/org/apache/nutch/urlfilter/prefix/TestPrefixURLFilter.java\r\n./plugin/urlfilter-suffix/src/test/org/apache/nutch/urlfilter/suffix/TestSuffixURLFilter.java\r\n./plugin/index-more/src/test/org/apache/nutch/indexer/more/TestMoreIndexingFilter.java\r\n./plugin/parse-ext/src/test/org/apache/nutch/parse/ext/TestExtParser.java\r\n./plugin/urlnormalizer-host/build.xml\r\n./plugin/urlnormalizer-host/src/test/org/apache/nutch/net/urlnormalizer/host/TestHostURLNormalizer.java\r\n./plugin/protocol-httpclient/src/test/org/apache/nutch/protocol/httpclient/TestProtocolHttpClient.java\r\n./plugin/urlfilter-validator/src/test/org/apache/nutch/urlfilter/validator/TestUrlValidator.java\r\n./plugin/parse-tika/build.xml\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestRTFParser.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestDOMContentUtils.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestImageMetadata.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestRobotsMetaProcessor.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TikaParserTest.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestFeedParser.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestEmbeddedDocuments.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestPdfParser.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestHtmlParser.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestXlsxParser.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestMSWordParser.java\r\n./plugin/parse-tika/src/test/org/apache/nutch/parse/tika/TestOOParser.java\r\n./plugin/mimetype-filter/build.xml\r\n./plugin/mimetype-filter/src/test/org/apache/nutch/indexer/filter/MimeTypeIndexingFilterTest.java\r\n./plugin/urlnormalizer-pass/src/test/org/apache/nutch/net/urlnormalizer/pass/TestPassURLNormalizer.java\r\n./plugin/any23/build.xml\r\n./plugin/any23/src/test/org/apache/nutch/any23/TestAny23ParseFilter.java\r\n./plugin/any23/src/test/org/apache/nutch/any23/TestAny23IndexingFilter.java\r\n./plugin/protocol-okhttp/src/test/org/apache/nutch/protocol/okhttp/TestBadServerResponses.java\r\n./plugin/protocol-okhttp/src/test/org/apache/nutch/protocol/okhttp/TestProtocolOkHttp.java\r\n./plugin/urlfilter-ignoreexempt/build.xml\r\n./plugin/urlfilter-fast/build.xml\r\n./plugin/urlfilter-fast/src/test/org/apache/nutch/urlfilter/fast/TestFastURLFilter.java\r\n./plugin/index-jexl-filter/src/test/org/apache/nutch/indexer/jexl/TestJexlIndexingFilter.java\r\n./plugin/urlfilter-regex/build.xml\r\n./plugin/urlfilter-regex/src/test/org/apache/nutch/urlfilter/regex/TestRegexURLFilter.java\r\n./plugin/lib-regex-filter/src/test/org/apache/nutch/urlfilter/api/RegexURLFilterBaseTest.java\r\n./plugin/headings/src/test/org/apache/nutch/parse/headings/TestHeadingsParseFilter.java\r\n./plugin/protocol-http/src/test/org/apache/nutch/protocol/http/TestBadServerResponses.java\r\n./plugin/protocol-http/src/test/org/apache/nutch/protocol/http/TestProtocolHttp.java\r\n./plugin/parse-swf/build.xml\r\n./plugin/parse-swf/src/test/org/apache/nutch/parse/swf/TestSWFParser.java\r\n./plugin/urlfilter-automaton/build.xml\r\n./plugin/urlfilter-automaton/src/test/org/apache/nutch/urlfilter/automaton/TestAutomatonURLFilter.java\r\n./plugin/parsefilter-regex/build.xml\r\n./plugin/parsefilter-regex/src/test/org/apache/nutch/parsefilter/regex/TestRegexParseFilter.java\r\n./plugin/feed/build.xml\r\n./plugin/feed/src/test/org/apache/nutch/parse/feed/TestFeedParser.java\r\n./plugin/index-anchor/src/test/org/apache/nutch/indexer/anchor/TestAnchorIndexingFilter.java\r\n./plugin/urlfilter-domain/build.xml\r\n./plugin/urlfilter-domain/src/test/org/apache/nutch/urlfilter/domain/TestDomainURLFilter.java\r\n./plugin/language-identifier/src/test/org/apache/nutch/analysis/lang/TestHTMLLanguageParser.java\r\n./plugin/parse-html/src/test/org/apache/nutch/parse/html/TestDOMContentUtils.java\r\n./plugin/parse-html/src/test/org/apache/nutch/parse/html/TestRobotsMetaProcessor.java\r\n./plugin/parse-html/src/test/org/apache/nutch/parse/html/TestHtmlParser.java\r\n./plugin/index-static/src/test/org/apache/nutch/indexer/staticfield/TestStaticFieldIndexerTest.java\r\n./plugin/parse-js/build.xml\r\n./plugin/parse-js/src/test/org/apache/nutch/parse/js/TestJSParseFilter.java\r\n./plugin/index-replace/build.xml\r\n./plugin/index-replace/src/test/org/apache/nutch/indexer/replace/TestIndexReplace.java\r\n./plugin/parse-metatags/build.xml\r\n./plugin/parse-metatags/src/test/org/apache/nutch/parse/metatags/TestMetatagParser.java\r\n./plugin/urlnormalizer-slash/build.xml\r\n./plugin/urlnormalizer-slash/src/test/org/apache/nutch/net/urlnormalizer/slash/TestSlashURLNormalizer.java\r\n./plugin/indexer-csv/src/test/org/apache/nutch/indexwriter/csv/TestCSVIndexWriter.java\r\n./plugin/urlnormalizer-ajax/src/test/org/apache/nutch/net/urlnormalizer/ajax/TestAjaxURLNormalizer.java\r\n./plugin/scoring-metadata/src/test/org/apache/nutch/scoring/metadata/TestMetadataScoringFilter.java\r\n./plugin/lib-http/src/test/org/apache/nutch/protocol/http/api/TestRobotRulesParser.java\r\n./plugin/subcollection/src/test/org/apache/nutch/collection/TestSubcollection.java\r\n./plugin/urlnormalizer-querystring/src/test/org/apache/nutch/net/urlnormalizer/querystring/TestQuerystringURLNormalizer.java\r\n./plugin/parse-zip/build.xml\r\n./plugin/parse-zip/src/test/org/apache/nutch/parse/zip/TestZipParser.java\r\n./plugin/index-links/src/test/org/apache/nutch/indexer/links/TestLinksIndexingFilter.java",
        "Issue Links": []
    },
    "NUTCH-2888": {
        "Key": "NUTCH-2888",
        "Summary": "Selenium Protocol: Support for Selenium 4",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "protocol",
        "Assignee": null,
        "Reporter": "Mikko Kivistoe",
        "Created": "16/Aug/21 11:50",
        "Updated": "16/Aug/21 11:50",
        "Resolved": null,
        "Description": "Hi,\nSelenium 4 is out and it's Grid version supports now HTTPS traffic between the Hub and Nodes. The Selenium 4 api has changed, and it would be good to have Nutch compatible with it",
        "Issue Links": []
    },
    "NUTCH-2889": {
        "Key": "NUTCH-2889",
        "Summary": "nutch indexer-elasticsearch plugin, doesn't work with https protocol",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Nadiia Heckman",
        "Created": "15/Sep/21 20:07",
        "Updated": "17/Aug/22 13:30",
        "Resolved": "17/Aug/22 13:30",
        "Description": "Indexer-elasticsearch plugin is throwing an error, if elasticsearch is running over ssl.\nNeed to add a configurable parameter for index-writers.xml, to define, if elasticsearch host is using https scheme.\nAlso it would be nice, to have connection timeout and socket timeout to be configured as well.",
        "Issue Links": [
            "/jira/browse/NUTCH-2903"
        ]
    },
    "NUTCH-2890": {
        "Key": "NUTCH-2890",
        "Summary": "Protocol-okhttp: upgrade okhttp to 4.9.1 to address infinite connection retries",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Sep/21 09:00",
        "Updated": "22/Sep/21 13:29",
        "Resolved": "22/Sep/21 13:05",
        "Description": "In rare situations and under heavy load (> 100 fetcher threads) I've seen that the protocol-okhttp plugin gets into a state where a significant amount of CPU time is spent in handling failed connections, see async-profiler screenshot. This is caused by okhttp#5819 which is fixed with okhttp 4.5.0).\nWe should upgrade to the okhttp 4.9.1 (the current stable version).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/694",
            "https://github.com/apache/nutch/pull/694"
        ]
    },
    "NUTCH-2891": {
        "Key": "NUTCH-2891",
        "Summary": "Upgrade to Tika 2.1",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Sep/21 11:22",
        "Updated": "01/Dec/21 10:30",
        "Resolved": "01/Dec/21 09:11",
        "Description": "There's already the second release of Tika 2 (2.1.0). Following the 2.0 release notes and the migration guide:\n\nTika 2 is more modular which should allow us to build a smaller parse-tika (66 MiB in the 1.18 binary package) by dropping rarely used parsers - but users should be able to include them if they build Nutch from the sources.\nthe language-identifier plugin needs to be upgraded as well (in addition to Nutch core and the parse-tika plugin). This would include or overlap with NUTCH-2449.\nto avoid that the PDF parser times out we probably want to disable the OCR by default, or at least, provide the configuration snippet for this purpose",
        "Issue Links": [
            "/jira/browse/NUTCH-2449",
            "/jira/browse/NUTCH-2860",
            "https://github.com/apache/nutch/pull/700"
        ]
    },
    "NUTCH-2892": {
        "Key": "NUTCH-2892",
        "Summary": "Upgrade to Any23 2.5",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/Sep/21 02:31",
        "Updated": "22/Nov/21 14:31",
        "Resolved": "22/Nov/21 13:33",
        "Description": "I recently released Any23 which includes some important fixes. I'll go ahead and upgrade.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/695"
        ]
    },
    "NUTCH-2893": {
        "Key": "NUTCH-2893",
        "Summary": "fireant upgrade dependency elasticsearch-rest-high-level-client in src/plugin/indexer-elastic/ivy.xml from 7.11.1 to 7.13.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/Sep/21 02:43",
        "Updated": "18/Sep/21 02:43",
        "Resolved": "18/Sep/21 02:43",
        "Description": "https://github.com/apache/nutch/pull/688",
        "Issue Links": []
    },
    "NUTCH-2894": {
        "Key": "NUTCH-2894",
        "Summary": "Java plugin compilation classpath: priorize plugin dependencies",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/Sep/21 19:29",
        "Updated": "22/Sep/21 09:29",
        "Resolved": "22/Sep/21 08:10",
        "Description": "When compiling Java classes of Nutch plugins, the classpath is constructed with Nutch core dependencies put in front of the plugin dependencies. Since NUTCH-2378 the runtime priorizes the plugin deps.\nSeen when fixing deprecation warnings caused by upgrading okhttp (NUTCH-2890,  2dcd804).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/696"
        ]
    },
    "NUTCH-2895": {
        "Key": "NUTCH-2895",
        "Summary": "Allow to add plugin dependency jars by wildcard",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            runtime",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "20/Sep/21 09:31",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "The plugin descriptors (plugin.xml) require to list all dependent jar files one by one as \"library\". This makes upgrading plugins which include a longer list of dependencies (parse-tika, any23, protocol-okhttp, Selenium-based protocols) a non-trivial task. Maybe we could add a \"wildcard\" rule that allows to add all (remaining) jars in the plugin folder to the classpath.\nSure this would make the Nutch plugin system differ from the original Eclipse plugin architecture. When looking into recent Eclipse plugins: dependencies are in the lib/ folder and listed in the file \"MANIFEST.MF\".",
        "Issue Links": []
    },
    "NUTCH-2896": {
        "Key": "NUTCH-2896",
        "Summary": "Protocol-okhttp: make connection pool configurable",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Sep/21 13:27",
        "Updated": "15/Aug/22 15:47",
        "Resolved": "15/Aug/22 14:38",
        "Description": "OkHttp's ConnectionPool \"holds up to 5 idle connections which will be evicted after 5 minutes of inactivity.\"  A pool of this size is suitable for site crawls but not for larger crawls over many different sites / hosts.\nNote: in the current version (4.9.1) the connection pool is implemented as a linked queue.  In order to scale beyond pool sizes exceeding 1000 we need to use a set of clients each with its own connection pool.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/697"
        ]
    },
    "NUTCH-2897": {
        "Key": "NUTCH-2897",
        "Summary": "Do not supress deprecated API warnings",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "documentation",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Sep/21 18:56",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "We suppress deprecated warnings in three places\n\nPlugin.java#L92-L96\nNutchJob.java#L35-L38, and\nTikaParser.java#L92-L95\n\nInstead of suppressing the warnings we should instead use the correct @Deprecated annotation and @deprecated Javadoc. This is not difficult to do and should have been done first time around.",
        "Issue Links": []
    },
    "NUTCH-2898": {
        "Key": "NUTCH-2898",
        "Summary": "IDE Setup for nutch with Intellij IDEA is not well documented",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "documentation",
        "Assignee": "Abu Sufian Milon",
        "Reporter": "Abu Sufian Milon",
        "Created": "13/Oct/21 11:24",
        "Updated": "19/Oct/21 15:37",
        "Resolved": "19/Oct/21 15:37",
        "Description": "I think, the title is pretty much clear about the issue.\u00a0\nI'm preparing a README.md pull request with a tested solution.\u00a0\nAlso I'm willing to contribute to wiki documentation. I've already created an account at https://cwiki.apache.org/.\u00a0\nUsername: \"liam.logan.web\"\nCan anyone please tell me, where I can find someone, to apply for documentation editing privilege?\nThanks.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/698"
        ]
    },
    "NUTCH-2899": {
        "Key": "NUTCH-2899",
        "Summary": "Remove needless warning about missing o/a/rat/anttasks/antlib.xml",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Oct/21 15:25",
        "Updated": "11/Nov/21 17:28",
        "Resolved": "11/Nov/21 16:17",
        "Description": "(reported in https://stackoverflow.com/questions/69677051/ant-eclipse-not-working-for-apache-nutch-repository)\nThe warning\n\nTrying to override old definition of task javac\r\n  [taskdef] Could not load definitions from resource org/apache/rat/anttasks/antlib.xml. It could not be found.\r\n\n\nis needless and should be removed by moving the task definition into the task it belongs to (\"rat-sources\").",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/701"
        ]
    },
    "NUTCH-2900": {
        "Key": "NUTCH-2900",
        "Summary": "Integrate Nutch with Kerberized Solr Cloud",
        "Type": "New Feature",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "None",
        "Component/s": "indexer",
        "Assignee": null,
        "Reporter": "Geng Hong",
        "Created": "23/Oct/21 08:30",
        "Updated": "31/Mar/22 14:00",
        "Resolved": null,
        "Description": "Currently, we are unable to integrate the Nutch with Solr Cloud that enabled with Kerberos authentication. The error message as below appears:\n\u00a0\nWARN auth.HttpAuthenticator - NEGOTIATE authentication error: No valid credentials provided (Mechanism level: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt))\n\u00a0\n<head>\n <meta http-equiv=\"Content-Type\" content=\"text/html;charset=utf-8\"/>\n <title>Error 401 Authentication required</title>\n </head>\n <body><h2>HTTP ERROR 401</h2>\n <p>Problem accessing /solr/admin/collections. Reason:\n <pre> Authentication required</pre></p>\n </body>\n </html>",
        "Issue Links": []
    },
    "NUTCH-2901": {
        "Key": "NUTCH-2901",
        "Summary": "migrate to maven or gradle",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Abandoned",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "hamide ahadi",
        "Created": "07/Nov/21 12:43",
        "Updated": "13/Jan/22 18:42",
        "Resolved": "13/Jan/22 18:42",
        "Description": "Dear Nutch developer,\nplease relax\u00a0 building process with replacing ant with maven or any modern build tools, it is so annoying to run and debug Nutch with Intellij Idea.\n\u00a0\nBest Regards,\nHamide",
        "Issue Links": [
            "/jira/browse/NUTCH-2934",
            "/jira/browse/NUTCH-2292"
        ]
    },
    "NUTCH-2902": {
        "Key": "NUTCH-2902",
        "Summary": "Jexl parsing error on statements",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "generator",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Nov/21 17:45",
        "Updated": "18/Nov/21 11:37",
        "Resolved": "18/Nov/21 10:51",
        "Description": "(reported by maxockner on the user mailing list)\nJexl 3.0 enforced that a JexlExpression does not contains statements. However, NUTCH-2368 intended that a JexlScript is allowed. Need to switch from JexlExpression to JexlScript in Generator.\nBut should also verify whether to fix other components where Jexl is used:\n\nJexlExchange\nJexlIndexingFilter\nCrawlDbReader",
        "Issue Links": []
    },
    "NUTCH-2903": {
        "Key": "NUTCH-2903",
        "Summary": "Unable to Connect to Elasticsearch over HTTPS",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Geng Hong",
        "Created": "17/Nov/21 02:19",
        "Updated": "23/Feb/23 09:37",
        "Resolved": "09/Jan/22 09:54",
        "Description": "Unable to connect ES from Nutch over HTTPS with the following error message:\n\u00a0\n\u00a02021-11-16 16:37:22,034 DEBUG client.RestClient - request [POST\u00a0\n\u00a0http://192.168.0.105:9200/_bulk?timeout=1m] failed\n\u00a0org.apache.http.ConnectionClosedException: Connection is closed\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.nio.protocol.HttpAsyncRequestExecutor.endOfInput(HttpA\n\u00a0syncRequestExecutor.java:356)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.client.InternalRequestExecutor.endOfInput(Int\n\u00a0ernalRequestExecutor.java:132)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(Def\n\u00a0aultNHttpClientConnection.java:261)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(Intern\n\u00a0alIODispatch.java:81)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(Intern\n\u00a0alIODispatch.java:39)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(Abstrac\n\u00a0tIODispatch.java:114)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.\n\u00a0java:162)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(Abstra\n\u00a0ctIOReactor.java:337)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(Abstr\n\u00a0actIOReactor.java:315)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOR\n\u00a0eactor.java:276)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.j\n\u00a0ava:104)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at\n\u00a0org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.r\n\u00a0un(AbstractMultiworkerIOReactor.java:591)\n\u00a0\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0at java.lang.Thread.run(Thread.java:748)",
        "Issue Links": [
            "/jira/browse/NUTCH-2831",
            "/jira/browse/NUTCH-2889",
            "https://github.com/apache/nutch/pull/703",
            "https://github.com/apache/nutch/pull/703"
        ]
    },
    "NUTCH-2904": {
        "Key": "NUTCH-2904",
        "Summary": "Upgrade to crawler-commons 1.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Nov/21 10:36",
        "Updated": "22/Nov/21 12:48",
        "Resolved": "22/Nov/21 12:48",
        "Description": "Crawler-commons 1.2 has been released. We should upgrade.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/702",
            "https://github.com/apache/nutch/pull/702"
        ]
    },
    "NUTCH-2905": {
        "Key": "NUTCH-2905",
        "Summary": "Mask sensitive strings in log output of index writers",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "indexer,                                            logging,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Nov/21 15:29",
        "Updated": "01/Dec/21 10:30",
        "Resolved": "01/Dec/21 10:14",
        "Description": "Mask sensitive tokens, secrets and passwords in the output of index writers.",
        "Issue Links": [
            "/jira/browse/NUTCH-2906",
            "https://github.com/apache/nutch/pull/704",
            "https://github.com/apache/nutch/pull/704"
        ]
    },
    "NUTCH-2906": {
        "Key": "NUTCH-2906",
        "Summary": "Allow to use credential provider for index writers configuration",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "17/Nov/21 18:00",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "See discussion on user@nutch: passwords and credentials need to be stored in the index-writers.xml in plain text. It should be possible to use a credential provider or other options to hide these sensitive strings, cf.\n\nHadoop CredentialProviderAPI\nConfiguration.getPassword",
        "Issue Links": [
            "/jira/browse/NUTCH-2905"
        ]
    },
    "NUTCH-2907": {
        "Key": "NUTCH-2907",
        "Summary": "protocol-selenium: HTTPS proxy not working",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "18/Nov/21 15:33",
        "Updated": "18/Nov/21 15:33",
        "Resolved": null,
        "Description": "(see bug report and discussion on user@nutch)\nThe Selenium protocol fails to access content via HTTPS if a proxy is configured.\nIn addition: verify that the proxy is always used to access content, even from within the attached browser instance.",
        "Issue Links": []
    },
    "NUTCH-2908": {
        "Key": "NUTCH-2908",
        "Summary": "Log mapreduce job messages and counters in local mode",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "logging",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Nov/21 16:32",
        "Updated": "01/Dec/21 10:30",
        "Resolved": "01/Dec/21 09:32",
        "Description": "Logging the Hadoop job counters in the hadoop.log in local mode is useful for debugging and also to understand what the MapReduce job does. See NUTCH-2519 - needs to be adapted after the Log4j2 upgrade (NUTCH-2885).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/707",
            "https://github.com/apache/nutch/pull/707"
        ]
    },
    "NUTCH-2909": {
        "Key": "NUTCH-2909",
        "Summary": "Establish a metrics naming convention",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "metrics",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "26/Nov/21 19:17",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "I revisited Nutch metrics counters and put some metrics documentation together for others to consult should they wish.\nI thought a comprehensive collection of all Nutch Counters would be useful so I put together a metrics table. One of this (unintended) outcomes was that this highlighted the variability in counter group names and metric names. For example\nMetric Group:\n\nCleaningJobStatus - upper camel case\nCrawlDB filter - inconsistent use of capitalization and space separated\nN/A - the DomainStatistics counters don't belong to a metric group\ninjector - lowercase named after the encapsulating Class\nWebGraph.outlinks - inconsistent use of capitalization and period separated\n\nThe Metric Name's are basically the same... pretty much all over the place.\nI am keen to bring some convention to the Nutch metrics definitions but this is not all plain sailing. I do understand that existing users may rely upon the above metrics as are and changing the values would have impacts downstream.\nPROPOSAL\nI would like to discuss introducing a naming convention which follows some simple principles motivated by a Datadog employees response on SO.\nAs a take on that post, I want to propose the following\n\n1. With regards to Metric Group the highest level of hierarchy is the product line or the process i.e., nutch. The highest level of hierarchy is always lowercase.\n2. The next level of hierarchy is the sub-component/tool, i.e., nutch.Injector, nutch.Generator, nutch.ParseSegment, nutch.SitemapProcessor, etc. This constituent is exactly as that of the enclosing Class. This way it is really simple to trace the metric back to the Class which it was defined within.\n3. The third level of the hierarchy is the metric group which is a general grouping of functionality for the metric being defined i.e. nutch.QueueFeeder.fetcher_status. This constituent is lowercase with words separated by underscore. If no obvious metric group exists simply provide the enclosing Class in lowercase i.e.,  nutch.Injector.injector.urls_filtered\n4. With regards to the Metric Name, the last level of hierarchy is the thing being measured i.e., urls_filtered, above_exception_threshold_in_queue, etc. Everything is lowercase and words separated by underscore. Same as #3 above.\nExample complete metrics\n\nnutch.Injector.injector.urls_filtered\nnutch.ResolverThread.update_host_db.checked_hosts\nnutch.WebGraph.outlinks.added links\n\n\nIt would be greatly appreciated if folks could chime in on the details of the proposal. I'm sure there are several areas which could be improved. \nI will mention that my specific driver for cleaning this up is that I would like to push Nutch metrics into Enterprise Splunk so that the Nutch crawler subsystem will be integrated with all the rest of the subsystems I am responsible for. We use Splunk for that kind of thing. I intend to do that by implementing the Java statsd client but I feel that comes after we clean up metrics and establish a metrics naming convention.\nThanks for any input.",
        "Issue Links": []
    },
    "NUTCH-2910": {
        "Key": "NUTCH-2910",
        "Summary": "FetchItemQueues overloaded constructor also interprets fetcher timeout as -1 e.g. no-timeout.",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "fetcher",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "28/Nov/21 01:37",
        "Updated": "03/Dec/21 12:57",
        "Resolved": "02/Dec/21 01:46",
        "Description": "The FetchItemQueues overloaded constructor attempts to obtain the NON-EXISTENT fetcher.timelimit configuration property.\n\n\r\nthis.timelimit = conf.getLong(\"fetcher.timelimit\", -1);\r\n\n\nAs you can see a default value of -1 is provided. The first parameter is however wrong. It should instead reference the following configuration property.\n\n\r\n<property>\r\n  <name>fetcher.timelimit.mins</name>\r\n  <value>-1</value>\r\n  <description>This is the number of minutes allocated to the fetching.\r\n  Once this value is reached, any remaining entry from the input URL list is skipped \r\n  and all active queues are emptied. The default value of -1 deactivates the time limit.\r\n  </description>\r\n</property>\r\n\n\nNote, fetcher.timelimit.mins\nI think that this essentially means the Fetcher has no time limit which is ofcourse not desired.",
        "Issue Links": []
    },
    "NUTCH-2911": {
        "Key": "NUTCH-2911",
        "Summary": "Add cleanup call in Fetcher.java",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "fetcher",
        "Assignee": null,
        "Reporter": "Prakhar Chaube",
        "Created": "30/Nov/21 13:10",
        "Updated": "03/Dec/21 15:29",
        "Resolved": "03/Dec/21 08:18",
        "Description": "Fetcher's inner class, FetcherRun overrides Hadoop Mapper's run().\nEven though Nutch's FetcherRun doesn't need an explicit call the Mapper's cleanup() (Which is a blank function), it would increase the readability and completeness of the run Method to do so.\nIdeally, every implementation of Mapper is supposed to do the following tasks:\n1. Perform Setup\n2.\u00a0 Call map on the data set\n3. Perform cleanups.\nMoreover, in case a custom Fetcher is written extending Fetcher.java cleanup could get easily missed out.\nPR for Fix: here",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/708"
        ]
    },
    "NUTCH-2912": {
        "Key": "NUTCH-2912",
        "Summary": "CrawlDatumProcessor to calculate crawl completeness",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "01/Dec/21 15:40",
        "Updated": "21/Dec/21 10:08",
        "Resolved": null,
        "Description": "Simple processor that calculates the completeness of the crawl per host.\n\u00a0\nThis does not account for known unknowns, e.g. unfetched URLs that haven't been found yet. Therefore, the calculated percentage can be highly volatile for beginning crawls.",
        "Issue Links": []
    },
    "NUTCH-2913": {
        "Key": "NUTCH-2913",
        "Summary": "nutch-default.xml: document properties set programatically",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "configuration",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "03/Dec/21 12:48",
        "Updated": "19/Aug/22 13:38",
        "Resolved": null,
        "Description": "(see also the discussion in NUTCH-2910)\nnutch-default.xml should include all properties used in Nutch, even those which are set programmatically in Java code in order to pass command-line options and job-global values from the main method (job client) to mapper/reducer tasks.\nWe could use the \"tags\" element (HADOOP-15005) to mark them in a unique way. But a comment in the property description would also be good.\nSee also the table https://cwiki.apache.org/confluence/display/NUTCH/NutchPropertiesCompleteList.",
        "Issue Links": []
    },
    "NUTCH-2914": {
        "Key": "NUTCH-2914",
        "Summary": "nutch-default.xml: remove obsolete and unused properties",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Trivial",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "configuration",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "03/Dec/21 13:13",
        "Updated": "17/Dec/21 09:09",
        "Resolved": "17/Dec/21 09:01",
        "Description": "The following properties in nutch-default.xml are not used in the Nutch codebase and should be removed:\n\nfetcher.verbose (use log levels)\nlink.loops.depth (removed by NUTCH-2201)\npublisher.queue.type (publishers are activated via plugin.includes)",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/709"
        ]
    },
    "NUTCH-2915": {
        "Key": "NUTCH-2915",
        "Summary": "Upgrade to log4j 2.15.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Critical",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "logging",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "12/Dec/21 19:56",
        "Updated": "14/Dec/21 16:30",
        "Resolved": "14/Dec/21 16:30",
        "Description": "See Apache Log4j Security Vulnerabilities.\nNotes:\n\nthe released 1.18 is not directly affected because it uses log4j 1.x which is not affected by CVE-2021-44228. The upgrade from log4j 1.x to 2.14.1 was done recently by NUTCH-2885.\nthe plugin indexer-elastic includes a transitive dependency to log4j-api-2.11.1 which is not affected - only log4j-core is according to comments by slf4j.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/713",
            "https://github.com/apache/nutch/pull/713"
        ]
    },
    "NUTCH-2916": {
        "Key": "NUTCH-2916",
        "Summary": "Fix log file rotation / rename default log file",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "logging",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "13/Dec/21 22:23",
        "Updated": "14/Dec/21 17:36",
        "Resolved": "14/Dec/21 16:36",
        "Description": "The log file rotation isn't working after the upgrade to log4j 2.x (NUTCH-2885):\n\nthe rotation patterns need to fit with (default) log file name: I'd propose now (was wrong in the discussion of [#692|github.com/apache/nutch/pull/692]) to use \"nutch.log\" as default name and to drop the old \"hadoop.log\"\nfile rotation should be also done on start-up\n\nWill open a PR...",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/714"
        ]
    },
    "NUTCH-2917": {
        "Key": "NUTCH-2917",
        "Summary": "Remove transitive dependency to log4j 1.x",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "logging",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "13/Dec/21 23:23",
        "Updated": "22/Dec/21 10:37",
        "Resolved": "22/Dec/21 09:14",
        "Description": "Despite NUTCH-2885 there's still a log4j 1.x jar (lib/log4j-1.2.17.jar) installed as a transitive dependency of Hadoop. Is it required or could it be excluded?",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/718",
            "https://github.com/apache/nutch/pull/718"
        ]
    },
    "NUTCH-2918": {
        "Key": "NUTCH-2918",
        "Summary": "Upgrade to log4j 2.16.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "logging",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "14/Dec/21 16:32",
        "Updated": "17/Dec/21 09:09",
        "Resolved": "17/Dec/21 08:54",
        "Description": "See https://lists.apache.org/thread/d6v4r6nosxysyq9rvnr779336yf0woz4 and NUTCH-2915.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/715",
            "https://github.com/apache/nutch/pull/715"
        ]
    },
    "NUTCH-2919": {
        "Key": "NUTCH-2919",
        "Summary": "NUTCH-2919 Upgrade to Tika 2.2.1 and Any23 2.6",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Dec/21 03:06",
        "Updated": "16/Jan/22 00:36",
        "Resolved": "15/Jan/22 23:24",
        "Description": "Tika 2.2.0 just released\nhttps://lists.apache.org/thread/rbnn1m02o38jkyfh14vjtslh11km26bb",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/717"
        ]
    },
    "NUTCH-2920": {
        "Key": "NUTCH-2920",
        "Summary": "Implement a indexer-opensearch plugin",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "18/Dec/21 04:42",
        "Updated": "06/Mar/23 10:56",
        "Resolved": "06/Mar/23 10:44",
        "Description": "We will be moving to AWS-managed OpenSearch in the near term and I would like to index our content there.\nAs of writing the OpenSearch project has published two plugin versions under thw Apache License v2 so far\nhttps://github.com/opensearch-project/opensearch-java/",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/761"
        ]
    },
    "NUTCH-2921": {
        "Key": "NUTCH-2921",
        "Summary": "DepthScoringFilter option to reset max_depth",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "20/Dec/21 11:46",
        "Updated": "21/Dec/21 12:55",
        "Resolved": null,
        "Description": "Once a max_depth has been set, it cannot be unset or reset. Here's a dirty solution to globally reset the maximum depth for all records.",
        "Issue Links": []
    },
    "NUTCH-2922": {
        "Key": "NUTCH-2922",
        "Summary": "Upgrade to log4j 2.17.0",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "logging",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Dec/21 12:21",
        "Updated": "22/Dec/21 09:16",
        "Resolved": "22/Dec/21 09:16",
        "Description": "Address CVE-2021-45105, see https://logging.apache.org/log4j/2.x/security.html#CVE-2021-45105",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/719"
        ]
    },
    "NUTCH-2923": {
        "Key": "NUTCH-2923",
        "Summary": "Add Job Id in Job Failure messages",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "crawldb,                                            fetcher,                                            generator,                                            injector",
        "Assignee": null,
        "Reporter": "Prakhar Chaube",
        "Created": "30/Dec/21 07:18",
        "Updated": "27/Jan/22 16:39",
        "Resolved": "27/Jan/22 16:10",
        "Description": "In Job classes in case job doesn't succeed a failure message is initialized as follows:\nString message = \"Generator job did not succeed, job status:\"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + job.getStatus().getState() + \", reason: \"\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 + job.getStatus().getFailureInfo();\nHowever, this message doesn't contain JobId which can make it really hard to reach the exact job from the logs. Adding JodId to this message will make it easier to track and locate for debugging.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/721"
        ]
    },
    "NUTCH-2924": {
        "Key": "NUTCH-2924",
        "Summary": "Generate maxCount expr evaluated only once",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16",
        "Fix Version/s": "1.20",
        "Component/s": "generator",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "30/Dec/21 16:01",
        "Updated": "12/Dec/22 16:57",
        "Resolved": "12/Dec/22 15:14",
        "Description": "The generate.maxCount expression is evaluated only once in the generator's reducer, instead, it must be set once per host.",
        "Issue Links": [
            "/jira/browse/NUTCH-2455"
        ]
    },
    "NUTCH-2925": {
        "Key": "NUTCH-2931 Improvements to 1.x REST API",
        "Summary": "Secure the Nutch REST API using Apache Shiro",
        "Type": "Sub-task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "REST_api,                                            security",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "04/Jan/22 17:01",
        "Updated": "25/Jan/22 03:50",
        "Resolved": null,
        "Description": "We use nutch-helm to provide a convenient mechanism for certain internal stakeholders to interact with Nutch via REST. \nAPI security is entirely lacking from the REST API.\nI propose to secure it using the popular Apache Shiro framework.\nI am using Protecting JAX-RS Resources with RBAC and Apache Shiro as a source of guidance.",
        "Issue Links": [
            "/jira/browse/NUTCH-1846"
        ]
    },
    "NUTCH-2926": {
        "Key": "NUTCH-2931 Improvements to 1.x REST API",
        "Summary": "Implement persistent storage for Nutch Webserver resources",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "nutch server,                                            storage",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "04/Jan/22 18:04",
        "Updated": "12/Jan/22 05:24",
        "Resolved": null,
        "Description": "The Nutch webserver caches resources (seed lists, configuration, jobs, etc.) in-memory. This is not a reliable or resilient solution for users who want a persistent Nutch server service for the enterprise. \nI therefore propose to add a persistence mechanism which will address this problem.\nI intend to use JOOQ as a thin layer on top of JDBC. This will provide flexibility for deploying a wide variety of RDBMS backends.\nh2 is a popular, very lightweight (~2.5 MB) appropriately-licensed solution we could use as the initial backend. I intend to use it in embedded mode with enabled persistence so we'll have data on the disk. This means that if we stop Nutch server we can restart and restore server resources from disk.\nSome resources\nhttps://www.jooq.org/\nhttps://h2database.com/html/download.html\nhttp://www.h2database.com/html/tutorial.html#using_jooq",
        "Issue Links": []
    },
    "NUTCH-2927": {
        "Key": "NUTCH-2927",
        "Summary": "indexer-elastic: use Java API client",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "09/Jan/22 10:46",
        "Updated": "01/Mar/23 17:26",
        "Resolved": null,
        "Description": "See Lewis comment in PR #713 (NUTCH-2903): \"High Level REST Client was deprecated in ES 7.15.0 in favor of the Java API Client\"",
        "Issue Links": [
            "/jira/browse/NUTCH-2960"
        ]
    },
    "NUTCH-2928": {
        "Key": "NUTCH-2928",
        "Summary": "Fix favicon of content pages",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "website",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "09/Jan/22 12:51",
        "Updated": "08/Sep/22 13:29",
        "Resolved": "08/Sep/22 13:29",
        "Description": "The content pages (eg. https://nutch.apache.org/) of the new Hugo-based website (see NUTCH-2826) show the Kube favicon instead of the Nutch one.\nCould be done by either removing/emptying the Kube layout snippet layouts/partials/favicon.html or by changing it to <link rel=\"shortcut icon\" type=\"image/ico\" href=\"/favicon.ico\">. The favicon.ico should be also included in the main branch.",
        "Issue Links": [
            "/jira/browse/NUTCH-2826"
        ]
    },
    "NUTCH-2929": {
        "Key": "NUTCH-2929",
        "Summary": "Fetcher: start threads slowly to avoid that resources are temporarily exhausted",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Jan/22 14:03",
        "Updated": "14/Jan/22 10:37",
        "Resolved": "14/Jan/22 09:42",
        "Description": "Fetcher spins all threads without any delay. This may cause that certain resources are temporarily exhausted if all threads start fetching the first pages simultaneously.\nThe issue has been observed by Tika warnings about overuse of the SAXParser pool which appeared only during the first 2-5 minutes of fetching a segment. See https://lists.apache.org/thread/lo6b9wdlxy2lz12wmosldgl9x9ov1cks - adding a short delay between thread launches makes the warnings disappear.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/722"
        ]
    },
    "NUTCH-2930": {
        "Key": "NUTCH-2930",
        "Summary": "Protocol-okhttp: implement IP filter",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "plugin,                                            protocol,                                            security",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Jan/22 15:02",
        "Updated": "19/Aug/22 14:56",
        "Resolved": "19/Aug/22 13:26",
        "Description": "In order to avoid information leakage to a public search index or web archive, it should be possible to configure Nutch in a way that no content is fetched from localhost, loop-back addresses, private address spaces.\nNUTCH-2527 adds the configuration snippets to exclude URLs pointing to private addresses. \nHowever, filtering URLs isn't enough because a DNS entry of an arbitrary host name may point to a private IP address. Blocking must happen on the protocol level because the IP address is only know in the protocol implementation. I'll add an implementation for protocol-okhttp.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/736"
        ]
    },
    "NUTCH-2931": {
        "Key": "NUTCH-2931",
        "Summary": "Improvements to 1.x REST API",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "None",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Jan/22 05:09",
        "Updated": "12/Jan/22 05:09",
        "Resolved": null,
        "Description": "Shi Wei's thread on user@ has motivated me to finally start flushing out some improvements I would like to make to the 1.x REST API. This issue is going to act as the parent.",
        "Issue Links": []
    },
    "NUTCH-2932": {
        "Key": "NUTCH-2931 Improvements to 1.x REST API",
        "Summary": "Create OpenAPI specification for Nutch 1.x REST API",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "None",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Jan/22 05:15",
        "Updated": "12/Jan/22 05:15",
        "Resolved": null,
        "Description": "The OpenAPI specification is a very clean and intuitive way for us to represent the Nutch 1.x REST API. I propose that we produce one. It would really help us produce better documentation. We could also use the code generation capabilities to improve/standardize the resulting server implementation.",
        "Issue Links": []
    },
    "NUTCH-2933": {
        "Key": "NUTCH-2931 Improvements to 1.x REST API",
        "Summary": "GET /seed doesn't return previously generated seed lists",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "REST_api",
        "Assignee": null,
        "Reporter": "Lewis John McGibbney",
        "Created": "12/Jan/22 05:24",
        "Updated": "21/Aug/22 10:02",
        "Resolved": null,
        "Description": "A GET on the /seed endpoint facilitates retrieval of any seedlists which were created during the current server runtime.\nCreated seed lists are persistent. That is to say they remain on disk even when nutch server is not running. \nAs of Nutch 1.18 seed lists generated by previous server runtime sessions will not be available if the server is shutdown and restarted.\nThe server should either load all seed lists from disk relative to where it was started or else load them from a persistent datastore such as h2.",
        "Issue Links": []
    },
    "NUTCH-2934": {
        "Key": "NUTCH-2934",
        "Summary": "Replace Apache Ant build system with Gradle",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "13/Jan/22 18:38",
        "Updated": "17/Feb/22 17:10",
        "Resolved": null,
        "Description": "My project proposal to USC's Senior Capstone Program was accepted. We are starting work on replacing Ant with Gradle on Tuesday 18th, January 2022. This ticket will act as a parent ticket for the entire project. More details will be flushed out in due course.",
        "Issue Links": [
            "/jira/browse/NUTCH-2293",
            "/jira/browse/NUTCH-2244",
            "/jira/browse/NUTCH-2901",
            "/jira/browse/NUTCH-2292",
            "/jira/browse/NUTCH-2638"
        ]
    },
    "NUTCH-2935": {
        "Key": "NUTCH-2935",
        "Summary": "DeduplicationJob: failure on URLs with invalid percent encoding",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "crawldb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "14/Jan/22 09:11",
        "Updated": "17/Jan/22 19:37",
        "Resolved": "17/Jan/22 18:59",
        "Description": "The DeduplicationJob may fail with an IllegalArgumentException on invalid percent encodings in URLs:\n\n2021-11-25 04:36:41,747 INFO mapreduce.Job: Task Id : attempt_1637669672674_0018_r_000193_0, Status : FAILED\r\nError: java.lang.IllegalArgumentException: URLDecoder: Illegal hex characters in escape (%) pattern - Error at index 0 in: \"YR\"\r\n        at java.base/java.net.URLDecoder.decode(URLDecoder.java:232)\r\n        at java.base/java.net.URLDecoder.decode(URLDecoder.java:142)\r\n        at org.apache.nutch.crawl.DeduplicationJob$DedupReducer.getDuplicate(DeduplicationJob.java:211)\r\n...\r\nException in thread \"main\" java.lang.RuntimeException: Crawl job did not succeed, job status:FAILED, reason: Task failed task_1637669672674_0018_r_000193\r\nJob failed as tasks failed. failedMaps:0 failedReduces:1 killedMaps:0 killedReduces: 0\r\n\n\nThe IllegalArgumentException should be caught, logged and if only one of the two URLs with duplicated content is invalid, it should be flagged as duplicate while the valid URL \"survives\".",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/723"
        ]
    },
    "NUTCH-2936": {
        "Key": "NUTCH-2936",
        "Summary": "Early registration of URL stream handlers provided by plugins may fail Hadoop jobs running in distributed mode if protocol-okhttp is used",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "plugin,                                            protocol",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Sebastian Nagel",
        "Created": "14/Jan/22 14:04",
        "Updated": "09/Aug/22 08:52",
        "Resolved": "09/Aug/22 07:28",
        "Description": "After merging NUTCH-2429 I've observed that Nutch jobs running in distributed mode may fail early with the following dubious error:\n\n2022-01-14 13:11:45,751 ERROR crawl.DedupRedirectsJob: DeduplicationJob: java.io.IOException: Error generating shuffle secret key\r\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:182)\r\n        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1565)\r\n        at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1562)\r\n        at java.base/java.security.AccessController.doPrivileged(Native Method)\r\n        at java.base/javax.security.auth.Subject.doAs(Subject.java:423)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)\r\n        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1562)\r\n        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1583)\r\n        at org.apache.nutch.crawl.DedupRedirectsJob.run(DedupRedirectsJob.java:301)\r\n        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)\r\n        at org.apache.nutch.crawl.DedupRedirectsJob.main(DedupRedirectsJob.java:379)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n        at org.apache.hadoop.util.RunJar.run(RunJar.java:323)\r\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:236)\r\nCaused by: java.security.NoSuchAlgorithmException: HmacSHA1 KeyGenerator not available\r\n        at java.base/javax.crypto.KeyGenerator.<init>(KeyGenerator.java:177)\r\n        at java.base/javax.crypto.KeyGenerator.getInstance(KeyGenerator.java:244)\r\n        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:179)\r\n        ... 16 more\r\n\n\nAfter removing the early registration of URL stream handlers (see NUTCH-2429) in NutchJob and NutchTool, the job starts without errors.\nNotes:\n\nthe job this error was observed a custom de-duplication job to flag redirects pointing to the same target URL. But I'll try to reproduce it with a standard Nutch job and in pseudo-distributed mode.\nshould also verify whether registering URL stream handlers works at all in distributed mode. Tasks are launched differently, not as NutchJob or NutchTool.",
        "Issue Links": [
            "/jira/browse/NUTCH-2949",
            "/jira/browse/NUTCH-2429",
            "https://github.com/apache/nutch/pull/726",
            "https://github.com/apache/nutch/pull/733"
        ]
    },
    "NUTCH-2937": {
        "Key": "NUTCH-2937",
        "Summary": "parse-tika: review dependency exclusions and avoid dependency conflicts in distributed mode",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "parser,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "15/Jan/22 14:14",
        "Updated": "21/Aug/22 11:44",
        "Resolved": null,
        "Description": "While testing NUTCH-2919 I've seen the following error caused by a conflicting dependency to commons-io:\n\n2.11.0 Nutch core\n2.11.0 parse-tika (excluded to avoid duplicated dependencies)\n2.5 provided by Hadoop\n\nThis causes errors parsing some office and other documents (but not all), for example:\n\n2022-01-15 01:36:31,365 WARN [FetcherThread] org.apache.nutch.parse.ParseUtil: Error parsing http://kurskrun.ru/privacypolicy with org.apache.nutch.parse.tika.TikaParser\r\njava.util.concurrent.ExecutionException: java.lang.NoSuchMethodError: 'org.apache.commons.io.input.CloseShieldInputStream org.apache.commons.io.input.CloseShieldInputStream.wrap(java.io.InputStream)'\r\n        at java.base/java.util.concurrent.FutureTask.report(FutureTask.java:122)\r\n        at java.base/java.util.concurrent.FutureTask.get(FutureTask.java:205)\r\n        at org.apache.nutch.parse.ParseUtil.runParser(ParseUtil.java:188)\r\n        at org.apache.nutch.parse.ParseUtil.parse(ParseUtil.java:92)\r\n        at org.apache.nutch.fetcher.FetcherThread.output(FetcherThread.java:715)\r\n        at org.apache.nutch.fetcher.FetcherThread.run(FetcherThread.java:431)\r\nCaused by: java.lang.NoSuchMethodError: 'org.apache.commons.io.input.CloseShieldInputStream org.apache.commons.io.input.CloseShieldInputStream.wrap(java.io.InputStream)'\r\n        at org.apache.tika.parser.microsoft.ooxml.OOXMLExtractorFactory.parse(OOXMLExtractorFactory.java:120)\r\n        at org.apache.tika.parser.microsoft.ooxml.OOXMLParser.parse(OOXMLParser.java:115)\r\n        at org.apache.tika.parser.CompositeParser.parse(CompositeParser.java:289)\r\n        at org.apache.nutch.parse.tika.TikaParser.getParse(TikaParser.java:151)\r\n        at org.apache.nutch.parse.tika.TikaParser.getParse(TikaParser.java:90)\r\n        at org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:34)\r\n        at org.apache.nutch.parse.ParseCallable.call(ParseCallable.java:23)\r\n        at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n        at java.base/java.lang.Thread.run(Thread.java:829)",
        "Issue Links": []
    },
    "NUTCH-2938": {
        "Key": "NUTCH-2938",
        "Summary": "Use Any23's RepositoryWriter to write structured data to Rdf4j repository",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "any23,                                            plugin",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "16/Jan/22 00:43",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "I have been running a patch which leverages Any23's RepositoryWriter (implemented as one of a number of TripleHandler's via CompositeTripleHandler) to write Any23 extractions to GraphDB. This enables us to build a content graph from data across the enterprise.\nThis feature is turned off by default so will not change existing Any23 behaviour. I have concerns about the performance of this patch because right now we need to create a new repository connection for each URL. This is not great so I will definitely improve on it.\nPR coming up.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/725"
        ]
    },
    "NUTCH-2939": {
        "Key": "NUTCH-2934 Replace Apache Ant build system with Gradle",
        "Summary": "Create Initial Jenkinsfile for Nutch Gradle Build",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            gradle,                                            jenkins",
        "Assignee": "Ryan Li",
        "Reporter": "Lewis John McGibbney",
        "Created": "17/Feb/22 17:14",
        "Updated": "25/Apr/22 23:41",
        "Resolved": null,
        "Description": "From the start we will be working on a Jenkins pipeline a.k.a Jenkinsfile. This issue will deliver one which we will use in a new build under https://ci-builds.apache.org/job/Nutch/\n\u00a0\nUpdate: GitHub pull request has been created for this issue at https://github.com/csci401-apache-nutch/nutch/pull/18",
        "Issue Links": []
    },
    "NUTCH-2940": {
        "Key": "NUTCH-2934 Replace Apache Ant build system with Gradle",
        "Summary": "Develop Gradle Core Build for Apache Nutch",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "James Simmons",
        "Created": "17/Feb/22 17:22",
        "Updated": "16/Jun/22 02:22",
        "Resolved": null,
        "Description": "This issue will focus on the build lifecycle management for the core build of Apache Nutch as seen here: https://github.com/apache/nutch/tree/master/src/java",
        "Issue Links": []
    },
    "NUTCH-2941": {
        "Key": "NUTCH-2934 Replace Apache Ant build system with Gradle",
        "Summary": "Migrate plugins over to Gradle build system",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Ryan Li",
        "Created": "28/Mar/22 00:23",
        "Updated": "28/Mar/22 00:23",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2942": {
        "Key": null,
        "Summary": null,
        "Type": null,
        "Status": null,
        "Priority": null,
        "Resolution": null,
        "Affects Version/s": null,
        "Fix Version/s": null,
        "Component/s": null,
        "Assignee": null,
        "Reporter": null,
        "Created": null,
        "Updated": null,
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2943": {
        "Key": "NUTCH-2934 Replace Apache Ant build system with Gradle",
        "Summary": "Implement core dependencies in build.gradle.kts",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Iman Arfa-Zanganeh",
        "Created": "04/Apr/22 18:08",
        "Updated": "22/Apr/22 21:56",
        "Resolved": "22/Apr/22 21:56",
        "Description": null,
        "Issue Links": [
            "https://github.com/apache/nutch/pull/727"
        ]
    },
    "NUTCH-2944": {
        "Key": "NUTCH-2934 Replace Apache Ant build system with Gradle",
        "Summary": "Create Gradle Javadoc task",
        "Type": "Sub-task",
        "Status": "In Progress",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "build,                                            gradle",
        "Assignee": "Lewis John McGibbney",
        "Reporter": "Lewis John McGibbney",
        "Created": "22/Apr/22 22:55",
        "Updated": "22/Apr/22 22:55",
        "Resolved": null,
        "Description": "Implement the equivalent of the Ant javadoc target using the Gradle Javadoc task https://docs.gradle.org/current/dsl/org.gradle.api.tasks.javadoc.Javadoc.html",
        "Issue Links": []
    },
    "NUTCH-2945": {
        "Key": "NUTCH-2945",
        "Summary": "Solr Index Writer pluging schema.xml missing a copyToField",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "indexer",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Danielle Fisla",
        "Created": "01/May/22 20:01",
        "Updated": "04/Mar/23 01:03",
        "Resolved": "17/Aug/22 13:44",
        "Description": "Solr Index Writer plugin schema.xml missing a copyToField\n\u00a0\njava.lang.Exception: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://localhost:8983/solr/nutch: copyF\nield dest :'description_str' is not an explicit field and doesn't match a dynamicField.\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492) ~[hadoop-mapreduce-client-common-3.1.3.jar:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:559) [hadoop-mapreduce-client-common-3.1.3.jar:?]\nCaused by: org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://localhost:8983/solr/nutch: copyField dest\u00a0\n:'description_str' is not an explicit field and doesn't match a dynamicField.\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:665) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:265) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:248) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.solr.client.solrj.SolrClient.request(SolrClient.java:1290) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.indexwriter.solr.SolrIndexWriter.push(SolrIndexWriter.java:250) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.indexwriter.solr.SolrIndexWriter.commit(SolrIndexWriter.java:219) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.indexer.IndexWriters.commit(IndexWriters.java:264) ~[apache-nutch-1.19-SNAPSHOT.jar:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.indexer.IndexerOutputFormat$1.close(IndexerOutputFormat.java:54) ~[apache-nutch-1.19-SNAPSHOT.jar:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.close(ReduceTask.java:551) ~[hadoop-mapreduce-client-core-3.1.3.jar:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:630) ~[hadoop-mapreduce-client-core-3.1.3.jar:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:390) ~[hadoop-mapreduce-client-core-3.1.3.jar:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:347) ~[hadoop-mapreduce-client-common-3.1.3.jar:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) ~[?:?]\n\u00a0 \u00a0 \u00a0 \u00a0 at java.lang.Thread.run(Thread.java:833) ~[?:?]",
        "Issue Links": [
            "/jira/browse/NUTCH-2957"
        ]
    },
    "NUTCH-2946": {
        "Key": "NUTCH-2946",
        "Summary": "Fetcher: optionally slow down fetching from hosts with repeated exceptions",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "03/May/22 14:07",
        "Updated": "19/May/22 14:53",
        "Resolved": "19/May/22 13:26",
        "Description": "The fetcher holds for every fetch queue a counter which counts the number of observed \"exceptions\" seen when fetching from the host (resp. domain or IP) bound to this queue.\nAs an improvement to increase the politeness of the crawler, the counter value could be used to dynamically increase the fetch delay for hosts where requests fail repeatedly with exceptions or HTTP status codes mapped to ProtocolStatus.EXCEPTION (HTTP 403 Forbidden, 429 Too many requests, 5xx server errors, etc.) Of course, this should be optional. The aim to reduce the load on such hosts already before the configured max. number of exceptions (property fetcher.max.exceptions.per.queue) is hit.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/728"
        ]
    },
    "NUTCH-2947": {
        "Key": "NUTCH-2947",
        "Summary": "Fetcher: keep state of empty fetch queues unless queue feeder is finished",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "03/May/22 14:27",
        "Updated": "15/Aug/22 15:47",
        "Resolved": "15/Aug/22 14:41",
        "Description": "If a fetch queue is empty (containing no fetch items) it may be removed from the list of queues. This also remove the state of a fetch queue, namely the next fetch time and the exception counter. If the queue feeder is still active it may happened that the same queue (i.e. associated with the same host/domain/IP) removed before is created again. In this case, certain aspects of fetcher politeness cannot be guaranteed anymore:\n\nthe fetch delay (via earliest next fetch time) and\nthe mechanism to block fetching from the same host/domain/IP with too many exceptions (NUTCH-769).\n\nThe issue was observed while verifying NUTCH-2946 in the fetcher logs:\n\n... 10:19:16,912 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 10:20:16,250 * queue foo.bar >> delayed next fetch by 79248 ms after 2 exceptions in queue\r\n... 10:21:52,675 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 10:25:40,931 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 10:27:45,066 * queue foo.bar >> delayed next fetch by 79248 ms after 2 exceptions in queue\r\n... 10:29:40,407 * queue foo.bar >> delayed next fetch by 100000 ms after 3 exceptions in queue\r\n... 10:41:48,870 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 10:47:54,946 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 10:52:46,792 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 10:57:43,470 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:01:12,220 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:04:24,621 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:18:40,398 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:21:09,437 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:34:36,052 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:39:17,898 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:40:35,472 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:50:34,224 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:51:27,547 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:53:04,783 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 11:54:04,404 * queue foo.bar >> delayed next fetch by 79248 ms after 2 exceptions in queue\r\n... 11:55:38,232 * queue foo.bar >> delayed next fetch by 100000 ms after 3 exceptions in queue\r\n... 11:57:37,942 * queue foo.bar >> delayed next fetch by 116096 ms after 4 exceptions in queue\r\n... 12:01:08,619 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue\r\n... 12:02:35,985 * queue foo.bar >> delayed next fetch by 50000 ms after 1 exceptions in queue",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/729"
        ]
    },
    "NUTCH-2948": {
        "Key": "NUTCH-2948",
        "Summary": "Upgrade dependencies to Any23 2.7 and Tika 2.3.0",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "parser,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "05/May/22 16:58",
        "Updated": "12/May/22 15:42",
        "Resolved": "12/May/22 14:23",
        "Description": "Note: I've tried to upgrade to Tika 2.4.0 but there was a test failure due to conflicting dependencies (Any23 2.7 depends on Tika 2.3.0. For now, let's upgrade from Tika 2.2.1 to 2.3.0.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/730"
        ]
    },
    "NUTCH-2949": {
        "Key": "NUTCH-2949",
        "Summary": "Tasks of a multi-threaded map runner may fail because of slow creation of URL stream handlers",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "net,                                            plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/May/22 09:20",
        "Updated": "09/Aug/22 07:35",
        "Resolved": "09/Aug/22 07:35",
        "Description": "While running a custom Nutch job (code here), many but not all task failed exceeding the the Hadoop task time-out (`mapreduce.task.timeout`) without generating any \"heartbeat\" (output, counter increments, log messages). Hadoop logs the stacks of all threads of the timed out task. That's the base for the excerpts below.\nThe job runs a MultithreadedMapper - most of the mapper threads (48 in total) are waiting for the URLStreamHandler in order to construct a java.net.URL object:\n\n\"Thread-11\" #27 prio=5 os_prio=0 cpu=243.78ms elapsed=647.25s tid=0x00007f3eb5b0f800 nid=0x8e651 waiting for monitor entry \u00a0[0x00007f3e84ef9000]\r\n\u00a0 \u00a0java.lang.Thread.State: BLOCKED (on object monitor)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:162)\r\n\u00a0 \u00a0 \u00a0 \u00a0 - waiting to lock <0x00000006a1bc0630> (a java.lang.String)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.plugin.PluginRepository.createURLStreamHandler(PluginRepository.java:597)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.plugin.URLStreamHandlerFactory.createURLStreamHandler(URLStreamHandlerFactory.java:95)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.net.URL.getURLStreamHandler(java.base@11.0.15/URL.java:1432)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.net.URL.<init>(java.base@11.0.15/URL.java:651)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.net.URL.<init>(java.base@11.0.15/URL.java:541)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at java.net.URL.<init>(java.base@11.0.15/URL.java:488)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.normalize(BasicURLNormalizer.java:179)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.net.URLNormalizers.normalize(URLNormalizers.java:318)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.crawl.Injector$InjectMapper.filterNormalize(Injector.java:157)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper$SitemapProcessor.getContent(SitemapInjector.java:670)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper$SitemapProcessor.process(SitemapInjector.java:439)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper.map(SitemapInjector.java:325)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper.map(SitemapInjector.java:145)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\r\n\u00a0 \u00a0 \u00a0 \u00a0 at org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper$MapRunner.run(MultithreadedMapper.java:274)\r\n\n\nOnly a single mapper thread is active:\n\n\"Thread-23\" #39 prio=5 os_prio=0 cpu=5830.17ms elapsed=647.09s tid=0x00007f3eb5b42800 nid=0x8e661 in Object.wait()  [0x00007f3e842ec000]\r\n   java.lang.Thread.State: RUNNABLE\r\n        at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(java.base@11.0.15/Native Method)\r\n        at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(java.base@11.0.15/NativeConstructorAccessorImpl.java:62)\r\n        at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(java.base@11.0.15/DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(java.base@11.0.15/Constructor.java:490)\r\n        at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:170)\r\n        - locked <0x00000006a1bc0630> (a java.lang.String)\r\n        at org.apache.nutch.plugin.PluginRepository.createURLStreamHandler(PluginRepository.java:597)\r\n        at org.apache.nutch.plugin.URLStreamHandlerFactory.createURLStreamHandler(URLStreamHandlerFactory.java:95)\r\n        at java.net.URL.getURLStreamHandler(java.base@11.0.15/URL.java:1432)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:651)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:541)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:488)\r\n        at org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.normalize(BasicURLNormalizer.java:179)\r\n        at org.apache.nutch.net.URLNormalizers.normalize(URLNormalizers.java:318)\r\n        at org.apache.nutch.crawl.Injector$InjectMapper.filterNormalize(Injector.java:157)\r\n        at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper$SitemapProcessor.getContent(SitemapInjector.java:670)\r\n        at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper$SitemapProcessor.process(SitemapInjector.java:439)\r\n        at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper.map(SitemapInjector.java:325)\r\n        at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper.map(SitemapInjector.java:145)\r\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\r\n        at org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper$MapRunner.run(MultithreadedMapper.java:274)\r\n\n\nThe stack of one thread shows a nested lock:\n\n\"Thread-26\" #42 prio=5 os_prio=0 cpu=24.93ms elapsed=647.07s tid=0x00007f3eb5b49800 nid=0x8e664 waiting for monitor entry  [0x00007f3e83fe7000]\r\n   java.lang.Thread.State: BLOCKED (on object monitor)\r\n        at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:162)\r\n        - waiting to lock <0x00000006a1bc0630> (a java.lang.String)\r\n        at org.apache.nutch.plugin.PluginRepository.createURLStreamHandler(PluginRepository.java:597)\r\n        at org.apache.nutch.plugin.URLStreamHandlerFactory.createURLStreamHandler(URLStreamHandlerFactory.java:95)\r\n        at java.net.URL.getURLStreamHandler(java.base@11.0.15/URL.java:1432)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:651)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:541)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:488)\r\n        at javax.crypto.JceSecurity.<clinit>(java.base@11.0.15/JceSecurity.java:239)\r\n        at javax.crypto.Cipher.getInstance(java.base@11.0.15/Cipher.java:540)\r\n        at sun.security.ssl.JsseJce.getCipher(java.base@11.0.15/JsseJce.java:190)\r\n        at sun.security.ssl.SSLCipher.isTransformationAvailable(java.base@11.0.15/SSLCipher.java:509)\r\n        at sun.security.ssl.SSLCipher.<init>(java.base@11.0.15/SSLCipher.java:498)\r\n        at sun.security.ssl.SSLCipher.<clinit>(java.base@11.0.15/SSLCipher.java:81)\r\n        at sun.security.ssl.CipherSuite.<clinit>(java.base@11.0.15/CipherSuite.java:65)\r\n        at sun.security.ssl.SSLContextImpl.getApplicableSupportedCipherSuites(java.base@11.0.15/SSLContextImpl.java:348)\r\n        at sun.security.ssl.SSLContextImpl$AbstractTLSContext.<clinit>(java.base@11.0.15/SSLContextImpl.java:580)\r\n        at java.lang.Class.forName0(java.base@11.0.15/Native Method)\r\n        at java.lang.Class.forName(java.base@11.0.15/Class.java:315)\r\n        at java.security.Provider$Service.getImplClass(java.base@11.0.15/Provider.java:1918)\r\n        at java.security.Provider$Service.newInstance(java.base@11.0.15/Provider.java:1894)\r\n        at sun.security.jca.GetInstance.getInstance(java.base@11.0.15/GetInstance.java:236)\r\n        at sun.security.jca.GetInstance.getInstance(java.base@11.0.15/GetInstance.java:164)\r\n        at javax.net.ssl.SSLContext.getInstance(java.base@11.0.15/SSLContext.java:168)\r\n        at org.apache.nutch.protocol.okhttp.OkHttp.<clinit>(OkHttp.java:97)\r\n        at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(java.base@11.0.15/Native Method)\r\n        at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(java.base@11.0.15/NativeConstructorAccessorImpl.java:62)\r\n        at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(java.base@11.0.15/DelegatingConstructorAccessorImpl.java:45)\r\n        at java.lang.reflect.Constructor.newInstance(java.base@11.0.15/Constructor.java:490)\r\n        at org.apache.nutch.plugin.Extension.getExtensionInstance(Extension.java:170)\r\n        - locked <0x00000006a1bc0410> (a java.lang.String)\r\n        at org.apache.nutch.plugin.PluginRepository.createURLStreamHandler(PluginRepository.java:597)\r\n        at org.apache.nutch.plugin.URLStreamHandlerFactory.createURLStreamHandler(URLStreamHandlerFactory.java:95)\r\n        at java.net.URL.getURLStreamHandler(java.base@11.0.15/URL.java:1432)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:651)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:541)\r\n        at java.net.URL.<init>(java.base@11.0.15/URL.java:488)\r\n        at org.apache.nutch.net.urlnormalizer.basic.BasicURLNormalizer.normalize(BasicURLNormalizer.java:179)\r\n        at org.apache.nutch.net.URLNormalizers.normalize(URLNormalizers.java:318)\r\n        at org.apache.nutch.crawl.Injector$InjectMapper.filterNormalize(Injector.java:157)\r\n        at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper$SitemapProcessor.getContent(SitemapInjector.java:670)\r\n        at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper$SitemapProcessor.process(SitemapInjector.java:439)\r\n        at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper.map(SitemapInjector.java:325)\r\n        at org.apache.nutch.crawl.SitemapInjector$SitemapInjectMapper.map(SitemapInjector.java:145)\r\n        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)\r\n        at org.apache.hadoop.mapreduce.lib.map.MultithreadedMapper$MapRunner.run(MultithreadedMapper.java:274)\r\n\n\nIt isn't a dead-lock because the lock is on two different String objects in different lines of code. However, a recursive barrier may make the performance impact of locks much worse. The stack also shows that Nutch's URLStreamHandlerFactory is also called from Java-internal methods. The cryptographic classes in the stack could also give a hint what's going wrong in NUTCH-2936.\nI'm not yet clear what the solution could be:\n\nExtension.getExtensionInstance() with nested synchronization is for sure not suitable to be called frequently\nmaybe caching the URL stream handlers solves the problem\nbut we should keep also NUTCH-2936 on the radar when looking for solutions",
        "Issue Links": [
            "/jira/browse/NUTCH-2429",
            "/jira/browse/NUTCH-2936"
        ]
    },
    "NUTCH-2950": {
        "Key": "NUTCH-2950",
        "Summary": "UpdateHostDb: performance improvements",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "hostdb",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/May/22 14:59",
        "Updated": "24/May/22 14:07",
        "Resolved": "24/May/22 12:55",
        "Description": "This issue addresses a couple of performance improvements when creating the HostDb:\n\navoid needless conversions between hostname and URL\nimprovements of HostDb serialization (write and read)\nparametrize logging and log less on level INFO\ndo not create DNS resolver threads if DNS look-ups are not requested by command-line options\n\nA patch/PR is ready. Depending on the chosen command-line options, a 10-20% speed-up should be visible if DNS look-ups, normalization and filtering are off.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/731"
        ]
    },
    "NUTCH-2951": {
        "Key": "NUTCH-2951",
        "Summary": "Crawl datum with metadata WRITABLE_GENERATE_TIME_KEY awaits fetching forever",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.16,                                            1.17,                                            1.18",
        "Fix Version/s": "1.19",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Lapadula Alessandro",
        "Created": "31/May/22 13:49",
        "Updated": "21/Jun/22 11:49",
        "Resolved": "21/Jun/22 10:37",
        "Description": "When a crawl datum contains a WRITABLE_GENERATE_TIME_KEY metadata it will wait forever.\n\u00a0\n\n\r\n      LongWritable oldGenTime = (LongWritable) crawlDatum.getMetaData()\r\n          .get(Nutch.WRITABLE_GENERATE_TIME_KEY);\r\n      if (oldGenTime != null) { // awaiting fetch & update\r\n        if (oldGenTime.get() + genDelay > curTime) // still wait for\r\n          // update\r\n          context.getCounter(\"Generator\", \"WAIT_FOR_UPDATE\").increment(1);\r\n        return;\r\n      }\n\nThe line\u00a0\n\n\r\ncontext.getCounter(\"Generator\", \"WAIT_FOR_UPDATE\").increment(1); \n\nhas been introduced in version 1.16 without adding the brackets.",
        "Issue Links": [
            "/jira/browse/NUTCH-2737",
            "https://github.com/apache/nutch/pull/732"
        ]
    },
    "NUTCH-2952": {
        "Key": "NUTCH-2952",
        "Summary": "Upgrade core dependencies (Hadoop 3.3.3, log4j 2.17.2)",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Jun/22 15:26",
        "Updated": "19/Aug/22 13:33",
        "Resolved": "09/Aug/22 07:30",
        "Description": "Upgrade the core dependencies to Hadoop 3.3.3 and log4j 2.17.2 - and some more.\n\nHadoop 3.3.3 announces full support for Java 11 and ARM architectures",
        "Issue Links": [
            "/jira/browse/NUTCH-2879",
            "https://github.com/apache/nutch/pull/734"
        ]
    },
    "NUTCH-2953": {
        "Key": "NUTCH-2953",
        "Summary": "Indexer Elastic to ignore SSL issues",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "indexer",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "20/Jun/22 10:17",
        "Updated": "09/Aug/22 08:52",
        "Resolved": "09/Aug/22 07:54",
        "Description": "IndexerElastic (in 1.18) has no support for transporting over HTTPS, but 1.19 does. But 1.19 has no support for ignore SSL issues that are common with self-signed certificates.\nThis patch is for 1.18 only and was made without knowing SSL support was already there in master. Hence, the difference in config property naming, protocol (1.18/patch)\u00a0 == scheme (1.19).",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/741"
        ]
    },
    "NUTCH-2954": {
        "Key": "NUTCH-2954",
        "Summary": "Add unit tests for URLStreamHandlerFactory",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            protocol,                                            test",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "24/Jun/22 12:45",
        "Updated": "19/Aug/22 13:09",
        "Resolved": null,
        "Description": "(suggested by lewismc\u00a0 in discussion of PR #733\n\u00a0I took a look at hadoop-hdfs TestUrlStreamHandler.java as well which I really like the look of. To build out some more confidence in this aspect of the Nutch codebase, we could create some tests for the nutch URLStreamHandlerFactory.java.",
        "Issue Links": []
    },
    "NUTCH-2955": {
        "Key": "NUTCH-2955",
        "Summary": "indexer-solr: replace deprecated/removed field type solr.LatLonType",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "05/Aug/22 07:55",
        "Updated": "17/Aug/22 16:25",
        "Resolved": "17/Aug/22 13:34",
        "Description": "(reported on Nutch user mailing list, see https://lists.apache.org/thread/k9bj3l0v3vfn759tdk50syqd0v2rbjgd)\nWith the introduction of LatLonPointSpatialField (SOLR-10039), the Solr field type LatLonType was deprecated and removed with Solr 9.0. We need to upgrade the Solr schema shipped with the indexer-solr plugin in order to ensure compatibility with newest Solr versions.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/737",
            "https://github.com/apache/nutch/pull/737"
        ]
    },
    "NUTCH-2956": {
        "Key": "NUTCH-2956",
        "Summary": "index-geoip: dependency upgrades and improvements",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "indexer,                                            plugin",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "06/Aug/22 13:01",
        "Updated": "09/Aug/22 10:15",
        "Resolved": "09/Aug/22 08:41",
        "Description": "Upgrades and improvements to the index-geoip plugin:\n\nupgrade the geoip2 dependencies\nexclude transitive dependencies (jackson libs) also provided by Nutch core deps\nallow to read GeoLite2-*.mmdb files without the need to rename them to GeoIP2-*.mmdb\nreview index field names in plugin and Nutch Solr schema:\n\t\nfix typos in field names\nremove unused fields from schema",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/738"
        ]
    },
    "NUTCH-2957": {
        "Key": "NUTCH-2957",
        "Summary": "indexer-solr / Solr schema: add fall-back field definitions for unknown index fields",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "None",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "06/Aug/22 13:25",
        "Updated": "17/Aug/22 16:25",
        "Resolved": "17/Aug/22 13:43",
        "Description": "If a Nutch index field isn't defined in the schema.xml (shipped with indexer-solr) the indexer fails, e.g. here by passing a static index field:\n\n$> bin/nutch indexchecker \\\r\n     -Dplugin.includes='protocol-http|parse-html|index-static|indexer-solr' \\\r\n     -D'index.static=foo:bar' \\\r\n     -doIndex http://localhost/\r\nException in thread \"main\" org.apache.solr.client.solrj.impl.HttpSolrClient$RemoteSolrException: Error from server at http://localhost:8983/solr/nutch: copyField dest :'foo_str' is not an explicit field and doesn't match a dynamicField.\r\n        at org.apache.solr.client.solrj.impl.HttpSolrClient.executeMethod(HttpSolrClient.java:665)\r\n        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:265)\r\n        at org.apache.solr.client.solrj.impl.HttpSolrClient.request(HttpSolrClient.java:248)\r\n        at org.apache.solr.client.solrj.SolrClient.request(SolrClient.java:1290)\r\n        at org.apache.nutch.indexwriter.solr.SolrIndexWriter.push(SolrIndexWriter.java:250)\r\n        ...\r\n\n\nWould be nice to have a fall-back field definition which allows for unspecified fields.",
        "Issue Links": [
            "/jira/browse/NUTCH-2945",
            "https://github.com/apache/nutch/pull/739"
        ]
    },
    "NUTCH-2958": {
        "Key": "NUTCH-2958",
        "Summary": "Upgrade to crawler-commons 1.3",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Implemented",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.19",
        "Component/s": "robots,                                            sitemap",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "08/Aug/22 12:25",
        "Updated": "12/Aug/22 20:03",
        "Resolved": "12/Aug/22 18:43",
        "Description": "see https://github.com/crawler-commons/crawler-commons/#28th-july-2022----crawler-commons-13-released",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/740",
            "https://github.com/apache/nutch/pull/740"
        ]
    },
    "NUTCH-2959": {
        "Key": "NUTCH-2959",
        "Summary": "Upgrade to Apache Tika 2.4.1",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "09/Aug/22 13:33",
        "Updated": "24/May/23 14:46",
        "Resolved": null,
        "Description": null,
        "Issue Links": []
    },
    "NUTCH-2960": {
        "Key": "NUTCH-2960",
        "Summary": "indexer-elastic: remove plugin from binary package to address licensing issues",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "12/Aug/22 18:40",
        "Updated": "01/Mar/23 17:27",
        "Resolved": null,
        "Description": "The license of Elasticsearch has changed with v7.11.0 and upwards and is (if correct) not more compatible with the Apache license. Accordingly, we should not further ship Elastic jars with the binary package.\nIt should be possible to keep the indexer-elastic plugin in the source package as an optional dependency (indexer-solr is the default indexing backend and more are available).",
        "Issue Links": [
            "/jira/browse/NUTCH-2988",
            "/jira/browse/NUTCH-2927"
        ]
    },
    "NUTCH-2961": {
        "Key": "NUTCH-2961",
        "Summary": "Upgrade dependencies of parsefilter-naivebayes",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.18",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "15/Aug/22 08:46",
        "Updated": "15/Aug/22 08:46",
        "Resolved": null,
        "Description": "The dependencies (Mahout 0.9, Lucene 5.5.0) of parsefilter-naivebayes date back to 2016/2017 and may need an upgrade.",
        "Issue Links": []
    },
    "NUTCH-2962": {
        "Key": "NUTCH-2962",
        "Summary": "Update and complete package info of protocol plugins",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "documentation,                                            plugin,                                            protocol",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "15/Aug/22 14:29",
        "Updated": "19/Aug/22 14:56",
        "Resolved": "19/Aug/22 13:27",
        "Description": "The package info files of the protocol plugins are partially outdated or missing.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/744"
        ]
    },
    "NUTCH-2963": {
        "Key": "NUTCH-2963",
        "Summary": "Upgrade dependencies before release of 1.19",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "19/Aug/22 15:35",
        "Updated": "21/Aug/22 11:47",
        "Resolved": "21/Aug/22 10:39",
        "Description": "(before preparing the 1.19 release candidates)\n\nlook for upgrades to dependencies of Nutch core and plugins\nonly minor upgrades which do not require API changes\nand focus on security upgrades",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/747"
        ]
    },
    "NUTCH-2964": {
        "Key": "NUTCH-2964",
        "Summary": "Add instructions how to combine Docker containers and Solr for indexing",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "docker",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "21/Aug/22 10:08",
        "Updated": "21/Aug/22 10:08",
        "Resolved": null,
        "Description": "With NUTCH-2883 the Nutch Docker containers can include the Nutch server and WebApp. However, indexing into Solr may require some additional instructions and settings. Alternatively, we may include Solr in the Docker image.",
        "Issue Links": []
    },
    "NUTCH-2965": {
        "Key": "NUTCH-2965",
        "Summary": "Nutch server requires a restart in order to reload modified configuration files",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "nutch server,                                            web gui",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "21/Aug/22 10:11",
        "Updated": "21/Aug/22 10:13",
        "Resolved": null,
        "Description": "(seen while testing Docker containers and NUTCH-2883)\nNutch server requires a restart in order to reload modified configuration files. This was observed because the Docker images include an empty nutch-site.xml and after adding http.agent.name I had to kill the Nutch server so that the modified configuration file is loaded.",
        "Issue Links": []
    },
    "NUTCH-2966": {
        "Key": "NUTCH-2966",
        "Summary": "Nutch WebApp and server sometimes fail to put all seeds into seed file",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "nutch server,                                            web gui",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "21/Aug/22 10:13",
        "Updated": "21/Aug/22 10:13",
        "Resolved": null,
        "Description": "(seen while testing Docker containers and NUTCH-2883)\nNutch WebApp and server sometimes fail to put all seeds into the seed file.",
        "Issue Links": []
    },
    "NUTCH-2967": {
        "Key": "NUTCH-2967",
        "Summary": "Docker: build image from local source tree",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "docker",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "22/Aug/22 10:54",
        "Updated": "22/Aug/22 10:54",
        "Resolved": null,
        "Description": "In order to test local changes in the source tree, would be nice to be able to build the Docker image from a local checkout of the Nutch sources, not pulling from Github.",
        "Issue Links": []
    },
    "NUTCH-2968": {
        "Key": "NUTCH-2968",
        "Summary": "Include Docker build configuration into release package",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "docker",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "22/Aug/22 11:21",
        "Updated": "22/Aug/22 11:21",
        "Resolved": null,
        "Description": "The Nutch release packages do not include the Docker build configuration. Maybe we want to include those in the source package, so that it's possible to build a Nutch image for a specific release. This would require NUTCH-2967.",
        "Issue Links": []
    },
    "NUTCH-2969": {
        "Key": "NUTCH-2969",
        "Summary": "Javadoc: Javascript search is not working when built on JDK 11",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.19",
        "Component/s": "build,                                            documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "22/Aug/22 13:17",
        "Updated": "22/Aug/22 14:59",
        "Resolved": "22/Aug/22 13:40",
        "Description": "Newer Java versions create a nice Javascript-backed search box on the Javadocs. When built on JDK 11 clicking on search results leads to 404 results with a /undefined/ path element in the URL. This is caused by JDK-8215291 - Nutch does not use Java modules.\nThe issue can be solved by adding --no-module-directories as argument to the javadoc command. However, this only works for JDK 11 and will break the javadoc build for later JDK versions, see JDK-8215582. \nSee also\n\nhttps://stackoverflow.com/questions/52326318/maven-javadoc-search-redirects-to-undefined-url\nhttps://github.com/crawler-commons/crawler-commons/pull/380\n\nNotes:\n\nseen while preparing a release candidate for 1.19 (will quickly commit the solution)\nadding --no-module-directories should be conditional for JDK 11 only\n\t\nant: see condition\ngradle:\n\nif (JavaVersion.current().isJava11()) {\r\n  options.addBooleanOption(\"-no-module-directories\", true)\r\n}",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/749"
        ]
    },
    "NUTCH-2970": {
        "Key": "NUTCH-2970",
        "Summary": "Ensure compatibility with JDK 17",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "build,                                            runtime",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Sep/22 10:19",
        "Updated": "28/Feb/23 17:05",
        "Resolved": null,
        "Description": "Ensure that Nutch builds and runs using Java/JDK 17",
        "Issue Links": [
            "/jira/browse/NUTCH-2987"
        ]
    },
    "NUTCH-2971": {
        "Key": "NUTCH-2970 Ensure compatibility with JDK 17",
        "Summary": "Unit tests fail with JDK 17",
        "Type": "Sub-task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "build,                                            test",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "11/Sep/22 10:24",
        "Updated": "11/Sep/22 10:25",
        "Resolved": null,
        "Description": "When build and run using JDK 17 unit tests in TestIndexerMapReduce and TestCrawlDbStates fail with:\n\njava.lang.reflect.InaccessibleObjectException: Unable to make protected final java.lang.Class ... accessible: ...",
        "Issue Links": []
    },
    "NUTCH-2972": {
        "Key": "NUTCH-2970 Ensure compatibility with JDK 17",
        "Summary": "Javadoc build fails using JDK 17",
        "Type": "Sub-task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "build,                                            documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "11/Sep/22 10:29",
        "Updated": "06/Mar/23 12:08",
        "Resolved": "06/Mar/23 10:46",
        "Description": "Creating the Javadocs fails using JDK 17:\n\n$> JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64 ant javadoc\r\n...\r\n  [javadoc] /home/wastl/data0/proj/crawler/nutch/git/trunk/src/java/org/apache/nutch/segment/SegmentMerger.java:79: error: heading used out of sequence: <H3>, compared to implicit preceding heading: <H1>\r\n  [javadoc]  * <h3>Important Notes</h3> <h4>Which parts are merged?</h4>\r\n...\r\n  [javadoc] 3 errors\r\n  [javadoc] 100 warnings\r\n\n\nAll 3 errors must be fixed, would be good also to address the warnings.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/760"
        ]
    },
    "NUTCH-2973": {
        "Key": "NUTCH-2973",
        "Summary": "Single domain names (eg https://localnet) can't be crawled - filtering fails",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Blocker",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "David Smith",
        "Created": "20/Oct/22 02:05",
        "Updated": "06/Mar/23 12:22",
        "Resolved": "06/Mar/23 12:22",
        "Description": "There appears to be a bug within the core of Nutch that fails to permit any single domain name URLs to be crawled.\u00a0 Example:\nhttps://localnet/something.aspx\nThe issue is that Nutch is rejecting any url with a single element domain name such as localnet above. \"localnet.com\" is not rejected, nor is \"local.localnet\". It almost feels as if there's a chunk of code within Nutch that's unrelated to the filtering mechanisms that rejects URLs outright if they don't have a WWW style format and a WWW-style domain such as .COM\nError message:\nTotal urls rejected by filters: 1\nI've checked and updated all the\u00a0filter files in the conf directory. Even making then incredibly permissive (effectively \"crawl everything\") has not helped.\u00a0 \u00a0 Immediately that a dot (.) is added to the domain name it is not rejected - eg blah.localnet.",
        "Issue Links": [
            "/jira/browse/NUTCH-2985"
        ]
    },
    "NUTCH-2974": {
        "Key": "NUTCH-2974",
        "Summary": "Ant build fails with \"Unparseable date\" on certain platforms",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "build",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "21/Nov/22 09:33",
        "Updated": "17/Feb/23 15:49",
        "Resolved": "17/Feb/23 14:18",
        "Description": "When touching the configuration templates the ant build fails on certain platforms, see NUTCH-2512 and recently by Kamil Mroczek on the users list, including a fix.\nHowever, we should also consider removing the \"touch\" action if it's not clear what the purpose of it is - it's there since the initial import of the Nutch source code to the Apache repository. Could be obsolete now.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/752"
        ]
    },
    "NUTCH-2975": {
        "Key": "NUTCH-2975",
        "Summary": "Generate 0 partition when used with sitemap",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Not A Problem",
        "Affects Version/s": "1.16",
        "Fix Version/s": "None",
        "Component/s": "generator,                                            sitemap",
        "Assignee": null,
        "Reporter": "Lucas Pauchard",
        "Created": "23/Nov/22 10:34",
        "Updated": "24/Nov/22 13:22",
        "Resolved": "24/Nov/22 09:02",
        "Description": "Issue\nWe are facing strange issue since we have updated our Proxmox from 7.2-4 to 7.2-11 which host the VMs/containers used for our Hadoop cluster.\nWhen we are using the sitemap component to add URLs, the generator process doesn't work. It generates 0 partition.\nBut if we call a second time the generator process, this time the generator actually create a partition segment.\nIt happens only when we use the sitemap process. If we use only the Injector process, this issue doesn't happen.\nI checked the logs and the generator just seems to find no record in the crawldb. It is like the crawldb wasn't available or the files are locked.\nHere is the command used :\nSitemap :\n\n\r\nhadoop jar <job> org.apache.nutch.crawl.Generator crawl_000_111/crawldb crawl_000_111/segment -numFetchers 2 -mapper=1 -reducer=1 -noFilter -noNorm -force\r\n\n\nIt returns as expected :\nSitemap output\n2022-11-23 10:37:22,194 INFO util.SitemapProcessor: SitemapProcessor: Total records rejected by filters: 0\n2022-11-23 10:37:22,195 INFO util.SitemapProcessor: SitemapProcessor: Total sitemaps from HostDb: 0\n2022-11-23 10:37:22,195 INFO util.SitemapProcessor: SitemapProcessor: Total sitemaps from seed urls: 1\n2022-11-23 10:37:22,196 INFO util.SitemapProcessor: SitemapProcessor: Total failed sitemap fetches: 0\n2022-11-23 10:37:22,196 INFO util.SitemapProcessor: SitemapProcessor: Total new sitemap entries added: 151\n\nGeneretor :\n\n\r\nhadoop jar <job> org.apache.nutch.crawl.Generator crawl_000_111/crawldb crawl_000_111/segment -numFetchers 2 -mapper=1 -reducer=1 -noFilter -noNorm -force\r\n\n\n1st time it returns :\n\n\r\n2022-11-23 11:25:15,202 WARN crawl.Generator: Generator: 0 records selected for fetching, exiting ...\r\n\n\n2nd time it returns :\n\n\r\n2022-11-23 11:27:43,007 INFO crawl.Generator: Generator: Partitioning selected urls for politeness.\r\n2022-11-23 11:27:44,009 INFO crawl.Generator: Generator: segment: crawl_000_111/segment/20221123112744\r\n...\r\n2022-11-23 11:28:34,061 INFO crawl.Generator: Generator: finished at 2022-11-23 11:28:34, elapsed: 00:01:53",
        "Issue Links": []
    },
    "NUTCH-2976": {
        "Key": "NUTCH-2976",
        "Summary": "SitemapProcessor: verify sitemap values added from sitemap to CrawlDB (priority, modification time and change frequency)",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "sitemap",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "24/Nov/22 13:19",
        "Updated": "24/Nov/22 13:19",
        "Resolved": null,
        "Description": "SitemapProcesser writes values from the sitemap into the CrawlDB without verification or plausibility check:\n\npriority - used as CrawlDatum score\nmodification time\nchange frequency - used as fetch interval\n\nSince these values in the sitemap cannot be trusted, the processor should make sure that they are in acceptable ranges:\n\npriority > 0.0 (a score of 0.0 would cause that a URL is never fetched)\nmodification time: not in the future\nchange frequency / fetch interval within [db.fetch.schedule.adaptive.min_interval, db.fetch.schedule.max_interval]\n\nSee also NUTCH-2975",
        "Issue Links": []
    },
    "NUTCH-2977": {
        "Key": "NUTCH-2977",
        "Summary": "Support for showing dependency tree",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "07/Dec/22 16:48",
        "Updated": "07/Dec/22 21:51",
        "Resolved": "07/Dec/22 17:15",
        "Description": "I am upgrading Nutch to slf4j 2 and need to get rid of old 1.7 stuff, and especially reload4j. I desperately need this function for that.\n\u00a0\n$ ant dependencytree\n\u00a0\nwill now show the tree.",
        "Issue Links": []
    },
    "NUTCH-2978": {
        "Key": "NUTCH-2978",
        "Summary": "Move to slf4j2 and remove log4j1 and reload4j",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Markus Jelsma",
        "Created": "07/Dec/22 16:57",
        "Updated": "22/Dec/22 11:33",
        "Resolved": null,
        "Description": "I got in trouble upgrading some dependencies and got a lot of LinkageErrors today, or with a Tika upgrade, disappearing logs. This patch fixes that by moving to slf4j2, using the corrent log4j2-slfj4-impl2 and getting rid of old log4j -> reload4j.\n\u00a0\nThis patch fixes it.",
        "Issue Links": []
    },
    "NUTCH-2979": {
        "Key": "NUTCH-2979",
        "Summary": "Upgrade Commons Text to 1.10.0",
        "Type": "Bug",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "build,                                            plugin",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "22/Dec/22 13:32",
        "Updated": "04/Jan/23 19:53",
        "Resolved": null,
        "Description": "In order to address CVE-2022-42889 we should upgrade to commons-text 1.10.0:\n\nNutch core depends on 1.4 which is not affected by the CVE\nthe plugins lib-htmlunit and any23 depend on a vulnerable commons-text version (1.5 - 1.9)",
        "Issue Links": []
    },
    "NUTCH-2980": {
        "Key": "NUTCH-2980",
        "Summary": "Upgrade Selenium Java to 4.7.2",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            protocol",
        "Assignee": null,
        "Reporter": "Kamil Mroczek",
        "Created": "20/Jan/23 03:47",
        "Updated": "18/Feb/23 15:47",
        "Resolved": "18/Feb/23 14:58",
        "Description": "Selenium version is quite old and had some issues processing a website. Once I switched to the latest version I was able to scrape that websites. Good to keep it up to date since we were already 1 major release behind.\nUpgrading Selenium Java from 3.141.59 to 4.7.2 and Selenium HTMLUnit from 2.35.1 to 4.7.0.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/753"
        ]
    },
    "NUTCH-2981": {
        "Key": "NUTCH-2981",
        "Summary": "Automatic creation of license list and notice files",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "20/Jan/23 14:38",
        "Updated": "20/Jan/23 14:38",
        "Resolved": null,
        "Description": "Compiling the list of licenses and the notice files is a complex task, recently done in NUTCH-2290 PR#743. This is supported by a Python Jupyter notebook, but should be further automatized, so that it can be run if necessary - on dependency upgrades or before releases. See also STORM-3361, STORM-3437 and Storm dev-tools.",
        "Issue Links": []
    },
    "NUTCH-2982": {
        "Key": "NUTCH-2982",
        "Summary": "Generator: parameter for URL normalization not passed forward",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "generator",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "26/Jan/23 12:56",
        "Updated": "06/Mar/23 12:08",
        "Resolved": "06/Mar/23 10:45",
        "Description": "The parameter \"norm\" (for URL normalization) is not passed properly forward in the chain of generate(...) methods, see Generator.java, 9a1ed40, line 771",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/754",
            "https://github.com/apache/nutch/pull/755"
        ]
    },
    "NUTCH-2983": {
        "Key": "NUTCH-2983",
        "Summary": "nutch-default.xml improvements",
        "Type": "Improvement",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "configuration,                                            documentation",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Feb/23 16:23",
        "Updated": "06/Mar/23 12:08",
        "Resolved": "06/Mar/23 10:50",
        "Description": "This issue covers a couple of improvements in the nutch-default.xml\n\nremoval of obsolete properties\ncomplete description of properties related to following internal/external links in CrawlDb and LinkDb\ntypos and formatting",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/756"
        ]
    },
    "NUTCH-2984": {
        "Key": "NUTCH-2984",
        "Summary": "Drop test proxy server and benchmark tool",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Done",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "test,                                            tool",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "24/Feb/23 14:24",
        "Updated": "17/Mar/23 16:38",
        "Resolved": "17/Mar/23 15:24",
        "Description": "The test proxy server and the benchmark took, introduced in NUTCH-863, is rarely used and one of the challenging tasks when resolving NUTCH-2596 (the proxy server is based on Jetty).\nIf there are no objections, I'd opt to remove it.",
        "Issue Links": [
            "/jira/browse/NUTCH-2596",
            "https://github.com/apache/nutch/pull/757",
            "https://github.com/apache/nutch/pull/757"
        ]
    },
    "NUTCH-2985": {
        "Key": "NUTCH-2985",
        "Summary": "Disable plugin urlfilter-validator by default",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "configuration,                                            urlfilter",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "24/Feb/23 15:22",
        "Updated": "06/Mar/23 12:08",
        "Resolved": "06/Mar/23 10:51",
        "Description": "The plugin urlfilter-validator is activated by default (in nutch-default.xml) but has two major issues which may confuse users of Nutch:\n\nsingle-part domain names (localhost, etc.) are not allowed (NUTCH-2973)\nIPv6 host names are rejected as invalid (NUTCH-2705)\n\nWhat about disabling it by default to overcome these issues?",
        "Issue Links": [
            "/jira/browse/NUTCH-2973",
            "https://github.com/apache/nutch/pull/759",
            "https://github.com/apache/nutch/pull/759"
        ]
    },
    "NUTCH-2986": {
        "Key": "NUTCH-2986",
        "Summary": "Depend urlfilter-validator on commons-validator routine UrlValidator",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "plugin,                                            urlfilter",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "24/Feb/23 15:32",
        "Updated": "24/Feb/23 15:32",
        "Resolved": null,
        "Description": "The plugin urlfilter-validator is an outdated copy of the class UrlValidator of the commons-validator project. Rewriting the plugin to directly use the recent commons-validator routine UrlValidator (see also UrlValidator source code) should \n\nsolve issues with the plugin (eg. NUTCH-2705)\nmake maintenance easier in the future",
        "Issue Links": []
    },
    "NUTCH-2987": {
        "Key": "NUTCH-2987",
        "Summary": "Upgrade to Java 17",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.20",
        "Fix Version/s": "1.20",
        "Component/s": "build",
        "Assignee": null,
        "Reporter": "Sebastian Nagel",
        "Created": "28/Feb/23 17:04",
        "Updated": "28/Feb/23 17:05",
        "Resolved": null,
        "Description": "Upgrade build to use Java 17 LTS. Depends on NUTCH-2970 and eventually HADOOP-17177.",
        "Issue Links": [
            "/jira/browse/HADOOP-17177",
            "/jira/browse/NUTCH-2970"
        ]
    },
    "NUTCH-2988": {
        "Key": "NUTCH-2988",
        "Summary": "Elasticsearch 7.13.2 compatible with ASL 2.0?",
        "Type": "Task",
        "Status": "Resolved",
        "Priority": "Minor",
        "Resolution": "Duplicate",
        "Affects Version/s": "None",
        "Fix Version/s": "None",
        "Component/s": "None",
        "Assignee": null,
        "Reporter": "Tim Allison",
        "Created": "28/Feb/23 21:08",
        "Updated": "01/Mar/23 17:27",
        "Resolved": "01/Mar/23 17:27",
        "Description": "In the latest release of at least the 1.x branch of Nutch, the elasticsearch high level java client is at 7.13.2, which is after the great schism.  Or, the last purely ASL 2.0 license was in 7.10.2.\nSo, do we need to downgrade to 7.10.2 or is Elasticsearch's new licensing plan suitable to be released within an ASF project?\nOr, is the client as opposed to the main search project still actually ASL 2.0?\nRef: https://github.com/elastic/elasticsearch/blob/v7.13.2/LICENSE.txt",
        "Issue Links": [
            "/jira/browse/NUTCH-2960"
        ]
    },
    "NUTCH-2989": {
        "Key": "NUTCH-2989",
        "Summary": "Can't have username/pw AND https on elastic-indexer?!",
        "Type": "Task",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Tim Allison",
        "Created": "01/Mar/23 20:28",
        "Updated": "3 days ago 12:07",
        "Resolved": null,
        "Description": "While working on NUTCH-2920, I copied+pasted the elastic indexer.  As part of that process, I noticed that basic auth doesn't work with https.\n\n\r\nif (auth) {\r\n        restClientBuilder\r\n            .setHttpClientConfigCallback(new HttpClientConfigCallback() {\r\n              @Override\r\n              public HttpAsyncClientBuilder customizeHttpClient(\r\n                  HttpAsyncClientBuilder arg0) {\r\n                return arg0.setDefaultCredentialsProvider(credentialsProvider);\r\n              }\r\n            });\r\n      }\r\n\r\n      // In case of HTTPS, set the client up for ignoring problems with self-signed\r\n      // certificates and stuff\r\n      if (\"https\".equals(scheme)) {\r\n        try {\r\n          SSLContextBuilder sslBuilder = SSLContexts.custom();\r\n          sslBuilder.loadTrustMaterial(null, new TrustSelfSignedStrategy());\r\n          final SSLContext sslContext = sslBuilder.build();\r\n\r\n          restClientBuilder.setHttpClientConfigCallback(new HttpClientConfigCallback() {\r\n            @Override\r\n            public HttpAsyncClientBuilder customizeHttpClient(HttpAsyncClientBuilder httpClientBuilder) {\r\n              // ignore issues with self-signed certificates\r\n              httpClientBuilder.setSSLHostnameVerifier(NoopHostnameVerifier.INSTANCE);\r\n              return httpClientBuilder.setSSLContext(sslContext);\r\n            }\r\n          });\r\n        } catch (Exception e) {\r\n          LOG.error(\"Error setting up SSLContext because: \" + e.getMessage(), e);\r\n        }\r\n      }\r\n\n\nOn NUTCH-2920, I fixed this for the opensearch-indexer by adding another if (auth) statement under the https branch.\nIf this is an actual issue, I'm happy to open a PR.  If I've misunderstood the code or the design, please close as \"not a problem\".",
        "Issue Links": []
    },
    "NUTCH-2990": {
        "Key": "NUTCH-2990",
        "Summary": "HttpRobotRulesParser to follow 5 redirects as specified by RFC 9309",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "protocol,                                            robots",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "17/Mar/23 15:49",
        "Updated": "21/May/23 19:29",
        "Resolved": null,
        "Description": "The robots.txt parser (HttpRobotRulesParser) follows only one redirect when fetching the robots.txt while the robots.txt RFC 9309 recommends to follow 5 redirects:\n 2.3.1.2. Redirects\nIt's possible that a server responds to a robots.txt fetch request with a redirect, such as HTTP 301 or HTTP 302 in the case of HTTP. The crawlers SHOULD follow at least five consecutive redirects, even across authorities (for example, hosts in the case of HTTP).\nIf a robots.txt file is reached within five consecutive redirects, the robots.txt file MUST be fetched, parsed, and its rules followed in the context of the initial authority. If there are more than five consecutive redirects, crawlers MAY assume that the robots.txt file is unavailable.\n(https://datatracker.ietf.org/doc/html/rfc9309#name-redirects)\nWhile following redirects, the parser should check whether the redirect location is itself a \"/robots.txt\" on a different host and then try to read it from the cache.",
        "Issue Links": []
    },
    "NUTCH-2991": {
        "Key": "NUTCH-2991",
        "Summary": "Support HTTP/S Header Authorization for Solr connections",
        "Type": "New Feature",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Implemented",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Marcos Gomez",
        "Created": "12/May/23 10:54",
        "Updated": "20/Jun/23 15:00",
        "Resolved": "06/Jun/23 12:52",
        "Description": "Currently only Basic Authentication is possible with Solr, but I have an issue to connect to a Solr instance that use Token authentication.\nSo it should be possible to define the value of Authorization Header to provide the name and token value, that will be send on the HTTP request to Solr.\nHeader:\n\nAuthorization: Name Value\n\nSo in the configuration should be able to set up something like this to populate the header:\u00a0\n\nsolr.auth.header.name=Bearer\nsolr.auth.header.value=JWT_TOKEN",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/763"
        ]
    },
    "NUTCH-2992": {
        "Key": "NUTCH-2992",
        "Summary": "Fetcher: always block fetch queues when exceptions threshold is reached",
        "Type": "Bug",
        "Status": "Resolved",
        "Priority": "Major",
        "Resolution": "Fixed",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "fetcher",
        "Assignee": "Sebastian Nagel",
        "Reporter": "Sebastian Nagel",
        "Created": "16/May/23 15:29",
        "Updated": "23/May/23 15:55",
        "Resolved": "23/May/23 14:25",
        "Description": "When the exception threshold of a fetch queue is reached (defined by fetcher.max.exceptions.per.queue, NUTCH-769), the queue must be blocked, even if it is empty at this time. Otherwise, if the queue feeder is still alive, new items may be added to the queue later on. See FetchItemQueues.java, line 306 for the reason and NUTCH-2947 for a similar bug.",
        "Issue Links": [
            "https://github.com/apache/nutch/pull/762"
        ]
    },
    "NUTCH-2993": {
        "Key": "NUTCH-2993",
        "Summary": "ScoringDepth plugin to skip depth check based on URL Pattern",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Minor",
        "Resolution": "Unresolved",
        "Affects Version/s": "None",
        "Fix Version/s": "1.20",
        "Component/s": "None",
        "Assignee": "Markus Jelsma",
        "Reporter": "Markus Jelsma",
        "Created": "08/Jun/23 12:23",
        "Updated": "28/Jun/23 16:05",
        "Resolved": null,
        "Description": "We do not want some crawl to go deep and broad, but instead focus it on a narrow section of sites.\nThis patch overrides maxDepth for outlinks of URLs matching a configured pattern. URL not matching the pattern get the default max depth value configured.",
        "Issue Links": []
    },
    "NUTCH-2994": {
        "Key": "NUTCH-2994",
        "Summary": "Implement an indexer for OpenSearch 2.x",
        "Type": "Improvement",
        "Status": "Open",
        "Priority": "Major",
        "Resolution": "Unresolved",
        "Affects Version/s": "1.19",
        "Fix Version/s": "1.20",
        "Component/s": "indexer,                                            plugin",
        "Assignee": null,
        "Reporter": "Tim Allison",
        "Created": "08/Jun/23 18:36",
        "Updated": "3 days ago 12:06",
        "Resolved": null,
        "Description": "Over on NUTCH-2920, we added an indexer for OpenSearch 1.x.  We should do this for 2.x.  This is blocked by: https://github.com/opensearch-project/opensearch-java/issues/181 \nWe could reinvent BulkProcessor/BulkIngester ourselves, but we really, really shouldn't do that.",
        "Issue Links": []
    }
}